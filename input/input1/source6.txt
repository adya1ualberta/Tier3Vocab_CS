cs1565 exercises exercises exercises in the following set of exercises reference to chapters and figures are made to fundamentals of database systems elmasri and navathe benjamin cummings edition reference to edition in parentheses exercise points consider the following set of requirements for a university database that is used to keep track of students transcripts a the university keeps track of each student name student number social security number current address and phone permanent address and phone birthdate sex class freshman sophomore graduate major department minor department if any and degree program b a b s ph d some user applications need to refer to the city state and zip of the student permanent address and to the students last name both social security number and student number have unique values for each student b each department is described by a name department code office number office phone and college both name and code have unique values for each department c each course has a course name description code number number of semester hours level and offering department the value of code number is unique for each course d each section has an instructor semester year course and section number the section number distinguishes different sections of the same course that are taught during the same semester year its values are up to the number of sections taught during each semester e a grade report has a student section and grade design an er schema for this application and draw an er diagram for that schema specify key attributes of each entity type and structural constraints on each relationship type note any specified requirements and make appropriate assumptions to make the specification complete exercise points programming required the purpose of this exercise is to introduce the concepts of relational database you will use oracle to do the following a define a relation called student with the following attributes studentname socialsecuritynumber coursemodule completiondate grade b input data into this relation student name ssn mod date grade john doe john doe john doe 70 terry smith 60 terry smith 55 terry smith 80 http people cs pitt edu chang exer html cs1565 exercises judy smith 85 judy smith 60 c formulate and answer the following queries can you formulate and answer every query in sql if not sketch an approach to process the query list the name of students who have completed any course module list the name of students who have completed course module q3 list the name of students who have completed at least two course modules list the name of students who have completed any course module during the period to 90 list the name of students who have skipped any course module for example judy smith has skipped course module and terry smith has skipped course module exercise points i specify the following queries on the database schema shown in figure fig in ed using a relational algebra and b sql show the result of each query if applied to the database of figure fig 6 in ed there are six subproblems each worth points a retrieve the names of employees in department who work more than hours per week on the productx project b list the names of employees who have a dependent with the same first name as themselves c find the names of employees that are directly supervised by franklin wong d for each project list the project name and the total hours per week by all employees spend on that project e retrieve the names of employees who work on every project f retrieve the names of employees who do not work on any project ii points draw a flowchart and write in pseudocode a program to translate simple sql statements of the form select attributelist from relationlist where conditions into relational algebra document and explain your algorithm carefully test your program or algorithm on the following examples select s from s where s select s city from s sp where p and s s sp s exercise points a a parts file with part as hash key includes records with the following part values 4871 7115 4750 the file uses buckets numbered to each http people cs pitt edu chang exer html cs1565 exercises budget is one disk block and holds two records load these records into the file in the given order using the hash function h k k mod calculate the average number of block accesses for a random retrieval on part b load the records of part a into expandable hash files based on extendible hashing show the structure of the directory and global and local depths at each step c a parts file with part as key field includes records with the following part values 60 71 suppose the search field values are inserted in the given order in a b tree of order p show how the tree will expand and what the final tree looks like exercise points programming required i points do exercise subproblems i a to i d using relational calculus a retrieve the names of employees in department who work more than hours per week on the productx project b list the names of employees who have a dependent with the same first name as themselves c find the names of employees that are directly supervised by franklin wong d for each project list the project name and the total hours per week by all employees spend on that project you may assume a sum function that can compute from a set of pairs projectname hours the results projectname totalhours ii points using oracle design a menudriven user interface for a student information system supporting the processing of queries of exercise the relation is the same as the one you used in exercise the menu looks something like the following you can design it better student information system please select an option ad a list the name of students who have completed any course module b list the name of students who have completed a course module c list the name of students who have completed at least two course modules d list the name of students who have completed a course module during a specified period note that after option b is selected the user should be prompted to enter module name similarly after option d is selected the user should be prompted to enter nodule name starting date and ending date you may find the following cs1565 exercises http people cs pitt edu chang exer html ssn course code sec num ssn ssn course code sec num gourse code ssn course code sec num sec num ii points design a hierarchical database schema for the university administrative database described in part i by drawing a schema diagram similar to the schema diagram for the company database shown in figure d fig in ed iii point refer again to figure d fig in ed the following is a pascal program to retrieve information from the company database describe the intended query get first employee while do begin get next dependent within parent employee while do begin if depname fname then writeln fname lname get next dependent within parent employee end get next employee end course description this course is the second data management course in the cs department as such it has the following main objectives first to study topics that go beyond the traditional relational database man agement system framework such as information retrieval data mining and data warehousing second to study security related issues as they arise in web database environments third to expose students to ad vanced database applications and also to related data information management and analysis technologies while gaining hands on experience prerequisites a grade of c or better in cs and cs is required or permission of the instructor good working knowledge of java and familiarity with unix are assumed class web page http db cs pitt edu courses all handouts and class notes will be published on the class web page you are expected to check this page frequently at least twice a week textbook there is no single textbook with enough coverage of all the material that we want to discuss in this class we will rely on online references and also on o reilly safari bookshelf for which the university has institutional access i e you will not have to buy extra books class communications policies new please read carefully october assignment released november assignment due november assignment released november assignment due november assignment released december assignment due december final exam mailing list all students will be automatically subscribed to the class mailing list so that they receive time sensitive announcements from the instructor and ta in class student responses we will use the socrative system to capture student responses to questions and record attendance it is crucial that you provide your pitt user account name e g at the name prompt to properly record your answers email to instructor and ta instead of email we will use the piazza system which is essentially a web based bulletin board for questions and clarifications to assignments more instructions will be posted on the class web site confidential email to instructor and ta in case you need to communicate with the instructor and ta outside of the piazza system i e for confidential matters you should send email to staff cs pitt edu we will make every effort to respond to all email requests within one business day at the latest due to spam filtering you should always use your pitt email address when sending email and include your full name cell phone use new please read carefully answering a cell phone or texting is very disruptive and hence any use of a cell phone to make or receive calls or text messages is not permitted in the class or recitation cell phones must be switched to silent mode and if you have a phone call which cannot wait until the end of the class you need to step out of the class and then answer it technology policy new please read carefully since this is the century the use of laptops tablets and other digital devices is allowed in class however when using digital devices in the classroom you must be mindful when you are emailing tweeting texting surfing etc you are not paying attention research shows that no one can multitask that well you included paying attention and taking good notes is essential to success in this course isn t that why you are here be respectful your use of digital devices should not distract other students in the class it is unlikely that taking notes or searching class related topics will be distracting to the other students however viewing videos of kittens or ice bucket challenges gone well or gone wrong will likely distract others complaints about inappropriate technology use in class will result in your privileges being curtailed or revoked be honest emailing surfing and the use of any other applications or technologies is not allowed during examinations doing so unless explicitly allowed is considered cheating in the exam and will be dealt accordingly audio video recording to ensure the free and open discussion of ideas students may not record classroom lectures discussion and or activities without the advance written permission of the instructor and any such recording properly approved in advance can be used solely for the student own private use grading policy unless explicitly noted otherwise the work in this course is to be done independently discus sions with other students on the assignments should be limited to understanding the statement of the problems except when assignments are to be done in groups in which case it is expected of members of the same group to work together cheating in any way including giving your work to someone else will result in an f for the course and a report to the appropriate university authority submissions that are alike in a substantive way will be considered to be cheating by all involved parties please protect yourselves by only storing your files in private directories and by retrieving all printouts promptly students are expected to abide by the dietrich school of arts and sciences academic integrity code of conduct posted at http www as pitt edu fac policies academic integrity all assignments must be submitted electronically grades can be appealed up to two weeks after they have been posted no appeals will be considered after that time late policy a late assignment will receive a deduction of points if it is up to one day past the deadline and points if it is up to two days past the deadline assignments that are past two days late will not be accepted make up policy students are expected to be present for all exams and quizzes make up exams will only be given in the event of an emergency and only if the instructor is informed in advance failure to notify the instructor prior to missing an exam will result in a zero for the exam final exam conflict policy in case you have a final exam conflict i e have more than two exams sched uled on the same date during finals week you need to notify the instructors of all classes involved in order to resolve the conflict by the sixth week of classes according to the university policy posted at http www registrar pitt edu final examination schedules html students with disabilities if you have a disability for which you are or may be requesting an accommodation you are encouraged to contact both your instructor and disability resources and services william pitt union or tty as early as possible in the term drs will verify your disability and determine reasonable accomodations for this course their web site is http www drs pitt edu religious observances in order to accommodate the observance of religious holidays students should inform the instructor by email of any such days that conflict with scheduled class activities within the first two weeks of the term copyrighted material all material provided through this web site is subject to copyright this applies to class recitation notes slides assignments solutions project descriptions etc you are allowed and expected to use all the provided material for personal use however you are strictly prohibited from sharing the material with others in general and from posting the material on the web or other file sharing venues in particular outline a detailed reading guide will be published on the web page along with the class notes and additional online articles and resources time permitting we will cover the following topics course description this special topics course aims to expose students to different data management data ma nipulation and data analysis techniques the class will cover all the major data management paradigms relational sql xml xquery rdf sparql including nosql and data stream processing approaches going beyond traditional data management techniques the class will expose students to information retrieval data mining data warehousing network analysis and other data analysis topics time permitting the class will include big data processing techniques such as the map reduce framework prerequisites a grade of c or better in cs and cs is required or permission of the instructor good working knowledge of java and familiarity with unix are assumed having passed a statistics course is highly encouraged anti requisites given the significant overlap with past offerings of students who have already passed will not be allowed to register for this class class web page http db cs pitt edu courses all handouts and class notes will be published on the class web page you are expected to check this page frequently at least twice a week textbook there is no single textbook with enough coverage of all the material that we want to discuss in this class we will rely on online references and also on o reilly safari bookshelf for which the university has institutional access i e you will not have to buy extra books course grading assignments there will be assignments most of which will have a significant programming component see important dates for deadlines class participation for both lecture and recitations including in class quizzes we will use the socrative system to capture student responses and record attendance midterm exam wednesday february am pm sensq final exam saturday april pm pm sensq important dates january assignment test released january assignment released january assignment due february assignment due february assignment released february assignment due february midterm exam class communications policies new please read carefully march assignment released march assignment released march assignment due april assignment released april assignment due april assignment due april final exam mailing list all students will be automatically subscribed to the class mailing list so that they receive time sensitive announcements from the instructor and ta in class student responses we will use the socrative system http www socrative com to capture student responses to questions and record attendance it is crucial that you provide your pitt user account name e g at the name prompt to properly record your answers email to instructor and ta instead of email we will use the piazza system which is essentially a web based bulletin board for questions and clarifications to assignments more instructions will be posted on the class web site confidential email in case you need to communicate with the instructor and ta outside of the piazza system i e for confidential matters you should send email to staff cs pitt edu we will make every effort to respond to all email requests within one business day at the latest due to spam filtering you should always use your pitt email address when sending email and include your full name cell phone use new please read carefully answering a cell phone or texting is very disruptive and hence any use of a cell phone to make or receive calls or text messages is not permitted in the class or recitation cell phones must be switched to silent mode and if you have a phone call which cannot wait until the end of the class you need to step out of the class and then answer it technology policy new please read carefully since this is the century the use of laptops tablets and other digital devices is allowed in class however when using digital devices in the classroom you must be mindful when you are emailing tweeting texting surfing etc you are not paying attention research shows that no one can multitask that well you included paying attention and taking good notes is essential to success in this course isn t that why you are here be respectful your use of digital devices should not distract other students in the class it is unlikely that taking notes or searching class related topics will be distracting to the other students however viewing videos of kittens or ice bucket challenges gone well or gone wrong will likely distract others complaints about inappropriate technology use in class will result in your privileges being curtailed or revoked be honest emailing surfing and the use of any other applications or technologies is not allowed during examinations doing so unless explicitly allowed is considered cheating in the exam and will be dealt accordingly audio video recording to ensure the free and open discussion of ideas students may not record classroom lectures discussion and or activities without the advance written permission of the instructor and any such recording properly approved in advance can be used solely for the student own private use grading policy unless explicitly noted otherwise the work in this course is to be done independently discus sions with other students on the assignments should be limited to understanding the statement of the problems except when assignments are to be done in groups in which case it is expected of members of the same group to work together cheating in any way including giving your work to someone else will result in an f for the course and a report to the appropriate university authority submissions that are alike in a substantive way will be considered to be cheating by all involved parties please protect yourselves by only storing your files in private directories and by retrieving all printouts promptly students are expected to abide by the dietrich school of arts and sciences academic integrity code of conduct posted at http www as pitt edu fac policies academic integrity all assignments must be submitted electronically grades can be appealed up to two weeks after they have been posted no appeals will be considered after that time late policy a late assignment will receive a deduction of points if it is up to one day past the deadline and points if it is up to two days past the deadline assignments that are past two days late will not be accepted make up policy students are expected to be present for all exams and quizzes make up exams will only be given in the event of an emergency and only if the instructor is informed in advance failure to notify the instructor prior to missing an exam will result in a zero for the exam final exam conflict policy in case you have a final exam conflict i e have more than two exams scheduled on the same date during finals week you need to notify the instructors of all classes involved in order to resolve the conflict by the sixth week of classes according to the university policy posted at http www registrar pitt edu classroomscheduling html students with disabilities if you have a disability for which you are or may be requesting an accommodation you are encouraged to contact both your instructor and disability resources and services william pitt union or tty as early as possible in the term drs will verify your disability and determine reasonable accomodations for this course their web site is http www drs pitt edu religious observances in order to accommodate the observance of religious holidays students should inform the instructor by email of any such days that conflict with scheduled class activities within the first two weeks of the term copyrighted material all material provided through this web site is subject to copyright this applies to class and recitation notes slides handouts assignments solutions project descriptions etc you are allowed and expected to use all the provided material for personal use however you are strictly prohibited from sharing the material with others in general and from posting the material on the web or other file sharing venues in particular outline a detailed reading guide will be published on the web page along with the class notes and additional online articles and resources time permitting we will cover the following topics data mining information retrieval recommendation systems pagerank network analysis data warehousing xml xpath xquery sql rdf sparql nosql advanced topics this special topics course aims to expose students to different data management data manipulation and data analysis techniques the class will cover all the major data management paradigms relational sql xml xquery rdf sparql including nosql and data stream processing approaches going beyond traditional data management techniques the class will expose students to information retrieval data mining data warehousing network analysis and other data analysis topics timepermitting the class will include big data processing techniques such as the map reduce framework prereq a grade of c or better in cs and cs is required or permission of the instructor good working knowledge of java and familiarity with unix are assumed having passed a statistics course is highly encouraged enrollment students currently registered textbook there is no single textbook with enough coverage of all the material that we want to discuss in this class we will rely on online references and also on o reilly safari bookshelf for which the university has institutional access i e you will not have to buy extra books web page http db cs pitt edu courses class blog http blogspot com socrative http www socrative com inclass quizzes secure data management and web applications fall home syllabus pdf class notes recitation notes assignments piazza submit faq general information when mon wed am 45 am where sennott square building map instructor prof alexandros labrinidis contact ta cory thoma contact purpose this course is the second data management course in the cs department as such it has the following main objectives first to study topics that go beyond the traditional relational database management system framework such as information retrieval data mining and data warehousing second to study securityrelated issues as they arise in webdatabase environments third to expose students to advanced database applications and also to related data information management and analysis technologies while gaining handson experience prereq a grade of c or better in cs and cs is required or permission of the instructor working knowledge of java and familiarity with unix are assumed enrollment students currently registered textbook there is no single textbook with enough coverage of all the material that we want to discuss in this class we will rely on online references and also on o reilly safari bookshelf for which the university has institutional access i e you will not have to buy extra books data mining is the mining or discovery of new information in terms of patterns or rules from vast amounts of data to be useful data mining must be carried out efficiently on large files and databases goals of data mining prediction determine how certain attributes will behave in the future for example how much sales volume a store will generate in a given period identification identify patterns in data for example newly wed couples tend to spend more money buying furnitures classification partition data into classes for example customers can be classified into different categories with different behavior in shopping optimization optimize the use of limited resources such as time space money or materials for example how to best use advertising to maximize profits sales types of knowledge discovered during data mining association rules for example when a male shopper buys a new car he is likely to buy a car cd classification hierarchies for example mutual funds may be classified into three categories growth income and stable sequence patterns sequence patterns are temporal associations for example if mortgage interest rate drops within six months period the sales of houses will increase by certain percentage patterns within time series such as stock price data behavior in time detection of similarity or segmentation for example health data may indicate similarity among subgroups of people techniques of data mining data mining is closely related to knowledge discovery in databases kdd the techniques of kdd includes pattern recognition clustering classification tree casebased reasoning ai techniques statistics neural networks and others applications of data mining marketing 27 data mining http people cs pitt edu chang html 2 finance manufacturing health care commercial data mining tools intelligent miner from ibm applies classification and association rules to detect rules and patterns and make predictions enterprise miner from sas applies decision trees neural nets clustering techniques statistics association rules many new tools are coming out on the market in recent years making data mining a very active research and development area introduction to data science what this class is about concepts and algorithms data mining information retrieval recommender systems pagerank network analysis data warehousing data summarization techniques xml xpath sql rdf sparql graph dbs cypther data visualization python data science libraries data intensive science observational data intensive science simulation sdss sloan digital sky survey gb night lsst large synoptic survey telescope tb night year lhc large hadron collider pb year ska square kilometer array pb hour data intensive science data intensive science turbulent combustion flow what the big deal with big data featured on the cover of nature and the economist what the big deal with big data and even has a dilbert cartoon big data definition the three vs volume size does matter velocity data at speed i e the data fire hose variety heterogeneity is the rule five more vs variability rapid change of data characteristics over time veracity ability to handle uncertainty inconsistency etc visibility protect privacy and provide security value usefulness ability to find the right hay colored needle in the haystack voracity strong appetite for data enter moore law enter bezos law storage capacity increase hdd capacity wikipedia data but human processing capacity remains roughly the same we refer to this as the big data same humans problem phases in the big data pipeline information retrieval cs spring introduction to data science alexandros labrinidis http labrinidis cs pitt edu university of pittsburgh what is information retrieval information organized into documents large number of documents data in documents is unstructured our mission should we choose to accept it locate documents that match a user needs how keywords sample documents like finding a needle in a haystack j or worse a hay colored needle this isn t mission difficult it mission impossible info retrieval vs database systems database systems structured data complex data models data updates transactions and concurrency control exact answers sorted results information retrieval unstructured data collection of documents mostly static approximate answers ranking of results how to retrieve information one way get keywords from user scan entire collection of documents return documents that match problems will not scale to large document collections e g the web will not rank results e g too many matches for labrinidis classic information retrieval collection of documents di where i n one or more keywords kx where x t task given keywords from user identify documents from collection that contain keywords rank documents in some way with most relevant documents first sample document the cleveland browns stunned the pittsburgh steelers with an epic second half turnaround to tie the score in the fourth quarter but shaun suisham kicked a yard field goal with no time left to pull out a victory sunday at heinz field that seemed assured at halftime should we use all words should remove stopwords articles and connectives e g the with an to in a at that should we preprocess any words should perform stemming reduce words to common grammatical root e g stunned stun are all terms equally relevant imagine two documents document a the university of pittsburgh is located in pittsburgh document b carnegie mellon university is located in pittsburgh q is one of the two documents more relevant with respect to a certain keyword i e expect it higher in the ranked results q which keyword relevance ranking single keyword how relevant is document dj to keyword ki frequency use the number of occurrences of ki in dj frequency f ki dj term frequency tf ki dj f ki dj if f ki dj otherwise term document matrix the occurrence of a term ki in a document dj establishes a relation between ki and dj a term document relation between ki and dj can be quantified by the frequency of term ki in document dj f f in matrix form this can be written as f f f f where fi j is the frequency of keyword i in document j example assume the following four documents source modern information retrieval edition term f i f i f i f i tf i tf i tf i tf i to do is be or not i am what think therefore da let it doc size words term f i f i f i f i tf i tf i tf i tf i to do is be or not i am what think therefore da let it doc size words w hat is res ult of q uery with keyword to how to handle multiple keywords most queries involve more than one keywords q how can we implement relevance ranking over a collection of documents using multiple keywords simple approach compute independent relevance measures add them up better approach determine importance weight of each keyword compute independent relevance measures compute weighted sum how to determine weights idea keywords that do not appear in many documents should be more important than those that do def inverse document frequency idfi for keyword ki idfi n ni where ni number of documents where ki appears n total number of documents putting it all together term weight associated with pair ki dj tf ki dj idf ki w i j fi j x n ni if fi j otherwise variants for first part fi j fi j variants for second part n ni term n i idf i d d d d to do is be or not i am what think therefore da let it term n i idf i d d d d to do is be or not i am what think therefore da let it another example the university of pittsburgh is located in pittsburgh carnegie mellon university is located in pittsburgh pittsburgh was voted most livable city steelers steelers the steelers won over the cleveland browns the pittsburgh steelers have won super bowls cleveland is located in ohio for keyword pittsburgh f pittsburgh j tf pittsburgh j log n pittsburgh idf pittsburgh w pittsburgh j understanding question question what is the idf for the keyword steelers possible answers for keyword steelers f steelers j tf steelers j n steelers idf steelers w steelers j query pittsburgh steelers doc4 w pittsburgh j doc2 doc4 w steelers j doc2 doc4 w pittsburgh s teelers j another example results query pittsburgh steelers the university of pittsburgh is located in pittsburgh carnegie mellon university is located in pittsburgh pittsburgh was voted most livable city steelers steelers the steelers won over the cleveland browns the pittsburgh steelers have won super bowls another example sorted results query pittsburgh steelers pittsburgh was voted most livable city steelers steelers the pittsburgh steelers have won super bowls the university of pittsburgh is located in pittsburgh idf steelers the steelers won over the cleveland browns 585 idf pittsburgh 585 carnegie mellon university is located in pittsburgh how to make ir scale alexandros labrinidis university of pittsburgh cs spring scaling to large collections effective index structure is crucial documents containing a specific term are located using an inverted index each keyword maps to a list of documents that contain it how to support or and semantics or compute union of sets and compute intersection of sets small example the university of pittsburgh is located in pittsburgh carnegie mellon university is located in pittsburgh pittsburgh was voted most livable city steelers steelers the steelers won over the cleveland browns the pittsburgh steelers have won super bowls cleveland is located in ohio preprocessing stop word removal the university of pittsburgh is located in pittsburgh carnegie mellon university is located in pittsburgh pittsburgh was voted most livable city steelers steelers the steelers won over the cleveland browns the pittsburgh steelers have won super bowls cleveland is located in ohio preprocessing lower case university pittsburgh located pittsburgh carnegie mellon university located pittsburgh pittsburgh voted livable city steelers steelers steelers won cleveland browns pittsburgh steelers won super bowls cleveland located ohio preprocessing stemming university pittsburgh locat pittsburgh carnegie mellon university locat pittsburgh pittsburgh vot livable city steeler steeler steeler won cleveland brown pittsburgh steeler won super bowl cleveland locat ohio inverted index example inverted index example more efficient approach that considers sparsity inverted index example store frequency counts for each keyword document combination inverted index example store frequency counts for each keyword document combination cs introduction to data science spring prof alexandros labrinidis department of computer science university of pittsburgh information retrieval jan assume the documents the university of pittsburgh is located in pittsburgh carnegie mellon university is located in pittsburgh pittsburgh was voted most livable city steelers steelers the steelers won over the cleveland browns the pittsburgh steelers have won super bowls cleveland is located in ohio compute the frequency f num of occurrences and the term frequency tf f of the keyword steelers against each of the above documents doc2 doc4 doc6 f steelers j tf steelers j compute the inverse document frequency idf of the keyword steelers against all of the above documents compute the weight w of the keyword steelers against each of the above documents doc2 doc4 doc6 w steelers j cheat sheet log2 information retrieval cs spring introduction to data science alexandros labrinidis http labrinidis cs pitt edu university of pittsburgh metrics alexandros labrinidis university of pittsburgh cs spring how to measure effectiveness approximate incomplete results are usual especially if using an index to expedite computation false negative fn a relevant document was not returned false positive fp an irrelevant document was returned true negative tn an irrelevant document was not returned true positive tp a relevant document was returned true false positive negative returned by ir technique yes no relevant document yes true positive tp false negative fn no false positive fp true negative tn effectiveness metrics r a relevant docs in the answer set document collection r relevant docs a answer set precision p r a a tp tp fp what percentage of retrieved documents are relevant to the query recall r r a r tp tp fn what percentage of the documents that are relevant to the query has been retrieved handout example txt txt txt txt txt txt txt txt txt txt txt assume a collection of documents named txt txt txt txt assume that we run two different relevance ranking algorithms and get the above ordered lists of documents that are relevant to the user query assume that the correct relevant documents in the collection are txt txt txt txt txt txt txt txt txt and txt understanding question question given the data from the handout what is the precision of possible answers handout example solutions txt txt txt txt txt txt txt txt txt txt txt assume that the correct relevant documents in the collection are txt txt txt txt txt txt txt txt txt and txt precision of recall of precision of recall of comparing sets alexandros labrinidis university of pittsburgh cs spring jaccard similarity of sets jaccard similarity of sets s and t is defined as size of intersection of s t size of union of s t t jaccard similarity of above sets alexandros labrinidis university of pittsburgh cs spring comparing rankings alexandros labrinidis university of pittsburgh cs spring how to compare rankings would like a similarity metric that allows us to compare two rankings and has a value that ranges between and means perfect agreement between the rankings means perfect disagreement between the rankings means the two rankings are completely independent two rankings perfect agreement document doc6 two rankings perfect agreement x y two rankings perfect disagreement document doc6 doc129 two rankings perfect disagreement x y two rankings how to measure document j j j j j j doc84 doc6 doc9 doc129 doc25 two rankings plot x y spearman coefficient if j and j are the positions of a document j in two rankings and then the spearman coefficient is s k j j j k where k is the total number of documents for previous example s what if rankings vary in similar ways assume documents dj and dk and the differences in their positions in rankings and respectively k j i e difference in ranking between dj and dk k j i e difference in ranking between dj and dk if these differences have same sign then document pair is dj and dk concordant if these differences have different sign then document pair is dj and dk discordant a simple way to measure strength of correlation between rankings is to measureconcordant discordant kendall tau coefficient a simple way to measure strength of correlation between rankings is to measureconcordant discordant p p measures probability that rankings are concordant probability that rankings are discordant kendall tau coefficient computation trick need to consider all possible pairs for each pair determine if c or d all possible pairs means for each of the two rankings once we have cs and ds fully determined count and report percentages kendall tau coefficient example all possible pairs under rank x all possible pairs under rank y example sx a sx b sy a sy b concordant t c total d total information retrieval january assume a collection of documents named txt txt txt txt assume that we run two different relevance ranking algorithms and get the following ordered lists of documents that are relevant to the user query relevant documents txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt txt assuming that the correct relevant documents in the collection are those listed in the third column above txt txt compute the following a maximum possible number of true positives for above document collection b maximum possible number of true negatives for above document collection c number of false positives for d number of false negatives for e precision of f recall of g number of false positives for h number of false negatives for i precision of j recall of assume that we run two different relevance ranking algorithms and get the following ordered lists of documents that are relevant to the user query txt txt txt txt txt txt txt txt compute the similarity of the two ranking algorithms using the kendall tau coefficient solutions a maximum possible number of true positives for above document collection number of correct answers b maximum possible number of true negatives for above document collection number of incorrect answers c number of false positives for should not have been returned but were d number of false negatives for should have been returned but were not e precision of correct results returned total returned f recall of correct results returned total correct g number of false positives for should not have been returned but were h number of false negatives for should have been returned but were not i precision of correct results returned total returned j recall of correct results returned total correct for simplicity of presentation we rename the different files as a b c d txt a txt c txt b txt d txt c txt a txt d txt b rankings of documents by different algorithms doc a b c d combinations based on rankings from a b concordant a is ranked higher than b under and under a c discordant a is ranked higher than c under but lower under a d discordant b c discordant b d discordant c d concordant combinations based on rankings from c d concordant c a discordant c b discordant d a discordant d b discordant a b concordant so we have kendal tau coefficient c d data mining cs spring introduction to data science alexandros labrinidis http labrinidis cs pitt edu university of pittsburgh association rule mining one specific type of data mining usually try to predict novel and interesting patterns from supermarket data famous examples purchase of diapers è purchase of beer http www dssresources com newsletters php how target figured out a teen girl was pregnant before her father did forbes feb http bit ly targetpregnant frequent itemsets alexandros labrinidis university of pittsburgh cs spring transactions example market basket model multiple items e g milk bread etc multiple baskets transactions assumption number of items in basket much smaller than total number of items transactions example compressed form items a milk b bread c cereal e eggs transactions example binary form items a milk b bread c cereal d sugar e eggs attributes converted to binary flags definitions item attribute value pair or simply value usually attributes are converted to binary flags for each value e g product a is written as a itemset l a subset of possible items example l a b e order unimportant transaction tid itemset tid is transaction id support and frequent itemsets support count of an itemset sup l number of transactions that support i e contain l example sup a b e and sup b c support percentage of an itemset supp l percentage of transactions that support i e contain l supp l sup l is total number of transactions example supp a b e and supp b c an itemset l is frequent if it has support count at least minsup sup i minsup question understanding question which of the following doubletons has support count of exactly based on the transaction data from the handout possible answers ab ac bc de af support counts for doubletons question understanding question what is the combined sum of the support counts of abc abd and abe based on the transaction data from the handout possible answers support count for size itemsets abc abd abe q any interesting observations a support count of abc is the minimum of support counts of ab bc ac subset property alexandros labrinidis university of pittsburgh cs spring subset property question understanding question if minsup can abf be frequent itemset based on the transaction data from the handout possible answers yes no association rules alexandros labrinidis university of pittsburgh cs spring association rules association rule r are disjoint and is non empty simplified definition has only one item meaning if transaction includes then it also has examples a b e a b c from frequent itemsets to association rules q given frequent set a b e what are possible association rules a b e a e b b e a a b e b a e e a b a b e empty rule or true a b e we will ignore empty rules from this point on definition of support for association rules association rule r i j example a b c support count for r sup r sup i j sup i u j example sup a b c sup a b u c sup a b c support percentage for r supp r supp i j supp i u j meaning fraction of transactions that involve both left hand side lhs and right hand side rhs itemsets definition of confidence for association rules association rule r i j example a b c confidence for r conf r conf i j sup i u j sup i example conf a b c sup a b c sup a b meaning probability that rhs will appear given that lhs appears associate rules example q given frequent set a b e what association rules have at least minsup and minconf a b e conf a e b conf b e a conf e a b conf do not qualify a b e conf b a e conf a b e conf question understanding question what is the confidence of association rule a b c based on the transaction data from the handout possible answers a priori algorithm alexandros labrinidis university of pittsburgh cs spring how to generate association rules find strong association rules an association rule has parameters minsup and minconf sup r minsup and conf r minconf problem statement find all association rules with given minsup and minconf first find all frequent itemsets start by finding one item sets easy q how a simply count the frequencies of all items finding itemsets next level apriori algorithm agrawal srikant idea use one item sets to generate two item sets two item sets to generate three item sets if a b is a frequent item set then a and b have to be frequent item sets as well subset property in general if x is frequent k item set then all k item subsets of x are also frequent compute k item set by merging k item sets an example given five three item sets a b c a b d a c d a c e b c d lexicographic order improves efficiency candidate four item sets a b c d q ok a yes because all item subsets are frequent a c d e q ok a no because c d e is not frequent implementation issues how to store support counts first step convert strings to integers using hash function naïve method a i j stores count for pair i j assume i j triangular matrix method a k stores count for pair i j assume i j k i n i j i stores data as n n n n triples method store triple i j c where c is count for pair i j and i j use hash table with i j as key source http www mmds org beyond binary data hierarchies drink à milk à low fat milk à stop shop low fat milk find associations on any level sequences over time cs introduction to data science spring prof alexandros labrinidis department of computer science university of pittsburgh data mining association rule mining jan assume the following market basket transactions for a fictional super market that carries the following products a angel hair pasta b bread c cereal d diapers e eggs and f flour a b d b c d a b c d a b d e b d a b c e a b c d c e a b c d c d e compute the support counts for each of the size itemsets a b c d e f count compute the support counts for each doubleton i e size itemsets f e d c b a b c d e af ae ad ac ab bf be bd bc cf ce cd df de ef compute the support counts for the following size itemsets abc abd abe cs introduction to data science spring prof alexandros labrinidis department of computer science university of pittsburgh data mining data clustering jan assume the following points drawn on a grid how many clusters will dbscan create if e radius and how many points will dbscan label as noise if e radius and how many clusters will dbscan create if e radius and how many points will dbscan label as noise if e radius and how many clusters will dbscan create if e radius and data mining ii cs spring introduction to data science alexandros labrinidis http labrinidis cs pitt edu university of pittsburgh data mining definition what is data mining computational process to discover patterns in large data sets data mining definition cont must produce novel and interesting patterns source http dilbert com strips comic a bit of history let us go back years ago in london england on august after several other outbreaks had occurred elsewhere in the city a major outbreak of cholera struck soho over the next three days people on or near broad street died in the next week three quarters of the residents had fled the area by september people had died and the mortality rate was percent in some parts of the city by the end of the outbreak people had died source http en wikipedia org wiki cholera outbreak john snow a physician wanted to investigate cause was skeptic of the miasma theory of bad air note that the germ theory was not created until by louis pasteur snow studied spread of disease made a map of fatalities map of soho with fatalities map of soho with fatalities zoom in snow own words on proceeding to the spot i found that nearly all the deaths had taken place within a short distance of the broad street pump there were only ten deaths in houses situated decidedly nearer to another street pump in five of these cases the families of the deceased persons informed me that they always sent to the pump in broad street as they preferred the water to that of the pumps which were nearer in three other cases the deceased were children who went to school near the pump in broad street with regard to the deaths occurring in the locality belonging to the pump there were instances in which i was informed that the deceased persons used to drink the pump water from broad street either constantly or occasionally the result of the inquiry then is that there has been no particular outbreak or prevalence of cholera in this part of london except among the persons who were in the habit of drinking the water of the above mentioned pump well i had an interview with the board of guardians of st james parish on the evening of the inst september and represented the above circumstances to them in consequence of what i said the handle of the pump was removed on the following day john snow letter to the editor of the medical times and gazette data collection question question pick a random number from to possible answers data mining tasks alexandros labrinidis university of pittsburgh cs spring typical data mining tasks anomaly detection association rule learning clustering classification regression summarization source http en wikipedia org wiki anomaly detection anomaly detection outlier change deviation detection the identification of unusual data records that might be interesting or data errors that require further investigation source visual and statistical thinking displays of evidence for making decisions edward tufte association rule learning association rule learning dependency modeling searches for relationships between variables for example a supermarket might gather data on customer purchasing habits using association rule learning the supermarket can determine which products are frequently bought together and use this information for marketing purposes this is sometimes referred to as market basket analysis clustering clustering is the task of discovering groups and structures in the data that are in some way or another similar without using known structures in the data members of a cluster should be more alike among each other than to members of other clusters clustering is one type of unsupervised learning classification classification is the task of generalizing known structure to apply to new data in other words learn a method to predict a label for new data from pre labeled classified data classification is one type of supervised learning examples an e mail program classifies an e mail as legitimate or as spam a lender classifies its customers as credit worthy or credit risky a credit card company identifies fraudulent transactions a phone company identifies which customer would abandon contract for another carrier a security agency identifies potential evil doers personalized medicine will drug work for specific patient regression regression attempts to find a function which models the data with the least error source http en wikipedia org wiki summarization summarization providing a more compact representation of the data set including visualization and report generation examples document summarization e g snippets in gmail choosing one representative member from each cluster choosing a few representative data points through sampling question what would be a trivial but simple summarization of a set of numbers answer computing the average or the median question understanding question which of the following cannot be a summarization operation for a data set consisting of just numbers possible answers computing the average computing the maximum computing the median computing the minimum multiplying all numbers by clustering alexandros labrinidis university of pittsburgh cs spring source http www mmds org overview methods of clustering hierarchical agglomerative bottom up initially each point is a cluster repeatedly combine the two nearest clusters into one divisive top down start with one cluster and recursively split it point assignment maintain a set of clusters points belong to nearest cluster source http www mmds org hierarchical clustering key operation repeatedly combine two nearest clusters three important questions how do you represent a cluster of more than one point how do you determine the nearness of clusters when to stop combining clusters hierarchical clustering source http www mmds org key operation repeatedly combine two nearest clusters how to represent a cluster of many points key problem as you merge clusters how do you represent the location of each cluster to tell which pair of clusters is closest euclidean case each cluster has a centroid average of its data points how to determine nearness of clusters measure cluster distances by distances of centroids source http www mmds org example hierarchical clustering data o data point x centroid dendrogram k means clustering alexandros labrinidis university of pittsburgh cs spring simple clustering k means works with numeric data only specify k i e how many clusters to generate pick k cluster centers at random assign every item to its nearest cluster center e g using euclidean distance move each cluster center to the mean of its assigned items repeat steps until convergence change in cluster assignments less than a threshold k means example step y pick initial cluster centers randomly x k means example step y assign each point to the closest cluster center x k means example step y move each cluster center to the mean of each cluster x k means example step reassign points closest to a different new cluster center y q which points are reassigned x k means example step y a three points with animation x k means example step y re compute cluster means x k means example step y move cluster centers to cluster means x discussion result can vary significantly depending on initial choice of seeds can get trapped in local minimum example initial cluster centers instances to increase chance of finding global optimum restart with different random seeds k means visualizations http stanford edu class visualizations kmeans kmeans html visualizing k means algorithm with js http tech nitoyon com en blog k means http www naftaliharris com blog visualizing k means clustering http www bytemuse com post k means clustering visualization k means clustering summary advantages simple understandable items automatically assigned to clusters disadvantages must pick number of clusters before hand all items forced into clusters too sensitive to outliers k means variations k medoids instead of mean use medians of each cluster mean of is mean of is median of is median advantage not affected by extreme values for large databases use sampling question understanding question what does the k in the k means clustering algorithm stand for possible answers k comes from the name of the inventor of the algorithm k sounds like c from cluster k is the exact number of clusters to generate k is the minimum number of clusters to generate actual number could be higher k is the maximum number of clusters to generate actual number could be lower dbscan algorithm alexandros labrinidis university of pittsburgh cs spring dbscan algorithm dbscan requires two parameters ε eps used to define ε neighborhoods and https en wikipedia org wiki dbscan the minimum number of points required to form a dense region minpts start with a not yet visited arbitrary point retrieve its ε neighborhood if ε neighborhood contains sufficiently many points a cluster is started otherwise the point is labeled as noise if a point is found to be a dense part of a cluster its ε neighborhood is also part of that cluster hence all points that are found within the ε neighborhood are added as is their own ε neighborhood when they are also dense process continues until the density connected cluster is completely found then a new unvisited point is retrieved and processed leading to the discovery of a further cluster or noise dbscan visualization http www naftaliharris com blog visualizing dbscan clustering questions understanding questions how many clusters will dbscan create if e radius and how many points will dbscan label as noise if e radius and how many clusters will dbscan create if e radius and how many points will dbscan label as noise if e radius and how many clusters will dbscan create if e radius and data warehousing cs spring introduction to data science alexandros labrinidis http labrinidis cs pitt edu university of pittsburgh what is a data warehouse a system used for reporting and data analysis integrating data from one or more disparate sources and creating a central repository of data a data warehouse dw stores current and historical data and is used for creating trending reports for senior management reporting such as annual and quarterly comparisons the data stored in the warehouse is uploaded from the operational systems such as marketing sales etc source http en wikipedia org wiki a data warehouse is a subject oriented integrated time variant and nonvolatile collection of data in support of management decision making process w h inmon source data mining concepts and techniques edition data warehouse subject oriented organized around major subjects such as customer product sales focusing on the modeling and analysis of data for decision makers not on daily operations or transaction processing provide a simple and concise view around particular subject issues by excluding data that are not useful in the decision support process data warehouse integrated constructed by integrating multiple heterogeneous data sources relational databases flat files records from online transaction processing oltp systems data cleaning and data integration techniques applied ensure consistency in naming conventions encoding structures attribute measures etc among different data sources e g hotel price currency tax breakfast covered etc when data is moved to the warehouse it is converted data warehouse time variant the time horizon for the data warehouse is significantly longer than that of operational systems operational database current value data data warehouse data provide information from a historical perspective e g past years every key structure in the data warehouse contains an element of time explicitly or implicitly but the key of operational data may or may not contain time element data warehouse non volatile a physically separate store of data transformed from the operational environment operational updates of data do not occur in the data warehouse environment does not require transaction processing recovery and concurrency control mechanisms requires only two operations in data accessing initial loading of data and access of data oltp vs olap oltp olap users clerk it professional knowledge worker function day to day operations decision support db design application oriented subject oriented data current up to date detailed flat relational isolated historical summarized multidimensional integrated consolidated usage repetitive ad hoc access read write index hash on prim key lots of scans unit of work short simple transaction complex query records accessed tens millionsusers thousands hundreds db size gb tb metric transaction throughput query throughput response source data mining concepts and techniques edition why a separate data warehouse high performance for both systems dbms tuned for oltp online transaction processing access methods indexing concurrency control recovery warehouse tuned for olap online analytical processing complex olap queries multidimensional view consolidation different functions and different data missing data decision support requires historical data which operational dbs do not typically maintain data consolidation dw requires consolidation aggregation summarization of data from heterogeneous sources data quality different sources typically use inconsistent data representations codes and formats which have to be reconciled note there are more and more systems which perform olap analysis directly on relational databases data warehouse a multi tiered architecture metadata olap server extract transform load refresh data warehouse data sources data marts data storage olap engine front end tools extraction transformation loading etl data extraction get data from multiple heterogeneous and external sources data cleaning detect errors in the data and rectify them when possible data transformation convert data from legacy or host format to warehouse format load sort summarize consolidate compute views check integrity and build indices and partitions refresh propagate the updates from the data sources to the warehouse show me the data alexandros labrinidis university of pittsburgh cs spring so what does the data look like http org data coffee chain xlsx dimension attributes date location area code market state attributes about customers market size source http www tableausoftware com attributes about products product product line product type type measure attributes can be aggregated upon inventory cost sales profit dimension attributes some dimension attributes could be organized using a concept hierarchy i e sequence of mappings from low level concepts to higher level concepts concept hierarchy examples location city à county à province or state à country à all time second à minute à hour à day à month à quarter à year à all second à minute à hour à day à week à year à all location hierarchy example measure attributes distributive if the result derived by applying the function to n aggregate values is the same as that derived by applying the function on all the data without partitioning e g count sum min max algebraic if it can be computed by an algebraic function with m arguments where m is a bounded integer each of which is obtained by applying a distributive aggregate function e g avg holistic if there is no constant bound on the storage size needed to describe a sub aggregate e g median mode rank how to summarize alexandros labrinidis university of pittsburgh cs spring crosstab definition a cross tab is a table where values for one of the dimension attributes form the row headers values for another dimension attribute form the column headers other dimension attributes are listed on top values in individual cells are aggregates of the values of the dimension attributes that specify the cell totals for every row and column are also pre computed crosstab example understanding question fill out the blanks in the following crosstab june july august september all months blue red green all colors understanding question question what is the correct value for the total number of sales of red cars for the month of august possible answers fill in understanding question question what is the correct value for the total number of sales of red cars for the month of august correct answer june july august september all months blue red green all colors understanding question question what is the correct value for the total number of sales of red cars for the month of august correct answer june july august september all months blue red green all colors relational representation of crosstabs crosstabs can be represented as relations value all is used to represent the aggregates sql standard uses null values instead of all generalization of crosstab for multiple dimensions generalization of cross tab for more than two dimensions is a data cube dimensional data cubes are easy to visualize crosstab can be used as two dimensional views of any n dimensional data cube data cube example axes represent different dimension attributes e g color size item name cells hold one measure attribute e g total sales called facts real data cubes have dimensions olap operations alexandros labrinidis university of pittsburgh cs spring typical olap operations roll up aggregate data further eliminate a dimension e g eliminate color or climb up a concept hierarchy e g go from months to quarters drill down reverse of roll up provide more details introduce additional dimensions or climb down a concept hierarchy e g from county to city slice select on one dimension of data cube è subcube dice select on two or more dimension of data cube è subcube pivot rotate rotate data axes to provide alternate visual representation roll up example number of autos sold pa oh md total jul 108 aug sep total number of autos sold roll up by month roll up by state drill down example number of autos sold number of autos sold pa oh md total jul 108 aug sep total drill down by state drill down example number of autos sold number of autos sold slice and dice red red blue gray md oh pa jul aug sep blue gray md oh pa jul aug sep blue jul aug sep total md blue oh pa color blue and state pa jul aug sep color blue understanding question question assume the data cube from the handout and that we perform a roll up operation on item name and then a slice operation with the condition size medium which of the following questions can be answered with the resulting sub cube possible answers yes no number of dresses sold in medium size yes no number of clothes items sold in medium size that are pastel in color yes no number of clothes items sold that are large size answer questions number of no dresses sold in medium size i number of clothes items sold yes in medium size that are pastel in color number of no clothes items sold that are large size data warehouse performance alexandros labrinidis university of pittsburgh cs spring data cube lattice drill down roll up hot to pre compute data cubes assuming state month color dimensions we need to pre compute i e materialize the following total state month color state month state color month color state month color efficient computation clear trade off between no materialization perform aggregation from raw data as needed and full materialization all subtotals are precomputed ahead of time full materialization no materialization extra storage overhead no storage overhead all queries fast all queries slow overhead with updates no update overhead solution hybrid approach is better partial materialization how to store data cubes relational olap rolap store and manage warehouse data in traditional dbms servers slow query performance scales well to high number of dimensions scales well to large datasets mature technology hybrid olap holap multidimensional olap molap store and manage warehouse data in array based multidimensional storage engines very efficient data access fast answers does not scale to large numbers of dimensions requires special purpose data store combine rolap and molap aolexloangdroiselsabrinidis university of pittsburgh cs spring sparsity imagine a data warehouse for giant eagle suppose dimensions are customer product store day with customers products stores and days data cube has 000 000 cells fortunately most cells are empty a given store does not sell every product on every day a given customer has never visited most of the stores a given customer has never purchased most products multi dimensional arrays are not an efficient way to store sparse data alexandros labrinidis university of pittsburgh cs spring data warehouse design alexandros labrinidis university of pittsburgh cs spring star schema snowflake schema data cleaning alexandros labrinidis university of pittsburgh cs spring data cleaning it a chore cleaning big data most time consuming least enjoyable data science task survey says may http www forbes com sites gilpress data preparation most time consuming least enjoyable data science task survey how to clean data develop domain rules based on semantics e g allowable ranges develop attribute link rules based on semantics e g each customer linked to single zip code plus this allows for correction in addition to detection identify outliers e g value indicates no temperature reading cs introduction to data science spring prof alexandros labrinidis department of computer science university of pittsburgh data warehousing jan fill out the blanks in the following crosstab summarizing total sales amounts for blue red and green cars in june july august and september from a fictional car dealership june july august september all months blue red green all colors assume the following data cube and that we perform a roll up operation on item name and then a slice operation with the condition size medium which of the following questions can be answered with the resulting sub cube a number of dresses sold in medium size b number of clothes items sold in medium size that are pastel in color c number of clothes items sold that are large size data summarization and data visualization cs spring introduction to data science alexandros labrinidis http labrinidis cs pitt edu university of pittsburgh types of attributes strings nominal e g hair color binary e g male female ordinal e g small medium large x large numeric discrete e g continuous e g measuring the central tendency basic statistical descriptions of data alexandros labrinidis university of pittsburgh cs spring mean or average arithmetic mean or average is the most common and effective measure of the center of a set of data x i xi n xn n if values xi are associated with weights wi we have the weighted arithmetic mean or weighted average x n i n wixi wi i median median is the numerical value separating the higher half of a data set from the lower half median can be found by arranging all the observations from lowest value to highest value and picking the middle one e g the median of is if there is an even number of observations then the median is usually defined to be the mean of the two middle values e g the median of is source http en wikipedia org wiki median mode mode is the value that appears most often in a set of data it is possible for the greatest frequency to correspond to multiple items unimodal just one bimodal just two trimodal just three if each data appears only once there is no mode for that data set symmetric vs skewed data positively skewed data mean median mode negatively skewed data mean median mode measuring the dispersion of data alexandros labrinidis university of pittsburgh cs spring range and quartiles range is the difference between max and min values split distribution into equal size consecutive sets quartiles split into sets percentiles split into sets percentile percentile inter quartile range iqr boxplots five number summary of a data distribution min median max boxplot incorporates five number summary outlier a value higher lower than x iqr outliers variance and standard deviation variance and standard deviation are measures of data dispersion they indicate how spread out a data set is variance o n n xi i x n xi x standard deviation the square ropot of variance i e er scatter plots and data correlation alexandros labrinidis university of pittsburgh cs spring scatter plots scatter plot is the most effective visual way to determine if there is correlation between two numeric attributes scatter plots cont a positive correlation b negative correlation scatter plots cont no observed correlation histograms alexandros labrinidis university of pittsburgh cs spring what is a histogram a histogram is a graphical representation of the distribution of data estimate of the probability distribution of a continuous variable first introduced by karl pearson a representation of tabulated frequencies shown as adjacent rectangles or squares over discrete intervals bins with an area proportional to the frequency of the observations in the interval source http en wikipedia org wiki histogram two main types equal width width of each bucket range is uniform equal frequency or equal depth buckets are created so roughly the frequency of each bucket is constant sample dataset list of prices from a store sorted tabulated prices frequencies cnt sample histogram singleton buckets tabulated prices frequencies cnt equal width histograms figure out range max min divide by number of required buckets say width of each bucket should be understanding question question given the following set of numbers and create an equal width histogram with buckets what is the second value of the third bucket in the resulting histogram possible answers fill in question understanding question given the following set of numbers and create an equal width histogram with buckets what is the second value of the third bucket in the resulting histogram answer sorted range max min bucket range à 60 equal frequency histograms figure out total frequency divide by number of required buckets say height of each bucket should be around freq freq freq freq understanding question question given the following set of numbers 63 88 and create an equal frequency equal depth histogram with buckets what is the second value of the third bucket in the resulting histogram possible answers fill in understanding question question given the following set of numbers 63 81 88 and create an equal frequency equal depth histogram with buckets what is the second value of the third bucket in the resulting histogram answer sorted 63 81 88 total items à bucket size for buckets bucket2 63 74 81 88 fgqsmdpzkq gl bpxnsmdpznfx the title is not a result of me sleeping on the keyboard any guesses histograms in cryptography encoded using a simple substitution cipher each letter of the alphabet is replaced with a different letter key zebracdfghijklmnopqstuvwxy source http practicalcryptography com ciphers simple substitution cipher can use letter frequency and a dictionary to break the cipher sampling alexandros labrinidis university of pittsburgh cs spring what is sampling represent one large data set using a much smaller one must preserve properties characteristics of original data set simple random sample without replacement srswor draw of the n tuples n probability of drawing any one tuple is n once an object is drawn it is removed from the population simple random sample with replacement srswr draw of the n tuples n each time a tuple is drawn it is replaced i e placed back into original set so that it may be drawn again sampling activity in class what is the distribution of freshman sophomore junior senior grad students in class let also get the true counts last socrative question sampling activity in class answer what is the distribution of freshman sophomore junior senior grad students in class why is the data so skewed answer is based on roster data not on how many students are currently in the classroom is there a difference is it expected sampling peoplesoft ids distribution of first digit of peoplesoft ids of students registered in the class show of hands how many start with sample bias fundamental for proper sampling must avoid having a sample that is not representative of the entire population i e must avoid sample bias classic example republican alf landon vs democrat franklin roosevelt literary digest poll in the presidential election surveyed over two million people chosen from the magazine subscriber list phone books and car registrations sample was not representative of entire population of voters not everyone could afford a phone or a car during the depression information visualization alexandros labrinidis university of pittsburgh cs spring making charts many types of charts for example https developers google com chart interactive docs gallery bar charts column charts line charts area charts geo charts scatter plots pie charts how to do good charts alexandros labrinidis university of pittsburgh cs spring how to do good charts there are a lot of things to keep in mind we could enumerate all the rules but alexandros labrinidis university of pittsburgh cs spring how to do bad charts alexandros labrinidis university of pittsburgh cs spring how to do bad charts you can do it rules charts should be accurate i e numbers correct etc work in teams share via solstice wireless projection system we critique them together coffee sales values over time for one store 120 coffee sales values over time for two stores store a store b 08 90 120 price of a big mac in different countries fake data usa canada greece france norway price of a big mac and an iphone in different countries fake data big mac iphone usa canada greece n a france norway cs introduction to data science spring prof alexandros labrinidis department of computer science university of pittsburgh data summarization and visualization feb given the following set of numbers 71 63 85 69 81 74 88 96 a create an equal width histogram with buckets socrative what is the second value of the third bucket in the resulting histogram b create an equal frequency equal depth histogram with buckets socrative what is the second value of the third bucket in the resulting histogram cs introduction to data science spring prof alexandros labrinidis department of computer science university of pittsburgh recommender systems feb assume the following ratings of movies by users a f the matrix gone with the wind jack and jill planes rocky iv alice bob christine david elaine frank compute distance christine frank distance david frank and distance elaine frank who has the highest distance from frank distance alice frank distance bob frank distance christine frank distance david frank distance elaine frank recommender systems cs spring introduction to data science alexandros labrinidis http labrinidis cs pitt edu university of pittsburgh what are recommender systems recommender systems or recommendation systems are a subclass of information filtering system that seek to predict the rating or preference that user would give to an item source http en wikipedia org wiki most popular type of recommender systems collaborative filtering source http en wikipedia org wiki examples of recommender systems movies music news books research articles search queries products in general restaurants social networks friends followers likes online dating app store what is the filter bubble a result of a personalized search in which a website algorithm selectively guesses what information a user would like to see this happens based on information about the user e g location past click and search history as a result users become separated from information that disagrees with their viewpoints effectively isolating them in their own cultural or ideological bubbles source http en wikipedia org wiki collaborative filtering alexandros labrinidis university of pittsburgh cs spring step collect data alexandros labrinidis university of pittsburgh cs spring different users different ratings alice stars bob stars christine stars david stars elaine stars frank has not seen it images used for educational purposes only different users different ratings alice stars bob has not seen it christine stars david has not seen it elaine stars frank stars images used for educational purposes only different users different ratings alice stars bob star christine stars david stars elaine star frank star images used for educational purposes only different users different ratings alice stars bob stars christine stars david stars elaine has not seen it frank stars images used for educational purposes only different users different ratings alice stars bob stars christine stars david stars elaine stars frank has not seen it images used for educational purposes only movie ratings the matrix gone with the wind jack and jill planes rocky iv alice bob christine david elaine frank step use the data alexandros labrinidis university of pittsburgh cs spring extremely simple approach source http xkcd com how can we compare different users treat ratings as sets and use jaccard similarity i e ratio of intersection over union of datasets j a b ka b ka bk source http en wikipedia org wiki for ratings it could be used to identify which ratings were the same example j alice bob j david elaine q what is the problem with this method how can we compare different users create a plot x axis is rating for one movie y axis is rating for another move points reflect ratings for the two movies from a particular user people in preference space a star for movie x and stars for movie y b star for movie x and stars for movie y f has not seen movie x star for movie y movie x how can we compare different users real example the matrix vs planes c a b d e the closer users are the more similar they are the matrix draw similarity real example jack and jill vs gone with the wind a b c d e f jack and jill understanding question question how many points does the top row y of the gwtw vs jack and jill plot have possible answers draw similarity real example jack and jill vs gone with the wind a b answer jack and jill how to compute similarity i start by computing the distance between two users i manhattan distance distance between points a and b is jack and jill how to compute similarity ii start by computing the distance between two users ii euclidean distance distance between points a and b is y1 jack and jill how to compute similarity iii given distance metric e g manhattan or euclidean we need to compute similarity between two people i j formula sim i j distance i j limits similarity score is close to if distance is close to similarity score is if distance is computing distances on n dimensions distance formulas are trivially generalizable for multiple dimensions i e to compute distance of people over multiple movies example assume we want to compute the distance of frank to all other users for simplicity let assume manhattan distance distance alice frank note we do not compute distance if there is no rating for one of the users distance bob frank understanding question question compute distance christine frank distance david frank and distance elaine frank who has the highest distance from frank possible answers alice bob christine david elaine understanding question question compute distance christine frank distance david frank and distance elaine frank who has the highest distance from frank answer alice d alice frank bob d bob frank christine d christine frank david d david frank elaine d elaine frank distance à similarity distance alice frank similarity alice frank distance bob frank similarity bob frank distance christine frank similarity christine frank distance david frank similarity david frank distance elaine frank similarity elaine frank making predictions alexandros labrinidis university of pittsburgh cs spring movie ratings à prediction the matrix gone with the wind jack and jill planes rocky iv alice bob christine david elaine frank q what if we just used the average rating to predict the missing ratings a although this may work in some cases e g jack and jill it will not work for the general case solution we utilize the similarity metric to give more weight to ratings from users who are similar to user in question compute weighted average rating predicted rating wi ri wi note only include non zero ratings movie ratings similarity similarity to frank the matrix gone with the wind jack and jill planes rocky iv alice bob christine david elaine frank prediction rating frank the matrix 1667 3333 2333 vs distance sensitivity alexandros labrinidis university of pittsburgh cs spring what if visually compared two users two users as axes movies are points the matrix gone with the wind jack and jill planes rocky iv bob what if visually compared two users two users as axes movies are points the matrix gone with the wind jack and jill planes rocky iv bob points on the diagonal è complete agreement what if visually compared two users helen jane on diagonal è complete agreement different angle line è agreement with skew e g grade inflation another similarity metric pearson correlation coefficient measures how well two data sets fit on a straight line ranges from to inclusive è perfect disagreement è perfect agreeement negative more complicated formula positive gives better results when ratings not normalized source http en wikipedia org wiki pearson correlation coefficient original formula simpler to compute approximation r p n xy x y n x2 x n y2 y general case collect ratings from users assign a weight to all users with respect to similarity with the active user multiple metrics for similarity select k users that have the highest similarity with the active user commonly called the neighborhood compute a prediction score from a weighted combination of the selected neighborhood ratings extensions plethora of similarity functions e g cosine similarity will cover next time item based similarity instead of user based similarity better for sparse datasets further reading item based collaborative filtering recommendation algorithms http files grouplens org papers pdf cs introduction to data science spring prof alexandros labrinidis department of computer science university of pittsburgh recommender systems feb assume the following ratings of movies by users a f the average ratings per movie and the predicted ratings for david from some unspecified collaborative filtering algorithm the matrix gone with the wind jack and jill planes rocky iv alice bob christine david elaine frank average predicted for david evaluating quality given david actual rankings compute the mean absolute error if we are to use the predicted values for david rankings remember the formula for mean absolute error n mae n pi i qi slope one what is the average difference in ratings between the matrix and planes slope one what would be the predicted value for frank rating of the matrix just utilizing the above differences and his rating of planes recommender systems ii cs spring introduction to data science alexandros labrinidis http labrinidis cs pitt edu university of pittsburgh item based collaborative filtering alexandros labrinidis university of pittsburgh cs spring 2017 movie ratings the matrix gone with the wind jack and jill planes rocky iv alice bob christine david elaine frank used based collaborative filtering collect ratings from users assign a weight to all users with respect to similarity with the active user multiple metrics for similarity select k users that have the highest similarity with the active user commonly called the neighborhood compute a prediction score from a weighted combination of the selected neighborhood ratings issues with user based collaborative filtering finding enough data for each user movie reviews submitted by students of class fall term students submitted reviews movie reviews unique movies movies with reviews despicable me inception pulp fiction the avengers the dark knight movies with reviews aliens blood diamond fight club gladiator predator the exorcist the matrix the room the shining need to recompute often as users add new rankings movie ratings item centric view alice bob christine david elaine frank the matrix gone with the wind jack and jill planes rocky iv movie ratings item centric view the matrix gone with the wind jack and jill planes rocky iv compute similarity identify users who reviewed both movies and perform one of two options compute distance and then similarity e g manhattan euclidean compute similarity directly e g pearson cosine similarity cosine similarity treat movie ratings for each movie as vectors a and b in m dimensional space m number of users measure similarity by computing the cosine of the angle between the two vectors source http en wikipedia org wiki the resulting similarity ranges from meaning exactly opposite to meaning exactly the same with usually indicating independence and in between values indicating intermediate similarity or dissimilarity online cosine similarity calculator http calculator vhex net calculator distance cosine distance cosine similarity example the matrix gone with the wind input cosine similarity cosine similarity matrix gwtw 8733 movie ratings example vector1 cosine similarity matrix jack and jill 09021 movie ratings example vector1 cosine similarity matrix planes 8711 movie ratings example the matrix vector1 vector2 cosine similarity matrix rocky iv 9803 rocky iv understanding question question assuming you want to compute the cosine similarity between the planes and rocky iv movies what is the sum of all the elements of the second vector i e for rocky iv you will use answers planes rocky iv understanding question question assuming you want to compute the cosine similarity between the planes and rocky iv movies what is the sum of all the elements of the second vector i e for rocky iv you will use answers 18 planes rocky iv rocky iv answer adjusted cosine similarity drawback of cosine similarity difference in rating scale between different users is not taken into account propose new metric p au i ru bu i ru where ru is the average of the user u ratings we have the similarities now what can proceed with weighted average method to predict missing values for ratings can sort items based on similarity and present first few of them as recommended items ordered from highest similarity to lowest this is what amazon com is doing considering the sign of the similarity similarity score in è use simple weighted average similarity score in è must use absolute value of weight in denominator è must normalize ranking normalize rankings convert from range to range de normalize rankings convert from to range normalize de normalize formulas minrating minr maxrating maxr normalize ru i minr maxr minr maxr minr de normalize ru i nru i maxr minr minr evaluating quality 2017 alexandros labrinidis university of pittsburgh cs spring 2017 how good are the predictions standard evaluation technique given a dataset allocate x to training i e input to the algorithm could be as high as remaining percent is test data set predict values for the test data without looking at the test data compare predicted value with test data value aggregate differences over entire test data set mean absolute error mae rating pi prediction qi compute absolute error between pi and qi aggregate over all predictions compute the average mae n xi pi qi mean absolute error example similarity to frank the matrix gone with the wind jack and jill planes rocky iv alice bob christine 1667 david 3333 elaine frank average diff prediction 196 536 318 583 833 diff 536 318 583 mean absolute error example average diff prediction 4 196 536 318 583 833 diff 536 318 583 mae average mae prediction 536 318 583 understanding question question given the table of the handout what is the mean absolute error for the predicted ratings for david compared to his actual ratings possible answers fill in understanding question question given the table of the handout what is the mean absolute error for the predicted ratings for david compared to his actual ratings answer actual predicted 4 and and and 4 so we have 4 root mean squared error rmse rating pi prediction qi compute error between pi and qi and square value aggregate over all predictions and divide by n compute square root rmse n i pi qi n slope one 2017 alexandros labrinidis university of pittsburgh cs spring 2017 not really picture used for educational purposes main idea behind slope one collaborative filtering algorithm user a ratings of two items and user b rating of a common item is used to predict user b unknown rating source slope one predictors for online rating based collaborative filtering by lemire and maclachlan sdm source http en wikipedia org wiki slope one example customer item a item b item c john mark 4 didn t rate it lucy didn t rate it 5 average difference in ratings between b and a is 5 4 5 on average item a is rated 5 above item b avg difference in ratings between c and a is 5 predict lucy rating for a using b 5 5 predict lucy rating for a using c 5 consider weighted average weight of users having rated both items 5 4 understanding question question given the table of the handout what is the average difference in ratings between the matrix and planes possible answers fill in understanding question question given the table of the handout what is the average difference in ratings between the matrix and planes answer 4 4 5 4 5 4 4 4 so planes is rated on average higher than the matrix understanding question question given the table of the handout what would be the predicted value for frank rating of the matrix just utilizing the above differences and his rating of planes possible answers fill in understanding question question given the table of the handout what would be the predicted value for frank rating of the matrix just utilizing the above differences and his rating of planes answer movielens data 2017 alexandros labrinidis university of pittsburgh cs spring 2017 movielens data http grouplens org datasets movielens movielens data sets were collected by the grouplens research project at the university of minnesota this data set smallest available consists of 000 ratings 5 from users on movies each user has rated at least movies files base training data set of original dataset test test data set 20 of original dataset files are a tab separated list of user id item id rating timestamp the time stamps are unix seconds since utc 2017 alexandros labrinidis university of pittsburgh cs spring 2017 general information when tue thu 30 am am where david lawrence hall instructor prof alexandros labrinidis contact ta anatoli shein contact purpose this course will provide an overview of data science technologies and techniques offering a holistic view of the field from data management manipulation to data analysis and data presentation the course will cover the main data management querying paradigms relational sql xml xquery rdf sparql graph cypher along with information retrieval data warehousing data mining data visualization and other data analysis topics the course will utilize python as the default programming language and leverage existing libraries as appropriate prerequisites a grade of c or better in cs is required or permission of the instructor good working knowledge of java and familiarity with unix are assumed having passed a statistics course is highly encouraged antirequisites given the significant overlap with past offerings of students who have already passed will not be allowed to register for this class the same applies for students who have passed in the spring term enrollment students currently registered textbook there is no single textbook with enough coverage of all the material that we will discuss in this class we will rely on online references and also on o reilly safari bookshelf for which the university has institutional access i e you will not have to buy extra books web page http org class blog http blog org socrative http socrative org or download the socrative app and select room quick intro to python january 2017 quick introduction to python cs introduction to data science alexandros labrinidis http org in adapted from scipy lecture notes http www scipy lectures org index html section the python language http www scipy lectures org intro language html print hello world hello world in a b a type b out int in print b a b out 18 in 4 b hello type b out 4 str in 5 b b out 5 hellohello in b out hellohello note above was example of dynamically typed variables in out in 8 a 4 type a out 8 int in c type c out float in 4 test 4 test out false in type test out bool in integer division python out in integer division python 5 out in out 5 in a b a b in python out in a float b out 5 in from future import division out 5 lists in 18 lists l red white blue gold black white type l out 18 list in print l print l print l print l print l causes syntax error red white blue gold black white blue white black in 20 l start stop contains the elements with index i such that start i i e i ranges from start to stop print l print l print l 4 print l red white blue red white blue blue gold blue gold black white in 21 lists are mutable l yellow print l l 4 light blue gold print l red white blue yellow black white red white light blue gold black white in l print l l append green print l red light blue gold black white red light blue gold black white green in l pop print l red light blue gold black white in l extend one two print l red light blue gold black white one two in l l print l red light blue gold black white in m alice bob cathy dre print m n l m print n alice bob cathy dre red light blue gold black white alice bob cathy dr strings in strings hello how are you hi what up tripling the quotes allows the string to span more than one line hello how are you print astr abcdefghij def print astr print astr print astr strings are not immutable astr z uncomment to see syntax error print astr hello how are you abcdefghij def def defghij def abcdefghij def in print astr print astr replace d print astr abcdefghij def abefghij def abcdefghij def in astr replace d out abefghijef in 30 fs an integer i a float f another string 0 alice fs out 30 an integer a float 0 010000 another string alice in i filename d txt i filename out processing_of_dataset_102 txt dictionaries in dictionaries courses intro to database systems web programming algorithm implementation courses intro to data science print courses web programming intro to data science intro in courses out 33 intro to database systems in print courses keys print courses values web programming intro to data science intro to database systems algorit in in courses out true in cs1655 in courses out 36 false in mixed a b hello mixed out hello a b in tweet user alex text today features an intro to python hashtags datascience python pitt print tweet print tweet text print tweet hashtags print tweet hashtags text today features an intro to python hashtags today features an intro to python datascience python pitt python 4 tuples in tuples immutable lists t hello there print t 0 print t u 0 print u u uncomment to see syntax error print u 54321 hello there 0 0 in 4 also_tup 5 try 13 except typeerror print cannot modify a tuple cannot modify a tuple 5 sets in sets unordered unique items set a b c a print print difference a b set a c b set c assignments in assignment a banana b banana a is b out true in c print c d c print d print c is d d 0 z print d print c d a b c d print d print c is d true z z 2 a b c d false in d 2 4 5 d print id c print id d control flow in control flow if 2 2 4 print tada a if a print elif a 2 print 2 else print more than 2 print range for idx in range 4 print idx for word in alice bob cathy dolores emily print word for word in alice bob cathy dolores emily print word the following evaluate to false any number equal to zero an empty container false none everything else evaluates to true 2 is 2 in 2 4 in 2 vowels aeiouy for i in powerful if i in vowels print i ar a b 2 2 c third for key val in ar items print key has value key val for key val in sorted ar items print key has value key val i 2 for i in range 5 x for x in range 25 if x 2 0 tada more than 2 0 2 3 4 5 0 2 3 alice bob cathy dolores emily alice bob cathy dolores emily o e u key a has value key c has value third key b has value 2 2 key a has value key b has value 2 2 key c has value third out 45 0 2 4 8 18 20 8 five word counting examples in five word counting examples alma mater wise and glorious child of light and bride of truth over fate and foe victorious dowered with eternal youth crowned with love of son and daughter thou shalt conquer as of yore dear old pittsburgh alma mater god preserve thee evermore split in info from https docs python org 2 library pprint html import pprint pp pprint prettyprinter indent 4 in version empty dict for word in if word in word else word pp pprint alma 2 bride child crowned dear dowered god light mater 2 over pittsburgh thee thou truth and 4 as conquer daughter eternal evermore fate foe glorious love of 4 old preserve shalt son victorious wise with 2 yore youth in version 2 empty dict for word in try word except keyerror word in version 3 empty dict for word in get word 0 word in version 4 from collections import defaultdict defaultdict int for word in word_counts word in version 5 from collections import counter word_counts counter functions in functions def squared x 5 returns the square of the input parameter with a default value of note default values are evaluated when function is defined not when return x x print squared 12 print squared 25 in using tuples to return multiple results def x y return x y x y sp print sp p print print p print p in pass by value https jeffknupp com blog 13 is python callbyvalue or callbyre def x y z x y append z new reference print x is x print y is y print z is z a immutable variable b mutable variable c mutable variable a b c print a is a print b is b print c is c x is y is 42 z is a is b is 99 42 c is in global variables scope rules http sebastianraschka com articles x 5 def addx y return x ylook but not touch print addx x 5 def setx y x y print x is d x setx 12 print x 15 x is 12 5 in in recitation python tutorial in this recitation you will be learning file i o json files and csv files in python packages you will need are os csv json part 2 file i o reading and writing text files is typically very straightforward here is an overview of file i o in python a getting input filename from command line enter name of input file b checking if file exists and manipulating pathnames if os path isfile os path join os getcwd c open close and modifying file position f open print f tell current position f read read one byte and move forward f next get bytes from the file until newline f seek 3 2 move back 3 characters before the end f seek 0 move back to beginning of file f close important if writing to file another way of opening the file is to use with statement the use of with statement establishes a context in which the file will be used and when control leaves the with block the file will be closed automatically you don t need to use the with statement but if you don t use it make sure you remember to close the file with open r as f print f tell d read write to files there are several ways of reading and writing to files in python here we show some of them one way of reading the file is f read an alternate way of reading line by line by iterating over the file is for line in f print line in order to write to a file f writelines yay written to file on friday d n part 3 json json file format is used to save generic data programmatic data structures as strings and is commonly used for serialization json is good for storing simple data structures into text file and sending data to other people zipcodes 60603 dump into a file f open json wb json dump zipcodes f f close load back into python f open json rb json load f f close compare print checking zipcodes print zipcodes part 4 json read in this example we will download a json file from a url and display the file one line at a time print downloading json file and printing entire file response get http org rec hours json print response content print loading as json and iterating one line at a time hours json loads response content print hours print niterating over json for line in hours print line part 5 convert to csv on your own for the next task you need to write a python program to convert the json file downloaded from part 4 into a csv file that you need to save to disk to do this you should look up the csv module and the writer function in particular part 6 read csv on your own for the next task you need to write a python program to read the csv file from disk and display its raw contents without recognizing rows or fields part read from csv rows on your own for the next task you need to write a python program to read the csv file from disk and display its contents one row at a time without recognizing fields part 8 read from csv cells on your own for the next task you need to write a python program to read the csv file from disk and display its contents one row at a time and then within each row one field at a time cs introduction to data science spring 2017 to get started you will first need to check if python is preinstalled in your machine by typing python on the command line if it then you are good to get started if not then here are some resources to help you setup and start coding in python please note that we will be using x in this course 2 version of python but not or higher a installation to install python visit https www python org downloads and download any 2 x release for your machine once the installation process is complete type python in command line again if python does not fire up then you will have to add python to your system path variable you can set the path using set command in windows or through the system variables dialogue box and export command in mac if you have both and installed please make sure that appears first on the system path b ide you are free to use any text editor or ide one option is to use pycharm ide with a free community edition for both windows and mac available here https www jetbrains com pycharm download c package manager you can install python packages using pip pip comes pre installed with 2 7 but you can also install it from here https pip pypa io en stable installing alternatively anaconda is a great package manager that comes with over a 100 pre installed python packages for data science and can found here https www continuum io downloads 1 2 additional credits zuha agha in this recitation you will be learning pandas dataframe basics and plotting in python packages you will need are pandas matplotlib first step is to import the packages above if import fails it means that the package is not installed 1 3 dataframe basics dataframe is a 2 dimensional labeled data structure with columns of potentially different types you can think of it like a spreadsheet or sql table dataframe accepts many different kinds of input dict of ndarrays lists dicts or series 2 d numpy ndarray structured or record ndarray a series another dataframe along with the data you can optionally pass index row labels and columns column labels arguments now what is a series series is a one dimensional labeled array capable of holding any data type integers strings floating point numbers python objects etc you can think of it as a 1 dimensional dataframe series objects can also have index creating a dataframe we will start off by creating a dataframe from weather undergraound data retreived from the url below to display the top n rows of the dataframe use the head command below the default is 5 rows now to find all the column names of the dataframe and their data types type the following command max temperaturef mean temperaturef min temperaturef max dew pointf meandew pointf min dewpointf max humidity mean humidity min humidity max sea level pressurein mean sea level pressurein min sea level pressurein max visibilitymiles mean visibilitymiles min visibilitymiles max wind speedmph mean wind speedmph max gust speedmph precipitationin object cloudcover events object winddirdegrees br object dtype object notice the type of est column we will find out why that relevant a few steps later accessing dataframe columns there are two ways to access a dataframe column the first way is accessing it like a dictionary as shown below we will be using head function to show the first few rows only you can also access multiple columns by passing list of column names in df est mean temperaturef head out est mean temperaturef 1 4 plotting 1 4 1 basic plot now lets start with basic plotting in python first we will use the plot function note that plot returns a tuple of handle and labels if you need the plot handle in the future you will assign a variable to the plot function return value in plt plot df est df meandew pointf plt plot df est df mean temperaturef plt legend 0 0 mean dew point mean temperature plt show that does not look too pretty let format the graph and plot again in initializing a larger figure fig plt figure figsize 6 plotting plt plot df est df meandew pointf plt plot df est df mean temperaturef plt legend 0 0 mean dew point mean temperature formatting graph are we ready to show the formatted graph now not yet because we want to save our graph figure this time in order to use the save command it is important to save before the show command because the show command clears the axis of the figure after displaying now lets try to plot the two graphs above on the same figure using subplots the code for plotting is the same as that shown above what is teh advantage of subplots it allows us to establish relationships between different variables and different statistics looking at the two subplots above do you notice any relation ship 1 4 3 scatter plot scatter plots are commonly used to show correlation between variables if the data points make a straight line going from the origin out to high x and high y values then the variables are said to have a positive correlation if the line goes from a high value on the y axis down to a high value on the x axis the variables have a negative correlation the closer the data points in the scatter plot the higher the correlation between the two variables or the stronger the relationship the graph above shows us that min and max temperature have a strong positive correlation 1 5 tasks for your tasks the input file is available at http db cs pitt edu courses rec the file consists of population density estimates and land area of several cities in usa you need to read the file into a dataframe and perform the following three tasks during the recitation task 1 plot a scatter plot of with land area on the x axis and estimate on the y axis after observing the plot do you think the two variables are strongly or weakly correlated is the corre lation positive or negative task 2 plot a bar plot showing each city population estimate given by estimate column task 3 now that you plotted a simple bar plot try plotting a grouped bar plot that shows both and estimate for each city on the same plot this means that there will be two grouped bars per city on your graph recitation association rule mining jan 25 2017 teaching assistant anatoli shein assume the following market basket transactions for a fictional super market that carries the following products a angel hair pasta b bread c cereal d diapers e eggs and f flour compute the support counts for each of the size 1 itemsets compute the support counts for each doubleton i e size 2 itemsets assuming a minimum support count threshold of 3 identify which of the above doubletons would be frequent itemsets for the frequent size 2 itemsets identified in generate all possible association rules and compute their confidence assuming a minimum confidence threshold of identify which association rules are good using the frequent size 2 itemsets from generate all possible candidate frequent size 3 itemsets discard those size 3 itemsets that are not possible to be frequent frequent means to have support count of at least 3 using the subset property compute the actual support counts of the size 3 itemsets identified in and select those with support count of at least 3 q7 for the frequent size 3 itemsets identified in generate all possible association rules and compute their confidence assuming a minimum confidence threshold of identify which association rules are good assume the following market basket transactions for a fictional super market that carries the following products a angel hair pasta b bread c cereal d diapers e eggs and f flour compute the support counts for each of the size 1 itemsets compute the support counts for each doubleton i e size 2 itemsets assuming a minimum support count threshold of 3 identify which of the above doubletons would be frequent itemsets for the frequent size 2 itemsets identified in generate all possible association rules and compute their confidence assuming a minimum confidence threshold of identify which association rules are good using the frequent size 2 itemsets from generate all possible candidate frequent size 3 itemsets discard those size 3 itemsets that are not possible to be frequent frequent means to have support count of at least 3 using the subset property cdf abd bcf abf adf acd abc bdf acf bcd compute the actual support counts of the size 3 itemsets identified in and select those with support count of at least 3 q7 for the frequent size 3 itemsets identified in generate all possible association rules and compute their confidence assuming a minimum confidence threshold of identify which association rules are good 1 cs introduction to data science spring 2017 1 1 instructor alexandros labrinidis teaching assistant anatoli shein 1 1 1 additional credits zuha agha 1 2 recitation 3 clustering 1 3 dataframes groupby numpy arrays we will start by reading our input file into a dataframe as learnt in last recitation next we will group our data by the cluster label and compute the mean of each cluster across all columns as shown below this will be used later to compare the output of our kmeans cluster ing 1 4 kmeans clustering kmeans is an unsupervised clustering algorithm useful for data big data analysis as it allows compressing the data into clusters based on their similarity the general idea of the algorithm is to start with a random initialization of cluster centers and then assign each data sample to the nearest cluster by measuring its distance to all cluster centers the process is repeated iteratively we will begin by intializing our object for kmeans the main parameter of kmeans cluster ing algorithm is the number of clusters to form given by parameter in the function below other paramters include init which specifies the method for initialization of cluster cen troids and which is a fixed seed used by the random number generator fixing the random number generator seed allows replicating results with repeated runs otherwise repeated run could be initialzed with different random seeds giving different outputs remember that an unsupervised algorithm means that the algorithm work without having any knowledge of the true labels in this case the crimecluster labels the algorithm only takes the input features and outputs cluster labels for each data sample we will only use two features as input to clustering here so that it is easy for us to visualize the clusters on a 2 dimensional scatter plot later you will be given a task where you will use all data features for clustering as well to run kmeans on a data use the following command let look at the output of our kmeans clusters one way is to look at the cluster centroids a cluster centroid is the mean of all data samples within a cluster so we can compare our cluster centroids to the actual cluster means we calculated earlier looking at the output we can see that the output of our predicted kmeans cluster centers matches the mean of actual clusters shown above but the order is slightly different that okay also notice the data structure of in 7 type out 7 numpy ndarray now what are numpy arrays numpy arrays are n dimenisonal arrays in python extremely useful for large scale linear algebra and matrix operations let look at out predicted cluster labels output now as you can see our predicted cluster labels are ordered differently let do the following to reorder our predicted cluster labels now next we will compare our predicted cluster labels with our ground truth cluster labels by adding our predicted labels as a column in the data frame remember that a column of the dataframe is a series object and you can create a series object from a numpy array to check if we have any rows where the crimecluster and predictedcluster labels do not match we can use the selection operators of the dataframe as shown below in df df predictedcluster df crimecluster out empty dataframe columns state murder assault urbanpop rape crimecluster predicted index the output shows that there are no rows where the two labels mismatch but you could have a different predicted labels output based on seed initialization or choice of k or cluster centers initialization 1 5 visualizing clusters now we will visualize our clusters using scatter plots learnt in the previous recitation good to see pennsylvania has a relatively low crime rate 1 6 cluster analysis the last step is to analyze the impact of different values for on our clustering output good clustering output will have clusters that are well separated with samples that are close to one another i e the sum of distances to cluster centroid is low within a cluster if samples within a cluster are too spread out and the boundary between the clusters is not well separated then the clustering output is not good enough the key to good clustering is finding the right cluster size note that the number of distinct clusters within a data is not known to the algorithm as the algorithm is unsupervised let try kmeans with 3 clusters looking at the outputs above we can see how the separation of clusters and the spread of clusters is affected as k changes 1 7 tasks you have to do the following three tasks on the file protein csv task 1 run kmeans clustering on protein csv file using only the white meat and red meat columns use your choice of value for inital k make sure you label all the countries on the scatter plot task 2 plot and observe the output with atleast three different values of k what do you think is the best choice for k and why task 3 using the best k found in the previous task run kmeans using all columns for input features observe the difference in output labels and cluster centroids using all columns versus the output using two columns looking at the output clusters do you think it is better or worse to use more input features course description this course will provide an overview of data science technologies and techniques offering a holistic view of the field from data management manipulation to data analysis and data presenta tion the course will cover the main data management querying paradigms relational sql xml xquery rdf sparql graph cypher along with information retrieval recommender systems data warehousing data mining data visualization and other data analysis topics the course will utilize python as the default programming language and leverage existing libraries as appropriate prerequisites a grade of c or better in cs is required or permission of the instructor good working knowledge of java or python and familiarity with unix are assumed having passed a statistics course is highly encouraged anti requisites given the significant overlap with past offerings of students who have already passed will not be allowed to register for this class the same applies for students who have passed in the spring term class web page http org all handouts and class notes will be published on the class web page you are expected to check this page frequently at least twice a week textbook there is no single textbook with enough coverage of all the material that we will discuss in this class in addition to the material prepared by the instructor we will rely on online references and also on o reilly safari bookshelf for which the university has institutional access i e you will not have to buy extra books course grading assignments there will be 6 assignments worth 6 most of which will have a significant programming component see important dates for deadlines class participation 4 for both lecture and recitations including in class quizzes we will use the socrative system to capture student responses and record attendance midterm exam thursday february 9 30 am 10 45 pm location tbd snow day tuesday february 9 30 am 10 45 pm location tbd final exam tuesday april 12 pm 1 50 pm location tbd important dates tue january 10 released fri january 13 released tue january assignment0 due fri january assignment1 due fri january assignment2 released fri february 10 assignment2 due fri february 10 assignment3 released thu february midterm exam tue february midterm exam snow day class communications policies new fri march 3 assignment3 due fri march 3 released spring break march 6 10 fri march assignment4 due fri march assignment5 released fri april 7 assignment5 due fri april 7 assignment6 released fri april 21 assignment6 due tue april 25 final exam mailing list all students will be automatically subscribed to the class mailing list so that they receive time sensitive announcements from the instructor and ta in class student responses we will use the socrative system http www socrative com to capture student responses to questions and record attendance it is crucial that 1 you bring to every class either a smartphone a tablet or a laptop in order to use the socrative system you must let the instructor know within the first two weeks of classes if this is a problem 2 you provide your pitt user account name e g to socrative at the name prompt in order for us to properly record your answers email to instructor and ta instead of email we will use the piazza system which is essentially a web based bulletin board for questions and clarifications to assignments more instructions will be posted on the class web site confidential email in case you need to communicate with the instructor and ta outside of the pi azza system i e for confidential matters such as grade related questions you should send email to staff cs pitt edu we will make every effort to respond to all email requests within one business day at the latest due to spam filtering you should always use your pitt email address when sending email and include your full name technology policy new since this is the century the use of laptops tablets and other digital devices is allowed in class however when using digital devices in the classroom you must be mindful when you are emailing tweeting texting surfing etc you are not paying attention research shows that no one can multitask that well you included paying attention and taking good notes is essential to success in this course isn t that why you are here be respectful your use of digital devices should not distract other students in the class it is unlikely that taking notes or searching class related topics will be distracting to the other students however viewing videos of kittens or ice bucket challenges gone well or gone wrong will likely distract others complaints about inappropriate technology use in class will result in your privileges being curtailed or revoked be honest emailing surfing and the use of any other applications or technologies is not allowed during examinations doing so unless explicitly allowed is considered cheating in the exam and will be dealt accordingly cell phone use new answering a cell phone or texting is very disruptive and hence any use of a cell phone to make or receive calls or text messages is not permitted in the class or recitation cell phones must be switched to silent mode and if you have a phone call which cannot wait until the end of the class you need to step out of the class and then answer it audio video recording to ensure the free and open discussion of ideas students may not record classroom lectures discussion and or activities without the advance written permission of the instructor and any such recording properly approved in advance can be used solely for the student own private use e mail communication policy each student is issued a university e mail address username pitt edu upon admittance this e mail address may be used by the university for official communication with students students are expected to read e mail sent to this account on a regular basis failure to read and react to university communications in a timely manner does not absolve the student from knowing and complying with the content of the communications the university provides an e mail forwarding service that allows students to read their e mail via other service providers e g hotmail aol yahoo students that choose to forward their e mail from their pitt edu address to another address do so at their own risk if e mail is lost as a result of forwarding it does not absolve the student from responding to official communications sent to their university e mail address to forward e mail sent to your university account go to http accounts pitt edu log into your account click on edit forwarding addresses and follow the instructions on the page be sure to log out of your account when you have finished for the full e mail communication policy go to www bc pitt edu policies policy 09 10 01 html grading policy unless explicitly noted otherwise the work in this course is to be done independently dis cussions with other students on the assignments should be limited to understanding the statement of the problems except when assignments are to be done in groups in which case it is expected of members of the same group to work together cheating in any way including giving your work to someone else or receiving someone else work will result in an f for the course and a report to the appropriate university authority submissions that are alike in a substantive way will be considered to be cheating by all involved parties please protect yourselves by only storing your files in private repositories private directories and by retrieving all printouts promptly students are expected to abide by the dietrich school of arts and sciences academic integrity code of conduct posted at http www as pitt edu fac policies academic integrity grades can be appealed up to two weeks after they have been posted no appeals will be considered after that time academic integrity policy cheating plagiarism will not be tolerated students suspected of violating the uni versity of pittsburgh policy on academic integrity noted below will be required to participate in the outlined procedural process as initiated by the instructor a minimum sanction of a zero score for the quiz exam or paper will be imposed for the full academic integrity policy go to www provost pitt edu info html assignment policies new very important all assignments must be submitted electronically we will use github for assignment submissions more instructions will be posted on the class web site it is your responsibility to make sure your repositories are private doing otherwise will violate the academic integrity policy it is crucial that you strictly adhere to the specifications for command line arguments input file format and output file format as specified in the assignment descriptions it will be allowed sometimes required to use additional python libraries in your assignments how ever you have up to five 5 calendar days to request if it would be ok to do so using the piazza system no additional libraries will be allowed to be included after that late policy a late assignment will receive a deduction of 5 points if it is up to one day past the deadline and 15 points if it is up to two days past the deadline assignments that are past two days late will not be accepted make up policy students are expected to be present for all exams and quizzes make up exams will only be given in the event of an emergency and only if the instructor is informed in advance failure to notify the instructor prior to missing an exam will result in a zero for the exam final exam conflict policy in case you have a final exam conflict i e have more than two exams scheduled on the same date during finals week you need to notify the instructors of all classes involved in order to resolve the conflict by the sixth week of classes according to university procedure posted at http www registrar pitt edu assets pdf pdf students with disabilities if you have a disability for which you are or may be requesting an accommodation you are encouraged to contact both your instructor and the office of disability resources and services 140 william pitt union as early as possible in the term disability resources and services will verify your disability and determine reasonable accommodations for this course their web site is http www drs pitt edu religious observances in order to accommodate the observance of religious holidays students should inform the instructor by email of any such days that conflict with scheduled class activities within the first two weeks of the term copyrighted material all material provided through this web site is subject to copyright this applies to class and recitation notes slides handouts assignments solutions project descriptions etc you are allowed and expected to use all the provided material for personal use however you are strictly prohibited from sharing the material with others in general and from posting the material on the web or other file sharing venues in particular outline a detailed reading guide will be published on the web page along with the class notes and additional online articles and resources time permitting we will cover the following topics data mining clustering information retrieval recommender systems pagerank network analysis data warehousing python data science libraries sql xml xpath rdf sparql graph databases cypher data visualization welcome to the university of saskatchewan department of computer science cmpt theory and application of data bases web site for class notes can be accessed by registered students via moodle cs usask ca class news last updated aug welcome course syllabus cmpt catalog description lectures assignments and projects dealing with the management storage and retrieval of large volumes of data concentrates on the relational data model and relational data base management systems topics include recovery and concurrency integrity and security query optimization normalization and semantic modeling additional topics include multimedia databases and other paradigms prerequisites cmpt and please note these prerequisites will not be waived for undergraduates without major industrial database experience lectures monday wednesday and friday from to see class schedule below the lectures will answer student questions and summarize discuss illustrate a selection of major points taken from the text given the amount of time available in lectures for each chapter it will not be possible to deal with every aspect of every chapter it is required that students read the assigned portions of the text prior to the class for which they are assigned students are strongly encouraged to identify questions they need answered during their reading of the text tutorials thursday at in thorv please note the first tutorial will be thursday sept students should not miss this tutorial software and lab the lectures and exams focus on standard sql students will be using the postgresql data management system for all assignments and their project has various differences from standard sql the text covers most of the sql commands that we will need but each particular database management system has its own peculiarities postgresql is not explicitly covered in the text and which will not be covered in the lectures the tutorials will deal with the specific software that is to be used for the assignments and the project students are required to attend tutorials individual accounts will be established on the department of computer science postgresql server you must have your completed assignments project in your account for them to be marked however this can be accomplished by using text files that have been developed and tested on your own system you can load and use a copy of postgresql on your own home windows or linux system cygwin http sources redhat com cygwin is a linux emulator for windows that optionally includes a copy of postgresql a good source for on line information on postgresql is http www postgresql org docs interactive index html students are expected to use dbvisualizer as a tool to interact with postgressql it is available on the computer science lab on both the windows and linux platforms and can be loaded to your own system from http www dbvis com the installation and initial use of both postgressql and dbvisualizer will be discussed in the first tutorial on thurs sept class website moodle cs usask ca course objectives a student successfully completing this course shall be able to identify the important characteristics of data and information that need to be served by a well structured database to apply databases to meeting the differing needs of various users to apply sql type databases to the storage and retrieval of data to apply entity relationship diagrams to the design of well structured databases to apply normalization and the concepts of relational databases to the design of well structured databases to apply concepts of temporal data to the design of well structured databases to apply constraints and triggers to maintain the integrity of data within a database to successfuly combine data and databases into a single database that better serves the combined interests involved to have an understanding of selected additional advanced concepts relating to databases student evaluation marking component marking weight assignments a a a project part report part report demonstration midterm final assignments and project students are required to complete a series of individual assignments and a team project each assignment builds on the previous assignment thus later assignments will involve fixing the previous assignments before adding new work the project will then build upon the projects of a number of students assignments must be submitted via the moodle system assignments are due at a m on the day specified there will be no marks given for late assignments attendance expectations it is expected that students will attend and participate in all lectures and tutorials students will be responsible for all material presented in the lectures final exam scheduling the registrar schedules all final examinations including deferred and supplemental examinations students are advised not to make travel arrangements for the exam period until the exam schedule has been posted note all students must be properly registered in order to attend lectures and receive credit for this course text connolly and begg database systems edition pearson note the edition is very close in content and will be sufficient for this class lecture and assignment schedule for fall all dates and topics subject to change topics assignments text chapters sept sept introduction to cmpt introduction to databases a assigned due before sept ch sept sept sept the database environment the relational model sql single table queries ch ch ch sept sept sept sql multi table queries sql updates sql standards discussion of assignment sql data types and constraints a assigned due oct ch ch ch sept sept sept sql tables and views sql transactions and access control sql example sql example cont a due before sept ch ch sept oct oct entity relationship modeling enhanced entity relationship modeling discussion of assignment a due oct a assigned due oct ch ch oct oct oct advanced sql advanced sql applied in postgres advanced sql example ch oct oct oct thanksgiving day no class today e r example e r example cont discussing assignment a due oct a assigned due oct oct oct oct normalization discussion of a previous year midterm midterm exam ch oct oct oct database design methodology temporal database temporal example issues in combining databases project teams announced a due oct project assigned part due nov part demo due am dec ch nov nov nov return midterm discuss class project requirements issues in combining databases cont issues in combining databases cont nov nov nov university closed fall mid term break nov nov nov security privacy transaction management object oriented dbmss project part due nov ch ch ch nov nov nov data warehousing olap data mining ch ch ch dec dec dec class project presentations no class today class project presentations no class today class wrap up project due am nov policies in this class late assignments late assignments will receive marks students with a sufficiently serious reason for being late with an assignment that is acceptable to the instructor may be allowed an extention the instructor will not accept notes from student health services as support for the student reason for being late with an assignment missed assignments each assignment builds on the previous assignment thus any missed assignment will have to be completed as a basis for completing following assignments and the class project however this completion will not result in any marks being awarded for the missed assignment all students are required to complete all assignments as a prerequisite to being placed into a class project team any student who does not complete assignment will receive a mark of on the class project missed examinations students who have missed an exam or assignment must contact their instructor as soon as possible arrangements to make up the exam may be arranged with the instructor missed exams throughout the year are left up to the discretion of the instructor if a student may make up the exam or write at a different time if a student knows prior to the exam that she he will not be able to attend they should let the instructor know before the exam final exams a student who is absent from a final examination through no fault of his or her own for medical or other valid reasons may apply to the college of arts and science dean office the application must be made within three days of the missed examination along with supporting documentary evidence deferred exams are written during the february mid term break for term courses and in early june for term and full year courses http www arts usask ca students transition tips php general policies incomplete course work and final grades when a student has not completed the required course work which includes any assignment or examination including the final examination by the time of submission of the final grades they may be granted an extension to permit completion of an assignment or granted a deferred examination in the case of absence from a final examination the student must apply to the instructor for such an extension and furnish satisfactory reasons for the deficiency deferred final examinations are granted as per college policy in the interim the instructor will submit a computed percentile grade for the course which factors in the incomplete course work as a zero along with a grade comment of inf incomplete failure if a failing grade if an extension is granted and the required assignment is submitted within the allotted time or if a deferred examination is granted and written in the case of absence from the final examination the instructor will submit a revised computed final percentage grade the grade change will replace the previous grade and any grade comment of inf incomplete failure will be removed for provisions governing examinations and grading students are referred to the university council regulations on examinations section of the calendar university of saskatchewan calendar academic courses policy academic honesty the university of saskatchewan is committed to the highest standards of academic integrity and honesty students are expected to be familiar with these standards regarding academic honesty and to uphold the policies of the university in this respect students are particularly urged to familiarize themselves with the provisions of the student conduct appeals section of the university secretary website and avoid any behavior that could potentially result in suspicions of cheating plagiarism misrepresentation of facts and or participation in an offence academic dishonesty is a serious offence and can result in suspension or expulsion from the university all students should read and be familiar with the regulations on academic student misconduct http www usask ca secretariat student conduct appeals studentacademicmisconduct pdf as well as the standard of student conduct in non academic matters and procedures for resolution of complaints and appeals http www usask ca secretariat student conduct appeals studentnon academicmisconduct pdf academic honesty is also defined and described in the department of computer science statement on academic honesty http www cs usask ca undergrad honesty php for more information on what academic integrity means for students see the student conduct appeals section of the university secretary website at http www usask ca secretariat student conduct appeals forms integritydefined pdf examinations with disability services for students dss students who have disabilities learning medical physical or mental health are strongly encouraged to register with disability services for students dss if they have not already done so students who suspect they may have disabilities should contact dss for advice and referrals in order to access dss programs and supports students must follow dss policy and procedures for more information check http www students usask ca disability or contact dss at or dss usask ca students registered with dss may request alternative arrangements for mid term and final examinations students must arrange such accommodations through dss by the stated deadlines instructors shall provide the examinations for students who are being accommodated by the deadlines established by dss recording of lectures the instructor will not be recording lectures and will not provide approval to record lectures except if such recording is authorized by dss date of last revision aug 2014 distributed multimedia systems http people cs pitt edu chang html distributed multimedia systems course description this course is one of the graduate courses in software engineering the emphasis of this course is on modeling and design of distributed multimedia systems a framework is presented for data management multimedia information management knowledge management communications management activities management interface management and applications to distributed systems realtime systems multimedia systems and information retrieval systems design based upon this framework various research topics of distributed multimedia systems will be explored prerequisites the student is expected to be familiar with programming languages formal languages theory database theory data structures and computer networks the following courses are required formal structures database or consent of instructor textbook no textbook is required in lieu of the textbook students will be asked to read selected papers in the proceedings of int l conference on distribued multimedia systems the emphasis will be on papers related to slow intelligence systems req as an introduction students are required to read an introduction to slow intelligence systems class notes class notes and selected papers will be provided to the class the notes will be available on the web reference books and journals the handbook of multimedia information management edited by w grosky r jain and r mehrotra prenticehall ptr isbn s k chang and e jungert symbolic projection for image information retrieval and visual reasoning academic press isbn course structure the first part of the course consists of the instructor lectures to present the elements of distributed multimedia systems mainly based upon the textbook and the instructor own research lecture notes will be provided on 25 cs2650 distributed multimedia systems http people cs pitt edu chang 265 html 2 the web for this part of the course this will set the stage for the second part of the course several exercises will be assigned to motivate the term projects there will be a takehome midterm examination in the second part of the course the students will study the textbook and each student will present a paper related to a chapter of the text book at the same time the students will start to work on term projects based upon the theory presented in the first part of the course there is no final examination the students will present their term projects and turn in the term project reports grading the student is expected to do four or five exercises take one midterm give one seminar presentation related to a chapter in the textbook make a project presentation demonstration and turn in the project report the project report should be made available to the instructor and the discussant one week prior to presentation seminars term project project ideas will be presented during the first part of the course then a topic can be chosen based upon the student interest unique features of this course we will use the web extensively for this course the class notes for the first part of the course will be available on the web the exercises and takehome midterm examination will also be available on the web students are encouraged to turn in exercises midterm examination by email make presentations by developing a webbased visual multimedia presentation and give the project demo presentation the same way note if you have a disability for which you are or may be requesting an accommodation you are encouraged to contact both your instructor and disability resources and services william pitt union 3837355 tty as early as possible in the term drs will verify your disability and determine reasonable accommodations for this course specialization generalization the concept of specialization generalization is associated with special types of entities known as superclasses and subclasses and the process of attribute inheritance we begin this section by defining superclasses and subclasses and by examining superclass subclass relationships we describe the process of attribute inheritance and contrast the process of specialization with the process of generaliza tion we then describe the two main types of constraints on superclass subclass rela tionships called participation and disjoint constraints we show how to represent specialization generalization in an eer diagram using uml we conclude this sec tion with a worked example of how specialization generalization may be introduced into the er model of the branch user views of the dreamhome case study described in appendix a and shown in figure superclasses and subclasses as we discussed in chapter an entity type represents a set of entities of the same type such as staff branch and propertyforrent we can also form entity types into a hierarchy containing superclasses and subclasses an entity type that includes one or more distinct subgroupings of its occurrences which must be represented in a data model a distinct subgrouping of occurrences of an entity type which must be represented in a data model entity types that have distinct subclasses are called superclasses for exam ple the entities that are members of the staff entity type may be classified as manager salespersonnel and secretary in other words the staff entity is referred to as the superclass of the manager salespersonnel and secretary subclasses the relationship between a superclass and any one of its subclasses is called a superclass subclass relationship for example staff manager has a superclass subclass relationship superclass subclass relationships each member of a subclass is also a member of the superclass in other words the entity in the subclass is the same entity in the superclass but has a distinct role the relationship between a superclass and a subclass is one to one and is called a superclass subclass relationship see section some superclasses may contain overlapping subclasses as illustrated by a member of staff who is both a manager and a member of sales personnel in this example manager and salespersonnel are overlapping subclasses of the staff superclass on the other hand not every member of a superclass is necessarily a member of a subclass for example members of staff without a distinct job role such as a manager or a member of sales personnel we can use superclasses and subclasses to avoid describing different types of staff with possibly different attributes within a single entity for example sales personnel may have special attributes such as salesarea and carallowance if all staff attributes and those specific to particular jobs are described by a single staff entity this may result in a lot of nulls for the job specific attributes clearly sales personnel have common attributes with other staff such as staffno name position and salary however it is the unshared attributes that cause problems when we try to represent all members of staff within a single entity we can also show relationships that are associated with only particular types of staff sub classes and not with staff in general for example sales personnel may have distinct relationships that are not appropriate for all staff such as salespersonnel uses car to illustrate these points consider the relation called allstaff shown in figure this relation holds the details of all members of staff no matter what position they hold a consequence of holding all staff details in one relation is that while the attributes appropriate to all staff are filled namely staffno name position and salary those that are only applicable to particular job roles are only partially filled for example the attributes associated with the manager mgrstartdate and bonus figure the allstaff relation holding details of all staff salespersonnel salesarea and carallowance and secretary typingspeed subclasses have values for those members in these subclasses in other words the attributes associ ated with the manager salespersonnel and secretary subclasses are empty for those members of staff not in these subclasses there are two important reasons for introducing the concepts of superclasses and subclasses into an er model first it avoids describing similar concepts more than once thereby saving time for the designer and making the er diagram more readable second it adds more semantic information to the design in a form that is familiar to many people for example the assertions that manager is a member of staff and flat is a type of property communicates significant semantic con tent in a concise form attribute inheritance as mentioned earlier an entity in a subclass represents the same real world object as in the superclass and may possess subclass specific attributes as well as those associated with the superclass for example a member of the salespersonnel subclass inherits all the attributes of the staff superclass such as staffno name position and sal ary together with those specifically associated with the salespersonnel subclass such as salesarea and carallowance a subclass is an entity in its own right and so it may also have one or more sub classes an entity and its subclasses and their subclasses and so on is called a type hierarchy type hierarchies are known by a variety of names including speciali zation hierarchy for example manager is a specialization of staff generalization hierarchy for example staff is a generalization of manager and is a hierarchy for example manager is a member of staff we describe the process of specializa tion and generalization in the following sections a subclass with more than one superclass is called a shared subclass in other words a member of a shared subclass must be a member of the associated super classes as a consequence the attributes of the superclasses are inherited by the shared subclass which may also have its own additional attributes this process is referred to as multiple inheritance specialization process the process of maximizing the differences between members of an entity by identifying their distinguishing characteristics specialization is a top down approach to defining a set of superclasses and their related subclasses the set of subclasses is defined on the basis of some distin guishing characteristics of the entities in the superclass when we identify a set of subclasses of an entity type we then associate attributes specific to each subclass where necessary and also identify any relationships between each subclass and other entity types or subclasses where necessary for example consider a model where all members of staff are represented as an entity called staff if we apply the process of specialization on the staff entity we attempt to identify differences between members of this entity such as members with distinctive attributes and or relationships as described earlier staff with the job roles of manager sales personnel and secretary have distinctive attributes and therefore we identify manager salespersonnel and secretary as subclasses of a specialized staff superclass generalization process the process of minimizing the differences between entities by identifying their common characteristics the process of generalization is a bottom up approach that results in the identifi cation of a generalized superclass from the original entity types for example con sider a model where manager salespersonnel and secretary are represented as distinct entity types if we apply the process of generalization on these entities we attempt to identify similarities between them such as common attributes and relationships as stated earlier these entities share attributes common to all staff and therefore we identify manager salespersonnel and secretary as subclasses of a generalized staff superclass as the process of generalization can be viewed as the reverse of the specialization process we refer to this modeling concept as specialization generalization diagrammatic representation of specialization generalization uml has a special notation for representing specialization generalization for example consider the specialization generalization of the staff entity into sub classes that represent job roles the staff superclass and the manager salespersonnel and secretary subclasses can be represented in an eer diagram as illustrated in figure note that the staff superclass and the subclasses being entities are represented as rectangles the subclasses are attached by lines to a triangle that points toward the superclass the label below the specialization generalization triangle shown as optional and describes the constraints on the relationship between the superclass and its subclasses these constraints are discussed in more detail in section attributes that are specific to a given subclass are listed in the lower section of the rectangle representing that subclass for example salesarea and carallow ance attributes are associated only with the salespersonnel subclass and are not applicable to the manager or secretary subclasses similarly we show attributes that are specific to the manager mgrstartdate and bonus and secretary typingspeed subclasses attributes that are common to all subclasses are listed in the lower section of the rectangle representing the superclass for example staffno name position and sal ary attributes are common to all members of staff and are associated with the staff superclass note that we can also show relationships that are only applicable to specific subclasses for example in figure the manager subclass is related to the branch entity through the manages relationship whereas the staff superclass is related to the branch entity through the has relationship figure specialization generalization of the staff entity into subclasses representing job roles we may have several specializations of the same entity based on different distin guishing characteristics for example another specialization of the staff entity may produce the subclasses fulltimepermanent and parttimetemporary which distinguishes between the types of employment contract for members of staff the specializa tion of the staff entity type into job role and contract of employment subclasses is shown in figure in this figure we show attributes that are specific to the fulltimepermanent salaryscale and holidayallowance and parttimetemporary hourlyrate subclasses as described earlier a superclass and its subclasses and their subclasses and so on is called a type hierarchy an example of a type hierarchy is shown in figure where the job roles specialization generalization shown in figure are expanded to show a shared subclass called salesmanager and the subclass called secretary with its own subclass called assistantsecretary in other words a member of the salesmanager shared subclass must be a member of the salespersonnel and manager subclasses as well as the staff superclass as a consequence the attributes of the staff superclass staffno name position and salary and the attributes of the subclasses salespersonnel salesarea and carallowance and manager mgrstartdate and bonus are inherited by the salesmanager subclass which also has its own additional attribute called salestarget assistantsecretary is a subclass of secretary which is a subclass of staff this means that a member of the assistantsecretary subclass must be a member of the secretary subclass and the staff superclass as a consequence the attributes of the staff super class staffno name position and salary and the attribute of the secretary subclass typingspeed are inherited by the assistantsecretary subclass which also has its own additional attribute called startdate figure specialization generalization of the staff entity into subclasses representing job roles and contracts of employment figure specialization generalization of the staff entity into job roles including a shared subclass called salesmanager and a subclass called secretary with its own subclass called assistantsecretary constraints on specialization generalization there are two constraints that may apply to a specialization generalization called participation constraints and disjoint constraints participation constraints determines whether every member in the superclass must partici pate as a member of a subclass a participation constraint may be mandatory or optional a superclass subclass relationship with mandatory participation specifies that every member in the super class must also be a member of a subclass to represent mandatory participation mandatory is placed in curly brackets below the triangle that points towards the superclass for example in figure the contract of employment specialization generalization is mandatory participation which means that every member of staff must have a contract of employment a superclass subclass relationship with optional participation specifies that a mem ber of a superclass need not belong to any of its subclasses to represent optional participation optional is placed in curly brackets below the triangle that points towards the superclass for example in figure the job role specialization generalization has optional participation which means that a member of staff need not have an additional job role such as a manager sales personnel or secretary disjoint constraints describes the relationship between members of the subclasses and indicates whether it is possible for a member of a superclass to be a member of one or more than one subclass the disjoint constraint only applies when a superclass has more than one subclass if the subclasses are disjoint then an entity occurrence can be a member of only one of the subclasses to represent a disjoint superclass subclass relationship or is placed next to the participation constraint within the curly brackets for exam ple in figure the subclasses of the contract of employment specialization generalization is disjoint which means that a member of staff must have a full time permanent or a part time temporary contract but not both if subclasses of a specialization generalization are not disjoint called nondis joint then an entity occurrence may be a member of more than one subclass to represent a nondisjoint superclass subclass relationship and is placed next to the participation constraint within the curly brackets for example in figure the job role specialization generalization is nondisjoint which means that an entity occurrence can be a member of both the manager salespersonnel and secretary sub classes this is confirmed by the presence of the shared subclass called salesmanager shown in figure note that it is not necessary to include the disjoint constraint for hierarchies that have a single subclass at a given level and for this reason only the participation constraint is shown for the salesmanager and assistantsecretary sub classes of figure the disjoint and participation constraints of specialization and generalization are distinct giving rise to four categories mandatory and disjoint optional and disjoint mandatory and nondisjoint and optional and nondisjoint worked example of using specialization generalization to model the branch view of the dreamhome case study the database design methodology described in this book includes the use of specialization generalization as an optional step step in building an eer model the choice to use this step is dependent on the complexity of the enterprise or part of the enterprise being modeled and whether using the additional concepts of the eer model will help the process of database design in chapter we described the basic concepts necessary to build an er model to represent the branch user views of the dreamhome case study this model was shown as an er diagram in figure in this section we show how specialization generalization may be used to convert the er model of the branch user views into an eer model as a starting point we first consider the entities shown in figure we examine the attributes and relationships associated with each entity to identify any similarities or differences between the entities in the branch user views require ments specification there are several instances where there is the potential to use specialization generalization as discussed shortly a for example consider the staff entity in figure which represents all mem bers of staff however in the data requirements specification for the branch user views of the dreamhome case study given in appendix a there are two key job roles mentioned namely manager and supervisor we have three options as to how we may best model members of staff the first option is to represent all mem bers of staff as a generalized staff entity as in figure the second option is to create three distinct entities staff manager and supervisor and the third option is to represent the manager and supervisor entities as subclasses of a staff superclass the option we select is based on the commonality of attributes and relationships associated with each entity for example all attributes of the staff entity are rep resented in the manager and supervisor entities including the same primary key namely staffno furthermore the supervisor entity does not have any additional attributes representing this job role on the other hand the manager entity has two additional attributes mgrstartdate and bonus in addition both the manager and supervisor entities are associated with distinct relationships namely manager manages branch and supervisor supervises staff based on this information we select the third option and create manager and supervisor subclasses of the staff superclass as shown in figure note that in this eer diagram the subclasses are shown above the superclass the relative positioning of the subclasses and superclass is not signifi cant however what is important is that the specialization generalization triangle points toward the superclass the specialization generalization of the staff entity is optional and disjoint shown as optional or as not all members of staff are managers or supervisors and in addition a single member of staff cannot be both a manager figure staff superclass with supervisor and manager subclasses and a supervisor this representation is particularly useful for displaying the shared attributes associated with these subclasses and the staff superclass and also the distinct relationships associated with each subclass namely manager manages branch and supervisor supervises staff b consider for specialization generalization the relationship between owners of property the data requirements specification for the branch user views describes two types of owner namely privateowner and businessowner as shown in figure again we have three options as to how we may best model own ers of property the first option is to leave privateowner and businessowner as two distinct entities as shown in figure the second option is to represent both types of owner as a generalized owner entity and the third option is to represent the privateowner and businessowner entities as subclasses of an owner superclass before we are able to reach a decision we first examine the attributes and relationships associated with these entities privateowner and businessowner entities share common attributes namely address and telno and have a similar relationship with property for rent namely privateowner powns propertyforrent and businessowner bowns propertyforrent however both types of owner also have different attributes for example privateowner has distinct attributes ownerno and name and businessowner has distinct attributes bname btype and contactname in this case we create a superclass called owner with privateowner and businessowner as subclasses as shown in figure the specialization generalization of the owner entity is mandatory and disjoint shown as mandatory or as an owner must be either a private owner or a busi ness owner but cannot be both note that we choose to relate the owner superclass to the propertyforrent entity using the relationship called owns the examples of specialization generalization described previously are relatively straightforward however the specialization generalization process can be taken further as illustrated in the following example c there are several persons with common characteristics described in the data requirements specification for the branch user views of the dreamhome case figure owner superclass with privateowner and businessowner subclasses study for example members of staff private property owners and clients all have number and name attributes we could create a person superclass with staff including manager and supervisor subclasses privateowner and client as sub classes as shown in figure we now consider to what extent we wish to use specialization generalization to represent the branch user views of the dreamhome case study we decide to use the specialization generalization examples described in a and b above but not c as shown in figure to simplify the eer diagram only attributes figure person superclass with staff including supervisor and manager subclasses privateowner and client subclasses figure an eer model of the branch user views of dreamhome with specialization generalization aggregation associated with primary keys or relationships are shown we leave out the rep resentation shown in figure from the final eer model because the use of specialization generalization in this case places too much emphasis on the rela tionship between entities that are persons rather than emphasizing the rela tionship between these entities and some of the core entities such as branch and propertyforrent the option to use specialization generalization and to what extent is a subjec tive decision in fact the use of specialization generalization is presented as an optional step in our methodology for conceptual database design discussed in chapter step as described in section the purpose of a data model is to provide the con cepts and notations that allow database designers and end users to unambiguously and accurately communicate their understanding of the enterprise data therefore if we keep these goals in mind we should use the additional concepts of specialization generalization only when the enterprise data is too complex to easily represent using only the basic concepts of the er model at this stage we may consider whether the introduction of specialization generalization to represent the branch user views of dreamhome is a good idea in other words is the requirement specification for the branch user views better represented as the er model shown in figure or as the eer model shown in figure we leave this for the reader to consider aggregation represents a has a or is part of relationship between entity types where one represents the whole and the other the part a relationship represents an association between two entity types that are con ceptually at the same level sometimes we want to model a has a or is part of relationship in which one entity represents a larger entity the whole consist ing of smaller entities the parts this special kind of relationship is called an aggregation booch et al aggregation does not change the meaning of navigation across the relationship between the whole and its parts or link the lifetimes of the whole and its parts an example of an aggregation is the has relationship which relates the branch entity the whole to the staff entity the part diagrammatic representation of aggregation uml represents aggregation by placing an open diamond shape at one end of the relationship line next to the entity that represents the whole in figure we redraw part of the eer diagram shown in figure to demonstrate aggre gation this eer diagram displays two examples of aggregation namely branch has staff and branch offers propertyforrent in both relationships the branch entity represents the whole and therefore the open diamond shape is placed beside this entity figure examples of aggregation branch has staff and branch offers propertyforrent composition a specific form of aggregation that represents an association between entities where there is a strong ownership and coinciden tal lifetime between the whole and the part aggregation is entirely conceptual and does nothing more than distinguish a whole from a part however there is a variation of aggregation called composition that represents a strong ownership and coincidental lifetime between the whole and the part booch et al in a composite the whole is responsible for the disposi tion of the parts which means that the composition must manage the creation and destruction of its parts in other words an object may be part of only one composite at a time there are no examples of composition in figure for the purposes of discussion consider an example of a composition namely the displays relationship which relates the newspaper entity to the advert entity as a composition this emphasizes the fact that an advert entity the part belongs to exactly one newspaper entity the whole this is in contrast to aggregation in which a part may be shared by many wholes for example a staff entity may be a part of one or more branches entities diagrammatic representation of composition uml represents composition by placing a filled in diamond shape at one end of the relationship line next to the entity that represents the whole in the relation ship for example to represent the newspaper displays advert composition the filled in diamond shape is placed next to the newspaper entity which is the whole in this relationship as shown in figure chapter summary figure an example of composition newspaper displays advert as discussed with specialization generalization the options to use aggregation and composition and to what extent are again subjective decisions aggregation and composition should be used only when there is a requirement to emphasize special relationships between entity types such as has a or is part of which has implications on the creation update and deletion of these closely related entities we discuss how to represent such constraints between entity types in our methodol ogy for logical database design in chapter step if we remember that the major aim of a data model is to unambiguously and accu rately communicate an understanding of the enterprise data we should only use the additional concepts of aggregation and composition when the enterprise data is too complex to easily represent using only the basic concepts of the er model chapter summary a superclass is an entity type that includes one or more distinct subgroupings of its occur rences which require to be represented in a data model a subclass is a distinct subgroup ing of occurrences of an entity type which require to be represented in a data model specialization is the process of maximizing the differences between members of an entity by identifying their distinguishing features generalization is the process of minimizing the differences between entities by identifying their common features there are two constraints that may apply to a specialization generalization called participa tion constraints and disjoint constraints a participation constraint determines whether every member in the superclass must participate as a member of a subclass a disjoint constraint describes the relationship between members of the subclasses and indicates whether it is possible for a member of a superclass to be a member of one or more than one subclass aggregation represents a has a or is part of relationship between entity types where one represents the whole and the other the part composition is a specific form of aggregation that represents an association between entities where there is a strong ownership and coincidental lifetime between the whole and the part what are the key differences between the er and eer models describe situations that would call for an enhanced entity relationship in data modeling describe and illustrate using an example the process of attribute inheritance what are the main reasons for introducing the concepts of superclasses and subclasses into an er model describe what a shared subclass represents and how this concept relates to multiple inheritance describe and contrast the process of specialization with the process of generalization describe the uml notation used to represent superclass subclass relationships describe and contrast the concepts of aggregation and composition and provide an example of each consider whether it is appropriate to introduce the enhanced concepts of specialization generalization aggrega tion and or composition for the case studies described in appendix b the features of eer and er models can co exist in the same diagram what situations lead to this co existence analyze the case presented in question and redraw the er diagram as an eer diagram with the additional enhanced concepts introduce specialization generalization concepts into the er model shown in figure and described in exer cise to show the following staff uses space provides parkinglot staffno pk spaceno pk parkinglotname pk name location extensiontelno capacity vehlicenseno nooffloors figure parking lot er model was described in exercise a the majority of parking spaces are under cover and each can be allocated for use by a member of staff for a monthly rate b parking spaces that are not under cover are free to use and each can be allocated for use by a member of staff c up to twenty covered parking spaces are available for use by visitors to the company however only mem bers of staff are able to book out a space for the day of the visit there is no charge for this type of booking but the member of staff must provide the visitor vehicle license number the final answer to this exercise is shown as figure the library case study described in exercise is extended to include the fact that the exercises library has a significant stock of books that are no longer suitable for loaning out these books can be sold for a fraction of the original cost however not all library books are eventu ally sold as many are considered too damaged to sell on or are simply lost or stolen each book copy that is suitable for selling has a price and the date that the book is no longer to be loaned out introduce enhanced concepts into the er model shown in figure and described in exercise to accommodate this extension to the original case study the answer to this exercise is shown as figure figure library er model was described in exercise chapter normalization when we design a database for an enterprise the main objective is to create an accurate representation of the data relationships between the data and constraints on the data that is pertinent to the enterprise to help achieve this objective we can use one or more database design techniques in chapters and we described a technique called er modeling in this chapter and the next we describe another database design technique called normalization normalization is a database design technique that begins by examining the rela tionships called functional dependencies between attributes attributes describe some property of the data or of the relationships between the data that is impor tant to the enterprise normalization uses a series of tests described as normal forms to help identify the optimal grouping for these attributes to ultimately iden tify a set of suitable relations that supports the data requirements of the enterprise although the main purpose of this chapter is to introduce the concept of func tional dependencies and describe normalization up to third normal form in chapter we take a more formal look at functional dependencies and also consider later normal forms that go beyond the purpose of normalization a technique for producing a set of relations with desirable prop erties given the data requirements of an enterprise the purpose of normalization is to identify a suitable set of relations that support the data requirements of an enterprise the characteristics of a suitable set of rela tions include the following the minimal number of attributes necessary to support the data requirements of the enterprise attributes with a close logical relationship described as functional dependency are found in the same relation minimal redundancy with each attribute represented only once with the impor tant exception of attributes that form all or part of foreign keys see section which are essential for the joining of related relations the benefits of using a database that has a suitable set of relations is that the database will be easier for the user to access and maintain the data and take up how normalization supports database design minimal storage space on the computer the problems associated with using a rela tion that is not appropriately normalized is described later in section how normalization supports database design normalization is a formal technique that can be used at any stage of database design however in this section we highlight two main approaches for using nor malization as illustrated in figure approach shows how normalization can be used as a bottom up standalone database design technique and approach shows how normalization can be used as a validation technique to check the struc ture of relations which may have been created using a top down approach such as er modeling no matter which approach is used the goal is the same creating a set of well designed relations that meet the data requirements of the enterprise figure shows examples of data sources that can be used for database design although the users requirements specification see section is the preferred data source it is possible to design a database based on the information taken directly from other data sources such as forms and reports as illustrated in this chapter and the next figure also shows that the same data source can be used for both approaches however although this is true in principle in practice the approach taken is likely to be determined by the size extent and complexity of the database being described by the data sources and by the preference and expertise of the database designer the opportunity to use figure how normalization can be used to support database design normalization as a bottom up standalone technique approach is often lim ited by the level of detail that the database designer is reasonably expected to manage however this limitation is not applicable when normalization is used as a validation technique approach as the database designer focuses on only part of the database such as a single relation at any one time therefore no matter what the size or complexity of the database normalization can be usefully applied data redundancy and update anomalies as stated in section a major aim of relational database design is to group attributes into relations to minimize data redundancy if this aim is achieved the potential benefits for the implemented database include the following updates to the data stored in the database are achieved with a minimal number of operations thus reducing the opportunities for data inconsistencies occurring in the database reduction in the file storage space required by the base relations thus minimiz ing costs of course relational databases also rely on the existence of a certain amount of data redundancy this redundancy is in the form of copies of primary keys or candidate keys acting as foreign keys in related relations to enable the modeling of relation ships between data in this section we illustrate the problems associated with unwanted data redun dancy by comparing the staff and branch relations shown in figure with the staffbranch relation shown in figure the staffbranch relation is an alternative format of the staff and branch relations the relations have the following form staff staffno sname position salary branchno branch branchno baddress staffbranch staffno sname position salary branchno baddress figure staff and branch relations data redundancy and update anomalies figure staffbranch relation note that the primary key for each relation is underlined in the staffbranch relation there is redundant data the details of a branch are repeated for every member of staff located at that branch in contrast the branch details appear only once for each branch in the branch relation and only the branch number branchno is repeated in the staff relation to represent where each member of staff is located relations that have redundant data may have problems called update anomalies which are classified as insertion deletion or modification anomalies insertion anomalies there are two main types of insertion anomaly which we illustrate using the staffbranch relation shown in figure to insert the details of new members of staff into the staffbranch relation we must include the details of the branch at which the staff are to be located for example to insert the details of new staff located at branch number we must enter the correct details of branch number so that the branch details are consist ent with values for branch in other tuples of the staffbranch relation the relations shown in figure do not suffer from this potential inconsistency because we enter only the appropriate branch number for each staff member in the staff relation instead the details of branch number are recorded in the database as a single tuple in the branch relation to insert details of a new branch that currently has no members of staff into the staffbranch relation it is necessary to enter nulls into the attributes for staff such as staffno however as staffno is the primary key for the staffbranch rela tion attempting to enter nulls for staffno violates entity integrity see section and is not allowed we therefore cannot enter a tuple for a new branch into the staffbranch relation with a null for the staffno the design of the rela tions shown in figure avoids this problem because branch details are entered in the branch relation separately from the staff details the details of staff ultimately located at that branch are entered at a later date into the staff relation deletion anomalies if we delete a tuple from the staffbranch relation that represents the last member of staff located at a branch the details about that branch are also lost from the database for example if we delete the tuple for staff number mary howe from the staffbranch relation the details relating to branch number are lost from the database the design of the relations in figure avoids this problem because branch tuples are stored separately from staff tuples and only the attribute branchno relates the two relations if we delete the tuple for staff number from the staff relation the details on branch number remain unaffected in the branch relation modification anomalies if we want to change the value of one of the attributes of a particular branch in the staffbranch relation for example the address for branch number we must update the tuples of all staff located at that branch if this modification is not car ried out on all the appropriate tuples of the staffbranch relation the database will become inconsistent in this example branch number may appear to have different addresses in different staff tuples the previous examples illustrate that the staff and branch relations of figure have more desirable properties than the staffbranch relation of figure this demonstrates that although the staffbranch relation is subject to update anomalies we can avoid these anomalies by decomposing the original relation into the staff and branch relations there are two important properties associated with decompo sition of a larger relation into smaller relations the lossless join property ensures that any instance of the original relation can be identified from corresponding instances in the smaller relations the dependency preservation property ensures that a constraint on the original relation can be maintained by simply enforcing some constraint on each of the smaller relations in other words we do not need to perform joins on the smaller relations to check whether a constraint on the original relation is violated later in this chapter we discuss how the process of normalization can be used to derive well formed relations however we first introduce functional dependencies which are fundamental to the process of normalization functional dependencies an important concept associated with normalization is functional dependency which describes the relationship between attributes maier in this section we describe functional dependencies and then focus on the particular characteris tics of functional dependencies that are useful for normalization we then discuss how functional dependencies can be identified and used to identify the primary key for a relation characteristics of functional dependencies for the discussion on functional dependencies assume that a relational schema has attributes a b c z and that the database is described by a single universal relation called r a b c z this assumption means that every attribute in the database has a unique name describes the relationship between attributes in a relation for example if a and b are attributes of relation r b is functionally dependent on a denoted a b if each value of a is associated with exactly one value of b a and b may each consist of one or more attributes functional dependency is a property of the meaning or semantics of the attrib utes in a relation the semantics indicate how attributes relate to one another and specify the functional dependencies between attributes when a functional depend ency is present the dependency is specified as a constraint between the attributes consider a relation with attributes a and b where attribute b is functionally dependent on attribute a if we know the value of a and we examine the relation that holds this dependency we find only one value of b in all the tuples that have a given value of a at any moment in time thus when two tuples have the same value of a they also have the same value of b however for a given value of b there may be several different values of a the dependency between attributes a and b can be represented diagrammatically as shown figure an alternative way to describe the relationship between attributes a and b is to say that a functionally determines b some readers may prefer this descrip tion as it more naturally follows the direction of the functional dependency arrow between the attributes refers to the attribute or group of attributes on the left hand side of the arrow of a functional dependency figure a functional dependency diagram when a functional dependency exists the attribute or group of attributes on the left hand side of the arrow is called the determinant for example in figure a is the determinant of b we demonstrate the identification of a functional depend ency in the following example example an example of a functional dependency consider the attributes staffno and position of the staff relation in figure for a specific staffno for example we can determine the position of that member of staff as manager in other words staffno functionally determines position as shown in figure a however figure b illustrates that the opposite is not true as position does not functionally determine staffno a member of staff holds one position however there may be several members of staff with the same position the relationship between staffno and position is one to one for each staff number there is only one position on the other hand the relationship between position and staffno is one to many there are several staff numbers associated with a given position in this example staffno is the determinant of this functional dependency for the purposes of normalization we are interested in identifying functional dependencies between attributes of a relation that have a one to one relationship between the attribute that makes up the determinant on the left hand side and the attribute on the right hand side of a dependency when identifying functional dependencies between attributes in a relation it is important to distinguish clearly between the values held by an attribute at a given point in time and the set of all possible values that an attribute may hold at different times in other words a functional dependency is a property of a relational schema intension and not a property of a particular instance of the schema extension see section this point is illustrated in the following example figure a staffno functionally determines position staffno position b position does not functionally determine staffno position x staffno example example of a functional dependency that holds for all time consider the values shown in staffno and sname attributes of the staff relation in figure we see that for a specific staffno for example we can determine the name of that member of staff as john white furthermore it appears that for a specific sname for example john white we can determine the staff number for that member of staff as can we therefore conclude that the staffno attribute functionally determines the sname attribute and or that the sname attribute functionally determines the staffno attribute if the values shown in the staff relation of figure represent the set of all possi ble values for staffno and sname attributes then the following functional dependencies hold staffno sname sname staffno however if the values shown in the staff relation of figure simply represent a set of values for staffno and sname attributes at a given moment in time then we are not so interested in such relationships between attributes the reason is that we want to identify functional dependencies that hold for all possible values for attributes of a rela tion as these represent the types of integrity constraints that we need to identify such constraints indicate the limitations on the values that a relation can legitimately assume one approach to identifying the set of all possible values for attributes in a relation is to more clearly understand the purpose of each attribute in that relation for example the purpose of the values held in the staffno attribute is to uniquely identify each member of staff whereas the purpose of the values held in the sname attribute is to hold the names of members of staff clearly the statement that if we know the staff number staffno of a member of staff we can determine the name of the member of staff sname remains true however as it is possible for the sname attribute to hold duplicate values for members of staff with the same name then we would not be able to determine the staff number staffno of some members of staff in this category the relationship between staffno and sname is one to one for each staff number there is only one name on the other hand the relationship between sname and staffno is one to many there can be sev eral staff numbers associated with a given name the functional dependency that remains true after consideration of all possible values for the staffno and sname attributes of the staff relation is staffno sname an additional characteristic of functional dependencies that is useful for nor malization is that their determinants should have the minimal number of attrib utes necessary to maintain the functional dependency with the attribute on the righthand side this requirement is called full functional dependency indicates that if a and b are attributes of a relation b is fully functionally dependent on a if b is functionally dependent on a but not on any proper subset of a a functional dependency a b is a full functional dependency if removal of any attribute from a results in the dependency no longer existing a functional depend ency a b is a partial dependency if there is some attribute that can be removed from a and yet the dependency still holds an example of how a full functional dependency is derived from a partial functional dependency is presented in example example example of a full functional dependency consider the following functional dependency that exists in the staff relation of figure staffno sname branchno it is correct to say that each value of staffno sname is associated with a single value of branchno however it is not a full functional dependency because branchno is also functionally dependent on a subset of staffno sname namely staffno in other words the functional dependency shown in the example is an example of a partial depend ency the type of functional dependency that we are interested in identifying is a full functional dependency as shown here staffno branchno additional examples of partial and full functional dependencies are discussed in section in summary the functional dependencies that we use in normalization have the following characteristics there is a one to one relationship between the attribute on the left hand side determinant and those on the right hand side of a functional dependency note that the relationship in the opposite direction that is from the right hand to the left hand side attributes can be a one to one relationship or one to many relationship they hold for all time the determinant has the minimal number of attributes necessary to maintain the dependency with the attribute on the right hand side in other words there must be a full functional dependency between the attribute on the left hand and right hand sides of the dependency so far we have discussed functional dependencies that we are interested in for the purposes of normalization however there is an additional type of functional dependency called a transitive dependency that we need to recognize because its existence in a relation can potentially cause the types of update anomaly discussed in section in this section we simply describe these dependencies so that we can identify them when necessary a condition where a b and c are attributes of a relation such that if a b and b c then c is transitively dependent on a via b provided that a is not functionally dependent on b or c an example of a transitive dependency is provided in example example example of a transitive functional dependency consider the following functional dependencies within the staffbranch relation shown in figure staffno sname position salary branchno baddress branchno baddress the transitive dependency branchno baddress exists on staffno via branchno in other words the staffno attribute functionally determines the baddress via the branchno attribute and neither branchno nor baddress functionally determines staffno an addi tional example of a transitive dependency is discussed in section in the following sections we demonstrate approaches to identifying a set of functional dependencies and then discuss how these dependencies can be used to identify a primary key for the example relations identifying functional dependencies identifying all functional dependencies between a set of attributes should be quite simple if the meaning of each attribute and the relationships between the attributes are well understood this type of information may be provided by the enterprise in the form of discussions with users and or appropriate documentation such as the users requirements specification however if the users are unavailable for consul tation and or the documentation is incomplete then depending on the database application it may be necessary for the database designer to use their common sense and or experience to provide the missing information example illustrates how easy it is to identify functional dependencies between attributes of a relation when the purpose of each attribute and the attributes relationships are well understood example identifying a set of functional dependencies for the staffbranch relation we begin by examining the semantics of the attributes in the staffbranch relation shown in figure for the purposes of discussion we assume that the position held and the branch determine a member of staff salary we identify the functional dependencies based on our understanding of the attributes in the relation as staffno sname position salary branchno baddress branchno baddress baddress branchno branchno position salary baddress position salary we identify five functional dependencies in the staffbranch relation with staffno branchno baddress branchno position and baddress position as determinants for each functional dependency we ensure that all the attributes on the right hand side are func tionally dependent on the determinant on the left hand side as a contrast to this example we now consider the situation where functional dependencies are to be identified in the absence of appropriate information about the meaning of attributes and their relationships in this case it may be possible to identify functional dependencies if sample data is available that is a true represen tation of all possible data values that the database may hold we demonstrate this approach in example example using sample data to identify functional dependencies consider the data for attributes denoted a b c d and e in the sample relation of figure it is important first to establish that the data values shown in this relation are representative of all possible values that can be held by attributes a b c d and e for the purposes of this example let us assume that this is true despite the relatively small amount of data shown in this relation the process of identifying the functional dependencies denoted to that exist between the attributes of the sample rela tion shown in figure is described next to identify the functional dependencies that exist between attributes a b c d and e we examine the sample relation shown in figure and identify when values in one col umn are consistent with the presence of particular values in other columns we begin with the first column on the left hand side and work our way over to the right hand side of the relation and then we look at combinations of columns in other words where values in two or more columns are consistent with the appearance of values in other columns figure the sample relation displaying data for attributes a b c d and e and the functional dependencies to that exist between these attributes for example when the value a appears in column a the value z appears in col umn c and when e appears in column a the value r appears in column c we can therefore conclude that there is a one to one relationship between attributes a and c in other words attribute a functionally determines attribute c and this is shown as functional dependency in figure furthermore as the values in column c are consistent with the appearance of particular values in column a we can also con clude that there is a relationship between attributes c and a in other words c functionally determines a and this is shown as in figure if we now consider attribute b we can see that when b or d appears in column b then w appears in column d and when f appears in column b then appears in column d we can therefore conclude that there is a relationship between attributes b and d in other words b functionally determines d and this is shown as in figure however attribute d does not functionally determine attribute b as a single unique value in column d such as w is not associated with a single consistent value in column b in other words when w appears in column d the values b or d appears in column b hence there is a one to many relationship between attributes d and b the final single attribute to consider is e and we find that the values in this column are not associated with the consistent appearance of particular values in the other columns in other words attribute e does not functionally determine attributes a b c or d we now consider combinations of attributes and the appearance of consistent values in other columns we conclude that unique combination of values in columns a and b such as a b is associated with a single value in column e which in this example is q in other words attributes a b functionally determines attribute e and this is shown as in figure however the reverse is not true as we have already stated that attribute e does not functionally determine any other attribute in the relation we also conclude that attributes b c functionally determine attribute e using the same reasoning described earlier and this functional dependency is shown as in figure we complete the examination of the relation shown in figure by considering all the remaining combinations of columns in summary we describe the function dependencies between attributes a to e in the sample relation shown in figure as follows a c fdl c a b d a b e b c e identifying the primary key for a relation using functional dependencies the main purpose of identifying a set of functional dependencies for a relation is to specify the set of integrity constraints that must hold on a relation an important integrity constraint to consider first is the identification of candidate keys one of which is selected to be the primary key for the relation we demonstrate the identi fication of a primary key for a given relation in the following two examples example identifying the primary key for the staffbranch relation in example we describe the identification of five functional dependencies for the staffbranch relation shown in figure the determinants for these functional dependencies are staffno branchno baddress branchno position and baddress position to identify the candidate key for the staffbranch relation we must identify the attribute or group of attributes that uniquely identifies each tuple in this relation if a relation has more than one candidate key we identify the candidate key that is to act as the primary key for the relation see section all attributes that are not part of the primary key non primary key attributes should be functionally dependent on the key the only candidate key of the staffbranch relation and therefore the primary key is staffno as all other attributes of the relation are functionally dependent on staffno although branchno baddress branchno position and baddress position are determi nants in this relation they are not candidate keys for the relation example identifying the primary key for the sample relation in example we identified five functional dependencies for the sample relation we examine the determinant for each functional dependency to identify the candidate key for the relation a suitable determinant must functionally determine the other attributes in the relation the determinants in the sample relation are a b c a b and b c however the only determinants that determine all the other attributes of the relation are a b and b c in the case of a b a functionally determines c b functionally determines d and a b functionally determines e in other words the attributes that make up the determinant a b can determine all the other attributes in the relation either separately as a or b or together as a b hence we see that an essen tial characteristic for a candidate key of a relation is that the attributes of a determinant either individually or working together must be able to functionally determine all the other attributes in the relation this characteristic is also true for the determinant b c but is not a characteristic of the other determinants in the sample relation namely a b or c as in each case they can determine only one other attribute in the relation in conclusion there are two candidate keys for the sample relation namely a b and b c and as each has similar characteristics such as number of attributes the selection of primary key for the sample relation is arbitrary the candidate key not selected to be the primary key is referred to as the alternate key for the sample relation so far in this section we have discussed the types of functional dependency that are most useful in identifying important constraints on a relation and how these dependencies can be used to identify a primary key or candidate keys for a given relation the concepts of functional dependencies and keys are central to the process of normalization we continue the discussion on functional dependencies in the next chapter for readers interested in a more formal coverage of this topic however in this chapter we continue by describing the process of normalization the process of normalization normalization is a formal technique for analyzing relations based on their primary key or candidate keys and functional dependencies codd the technique involves a series of rules that can be used to test individual relations so that a data base can be normalized to any degree when a requirement is not met the relation violating the requirement must be decomposed into relations that individually meet the requirements of normalization three normal forms were initially proposed called first normal form second normal form and third normal form subsequently r boyce and e f codd introduced a stronger definition of third normal form called boyce codd normal form bcnf codd with the exception of all these normal forms are based on functional dependencies among the attributes of a relation maier higher normal forms that go beyond bcnf were introduced later such as fourth normal form and fifth normal form fagin however these later normal forms deal with situations that are very rare in this chapter we describe only the first three normal forms and leave discussion of bcnf and to the next chapter normalization is often executed as a series of steps each step corresponds to a specific normal form that has known properties as normalization proceeds the relations become progressively more restricted stronger in format and also less vulnerable to update anomalies for the relational data model it is important to recognize that it is only first normal form that is critical in creating rela tions all subsequent normal forms are optional however to avoid the update anomalies discussed in section it is generally recommended that we proceed to at least third normal form figure illustrates the relationship between the various normal forms it shows that some relations are also in and that some relations are also in and so on in the following sections we describe the process of normalization in detail figure provides an overview of the process and highlights the main actions the process of normalization figure diagrammatic illustration of the relationship between the normal forms figure diagrammatic illustration of the process of normalization taken in each step of the process the number of the section that covers each step of the process is also shown in this figure in this chapter we describe normalization as a bottom up technique extracting information about attributes from sample forms that are first transformed into table format which is described as being in unnormalized form unf this table is then subjected progressively to the different requirements associated with each normal form until ultimately the attributes shown in the original sample forms are represented as a set of relations although the example used in this chapter proceeds from a given normal form to the one above this is not necessarily the case with other examples as shown in figure the resolution of a particular problem with say a relation may result in the relation being transformed to relations or in some cases directly into relations in one step to simplify the description of normalization we assume that a set of functional dependencies is given for each relation in the worked examples and that each relation has a designated primary key in other words it is essential that the mean ing of the attributes and their relationships is well understood before beginning the process of normalization this information is fundamental to normalization and is used to test whether a relation is in a particular normal form in section we begin by describing first normal form in sections and we describe second normal form and third normal forms based on the primary key of a relation and then present a more general defini tion of each in section the more general definitions of and take into account all candidate keys of a relation rather than just the primary key first normal form before discussing first normal form we provide a definition of the state prior to first normal form a table that contains one or more repeating groups a relation in which the intersection of each row and column con tains one and only one value in this chapter we begin the process of normalization by first transferring the data from the source for example a standard data entry form into table format with rows and columns in this format the table is in unnormalized form and is referred to as an unnormalized table to transform the unnormalized table to first normal form we identify and remove repeating groups within the table a repeating group is an attribute or group of attributes within a table that occurs with multiple values for a single occurrence of the nominated key attribute for that table note that in this context the term key refers to the attribute that uniquely identify each row within the unnormalized table there are two common approaches to removing repeating groups from unnormalized tables by entering appropriate data in the empty columns of rows containing the repeating data in other words we fill in the blanks by duplicating the nonrepeating data where required this approach is commonly referred to as flattening the table by placing the repeating data along with a copy of the original key attribute in a separate relation sometimes the unnormalized table may contain more than one repeating group or repeating groups within repeating groups in such cases this approach is applied repeatedly until no repeating groups remain a set of relations is in if it contains no repeating groups for both approaches the resulting tables are now referred to as relations containing atomic or single values at the intersection of each row and column although both approaches are correct approach introduces more redundancy into the original unf table as part of the flattening process whereas approach creates two or more relations with less redundancy than in the original unf table in other words approach moves the original unf table further along the nor malization process than approach however no matter which initial approach is taken the original unf table will be normalized into the same set of relations we demonstrate both approaches in the following worked example using the dreamhome case study example first normal form a collection of simplified dreamhome leases is shown in figure the lease on top is for a client called john kay who is leasing a property in glasgow which is owned by tina murphy for this worked example we assume that a client rents a given property only once and cannot rent more than one property at any one time figure collection of simplified dreamhome leases sample data is taken from two leases for two different clients called john kay and aline stewart and is transformed into table format with rows and columns as shown in figure this is an example of an unnormalized table figure clientrental unnormalized table we identify the key attribute for the clientrental unnormalized table as clientno next we identify the repeating group in the unnormalized table as the property rented details which repeats for each client the structure of the repeating group is repeating group propertyno paddress rentstart rentfinish rent ownerno oname as a consequence there are multiple values at the intersection of certain rows and col umns for example there are two values for propertyno and for the client named john kay to transform an unnormalized table into we ensure that there is a single value at the intersection of each row and column this is achieved by removing the repeating group with the first approach we remove the repeating group property rented details by entering the appropriate client data into each row the resulting first normal form clientrental relation is shown in figure figure first normal form clientrental relation in figure we present the functional dependencies fdl to for the clientrental relation we use the functional dependencies as discussed in section to identify candidate keys for the clientrental relation as being composite keys comprising clientno propertyno clientno rentstart and propertyno rentstart we select clientno propertyno as the primary key for the relation and for clarity we place the attributes that make up the primary key together at the left hand side of the relation in this example we assume that the rentfinish attribute is not appropriate as a component of a candidate key as it may contain nulls see section figure functional dependencies of the clientrental relation the clientrental relation is defined as follows clientrental clientno propertyno cname paddress rentstart rentfinish rent ownerno oname the clientrental relation is in as there is a single value at the intersection of each row and column the relation contains data describing clients property rented and property owners which is repeated several times as a result the clientrental relation contains significant data redundancy if implemented the relation would be sub ject to the update anomalies described in section to remove some of these we must transform the relation into second normal form which we discuss shortly with the second approach we remove the repeating group property rented details by placing the repeating data along with a copy of the original key attribute clientno in a separate relation as shown in figure figure alternative client and propertyrental owner relations with the help of the functional dependencies identified in figure we identify a primary key for the relations the format of the resulting relations are as follows client clientno cname propertyrentalowner clientno propertyno paddress rentstart rentfinish rent ownerno oname the client and propertyrentalowner relations are both in as there is a single value at the intersection of each row and column the client relation contains data describing clients and the propertyrentalowner relation contains data describing property rented by clients and property owners however as we see from figure this relation also contains some redundancy and as a result may suffer from similar update anomalies to those described in section to demonstrate the process of normalizing relations from to we use only the clientrental relation shown in figure however recall that both approaches are correct and will ultimately result in the production of the same relations as we con tinue the process of normalization we leave the process of completing the normaliza tion of the client and propertyrentalowner relations as an exercise for the reader which is given at the end of this chapter second normal form second normal form is based on the concept of full functional dependency which we described in section second normal form applies to relations with composite keys that is relations with a primary key composed of two or more attributes a relation with a single attribute primary key is automatically in at least a relation that is not in may suffer from the update anomalies discussed in section for example suppose we wish to change the rent of property num ber we have to update two tuples in the clientrental relation in figure if only one tuple is updated with the new rent this results in an inconsistency in the database a relation that is in first normal form and every non primary key attribute is fully functionally dependent on the primary key the normalization of relations to involves the removal of partial dependencies if a partial dependency exists we remove the partially dependent attribute from the relation by placing them in a new relation along with a copy of their determinant we demonstrate the process of converting relations to relations in the following example example second normal form as shown in figure the clientrental relation has the following functional depend encies clientno propertyno rentstart rentfinish primary key clientno cname partial dependency propertyno paddress rent ownerno oname partial dependency ownerno oname transitive dependency clientno rentstart propertyno paddress rentfinish rent ownerno oname candidate key propertyno rentstart clientno cname rentfinish candidate key figure second normal form relations derived from the clientrental relation using these functional dependencies we continue the process of normalizing the clientrental relation we begin by testing whether the clientrental relation is in by identifying the presence of any partial dependencies on the primary key we note that the client attribute cname is partially dependent on the primary key in other words on only the clientno attribute represented as the property attributes paddress rent ownerno oname are partially dependent on the primary key that is on only the propertyno attribute represented as the property rented attributes rentstart and rentfinish are fully dependent on the whole primary key that is the clientno and proper tyno attributes represented as the identification of partial dependencies within the clientrental relation indicates that the relation is not in to transform the clientrental relation into requires the creation of new relations so that the non primary key attributes are removed along with a copy of the part of the primary key on which they are fully functionally dependent this results in the creation of three new relations called client rental and propertyowner as shown in figure these three relations are in second normal form as every non primary key attribute is fully functionally dependent on the primary key of the relation the relations have the following form client clientno cname rental clientno propertyno rentstart rentfinish propertyowner propertyno paddress rent ownerno oname third normal form although relations have less redundancy than those in they may still suffer from update anomalies for example if we want to update the name of an owner such as tony shaw ownerno we have to update two tuples in the propertyowner relation of figure if we update only one tuple and not the other the database would be in an inconsistent state this update anomaly is caused by a transitive dependency which we described in section we need to remove such dependencies by progressing to third normal form a relation that is in first and second normal form and in which no non primary key attribute is transitively dependent on the primary key the normalization of relations to involves the removal of transitive dependencies if a transitive dependency exists we remove the transitively depend ent attribute from the relation by placing the attribute in a new relation along with a copy of the determinant we demonstrate the process of converting relations to relations in the following example example third normal form the functional dependencies for the client rental and propertyowner relations derived in example are as follows client clientno cname primary key rental clientno propertyno rentstart rentfinish primary key clientno rentstart propertyno rentfinish candidate key propertyno rentstart clientno rentfinish candidate key propertyowner propertyno paddress rent ownerno oname primary key ownerno oname transitive dependency all the non primary key attributes within the client and rental relations are functionally dependent on only their primary keys the client and rental relations have no transitive dependencies and are therefore already in note that where a functional depend ency fd is labeled with a prime such as this indicates that the dependency has altered compared with the original functional dependency shown in figure all the non primary key attributes within the propertyowner relation are functionally dependent on the primary key with the exception of oname which is transitively depend ent on ownerno represented as this transitive dependency was previously identified in figure to transform the propertyowner relation into we must first remove this transitive dependency by creating two new relations called propertyforrent and owner as shown in figure the new relations have the following form propertyforrent propertyno paddress rent ownerno owner ownerno oname the propertyforrent and owner relations are in as there are no further transitive dependencies on the primary key figure third normal form relations derived from the propertyowner relation general definitions of and figure the decomposition of the clientrental relation into relations the clientrental relation shown in figure has been transformed by the process of normalization into four relations in figure illustrates the process by which the original relation is decomposed into the relations the resulting relations have the form client clientno cname rental clientno propertyno rentstart rentfinish propertyforrent propertyno paddress rent ownerno owner ownerno oname the original clientrental relation shown in figure can be recreated by join ing the client rental propertyforrent and owner relations through the primary key foreign key mechanism for example the ownerno attribute is a primary key within the owner relation and is also present within the propertyforrent relation as a foreign key the ownerno attribute acting as a primary key foreign key allows the association of the propertyforrent and owner relations to identify the name of property owners the clientno attribute is a primary key of the client relation and is also present within the rental relation as a foreign key note that in this case the clientno attribute in the rental relation acts both as a foreign key and as part of the primary key of this relation similarly the propertyno attribute is the primary key of the propertyforrent relation and is also present within the rental relation acting both as a foreign key and as part of the primary key for this relation in other words the normalization process has decomposed the original clientrental relation using a series of relational algebra projections see section this results in a lossless join also called nonloss or nonadditive join decomposition which is reversible using the natural join operation the client rental propertyforrent and owner relations are shown in figure general definitions of and the definitions for and given in sections and disallow par tial or transitive dependencies on the primary key of relations to avoid the update anomalies described in section however these definitions do not take into account other candidate keys of a relation if any exist in this section we present more general definitions for and that take into account candidate keys of a relation note that this requirement does not alter the definition for as this normal form is independent of keys and functional dependencies for the general definitions we state that a candidate key attribute is part of any candidate key and figure a summary of the relations derived from the clientrental relation that partial full and transitive dependencies are with respect to all candidate keys of a relation a relation that is in first normal form and every non candidate key attribute is fully functionally dependent on any candidate key a relation that is in first and second normal form and in which no non candidate key attribute is transitively depend ent on any candidate key when using the general definitions of and we must be aware of partial and transitive dependencies on all candidate keys and not just the primary key this can make the process of normalization more complex however the general definitions place additional constraints on the relations and may identify hidden redundancy in relations that could be missed the trade off is whether it is better to keep the process of normalization simpler by examining dependencies on primary keys only which allows the identification of the most problematic and obvious redundancy in relations or to use the general definitions and increase the opportunity to identify missed redundancy in fact it is often the case that whether we use the definitions based on primary keys or the general definitions of and the decomposition of relations is the same for example if we apply the general definitions of and to examples and described in sections and section the same decomposi tion of the larger relations into smaller relations results the reader may wish to verify this fact in the following chapter we re examine the process of identifying functional dependencies that are useful for normalization and take the process of normaliza tion further by discussing normal forms that go beyond such as bcnf also in this chapter we present a second worked example taken from the dreamhome case study that reviews the process of normalization from unf through to bcnf review questions normalization is a technique for producing a set of relations with desirable properties given the data require ments of an enterprise normalization is a formal method that can be used to identify relations based on their keys and the functional dependencies among their attributes relations with data redundancy suffer from update anomalies which can be classified as insertion deletion and modification anomalies one of the main concepts associated with normalization is functional dependency which describes the relationship between attributes in a relation for example if a and b are attributes of relation r b is functionally dependent on a denoted a b if each value of a is associated with exactly one value of b a and b may each consist of one or more attributes the determinant of a functional dependency refers to the attribute or group of attributes on the left hand side of the arrow the main characteristics of functional dependencies that we use for normalization have a one to one relationship between attribute on the left hand and right hand sides of the dependency hold for all time and are fully func tionally dependent unnormalized form unf is a table that contains one or more repeating groups first normal form is a relation in which the intersection of each row and column contains one and only one value second normal form is a relation that is in first normal form and every non primary key attribute is fully functionally dependent on the primary key full functional dependency indicates that if a and b are attributes of a relation b is fully functionally dependent on a if b is functionally dependent on a but not on any proper subset of a third normal form is a relation that is in first and second normal form in which no non primary key attribute is transitively dependent on the primary key transitive dependency is a condition where a b and c are attributes of a relation such that if a b and b c then c is transitively dependent on a via b provided that a is not functionally dependent on b or c general definition for second normal form is a relation that is in first normal form and every non candidate key attribute is fully functionally dependent on any candidate key in this definition a candidate key attribute is part of any candidate key general definition for third normal form is a relation that is in first and second normal form in which no non candidate key attribute is transitively dependent on any candidate key in this definition a candidate key attribute is part of any candidate key describe the purpose of normalizing data discuss the alternative ways that normalization can be used to support database design how does normalization eradicate update anomalies from a relation describe the concept of functional dependency what are the main characteristics of functional dependencies that are used for normalization describe how a database designer typically identifies the set of functional dependencies associated with a relation describe factors that would influence the choice of normalization or er modeling when designing a database why is normalization regarded as a bottom up design approach how does it differ from er modeling describe the two approaches to converting an unf table to relation the second normal form is realized by removing partial dependencies from relations briefly describe the term partial dependency describe the concept of transitive dependency and describe how this concept relates to provide an exam ple to illustrate your answer discuss how the definitions of and based on primary keys differ from the general definitions of and provide an example to illustrate your answer normalization is an important concept for database professionals whether you are the designer database analyst or administrator it is useful for designing situation verification as well as performance tuning what are the basic issues to be aware of before carrying out the normalization process examine the patient medication form for the wellmeadows hospital case study see appendix b shown in figure a identify the functional dependencies represented by the attributes shown in the form in figure state any assumptions that you make about the data and the attributes shown in this form b describe and illustrate the process of normalizing the attributes shown in figure to produce a set of well designed relations c identify the primary alternate and foreign keys in your relations figure the wellmeadows hospital patient medication form the table shown in figure lists sample dentist patient appointment data a patient is given an appointment at a specific time and date with a dentist located at a particular surgery on each day of patient appointments a dentist is allocated to a specific surgery for that day a the table shown in figure is susceptible to update anomalies provide examples of insertion deletion and update anomalies b identify the functional dependencies represented by the attributes shown in the table of figure state any assumptions you make about the data and the attributes shown in this table c describe and illustrate the process of normalizing the table shown in figure to relations identify the primary alternate and foreign keys in your relations figure table displaying sample dentist patient appointment data an agency called instant cover supplies part time temporary staff to hotels within scotland the table shown in figure displays sample data which lists the time spent by agency staff working at various hotels the na tional insurance number nin is unique for every member of staff a the table shown in figure is susceptible to update anomalies provide examples of insertion deletion and update anomalies b identify the functional dependencies represented by the attributes shown in the table of figure state any assumptions that you make about the data and the attributes shown in this table c describe and illustrate the process of normalizing the table shown in figure to identify primary alternate and foreign keys in your relations figure table displaying sample data for the instant cover agency a company called fastcabs provides a taxi service to clients the table shown in figure displays some details of client bookings for taxis assume that a taxi driver is assigned to a single taxi but a taxi can be assigned to one or more drivers a identify the functional dependencies that exist between the columns of the table in figure and identify the primary key and any alternate key if present for the table b describe why the table in figure is not in c the table shown in figure is susceptible to update anomalies provide examples of how insertion dele tion and modification anomalies could occur on this table jobid jobdate time driverid driver name taxiid clientid clientname jobpickupaddress joe bull anne woo storrie rd paisley joe bull anne woo storrie rd paisley tom win anne woo high street paisley jim jones mark tin lady lane paisley steven win john seal red road paisley tom win karen bow high street paisley figure table displaying sample data for fastcabs applying normalisation to on the table shown in figure results in the formation of the three tables shown in figure a identify the functional dependencies that exist between the columns of each table in figure and identify the primary key and any alternate and foreign key if present for each table b describe why storing the fastcabs data across three tables avoids the update anomalies described in exercise b c describe how the original table shown in figure can be re created through relational joins between primary key and foreign keys columns of the tables in figure jobid jobdatetime driverid clientid jobpickupaddress storrier rd paisley storrier rd paisley c1 high street paisley lady lane paisley red road paisley 00 high street paisley drierid drivername taxid joe bull tom win jim jones steven win clientid clientname c1 anne woo c2 mark tin c3 john seal karen bow figure tables in displaying sample data for fastcabs students can lease university flats and some of the details of leases held by students for places in university flats are shown in figure a place number placeno uniquely identifies each single room in all flats and is used when leasing a room to a student a identify the functional dependencies that exist between the columns of the table in figure and identify the primary key and any alternate key if present for the table b describe why the table in figure is not in c the table shown in figure is susceptible to update anomalies provide examples of how insertion deletion and modification anomalies could occur on this table leaseno bannerid placeno fname iname startdate finishdate flatno flataddress jane watt high street paisley jane watt storrie road paisley tom jones 30 storrie road paisley karen black 30 lady lane paisley steven smith 30 111 storrie road paisley tom jones 30 high street paisley figure table displaying sample data for university accommodation applying normalisation to on the table shown in figure results in the formation of the four tables shown in figure a identify the functional dependencies that exist between the columns of each table in figure and identify the primary key and any alternate and foreign key if present for each table b describe why storing the university accommodation data across four tables avoids the update anomalies described in exercise b c describe how the original table shown in figure can be re created through relational joins between primary key and foreign keys columns of the tables in figure figure tables in displaying sample data of university accommodation chapter advanced normalization in the previous chapter we introduced the technique of normalization and the concept of functional dependencies between attributes we described the benefits of using normalization to support database design and demonstrated how attrib utes shown on sample forms are transformed into and relations in this chapter we return to consider functional dependencies and describe normal forms that go beyond such as bcnf and relations in are normally sufficiently well structured to prevent the problems associated with data redundancy which was described in section however later normal forms were created to identify relatively rare problems with relations that if not corrected might result in undesirable data redundancy more on functional dependencies one of the main concepts associated with normalization is functional dependency which describes the relationship between attributes maier in the previ ous chapter we introduced this concept in this section we describe this concept in a more formal and theoretical way by discussing inference rules for functional dependencies inference rules for functional dependencies in section we identified the characteristics of the functional dependencies that are most useful in normalization however even if we restrict our attention to functional dependencies with a one to one relationship between attributes on the left hand and right hand sides of the dependency that hold for all time and are fully functionally dependent then the complete set of functional dependencies for a given relation can still be very large it is important to find an approach that can reduce that set to a manageable size ideally we want to identify a set of functional dependencies represented as x for a relation that is smaller than the complete set of functional dependencies represented as y for that relation and has the property that every functional dependency in y is implied by the functional dependencies in x hence if we enforce the integrity constraints defined by the functional dependencies in x we automatically enforce the integrity constraints defined in more on functional dependencies the larger set of functional dependencies in y this requirement suggests that there must be functional dependencies that can be inferred from other functional dependencies for example functional dependencies a b and b c in a relation implies that the functional dependency a c also holds in that relation a c is an example of a transitive functional dependency and was discussed previously in sections and how do we begin to identify useful functional dependencies on a relation normally the database designer starts by specifying functional dependencies that are semantically obvious however there are usually numerous other functional dependencies in fact the task of specifying all possible functional dependencies for real database projects is more often than not impractical however in this section we do consider an approach that helps identify the complete set of func tional dependencies for a relation and then discuss how to achieve a minimal set of functional dependencies that can represent the complete set the set of all functional dependencies that are implied by a given set of func tional dependencies x is called the closure of x written x we clearly need a set of rules to help compute x from x a set of inference rules called armstrong axioms specifies how new functional dependencies can be inferred from given ones armstrong for our discussion let a b and c be subsets of the attrib utes of the relation r armstrong axioms are as follows reflexivity if b is a subset of a then a b augmentation if a b then a c b c transitivity if a b and b c then a c note that each of these three rules can be directly proved from the definition of functional dependency the rules are complete in that given a set x of functional dependencies all functional dependencies implied by x can be derived from x using these rules the rules are also sound in that no additional functional depend encies can be derived that are not implied by x in other words the rules can be used to derive the closure of x several further rules can be derived from the three given previously that simplify the practical task of computing x in the following rules let d be another subset of the attributes of relation r then self determination a a decomposition if a b c then a b and a c union if a b and a c then a b c composition if a b and c d then a c b d rule reflexivity and rule self determination state that a set of attributes always determines any of its subsets or itself because these rules generate func tional dependencies that are always true such dependencies are trivial and as stated earlier are generally not interesting or useful rule augmentation states that adding the same set of attributes to both the left hand and right hand sides of a dependency results in another valid dependency rule transitivity states that functional dependencies are transitive rule decomposition states that we can remove attributes from the right hand side of a dependency applying this rule repeatedly we can decompose a b c d functional dependency into the set of dependencies a b a c and a d rule union states that we can do the opposite we can combine a set of dependencies a b a c and a d into a single functional dependency a b c d rule composition is more general than rule and states that we can combine a set of nonoverlapping dependencies to form another valid dependency to begin to identify the set of functional dependencies f for a relation we typi cally first identify the dependencies that are determined from the semantics of the attributes of the relation then we apply armstrong axioms rules to to infer additional functional dependencies that are also true for that relation a systematic way to determine these additional functional dependencies is to first determine each set of attributes a that appears on the left hand side of some functional dependencies and then to determine the set of all attributes that are dependent on a thus for each set of attributes a we can determine the set a of attributes that are functionally determined by a based on f a is called the closure of a under f minimal sets of functional dependencies in this section we introduce what is referred to as equivalence of sets of functional dependencies a set of functional dependencies y is covered by a set of func tional dependencies x if every functional dependency in y is also in x that is every dependency in y can be inferred from x a set of functional dependencies x is minimal if it satisfies the following conditions every dependency in x has a single attribute on its right hand side we cannot replace any dependency a b in x with dependency c b where c is a proper subset of a and still have a set of dependencies that is equivalent to x we cannot remove any dependency from x and still have a set of dependencies that is equivalent to x a minimal set of dependencies should be in a standard form with no redundancies a minimal cover of a set of functional dependencies x is a minimal set of depend encies xmin that is equivalent to x unfortunately there can be several minimal cov ers for a set of functional dependencies we demonstrate the identification of the minimal cover for the staffbranch relation in the following example example identifying the minimal set of functional dependencies of the staff branch relation we apply the three conditions described previously on the set of functional dependencies for the staffbranch relation listed in example to produce the following functional dependencies staffno sname staffno position staffno salary staffno branchno branchno baddress baddress branchno branchno position salary baddress position salary these functional dependencies satisfy the three conditions for producing a minimal set of functional dependencies for the staffbranch relation condition ensures that every dependency is in a standard form with a single attribute on the right hand side conditions and ensure that there are no redundancies in the dependencies either by having redundant attributes on the left hand side of a dependency condition or by having a dependency that can be inferred from the remaining functional dependen cies in x condition in the following section we return to consider normalization we begin by dis cussing bcnf a stronger normal form than boyce codd normal form bcnf in the previous chapter we demonstrated how and disallow partial and transitive dependencies on the primary key of a relation respectively relations that have these types of dependencies may suffer from the update anomalies discussed in section however the definitions of and discussed in sections and respectively do not consider whether such dependencies remain on other candidate keys of a relation if any exist in section we presented general definitions for and that disallow partial and transitive dependencies on any candidate key of a relation respectively application of the general definitions of and may identify additional redundancy caused by dependencies that violate one or more candidate keys however despite these additional constraints dependencies can still exist that will cause redundancy to be present in rela tions this weakness in resulted in the presentation of a stronger normal form called boyce codd normal form bcnf codd definition of bcnf bcnf is based on functional dependencies that take into account all candidate keys in a relation however bcnf also has additional constraints compared with the general definition of given in section a relation is in bcnf if and only if every determinant is a candidate key to test whether a relation is in bcnf we identify all the determinants and make sure that they are candidate keys recall that a determinant is an attribute or a group of attributes on which some other attribute is fully functionally dependent the difference between and bcnf is that for a functional dependency a b allows this dependency in a relation if b is a primary key attribute and a is not a candidate key whereas bcnf insists that for this dependency to remain in a relation a must be a candidate key therefore bcnf is a stronger form of such that every relation in bcnf is also in however a relation in is not necessarily in bcnf before considering the next example we re examine the client rental propertyforrent and owner relations shown in figure the client propertyforrent and owner relations are all in bcnf as each relation only has a single determinant which is the candidate key however recall that the rental relation contains the three determinants clientno propertyno clientno rentstart and propertyno rentstart originally identified in example as shown below clientno propertyno rentstart rentfinish clientno rentstart propertyno rentfinish propertyno rentstart clientno rentfinish as the three determinants of the rental relation are also candidate keys the rental relation is also already in bcnf violation of bcnf is quite rare as it may happen only under specific conditions the potential to violate bcnf may occur when the relation contains two or more composite candidate keys or the candidate keys overlap that is have at least one attribute in common in the following example we present a situation in which a relation violates bcnf and demonstrate the transformation of this relation to bcnf this example dem onstrates the process of converting a inf relation to bcnf relations example boyce codd normal form bcnf in this example we extend the dreamhome case study to include a description of client interviews by members of staff the information relating to these interviews is in the clientlnterview relation shown in figure the members of staff involved in interview ing clients are allocated to a specific room on the day of interview however a room may be allocated to several members of staff as required throughout a working day a client is interviewed only once on a given date but may be requested to attend further interviews at later dates figure clientlnterview relation the clientlnterview relation has three candidate keys clientno interviewdate staffno interviewdate interviewtime and roomno interviewdate interviewtime therefore the clientlnterview relation has three composite candidate keys which overlap by sharing the common attribute interviewdate we select clientno interviewdate to act as the primary key for this relation the clientlnterview relation has the following form clientlnterview clientno interviewdate interviewtime staffno roomno the clientlnterview relation has the following functional dependencies clientno interviewdate interviewtime staffno roomno primary key staffno interviewdate interviewtime clientno candidate key roomno interviewdate interviewtime staffno clientno candidate key staffno interviewdate roomno we examine the functional dependencies to determine the normal form of the clientlnterview relation as functional dependencies fdl and are all candi date keys for this relation none of these dependencies will cause problems for the relation the only functional dependency that requires discussion is staffno inter viewdate roomno represented as even though staffno interviewdate is not a candidate key for the clientlnterview relation this functional dependency is allowed in because roomno is a primary key attribute being part of the candidate key roomno interviewdate interviewtime as there are no partial or transitive dependen cies on the primary key clientno interviewdate and functional dependency is allowed the clientlnterview relation is in however this relation is not in bcnf a stronger normal form of due to the presence of the staffno interviewdate determinant which is not a candidate key for the relation bcnf requires that all determinants in a relation must be a can didate key for the relation as a consequence the clientlnterview relation may suffer from update anomalies for example to change the room number for staff number on the may we must update two tuples if only one tuple is updated with the new room number this results in an inconsistent state for the database to transform the clientlnterview relation to bcnf we must remove the violating functional dependency by creating two new relations called interview and staffroom as shown in figure the interview and staffroom relations have the following form interview clientno interviewdate interviewtime staffno staffroom staffno interviewdate roomno we can decompose any relation that is not in bcnf into bcnf as illustrated however it may not always be desirable to transform a relation into bcnf for example if there is a functional dependency that is not preserved when we perform the decomposition that is the determinant and the attributes that it determines are placed in different relations in this situation it is difficult to enforce the functional dependency in the relation and an important constraint is lost when this occurs it may be better to stop at which always preserves figure the interview and staffroom bcnf relations dependencies note that in example in creating the two bcnf relations from the original clientlnterview relation we have lost the functional dependency roomno interviewdate interviewtime staffno clientno represented as as the determinant for this dependency is no longer in the same relation however we must recognize that if the functional dependency staffno interviewdate roomno represented as is not removed the clientlnterview relation will have data redundancy the decision as to whether it is better to stop the normalization at or pro gress to bcnf is dependent on the amount of redundancy resulting from the pres ence of and the significance of the loss of for example if it is the case that members of staff conduct only one interview per day then the presence of in the clientlnterview relation will not cause redundancy and therefore the decomposition of this relation into two bcnf relations is not helpful or necessary on the other hand if members of staff conduct numerous interviews per day then the pres ence of in the clientlnterview relation will cause redundancy and normalization of this relation to bcnf is recommended however we should also consider the significance of losing in other words does convey important information about client interviews that must be represented in one of the resulting relations the answer to this question will help to determine whether it is better to retain all functional dependencies or remove data redundancy review of normalization up to bcnf the purpose of this section is to review the process of normalization described in the previous chapter and in section we demonstrate the process of transform ing attributes displayed on a sample report from the dreamhome case study into a set of bcnf relations in this worked example we use the definitions of and that are based on the primary key of a relation we leave the normalization of this worked example using the general definitions of and as an exercise for the reader example first normal form to boyce codd normal form bcnf in this example we extend the dreamhome case study to include property inspection by members of staff when staff are required to undertake these inspections they are allo cated a company car for use on the day of the inspections however a car may be allocated to several members of staff as required throughout the working day a member of staff may inspect several properties on a given date but a property is inspected only once on a given date examples of the dreamhome property inspection report are presented in figure the report on top describes staff inspections of property in glasgow first normal form we first transfer sample data held on two property inspection reports into table format with rows and columns this is referred to as the staffpropertylnspection unnormalized table and is shown in figure we identify the key attribute for this unnormalized table as propertyno we identify the repeating group in the unnormalized table as the property inspection and staff details which repeats for each property the structure of the repeating group is repeating group idate itime comments staffno sname carreg figure dreamhome property inspection reports figure staffpropertylnspection unnormalized table as a consequence there are multiple values at the intersection of certain rows and columns for example for propertyno there are three values for idate oct apr l oct we transform the unnormalized form to first normal form using the first approach described in section with this approach we remove the repeating group property inspection and staff details by entering the appropriate property details nonrepeating data into each row the resulting first normal form staffpropertylnspection relation is shown in figure in figure we present the functional dependencies to for the staffpropertylnspection relation we use the functional dependencies as discussed in section to identify candidate keys for the staffpropertylnspection relation as being composite keys comprising propertyno idate staffno idate itime and carreg idate itime we select propertyno idate as the primary key for this relation for clarity we figure the first normal form staffpropertylnspection relation place the attributes that make up the primary key together at the left hand side of the relation the staffpropertylnspection relation is defined as follows staffpropertylnspection propertyno idate itime paddress comments staffno sname carreg the staffpropertylinspection relation is in as there is a single value at the intersec tion of each row and column the relation contains data describing the inspection of property by members of staff with the property and staff details repeated several times as a result the staffpropertylnspection relation contains significant redundancy if imple mented this inf relation would be subject to update anomalies to remove some of these we must transform the relation into second normal form second normal form the normalization of inf relations to involves the removal of partial dependencies on the primary key if a partial dependency exists we remove the functionally depend ent attributes from the relation by placing them in a new relation with a copy of their determinant figure functional dependencies of the staffpropertylnspection relation as shown in figure the functional dependencies fdl to of the staffpropertylnspection relation are as follows propertyno idate itime comments staffno sname carreg primary key propertyno paddress partial dependency staffno sname transitive dependency staffno idate carreg carreg idate itime propertyno paddress comments staffno sname candidate key staffno idate itime propertyno paddress comments candidate key using the functional dependencies we continue the process of normalizing the staffpropertylnspection relation we begin by testing whether the relation is in by identifying the presence of any partial dependencies on the primary key we note that the property attribute paddress is partially dependent on part of the primary key namely the propertyno represented as whereas the remaining attributes itime comments staffno sname and carreg are fully dependent on the whole primary key propertyno and idate represented as note that although the determinant of the functional dependency staffno idate carreg represented as requires only the idate attribute of the primary key we do not remove this dependency at this stage as the determinant also includes another non primary key attribute namely staffno in other words this dependency is not wholly dependent on part of the primary key and therefore does not violate the identification of the partial dependency propertyno paddress indicates that the staffpropertylnspection relation is not in to transform the relation into requires the creation of new relations so that the attributes that are not fully dependent on the primary key are associated with only the appropriate part of the key the staffpropertylnspection relation is transformed into second normal form by remov ing the partial dependency from the relation and creating two new relations called property and propertylnspection with the following form property propertyno paddress propertylnspection propertyno idate itime comments staffno sname carreg these relations are in as every non primary key attribute is functionally depend ent on the primary key of the relation third normal form the normalization of relations to involves the removal of transitive dependen cies if a transitive dependency exists we remove the transitively dependent attributes from the relation by placing them in a new relation along with a copy of their determi nant the functional dependencies within the property and propertylinspection relations are as follows property relation propertyno paddress propertylnspection relation propertyno idate itime comments staffno sname carreg staffno sname staffno idate carreg carreg idate itime propertyno comments staffno sname staffno idate itime propertyno comments as the property relation does not have transitive dependencies on the primary key it is therefore already in however although all the non primary key attributes within the propertylnspection relation are functionally dependent on the primary key sname is also transitively dependent on staffno represented as we also note the functional depend ency staffno idate carreg represented as has a non primary key attribute carreg partially dependent on a non primary key attribute staffno we do not remove this depend ency at this stage as part of the determinant for this dependency includes a primary key attribute idate in other words this dependency is not wholly transitively dependent on non primary key attributes and therefore does not violate in other words as described in section when considering all candidate keys of a relation the staffno idate carreg dependency is allowed in because carreg is a primary key attribute as it is part of the candidate key carreg idate itime of the original propertylnspection relation to transform the propertylnspection relation into we remove the transitive depend ency staffno sname by creating two new relations called staff and propertylnspect with the form staff staffno sname propertylnspect propertyno idate itime comments staffno carreg the staff and propertyinspect relations are in as no non primary key attribute is wholly functionally dependent on another non primary key attribute thus the staffpropertylnspection relation shown in figure has been transformed by the process of normalization into three relations in with the following form property propertyno paddress staff staffno sname propertylnspect propertyno idate itime comments staffno carreg boyce codd normal form bcnf we now examine the property staff and propertylnspect relations to determine whether they are in bcnf recall that a relation is in bcnf if every determinant of a relation is a candidate key therefore to test for bcnf we simply identify all the determinants and make sure they are candidate keys the functional dependencies for the property staff and propertylnspect relations are as follows property relation propertyno paddress staff relation staffno sname propertylnspect relation propertyno idate itime comments staffno carreg staffno idate carreg carreg idate itime propertyno comments staffno staffno idate itime propertyno comments we can see that the property and staff relations are already in bcnf as the determinant in each of these relations is also the candidate key the only relation that is not in bcnf is propertylnspect because of the presence of the determinant staffno idate which is not a candidate key represented as as a consequence the propertylnspect relation may suffer from update anomalies for example to change the car allocated to staff number on the apr we must update two tuples if only one tuple is updated with the new car registration number this results in an inconsistent state for the database fourth normal form to transform the propertylnspect relation into bcnf we must remove the dependency that violates bcnf by creating two new relations called staffcar and inspection with the form staffcar staffno idate carreg inspection propertyno idate itime comments staffno the staffcar and inspection relations are in bcnf as the determinant in each of these relations is also a candidate key in summary the decomposition of the staffpropertylnspection relation shown in fig ure into bcnf relations is shown in figure in this example the decomposition of the original staffpropertylnspection relation to bcnf relations has resulted in the loss of the functional dependency carreg idate itime propertyno paddress comments staffno sname as parts of the determinant are in different relations represented as however we recognize that if the functional dependency staffno idate carreg rep resented as is not removed the propertylnspect relation will have data redundancy figure decomposition of the staffpropertylnspection relation into bcnf relations the resulting bcnf relations have the following form property propertyno paddress staff staffno sname inspection propertyno idate itime comments staffno staffcar staffno idate carreg the original staffpropertylnspection relation shown in figure can be recreated from the property staff inspection and staffcar relations using the primary key foreign key mechanism for example the attribute staffno is a primary key within the staff relation and is also present within the inspection relation as a foreign key the foreign key allows the association of the staff and inspection relations to identify the name of the member of staff undertaking the property inspection fourth normal form although bcnf removes any anomalies due to functional dependencies further research led to the identification of another type of dependency called a multi valued dependency mvd which can also cause data redundancy fagin in this section we briefly describe a multi valued dependency and the association of this type of dependency with fourth normal form figure a the branchstaffowner relation multi valued dependency the possible existence of multi valued dependencies in a relation is due to which disallows an attribute in a tuple from having a set of values for example if we have two multi valued attributes in a relation we have to repeat each value of one of the attributes with every value of the other attribute to ensure that tuples of the relation are consistent this type of constraint is referred to as a multi valued dependency and results in data redundancy consider the branchstaffowner relation shown in figure which displays the names of members of staff sname and property owners oname at each branch office branchno in this example assume that staff name sname uniquely identifies each member of staff and that the owner name oname uniquely identifies each owner in this example members of staff called ann beech and david ford work at branch and property owners called carol parrel and tina murphy are regis tered at branch however as there is no direct relationship between members of staff and property owners at a given branch office we must create a tuple for every combination of member of staff and owner to ensure that the relation is consistent represents a dependency between attributes for example a b and c in a relation such that for each value of a there is a set of values for b and a set of values for c however the set of values for b and c are independent of each other we represent a mvd between attributes a b and c in a relation using the fol lowing notation a b a c a multi valued dependency constraint potentially exists in the branchstaffowner relation because two independent relationships are represented in the same relation we specify the mvd constraint in the branchstaffowner relation shown in figure a as follows branchno sname branchno oname a multi valued dependency can be further defined as being trivial or nontrivial a mvd a b in relation r is defined as being trivial if a b is a subset of a or b a b r a mvd is defined as being nontrivial if neither a nor b are satisfied a trivial mvd does not specify a constraint on a relation a nontrivial mvd does specify a constraint figure b the branchstaff and branchowner relations the branchstaffowner relation shown in figure a contains two nontrivial dependencies that is branchno sname and branchno oname with branchno not a candidate key of the relation the branchstaffowner relation is therefore constrained by the nontrivial mvds to repeat rows to ensure that the relation remains consistent in terms of the relationship between the sname and oname attributes for example if we wanted to add a new property owner for branch we would have to create two new tuples one for each member of staff to ensure that the relation remains consistent this is an example of an update anomaly caused by the presence of the nontrivial mvds we therefore clearly require a normal form that prevents relational structures such as the branchstaffowner relation definition of fourth normal form a relation is in if and only if for every nontrivial multi valued dependency a b a is a candidate key of the relation fourth normal form prevents a relation from containing a nontrivial mvd without the associated determinant being a candidate key for the relation fagin when the rule is violated the potential for data redundancy exists as shown previously in figure a the normalization of a relation breaking the rule requires the removal of the offending mvd from the relation by placing the multi valued attribute in a new relation along with a copy of the determinant for example the branchstaffowner relation in figure a is not in because of the presence of two nontrivial mvds we decompose the branchstaffowner relation into the branchstaff and branchowner relations as shown in figure b both new relations are in because the branchstaff relation contains the trivial mvd branchno sname and the branchowner relation contains the trivial mvd branchno oname note that the relations do not display data redundancy and the potential for update anomalies is removed for example to add a new property owner for branch we simply create a single tuple in the branchowner relation for a detailed discussion on the interested reader is referred to date and elmasri and navathe fifth normal form whenever we decompose a relation into two relations the resulting relations have the lossless join property this property refers to the ability to rejoin the resulting relations to produce the original relation however there are cases in which there is a requirement to decompose a relation into more than two relations although rare these cases are managed by join dependency and fifth normal form in this section we briefly describe the lossless join dependency and the association with lossless join dependency a property of decomposition that ensures that no spurious tuples are generated when relations are reunited through a natural join operation in splitting relations by projection we are very explicit about the method of decom position in particular we are careful to use projections that can be reversed by joining the resulting relations so that the original relation is reconstructed such a decomposition is called a lossless join also called a nonloss or nonadditive join decomposition because it preserves all the data in the original relation and does not result in the creation of additional spurious tuples for example figures a and b show that the decomposition of the branchstaffowner relation into the branchstaff and branchowner relations has the lossless join property in other words the original branchstaffowner relation can be reconstructed by performing a natural join operation on the branchstaff and branchowner relations in this exam ple the original relation is decomposed into two relations however there are cases were we require to perform a lossless join decompose of a relation into more than two relations aho et al these cases are the focus of the lossless join dependency and definition of fifth normal form a relation is in if and only if for every join dependency rn in a relation r each projection includes a candidate key of the original relation fifth normal form prevents a relation from containing a nontrivial join dependency jd without the associated projection including a candidate key of the original relation fagin nontrivial jds that are not associated with candi date keys are very rare so relations are normally also in although rare let us examine what potential problems exist for a relation that breaks the rules of the propertyitemsupplier relation shown in figure a is not in as it contains a nontrivial join dependency constraint this relation describes properties propertyno that require certain items itemdescription which are supplied by suppliers supplierno to the properties propertyno furthermore whenever a property p requires a certain item i and a supplier supplies that item i and the supplier already supplies at least one item to that property p then the supplier will also supply the required item i to property p in this example assume that a description of an item itemdescription uniquely identifies each type of item figure a illegal state for propertyltem supplier relation and b legal state for propertyltem supplier relation to identify the type of constraint on the propertyltemsupplier relation in figure a consider the following statement if property requires bed from data in tuple supplier supplies property from data in tuple supplier provides bed from data in tuple then supplier provides bed for property this example illustrates the cyclical nature of the constraint on the propertyltemsupplier relation if this constraint holds then the tuple bed must exist in any legal state of the propertyltemsupplier relation as shown in figure b this is an example of a type of update anomaly and we say that this relation contains a non trivial join dependency jd constraint describes a type of dependency for example for a relation r with subsets of the attributes of r denoted as a b z a relation r satisfies a join dependency if and only if every legal value of r is equal to the join of its projections on a b z as the propertyltemsupplier relation contains a join dependency it is therefore not in to remove the join dependency we decompose the propertyltemsupplier relation into three relations namely propertyltem itemsupplier and propertysupplier relations as shown in figure we say that the propertyltemsupplier relation with the form a b c satisfies the join dependency jd a b b c a c it is important to note that performing a natural join on any two relations will pro duce spurious tuples however performing the join on all three will recreate the original propertyltemsupplier relation for a detailed discussion on the interested reader is referred to date and elmasri and navathe figure propertyltem itemsupplier and propertysupplier relations chapter summary inference rules can be used to identify the set of all functional dependencies associated with a relation this set of dependencies can be very large for a given relation inference rules called armstrong axioms can be used to identify a minimal set of functional dependencies from the set of all functional dependencies for a relation a relation is in boyce codd normal form bcnf if and only if every determinant is a candidate key a relation is in fourth normal form if and only if for every nontrivial multi valued dependency a b a is a candidate key of the relation a multi valued dependency mvd represents a dependency between attributes a b and c in a relation such that for each value of a there is a set of values of b and a set of values for c however the set of values for b and c are independent of each other a lossless join dependency is a property of decomposition which means that no spurious tuples are gener ated when relations are combined through a natural join operation a relation is in fifth normal form if and only if for every join dependency rn in a relation r each projection includes a candidate key of the original relation review questions what is the overall purpose of using inference rules and armstrong axioms while developing a database what conditions would make a relation violate boyce codd normal form bcnf discuss the purpose of boyce codd normal form bcnf and discuss how bcnf differs from provide an example to illustrate your answer describe the concept of multi valued dependency and discuss how this concept relates to provide an example to illustrate your answer describe the concept of join dependency and discuss how this concept relates to provide an example to illustrate your answer exercises on completion of exercise examine the relations created to represent the attributes shown in the wellmeadows hospital form shown in figure determine whether these relations are also in bcnf if not transform the relations that do not conform into bcnf on completion of exercise examine the relations created to represent the attributes shown in the relation that displays dentist patient appointment data in figure determine whether these relations are also in bcnf if not transform the relations that do not conform into bcnf bcnf is a stronger form of thus every relation in bcnf is also in however a relation in is not necessarily in bcnf using example to emphasize your case describe the situations when a relation is in bcnf and when it is not the relation shown in figure lists the students studentname enrolled in a postgraduate program at mzumbe university every student is required to provide information concerning names of sponsor sponsorname at least two referees refereename and three supervisors supervisorname assume that sponsors referees and supervisors are not allowed to be related to more than one student studentname refereename sponsorname supervisorname figure student registrations a describe why the relation shown in figure is not in b the relation shown in figure violates normalization principles what are the problems users are likely to face c describe how you would normalize the relation shown in figure to the relation shown in figure describes hospitals hospitalname that require certain items itemdescrip tion which are supplied by suppliers supplierno to the hospitals hospitalname furthermore whenever a hospital h requires a certain item i and a supplier supplies that item i and the supplier already supplies at least one item to that hospital h then the supplier will also supply the required item i to the hospital h in this example assume that a description of an item itemdescription uniquely identifies each type of item a describe why the relation shown in figure is not in b describe and illustrate the process of normalizing the relation shown in figure to figure the hospitalltemsupplier relation part chapter methodology conceptual database design chapter methodology logical database design for the relational model chapter methodology physical database design for relational databases chapter methodology monitoring and tuning the operational system ch apte r methodology conceptual database design in chapter we described the main stages of the database system development lifecycle one of which is database design this stage starts only after a complete analysis of the enterprise requirements has been undertaken in this chapter and chapters we describe a methodology for the database design stage of the database system development lifecycle for relational databases the methodology is presented as a step by step guide to the three main phases of database design namely conceptual logical and physical design see figure the main aim of each phase is as follows conceptual database design to build the conceptual representation of the database which includes identification of the important entities relationships and attributes logical database design to translate the conceptual representation to the logi cal structure of the database which includes designing the relations physical database design to decide how the logical structure is to be physically implemented as base relations in the target dbms introduction to the database design methodology before presenting the methodology we discuss what a design methodology repre sents and describe the three phases of database design finally we present guide lines for achieving success in database design what is a design methodology a structured approach that uses procedures techniques tools and documentation aids to support and facilitate the process of design introduction to the database design methodology a design methodology consists of phases each containing a number of steps that guide the designer in the techniques appropriate at each stage of the project a design methodology also helps the designer to plan manage control and evalu ate database development projects furthermore it is a structured approach for analyzing and modeling a set of requirements for a database in a standardized and organized manner conceptual logical and physical database design in presenting this database design methodology the design process is divided into three main phases conceptual logical and physical database design the process of constructing a model of the data used in an enterprise independent of all physical considerations the conceptual database design phase begins with the creation of a conceptual data model of the enterprise that is entirely independent of implementation details such as the target dbms application programs programming languages hard ware platform performance issues or any other physical considerations the process of constructing a model of the data used in an enterprise based on a specific data model but independent of a particular dbms and other physical considerations the logical database design phase maps the conceptual data model on to a logi cal model which is influenced by the data model for the target database for exam ple the relational model the logical data model is a source of information for the physical design phase providing the physical database designer with a vehicle for making trade offs that are very important to the design of an efficient database the process of producing a description of the implementation of the database on secondary storage it describes the base relations file organizations and indexes used to achieve efficient access to the data and any associated integrity constraints and security measures the physical database design phase allows the designer to make decisions on how the database is to be implemented therefore physical design is tailored to a specific dbms there is feedback between physical and logical design because decisions taken during physical design for improving performance may affect the logical data model critical success factors in database design the following guidelines are often critical to the success of database design work interactively with the users as much as possible follow a structured methodology throughout the data modeling process employ a data driven approach incorporate structural and integrity considerations into the data models combine conceptualization normalization and transaction validation techniques into the data modeling methodology use diagrams to represent as much of the data models as possible use a database design language dbdl to represent additional data semantics that cannot easily be represented in a diagram build a data dictionary to supplement the data model diagrams and the dbdl be willing to repeat steps these factors are built into the methodology we present for database design overview of the database design methodology in this section we present an overview of the database design methodology the steps in the methodology are as follows conceptual database design step build conceptual data model step identify entity types step identify relationship types step identify and associate attributes with entity or relationship types step determine attribute domains step ldetermine candidate primary and alternate key attributes step consider use of enhanced modeling concepts optional step step check model for redundancy step validate conceptual data model against user transactions step review conceptual data model with user logical database design for the relational model step build logical data model step derive relations for logical data model step validate relations using normalization step validate relations against user transactions step check integrity constraints step review logical data model with user step merge logical data models into global model optional step step check for future growth physical database design for relational databases step translate logical data model for target dbms step design base relations overview of the database design methodology step design representation of derived data step design general constraints step design file organizations and indexes step analyze transactions step choose file organizations step choose indexes step estimate disk space requirements step design user views step design security mechanisms step consider the introduction of controlled redundancy step monitor and tune the operational system this methodology can be used to design relatively simple to highly complex database systems just as the database design stage of the database systems devel opment lifecycle see section has three phases conceptual logical and physical design so too has the methodology step creates a conceptual database design step creates a logical database design and steps to create a physical database design depending on the complexity of the database system being built some of the steps may be omitted for example step of the methodology is not required for database systems with a single user view or database systems with mul tiple user views being managed using the centralized approach see section for this reason we refer to the creation of a single conceptual data model only in step and single logical data model only in step however if the database designer is using the view integration approach see section to manage user views for a database system then steps and may be repeated as necessary to create the required number of models which are then merged in step in chapter we introduced the term local conceptual data model or local logical data model to refer to the modeling of one or more but not all user views of a database system and the term global logical data model to refer to the modeling of all user views of a database system however the methodology is presented using the more general terms conceptual data model and logical data model with the exception of the optional step which necessitates the use of the terms local logical data model and global logical data model as it is this step that describes the tasks necessary to merge separate local logical data models to produce a global logical data model an important aspect of any design methodology is to ensure that the models produced are repeatedly validated so that they continue to be an accurate repre sentation of the part of the enterprise being modeled in this methodology the data models are validated in various ways such as by using normalization step by ensuring the critical transactions are supported steps and and by involving the users as much as possible steps and the logical model created at the end of step is then used as the source of information for physical database design described in steps to again depend ing on the complexity of the database systems being designed and or the func tionality of the target dbms some of the steps of physical database design may be omitted for example step may not be applicable for certain pc based dbmss the steps of physical database design are described in detail in chapters and database design is an iterative process that has a starting point and an almost endless procession of refinements although the steps of the methodology are presented here as a procedural process it must be emphasized that this does not imply that it should be performed in this manner it is likely that knowledge gained in one step may alter decisions made in a previous step similarly it may be useful to look briefly at a later step to help with an earlier step therefore the methodology should act as a framework to help guide the designer through database design effectively to illustrate the database design methodology we use the dreamhome case study the dreamhome database has several user views director manager supervisor assistant and client that are managed using a combination of the centralized and view integration approaches see section applying the centralized approach resulted in the identification of two collections of user views called staffclient user views and branch user views the user views represented by each collection are as follows staffclient user views representing supervisor assistant and client user views branch user views representing director and manager user views in this chapter which describes step of the methodology we use the staffclient user views to illustrate the building of a conceptual data model and then in the following chapter which describes step we describe how this model is translated into a logical data model as the staffclient user views represent only a subset of all the user views of the dreamhome database it is more correct to refer to the data models as local data models however as stated earlier when we described the methodology and the worked examples for simplicity we use the terms conceptual data model and logical data model until the optional step which describes the integration of the local logical data models for the staffclient user views and the branch user views conceptual database design methodology this section provides a step by step guide for conceptual database design step build conceptual data model to build a conceptual data model of the data requirements of the enterprise the first step in conceptual database design is to build one or more conceptual data models of the data requirements of the enterprise a conceptual data model comprises entity types relationship types attributes and attribute domains primary keys and alternate keys integrity constraints the conceptual data model is supported by documentation including er dia grams and a data dictionary which is produced throughout the development of the model we detail the types of supporting documentation that may be produced as we go through the various steps the tasks involved in step are step identify entity types step identify relationship types step identify and associate attributes with entity or relationship types step determine attribute domains step determine candidate primary and alternate key attributes step consider use of enhanced modeling concepts optional step step check model for redundancy step validate conceptual data model against user transactions step review conceptual data model with user step identify entity types to identify the required entity types the first step in building a conceptual data model is to determine and define the main objects that the users are interested in these objects are the entity types for the model see section one method of identifying entities is to examine the users requirements specification from this specification we identify nouns or noun phrases that are mentioned for example staff number staff name property number property address rent number of rooms we also look for major objects such as people places or concepts of interest excluding those nouns that are merely qualities of other objects for example we could group staff number and staff name with an object or entity called staff and group property number property address rent and number of rooms with an entity called propertyforrent an alternative way of identifying entities is to look for objects that have an exist ence in their own right for example staff is an entity because staff exist whether or not we know their names positions and dates of birth if possible the users should assist with this activity it is sometimes difficult to identify entities because of the way they are presented in the users requirements specification users often talk in terms of examples or analogies instead of talking about staff in general users may mention people names in some cases users talk in terms of job roles particularly when people or organizations are involved these roles may be job titles or responsibilities such as director manager supervisor or assistant to confuse matters further users frequently use synonyms and homonyms two words are synonyms when they have the same meaning for example branch and office homonyms occur when the same word can have different meanings depend ing on the context for example the word program has several alternative mean ings such as a course of study a series of events a plan of work and an item on the television it is not always obvious whether a particular object is an entity a relationship or an attribute for example how would we classify marriage in fact depending on the actual requirements we could classify marriage as any or all of these design is subjective and different designers may produce different but equally valid interpretations the activity therefore relies to a certain extent on judgement and experience database designers must take a very selective view of the world and categorize the things that they observe within the context of the enterprise thus there may be no unique set of entity types deducible from a given requirements specification however successive iterations of the design process should lead to the choice of entities that are at least adequate for the system required for the staffclient user views of dreamhome we identify the following entities staff propertyforrent privateowner businessowner client preference lease document entity types as entity types are identified assign them names that are meaningful and obvious to the user record the names and descriptions of entities in a data dictionary if possible document the expected number of occurrences of each entity if an entity is known by different names the names are referred to as synonyms or aliases which are also recorded in the data dictionary figure shows an extract from the data dictionary that documents the entities for the staffclient user views of dreamhome step identify relationship types to identify the important relationships that exist between the entity types having identified the entities the next step is to identify all the relationships that exist between these entities see section when we identify entities one method figure extract from the data dictionary for the staffclient user views of dreamhome showing a description of entities is to look for nouns in the users requirements specification again we can use the grammar of the requirements specification to identify relationships typically rela tionships are indicated by verbs or verbal expressions for example staff manages propertyforrent privateowner owns propertyforrent propertyforrent associatedwith lease the fact that the requirements specification records these relationships suggests that they are important to the enterprise and should be included in the model we are interested only in required relationships between entities in the previ ous examples we identified the staff manages propertyforrent and the privateowner owns propertyforrent relationships we may also be inclined to include a relation ship between staff and privateowner for example staff assists privateowner however although this is a possible relationship from the requirements specification it is not a relationship that we are interested in modeling in most instances the relationships are binary in other words the relation ships exist between exactly two entity types however we should be careful to look out for complex relationships that may involve more than two entity types see section and recursive relationships that involve only one entity type see section great care must be taken to ensure that all the relationships that are either explicit or implicit in the users requirements specification are detected in princi ple it should be possible to check each pair of entity types for a potential relation ship between them but this would be a daunting task for a large system comprising hundreds of entity types on the other hand it is unwise not to perform some such check and the responsibility is often left to the analyst designer however miss ing relationships should become apparent when we validate the model against the transactions that are to be supported step use entity relationship er diagrams it is often easier to visualize a complex system rather than decipher long textual descriptions of a users requirements specification we use er diagrams to rep resent entities and how they relate to one another more easily throughout the database design phase we recommend that er diagrams be used whenever neces sary to help build up a picture of the part of the enterprise that we are modeling in this book we use uml but other notations perform a similar function see appendix c determine the multiplicity constraints of relationship types having identified the relationships to model we next determine the multiplicity of each relationship see section if specific values for the multiplicity are known or even upper or lower limits document these values as well multiplicity constraints are used to check and maintain data quality these constraints are assertions about entity occurrences that can be applied when the database is updated to determine whether the updates violate the stated rules of the enterprise a model that includes multiplicity constraints more explicitly represents figure first cut er diagram showing entity and relationship types for the staffclient user views of dreamhome the semantics of the relationships and results in a better representation of the data requirements of the enterprise check for fan and chasm traps having identified the necessary relationships check that each relationship in the er model is a true representation of the real world and that fan or chasm traps have not been created inadvertently see section figure shows the first cut er diagram for the staffclient user views of the dreamhome case study document relationship types as relationship types are identified assign them names that are meaningful and obvious to the user also record relationship descriptions and the multiplicity con straints in the data dictionary figure shows an extract from the data diction ary that documents the relationships for the staffclient user views of dreamhome step identify and associate attributes with entity or relationship types to associate attributes with appropriate entity or relationship types the next step in the methodology is to identify the types of facts about the entities and relationships that we have chosen to be represented in the database in a figure extract from the data dictionary for the staffclient user views of dreamhome showing a description of relationships similar way to identifying entities we look for nouns or noun phrases in the users requirements specification the attributes can be identified where the noun or noun phrase is a property quality identifier or characteristic of one of these enti ties or relationships see section by far the easiest thing to do when we have identified an entity x or a relation ship y in the requirements specification is to ask what information are we required to hold on x or y the answer to this question should be described in the specification however in some cases it may be necessary to ask the users to clarify the require ments unfortunately they may give answers to this question that also contain other concepts so the users responses must be carefully considered simple composite attributes it is important to note whether an attribute is simple or composite see section composite attributes are made up of simple attributes for example the address attribute can be simple and hold all the details of an address as a single value such as dumbarton road glasgow however the address attribute may also represent a composite attribute made up of simple attributes that hold the address details as separate values in the attributes street dumbarton road city glasgow and postcode the option to represent address details as a simple or composite attribute is determined by the users requirements if the user does not need to access the separate components of an address we represent the address attribute as a simple attribute on the other hand if the user does need to access the individual components of an address we represent the address attribute as being composite made up of the required simple attributes in this step it is important that we identify all simple attributes to be repre sented in the conceptual data model including those attributes that make up a composite attribute single multi valued attributes in addition to being simple or composite an attribute can also be single valued or multi valued see section most attributes encountered will be single valued but occasionally a multi valued attribute may be encountered that is an attribute that holds multiple values for a single entity occurrence for example we may identify the attribute telno the telephone number of the client entity as a multi valued attribute on the other hand client telephone numbers may have been identified as a sepa rate entity from client this is an alternative and equally valid way to model this as you will see in step multi valued attributes are mapped to relations anyway so both approaches produce the same end result derived attributes attributes whose values are based on the values of other attributes are known as derived attributes see section examples of derived attributes include the age of a member of staff the number of properties that a member of staff manages the rental deposit calculated as twice the monthly rent often these attributes are not represented in the conceptual data model however sometimes the value of the attribute or attributes on which the derived attribute is based may be deleted or modified in this case the derived attribute must be shown in the data model to avoid this potential loss of information however if a derived attribute is shown in the model we must indicate that it is derived the representation of derived attributes will be considered during physical database design depending on how an attribute is used new values for a derived attribute may be calculated each time it is accessed or when the value it is derived from changes however this issue is not the concern of conceptual database design and is discussed in more detail in step in chapter potential problems when identifying the entities relationships and attributes for the user views it is not uncommon for it to become apparent that one or more entities relationships or attributes have been omitted from the original selection in this case return to the previous steps document the new entities relationships or attributes and re examine any associated relationships as there are generally many more attributes than entities and relationships it may be useful to first produce a list of all attributes given in the users requirements specification as an attribute is associated with a particular entity or relationship remove the attribute from the list in this way we ensure that an attribute is associ ated with only one entity or relationship type and when the list is empty that all attributes are associated with some entity or relationship type we must also be aware of cases where attributes appear to be associated with more than one entity or relationship type as this can indicate the following we have identified several entities that can be represented as a single entity for example we may have identified entities assistant and supervisor both with the attributes staffno the staff number name sex and dob date of birth which can be represented as a single entity called staff with the attributes staffno the staff number name sex dob and position with values assistant or supervisor on the other hand it may be that these entities share many attributes but there are also attributes or relationships that are unique to each entity in this case we must decide whether we want to generalize the entities into a single entity such as staff or leave them as specialized entities representing distinct staff roles the consideration of whether to specialize or generalize entities was dis cussed in chapter and is addressed in more detail in step we have identified a relationship between entity types in this case we must associate the attribute with only one entity the parent entity and ensure that the relationship was previously identified in step if this is not the case the documentation should be updated with details of the newly identified relation ship for example we may have identified the entities staff and propertyforrent with the following attributes staff staffno name position sex dob propertyforrent propertyno street city postcode type rooms rent managername the presence of the managername attribute in propertyforrent is intended to rep resent the relationship staff manages propertyforrent in this case the managername attribute should be omitted from propertyforrent and the relationship manages should be added to the model dreamhome attributes for entities for the staffclient user views of dreamhome we identify and associate attributes with entities as follows staff staffno name composite fname iname position sex dob propertyforrent propertyno address composite street city postcode type rooms rent privateowner ownerno name composite fname iname address telno businessowner ownerno bname btype address telno contactname client clientno name composite fname iname telno email preference preftype maxrent lease leaseno paymentmethod deposit derived as propertyforrent rent depositpaid rentstart rentfinish duration derived as rentfinish rentstart dreamhome attributes for relationships some attributes should not be associated with entities but instead should be asso ciated with relationships for the staffclient user views of dreamhome we identify and associate attributes with relationships as follows views viewdate comment document attributes as attributes are identified assign them names that are meaningful to the user record the following information for each attribute attribute name and description data type and length any aliases that the attribute is known by whether the attribute is composite and if so the simple attributes that make up the composite attribute whether the attribute is multi valued figure extract from the data dictionary for the staffclient user views of dreamhome showing a description of attributes whether the attribute is derived and if so how it is to be computed any default value for the attribute figure shows an extract from the data dictionary that documents the attributes for the staffclient user views of dreamhome step determine attribute domains to determine domains for the attributes in the conceptual data model the objective of this step is to determine domains for all the attributes in the model see section a domain is a pool of values from which one or more attributes draw their values for example we may define the attribute domain of valid staff numbers staffno as being a five character variable length string with the first two characters as letters and the next one to three characters as digits in the range the possible values for the sex attribute of the staff entity as being either m or f the domain of this attribute is a single character string consisting of the val ues m or f a fully developed data model specifies the domains for each attribute and includes allowable set of values for the attribute sizes and formats of the attribute further information can be specified for a domain such as the allowable operations on an attribute and which attributes can be compared with other attributes or used in combination with other attributes however implement ing these characteristics of attribute domains in a dbms is still the subject of research document attribute domains as attribute domains are identified record their names and characteristics in the data dictionary update the data dictionary entries for attributes to record their domain in place of the data type and length information step determine candidate primary and alternate key attributes to identify the candidate key for each entity type and if there is more than one candidate key to choose one to be the primary key and the others as alternate keys this step is concerned with identifying the candidate key for an entity and then selecting one to be the primary key see section a candidate key is a mini mal set of attributes of an entity that uniquely identifies each occurrence of that entity we may identify more than one candidate key in which case we must choose one to be the primary key the remaining candidate keys are called alternate keys people names generally do not make good candidate keys for example we may think that a suitable candidate key for the staff entity would be the composite attribute name the member of staff name however it is possible for two people with the same name to join dreamhome which would clearly invalidate the choice of name as a candi date key we could make a similar argument for the names of property owners in such cases rather than coming up with combinations of attributes that may provide uniqueness it may be better to use an existing attribute that would always ensure uniqueness such as the staffno attribute for the staff entity and the ownerno attribute for the privateowner entity or define a new attribute that would provide uniqueness when choosing a primary key from among the candidate keys use the following guidelines to help make the selection the candidate key with the minimal set of attributes the candidate key that is least likely to have its values changed the candidate key with fewest characters for those with textual attribute the candidate key with smallest maximum value for those with numerical attribute the candidate key that is easiest to use from the users point of view in the process of identifying primary keys note whether an entity is strong or weak if we are able to assign a primary key to an entity the entity is referred to as being strong on the other hand if we are unable to identify a primary key for an entity the entity is referred to as being weak see section the primary key of a weak entity can be identified only when we map the weak entity and its relation ship with its owner entity to a relation through the placement of a foreign key in that relation the process of mapping entities and their relationships to relations is described in step and therefore the identification of primary keys for weak entities cannot take place until that step dreamhome primary keys the primary keys for the staffclient user views of dreamhome are shown in figure note that the preference entity is a weak entity and as identified previously the views relationship has two attributes viewdate and comment figure er diagram for the staffclient user views of dreamhome with primary keys added document primary and alternate keys record the identification of primary and any alternate keys in the data dictionary step consider use of enhanced modeling concepts optional step to consider the use of enhanced modeling concepts such as specialization generalization aggregation and composition in this step we have the option to continue the development of the er model using the enhanced modeling concepts discussed in chapter namely specialization generalization aggregation and composition if we select the specialization approach we attempt to highlight differences between entities by defining one or more subclasses of a superclass entity if we select the generalization approach we attempt to identify common features between entities to define a generalizing superclass entity we may use aggregation to represent a has a or is part of rela tionship between entity types where one represents the whole and the other the part we may use composition a special type of aggregation to represent an association between entity types where there is a strong ownership and coincidental lifetime between the whole and the part for the staffclient user views of dreamhome we choose to generalize the two entities privateowner and businessowner to create a superclass owner that contains the common attributes ownerno address and telno the relationship that the owner superclass has with its subclasses is mandatory and disjoint denoted as mandatory or each member of the owner superclass must be a member of one of the sub classes but cannot belong to both figure revised er diagram for the staffclient user views of dreamhome with specialization generalization added in addition we identify one specialization subclass of staff namely supervisor specifically to model the supervises relationship the relationship that the staff superclass has with the supervisor subclass is optional a member of the staff super class does not necessarily have to be a member of the supervisor subclass to keep the design simple we decide not to use aggregation or composition the revised er diagram for the staffclient user views of dreamhome is shown in figure there are no strict guidelines on when to develop the er model using enhanced modeling concepts as the choice is often subjective and dependent on the par ticular characteristics of the situation that is being modeled as a useful rule of thumb when considering the use of these concepts always attempt to represent the important entities and their relationships as clearly as possible in the er diagram therefore the use of enhanced modeling concepts should be guided by the read ability of the er diagram and the clarity by which it models the important entities and relationships these concepts are associated with enhanced er modeling however as this step is optional we simply use the term er diagram when referring to the diagram matic representation of data models throughout the methodology step check model for redundancy to check for the presence of any redundancy in the model in this step we examine the conceptual data model with the specific objective of identifying whether there is any redundancy present and removing any that does exist the three activities in this step are re examine one to one relationships remove redundant relationships consider time dimension re examine one to one relationships in the identification of entities we may have identified two entities that represent the same object in the enterprise for example we may have identified the two entities client and renter that are actually the same in other words client is a syno nym for renter in this case the two entities should be merged together if the primary keys are different choose one of them to be the primary key and leave the other as an alternate key remove redundant relationships a relationship is redundant if the same information can be obtained via other rela tionships we are trying to develop a minimal data model and as redundant rela tionships are unnecessary they should be removed it is relatively easy to identify whether there is more than one path between two entities however this does not necessarily imply that one of the relationships is redundant as they may represent different associations between the entities for example consider the relationships between the propertyforrent lease and client entities shown in figure there are two ways to find out which clients rent which properties there is the direct route using the rents relationship between the client and propertyforrent entities and there is the indirect route using the holds and associatedwith relationships via the lease entity before we can assess whether both routes are required we need figure remove the redundant relationship called rents to establish the purpose of each relationship the rents relationship indicates which client rents which property on the other hand the holds relationship indi cates which client holds which lease and the associatedwith relationship indicates which properties are associated with which leases although it is true that there is a relationship between clients and the properties they rent this is not a direct relationship and the association is more accurately represented through a lease the rents relationship is therefore redundant and does not convey any additional information about the relationship between propertyforrent and client that cannot more correctly be found through the lease entity to ensure that we create a mini mal model the redundant rents relationship must be removed consider time dimension the time dimension of relationships is important when assessing redundancy for example consider the situation in which we wish to model the relationships between the entities man woman and child as illustrated in figure clearly there are two paths between man and child one via the direct relationship fatherof and the other via the relationships marriedto and motherof consequently we may think that the relationship fatherof is unnecessary however this would be incorrect for two reasons the father may have children from a previous marriage and we are modeling only the father current marriage through a relationship the father and mother may not be married or the father may be married to someone other than the mother or the mother may be married to someone who is not the father in either case the required relationship could not be modeled without the fatherof relationship the message is that it is important to examine the meaning of each relationship between entities when assessing redundancy at the end of this step we have simplified the local conceptual data model by removing any inherent redundancy step validate conceptual data model against user transactions to ensure that the conceptual data model supports the required transactions figure example of a nonredundant relationship fatherof we now have a conceptual data model that represents the data requirements of the enterprise the objective of this step is to check the model to ensure that the model supports the required transactions using the model we attempt to perform the operations manually if we can resolve all transactions in this way we have checked that the conceptual data model supports the required transactions however if we are unable to perform a transaction manually there must be a problem with the data model which must be resolved in this case it is likely that we have omitted an entity a relationship or an attribute from the data model we examine two possible approaches to ensuring that the conceptual data model supports the required transactions describing the transactions using transaction pathways describing the transaction using the first approach we check that all the information entities relationships and their attributes required by each transaction is provided by the model by documenting a description of each transaction requirements we illustrate this approach for an example dreamhome transaction listed in appendix a from the staffclient user views transaction d list the details of properties managed by a named member of staff at the branch the details of properties are held in the propertyforrent entity and the details of staff who manage properties are held in the staff entity in this case we can use the staff manages propertyforrent relationship to produce the required list using transaction pathways the second approach to validating the data model against the required transac tions involves diagrammatically representing the pathway taken by each transaction directly on the er diagram an example of this approach for the query transac tions for the staffclient user views listed in appendix a is shown in figure clearly the more transactions that exist the more complex this diagram would become so for readability we may need several such diagrams to cover all the transactions this approach allows the designer to visualize areas of the model that are not required by transactions and those areas that are critical to transactions we are therefore in a position to directly review the support provided by the data model for the transactions required if there are areas of the model that do not appear to be used by any transactions we may question the purpose of representing this information in the data model on the other hand if there are areas of the model that are inadequate in providing the correct pathway for a transaction we may need to investigate the possibility that critical entities relationships or attributes have been missed it may look like a lot of hard work to check every transaction that the model has to support in this way and it certainly can be as a result it may be tempting to omit this step however it is very important that these checks are performed now figure using pathways to check whether the conceptual data model supports the user transactions rather than later when it is much more difficult and expensive to resolve any errors in the data model step review conceptual data model with user to review the conceptual data model with the users to ensure that they consider the model to be a true representation of the data requirements of the enterprise before completing step we review the conceptual data model with the user the conceptual data model includes the er diagram and the supporting docu mentation that describes the data model if any anomalies are present in the data model we must make the appropriate changes which may require repeating the previous step we repeat this process until the user is prepared to sign off the model as being a true representation of the part of the enterprise that we are modeling the steps in this methodology are summarized in appendix d the next chapter describes the steps of the logical database design methodology a design methodology is a structured approach that uses procedures techniques tools and documentation aids to support and facilitate the process of design database design includes three main phases conceptual logical and physical database design conceptual database design is the process of constructing a model of the data used in an enterprise inde pendent of all physical considerations conceptual database design begins with the creation of a conceptual data model of the enterprise which is entirely independent of implementation details such as the target dbms application programs programming languages hardware platform performance issues or any other physical considerations logical database design is the process of constructing a model of the data used in an enterprise based on a specific data model such as the relational model but independent of a particular dbms and other physical considerations logical database design translates the conceptual data model into a logical data model of the enterprise physical database design is the process of producing a description of the implementation of the database on secondary storage it describes the base relations file organizations and indexes used to achieve efficient access to the data and any associated integrity constraints and security measures the physical database design phase allows the designer to make decisions on how the database is to be imple mented therefore physical design is tailored to a specific dbms there is feedback between physical and conceptual logical design because decisions taken during physical design to improve performance may affect the structure of the conceptual logical data model there are several critical factors for the success of the database design stage including for example working interactively with users and being willing to repeat steps the main objective of step of the methodology is to build a conceptual data model of the data require ments of the enterprise a conceptual data model comprises entity types relationship types attributes attribute domains primary keys and alternate keys a conceptual data model is supported by documentation such as er diagrams and a data dictionary which is produced throughout the development of the model the conceptual data model is validated to ensure it supports the required transactions two possible approaches to ensure that the conceptual data model supports the required transactions are checking that all the information entities relationships and their attributes required by each transaction is provided by the model by documenting a description of each transaction requirements diagrammatically representing the pathway taken by each transaction directly on the er diagram describe the purpose of a design methodology describe the main phases involved in database design identify important factors in the success of database design discuss the important role played by users in the process of database design describe the main objective of conceptual database design what is the purpose of conceptual database design how is it utilized while designing a database exercises how would you identify entity and relationship types from a user requirements specification how would you identify attributes from a user requirements specification and then associate the attributes with entity or relationship types using enhanced modeling concepts in the conceptual design is optional describe the possible eer concepts that can be presented in a conceptual model how would you check a data model for redundancy give an example to illustrate your answer discuss why you would want to validate a conceptual data model and describe two approaches to validating a conceptual data model identify and describe the purpose of the documentation generated during conceptual database design the dreamhome case study create a conceptual data model for the branch user views of dreamhome documented in appendix a compare your er diagram with figure and justify any differences found analyze the dreamhome case study and examine if there are situations that call for enhanced modeling present the enhanced data model of the case the university accommodation office case study describe how you will approach the task of creating the conceptual model for university accommodation documented in appendix b create a conceptual data model for the case study state any assumptions necessary to support your design check that the conceptual data model supports the required transactions the easydrive school of motoring case study analyze the easydrive school of motoring case study documented in appendix b and prepare a data dictionary detailing all the key conceptual issues create a conceptual data model for the case study state any assumptions necessary to support your design check that the conceptual data model supports the required transactions the wellmeadows hospital case study identify user views for the medical director and charge nurse in the wellmeadows hospital case study described in appendix b provide a users requirements specification for each of these user views create conceptual data models for each of the user views state any assumptions necessary to support your design chapter methodology logical database design for the relational model in chapter we described the main stages of the database system development lifecycle one of which is database design this stage is made up of three phases conceptual logical and physical database design in the previous chapter we introduced a methodology that describes the steps that make up the three phases of database design and then presented step of this methodology for conceptual database design in this chapter we describe step of the methodology which translates the con ceptual model produced in step into a logical data model the methodology for logical database design described in this book also includes an optional step which is required when the database has multiple user views that are managed using the view integration approach see section in this case we repeat steps to as necessary to create the required number of local logical data models which are then finally merged in step to form a global logical data model a local logical data model represents the data requirements of one or more but not all user views of a database and a global logical data model represents the data requirements for all user views see section however after concluding step we cease to use the term global logical data model and simply refer to the final model as being a logi cal data model the final step of the logical database design phase is to con sider how well the model is able to support possible future developments for the database system it is the logical data model created in step that forms the starting point for physical database design which is described as steps to in chapters and throughout the methodology the terms entity and relationship are used in place of entity type and relationship type where the meaning is obvious type is generally only added to avoid ambiguity logical database design methodology for the relational model this section describes the steps of the logical database design methodology for the relational model step build logical data model to translate the conceptual data model into a logical data model and then to validate this model to check that it is structurally cor rect and able to support the required transactions in this step the main objective is to translate the conceptual data model created in step into a logical data model of the data requirements of the enterprise this objective is achieved by following these activities step derive relations for logical data model step validate relations using normalization step validate relations against user transactions step check integrity constraints step review logical data model with user step merge logical data models into global data model optional step step check for future growth we begin by deriving a set of relations relational schema from the conceptual data model created in step the structure of the relational schema is validated using normalization and then checked to ensure that the relations are capable of supporting the transactions given in the users requirements specification we next check whether all important integrity constraints are represented by the logical data model at this stage the logical data model is validated by the users to ensure that they consider the model to be a true representation of the data requirements of the enterprise the methodology for step is presented so that it is applicable for the design of simple to complex database systems for example to create a database with a single user view or with multiple user views that are managed using the centralized approach see section then step is omitted if however the database has multiple user views that are being managed using the view integration approach see section then steps to are repeated for the required number of data models each of which represents different user views of the database system in step these data models are merged step concludes with an assessment of the logical data model which may or may not have involved step to ensure that the final model is able to support possible future developments on completion of step we should have a single logical data model that is a correct comprehensive and unambiguous representa tion of the data requirements of the enterprise we demonstrate step using the conceptual data model created in the previous chapter for the staffclient user views of the dreamhome case study and represented in figure as an er diagram we also use the branch user views of dreamhome which is represented in figure as an er diagram to illustrate some concepts that are not present in the staffclient user views and to demonstrate the merging of data models in step figure conceptual data model for the staffclient user views showing all attributes step derive relations for logical data model to create relations for the logical data model to represent the entities relationships and attributes that have been identified in this step we derive relations for the logical data model to represent the entities relationships and attributes we describe the composition of each relation using a dbdl for relational databases using the dbdl we first specify the name of the relation followed by a list of the relation simple attributes enclosed in brackets we then identify the primary key and any alternate and or foreign key of the relation following the identification of a foreign key the relation containing the referenced primary key is given any derived attributes are also listed along with how each one is calculated the relationship that an entity has with another entity is represented by the pri mary key foreign key mechanism in deciding where to post or place the foreign key attribute we must first identify the parent and child entities involved in the relationship the parent entity refers to the entity that posts a copy of its pri mary key into the relation that represents the child entity to act as the foreign key we describe how relations are derived for the following structures that may occur in a conceptual data model strong entity types weak entity types one to many binary relationship types one to one binary relationship types one to one recursive relationship types superclass subclass relationship types many to many binary relationship types complex relationship types multi valued attributes for most of the examples discussed in the following sections we use the concep tual data model for the staffclient user views of dreamhome which is represented as an er diagram in figure strong entity types for each strong entity in the data model create a relation that includes all the sim ple attributes of that entity for composite attributes such as name include only the constituent simple attributes fname and iname in the relation for example the com position of the staff relation shown in figure is staff staffno fname iname position sex dob primary key staffno weak entity types for each weak entity in the data model create a relation that includes all the simple attributes of that entity the primary key of a weak entity is partially or fully derived from each owner entity and so the identification of the primary key of a weak entity cannot be made until after all the relationships with the owner entities have been mapped for example the weak entity preference in figure is initially mapped to the following relation preference preftype maxrent primary key none at present in this situation the primary key for the preference relation cannot be identified until after the states relationship has been appropriately mapped one to many binary relationship types for each binary relationship the entity on the one side of the relationship is designated as the parent entity and the entity on the many side is designated as the child entity to represent this relationship we post a copy of the primary key attribute of the parent entity into the relation representing the child entity to act as a foreign key for example the staff registers client relationship shown in figure is a relationship as a single member of staff can register many clients in this example staff is on the one side and represents the parent entity and client is on the many side and represents the child entity the relationship between these entities is established by placing a copy of the primary key of the staff parent entity staffno into the client child relation the composition of the staff and client relations is in the case where a relationship has one or more attributes these attributes should follow the posting of the primary key to the child relation for example if the staff registers client relationship had an attribute called dateregister representing when a member of staff registered the client this attribute should also be posted to the client relation along with the copy of the primary key of the staff relation namely staffno one to one binary relationship types creating relations to represent a relationship is slightly more complex as the cardinality cannot be used to help identify the parent and child entities in a rela tionship instead the participation constraints see section are used to help decide whether it is best to represent the relationship by combining the entities involved into one relation or by creating two relations and posting a copy of the primary key from one relation to the other we consider how to create relations to represent the following participation constraints a mandatory participation on both sides of relationship b mandatory participation on one side of relationship c optional participation on both sides of relationship a mandatory participation on both sides of relationship in this case we should combine the entities involved into one relation and choose one of the pri mary keys of the original entities to be the primary key of the new relation while the other if one exists is used as an alternate key the client states preference relationship is an example of a relationship with mandatory participation on both sides in this case we choose to merge the two relations together to give the following client relation client clientno fname iname telno email preftype maxrent staffno primary key clientno alternate key email foreign key staffno references staff staffno in the case where a relationship with mandatory participation on both sides has one or more attributes these attributes should also be included in the merged relation for example if the states relationship had an attribute called datestated recording the date the preferences were stated this attribute would also appear as an attribute in the merged client relation note that it is possible to merge two entities into one relation only when there are no other direct relationships between these two entities that would prevent this such as a relationship if this were the case we would need to represent the states relationship using the primary key foreign key mechanism we discuss how to designate the parent and child entities in this type of situation in part c shortly b mandatory participation on one side of a relationship in this case we are able to identify the parent and child entities for the relationship using the par ticipation constraints the entity that has optional participation in the relationship is designated as the parent entity and the entity that has mandatory participation in the relationship is designated as the child entity as described previously a copy of the primary key of the parent entity is placed in the relation representing the child entity if the relationship has one or more attributes these attributes should follow the posting of the primary key to the child relation for example if the client states preference relationship had partial participa tion on the client side in other words not every client specifies preferences then the client entity would be designated as the parent entity and the preference entity would be designated as the child entity therefore a copy of the primary key of the client parent entity clientno would be placed in the preference child relation giving note that the foreign key attribute of the preference relation also forms the relation primary key in this situation the primary key for the preference relation could not have been identified until after the foreign key had been posted from the client rela tion to the preference relation therefore at the end of this step we should identify any new primary key or candidate keys that have been formed in the process and update the data dictionary accordingly c optional participation on both sides of a relationship in this case the designation of the parent and child entities is arbitrary unless we can find out more about the relationship that can help us make a decision one way or the other for example consider how to represent a staff uses car relationship with optional participation on both sides of the relationship note that the discussion that follows is also relevant for relationships with mandatory participation for both entities where we cannot select the option to combine the entities into a single relation if there is no additional information to help select the parent and child entities the choice is arbitrary in other words we have the choice to post a copy of the primary key of the staff entity to the car entity or vice versa however assume that the majority of cars but not all are used by staff and that only a minority of staff use cars the car entity although optional is closer to being mandatory than the staff entity we therefore designate staff as the parent entity and car as the child entity and post a copy of the primary key of the staff entity staffno into the car relation one to one recursive relationships for a recursive relationship follow the rules for participation as described pre viously for a relationship however in this special case of a relationship the entity on both sides of the relationship is the same for a recursive relationship with mandatory participation on both sides represent the recursive relationship as a single relation with two copies of the primary key as before one copy of the primary key represents a foreign key and should be renamed to indicate the rela tionship it represents for a recursive relationship with mandatory participation on only one side we have the option to create a single relation with two copies of the primary key as described previously or to create a new relation to represent the relationship the new relation would have only two attributes both copies of the primary key as before the copies of the primary keys act as foreign keys and have to be renamed to indicate the purpose of each in the relation for a recursive relationship with optional participation on both sides again create a new relation as described earlier superclass subclass relationship types for each superclass subclass relationship in the conceptual data model we iden tify the superclass entity as the parent entity and the subclass entity as the child entity there are various options on how to represent such a relationship as one or more relations the selection of the most appropriate option is dependent on a number of factors such as the disjointness and participation constraints on the table guidelines for the representation of a superclass subclass relationship based on the participation and disjoint constraints participation constraint disjoint constraint relations required mandatory nondisjoint and single relation with one or more discriminators to distinguish the type of each tuple optional nondisjoint and two relations one relation for superclass and one relation for all subclasses with one or more discriminators to distinguish the type of each tuple mandatory disjoint or many relations one relation for each combined superclass subclass optional disjoint or many relations one relation for superclass and one for each subclass superclass subclass relationship see section whether the subclasses are involved in distinct relationships and the number of participants in the superclass subclass relationship guidelines for the representation of a superclass subclass relationship based only on the participation and disjoint constraints are shown in table for example consider the owner superclass subclass relationship shown in figure from table there are various ways to represent this relation ship as one or more relations as shown in figure the options range from placing all the attributes into one relation with two discriminators pownerflag and bownerflag indicating whether a tuple belongs to a particular subclass option to dividing the attributes into three relations option in this case the most appropriate representation of the superclass subclass relationship is determined by the constraints on this relationship from figure the relationship that the owner superclass has with its subclasses is mandatory and disjoint as each member of the owner superclass must be a member of one of the subclasses privateowner or businessowner but cannot belong to both we therefore select option as the best representation of this relationship and create a separate relation to represent each subclass and include a copy of the primary key attribute of the superclass in each it must be stressed that table is for guidance only as there may be other factors that influence the final choice for example with option mandatory nondisjoint we have chosen to use two discriminators to distinguish whether the tuple is a member of a particular subclass an equally valid way to represent this would be to have one discriminator that distinguishes whether the tuple is a mem ber of privateowner businessowner or both alternatively we could dispense with discriminators all together and simply test whether one of the attributes unique to a particular subclass has a value present to determine whether the tuple is a member of that subclass in this case we would have to ensure that the attributes examined allowed nulls to indicate nonmembership of a particular subclass in figure there is another superclass subclass relationship between staff and supervisor with optional participation however as the staff superclass only has one subclass supervisor there is no disjoint constraint in this case as there are many more supervised staff than supervisors we choose to represent this relation ship as a single relation staff staffno fname iname position sex dob supervisorstaffno primary key staffno foreign key supervisorstaffno references staff staffno if we had left the superclass subclass relationship as a recursive relationship as we had it originally in figure with optional participation on both sides this would have resulted in the same representation as previously many to many binary relationship types for each binary relationship create a relation to represent the relationship and include any attributes that are part of the relationship we post a copy of the primary key attribute of the entities that participate in the relationship into the new relation to act as foreign keys one or both of these foreign keys will also form the primary key of the new relation possibly in combination with one or more of the attributes of the relationship if one or more of the attributes that figure various representations of the owner superclass subclass relationship based on the participation and disjointness constraints shown in table form the relationship provide uniqueness then an entity has been omitted from the conceptual data model although this mapping process resolves this issue for example consider the relationship client views propertyforrent shown in figure in this example the views relationship has two attributes called dateview and comments to represent this we create relations for the strong entities client and propertyforrent and we create a relation viewing to represent the relation ship views to give complex relationship types for each complex relationship create a relation to represent the relationship and include any attributes that are part of the relationship we post a copy of the pri mary key attribute of the entities that participate in the complex relationship into the new relation to act as foreign keys any foreign keys that represent a many relationship for example generally will also form the primary key of this new relation possibly in combination with some of the attributes of the relationship for example the ternary registers relationship in the branch user views rep resents the association between the member of staff who registers a new client at a branch as shown in figure to represent this we create relations for the strong entities branch staff and client and we create a relation registration to repre sent the relationship registers to give figure relations for the staffclient user views of dreamhome note that the registers relationship is shown as a binary relationship in figure and this is consistent with its composition in figure the discrepancy between how registers is modeled in the staffclient as a binary relationship and branch as a complex ternary relationship user views of dreamhome is discussed and resolved in step multi valued attributes for each multi valued attribute in an entity create a new relation to represent the multi valued attribute and include the primary key of the entity in the new relation to act as a foreign key unless the multi valued attribute is itself an alternate key of the entity the primary key of the new relation is the combination of the multi valued attribute and the primary key of the entity for example in the branch user views to represent the situation where a single branch has up to three telephone numbers the telno attribute of the branch entity has been defined as being a multi valued attribute as shown in figure to represent this we create a relation for the branch entity and we create a new relation called telephone to represent the multi valued attribute telno to give table summary of how to map entities and relationships to relations entity relationship mapping strong entity create relation that includes all simple attributes weak entity create relation that includes all simple attributes primary key still has to be identified after the relationship with each owner entity has been mapped binary relationship post primary key of entity on the one side to act as foreign key in relation representing entity on the many side any attributes of relationship are also posted to the many side binary relationship a mandatory participation on both sides b mandatory participation on one side c optional participation on both sides combine entities into one relation post primary key of entity on the optional side to act as foreign key in relation representing entity on the mandatory side arbitrary without further information superclass subclass relationship see table binary relationship complex relationship create a relation to represent the relationship and include any attributes of the relationship post a copy of the primary keys from each of the owner entities into the new relation to act as foreign keys multi valued attribute create a relation to represent the multi valued attribute and post a copy of the primary key of the owner entity into the new relation to act as a foreign key table summarizes how to map entities and relationships to relations document relations and foreign key attributes at the end of step document the composition of the relations derived for the logical data model using the dbdl the relations for the staffclient user views of dreamhome are shown in figure now that each relation has its full set of attributes we are in a position to identify any new primary and or alternate keys this is particularly important for weak entities that rely on the posting of the primary key from the parent entity or entities to form a primary key of their own for example the weak entity viewing now has a composite primary key made up of a copy of the primary key of the propertyforrent entity propertyno and a copy of the primary key of the client entity clientno the dbdl syntax can be extended to show integrity constraints on the foreign keys step the data dictionary should also be updated to reflect any new pri mary and alternate keys identified in this step for example following the posting of primary keys the lease relation has gained new alternate keys formed from the attributes propertyno rentstart and clientno rentstart step validate relations using normalization to validate the relations in the logical data model using normali zation in the previous step we derived a set of relations to represent the conceptual data model created in step in this step we validate the groupings of attributes in each relation using the rules of normalization the purpose of normalization is to ensure that the set of relations has a minimal yet sufficient number of attributes necessary to support the data requirements of the enterprise also the relations should have minimal data redundancy to avoid the problems of update anomalies discussed in section however some redundancy is essential to allow the joining of related relations the use of normalization requires that we first identify the functional dependen cies that hold between the attributes in each relation the characteristics of func tional dependencies that are used for normalization were discussed in section and can be identified only if the meaning of each attribute is well understood the functional dependencies indicate important relationships between the attributes of a relation it is those functional dependencies and the primary key for each relation that are used in the process of normalization the process of normalization takes a relation through a series of steps to check whether the composition of attributes in a relation conforms or otherwise with the rules for a given normal form such as and the rules for each normal form were discussed in detail in sections to to avoid the problems associ ated with data redundancy it is recommended that each relation be in at least the process of deriving relations from a conceptual data model should produce relations that are already in if however we identify relations that are not in this may indicate that part of the logical data model and or conceptual data model is incorrect or that we have introduced an error when deriving the relations from the conceptual data model if necessary we must restructure the problem relation and or data model to ensure a true representation of the data require ments of the enterprise it is sometimes argued that a normalized database design does not provide maxi mum processing efficiency however the following points can be argued a normalized design organizes the data according to its functional dependen cies consequently the process lies somewhere between conceptual and physical design the logical design may not be the final design it should represent the database designer best understanding of the nature and meaning of the data required by the enterprise if there are specific performance criteria the physical design may be different one possibility is that some normalized relations are denormalized and this approach is discussed in detail in step of the physical database design methodology see chapter a normalized design is robust and free of the update anomalies discussed in section modern computers are much more powerful than those that were available a few years ago it is sometimes reasonable to implement a design that gains ease of use at the expense of additional processing to use normalization a database designer must understand completely each attribute that is to be represented in the database this benefit may be the most important normalization produces a flexible database design that can be extended easily step validate relations against user transactions to ensure that the relations in the logical data model support the required transactions the objective of this step is to validate the logical data model to ensure that the model supports the required transactions as detailed in the users requirements specification this type of check was carried out in step to ensure that the conceptual data model supported the required transactions in this step we check whether the relations created in the previous step also support these transactions and thereby ensure that no error has been introduced while creating relations using the relations the primary key foreign key links shown in the relations the er diagram and the data dictionary we attempt to perform the operations manu ally if we can resolve all transactions in this way we have validated the logical data model against the transactions however if we are unable to perform a transaction manually there must be a problem with the data model that must be resolved in this case it is likely that an error has been introduced while creating the relations and we should go back and check the areas of the data model that the transaction is accessing to identify and resolve the problem step check integrity constraints to check whether integrity constraints are represented in the logi cal data model integrity constraints are the constraints that we wish to impose in order to protect the database from becoming incomplete inaccurate or inconsistent although dbms controls for integrity constraints may or may not exist this is not the ques tion here at this stage we are concerned only with high level design that is specifying what integrity constraints are required irrespective of how this might be achieved a logical data model that includes all important integrity constraints is a true representation of the data requirements for the enterprise we consider the following types of integrity constraint required data attribute domain constraints multiplicity entity integrity referential integrity general constraints required data some attributes must always contain a valid value in other words they are not allowed to hold nulls for example every member of staff must have an associated job position such as supervisor or assistant these constraints should have been identified when we documented the attributes in the data dictionary step attribute domain constraints every attribute has a domain that is a set of values that are legal for example the sex of a member of staff is either m or f so the domain of the sex attribute is a single character string consisting of m or f these constraints should have been identified when we chose the attribute domains for the data model step multiplicity multiplicity represents the constraints that are placed on relationships between data in the database examples of such constraints include the requirements that a branch has many staff and a member of staff works at a single branch ensuring that all appropriate integrity constraints are identified and represented is an important part of modeling the data requirements of an enterprise in step we defined the relationships between entities and all integrity constraints that can be represented in this way were defined and documented in this step entity integrity the primary key of an entity cannot hold nulls for example each tuple of the staff relation must have a value for the primary key attribute staffno these constraints should have been considered when we identified the primary keys for each entity type step referential integrity a foreign key links each tuple in the child relation to the tuple in the parent rela tion containing the matching candidate key value referential integrity means that if the foreign key contains a value that value must refer to an existing tuple in the parent relation for example consider the staff manages propertyforrent relationship the staffno attribute in the propertyforrent relation links the property for rent to the tuple in the staff relation containing the member of staff who manages that property if staffno is not null it must contain a valid value that exists in the staffno attribute of the staff relation or the property will be assigned to a nonexistent member of staff there are two issues regarding foreign keys that must be addressed the first considers whether nulls are allowed for the foreign key for example can we store the details of a property for rent without having a member of staff specified to manage it that is can we specify a null staffno the issue is not whether the staff number exists but whether a staff number must be specified in general if the participation of the child relation in the relationship is mandatory then nulls are not allowed optional then nulls are allowed the second issue we must address is how to ensure referential integrity to do this we specify existence constraints that define conditions under which a candidate key or foreign key may be inserted updated or deleted for the staff manages propertyforrent relationship consider the following cases case insert tuple into child relation propertyforrent to ensure referen tial integrity check that the foreign key attribute staffno of the new propertyforrent tuple is set to null or to a value of an existing staff tuple case delete tuple from child relation propertyforrent if a tuple of a child relation is deleted referential integrity is unaffected case update foreign key of child tuple propertyforrent this case is similar to case to ensure referential integrity check whether the staffno of the updated propertyforrent tuple is set to null or to a value of an existing staff tuple case insert tuple into parent relation staff inserting a tuple into the parent relation staff does not affect referential integrity it simply becomes a parent with out any children in other words a member of staff without properties to manage case delete tuple from parent relation staff if a tuple of a parent relation is deleted referential integrity is lost if there exists a child tuple referencing the deleted parent tuple in other words if the deleted member of staff currently man ages one or more properties there are several strategies we can consider no action prevent a deletion from the parent relation if there are any ref erenced child tuples in our example you cannot delete a member of staff if he or she currently manages any properties cascade when the parent tuple is deleted automatically delete any refer enced child tuples if any deleted child tuple acts as the parent in another rela tionship then the delete operation should be applied to the tuples in this child relation and so on in a cascading manner in other words deletions from the parent relation cascade to the child relation in our example deleting a mem ber of staff automatically deletes all properties he or she manages clearly in this situation this strategy would not be wise if we have used the enhanced mod eling technique of composition to relate the parent and child entities cascade should be specified see section set null when a parent tuple is deleted the foreign key values in all corre sponding child tuples are automatically set to null in our example if a member of staff is deleted indicate that the current assignment of those properties previ ously managed by that employee is unknown we can consider this strategy only if the attributes constituting the foreign key accept nulls set default when a parent tuple is deleted the foreign key values in all corresponding child tuples should automatically be set to their default values in our example if a member of staff is deleted indicate that the current assign ment of some properties is being handled by another default member of staff such as the manager we can consider this strategy only if the attributes consti tuting the foreign key have default values defined no check when a parent tuple is deleted do nothing to ensure that referen tial integrity is maintained case update primary key of parent tuple staff if the primary key value of a parent relation tuple is updated referential integrity is lost if there exists a child tuple referencing the old primary key value that is if the updated member of staff figure referential integrity constraints for the relations in the staffclient user views of dreamhome currently manages one or more properties to ensure referential integrity the strategies described earlier can be used in the case of cascade the updates to the primary key of the parent tuple are reflected in any referencing child tuples and if a referencing child tuple is itself a primary key of a parent tuple this update will also cascade to its referencing child tuples and so on in a cascading manner it is normal for updates to be specified as cascade the referential integrity constraints for the relations that have been created for the staffclient user views of dreamhome are shown in figure general constraints finally we consider constraints known as general constraints updates to entities may be controlled by constraints governing the real world transactions that are represented by the updates for example dreamhome has a rule that prevents a member of staff from managing more than properties at the same time document all integrity constraints document all integrity constraints in the data dictionary for consideration during physical design step review logical data model with user to review the logical data model with the users to ensure that they consider the model to be a true representation of the data require ments of the enterprise the logical data model should now be complete and fully documented however to confirm that this is the case users are requested to review the logical data model to ensure that they consider the model to be a true representation of the data require ments of the enterprise if the users are dissatisfied with the model then some repetition of earlier steps in the methodology may be required if the users are satisfied with the model then the next step taken depends on the number of user views associated with the database and more importantly how they are being managed if the database system has a single user view or multiple user views that are being managed using the centralization approach see section then we proceed directly to the final step of step step if the database has multiple user views that are being managed using the view integration approach see section then we proceed to step the view integration approach results in the creation of several logical data models each of which represents one or more but not all user views of a database the purpose of step is to merge these data models to create a single logical data model that represents all user views of a database however before we consider this step we discuss briefly the relation ship between logical data models and data flow diagrams relationship between logical data model and data flow diagrams a logical data model reflects the structure of stored data for an enterprise a data flow diagram dfd shows data moving about the enterprise and being stored in datastores all attributes should appear within an entity type if they are held within the enterprise and will probably be seen flowing around the enterprise as a data flow when these two techniques are being used to model the users requirements specification we can use each one to check the consistency and completeness of the other the rules that control the relationship between the two techniques are each datastore should represent a whole number of entity types attributes on data flows should belong to entity types step merge logical data models into global model optional step to merge local logical data models into a single global logical data model that represents all user views of a database this step is necessary only for the design of a database with multiple user views that are being managed using the view integration approach to facilitate the descrip tion of the merging process we use the terms local logical data model and global logical data model a local logical data model represents one or more but not all user views of a database whereas global logical data model represents all user views of a database in this step we merge two or more local logical data models into a single global logical data model the source of information for this step is the local data models created through step and steps to of the methodology although each local logical data model should be correct comprehensive and unambiguous each model is a repre sentation only of one or more but not all user views of a database in other words each model represents only part of the complete database this may mean that there are inconsistencies as well as overlaps when we look at the complete set of user views thus when we merge the local logical data models into a single global model we must endeavor to resolve conflicts between the user views and any over laps that exist therefore on completion of the merging process the resulting global logical data model is subjected to validations similar to those performed on the local data models the validations are particularly necessary and should be focused on areas of the model that are subjected to most change during the merging process the activities in this step include step merge local logical data models into global logical data model step validate global logical data model step review global logical data model with users we demonstrate this step using the local logical data model developed previously for the staffclient user views of the dreamhome case study and using the model devel oped in chapters and for the branch user views of dreamhome figure shows the relations created from the er model for the branch user views given in figure we leave it as an exercise for the reader to show that this mapping is correct see exercise step merge local logical data models into global logical data model to merge local logical data models into a single global logical data model up to this point for each local logical data model we have produced an er dia gram a relational schema a data dictionary and supporting documentation that describes the constraints on the data in this step we use these components to iden tify the similarities and differences between the models and thereby help merge the models together for a simple database system with a small number of user views each with a small number of entity and relationship types it is a relatively easy task to compare the local models merge them together and resolve any differences that exist however in a large system a more systematic approach must be taken we present one approach that may be used to merge the local models together and resolve any inconsistencies found for a discussion on other approaches the interested reader is referred to the papers by batini and lanzerini biskup and convent spaccapietra et al and bouguettaya et al some typical tasks in this approach are review the names and contents of entities relations and their candidate keys review the names and contents of relationships foreign keys merge entities relations from the local data models figure relations for the branch user views of dreamhome include without merging entities relations unique to each local data model merge relationships foreign keys from the local data models include without merging relationships foreign keys unique to each local data model check for missing entities relations and relationships foreign keys check foreign keys check integrity constraints draw the global er relation diagram update the documentation in some of these tasks we have used the term entities relations and relation ships foreign keys this allows the designer to choose whether to examine the er models or the relations that have been derived from the er models in conjunc tion with their supporting documentation or even to use a combination of both approaches it may be easier to base the examination on the composition of relations as this removes many syntactic and semantic differences that may exist between different er models possibly produced by different designers perhaps the easiest way to merge several local data models together is first to merge two of the data models to produce a new model and then successively to merge the remaining local data models until all the local models are represented in the final global data model this may prove a simpler approach than trying to merge all the local data models at the same time review the names and contents of entities relations and their candidate keys it may be worthwhile to review the names and descriptions of entities relations that appear in the local data models by inspecting the data dictionary problems can arise when two or more entities relations have the same name but are in fact different homonyms are the same but have different names synonyms it may be necessary to compare the data content of each entity relation to resolve these problems in particular use the candidate keys to help identify equivalent entities relations that may be named differently across user views a comparison of the relations in the branch and staffclient user views of dreamhome is shown in table the relations that are common to each user views are highlighted review the names and contents of relationships foreign keys this activity is the same as described for entities relations a comparison of the foreign keys in the branch and staffclient user views of dreamhome is shown in table the foreign keys that are common to each user view are highlighted note in particular that of the relations that are common to both user views the staff and propertyforrent relations have an extra foreign key branchno this initial comparison of the relationship names foreign keys in each view again gives some indication of the extent to which the user views overlap however it is important to recognize that we should not rely too heavily on the fact that enti ties or relationships with the same name play the same role in both user views however comparing the names of entities relations and relationships foreign keys is a good starting point when searching for overlap between the user views as long as we are aware of the pitfalls we must be careful of entities or relationships that have the same name but in fact represent different concepts also called homonyms an example of this occur rence is the staff manages propertyforrent staffclient user views and manager manages branch branch user views obviously the manages relationship in this case means something different in each user view we must therefore ensure that entities or relationships that have the same name represent the same concept in the real world and that the names that differ in each user view represent different concepts to achieve this we compare the attributes and in particular the keys associated with each entity and also their associated table a comparison of the names of entities relations and their candidate keys in the branch and staffclient user views branch user views staffclient user views entity relation candidate keys entity relation candidate keys branch branchno postcode telephone telno staff staffno staff staffno manager staffno privateowner ownerno privateowner ownerno businessowner bname telno businessowner bname telno ownerno client clientno email client clientno email propertyforrent propertyno propertyforrent viewing propertyno clientno propertyno lease leaseno propertyno rentstart clientno rentstart lease leaseno propertyno rentstart clientno rentstart registration clientno branchno newspaper newpapername telno advert propertyno newspapername dateadvert relationships with other entities we should also be aware that entities or relation ships in one user view may be represented simply as attributes in another user view for example consider the scenario where the branch entity has an attribute called manager name in one user view which is represented as an entity called manager in another user view merge entities relations from the local data models examine the name and content of each entity relation in the models to be merged to determine whether entities relations represent the same thing and can therefore be merged typical activities involved in this task include merging entities relations with the same name and the same primary key merging entities relations with the same name but different primary keys merging entities relations with different names using the same or different primary keys table a comparison of the foreign keys in the branch and staffclient user views branch user views staffclient user views child relation foreign keys parent relation child relation foreign keys parent relation telephonea branchno branch branchno manager staffno staff staffno business owner business owner propertyforrent ownerno bname staffno branchno privateowner ownerno businessowner ownerno staff staffno branch branchno propertyforrent viewing ownerno ownerno staffno clientno propertyno privateowner ownerno privateowner ownerno staff staffno client clientno propertyforrent propertyno registrationb clientno branchno staffno client clientno branch branchno staff staffno newspaper advertc propertyno newspapername propertyforrent propertyno newspaper newspapername athe telephone relation is created from the multi valued attribute telno bthe registration relation is created from the ternary relationship registers cthe advert relation is created from the many to many relationship advertises figure merging the privateowner relations from the branch and staffclient user views merging entities relations with the same name and the same primary key generally entities relations with the same primary key represent the same real world object and should be merged the merged entity relation includes the attributes from the original entities relations with duplicates removed for exam ple figure lists the attributes associated with the relation privateowner defined in the branch and staffclient user views the primary key of both relations is own erno we merge these two relations together by combining their attributes so that the merged privateowner relation now has all the original attributes associated with both privateowner relations note that there is conflict between the user views on how we should represent the name of an owner in this situation we should if possible consult the users of each user view to determine the final representation note that in this example we use the decomposed version of the owner name represented by the fname and iname attributes in the merged global view in a similar way from table the staff client propertyforrent and lease relations have the same primary keys in both user views and the relations can be merged as discussed earlier merging entities relations with the same name but different primary keys in some situations we may find two entities relations with the same name and simi lar candidate keys but with different primary keys in this case the entities rela tions should be merged together as described previously however it is necessary to choose one key to be the primary key the others becoming alternate keys for example figure lists the attributes associated with the two relations businessowner defined in the two user views the primary key of the businessowner relation in the branch user views is bname and the primary key of the businessowner relation in the staffclient user views is ownerno however the alternate key for businessowner in the staffclient user views is bname although the primary keys are different the primary key of businessowner in the branch user views is the alternate key of businessowner in the staffclient user views we merge these two relations together as shown in figure and include bname as an alternate key merging entities relations with different names using the same or different primary keys in some cases we may identify entities relations that have different figure merging the businessowner relations with different primary keys names but appear to have the same purpose these equivalent entities relations may be recognized simply by their name which indicates their similar purpose their content and in particular their primary key their association with particular relationships an obvious example of this occurrence would be entities called staff and employee which if found to be equivalent should be merged include without merging entities relations unique to each local data model the previous tasks should identify all entities relations that are the same all remaining entities relations are included in the global model without change from table the branch telephone manager registration newspaper and advert rela tions are unique to the branch user views and the viewing relation is unique to the staffclient user views merge relationships foreign keys from the local data models in this step we examine the name and purpose of each relationship foreign key in the data models before merging relationships foreign keys it is important to resolve any conflicts between the relationships such as differences in multiplicity constraints the activities in this step include merging relationships foreign keys with the same name and the same purpose merging relationships foreign keys with different names but the same purpose using table and the data dictionary we can identify foreign keys with the same name and the same purpose which can be merged into the global model note that the registers relationship in the two user views essentially represents the same event in the staffclient user views the registers relationship models a member of staff registering a client and this is represented using staffno as a foreign key in client in the branch user views the situation is slightly more com plex due to the additional modeling of branches and this requires a new relation called registration to model a member of staff registering a client at a branch in this case we ignore the registers relationship in the staffclient user views and include the equivalent relationships foreign keys from the branch user views in the next step include without merging relationships foreign keys unique to each local data model again the previous task should identify relationships foreign keys that are the same by definition they must be between the same entities relations which would have been merged together earlier all remaining relationships foreign keys are included in the global model without change check for missing entities relations and relationships foreign keys perhaps one of the most difficult tasks in producing the global model is identi fying missing entities relations and relationships foreign keys between different local data models if a corporate data model exists for the enterprise this may reveal entities and relationships that do not appear in any local data model alternatively as a preventative measure when interviewing the users of a specific user views ask them to pay particular attention to the entities and relationships that exist in other user views otherwise examine the attributes of each entity relation and look for references to entities relations in other local data models we may find that we have an attribute associated with an entity relation in one local data model that corresponds to a primary key alternate key or even a non key attribute of an entity relation in another local data model check foreign keys during this step entities relations and relationships foreign keys may have been merged primary keys changed and new relationships identified confirm that the foreign keys in child relations are still correct and make any necessary modifica tions the relations that represent the global logical data model for dreamhome are shown in figure check integrity constraints confirm that the integrity constraints for the global logical data model do not conflict with those originally specified for each user view for example if any new relationships have been identified and new foreign keys have been created ensure that appropriate referential integrity constraints are specified any conflicts must be resolved in consultation with the users figure relations that represent the global logical data model for dreamhome draw the global er relation diagram we now draw a final diagram that represents all the merged local logical data models if relations have been used as the basis for merging we call the resulting diagram a global relation diagram which shows primary keys and foreign keys if local er diagrams have been used the resulting diagram is simply a global er diagram the global relation diagram for dreamhome is shown in figure figure global relation diagram for dreamhome update the documentation update the documentation to reflect any changes made during the development of the global data model it is very important that the documentation is up to date and reflects the current data model if changes are made to the model subsequently either during database implementation or during maintenance then the docu mentation should be updated at the same time out of date information will cause considerable confusion at a later time step validate global logical data model to validate the relations created from the global logical data model using the technique of normalization and to ensure that they support the required transactions if necessary this step is equivalent to steps and in which we validated each local logical data model however it is necessary to check only those areas of the model that resulted in any change during the merging process in a large system this will sig nificantly reduce the amount of rechecking that needs to be performed step review global logical data model with users to review the global logical data model with the users to ensure that they consider the model to be a true representation of the data requirements of an enterprise the global logical data model for the enterprise should now be complete and accurate the model and the documentation that describes the model should be reviewed with the users to ensure that it is a true representation of the enterprise to facilitate the description of the tasks associated with step it is necessary to use the terms local logical data model and global logical data model however at the end of this step when the local data models have been merged into a single global data model the distinction between the data models that refer to some or all user views of a database is no longer necessary therefore after completing this step we refer to the single global data model using the simpler term logical data model for the remaining steps of the methodology step check for future growth to determine whether there are any significant changes likely in the foreseeable future and to assess whether the logical data model can accommodate these changes logical database design concludes by considering whether the logical data model which may or may not have been developed using step is capable of being extended to support possible future developments if the model can sustain cur rent requirements only then the life of the model may be relatively short and significant reworking may be necessary to accommodate new requirements it is important to develop a model that is extensible and has the ability to evolve to support new requirements with minimal effect on existing users of course this may be very difficult to achieve as the enterprise may not know what it wants to do in the future even if it does it may be prohibitively expensive both in time and money to accommodate possible future enhancements now therefore it may be necessary to be selective in what is accommodated consequently it is worth exam ining the model to check its ability to be extended with minimal impact however it is not necessary to incorporate any changes into the data model unless requested by the user at the end of step the logical data model is used as the source of information for physical database design which is described in the following two chapters as steps to of the methodology for readers familiar with database design a summary of the steps of the meth odology is presented in appendix d chapter summary the database design methodology includes three main phases conceptual logical and physical database design logical database design is the process of constructing a model of the data used in an enterprise based on a specific data model but independent of a particular dbms and other physical considerations a logical data model includes er diagram relational schema and supporting documentation such as the data dictionary which is produced throughout the development of the model the purpose of step of the methodology for logical database design is to derive a relational schema from the conceptual data model created in step in step the relational schema is validated using the rules of normalization to ensure that each relation is struc turally correct normalization is used to improve the model so that it satisfies various constraints that avoids unnecessary duplication of data in step the relational schema is also validated to ensure that it supports the transactions given in the users requirements specification in step the integrity constraints of the logical data model are checked integrity constraints are the con straints that are to be imposed on the database to protect the database from becoming incomplete inaccurate or inconsistent the main types of integrity constraints include required data attribute domain constraints multi plicity entity integrity referential integrity and general constraints in step the logical data model is validated by the users step of logical database design is an optional step and is required only if the database has multiple user views that are being managed using the view integration approach see section which results in the creation of two or more local logical data models a local logical data model represents the data requirements of one or more but not all user views of a database in step these data models are merged into a global logical data model which represents the requirements of all user views this logical data model is again validated using normalization against the required transaction and by users logical database design concludes with step which includes consideration of whether the model is capa ble of being extended to support possible future developments at the end of step the logical data model which may or may not have been developed using step is the source of information for physical database design described as steps to in chapters and describe the steps used to build a logical data model describe the rules for deriving relations that represent a strong entity types b weak entity types c one to many binary relationship types d one to one binary relationship types e one to one recursive relationship types f superclass subclass relationship types g many to many binary relationship types h complex relationship types i multi valued attributes give examples to illustrate your answers discuss how the technique of normalization can be used to validate the relations derived from the conceptual data model database design is quite complex and important discuss the role played by users during the design process describe database design language dbdl discuss how it is used to derive relations during the logical database design phase describe the purpose of merging data models discuss the difference between local logical data model and global logical data model why is it important to check the logical data model for future growth derive relations from the following conceptual data model shown in figure the dreamhome case study create a relational schema for the branch user view of dreamhome based on the conceptual data model produced in exercise and compare your schema with the relations listed in figure justify any differences found the university accommodation office case study create and validate a logical data model from the conceptual data model for the university accommodation office case study created in exercise the easydrive school of motoring case study create and validate a logical data model from the conceptual data model for the easydrive school of motoring case study created in exercise the wellmeadows hospital case study create and validate the local logical data models for each of the local conceptual data models of the wellmeadows hospital case study identified in exercise figure an example conceptual data model merge the local data models to create a global logical data model of the wellmeadows hospital case study state any assumptions necessary to support your design the parking lot case study present the relational schema mapped from the parking lot eer model shown in figure and described in exercises and figure an eer model of the parking lot case study the library case study describe the relational schema mapped from the library eer model shown in figure and described in exercises and figure an eer model of the library case study the er diagram in figure shows only entities and primary key attributes the absence of recognizable named entities or relationships is to emphasize the rule based nature of the mapping rules described previously in step of logical database design figure an example er model answer the following questions with reference to how the er model in figure maps to relational tables a how many relations will represent the er model b how many foreign keys are mapped to the relation representing x c which relation will have no foreign key d using only the letter identifier for each entity provide appropriate names for the rela tions mapped from the er model e if the cardinality for each relationship is changed to one to one with total participation for all entities how many relations would be derived from this version of the er model chapter methodology physical database design for relational databases in this chapter and the next we describe and illustrate by example a physical data base design methodology for relational databases the starting point for this chapter is the logical data model and the documentation that describes the model created in the conceptual logical database design methodol ogy described in chapters and the methodology started by producing a con ceptual data model in step and then derived a set of relations to produce a logical data model in step the derived relations were validated to ensure they were cor rectly structured using the technique of normalization described in chapters and and to ensure that they supported the transactions the users require in the third and final phase of the database design methodology the designer must decide how to translate the logical database design that is the entities attributes relationships and constraints into a physical database design that can be implemented using the target dbms as many parts of physical database design are highly dependent on the target dbms there may be more than one way of implementing any given part of the database consequently to do this work properly the designer must be fully aware of the functionality of the target dbms and must understand the advantages and disadvantages of each alterna tive approach for a particular implementation for some systems the designer may also need to select a suitable storage strategy that takes account of intended database usage structure of this chapter in section we provide a comparison of logical and physical database design in section we provide an overview of the physical database design methodology and briefly describe the main activi ties associated with each design phase in section we focus on the method ology for physical database design and present a detailed description of the first four steps required to build a physical data model in these steps we show how to convert the relations derived for the logical data model into a specific data base implementation we provide guidelines for choosing storage structures for the base relations and deciding when to create indexes in places we show physical implementation details to clarify the discussion in chapter we complete our presentation of the physical database design methodology and discuss how to monitor and tune the operational system in particular we consider when it is appropriate to denormalize the logical data model and introduce redundancy appendix d presents a summary of the data base design methodology for those readers who are already familiar with data base design and require merely an overview of the main steps comparison of logical and physical database design in presenting a database design methodology we divide the design process into three main phases conceptual logical and physical database design the phase prior to physical design logical database design is largely independent of implementation details such as the specific functionality of the target dbms and application programs but is dependent on the target data model the output of this process is a logical data model consisting of an er relation diagram relational schema and supporting documentation that describes this model such as a data dictionary together these represent the sources of information for the physical design process and provide the physical database designer with a vehicle for mak ing tradeoffs that are so important to an efficient database design whereas logical database design is concerned with the what physical database design is concerned with the how it requires different skills that are often found in dif ferent people in particular the physical database designer must know how the com puter system hosting the dbms operates and must be fully aware of the functionality of the target dbms as the functionality provided by current systems varies widely physical design must be tailored to a specific dbms however physical database designisnotanisolatedactivity thereisoftenfeedbackbetweenphysical logical and application design for example decisions taken during physical design for improv ing performance such as merging relations together might affect the structure of the logical data model which will have an associated effect on the application design overview of the physical database design methodology overview of the physical database design methodology the process of producing a description of the implementation of the database on secondary storage it describes the base relations file organizations and indexes used to achieve efficient access to the data and any associated integrity constraints and security measures the steps of the physical database design methodology are as follows step translate logical data model for target dbms step design base relations step design representation of derived data step design general constraints step design file organizations and indexes step analyze transactions step choose file organizations step choose indexes step estimate disk space requirements step design user views step design security mechanisms step consider the introduction of controlled redundancy step monitor and tune the operational system the physical database design methodology presented in this book is divided into six main steps numbered consecutively from to follow the three steps of the conceptual and logical database design methodology step of physical database design involves the design of the base relations and general constraints using the available functionality of the target dbms this step also considers how we should represent any derived data present in the data model step involves choosing the file organizations and indexes for the base relations typically pc dbmss have a fixed storage structure but other dbmss tend to pro vide a number of alternative file organizations for data from the user viewpoint the internal storage representation for relations should be transparent the user should be able to access relations and tuples without having to specify where or how the tuples are stored this requires that the dbms provides physical data independ ence so that users are unaffected by changes to the physical structure of the data base as discussed in section the mapping between the logical data model and physical data model is defined in the internal schema as shown in figure the designer may have to provide the physical design details to both the dbms and the operating system for the dbms the designer may have to specify the file organizations that are to be used to represent each relation for the operating sys tem the designer must specify details such as the location and protection for each file we recommend that the reader reviews appendix f on file organization and storage structures before reading step of the methodology step involves deciding how each user view should be implemented step involves designing the security measures necessary to protect the data from unauthorized access including the access controls that are required on the base relations step described in chapter considers relaxing the normalization con straints imposed on the logical data model to improve the overall performance of the system this step should be undertaken only if necessary because of the inherent problems involved in introducing redundancy while still maintaining con sistency step chapter is an ongoing process of monitoring the operational system to identify and resolve any performance problems resulting from the design and to implement new or changing requirements appendix d presents a summary of the methodology for those readers who are already familiar with database design and require merely an overview of the main steps the physical database design methodology for relational databases this section provides a step by step guide to the first four steps of the physical data base design methodology for relational databases in places we demonstrate the close association between physical database design and implementation by describ ing how alternative designs can be implemented using various target dbmss the remaining two steps are covered in the next chapter step translate logical data model for target dbms to produce a relational database schema from the logical data model that can be implemented in the target dbms the first activity of physical database design involves the translation of the rela tions in the logical data model into a form that can be implemented in the target relational dbms the first part of this process entails collating the information gathered during logical database design and documented in the data dictionary along with the information gathered during the requirements collection and analy sis stage and documented in the systems specification the second part of the pro cess uses this information to produce the design of the base relations this process requires intimate knowledge of the functionality offered by the target dbms for example the designer will need to know how to create base relations whether the system supports the definition of primary keys foreign keys and alternate keys whether the system supports the definition of required data that is whether the system allows attributes to be defined as not null whether the system supports the definition of domains whether the system supports relational integrity constraints whether the system supports the definition of general constraints the three activities of step are step design base relations step design representation of derived data step design general constraints step design base relations to decide how to represent the base relations identified in the logi cal data model in the target dbms to start the physical design process we first collate and assimilate the information about the relations produced during logical database design the necessary infor mation can be obtained from the data dictionary and the definition of the relations described using the dbdl for each relation identified in the logical data model we have a definition consisting of the name of the relation a list of simple attributes in brackets the primary key and where appropriate alternate keys ak and foreign keys fk referential integrity constraints for any foreign keys identified from the data dictionary we also have for each attribute its domain consisting of a data type length and any constraints on the domain an optional default value for the attribute whether the attribute can hold nulls whether the attribute is derived and if so how it should be computed to represent the design of the base relations we use an extended form of the dbdl to define domains default values and null indicators for example for the propertyforrent relation of the dreamhome case study we may produce the design shown in figure implementing base relations the next step is to decide how to implement the base relations this decision is dependent on the target dbms some systems provide more facilities than others for defining base relations we have previously demonstrated how to implement base relations using the iso sql standard section we also show how to implement base relations using microsoft office access appendix h and oracle appendix h document design of base relations the design of the base relations should be fully documented along with the reasons for selecting the proposed design in particular document the reasons for selecting one approach when many alternatives exist figure dbdl for the propertyforrent relation step design representation of derived data to decide how to represent any derived data present in the logical data model in the target dbms attributes whose value can be found by examining the values of other attributes are known as derived or calculated attributes for example the following are all derived attributes the number of staff who work in a particular branch the total monthly salaries of all staff the number of properties that a member of staff handles often derived attributes do not appear in the logical data model but are docu mented in the data dictionary if a derived attribute is displayed in the model a is used to indicate that it is derived see section the first step is to examine the logical data model and the data dictionary and produce a list of all derived attributes from a physical database design perspective whether a derived figure the propertyforrent relation and a simplified staff relation with the derived attribute noofproperties attribute is stored in the database or calculated every time it is needed is a tradeoff the designer should calculate the additional cost to store the derived data and keep it consistent with opera tional data from which it is derived the cost to calculate it each time it is required the less expensive option is chosen subject to performance constraints for the previous example we could store an additional attribute in the staff relation repre senting the number of properties that each member of staff currently manages a simplified staff relation based on the sample instance of the dreamhome database shown in figure with the new derived attribute noofproperties is shown in figure the additional storage overhead for this new derived attribute would not be par ticularly significant the attribute would need to be updated every time a member of staff were assigned to or deassigned from managing a property or the property was removed from the list of available properties in each case the noofproperties attribute for the appropriate member of staff would be incremented or decre mented by it would be necessary to ensure that this change is made consistently to maintain the correct count and thereby ensure the integrity of the database when a query accesses this attribute the value would be immediately available and would not have to be calculated on the other hand if the attribute is not stored directly in the staff relation it must be calculated each time it is required this involves a join of the staff and propertyforrent relations thus if this type of query is frequent or is considered to be critical for performance purposes it may be more appropriate to store the derived attribute rather than calculate it each time it may also be more appropriate to store derived attributes whenever the dbms query language cannot easily cope with the algorithm to calculate the derived attribute for example sql has a limited set of aggregate functions as we discussed in chapter document design of derived data the design of derived data should be fully documented along with the reasons for selecting the proposed design in particular document the reasons for selecting one approach where many alternatives exist step design general constraints to design the general constraints for the target dbms updates to relations may be constrained by integrity constraints governing the real world transactions that are represented by the updates in step we designed a number of integrity constraints required data domain constraints and entity and referential integrity in this step we have to consider the remaining general constraints the design of such constraints is again dependent on the choice of dbms some systems provide more facilities than others for defining general constraints as in the previous step if the system is compliant with the sql stan dard some constraints may be easy to implement for example dreamhome has a rule that prevents a member of staff from managing more than properties at the same time we could design this constraint into the sql create table statement for propertyforrent using the following clause constraint staffnothandlingtoomuch check not exists select staffno from propertyforrent group by staffno having count alternatively a trigger could be used to enforce some constraints as we illustrated in section in some systems there will be no support for some or all of the general constraints and it will be necessary to design the constraints into the appli cation for example there are very few relational dbmss if any that would be able to handle a time constraint such as at 30 on the last working day of each year archive the records for all properties sold that year and delete the associated records document design of general constraints the design of general constraints should be fully documented in particular docu ment the reasons for selecting one approach where many alternatives exist step design file organizations and indexes to determine the optimal file organizations to store the base rela tions and the indexes that are required to achieve acceptable perfor mance that is the way in which relations and tuples will be held on secondary storage one of the main objectives of physical database design is to store and access data in an efficient way see appendix f although some storage structures are efficient for bulk loading data into the database they may be inefficient after that thus we may have to choose to use an efficient storage structure to set up the database and then choose another for operational use again the types of file organization available are dependent on the target dbms some systems provide more choice of storage structures than others it is extremely important that the physical database designer fully understands the stor age structures that are available and how the target system uses these structures this may require the designer to know how the system query optimizer functions for example there may be circumstances under which the query optimizer would not use a secondary index even if one were available thus adding a secondary index would not improve the performance of the query and the resultant overhead would be unjustified we discuss query processing and optimization in chapter as with logical database design physical database design must be guided by the nature of the data and its intended use in particular the database designer must understand the typical workload that the database must support during the require ments collection and analysis stage there may have been requirements specified about how fast certain transactions must run or how many transactions must be processed per second this information forms the basis for a number of decisions that will be made during this step with these objectives in mind we now discuss the activities in step step analyze transactions step choose file organizations step choose indexes step estimate disk space requirements step design file organizations and indexes step analyze transactions to understand the functionality of the transactions that will run on the database and to analyze the important transactions to carry out physical database design effectively it is necessary to have knowledge of the transactions or queries that will run on the database this includes both qualitative and quantitative information in analyzing the transactions we attempt to identify performance criteria such as the transactions that run frequently and will have a significant impact on performance the transactions that are critical to the operation of the business the times during the day week when there will be a high demand made on the database called the peak load we use this information to identify the parts of the database that may cause perfor mance problems at the same time we need to identify the high level functionality of the transactions such as the attributes that are updated in an update transaction or the criteria used to restrict the tuples that are retrieved in a query we use this information to select appropriate file organizations and indexes in many situations it is not possible to analyze all the expected transactions so we should at least investigate the most important ones it has been suggested that the most active of user queries account for of the total data access wiederhold this rule may be used as a guideline in carrying out the analysis to help identify which transactions to investigate we can use a transac tion relation cross reference matrix which shows the relations that each transaction accesses and or a transaction usage map which diagrammatically indicates which relations are potentially heavily used to focus on areas that may be problematic one way to proceed is to map all transaction paths to relations determine which relations are most frequently accessed by transactions analyze the data usage of selected transactions that involve these relations map all transaction paths to relations in steps and of the conceptual logical database design methodology we validated the data models to ensure they supported the transactions that the users require by mapping the transaction paths to entities relations if a transac tion pathway diagram was used similar to the one shown in figure we may be able to use this diagram to determine the relations that are most frequently accessed on the other hand if the transactions were validated in some other way it may be useful to create a transaction relation cross reference matrix the matrix shows in a visual way the transactions that are required and the relations they access for example table shows a transaction relation cross reference matrix for the following selection of typical entry update delete and query trans actions for dreamhome see appendix a a enter the details for a new property and the owner such as details of property number in glasgow owned by tina murphy b update delete the details of a property c identify the total number of staff in each position at branches in glasgow d list the property number address type and rent of all properties in glasgow ordered by rent e list the details of properties for rent managed by a named member of staff f identify the total number of properties assigned to each member of staff at a given branch staffclient view branch view the matrix indicates for example that transaction a reads the staff table and also inserts tuples into the propertyforrent and privateowner businessowner relations to be more useful the matrix should indicate in each cell the number of accesses over some time interval for example hourly daily or weekly however to keep the matrix simple we do not show this information this matrix shows that both the staff and propertyforrent relations are accessed by five of the six transactions and so table cross referencing transactions and relations transaction relation a b c d e f i r u d i r u d i r u d i r u d i r u d i r u d branch x x x telephone staff x x x x x manager privateowner x businessowner x propertyforrent x x x x x x x viewing client registration lease newspaper advert i insert r read u update d delete efficient access to these relations may be important to avoid performance problems we therefore conclude that a closer inspection of these transactions and relations are necessary determine frequency information in the requirements specification for dreamhome given in section it was estimated that there are about properties for rent and staff distributed over branch offices with an average of and a maximum of proper ties at each branch figure shows the transaction usage map for transactions c d e and f which all access at least one of the staff and propertyforrent relations with these numbers added due to the size of the propertyforrent relation it will be important that access to this relation is as efficient as possible we may now decide that a closer analysis of transactions involving this particular relation would be useful in considering each transaction it is important to know not only the average and maximum number of times that it runs per hour but also the day and time that the transaction is run including when the peak load is likely for example some transactions may run at the average rate for most of the time but have a peak load ing between 00 and 00 on a thursday prior to a meeting on friday morning other transactions may run only at specific times for example 00 00 on fridays saturdays which is also their peak loading when transactions require frequent access to particular relations then their patern of operation is very important if these transactions operate in a mutually figure transaction usage map for some sample transactions showing expected occurrences exclusive manner the risk of likely performance problems is reduced however if their operating patterns conflict potential problems may be alleviated by examin ing the transactions more closely to determine whether changes can be made to the structure of the relations to improve performance as we discuss in step in the next chapter alternatively it may be possible to reschedule some transactions so that their operating patterns do not conflict for example it may be possible to leave some summary transactions until a quieter time in the evening or overnight analyze data usage having identified the important transactions we now analyze each one in more detail for each transaction we should determine the relations and attributes accessed by the transaction and the type of access that is whether it is an insert update delete or retrieval also known as a query transaction for an update transaction note the attributes that are updated as these attributes may be candidates for avoiding an access structure such as a secondary index the attributes used in any predicates in sql the predicates are the conditions specified in the where clause check to see whether the predicates involve pattern matching for example name like smith range searches for example salary between and exact match key retrieval for example salary this applies not only to queries but also to update and delete transactions which can restrict the tuples to be updated deleted in a relation these attributes may be candidates for access structures for a query the attributes that are involved in the join of two or more relations again these attributes may be candidates for access structures the expected frequency at which the transaction will run for example the trans action will run approximately times per day the performance goals for the transaction for example the transaction must complete within second the attributes used in any predicates for very frequent or critical transactions should have a higher priority for access structures figure shows an example of a transaction analysis form for transaction d this form shows that the average frequency of this transaction is times per hour with a peak loading of times per hour daily between 00 and 00 figure example transaction analysis form in other words typically half the branches will run this transaction per hour and at peak time all branches will run this transaction once per hour the form also shows the required sql statement and the transaction usage map at this stage the full sql statement may be too detailed but the types of details that are shown adjacent to the sql statement should be identified namely any predicates that will be used any attributes that will be required to join relations together for a query trans action attributes used to order results for a query transaction attributes used to group data together for a query transaction any built in functions that may be used such as avg sum any attributes that will be updated by the transaction this information will be used to determine the indexes that are required as we discuss next below the transaction usage map there is a detailed breakdown docu menting how each relation is accessed reads in this case how many tuples will be accessed each time the transaction is run how many tuples will be accessed per hour on average and at peak loading times the frequency information will identify the relations that will need careful consid eration to ensure that appropriate access structures are used as mentioned previ ously the search conditions used by transactions that have time constraints become higher priority for access structures step choose file organizations to determine an efficient file organization for each base relation one of the main objectives of physical database design is to store and access data in an efficient way for example if we want to retrieve staff tuples in alphabetical order of name sorting the file by staff name is a good file organization however if we want to retrieve all staff whose salary is in a certain range searching a file ordered by staff name would not be particularly efficient to complicate matters some file organizations are efficient for bulk loading data into the database but inefficient after that in other words we may want to use an efficient storage struc ture to set up the database and then change it for normal operational use the objective of this step therefore is to choose an optimal file organization for each relation if the target dbms allows this in many cases a relational dbms may give little or no choice for choosing file organizations although some may be established as indexes are specified however as an aid to understanding file organizations and indexes more fully we provide guidelines in appendix f for selecting a file organization based on the following types of file heap hash indexed sequential access method isam b tree clusters if the target dbms does not allow the choice of file organizations this step can be omitted document choice of file organizations the choice of file organizations should be fully documented along with the rea sons for the choice in particular document the reasons for selecting one approach where many alternatives exist step choose indexes to determine whether adding indexes will improve the perfor mance of the system one approach to selecting an appropriate file organization for a relation is to keep the tuples unordered and create as many secondary indexes as necessary another approach is to order the tuples in the relation by specifying a primary or clustering index see appendix f in this case choose the attribute for ordering or clustering the tuples as the attribute that is used most often for join operations as this makes the join operation more efficient or the attribute that is used most often to access the tuples in a relation in order of that attribute if the ordering attribute chosen is a key of the relation the index will be a primary index if the ordering attribute is not a key the index will be a clustering index remember that each relation can have only either a primary index or a clustering index specifying indexes we saw in section that an index can usually be created in sql using the create index statement for example to create a primary index on the propertyforrent relation based on the propertyno attribute we might use the following sql statement create unique index propertynoind on propertyforrent propertyno to create a clustering index on the propertyforrent relation based on the staffno attribute we might use the following sql statement create index staffnoind on propertyforrent staffno cluster as we have already mentioned in some systems the file organization is fixed for example until recently oracle has supported only b trees but has now added support for clusters on the other hand ingres offers a wide set of different index structures that can be chosen using the following optional clause in the create index statement structure btree isam hash heap choosing secondary indexes secondary indexes provide a mechanism for specifying an additional key for a base relation that can be used to retrieve data more efficiently for example the propertyforrent relation may be hashed on the property number propertyno the pri mary index however there may be frequent access to this relation based on the rent attribute in this case we may decide to add rent as a secondary index there is an overhead involved in the maintenance and use of secondary indexes that has to be balanced against the performance improvement gained when retrieving data this overhead includes adding an index record to every secondary index whenever a tuple is inserted into the relation updating a secondary index when the corresponding tuple in the relation is updated the increase in disk space needed to store the secondary index possible performance degradation during query optimization as the query opti mizer may consider all secondary indexes before selecting an optimal execution strategy guidelines for choosing a wish list of indexes one approach to determining which secondary indexes are needed is to produce a wish list of attributes that we consider to be candidates for indexing and then to examine the impact of maintaining each of these indexes we provide the following guidelines to help produce such a wish list do not index small relations it may be more efficient to search the relation in memory than to store an additional index structure in general index the primary key of a relation if it is not a key of the file organization although the sql standard provides a clause for the specification of primary keys as discussed in section it should be noted that this does not guarantee that the primary key will be indexed add a secondary index to a foreign key if it is frequently accessed for exam ple we may frequently join the propertyforrent relation and the privateowner business owner relations on the attribute ownerno the owner number therefore it may be more efficient to add a secondary index to the propertyforrent relation based on the attribute ownerno note that some dbmss may automatically index foreign keys add a secondary index to any attribute that is heavily used as a secondary key for example add a secondary index to the propertyforrent relation based on the attribute rent as discussed previously add a secondary index on attributes that are frequently involved in a selection or join criteria b order by c group by d other operations involving sorting such as union or distinct add a secondary index on attributes involved in built in aggregate func tions along with any attributes used for the built in functions for example to find the average staff salary at each branch we could use the following sql query select branchno avg salary from staff group by branchno from the previous guideline we could consider adding an index to the branchno attribute by virtue of the group by clause however it may be more efficient to consider an index on both the branchno attribute and the salary attribute this may allow the dbms to perform the entire query from data in the index alone without having to access the data file this is sometimes called an index only plan as the required response can be produced using only data in the index as a more general case of the previous guideline add a secondary index on attributes that could result in an index only plan avoid indexing an attribute or relation that is frequently updated avoid indexing an attribute if the query will retrieve a significant proportion for example of the tuples in the relation in this case it may be more efficient to search the entire relation than to search using an index avoid indexing attributes that consist of long character strings if the search criteria involve more than one predicate and one of the terms con tains an or clause and the term has no index sort order then adding indexes for the other attributes is not going to help improve the speed of the query because a linear search of the relation will still be required for example assume that only the type and rent attributes of the propertyforrent relation are indexed and we need to use the following query select from propertyforrent where type flat or rent or rooms although the two indexes could be used to find the tuples where type flat or rent the fact that the rooms attribute is not indexed will mean that these indexes cannot be used for the full where clause thus unless there are other queries that would benefit from having the type and rent attributes indexed there would be no benefit gained from indexing them for this query on the other hand if the predicates in the where clause were and ed together the two indexes on the type and rent attributes could be used to optimize the query removing indexes from the wish list having drawn up the wish list of potential indexes we should now consider the impact of each of these on update transactions if the maintenance of the index is likely to slow down important update transactions then consider dropping the index from the list note however that a particular index may also make update operations more efficient for example if we want to update a member of staff salary given the member staff number staffno and we have an index on staffno then the tuple to be updated can be found more quickly it is a good idea to experiment when possible to determine whether an index is improving performance providing very little improvement or adversely impacting performance in the last case clearly we should remove this index from the wish list if there is little observed improvement with the addition of the index further examination may be necessary to determine under what circumstances the index will be useful and whether these circumstances are sufficiently important to warrant the implementation of the index some systems allow users to inspect the optimizer strategy for executing a particular query or update sometimes called the query execution plan qep for example microsoft office access has a performance analyzer oracle has an explain plan diagnostic utility see section db2 has an explain util ity and ingres has an online qep viewing facility when a query runs slower than expected it is worth using such a facility to determine the reason for the slowness and to find an alternative strategy that may improve the performance of the query if a large number of tuples are being inserted into a relation with one or more indexes it may be more efficient to drop the indexes first perform the inserts and then recreate the indexes afterwards as a rule of thumb if the insert will increase the size of the relation by at least drop the indexes temporarily updating the database statistics the query optimizer relies on database statistics held in the system catalog to select the optimal strategy whenever we create an index the dbms automatically adds the presence of the index to the system catalog however we may find that the dbms requires a utility to be run to update the statistics in the system catalog relating to the relation and the index document choice of indexes the choice of indexes should be fully documented along with the reasons for the choice in particular if there are performance reasons why some attributes should not be indexed these should also be documented file organizations and indexes for dreamhome with microsoft office access like most if not all pc dbmss microsoft office access uses a fixed file organi zation so if the target dbms is microsoft office access step can be omitted microsoft office access does however support indexes as we will now briefly dis cuss in this section we use the terminology of office access which refers to a rela tion as a table with fields and records guidelines for indexes in office access the primary key of a table is automati cally indexed but a field whose data type is memo hyperlink or ole object cannot be indexed for other fields microsoft advises indexing a field if all the following apply the field data type is text number currency or date time the user anticipates searching for values stored in the field the user anticipates sorting values in the field the user anticipates storing many different values in the field if many of the values in the field are the same the index may not significantly speed up queries in addition microsoft advises indexing fields on both sides of a join or creating a relationship between these fields in which case office access will automatically create an index on the for eign key field if one does not exist already when grouping records by the values in a joined field specifying group by for the field that is in the same table as the field the aggregate is being calcu lated on microsoft office access can optimize simple and complex predicates which are called expressions in office access for certain types of complex expressions microsoft office access uses a data access technology called rushmore to achieve a greater level of optimization a complex expression is formed by combining two simple expressions with the and or or operator such as branchno and rooms type flat or rent in office access a complex expression is fully or partially optimizable depending on whether one or both simple expressions are optimizable and which operator was used to combine them a complex expression is rushmore optimizable if all three of the following conditions are true the expression uses and or or to join two conditions both conditions are made up of simple optimizable expressions both expressions contain indexed fields the fields can be indexed individually or they can be part of a multiple field index indexes for dreamhome before creating the wish list we ignore small tables from further consideration as small tables can usually be processed in memory without requiring additional indexes for dreamhome we ignore the branch telephone manager and newspaper tables from further consideration based on the guidelines provided earlier create the primary key for each table which will cause office access to auto matically index this field ensure all relationships are created in the relationships window which will cause office access to automatically index the foreign key fields as an illustration of which other indexes to create we consider the query transac tions listed in appendix a for the staffclient user views of dreamhome we can produce a summary of interactions between the base tables and these transactions shown in table this figure shows for each table the transaction that oper ate on the table the type of access a search based on a predicate a join together with the join field any ordering field and any grouping field and the frequency with which the transaction runs table interactions between base tables and query transactions for the staffclient user views of dreamhome table transaction field frequency per day staff a d predicate fname iname a join staff on supervisorstaffno b ordering fname iname b predicate position client e join staff on staff no j predicate fname iname propertyforrent c predicate rentfinish k predicate rentfinish c join privateowner businessowner on ownerno d join staff on staff no f predicate city f predicate rent g join client on clientno viewing i join client on clientno lease c join propertyforrent on propertyno join propertyforrent on propertyno j join client on clientno based on this information we choose to create the additional indexes shown in table we leave it as an exercise for the reader to choose additional indexes to create in microsoft office access for the transactions listed in appendix a for the branch view of dreamhome see exercise file organizations and indexes for dreamhome with oracle in this section we repeat the previous exercise of determining appropriate file organizations and indexes for the staffclient user views of dreamhome once again table additional indexes to be created in microsoft office access based on the query transactions for the staffclient user views of dreamhome table index staff fname iname position client fname iname propertyforrent rentfinish city rent we use the terminology of the dbms oracle refers to a relation as a table with columns and rows oracle automatically adds an index for each primary key in addition oracle recommends that unique indexes not be explicitly defined on tables but instead unique integrity constraints be defined on the desired columns oracle enforces unique integrity constraints by automatically defining a unique index on the unique key exceptions to this recommendation are usually performance related for example using a create table as select with a unique constraint is slower than creating the table without the constraint and then manually creating a unique index assume that the tables are created with the identified primary alternate and for eign keys specified we now identify whether any clusters are required and whether any additional indexes are required to keep the design simple we will assume that clusters are not appropriate again considering just the query transactions listed in appendix a for the staffclient user views of dreamhome there may be performance benefits in adding the indexes shown in table again we leave it as an exercise for the reader to choose additional indexes to create in oracle for the transactions listed in appendix a for the branch view of dreamhome see exercise step estimate disk space requirements to estimate the amount of disk space that will be required by the database it may be a requirement that the physical database implementation can be handled by the current hardware configuration even if this is not the case the designer still table additional indexes to be created in oracle based on the query transactions for the staffclient user views of dreamhome table index staff fname iname supervisorstaffno position client staffno fname iname propertyforrent ownerno staffno clientno rentfinish city rent viewing clientno lease propertyno clientno has to estimate the amount of disk space that is required to store the database in case new hardware has to be procured the objective of this step is to estimate the amount of disk space that is required to support the database implementation on secondary storage as with the previous steps estimating the disk usage is highly dependent on the target dbms and the hardware used to support the database in general the estimate is based on the size of each tuple and the number of tuples in the relation the latter estimate should be a maximum number but it may also be worth considering how the relation will grow and modifying the resulting disk size by this growth factor to determine the potential size of the database in the future in appendix j see companion web site we illustrate the process for estimating the size of relations created in oracle step design user views to design the user views that were identified during the require ments collection and analysis stage of the database system develop ment lifecycle the first phase of the database design methodology presented in chapter involved the production of a conceptual data model for either the single user view or a number of combined user views identified during the requirements collection and analysis stage in section we identified four user views for dreamhome named director manager supervisor assistant and client following an analysis of the data requirements for these user views we used the centralized approach to merge the requirements for the user views as follows branch consisting of the director and manager user views staffclient consisting of the supervisor assistant and client user views in step the conceptual data model was mapped to a logical data model based on the relational model the objective of this step is to design the user views identi fied previously in a standalone dbms on a pc user views are usually a conveni ence to simplify database requests however in a multi user dbms user views play a central role in defining the structure of the database and enforcing security in section we discussed the major advantages of views such as data independ ence reduced complexity and customization we previously discussed how to create views using the iso sql standard section and how to create views stored queries in microsoft office access appendix m see companion web site document design of user views the design of the individual user views should be fully documented step design security mechanisms to design the security mechanisms for the database as specified by the users during the requirements and collection stage of the database system development lifecycle chapter summary a database represents an essential corporate resource and so security of this resource is extremely important during the requirements collection and analysis stage of the database system development lifecycle specific security requirements should have been documented in the system requirements specification see section 11 the objective of this step is to decide how these security requirements will be real ized some systems offer different security facilities than others again the database designer must be aware of the facilities offered by the target dbms as we discuss in chapter relational dbmss generally provide two types of database security system security data security system security covers access and use of the database at the system level such as a user name and password data security covers access and use of database objects such as relations and views and the actions that users can have on the objects again the design of access rules is dependent on the target dbms some systems provide more facilities than others for designing access rules we have previously discussed how to create access rules using the discretionary grant and revoke statements of the iso sql standard section we also show how to create access rules using microsoft office access appendix h and oracle appendix h we discuss security more fully in chapter document design of security measures the design of the security measures should be fully documented if the physical design affects the logical data model this model should also be updated chapter summary physical database design is the process of producing a description of the implementation of the database on secondary storage it describes the base relations and the storage structures and access methods used to access the data effectively along with any associated integrity constraints and security measures the design of the base relations can be undertaken only once the designer is fully aware of the facilities offered by the target dbms the initial step step of physical database design is the translation of the logical data model into a form that can be implemented in the target relational dbms the next step step designs the file organizations and access methods that will be used to store the base rela tions this involves analyzing the transactions that will run on the database choosing suitable file organizations based on this analysis choosing indexes and finally estimating the disk space that will be required by the implementation secondary indexes provide a mechanism for specifying an additional key for a base relation that can be used to retrieve data more efficiently however there is an overhead involved in the maintenance and use of secondary indexes that has to be balanced against the performance improvement gained when retrieving data one approach to selecting an appropriate file organization for a relation is to keep the tuples unordered and create as many secondary indexes as necessary another approach is to order the tuples in the relation by specifying a primary or clustering index one approach to determining which secondary indexes are needed is to produce a wish list of attributes that we consider are candidates for indexing and then to examine the impact of maintaining each of these indexes the objective of step is to design an implementation of the user views identified during the requirements collection and analysis stage such as using the mechanisms provided by sql a database represents an essential corporate resource so security of this resource is extremely important the objective of step is to design the realization of the security mechanisms identified during the requirements collection and analysis stage physical database design depends much on the logical and conceptual design discuss the validity of this statement discuss why it is important to analyze transactions before implementing the index describe the purpose of the main steps in the physical design methodology presented in this chapter describe the rule and how it is used in the physical database model the dreamhome case study in step we chose the indexes to create in microsoft office access for the query transactions listed in appendix a for the staffclient user views of dreamhome choose indexes to create in microsoft office access for the query transactions listed in appendix a for the branch user views of dreamhome repeat exercise using oracle as the target dbms create a physical database design for the logical design of the dreamhome case study described in chapter based on the dbms that you have access to implement this physical design for dreamhome created in exercise the university accommodation office case study 9 based on the logical data model developed in exercise create a physical database design for the university accommodation office case study described in appendix b i based on the dbms that you have access to implement the university accommodation office database using the physical design created in exercise 9 the easydrive school of motoring case study 11 based on the logical data model developed in exercise 11 create a physical database design for the easydrive school of motoring case study described in appendix b based on the dbms that you have access to 12 based on the physical design you just created for easydrive school of motoring describe critical performance challenges that can be foreseen the wellmeadows hospital case study based on the logical data model developed in exercise 13 create a physical database design for the wellmeadows hospital case study described in appendix b based on the dbms that you have access to 14 implement the wellmeadows hospital database using the physical design created in exercise 13 chapter methodology monitoring and tuning the operational system in the previous chapter we presented the first five steps of the physical database design methodology for relational databases in this chapter we describe and illus trate by example the final two steps of the physical database design methodology we provide guidelines for determining when to denormalize the logical data model and introduce redundancy and then discuss the importance of monitoring the operational system and continuing to tune it in places we show physical imple mentation details to clarify the discussion denormalizing and introducing controlled redundancy step consider the introduction of controlled redundancy to determine whether introducing redundancy in a controlled manner by relaxing the normalization rules will improve the per formance of the system as we discussed in chapter 14 and normalization is a technique for deciding which attributes belong together in a relation one of the basic aims of relational database design is to group attributes together in a relation because there is a func tional dependency between them the result of normalization is a logical database design that is structurally consistent and has minimal redundancy however it is sometimes argued that a normalized database design does not provide maximum processing efficiency consequently there may be circumstances in which it may be necessary to accept the loss of some of the benefits of a fully normalized design in favor of performance this should be considered only when it is estimated that the system will not be able to meet its performance requirements we are not advocat ing that normalization should be omitted from logical database design normaliza tion forces us to understand completely each attribute that has to be represented in the database this may be the most important factor that contributes to the overall success of the system in addition the following factors have to be considered denormalization makes implementation more complex denormalization often sacrifices flexibility denormalization may speed up retrievals but slows down updates formally the term denormalization refers to a refinement to the relational schema such that the degree of normalization for a modified relation is less than the degree of at least one of the original relations we also use the term more loosely to refer to situations in which we combine two relations into one new relation and the new relation is still normalized but contains more nulls than the original relations some authors refer to denormalization as usage refinement as a general rule of thumb if performance is unsatisfactory and a relation has a low update rate and a very high query rate denormalization may be a viable option the transaction relation cross reference matrix that may have been produced in step provides useful information for this step the matrix summarizes in a visual way the access patterns of the transactions that will run on the database it can be used to highlight possible candidates for denormalization and to assess the effects this would have on the rest of the model more specifically in this step we consider duplicating certain attributes or join ing relations together to reduce the number of joins required to perform a query indirectly we have encountered an implicit example of denormalization when dealing with address attributes for example consider the definition of the branch relation branch branchno street city postcode mgrstaffno strictly speaking this relation is not in third normal form postcode the post or zip code functionally determines city in other words we can determine the value of the city attribute given a value for the postcode attribute hence the branch relation is in to normalize the relation to it would be necessary to split the rela tion into two as follows branch branchno street postcode mgrstaffno postcode postcode city however we would rarely wish to access the branch address without the city attrib ute this would mean that we would have to perform a join whenever we want a complete address for a branch as a result we settle for and implement the original branch relation unfortunately there are no fixed rules for determining when to denormalize relations in this step we discuss some of the more common situations for consider ing denormalization for additional information the interested reader is referred to rogers and fleming and von halle in particular we consider denormalization in the following situations specifically to speed up frequent or critical transactions step combining one to one relationships step duplicating non key attributes in one to many relationships to reduce joins step duplicating foreign key attributes in one to many relationships to reduce joins step duplicating attributes in many to many relationships to reduce joins step introducing repeating groups step 6 creating extract tables step partitioning relations to illustrate these steps we use the relation diagram shown in figure a and the sample data shown in figure b figure a sample relation diagram figure b sample relations figure combined client and interview a revised extract from the relation diagram b combined relation step combining one to one relationships re examine one to one relationships to determine the effects of combining the relations into a single relation combination should be considered only for relations that are frequently referenced together and infrequently referenced sepa rately consider for example the relationship between client and interview as shown in figure the client relation contains information on potential renters of property the interview relation contains the date of the interview and comments made by a member of staff about a client we could combine these two relations together to form a new relation clientlnterview as shown in figure because the relationship between client and interview is and the participation is optional there may be a significant number of nulls in the combined relation clientlnterview depending on the proportion of tuples involved in the participation as shown in figure b if the original client relation is large and the proportion of tuples involved in the participation is small there will be a significant amount of wasted space step duplicating non key attributes in one to many relationships to reduce joins with the specific aim of reducing or removing joins from frequent or critical queries consider the benefits that may result in duplicating one or more non key attributes of the parent relation in the child relation in a relationship for example whenever the propertyforrent relation is accessed it is very common for the owner name to be accessed at the same time a typical sql query would be select p o lname from propertyforrent p privateowner o where p ownerno o ownerno and branchno b003 figure revised propertyforrent relation with duplicated iname attribute from the privateowner relation based on the original relation diagram and sample relations shown in figure if we duplicate the iname attribute in the propertyforrent relation we can remove the privateowner relation from the query which in sql becomes select p from propertyforrent p where branchno b003 based on the revised relation shown in figure the benefits that result from this change have to be balanced against the prob lems that may arise for example if the duplicated data is changed in the parent relation it must be updated in the child relation further for a relationship there may be multiple occurrences of each data item in the child relation for example the names farrel and shaw both appear twice in the revised propertyforrent relation in which case it becomes necessary to maintain consistency of multiple copies if the update of the iname attribute in the privateowner and propertyforrent relation cannot be automated the potential for loss of integrity is considerable an associated problem with duplication is the additional time that is required to main tain consistency automatically every time a tuple is inserted updated or deleted in our case it is unlikely that the name of the owner of a property will change so the duplication may be warranted another problem to consider is the increase in storage space resulting from the duplication again with the relatively low cost of secondary storage nowadays this may not be so much of a problem however this is not a justification for arbitrary duplication a special case of a one to many relationship is a lookup table sometimes called a reference table or pick list typically a lookup table contains a code and a description for example we may define a lookup parent table for property type and modify the propertyforrent child table as shown in figure the advan tages of using a lookup table are reduction in the size of the child relation the type code occupies byte as opposed to bytes for the type description if the description can change which is not the case in this particular example it is easier changing it once it in the lookup table as opposed to changing it many times in the child relation the lookup table can be used to validate user input figure lookup table for property type a relation diagram b sample relations if the lookup table is used in frequent or critical queries and the description is unlikely to change consideration should be given to duplicating the description attribute in the child relation as shown in figure the original lookup table is not redundant it can still be used to validate user input however by duplicating the description in the child relation we have eliminated the need to join the child relation to the lookup table figure modified propertyforrent relation with duplicated description attribute step duplicating foreign key attributes in one to many relationships to reduce joins again with the specific aim of reducing or removing joins from frequent or critical queries consider the benefits that may result in duplicating one or more of the for eign key attributes in a relationship for example a frequent query for dreamhome is to list all the private property owners at a branch using an sql query of the following form select o lname from propertyforrent p privateowner o where p ownerno o ownerno and branchno b003 based on the original data shown in figure in other words because there is no direct relationship between privateowner and branch then to get the list of owners we have to use the propertyforrent relation to gain access to the branch number branchno we can remove the need for this join by duplicating the foreign key branchno in the privateowner relation that is we introduce a direct relationship between the branch and privateowner relations in this case we can simplify the sql query to select o lname from privateowner o where branchno b003 based on the revised relation diagram and privateowner relation shown in figure 6 if this change is made it will be necessary to introduce additional foreign key con straints as discussed in step figure 6 duplicating the foreign key branchno in the privateowner relation a revised simplified relation diagram with branchno included as a foreign key b revised privateowner relation if an owner could rent properties through many branches the previous change would not work in this case it would be necessary to model a many to many relationship between branch and privateowner note also that the propertyforrent rela tion has the branchno attribute because it is possible for a property not to have a member of staff allocated to it particularly at the start when the property is first taken on by the agency if the propertyforrent relation did not have the branch number it would be necessary to join the propertyforrent relation to the staff relation based on the staffno attribute to get the required branch number the original sql query would then become select o lname from staff propertyforrent p privateowner o where staffno p staffno and p ownerno o ownerno and branchno b003 removing two joins from the query may provide greater justification for creating a direct relationship between privateowner and branch and thereby duplicating the foreign key branchno in the privateowner relation step duplicating attributes in many to many relationships to reduce joins during logical database design we mapped each relationship into three rela tions the two relations derived from the original entities and a new relation rep resenting the relationship between the two entities now if we wish to produce information from the relationship we have to join these three relations in some circumstances it may be possible to reduce the number of relations to be joined by duplicating attributes from one of the original entities in the intermediate relation for example the relationship between client and propertyforrent has been decomposed by introducing the intermediate viewing relation consider the require ment that the dreamhome sales staff should contact clients who have still to make a comment on the properties they have viewed however the sales staff need only the street attribute of the property when talking to the clients the required sql query is select p street c v viewdate from client c viewing v propertyforrent p where v propertyno p propertyno and c clientno v clientno and comment is null based on the relation model and sample data shown in figure if we dupli cate the street attribute in the intermediate viewing relation we can remove the propertyforrent relation from the query giving the sql query select c v street v viewdate from client c viewing v where c clientno v clientno and comment is null based on the revised viewing relation shown in figure step introducing repeating groups repeating groups were eliminated from the logical data model as a result of the requirement that all entities be in first normal form repeating groups were figure duplicating the street attribute from the propertyforrent relation in the viewing relation separated out into a new relation forming a relationship with the original par ent relation occasionally reintroducing repeating groups is an effective way to improve system performance for example each dreamhome branch office has a maximum of three telephone numbers although all offices do not necessarily have the same number of lines in the logical data model we created a telephone entity with a three to one relationship with branch resulting in two relations as shown in figure if access to this information is important or frequent it may be more efficient to combine the relations and store the telephone details in the original branch relation with one attribute for each telephone as shown in figure in general this type of denormalization should be considered only in the follow ing circumstances the absolute number of items in the repeating group is known in this example there is a maximum of three telephone numbers the number is static and will not change over time the maximum number of telephone lines is fixed and is not expected to change the number is not very large typically not greater than although this is not as important as the first two conditions figure branch incorporating repeating group a revised relation diagram b revised relation sometimes it may be only the most recent or current value in a repeating group or just the fact that there is a repeating group that is needed most frequently in the previous example we may choose to store one telephone number in the branch relation and leave the remaining numbers for the telephone relation this would remove the presence of nulls from the branch relation as each branch must have at least one telephone number step 6 creating extract tables there may be situations where reports have to be run at peak times during the day these reports access derived data and perform multirelation joins on the same set of base relations however the data the report is based on may be relatively static or in some cases may not have to be current that is if the data is a few hours old the report would be perfectly acceptable in this case it may be possible to create a single highly denormalized extract table based on the relations required by the reports and allow the users to access the extract table directly instead of the base relations the most common technique for producing extract tables is to create and populate the tables in an overnight batch run when the system is lightly loaded step partitioning relations rather than combining relations together an alternative approach that addresses the key problem with supporting very large relations and indexes is to decompose them into a number of smaller and more manageable pieces called partitions as illustrated in figure 9 there are two main types of partitioning horizontal par titioning and vertical partitioning distributes the tuples of a relation across a number of smaller relations distributes the attributes of a relation across a number of smaller relations the primary key is duplicated to allow the original relation to be reconstructed figure 9 horizontal and vertical partitioning figure oracle sql statement to create a hash partition create table archivedpropertyforrentpartition propertyno not null street not null city not null postcode type char not null rooms smallint not null rent number 6 not null ownerno not null staffno branchno char not null primary key propertyno foreign key ownerno references privateowner ownerno foreign key staffno references staff staffno foreign key branchno references branch branchno partition by hash branchno partition tablespace partition tablespace partition tablespace partition tablespace partitions are particularly useful in applications that store and analyze large amounts of data for example dreamhome maintains an archivedpropertyforrent relation with several hundreds of thousands of tuples that are held indefinitely for analysis purposes searching for a particular tuple at a branch could be quite time consuming however we could reduce this time by horizontally partitioning the relation with one partition for each branch we can create a hash partition for this scenario in oracle using the sql statement shown in figure as well as hash partitioning other common types of partitioning are range each partition is defined by a range of values for one or more attributes and list each partition is defined by a list of values for an attribute there are also com posite partitions such as range hash and list hash each partition is defined by a range or a list of values and then each partition is further subdivided based on a hash function there may also be circumstances in which we frequently examine particular attributes of a very large relation and it may be appropriate to vertically partition the relation into those attributes that are frequently accessed together and another vertical partition for the remaining attributes with the primary key replicated in each partition to allow the original relation to be reconstructed using a join partitioning has a number of advantages improved load balancing partitions can be allocated to different areas of secondary storage thereby permitting parallel access while at the same time minimizing the contention for access to the same storage area if the relation was not partitioned improved performance by limiting the amount of data to be examined or pro cessed and by enabling parallel execution performance can be enhanced increased availability if partitions are allocated to different storage areas and one storage area were to become unavailable the other partitions would still be available improved recovery smaller partitions can be recovered more efficiently equally well the dba may find backing up smaller partitions easier than backing up very large relations security data in a partition can be restricted to those users who require access to it with different partitions having different access restrictions partitioning can also have a number of disadvantages complexity partitioning is not usually transparent to end users and queries that utilize more than one partition become more complex to write reduced performance queries that combine data from more than one partition may be slower than a nonpartitioned approach duplication vertical partitioning involves duplication of the primary key this leads not only to increased storage requirements but also to potential inconsisten cies arising consider implications of denormalization consider the implications of denormalization on the previous steps in the method ology for example it may be necessary to reconsider the choice of indexes on the relations that have been denormalized to establish whether existing indexes should be removed or additional indexes added in addition it will be necessary to consider how data integrity will be maintained common solutions are triggers triggers can be used to automate the updating of derived or duplicated data see section transactions build into each application transactions that make the updates to denormalized data as a single atomic action batch reconciliation run batch programs at appropriate times to make the denor malized data consistent in terms of maintaining integrity triggers provide the best solution although they can cause performance problems the advantages and disadvantages of denor malization are summarized in table table advantages and disadvantages of denormalization advantages disadvantages can improve performance by may speed up retrievals but can slow down updates precomputing derived data always application specific and needs to be re evaluated if the application changes minimizing the need for joins can increase the size of relations reducing the number of foreign keys in relations may simplify implementation in some cases but may make it more complex in others reducing the number indexes thereby saving storage space sacrifices flexibility reducing the number of relations document introduction of redundancy the introduction of redundancy should be fully documented along with the reasons for introducing it in particular document the reasons for selecting one approach where many alternatives exist update the logical data model to reflect any changes made as a result of denormalization monitoring the system to improve performance step monitor and tune the operational system to monitor the operational system and improve the performance of the system to correct inappropriate design decisions or reflect changing requirements for this activity we should remember that one of the main objectives of physical database design is to store and access data in an efficient way see appendix f there are a number of factors that we may use to measure efficiency transaction throughput this is the number of transactions that can be processed in a given time interval in some systems such as airline reservations high transac tion throughput is critical to the overall success of the system response time this is the elapsed time for the completion of a single transac tion from a user point of view we want to minimize response time as much as possible however there are some factors that influence response time that the designer may have no control over such as system loading or communication times response time can be shortened by reducing contention and wait times particularly disk i o wait times reducing the amount of time for which resources are required using faster components disk storage this is the amount of disk space required to store the database files the designer may wish to minimize the amount of disk storage used however there is no one factor that is always correct typically the designer must trade one factor off against another to achieve a reasonable balance for example increasing the amount of data stored may decrease the response time or transac tion throughput the initial physical database design should not be regarded as static but should be considered to be an estimate of how the operational system might perform once the initial design has been implemented it will be neces sary to monitor the system and tune it as a result of observed performance and changing requirements see step many dbmss provide the dba with utilities to monitor and tune the operation of the system there are many benefits to be gained from tuning the database tuning can avoid the procurement of additional hardware it may be possible to downsize the hardware configuration this results in less and cheaper hardware and consequently less expensive maintenance a well tuned system produces faster response times and better throughput which in turn makes the users and hence the organization more productive improved response times can improve staff morale improved response times can increase customer satisfaction these last two benefits are less tangible than the others however we can certainly state that slow response times may demoralize staff and potentially lose customers to tune an operational system the physical database designer must be aware of how the various hardware components interact and affect database performance as we now discuss understanding system resources main memory main memory accesses are significantly faster than secondary stor age accesses sometimes tens or even hundreds of thousands of times faster in gen eral the more main memory available to the dbms and the database applications the faster the applications will run however it is sensible always to have a minimum of of main memory available it is also advisable not to have any more than available otherwise main memory is not being used optimally when there is insuf ficient memory to accommodate all processes the operating system transfers pages of processes to disk to free up memory when one of these pages is next required the operating system has to transfer it back from disk sometimes it is necessary to swap entire processes from memory to disk and back again to free up memory problems occur with main memory when paging or swapping becomes excessive to ensure efficient usage of main memory it is necessary to understand how the target dbms uses main memory what buffers it keeps in main memory what parameters exist to allow the size of the buffers to be adjusted and so on for exam ple oracle keeps a data dictionary cache in main memory that ideally should be large enough to handle of data dictionary accesses without having to retrieve the information from disk it is also necessary to understand the access patterns of users an increase in the number of concurrent users accessing the database will result in an increase in the amount of memory being utilized cpu the cpu controls the tasks of the other system resources and executes user processes and is the most costly resource in the system so it needs to be correctly utilized the main objective for this component is to prevent cpu contention in which processes are waiting for the cpu cpu bottlenecks occur when either the operating system or user processes make too many demands on the cpu this is often a result of excessive paging it is necessary to understand the typical workload through a hour period and ensure that sufficient resources are available for not only the normal workload but also the peak workload for example if the system has cpu utilization and idle during the normal workload then there may not be sufficient scope to handle the peak workload one option is to ensure that during peak load no unnecessary jobs are being run and that such jobs are instead run in off hours another option may be to consider multiple cpus which allows the processing to be distributed and operations to be performed in parallel cpu mips millions of instructions per second can be used as a guide in com paring platforms and determining their ability to meet the enterprise through put requirements disk i o with any large dbms there is a significant amount of disk i o involved in storing and retrieving data disks usually have a recommended i o rate and figure 11 typical disk configuration when this rate is exceeded i o bottlenecks occur although cpu clock speeds have increased dramatically in recent years i o speeds have not increased proportion ately the way in which data is organized on disk can have a major impact on the overall disk performance one problem that can arise is disk contention this occurs when multiple processes try to access the same disk simultaneously most disks have limits on both the number of accesses and the amount of data they can transfer per second and when these limits are reached processes may have to wait to access the disk to avoid this it is recommended that storage should be evenly distributed across available drives to reduce the likelihood of performance prob lems occurring figure 11 illustrates the basic principles of distributing the data across disks the operating system files should be separated from the database files the main database files should be separated from the index files the recovery log file see section should be separated from the rest of the database if a disk still appears to be overloaded one or more of its heavily accessed files can be moved to a less active disk this is known as distributing i o load balancing can be achieved by applying this principle to each of the disks until they all have approximately the same amount of i o once again the physical database designer needs to understand how the dbms operates the characteristics of the hardware and the access patterns of the users disk i o has been revolutionized with the introduction of raid redundant array of independent disks technology raid works on having a large disk array comprising an arrangement of several independent disks that are organized to increase performance and at the same time improve reliability we discuss raid in section network when the amount of traffic on the network is too great or when the number of network collisions is large network bottlenecks occur each of the previous resources may affect other system resources additionally an improvement in one resource may effect an improvement in other system resources for example procuring more main memory should result in less paging which should help avoid cpu bottlenecks more effective use of main memory may result in less disk i o summary tuning is an activity that is never complete throughout the life of the system it will be necessary to monitor performance particularly to account for changes in the environment and user requirements however making a change to one area of an operational system to improve performance may have an adverse effect on another area for example adding an index to a relation may improve the performance of one transaction but it may adversely affect another perhaps more important transaction therefore care must be taken when making changes to an operational system if possible test the changes either on a test database or alter natively when the system is not being fully used such as outside of working hours document tuning activity the mechanisms used to tune the system should be fully documented along with the reasons for tuning it in the closen way in particular document the reasons for selecting one opproach where many alternatives exist new requirement for dreamhome as well as tuning the system to maintain optimal performance it may also be nec essary to handle changing requirements for example suppose that after some months as a fully operational database several users of the dreamhome system raise two new requirements ability to hold pictures of the properties for rent together with comments that describe the main features of the property in microsoft office access we are able to accommodate this request using ole object linking and embedding fields which are used to store data such as microsoft word or microsoft excel documents pictures sound and other types of binary data created in other programs ole objects can be linked to or embedded in a field in a microsoft office access table and then displayed in a form or report to implement this new requirement we restructure the propertyforrent table to include a a field called picture specified as an ole data type this field holds graphical images of properties created by scanning photographs of the properties for rent and saving the images as bmp bitmapped graphic files b a field called comments specified as a memo data type capable of storing lengthy text a form based on some fields of the propertyforrent table including the new fields is shown in figure 12 the main problem associated with the storage of graphic images is the large amount of disk space required to store the image files we would therefore need to continue to monitor the performance of the dreamhome database to ensure that satisfying this new requirement does not compromise the system performance ability to publish a report describing properties available for rent on the web this requirement can be accommodated in both microsoft office access and oracle as both dbmss provide many features for developing a web application and publishing on the internet however to use these features we require a web browser such as microsoft internet explorer or mozilla firefox and a modem or other network connection to access the internet in chapter we describe in detail the technologies used in the integration of databases and the web figure 12 form based on propertyforrent table with new picture and comments fields enable property owners to access details of their properties and comments made by pro spective clients on the web this requirement is similar to the previous requirement in terms of technology but also requires that the database is restructured to allow for the storage of a property owner login details which include email and password once logged in a property owner will be able to view only the details relating to their prop erties the client relation restructured to store a property owner login details is shown in figure exercises performance is unsatisfactory and a relation has a low update rate and a very high query rate denormalization may be a viable option the final step step of physical database design is the ongoing process of monitoring and tuning the operational system to achieve maximum performance one of the main objectives of physical database design is to store and access data in an efficient way there are a number of factors that can be used to measure efficiency including throughput response time and disk storage to improve performance it is necessary to be aware of how the following four basic hardware components interact and affect system performance main memory cpu disk i o and network describe the purpose of the main steps in the physical design methodology presented in this chapter under what circumstances would you want to denormalize a logical data model use examples to illustrate your answer what factors can be used to measure efficiency discuss how partitioning and denormalization improve query efficiency what are the benefits of partitioning a relation 6 analyze the conceptual and logical model for the dreamhome case study presented in figures a and b respectively suggest areas that require either denormalization or partitioning make any necessary constructive assumptions to justify your case part chapter security and administration chapter professional legal and ethical issues in data management chapter transaction management chapter query processing ch apte r security and administration data is a valuable resource that must be strictly controlled and managed as must any corporate resource part or all of the corporate data may have strategic impor tance to an organization and should therefore be kept secure and confidential in chapter we discussed the database environment and in particular the typi cal functions and services of a database management system dbms these func tions and services include authorization services such that a dbms must furnish a mechanism to ensure that only authorized users can access the database in other words the dbms must ensure that the database is secure the term security refers to the protection of the database against unauthorized access either intentional or accidental besides the services provided by the dbms discussions on database security could also include broader issues associated with securing the database and its environment however these issues are outside the scope of this book and the interested reader is referred to pfleeger database security in this section we describe the scope of database security and discuss why organiza tions must take seriously potential threats to their computer systems we also iden tify the range of threats and their consequences on computer systems the mechanisms that protect the database against intentional or accidental threats security considerations apply to not just the data held in a database breaches of security may affect other parts of the system which may in turn affect the database consequently database security encompasses hardware software people and data to effectively implement security requires appropriate controls which are defined in specific mission objectives for the system this need for security though often neglected or overlooked in the past is now increasingly recognized by organiza tions the reason for this turnaround is the increasing amounts of crucial corporate data being stored on computer and the acceptance that any loss or unavailability of this data could prove to be disastrous a database represents an essential corporate resource that should be properly secured using appropriate controls we consider database security in relation to the following situations theft and fraud loss of confidentiality secrecy loss of privacy loss of integrity loss of availability these situations broadly represent areas in which the organization should seek to reduce risk that is the possibility of incurring loss or damage in some situations these areas are closely related such that an activity that leads to loss in one area may also lead to loss in another in addition events such as fraud or loss of privacy 1 database security may arise because of either intentional or unintentional acts and do not necessarily result in any detectable changes to the database or the computer system theft and fraud affect not only the database environment but also the entire organization as it is people who perpetrate such activities attention should focus on reducing the opportunities for this occurring theft and fraud do not necessar ily alter data as is the case for activities that result in either loss of confidentiality or loss of privacy confidentiality refers to the need to maintain secrecy over data usually only data that is critical to the organization whereas privacy refers to the need to pro tect data about individuals breaches of security resulting in loss of confidentiality could for instance lead to loss of competitiveness and loss of privacy could lead to legal action being taken against the organization loss of data integrity results in invalid or corrupted data which may seriously affect the operation of an organization many organizations are now seeking virtually continuous operation the so called availability that is hours a day days a week loss of availability means that the data or the system or both cannot be accessed which can seriously affect an organization financial performance in some cases events that cause a system to be unavailable may also cause data corruption database security aims to minimize losses caused by anticipated events in a cost effective manner without unduly constraining the users in recent times computer based criminal activities have significantly increased and are forecast to continue to rise over the next few years 1 1 threats any situation or event whether intentional or accidental that may adversely affect a system and consequently the organization a threat may be caused by a situation or event involving a person action or circum stance that is likely to bring harm to an organization the harm may be tangible such as loss of hardware software or data or intangible such as loss of credibility or client confidence the problem facing any organization is to identify all possible threats therefore as a minimum an organization should invest time and effort in identifying the most serious threats in the previous section we identified areas of loss that may result from intentional or unintentional activities although some types of threat can be either intentional or unintentional the impact remains the same intentional threats involve people and may be perpetrated by both authorized users and unauthorized users some of whom may be external to the organization any threat must be viewed as a potential breach of security that if successful will have a certain impact table 1 presents examples of various types of threat listed under the area on which they may have an impact for example viewing and disclosing unauthorized data as a threat may result in theft and fraud loss of confidentiality and loss of privacy for the organization the extent that an organization suffers as a result of a threat succeeding depends upon a number of factors such as the existence of countermeasures and contingency plans for example if a hardware failure occurs corrupting secondary storage all processing activity must cease until the problem is resolved the recovery will table 1 examples of threats threat theft and fraud loss of confidentiality loss of privacy loss of integrity loss of availability using another person means of access unauthorized amendment or copying of data program alteration inadequate policies and procedures that allow a mix of confidential and normal output wire tapping illegal entry by hacker blackmail creating trapdoor into system theft of data programs and equipment failure of security mechanisms giving greater access than normal staff shortages or strikes inadequate staff training viewing and disclosing unauthorized data electronic interference and radiation data corruption owing to power loss or surge fire electrical fault lightning strike arson flood bomb physical damage to equipment breaking cables or disconnection of cables introduction of viruses depend upon a number of factors which include when the last backups were taken and the time needed to restore the system an organization needs to identify the types of threat that it may be subjected to and to initiate appropriate plans and countermeasures bearing in mind the costs of implementing them obviously it may not be cost effective to spend considerable time effort and money on potential threats that may result only in figure 1 summary of potential threats to computer systems minor inconvenience the organization business may also influence the types of threat that should be considered some of which may be rare however rare events should be taken into account particularly if their impact would be signifi cant a summary of the potential threats to computer systems is represented in figure 1 countermeasures computer based controls the types of countermeasure to threats on computer systems range from physical controls to administrative procedures despite the range of computer based controls that are available it is worth noting that generally the security of a dbms is only as good as that of the operating system owing to their close association representation of a typical multi user computer environment is shown in figure in this section figure representation of a typical multi user computer environment we focus on the following computer based security controls for a multi user environ ment some of which may not be available in the pc environment authorization access controls views backup and recovery integrity encryption and raid technology 1 authorization the granting of a right or privilege that enables a subject to have legitimate access to a system or a system object authorization controls can be built into the software and govern not only what system or object a specified user can access but also what the user may do with it the process of authorization involves authentication of subjects requesting access to objects where subject represents a user or program and object represents a database table view procedure trigger or any other object that can be created within the system a mechanism that determines whether a user is who he or she claims to be a system administrator is usually responsible for allowing users to have access to a computer system by creating individual user accounts each user is given a unique identifier which is used by the operating system to determine who they are associated with each identifier is a password chosen by the user and known to the operating system which must be supplied to enable the operating system to verify or authenticate who the user claims to be this procedure allows authorized use of a computer system but does not neces sarily authorize access to the dbms or any associated application programs a separate similar procedure may have to be undertaken to give a user the right to use the dbms the responsibility to authorize use of the dbms usually rests with the database administrator dba who must also set up individual user accounts and passwords using the dbms itself some dbmss maintain a list of valid user identifiers and associated passwords which can be distinct from the operating system list however other dbmss main tain a list whose entries are validated against the operating system list based on the current user login identifier this prevents a user from logging on to the dbms with one name having already logged on to the operating system using a different name cloud computing services describes the varied forms of computing software or hardware resources that are delivered over a network and accessed typically from a web browser or mobile application see section this technology underpins many internet based business and consumer products and services and typically requires complex infrastructure that provide remote access to data software or computation cloud computing is increasingly being used by companies as a way of providing and accessing scalable software applications more conveniently at lower risk cost and as a way of lowering the cost of overall investment in it infrastructure for many web and cloud based services depending on the level of security the process of authentication can typically be automated with or without computer human intervention for user account validation using email or other internet services to ensure that individuals are who they say they are and the appropriate account is created online banking services financial retail other online retail cor porate applications products and server services as well as e government services follow varying combinations of account validation access controls the typical way to provide access controls for a database system is based on the granting and revoking of privileges a privilege allows a user to create or access that is read write or modify some database object such as a relation view or index or to run certain dbms utilities privileges are granted to users to accom plish the tasks required for their jobs as excessive granting of unnecessary privi leges can compromise security a privilege should be granted to a user only if that user cannot accomplish his or her work without that privilege a user who creates a database object such as a relation or a view automatically gets all privileges on that object the dbms subsequently keeps track of how these privileges are granted to other users and possibly revoked and ensures that at all times only users with necessary privileges can access an object discretionary access control dac most commercial dbmss provide an approach to managing privileges that uses sql called discretionary access control dac the sql standard supports dac through the grant and revoke commands the grant command gives privi leges to users and the revoke command takes away privileges we discussed how the sql standard supports discretionary access control in section 6 discretionary access control though effective has certain weaknesses in particular an unauthorized user can trick an authorized user into disclosing sensitive data for example an unauthorized user such as an assistant in the dreamhome case study can create a relation to capture new client details and give access privileges to an authorized user such as a manager without their knowledge the assistant can then alter some application programs that the manager uses to include some hidden instruction to copy sensitive data from the client relation that only the manager has access to into the new relation created by the assistant the unauthorized user namely the assistant now has a copy of the sensitive data namely new clients of dreamhome and to cover up his or her actions now modifies the altered application programs back to the original form clearly an additional security approach is required to remove such loopholes and this requirement is met in an approach called mandatory access control mac which we discuss in detail next although discretionary access control is typically provided by most commercial dbmss only some also provide support for mandatory access control mandatory access control mac mandatory access control mac is based on system wide policies that cannot be changed by individual users in this approach each database object is assigned a security class and each user is assigned a clearance for a security class and rules are imposed on reading and writing of database objects by users the dbms deter mines whether a given user can read or write a given object based on certain rules that involve the security level of the object and the clearance of the user these rules seek to ensure that sensitive data can never be passed on to another user with out the necessary clearance the sql standard does not include support for mac a popular model for mac is called bell lapadula model bell and lapadula which is described in terms of objects such as relations views tuples and attributes subjects such as users and programs security classes and clearances each database object is assigned a security class and each subject is assigned a clear ance for a security class the security classes in a system are ordered with a most secure class and a least secure class for our discussion of the model we assume that there are four classes top secret ts secret s confidential c and unclassified u and we denote the class of an object or subject a as class a therefore for this system ts s c u where a b means that class a data has a higher security level than class b data the bell lapadula model imposes two restrictions on all reads and writes of database objects 1 simple security property subject s is allowed to read object o only if class s class o for example a user with ts clearance can read a relation with c clearance but a user with c clearance cannot read a relation with ts classification _property subject s is allowed to write object o only if class s class o for example a user with s clearance can only write objects with s or ts classification if discretionary access controls are also specified these rules represent additional restrictions thus to read or write a database object a user must have the necessary privileges provided through the sql grant command see section 6 and the security classes of the user and the object must satisfy the restrictions given previously multilevel relations and polyinstantiation in order to apply mandatory access control policies in a relational dbms a secu rity class must be assigned to each database object the objects can be at the granu larity of relations tuples or even individual attribute values assume that each tuple is assigned a security class this situation leads to the concept of a multilevel relation which is a relation that reveals different tuples to users with different security clearances for example the client relation with an additional attribute displaying the secu rity class for each tuple is shown in figure a users with s and ts clearance will see all tuples in the client relation however a user with c clearance will see only the first two tuples and a user with u clear ance will see no tuples at all assume that a user with clearance c wishes to enter a tuple david sinclaire into the client relation where the primary key of the relation is clientno this insertion is disallowed because it violates the primary key constraint see section for this relation however the inability to insert this new tuple informs the user with clearance c that a tuple exists with a primary key value of at a higher security class than c this compromises the security requirement that users should not be able to infer any information about objects that have a higher security classification this problem of inference can be solved by including the security classifica tion attribute as part of the primary key for a relation in the previous example the insertion of the new tuple into the client relation is allowed and the relation instance is modified as shown in figure b users with clearance c see the first two tuples and the newly added tuple but users with clearance s or ts see all five tuples the result is a relation with two tuples with a clientno of which can be confusing this situation may be dealt with by assuming that the tuple with the higher classification takes priority over the other or by revealing a single tuple only according to the user clearance the presence of data objects that appear to have different values to users with different clearances is called polyinstantiation figure a the client relation with an additional attribute displaying the security class for each tuple figure b the client relation with two tuples displaying clientno as the primary key for this relation is clientno securityclass although mandatory access control does address a major weakness of discretionary access control a major disadvantage of mac is the rigidity of the mac environment for example mac policies are often established by database or systems administra tors and the classification mechanisms are sometimes considered to be inflexible views a view is the dynamic result of one or more relational operations operat ing on the base relations to produce another relation a view is a virtual relation that does not actually exist in the database but is produced upon request by a particular user at the time of request the view mechanism provides a powerful and flexible security mechanism by hid ing parts of the database from certain users the user is not aware of the existence of any attributes or rows that are missing from the view a view can be defined over several relations with a user being granted the appropriate privilege to use it but not to use the base relations in this way using a view is more restrictive than sim ply having certain privileges granted to a user on the base relation we discussed views in detail in sections and backup and recovery the process of periodically copying of the database and log file and possibly programs to offline storage media a dbms should provide backup facilities to assist with the recovery of a database following failure it is always advisable to make backup copies of the database and log file at regular intervals and to ensure that the copies are in a secure location in the event of a failure that renders the database unusable the backup copy and the details captured in the log file are used to restore the database to the latest pos sible consistent state a description of how a log file is used to restore a database is described in more detail in section 3 3 the process of keeping and maintaining a log file or journal of all changes made to the database to enable recovery to be undertaken effectively in the event of a failure a dbms should provide logging facilities sometimes referred to as journaling which keep track of the current state of transactions and database changes to pro vide support for recovery procedures the advantage of journaling is that in the event of a failure the database can be recovered to its last known consistent state using a backup copy of the database and the information contained in the log file if no journaling is enabled on a failed system the only means of recovery is to restore the database using the latest backed up version of the database however without a log file any changes made after the last backup to the database will be lost the process of journaling is discussed in more detail in section 3 3 integrity integrity constraints also contribute to maintaining a secure database system by preventing data from becoming invalid and hence giving misleading or incorrect results integrity constraints were discussed in detail in section 3 6 encryption the encoding of the data by a special algorithm that renders the data unreadable by any program without the decryption key if a database system holds particularly sensitive data it may be deemed necessary to encode it as a precaution against possible external threats or attempts to access it some dbmss provide an encryption facility for this purpose the dbms can access the data after decoding it although there is a degradation in performance because of the time taken to decode it encryption also protects data transmitted over communication lines there are a number of techniques for encoding data to conceal the information some are termed irreversible and others reversible irreversible techniques as the name implies do not permit the original data to be known however the data can be used to obtain valid statistical information reversible techniques are more commonly used to transmit data securely over insecure networks requires the use of a cryptosystem which includes an encryption key to encrypt the data plaintext an encryption algorithm that with the encryption key transforms the plaintext into ciphertext a decryption key to decrypt the ciphertext a decryption algorithm that with the decryption key transforms the ciphertext back into plaintext one technique called symmetric encryption uses the same key for both encryp tion and decryption and relies on safe communication lines for exchanging the key however most users do not have access to a secure communication line and to be really secure the keys need to be as long as the message leiss however most working systems are based on user keys shorter than the message one scheme used for encryption is the data encryption standard des which is a stand ard encryption algorithm developed by ibm this scheme uses one key for both encryption and decryption which must be kept secret although the algorithm need not be the algorithm transforms each bit block of plaintext using a bit key the des is not universally regarded as being very secure and some authors main tain that a larger key is required for example a scheme called pgp pretty good privacy uses a bit symmetric algorithm for bulk encryption of the data it sends keys with bits are now probably breakable by major governments with special hardware albeit at substantial cost however this technology will be within the reach of organized criminals major organizations and smaller governments in a few years although it is envisaged that keys with bits will also become breakable in the future it is probable that keys with bits will remain unbeakable for the foreseeable future the terms strong authentication and weak authentication are sometimes used to distinguish between algorithms that to all intents and purposes cannot be bro ken with existing technologies and knowledge strong from those that can be weak another type of cryptosystem uses different keys for encryption and decryption and is referred to as asymmetric encryption one example is public key cryp tosystems which use two keys one of which is public and the other private the encryption algorithm may also be public so that anyone wishing to send a user a message can use the user publicly known key in conjunction with the algorithm to encrypt it only the owner of the private key can then decipher the message public key cryptosystems can also be used to send a digital signature with a message and prove that the message came from the person who claimed to have sent it the most well known asymmetric encryption is rsa the name is derived from the initials of the three designers of the algorithm generally symmetric algorithms are much faster to execute on a computer than those that are asymmetric however in practice they are often used together so that a public key algorithm is used to encrypt a randomly generated encryption key and the random key is used to encrypt the actual message using a symmetric algorithm we discuss encryption in the context of the web in section raid redundant array of independent disks the hardware that the dbms is running on must be fault tolerant meaning that the dbms should continue to operate even if one of the hardware components fails this suggests having redundant components that can be seamlessly integrated into the working system whenever there is one or more component failures the main hard ware components that should be fault tolerant include disk drives disk controllers cpu power supplies and cooling fans disk drives are the most vulnerable compo nents with the shortest times between failure of any of the hardware components one solution is the use of redundant array of independent disks raid technology raid originally stood for redundant array of inexpensive disks but more recently the i in raid has come to stand for independent raid works on having a large disk array comprising an arrangement of several independent disks that are organized to improve reliability and at the same time increase performance performance is increased through data striping the data is segmented into equal size partitions the striping units which are transparently distributed across multi ple disks this gives the appearance of a single large fast disk although the data is actually distributed across several smaller disks striping improves overall i o performance by allowing multiple i os to be serviced in parallel at the same time data striping also balances the load among disks reliability is improved through storing redundant information across the disks using a parity scheme or an error correcting scheme such as reed solomon codes see for example pless in a parity scheme each byte may have a parity bit associated with it that records whether the number of bits in the byte that are set to 1 is even or odd if the number of bits in the byte becomes corrupted the new parity of the byte will not match the stored parity similarly if the stored par ity bit becomes corrupted it will not match the data in the byte error correcting schemes store two or more additional bits and can reconstruct the original data if a single bit becomes corrupt these schemes can be used through striping bytes across disks there are a number of different disk configurations with raid termed raid levels a brief description of each raid level is given here with a diagrammatic representation for each of the main levels in figure in this figure the numbers represent sequential data blocks and the letters indicate segments of a data block raid nonredundant this level maintains no redundant data and therefore has the best write performance as updates do not have to be replicated data striping is performed at the level of blocks a diagrammatic representation of raid is shown in figure a raid 1 mirrored this level maintains mirrors two identical copies of the data across different disks to maintain consistency in the presence of disk failure writes may not be performed simultaneously this is the most expensive storage solution a diagrammatic representation of raid 1 is shown in figure b raid 1 nonredundant and mirrored this level combines striping and mir roring raid memory style error correcting codes with this level the striping unit is a single bit and hamming codes are used as the redundancy scheme a diagrammatic representation of raid is shown in figure c raid 3 bit interleaved parity this level provides redundancy by storing parity information on a single disk in the array this parity information can be used to recover the data on other disks should they fail this level uses less storage space than raid 1 but the parity disk can become a bottleneck a diagrammatic representation of raid 3 is shown in figure d raid block interleaved parity with this level the striping unit is a disk block a parity block is maintained on a separate disk for corresponding blocks from a number of other disks if one of the disks fails the parity block can be used with the corresponding blocks from the other disks to restore the blocks of the failed disk a diagrammatic representation of raid is shown in figure e raid block interleaved distributed parity this level uses parity data for redundancy in a similar way to raid 3 but stripes the parity data across all the disks similar to the way in which the source data is striped this alleviates the bottleneck on the parity disk a diagrammatic representation of raid is shown in figure f raid 6 p q redundancy this level is similar to raid but additional redundant data is maintained to protect against multiple disk failures error correcting codes are used instead of parity figure raid levels the numbers represent sequential data blocks and the letters indicate segments of a data block 3 security in microsoft office access dbms oracle for example recommends use of raid 1 for the redo log files for the database files oracle recommends either raid provided the write overhead is acceptable otherwise oracle recommends either raid 1 or raid 1 a fuller discussion of raid is outwith the scope of this book and the interested reader is referred to the papers by chen and patterson and chen et al 3 security in microsoft office access dbms in this section we provide an overview of the security measures provided by microsoft office access dbms in section 6 we described how sql can be used to control access to a database through the sql grant and revoke state ments however microsoft office access does not support these statements but instead provides the following methods for securing a database splitting the database setting a password for the database trusting enabling the disabled content in a database packaging signing and deploying the database splitting the database the most secure way to protect data in a database is to store the database tables separately from the database application objects such as forms and reports this action is referred to as splitting the database office access provides a database splitter wizard available through the access database button of the tools options in the move data section clicking the access database button displays the database splitter window shown in figure the location of the backend database is specified and once copied to the new location the backend database can be further protected by assigning a password as described in the following section figure the database splitter wizard window figure 6 securing the dreamhome database using a password a the set database password dialog box b the password required dialog box shown at startup setting a password for the database a simple way to secure a database is to set a password for opening the database setting a password is available through the encrypt with password option in the file info section once a password has been set a dialog box requesting the password will be displayed when the database is opened only users who type the correct password will be allowed to open the database this method is secure as microsoft access encrypts the password so that it cannot be accessed by reading the database file directly however once a data base is open all the objects contained within the database are available to the user figure 6 a shows the dialog box to set the password and figure 6 b shows the dialog box requesting the password whenever the database is opened microsoft access has improved encryption encoding for database passwords and integration with microsoft sharepoint for more effective management and use of information tracking applications compared to previous versions trusting enabling the disabled content in a database the trust center is a dialog box that can be used to trust enable the disabled content in a database the trust center dialog box is shown in figure the trust center can be used to create or change trusted locations and to set security options for office access databases those settings affect how new and existing databases behave when they are opened in that instance of access the trust center also contains logic for evaluating the components in a database and for determining whether the database is safe to open or whether the trust center should disable the database and let the user decide to enable it packaging signing and deploying the database the package and sign fea ture places the database is an access deployment accdc file signs the package and then places the code signed package to the desired location users can then extract the database from the package and work directly in the database not in the package file packaging a database and signing the package are ways to convey trust when the users receive the package the signature confirms that the database has not been tampered with if the users trust the author they can enable the content an overview of microsoft office access dbms is provided in appendix h available on the web site security in oracle dbms in appendix h we provide an overview of dbms in this section we focus on the security measures provided by oracle in this section we examine how oracle provides two types of security system security and data security as with office access one form of system security used by oracle is the standard user name and password mechanism whereby a user has to provide a valid user name and password before access can be gained to the database although the respon sibility to authenticate users can be devolved to the operating system figure illustrates the creation of a new user called beech with password authentication set whenever user beech tries to connect to the database this user will be presented with a connect or log on dialog box similar to the one illustrated in figure 9 prompting for a user name and password to access the specified database privileges as we discussed in section a privilege is a right to execute a particular type of sql statement or to access another user objects some examples of oracle privileges include the right to connect to the database create a session create a table select rows from another user table in oracle there are two distinct categories of privileges system privileges object privileges system privileges a system privilege is the right to perform a particular action or to perform an action on any schema objects of a particular type for example figure the microsoft office access trust center figure creation of a new user called beech with password authentication set figure 9 log on dialog box requesting user name password and the name of the database the user wishes to connect to the privileges to create tablespaces and to create users in a database are system privileges there are more than eighty distinct system privileges in oracle system privileges are granted to or revoked from users and roles discussed below using either of the following grant system privileges roles dialog box and revoke system privileges roles dialog box of the oracle security manager sql grant and revoke statements see section 6 however only users who are granted a specific system privilege with the admin option or users with the grant any privilege system privilege can grant or revoke system privileges object privileges an object privilege is a privilege or right to perform a par ticular action on a specific table view sequence procedure function or package different object privileges are available for different types of object for example the privilege to delete rows from the staff table is an object privilege some schema objects such as clusters indexes and triggers do not have associ ated object privileges their use is controlled with system privileges for example to alter a cluster a user must own the cluster or have the alter any cluster system privilege a user automatically has all object privileges for schema objects contained in his or her schema a user can grant any object privilege on any schema object he or she owns to any other user or role if the grant includes the with grant option of the grant statement the grantee can further grant the object privilege to other users otherwise the grantee can use the privilege but cannot grant it to other users the object privileges for tables and views are shown in table table what each object privilege allows a grantee to do with tables and views object privilege table view alter change the table definition with the alter table statement n a delete remove rows from the table with the delete statement note select privilege on the table must be granted along with the delete privilege remove rows from the view with the delete statement index create an index on the table with the create index statement n a insert add new rows to the table with the insert statement add new rows to the view with the insert statement references create a constraint that refers to the table cannot grant this privilege to a role n a select query the table with the select statement query the view with the select statement update change data in the table with the update statement note select privilege on the table must be granted along with the update privilege change data in the view with the update statement roles a user can receive a privilege in two different ways 1 privileges can be explicitly granted to users for example a user can explicitly grant the privilege to insert rows into the propertyforrent table to the user beech grant insert on propertyforrent to beech privileges can also be granted to a role a named group of privileges and then the role granted to one or more users for example a user can grant the privi leges to select insert and update rows from the propertyforrent table to the role named assistant which in turn can be granted to the user beech a user can have access to several roles and several users can be assigned the same roles figure illustrates the granting of these privileges to the role assistant using the oracle security manager figure setting the insert select and update privileges on the propertyforrent table to the role assistant because roles allow for easier and better management of privileges privileges should normally be granted to roles and not to specific users dbmss and web security in chapter we provide a general overview of dbmss on the web in this section we focus on how to make a dbms secure on the web those readers unfamilair with the terms and technologies associated with dbmss on the web are advised to read chapter before reading this section internet communication relies on tcp ip as the underlying protocol however tcp ip and http were not designed with security in mind without special soft ware all internet traffic travels in the clear and anyone who monitors traffic can read it this form of attack is relatively easy to perpetrate using freely available packet sniffing software as the internet has traditionally been an open network consider for example the implications of credit card numbers being intercepted by unethical parties during transmission when customers use their cards to pur chase products over the internet the challenge is to transmit and receive informa tion over the internet while ensuring that it is inaccessible to anyone but the sender and receiver privacy it has not been changed during transmission integrity the receiver can be sure it came from the sender authenticity the sender can be sure the receiver is genuine nonfabrication the sender cannot deny he or she sent it nonrepudiation however protecting the transaction solves only part of the problem once the infor mation has reached the web server it must also be protected there with the three tier architecture that is popular in a web environment we also have the complexity of ensuring secure access to and of the database today most parts of such archi tecture can be secured but it generally requires different products and mechanisms one other aspect of security that has to be addressed in the web environment is that information transmitted to the client machine may have executable content for example html pages may contain activex controls javascript vbscript and or one or more java applets executable content can perform the following malicious actions and measures need to be taken to prevent them corrupt data or the execution state of programs reformat complete disks perform a total system shutdown collect and download confidential data such as files or passwords to another site usurp identity and impersonate the user or user computer to attack other tar gets on the network lock up resources making them unavailable for legitimate users and programs cause nonfatal but unwelcome effects especially on output devices in earlier sections we identified general security mechanisms for database sys tems however the increasing accessibility of databases on the public internet and private intranets requires a re analysis and extension of these approaches in this section we address some of the issues associated with database security in these environments 1 proxy servers in a web environment a proxy server is a computer that sits between a web browser and a web server it intercepts all requests to the web server to determine whether it can fulfill the requests itself if not it forwards the requests to the web server proxy servers have two main purposes to improve performance and filter requests improve performance because a proxy server saves the results of all requests for a certain amount of time it can significantly improve performance for groups of users for example assume that user a and user b access the web through a proxy server first user a requests a certain web page and slightly later user b requests the same page instead of forwarding the request to the web server where that page resides the proxy server simply returns the cached page that it had already fetched for user a because the proxy server is often on the same network as the user this is a much faster operation real proxy servers such as those employed by compuserve and america online can support thousands of users filter requests proxy servers can also be used to filter requests for example an organization might use a proxy server to prevent its employees from accessing a specific set of web sites firewalls the standard security advice is to ensure that web servers are unconnected to any in house networks and regularly backed up to recover from inevitable attacks when the web server must be connected to an internal network for example to access the company database firewall technology can help to prevent unauthor ized access provided that it has been installed and maintained correctly a firewall is a system designed to prevent unauthorized access to or from a private network firewalls can be implemented as both hardware and software or a combination of both they are frequently used to prevent unauthorized internet users from accessing private networks connected to the internet especially intranets all messages entering or leaving the intranet pass through the firewall which examines each message and blocks those that do not meet the specified secu rity criteria there are several types of firewall technique packet filter which looks at each packet entering or leaving the network and accepts or rejects it based on user defined rules packet filtering is a fairly effec tive mechanism and transparent to users but can be difficult to configure in addition it is susceptible to ip spoofing ip spoofing is a technique used to gain unauthorized access to computers in which the intruder sends messages to a computer with an ip address indicating that the message is coming from a trusted port application gateway which applies security mechanisms to specific applications such as ftp and telnet servers this is a very effective mechanism but can degrade performance circuit level gateway which applies security mechanisms when a tcp or udp user datagram protocol connection is established once the connection has been made packets can flow between the hosts without further checking proxy server which intercepts all messages entering and leaving the network the proxy server in effect hides the true network addresses in practice many firewalls provide more than one of these techniques a firewall is considered a first line of defense in protecting private information for greater security data can be encrypted as discussed next and earlier in section 6 3 message digest algorithms and digital signatures a message digest algorithm or one way hash function takes an arbitrarily sized string the message and generates a fixed length string the digest or hash a digest has the following characteristics it should be computationally infeasible to find another message that will generate the same digest the digest does not reveal anything about the message a digital signature consists of two pieces of information a string of bits that is computed from the data that is being signed along with the private key of the individual or organization wishing the signature the signature can be used to verify that the data comes from this individual or organization like a handwritten signature a digital signature has many useful properties its authenticity can be verified using a computation based on the corresponding public key it cannot be forged assuming that the private key is kept secret it is a function of the data signed and cannot be claimed to be the signature for any other data the signed data cannot be changed otherwise the signature will no longer verify the data as being authentic some digital signature algorithms use message digest algorithms for parts of their computations others for efficiency compute the digest of a message and digitally sign the digest rather than signing the message itself digital certificates a digital certificate is an attachment to an electronic message used for security purposes most commonly to verify that a user sending a message is who he or she claims to be and to provide the receiver with the means to encode a reply an individual wishing to send an encrypted message applies for a digital cer tificate from a certificate authority ca such as verisign thawte geotrust or comodo the ca issues an encrypted digital certificate containing the appli cant public key and a variety of other identification information the ca makes its own public key readily available through printed material or perhaps on the internet the recipient of an encrypted message uses the ca public key to decode the digital certificate attached to the message verifies it as issued by the ca and then obtains the sender public key and identification information held within the cer tificate with this information the recipient can send an encrypted reply verisign developed the following classes for digital certificates class 1 for individuals intended for email class for organizations for which proof of identity is required class 3 for servers software signing for which independent verification and checking of identity and authority is carried out by issuing a ca class for online business transactions between computers class for private organization or government security nowadays the most common use of certificates is for s http sites a web browser validates that a secure sockets layer ssl web server is authentic so that the users can be assured that the interaction with the web site has no eavesdroppers and that the site is what is claims to be s http and ssl are discussed shortly clearly the ca role in this process is critical acting as a go between for the two parties in a large distributed complex network like the internet this third party trust model is necessary as clients and servers may not have an established mutual trust yet both want to have a secure session however because each party trusts the ca and because the ca is vouching for each party identification and trustworthi ness by signing their certificates each party recognizes and implicitly trusts each other the most widely used standard for digital certificates is x kerberos kerberos is a server of secured user names and passwords named after the three headed monster in greek mythology that guarded the gate of hell the impor tance of kerberos is that it provides one centralized security server for all data and resources on the network database access login authorization control and other security features are centralized on trusted kerberos servers kerberos has a similar function to that of a certificate server to identify and validate a user security com panies are currently investigating a merger of kerberos and certificate servers to provide a network wide secure system 6 secure sockets layer and secure http many large internet product developers agreed to use an encryption protocol known as secure sockets layer ssl developed by netscape for transmitting private documents over the internet ssl works by using a private key to encrypt data that is transferred over the ssl connection both firefox and internet explorer support ssl and many web sites use this protocol to obtain confidential user information such as credit card numbers the protocol layered between application level protocols such as http and the tcp ip transport level protocol is designed to prevent eavesdropping tampering and message forgery because ssl is layered under application level protocols it may be used for other applica tion level protocols such as ftp and nntp another protocol for transmitting data securely over the web is secure http s http a modified version of the standard http protocol s http was developed by enterprise integration technologies eit which was acquired by verifone inc in whereas ssl creates a secure connection between a cli ent and a server over which any amount of data can be sent securely s http is designed to transmit individual messages securely ssl and s http therefore can be seen as complementary rather than competing technologies both pro tocols have been submitted to the internet engineering task force ietf for approval as standards by convention web pages that require an ssl connec tion start with https instead of http not all web browsers and servers support ssl s http basically these protocols allow the browser and server to authenticate one another and secure information that subsequently flows between them through the use of cryptographic techniques such as encryption and digital signatures these protocols allow web browsers and servers to authenticate each other permit web site owners to control access to particular servers directories files or services allow sensitive information for example credit card numbers to be shared between browser and server yet remain inaccessible to third parties ensure that data exchanged between browser and server is reliable that is cannot be corrupted either accidentally or deliberately without detection a key component in the establishment of secure web sessions using the ssl or s http protocols is the digital certificate discussed previously without authentic and trustworthy certificates protocols like ssl and s http offer no security at all secure electronic transactions and secure transaction technology the secure electronic transactions set protocol is an open interoperable stand ard for processing credit card transactions over the internet created jointly by netscape microsoft visa mastercard gte saic terisa systems and verisign set goal is to allow credit card transactions to be as simple and secure on the internet as they are in retail stores to address privacy concerns the transaction is split in such a way that the merchant has access to information about what is being purchased how much it costs and whether the payment is approved but no infor mation on what payment method the customer is using similarly the card issuer for example visa has access to the purchase price but no information on the type of merchandise involved certificates are heavily used by set both for certifying a cardholder and for certifying that the merchant has a relationship with the financial institution the mechanism is illustrated in figure 11 though both microsoft and visa international are major participants in the set specifications they currently pro vide the secure transaction technology stt protocol which has been designed figure 11 a set transaction to handle secure bank payments over the internet stt uses des encryption of information rsa encryption of bankcard information and strong authentication of all parties involved in the transaction java security in section 7 we introduce the java language as an increasingly important lan guage for web development those readers unfamiliar with java are advised to read section 7 before reading this section safety and security are integral parts of java design with the sandbox ensur ing that an untrusted and possibly malicious application cannot gain access to system resources to implement this sandbox three components are used a class loader a bytecode verifier and a security manager the safety features are pro vided by the java language and the java virtual machine jvm and enforced by the compiler and the runtime system security is a policy that is built on top of this safety layer two safety features of the java language relate to strong typing and automatic garbage collection in this section we look at two other features the class loader and the bytecode verifier to complete this section on java security we examine the jvm security manager the class loader the class loader as well as loading each required class and checking it is in the correct format additionally checks whether the application applet violates system security by allocating a namespace namespaces are hierarchical and allow the jvm to group classes based on where they originate local or remote a class loader never allows a class from a less protected namespace to replace a class from a more protected namespace in this way the file system i o primitives which are defined in a local java class cannot be invoked or indeed overridden by classes from outside the local machine an executing jvm allows multiple class loaders each with its own namespace to be active simultaneously as browsers and java applications can typically provide their own class loader albeit based on a recommended template from sun microsystems this may be viewed as a weakness in the security model however some argue that this is a strength of the language allowing system administrators to implement their own presumably tighter security measures the bytecode verifier before the jvm will allow an application applet to run its code must be verified the verifier assumes that all code is meant to crash or violate system security and performs a series of checks including the execution of a theorem prover to ensure that this is not the case typical checks include verifying that compiled code is correctly formatted internal stacks will not overflow underflow no illegal data conversions will occur for example integer to pointer this ensures that variables will not be granted access to restricted memory areas bytecode instructions are appropriately typed all class member accesses are valid the security manager the java security policy is application specific a java application such as a java enabled web browser or a web server defines and implements its own security policy each of these applications implements its own security manager a java enabled web browser contains its own applet security manager and any applets downloaded by this browser are subject to its policies generally the security manager performs runtime verification of potentially dangerous methods that is methods that request i o network access or attempt to define a new class loader in general downloaded applets are prevented from reading and writing files on the client file system this also prevents applets storing persistent data for example a database on the client side although the data could be sent back to the host for storage making network connections to machines other than the host that provided the compiled class files this is either the host where the html page came from or the host specified in the codebase parameter in the applet tag with codebase taking precedence starting other programs on the client loading libraries defining method calls allowing an applet to define native method calls would give the applet direct access to the underlying operating system these restrictions apply to applets that are downloaded over the public internet or company intranet they do not apply to applets on the client local disk and in a directory that is on the client classpath local applets are loaded by the file system loader and as well as being able to read and write files are allowed to exit the virtual machine and are not passed through the bytecode verifier the jdk java development kit appletviewer also slightly relaxes these restrictions by letting the user define an explicit list of files that can be accessed by downloaded applets in a similar way microsoft internet explorer 0 introduced the concept of zones and some zones may be trusted and others untrusted java applets loaded from certain zones are able to read and write to files on the client hard drive the zones with which this is possible are customizable by the network administrators enhanced applet security the sandbox model was introduced with the first release of the java applet api in january although this model does generally protect systems from untrusted code obtained from the network it does not address several other security and pri vacy issues authentication is needed to ensure that an applet comes from where it claims to have come from further digitally signed and authenticated applets can then be raised to the status of trusted applets and subsequently allowed to run with fewer security restrictions the java security api available in jdk 1 1 contains apis for digital signatures message digests key management and encryption decryption subject to united states export control regulations work is in progress to define an infrastructure that allows flexible security policies for signed applets 9 activex security the activex security model is considerably different from java applets java achieves security by restricting the behavior of applets to a safe set of instructions activex on the other hand places no restrictions on what a control can do instead each activex control can be digitally signed by its author using a system called authenticode the digital signatures are then certified by a ca this security model places the responsibility for the computer security on the user before the browser downloads an activex control that has not been signed or has been certified by an unknown ca it presents a dialog box warning the user that this action may not be safe the user can then abort the transfer or continue and accept the consequences 6 data administration and database administration the data administration da and database administrator dba are responsible for managing and controlling the activities associated with the corporate data and the corporate database respectively the da is more concerned with the early 6 data administration and database administration stages of the lifecycle from planning through to logical database design in con trast the dba is more concerned with the later stages from application physical database design to operational maintenance in this final section of the chapter we discuss the purpose and tasks associated with data and database administration 6 1 data administration the management of the data resource which includes database planning development and maintenance of standards policies and procedures and conceptual and logical database design the da is responsible for the corporate data resource which includes noncom puterized data and in practice is often concerned with managing the shared data of users or application areas of an organization the da has the primary respon sibility of consulting with and advising senior managers and ensuring that the application of database technologies continues to support corporate objectives in some enterprises data administration is a distinct functional area in others it may be combined with database administration the tasks associated with data administration are described in table 3 table 3 data administration tasks selecting appropriate productivity tools assisting in the development of the corporate it is and enterprise strategies undertaking feasibility studies and planning for database development developing a corporate data model determining the organization data requirements setting data collection standards and establishing data formats estimating volumes of data and likely growth determining patterns and frequencies of data usage undertaking conceptual and logical database design liaising with database administration staff and application developers to ensure applications meet all stated requirements educating users on data standards and legal responsibilities keeping up to date with it is and enterprise developments ensuring documentation is up to date and complete including standards policies procedures use of the data dictionary and controls on end users managing the data dictionary liaising with users to determine new requirements and to resolve difficulties over data access or performance developing a security policy table database administration tasks evaluating and selecting dbms products undertaking physical database design defining security and integrity constraints developing test strategies responsible for signing off on the implemented database system performing backups routinely ensuring that documentation is complete including in house produced material keeping up to date with software and hardware developments and costs and installing updates as necessary 6 database administration the management of the physical realization of a database system which includes physical database design and imple mentation setting security and integrity controls monitor ing system performance and reorganizing the database as necessary the database administration staff are more technically oriented than the data administration staff requiring knowledge of specific dbmss and the operating system environment although the primary responsibilities are centered on devel oping and maintaining systems using the dbms software to its fullest extent dba staff also assist da staff in other areas as indicated in table the number of staff assigned to the database administration functional area varies and is often determined by the size of the organization the tasks of database administration are described in table 4 6 3 comparison of data and database administration the preceding sections examined the purpose and tasks associated with data administration and database administration in this final section we briefly contrast these functional areas table summarizes the main task differences of data administration and database administration perhaps the most obvious difference lies in the nature of the work carried out data administration staff tend to be much more managerial whereas the database administration staff tend to be more technical chapter summary table data administration and database administration main task differences data administration database administration involved in strategic is planning evaluates new dbmss determines long term goals executes plans to achieve goals enforces standards policies and procedures enforces standards policies and procedures determines data requirements implements data requirements develops conceptual and logical database design develops logical and physical database design develops and maintains corporate data model implements physical database design coordinates system development monitors and controls database managerial orientation technical orientation dbms independent dbms dependent chapter summary database security is the mechanisms that protect the database against intentional or accidental threats database security is concerned with avoiding the following situations theft and fraud loss of confidentiality secrecy loss of privacy loss of integrity and loss of availability a threat is any situation or event whether intentional or accidental that will adversely affect a system and consequently an organization computer based security controls for the multiuser environment include authorization access controls views backup and recovery integrity encryption and raid technology authorization is the granting of a right or privilege that enables a subject to have legitimate access to a system or a system object authentication is a mechanism that determines whether a user is who he or she claims to be most commercial dbmss provide an approach called discretionary access control dac which manages privileges using sql the sql standard supports dac through the grant and revoke commands some commercial dbmss also provide an approach to access control called mandatory access control mac which is based on system wide policies that cannot be changed by individual users in this approach each database object is assigned a security class and each user is assigned a clearance for a security class and rules are imposed on reading and writing of database objects by users the sql standard does not include support for mac a view is the dynamic result of one or more relational operations operating on the base relations to produce another relation a view is a virtual relation that does not actually exist in the database but is produced upon request by a particular user at the time of request the view mechanism provides a powerful and flexible security mechanism by hiding parts of the database from certain users backup is the process of periodically taking a copy of the database and log file and possibly programs on to offline storage media journaling is the process of keeping and maintaining a log file or journal of all changes made to the database to enable recovery to be undertaken effectively in the event of a failure integrity constraints also contribute to maintaining a secure database system by preventing data from becoming invalid and hence giving misleading or incorrect results encryption is the encoding of the data by a special algorithm that renders the data unreadable by any program without the decryption key cloud computing is the use of computing software or hardware resources that are delivered over a network and accessed typically from a web browser or mobile application microsoft office access dbms provides four methods to secure the database including splitting the database setting a password trusting enabling the disabled content of a database packaging signing and deploying the database oracle dbms provides two types of security measure system security and data security system security enables the setting of a password for opening a database and data security provides user level security which can be used to limit the parts of a database that a user can read and update the security measures associated with dbmss on the web include proxy servers firewalls message digest algorithms and digital signatures digital certificates kerberos secure sockets layer ssl and secure http s http secure electronic transactions set and secure transaction technology sst java security and activex security data administration is the management of the data resource including database planning development and maintenance of standards policies and procedures and conceptual and logical database design database administration is the management of the physical realization of a database system including physical database design and implementation setting security and integrity controls monitoring system perfor mance and reorganizing the database as necessary 1 explain the purpose and scope of database security the scope of database security extends beyond just dbms controls discuss the role of the database administra tor in database security and recovery 3 explain the following in terms of providing security for a database a authorization b authetication c access controls d views e backup and recovery f integrity g encryption h raid technology 4 distinguish between role and privilege in the context of object and systems privilege describe the security challenges associated with the use of dbmss on the web 6 describe the approaches for securing dbmss on the web 7 describe cloud computing services and the databases within their context 8 describe any specific security measures for databases in mobile applications and devices 9 what tasks are associated with data administration what tasks are associated with database administration 11 examine any dbms used by your organization and identify the security measures provided 12 identify the types of security approach that are used by your organization to secure any dbmss that are accessible over the web exercises 13 consider the dreamhome case study described in chapter 11 list the potential threats that could occur and propose countermeasures to overcome them 14 consider the wellmeadows hospital case study described in appendix b 3 list the potential threats that could occur and propose countermeasures to overcome them investigate whether data administration and database administration exist as distinct functional areas within your organization if identified describe the organization responsibilities and tasks associated with each functional area describe the use and potential uses of cloud computing services by your organization and list the potential threats implications and countermeasures analyze the security features of three dbmss and discuss the similarities and differences between them you are contracted to deploy database security for the university accommodation system describe how you will approach the project chapter professional legal and ethical issues in data management as discussed in chapter data and database administrators should be well versed in issues related to the day to day internal control over growing volumes of organi zational data such managers however will increasingly find themselves delving deeply into the relatively unfamiliar territory of legal and ethical compliance with the increased scrutiny of corporations brought on by the massive scandals emerging in the past several years for example enron and worldcom in the united states and parmalat in europe have come new laws and regulations that will bring about significant changes in the way organizations operate the rise in cases of identity theft has likewise brought increased scrutiny upon data management practices in the context of this book the impact of legal and ethical issues on the it functions quite often revolves around the management of data the aim of this chapter is therefore to define the underlying issues to illustrate the data management context in which these issues present themselves and to provide best practices for data and database administrators with respect to these issues 1 defining legal and ethical issues in it it is interesting to first consider why it is deemed necessary to included a chapter on legal it governance and ethical issues in a book about database systems the answer is simple organizations around the world increasingly find themselves hav ing to answer tough questions about the conduct and character of their employees and the manner in which their activities are carried out at the same time we need to develop knowledge of what constitutes professional and unprofessional behavior additionally in order to ensure more transparency and effectiveness in it resource and data management organizations face increasing pressures to instill processes and policies that are in support of these efforts 1 1 defining ethics in the context of it in the past it workers in their roles supporting the organization may have felt largely shielded from the actions and deeds or misdeeds as the case may be of their managers and executives after all the executives and managers define how the organization operates with it staff simply implementing what they dictate today however this is no longer the case the confluence of massive increases in storage and processing power influence on the rest of the organization and the increase in scrutiny by shareholders watchdogs the media and quite possibly insid ers have placed it staff in the limelight it is not unusual for example for even a small ecommerce vendor to amass many terabytes bytes of clickstream 1 data from users financial services firms deal with petabytes bytes of financial transactions involving hundreds of millions of individuals and business entities within such a technology climate it departments are being asked to leverage these mountains of data to achieve business advantages as was discussed in chapter individuals intent on identity theft and fraud who could come from inside or clickstream is a web or application log that traces where and when a user clicks the links on a web site or application 1 defining legal and ethical issues in it outside of the organization find these volumes of data extremely tempting at the same time overzealous managers may become tempted by the possibilities of busi ness intelligence technologies whose usage can push against ethical boundaries we discuss business intelligence in chapters to needless to say legislators and government regulations are having a hard time keeping up with the rapidly shifting technological landscape in addition new and emerging regulations require accu rate data analysis to demonstrate compliance and conformity for the purposes of this chapter we start with some basic definitions and discuss some example scenarios to illustrate best practices a set of principles of correct conduct or a theory or a system of moral values there are many definitions of ethics and most seem quite distant from what the business or technology person faces on a daily basis it might be helpful therefore to think of ethical behavior as doing what is right according to the standards of society this of course begs the question of whose society as what might be con sidered ethical behavior in one culture country religion and ethnicity might not be so in another tackling this particular debate is beyond the scope of this book 1 the difference between ethical and legal behavior another point of confusion may be the contrast between what is ethical and what is legal we may think of laws as simply enforcing certain ethical behaviors for exam ple most would agree that it is unethical to steal from another enough individuals seem capable of dismissing this aspect of doing what is right that in most societies laws must be enacted to enforce this particular ethical behavior this line of think ing leads to two familiar ideas what is ethical is legal and what is unethical is illegal many examples can be provided to support these claims this claim however brings up the question is all unethical behavior illegal consider a lonely database administrator for dreamhome who is using his or her administrative access privileges to query the dreamhome database for clients whose property rental patterns suggest that they are single it may very well be the case that no laws are broken while at the same time the company would consider this unethical behavior another question that can be asked is all ethical behavior legal consider for example the u s securities and exchange commission sec reg nms regulation national market system this sweeping set of regulations aims to alter the way in which equities stocks are traded in u s markets one aspect of reg nms is called the order protection rule and states that a trade cannot execute at one price if a better price can be found in a different fast exchange or market a fast market being one capable of subsecond executions for example suppose that ibm shares are selling for 15 with buyers available for shares on the new york stock exchange nyse but they are selling for 14 with buyers for only shares on instinet an electronic stock market the nyse is required under reg nms to route the shares to the market with the cheaper price however institutional investors such as mutual funds and retirement funds trading very large blocks of stock need to trade on the market with the greatest liquidity that is with the most shares available to buy or sell ensuring the fastest completion of their order even if it might be a penny or two less advantageous at that moment the reason is that if the market sees such a large order coming the bid or offer will move away from the current price that is a large sell order will drive the price down as a result under reg nms which was intended to ensure best price execution for all clients changes what appears to be ethical for an institutional investor to be illegal for their broker or the exchange receiving the order even on their client instructions in essence ethics precedes the law ethical codes of practice help determine whether specific laws should be introduced ethics fills the gap between the time when technology creates new problems and the time when laws are introduced 1 3 ethical behavior in it a survey conducted by techrepublic techrepublic com an it oriented web por tal maintained by cnet networks reported that of the it workers polled indicated they had been asked to do something unethical by their supervisors thornberry although the survey did not delve into specifics actions clearly this is a troubling figure actions that most consider unethical and also illegal include installing unlicensed software see section 4 5 accessing personal infor mation and divulging trade secrets another aspect of database technology concerns the use of data mining and data warehousing to aggregate and find associations and patterns among disparate data such business intelligence tools have evolved significantly over the past decade and coupled with the tremendous increases in processing power and storage afford even small and medium sized businesses the ability to examine client behavior on an extremely fine grain level there are many clearly ethical uses for such technology for example consider the case of a suspect who is placed near the scene of a crime by linking the suspect car identified by a video scan of the license plate to the suspect cash machine transaction however just as easily we can think of many examples of situations that would violate ethical and privacy standards for example as a database admin istrator imagine that you are asked to implement a schema for a loan application process that includes the applicant race or national origin a growing number of daily events trigger data collection on a grand scale for example making a cell phone call making a purchase with a credit or debit card applying for a loan passing through an automated toll booth or subway turnstile driving around london visiting a doctor or a pharmacy or simply clicking on a link on a web site all generate transaction logs that can potentially be correlated joined and otherwise mined for patterns that many business and government enti ties would take great interest in the temptation to collect and mine data is made even greater as businesses are pressured to gain competitive advantage these examples illustrate legal behavior that many would consider unethical the issue comes down to recognizing the fact that although advances in it make these kinds of analysis possible governments and legal communities have been slow to recognize the potential threats to privacy and thus the decision of whether these activities should be carried out is left up to the individual organization such deci sions in turn necessarily depend upon the organization culture and awareness of what constitutes ethical behavior in section 3 we discuss best practices for developing a culture of ethical behavior among it workers it governance corporate it has become more sophisticated within organizations and as high lighted earlier the influence of data and it resources on the rest of the organiza tion and its associated business strategies has increased dramatically to become a vital function to support innovation efficiencies and growth in most organizations over the past few decades as a result of this alignment of it investment to business has become a major concern of board and executive management governance became an important issue in the wake of large corporate scan dals in that resulted in the adoption of practice and legislation for defining organizational objectives rules and processes for monitoring financial and cor porate performance to ensure that the objectives are transparently obtained it governance builds on priorities in accountability decision rights management and use of it resources to ensure that organizational business objectives are met with it additionally it governance must also inculcate legislative regulations and or voluntary regulations standards and best practice put in place to reduce illegal behavior improve efficiencies and risks of it failures or vulnerabilities on other companies national infrastructure consumers and the natural environment as a result of this we provide the following widely accepted definition for it governance weill and ross used for specifying the decision rights and accountability framework to encourage desirable behavior in the use of it 2 legislation and its impact on the it function as mentioned in the previous section often the distinction between what is legal and what is ethical becomes blurred by the introduction of or amendments to laws and regulations that affect how data may be collected processed and distributed in this section we discuss several recent regulations and discuss the impact such regulations have on data and database administration functions 2 1 securities and exchange commission sec regulation national market system nms in the context of activities that appear ethical but are in fact illegal we considered the sec regulation nms and the order protection rule under which an activ ity that is acceptable to one facet of the investment community purchasing a large block of shares at an inferior price was deemed illegal under the new regulation as a result of this regulation financial services firms are now required to collect market data so that they can demonstrate that a better price was indeed not avail able at the time the trade was executed the data administrator for a financial services firm would thus need to be aware of this regulation and how it affects the trading operations within the firm additional databases would need to be devel oped to link the trading activity to the market data and reporting tools would be required to generate confirmation reports and ad hoc reports to satisfy sec inquiries 2 2 the sarbanes oxley act cobit and coso in the aftermath of major financial frauds allegedly carried out within companies such as enron worldcom parmalat and others both the u s and european governments have put forth legislation to tighten requirements on how companies form their board of directors interact with auditors and report their financial statements in the united states this legislation has taken the form of the sarbanes oxley act of sox which also affects european companies who are listed on u s exchanges although the focus of sox is on accounting and finance related issues data management and auditing of databases and applications have been thrust to the forefront as companies must now certify the accuracy of their financial data from a data administrator perspective sox places increased requirements on the security and auditing of financial data and this requirement has implica tions for how data is collected processed secured and possibly reported both internally and externally to the organization a set of rules that an organization adopts to ensure that policies and procedures are not violated data is properly secured and reliable and operations can be carried out efficiently in order to comply with sox companies must adopt a formal control frame work for information management risks two leading frameworks are the control objectives for information and related technology cobit and the committee of sponsoring organizations of the treadway commission coso as we now discuss cobit was created in by the it governance institute itgi and has since gone though four revisions to its current form cobit 5 released in by the information systems audit and control association isaca and itgi the isaca web site for cobit www isaca org cobit explains cobit 5 provides a comprehensive framework that assists enterprises in achieving their objectives for the governance and management of enterprise information and technology assets it cobit 5 enables it to be governed and managed in a holistic manner for the entire enterprise taking in the full end to end business and it functional areas of responsibility considering the it related interests of internal and external stakeholders cobit 5 consolidates and integrates cobit 4 1 val it 2 0 and risk it frame works and draws from isaca it assurance framework itaf and the business model for information security bmis it aligns with a number of frameworks and standards such as information technology infrastructure library itil international organization for standardization iso project management body of knowledge pmbok and the open group architecture framework togaf the cobit 5 processes are split into governance and management areas these two areas contain a total of five domains and processes governance of enterprise it evaluate direct and monitor edm governance ensures that enterprise objectives are achieved by evaluating stakeholder needs conditions and options setting direction through prioritisation and decision making and monitoring performance compliance and progress against agreed on direc tion and objectives this domain consists of five processes ensure governance framework setting and maintenance ensure benefits delivery ensure risk optimization ensure resource optimization ensure stakeholder transparency management of enterprise it align plan and organize apo apo covers the use of information and technology and how best it can be used in an organization to help achieve the organization goals and objectives it also highlights the organizational and infrastructural form it is to take in order to achieve the optimal results and produce maximum benefits from its use this domain consists of 13 processes manage the it management framework manage strategy manage enterprise architecture manage innovation manage portfolio manage budget and costs manage human relations manage relationships manage service agreements manage suppliers manage quality manage risk manage security build acquire and implement bai bai covers identifying it require ments acquiring the technology and implementing it within the organization current business processes this domain consists of processes manage programs and projects manage requirements definition manage solutions identification and build manage availability and capacity manage organizational change enablement manage changes manage changes acceptance and transitioning manage knowledge manage assets manage configuration deliver service and support dss dss covers the delivery aspects of it such as the execution of the applications within the it system and its results and the support processes that enable the effective and efficient execution of these it systems this domain consists of six processes manage operations manage service requests and incidents manage problems manage continuity manage security services manage business process controls monitor evaluate and assess mea mea deals with an organization strat egy in assessing its needs and whether or not the current it system still meets the objectives for which it was designed and the controls necessary to comply with regulatory requirements monitoring also covers the issue of an independ ent assessment of the effectiveness of the it system ability to meet business objectives and the organization control processes by internal and external auditors this domain consists of three processes monitor evaluate and assess performance and conformance monitor evaluate and assess the system of internal control evaluate and assess compliance with external requirements on the other hand the coso framework focuses more narrowly on internal con trols and consists of five major components including control environment establishes a culture of control accountability and ethical behavior risk assessment evaluates the risks faced in carrying out the organization objec tives control activities implements controls necessary to mitigate risks information and communications specifies the paths of reporting and communica tion within an organization and between the organization and its trading partners monitoring assessing the effectiveness of controls put in place clearly data administrators should be directly involved in each of these compo nents in large organizations this would mean working closely with senior manage ment to make them aware of the impact of implementing these controls from the it perspective significant resources new software hardware databases training and new personnel will need to be lobbied for it is worth noting that although cobit provides guidelines on policies and standards for organizations it does not provide recommendations on implement ing them within processes and designing supporting structures for the guidelines this is something that organizations have to interpret for themselves 2 3 the health insurance portability and accountability act the health insurance portability and accountability act hipaa of is admin istered by the department of health and human services in the united states and affects all providers of healthcare and health insurance provisions of the act are being rolled out in stages with deadlines for implementation that depend upon the size and nature of the healthcare provider the act has several main provisions of which the following five are of direct importance to database management 1 privacy of patient information mandated by april for most organizations patients are now required to sign consent forms to allow their healthcare pro viders to share medical information with other providers and insurers data administrators must ensure that databases and systems track patient consent and allow or deny data transfers either electronically or on paper accordingly 2 standardizing electronic health medical records and transactions between healthcare organizations mandated by october a series of standards have been developed that cover typical healthcare transactions such as claims enrollment patient eligibility payments for data administrators this has meant changing enterprise data models to include additional attributes and to work with elec tronic data interchange software vendors to ensure that healthcare data can be exchanged in a standard format between organizations 3 establishing a nationally recognized identifier for employees to be used by all employee health plans july for most organizations such an identifier which may not be the social security number will then be used in all subsequent transactions between healthcare organizations again data administrators need to introduce changes to enterprise data models to include these alternative identifiers 4 standards for the security of patient data and transactions involving this data april for most organizations patient data must be secured both within database systems as well as when transmitted between organizations failure to secure patient data can result in large fines database vendors such as oracle and ibm is the computer to computer exchange of business data over a network typical transactions include purchase orders invoices payments and so on now offer tools to encrypt data within columns of database tables and provide facilities to enable fine grained auditing of database transactions 5 need for a nationally recognized identifier for healthcare organizations and individual providers the implications for this provision are similar to those for the stand ardized employee identifier discussed previously 2 4 the european union eu directive on data protection of the official title of the eu data protection directive is the directive ec of the european parliament and of the council of october on the protection of individuals with regard to the processing of personal data and on the free movement of such data ojec this directive adopted by all eu members in spans articles and is perhaps the most comprehensive of all similar directives or acts in the world today an in depth treatment of all aspects of the directive is beyond the scope of this book however some articles of the directive merit particular attention articles 6 and 7 consist of 11 requirements of which 8 were used as a basis for the u k data protection principles described in the next section article 8 focuses on the processing of personal data revealing racial or ethnic origin political opinions religious or philosophical beliefs trade union mem bership and the processing of data concerning health or sex life this activity is generally prohibited however exceptions are noted including that if the subject gives consent the processor is ordered by law or does the work in accord ance with its normal business functions and so on articles 10 11 and 12 address how data is collected and the rights of individuals to see their data and appeal for corrections articles 16 and 17 address the confidentiality and security measures taken while data is collected and processed articles 18 through deal with how a processor notifies an eu member of its intention to process data and situations under which the eu member will publi cize the processing operations the eu directive is founded on seven principles notice subjects whose data is being collected should be given notice of such collection purpose data collected should be used only for stated purposes and for no other purposes consent personal data should not be disclosed or shared with third parties with out consent from its subject security once collected personal data should be kept safe and secure from poten tial abuse theft or loss disclosure subjects whose personal data is being collected should be informed as to the party or parties collecting such data access subjects should be granted access to their personal data and allowed to correct any inaccuracies accountability subjects should be able to hold personal data collectors accountable for adhering to all of these principles in this directive personal data means any information relating to an identified or identifiable natural person data subject an identifiable person is one who can be identified directly or indirectly in particular by reference to an identification number or to one or more factors specific to his physical physiological mental economic cultural or social identity article data is considered personal when it allows anyone to link information to a specific person even if the person or entity holding that data cannot make that link examples of such data include addresses bank statements and credit card numbers processing is also broadly defined and involves any manual or automatic operation on personal data including its collection recording organization stor age modification retrieval use transmission dissemination or publication and even blocking erasure or destruction article the directive applies not only when the controller is based or operates within the eu but whenever the controller uses equipment located inside the eu to process personal data thus organizations from outside the eu who process personal data inside the eu must still comply with this directive in january the european commission unveiled a draft legislative package to create a single european data protection law that will be applicable in all mem ber states of the european union proposed changes include applicability of the law for all non eu companies without any establishment in the eu provided that the processing of data is directed at eu residents an obligatory opt in consent mechanism so that any processing of personal data will require clear information to be provided to concerned individuals as well as specific and explicit consent to be obtained from such individuals for the process ing of their data making a safe transfer of data outside of the eu including data in clouds easier in the event that the parties involved commit themselves to binding corporate rules new privacy rights including data subject right of portability and the right to be forgotten will be established in the eu the former right will allow a transfer of all data from one provider to another upon request for example transfer of a social media profile whereas the right to be forgotten will allow people to erase the historical data the processing of data of individuals under the age of 13 will normally require parental consent which will make it more difficult for companies to conduct busi ness aimed at minors companies must notify eu data protection authorities as well as the individuals whose data are concerned by any breaches of data protection regulations or data leaks within hours of discovering the breach harsh sanctions where breaches of the unified eu data protection law occur with penalties of up to 2 of a company worldwide turnover for severe data protec tion breaches 2 5 the united kingdom data protection act of the intent of the united kingdom data protection act dpa of opsi is to uphold eight data protection principles that are outlined in table 1 these principles were borrowed from the eu directive on data protection under this act citizens have the right to request to inspect copies of data that table 1 u k data protection act opsi 1 personal data shall be processed fairly and lawfully and in particular shall not be processed unless it is consented to or necessary the conditions under which processing is considered necessary are explicitly listed in schedule 2 and schedule 3 of the act 2 personal data shall be obtained only for one or more specified and lawful purposes and shall not be further processed in any manner incompatible with that purpose or those purposes 3 personal data shall be adequate relevant and not excessive in relation to the purpose or purposes for which they are processed 4 personal data shall be accurate and where necessary kept up to date 5 personal data processed for any purpose or purposes shall not be kept for longer than is necessary for that purpose or those purposes 6 personal data shall be processed in accordance with the rights of data subjects under this act 7 appropriate technical and organizational measures shall be taken against unauthorized or unlawful processing of personal data and against accidental loss or destruction of or damage to personal data 8 personal data shall not be transferred to a country or territory outside the european economic area unless that country or territory ensures an adequate level of protection for the rights and freedoms of data subjects in relation to the processing of personal data any organization keeps about them and to request inaccuracies to be corrected organizations that collect and maintain such data must have clear policies regard ing how to respond to requests to inspect data as well as requests to share data with other organizations such policies clearly need to be consistent with the law as well as the ethical standards of the organization 2 6 access to information laws according to righttoinfo org the organization that monitors and lobbies right to information laws as of january over countries had nationwide laws provid ing the right of and procedures for public access to government held information the first country to establish an access to information law was sweden in fol lowed by finland in the us in and the remaining countries to date over the following years the overarching driving force behind these laws similar to the dpa is to increase openness and accountability in public authorities for some countries an additional driving force is to reduce corruption and encourage formal practice or legal behaviors the uk established the freedom of information act and freedom of information scotland act in foisa this provides any member of the public the right to access the wealth of information held by public authorities from hospitals and emergency services to local authorities central government and higher education institutions the act has eight parts with several provisions within them and overlaps with the dpa the first two that provide the overview of the law are described in table 2 table 2 excerpt from uk freedom of information act part 1 access to information held by public authorities provides for the general right of access to recorded information held by public authorities and specifies the conditions which need to be fulfilled before an authority is obliged to comply with a request describes the effect of the exemptions in part ii on the obligations under section 1 provides for the act to cover the bodies persons or office holders specified in schedule 1 and publicly owned companies and includes a power to specify further public authorities for the purpose of the act allows public authorities to charge fees in accordance with regulations made by the secretary of state provides for time limits for complying with a request makes special provision relating to public records transferred to the public record office etc requires public authorities to provide advice and assistance to applicants requires public authorities to state the basis for refusal of a request renames the data protection commissioner and data protection tribunal with consequential amendments to other legislation being made in schedule 2 requires public authorities to adopt and maintain a publication scheme and to publish information in accordance with it part 2 exemption information sets out the circumstances in which information is exempt information part 4 enforcement enables an applicant who is not satisfied with the response by a public authority to a request for information to apply to the commissioner for a decision on whether the authority has acted in accordance with the provisions of the act subject to certain conditions for example the exhaustion of other means of complaint the commissioner is under a duty to reach a decision describes the investigative and enforcement powers of the commissioner it confirms that the act does not give rise to any right of action against public authorities for breach of statutory duty this part also provides for the circumstances in which a certificate may be issued by an accountable person in respect of a decision notice or enforcement notice issued by the commissioner in respect of the disclosure of exempt information the effect of such a certificate is that a public authority need not comply with the commissioner notice the uk freedom of information laws are enforced by the information commissioner and they make recommendations on a model publication scheme that the relevant organizations adopt or work to comply with depending on the request and how accessible the information is from databases and records compli ance to these laws and their deadlines typically have potential administrative cost implications to an organization so as a result many choose to implement policies and procedures and provide training to staff as an integral part of their govern ance structures this is often handled by a department and or individual primarily responsible for legal compliance information or data management or a depart ment also handing dpa requests where requests reflect a larger administrative threshold cost to extract a decision is made as to whether to charge the requestor however in order to reduce some of the administration and costs to comply with and foia or foisa many organizations include within their policies procedures for voluntarily publishing non exempt information regularly often to a web site accessible to the public or internally additionally as a result implications for recording storing and managing data across the organization in such a way that it can be made accessible at short notice with relatively low costs to comply with the information commissioner deadlines are also taken into consideration in the decision making structures information and data management policies 2 7 international banking basel ii accords the international convergence of capital measurement and capital standards oth erwise known as basel ii is a revision to the basel capital accord basel i produced by the basel committee on banking supervision bcbs these are recommended policies that must be enacted into law in each individual country and monitored by the appropriate national regulators in the u s this is the federal reserve bank for the 10 largest internationally active u s banks and the sec for securities firms these two regulators were created to level the play ing field among globally competitive institutions and to set standards to minimize systemic risk in the world financial system institutions in the international bank ing system are interconnected in a number of ways through agreements loans and other credit and debt obligations the fear has been that the failure of one large firm could cause defaults in countries and organizations far removed from the failing institution the basel ii framework consists of three main pillars 1 minimum capital requirements institutions must maintain sufficient capital funds given the level of risk inherent in their portfolio of assets loans securi ties and so on the measurement of risk has been revised and expanded to include a credit risk the risk that creditors will not be able to repay their debts principle and interest b market risk the risk that all investments will decline in value as the entire market or economy declines or due to industry or firm specific causes including interest rate and currency risks c interest rate risk the risk that investment will lose value as a result of inter est rate increases d operational risk the risk of loss as a result of poor internal controls operations systems or human resources or the risk of loss as a result of some external event such as a natural disaster 2 supervisory review process management must understand and actively control the risks have sufficient internal risk controls and timely reporting including compensation plans that reward appropriate risk management behavior 3 market discipline institutions must publicly disclose information about their capital adequacy risk exposures and the processes by which they measure and mitigate risks in order to calculate market risks firms must aggregate positions from all trading loans credit card cars homes business and so on and financial operations at least daily and for trading in real time value at risk calculations need historical data of one or two years or more for each asset to be able to calculate the variance covariance matrices required by risk models including monte carlo simulations credit risk models use external sources such as standard poor credit rating for public companies and large liquid instruments but banks must still maintain significant data to execute their internal risk models this includes credit histories business financial data loan officer reports and so on assessing operational risk is even more data intensive as basel ii requires at least five years of data operational risk assessment requires analysis of high frequency but low value events and more critically high value infrequent events for which there is little statistical evidence upon which to base capital requirements in the united states and europe con sortiums are being established where banks are sharing operational risk events so that each member has a base upon which to develop their internal models of opera tional risk as with the sarbanes oxley legislation having effective internal controls in place also plays a large role in mitigating operation risk following the financial crisis of 9 bcbs significantly revised its existing capital adequacy guidelines creating basel iii the objective was to strengthen global capital and liquidity rules with the goal of promoting a more resilient banking sector the finance ministers and central bank governors for major economies endorsed basel iii at the november summit in seoul while not all details have been finalized the core principles have been agreed and a deadline has been established for institutions to complete implementation as with basel ii the quality of the data and meta data financial institutions hold and exchange will be of paramount importance in implementing basel iii 3 establishing a culture of legal and ethical data stewardship the complexity and it implications of the recent legislation discussed in the pre vious section raise issues that are vital to employees throughout an organization senior managers such as board members presidents chief information officers cios and data administrators are increasingly finding themselves liable for any violations of these laws it is therefore mandatory that official policies be created and articulated to employees at all organizational levels an obvious question arises where do we start some basic steps are outlined next 3 1 developing an organization wide policy for legal and ethical behavior first it is important that the senior management team is aware of new legislation and changes in industry practice an assessment of how these changes affect the organiza tion is also a critical first step often issues will arise as a result of the growing inter connected nature of global business for example a firm may find that it is subject to the stricter laws of a foreign country in which it does business and as a result may have to adjust its entire operations according to the most stringent standard next data administrators and cios need to assess how legislation affects the flow of data through the organization special attention must be paid to how data is collected stored secured and accessed by users who may be internal or external to the organization many of the security techniques discussed in chapter can be applied in this case following this new or revised operating procedures must be documented and communicated to all affected parties again an assessment may reveal additional employees or trading partners involved in working with sensitive data or processes that were previously overlooked once explicit rules for conducting business within legal parameters have been developed a similar set of ethical principles for the business should be developed often organizations already have a corporate wide statement of ethics that can be used as a starting point as already noted any resulting policies must be docu mented and articulated to all employees in such a way that they come to understand the seriousness with which senior management takes these issues finally lapses in legal and ethical behavior must be dealt with swiftly and fairly and within the guidelines made known to all employees such incidences can also serve to help refine policies and procedures going forward such that legal and ethi cal policy statements evolve over time to adapt to new business situations another potential source of general guidelines can be an existing codes of ethics codes of conduct and or codes of practice document that has been adopted by a professional it related society or organization the next section discusses two such codes documents 3 2 professional organizations and codes of ethics many professional organizations have a code of ethics that all members pledge to uphold perhaps the most comprehensive code of ethics for it comes from the association for computing machinery acm an organization in existence since with more than 000 members worldwide www acm org the acm code of ethics and professional conduct acm consists of statements of per sonal responsibility in four main categories fundamental ethical considerations this category addresses eight areas contribute to society and human well being avoid harm to others be honest and trustworthy be fair and take action not to discriminate honor property rights including copyrights and patent give proper credit for intellectual property respect the privacy of others honor confidentiality specific considerations of professional conduct this category addresses eight areas strive to achieve the highest quality effectiveness and dignity in both the pro cess and products of professional work acquire and maintain professional competence know and respect existing laws pertaining to professional work accept and provide appropriate professional review give comprehensive and thorough evaluations of computer systems and their impacts including analysis of possible risks honor contracts agreements and assigned responsibilities improve public understanding of computing and its consequences access computing and communication resources only when authorized to do so considerations for individuals in leadership roles this category covers six areas articulate social responsibilities of members of an organizational unit and encourage full acceptance of those responsibilities manage personnel and resources to design and build information systems that enhance the quality of working life acknowledge and support proper and authorized uses of an organization computing and communication resources ensure that users and those who will be affected by a system have their needs clearly articulated during the assessment and design of requirements later the system must be validated to meet requirements articulate and support policies that protect the dignity of users and others affected by a computing system create opportunities for members of the organization to learn the principles and limitations of computer systems compliance with the code this final category addresses two main points uphold and promote the principles of this code treat violations of this code as inconsistent with membership in the acm the british computer society www bcs org was founded in and currently has more than 000 members in countries the bcs code of conduct bcs which all bcs members agree to uphold specifies conduct in four main areas 1 public interest a you shall have due regard for public health privacy security and well being of others and the environment b you shall have due regard for the legitimate rights of third parties c you shall conduct your professional activities without discrimination on the grounds of sex sexual orientation marital status nationality color race ethnic origin religion age or disability or of any other condition or requirement d you shall promote equal access to the benefits of it and seek to promote the inclusion of all sectors in society wherever opportunities arise 2 professional competence and integrity a you shall only undertake to do work or provide a service that is within your professional competence b you shall not claim any level of competence that you do not possess c you shall develop your professional knowledge skills and competence on a continuing basis maintaining awareness of technological developments procedures and standards that are relevant to your field d you shall ensure that you have the knowledge and understanding of legislation and that you comply with such legislation in carrying out your professional responsibilities e you shall respect and value alternative viewpoints and seek accept and offer honest criticisms of work f you shall avoid injuring others their property reputation or employment by false or malicious or negligent action or inaction g you shall reject and will not make any offer of bribery or unethical inducement 3 duty to relevant authority a you shall carry out your professional responsibilities with due care and diligence in accordance with the relevant authority requirements while exercising your professional judgement at all times b you shall seek to avoid any situation that may give rise to a conflict of inter est between you and your relevant authority c you shall accept professional responsibility for your work and for the work of colleagues who are defined in a given context as working under your supervision d you shall not disclose or authorize to be disclosed or use for personal gain or to benefit a third party confidential information except with the permission of your relevant authority or as required by legislation e you shall not misrepresent or withhold information on the performance of products systems or services unless lawfully bound by a duty of confi dentiality not to disclose such information or take advantage of the lack of relevant knowledge or inexperience of others 4 duty to the profession a you shall accept your personal duty to uphold the reputation of the profes sion and not take any action which could bring the profession into disre pute b you shall seek to improve professional standards through participation in their development use and enforcement c you shall uphold the reputation and good standing of bcs the chartered institute for it d you shall act with integrity and respect in your professional relationships with all members of bcs and with members of other professions with whom you work in a professional capacity e you shall notify bcs if convicted of a criminal offence or upon becom ing bankrupt or disqualified as a company director and in each case give details of the relevant jurisdiction f you shall encourage and support fellow members in their professional development the acm code and bcs code are similar in that both begin by establishing grounding in providing an overall benefit to society from that point performing one professional duties to the highest possible standard and carrying out duties in a legal and ethical manner are paramount recognition of intellectual property rights discussed next and acknowledgement of sources respecting privacy and confidentiality and overall concern for public health safety and the environment are also common themes both codes explicitly mention a member duty to under stand and comply with all relevant laws regulations and standards something that is highly relevant as discussed in this chapter both codes also mention duties to one superiors as well as to the public at large it should not be surprising that the two major computer societies in the united states and united kingdom share much common ground due to their common language and general common ground regarding law and ethics however not all countries share the same societal values as the u s and u k therefore we can find situations in several countries where concepts such as an individual right to privacy and antidiscrimination are not consistent with u s and u k norms these existing codes and others as cited by lee can be used as a resource for organizations wishing to establish their own similar codes 3 3 developing an organization wide policy for legal and ethical behavior for dreamhome in this section we outline the steps the dreamhome property rentals company might take to develop an organization wide policy that addresses legal and ethical behav ior as a business dreamhome interacts with private and business property owners clients who wish to or who already rent property and other organizations such as newspaper businesses on a daily basis some of the data dreamhome maintains such as the amount a client pays to rent a particular property could be considered sensitive information in a similar fashion a client rental history and payment information are also considered very sensitive dreamhome policy should therefore explicitly address interactions between dreamhome staff and clients and business partners such as owners critical points include treating clients with respect e g in email and over the phone treating business partners with respect taking special care to limit information disclosure to business partners includ ing the proper procedures for handling information requests the security of clients and other business data critical points include raising awareness of the sensitivity of a client personal data payment history credit card numbers and rental history ensuring appropriate security measures are maintained to protect these sensi tive data proper procedures for handling data requests from º internal employees e g proposals for data mining or accessing sensitive cli ent data º owners e g request to reset a password º business partners if such data sharing is allowed º and possibly law enforcement e g a request for a client payment informa tion or rental history the use of company resources hardware software internet and so on critical points include computer hardware may not be removed from the premises without branch manager approval licensed computer software may not be copied distributed or otherwise used improperly additional software may not be installed without it department approval internet resources may not be used for noncompany business ramifications for violating the security and or trust of clients and business partner data critical points include all violations will be documented and presented before an oversight board con sisting of representatives from different business areas as well as management levels serious violations will also be reported to appropriate authorities willful or malicious violations will be met with dismissal and prosecution other violations will result in sanctions commensurate with the seriousness of the violation as determined by the oversight board finally dreamhome should further establish a procedure by which the policy is reviewed annually and or in the wake of any major violations of the policy or other incidents to ensure that the policy does not become outdated as the technology and business environment changes 4 intellectual property in this final section we introduce some of the main concepts underlying intellec tual property sometimes referred to by the simple acronym ip it is important that data and database administrators as well as business analysts and software developers recognize and understand the issues surrounding ip to ensure both that their ideas can be protected and that other people rights are not infringed we start with a definition the product of human creativity in the industrial scientific literary and artistic fields intellectual property includes inventions inventive ideas designs patents and patent applications discoveries improvements trademarks designs and design rights registered and unregistered written work including computer software and know how devised developed or written by an individual or set of individuals ip generated through the course of employment legally belongs to the employer unless specifically agreed otherwise in the same way that ownership of tangible products gives the owner rights the ownership of intangible property attempts to provide similar rights to allow owners the rights of exclusivity to give away license or sell their intellectual property although the exclusive nature of intellectual property rights can seem strong the strength of ip laws are tempered by limita tions placed on their duration and or scope as well as the owner freedom not to enforce their rights we can distinguish two types of ip background ip ip that exists before an activity takes place foreground ip ip that is generated during an activity a project may make use of background ip owned by someone other than the organization but in this case relevant contractual arrangements should be put in place with the owner of the ip there are three main ways to protect ip rights or ipr patents copyright and trademarks as we now discuss 4 1 patent provides an exclusive legal right for a set period of time to make use sell or import an invention patents are granted by a government when an individual or organization can dem onstrate the invention is new the invention is in some way useful the invention involves an inventive step in addition one of the key considerations of the patent system is that the patent application must disclose how the invention works this information is then dis seminated to the public once a patent is issued thereby increasing the wealth of public knowledge patents give effective protection for new technology that will lead to a product composition or process with significant long term commercial gain note however that artistic creations mathematical models plans schemes or other purely mental processes cannot be patented 4 2 copyright provides an exclusive legal right for a set period of time to repro duce and distribute a literary musical audiovisual or other work of authorship unlike a patent where rights are granted through a formal application process copyright comes into effect as soon as what is created takes a fixed form for example in writing or in sound copyright covers not just work like books arti cles song lyrics music cds videos dvds and tv programs but also computer software databases technical drawings and designs and multimedia copyright holders can sell the rights to their works to individuals or organizations in return for payment which are often referred to as royalties there are some exceptions to copyright so that some minor uses may not infringe copyright for example lim ited use for noncommercial research private study and teaching purposes copyright also gives moral rights to be identified as the creator of certain kinds of material and to object to distortion or mutilation of it although copyright does not require registration many countries allow for registration of works for example to identify titles of works or to serve as prima facie evidence in a court of law when copyright is infringed or disputed 4 3 trademark provides an exclusive legal right to use a word symbol image sound or some other distinctive in connection with certain goods or services element that identifies the source of origin a third form of protection is the trademark generally trademarks are intended to be associated with specific goods and services and as a result they assist con sumers in identifying the nature and quality of the products they purchase like patents and copyright a trademark gives the owner exclusive legal rights to use license or sell the goods and services for which it is registered like copyright a trademark does not have to be registered although registration may be advisable as it can be expensive and time consuming to take action under common law for the dreamhome property rentals case study the company may decide to trademark their business name 4 4 intellectual property rights issues for software as noted earlier it is important to understand ipr for a number of reasons to understand your own right or your organization right as a producer of origi nal ideas and works to recognize the value of original works to understand the procedures for protecting and exploiting such work to know the legal measures that can be used to defend against the illegal use of such work to be fair and sensible about legitimate use of your work for nonprofit purposes in this section we briefly discuss some of the issues related specifically to ipr and software software and patentability in the and there were extensive discussions on whether patents or cop yright should provide protection for computer software these discussions resulted in a generally accepted principle that software should be protected by copyright whereas apparatus using software should be protected by patent however this is less clear nowadays although the u k specifically excludes software from patent protection there has been some latitude in the interpretation where software forms part of the overall machinery or industrial process therefore an application to just patent a piece of software will be refused but an application to patent some technical effect that is produced by a piece of software will be considered subject to constraints discussed in section 5 1 in the united states patentability has been extended to cover what are termed business methods and many software patents have been granted and more have been applied for particularly with the growth of the internet mobile operating systems applications and e commerce the introduction of smartphones and tablet computers in recent years has opened up a competitive arena for new innovations and dominance of operating systems and devices that has given rise to fierce and strategic battles by large mul tinational companies like google apple samsung microsoft motorola and htc over their patents some of these disputes and applications have been controversial because in addition to the objectives of protecting the innovation the resultant court battles have been seen as aimed at tactically preventing or delaying sales of devices in certain regions and therefore market share this represents just one example where the law faces new challenges in adapting to changes and innova tions that are taking place in the environment to ensure legal and ethical behaviors in organizations when it comes to intellectual property software and copyright all software has one or more authors who assert the right to their intellectual property in what they have written copyright applies therefore to all software whether or not you have paid money for it and the distribution and use of software is subject to a license that specifies the terms of use the conditions that apply to a particular piece of software depend on a number of things but in general there are four types of license commercial software perpetual use in this case a fee is paid for the software and the license allows the software to be used for as long as you like generally on one machine and to make copies only for the purpose of backup if something goes wrong with the machine software can be transferred to a different machine but it must be deleted from the old one first in some cases a license may permit use on more than one machine but this would be explicit in the license terms commercial software annual fee this is similar to the perpetual use license but a fee may be required for each year of continued use and in most cases the software stops working unless the fee is paid and a new license key is issued by the supplier annual rental often applies to site licenses in which once the fee is paid the organization may use the software on as many machines as it likes and to software on mainframe or server computers again the license terms will be explicit as to what use is allowed shareware software made available initially for a free trial period if after the initial period for example 30 days we wish to continue using the software we are asked to send a usually small fee to the author of the software in some cases the software enforces this by refusing to work after the trial period but irrespective of this by using the software you are accepting the license terms and are infringing the author copyright if we continue to use the software after the trial period in return for the fee a more up to date version of the software may be provided that does not constantly remind the user to register and pay for the software freeware software made available free for certain categories of use such as education or personal use there are two main types of freeware software that is distributed without the source code preventing modification by users and open source software oss the latter is usually issued under a license such as the gnu public license gpl that specifies the terms and conditions of free use the main restrictions are that the software cannot be used for commercial purposes although we are usually allowed to modify the software but are duty bound to submit any improvements that we make to the author so that they can incorporate them in future releases a second restriction is that the text of the copyrighted gpl license itself be included with any redistribution note that in neither of the first two cases are we permitted to attempt to modify or reverse engineer the software or remove any copyright messages and the like all software has license conditions even software downloaded free from the internet and failure to comply with the license conditions is an infringement of copyright 4 5 intellectual property rights issues for data consideration must also be paid to data that an organization collects processes and possibly shares with its trading partners in conjunction with senior management and legal counsel data administrators must define and enforce policies that govern when data can be shared and in what ways it can be used within the organization for example consider the data that dreamhome maintains on its clients rental habits it is entirely possible that other retailers target marketing firms or even law enforcement would be interested in gaining access to detailed transaction histories of clients for some businesses sharing limited data such as purchase patterns without revealing individual identities or aggregated data may make sense from a revenue standpoint in the event that a business case is made for sharing data appropriate licensing of the data must be put into effect so that it is not reshared with other parties although not exclusively a cloud computing problem with its increasing use and transnational data storage the ability to have complete control over this issue of access to personal or corporate ip has increased as a concern for example some governments have established laws that provide them with unrestricted access to cor porate or individual private data the usa patriot act established as a direct response to the terrorist attacks of september forces u s companies with local or foreign servers or companies with data controlled by a u s company to release data on individuals even if their local laws for example eu prevent them from doing so since a u s company and its subsidiaries operating in europe are still subject to the u s patriot act european customers that use their services are expos ing themselves to u s laws the u s and eu swiss safe harbour framework was established to avoid this situation however there have been reports from some us companies operating in europe that have admitted that this has proved ineffective review questions it governance is used for specifying the decision rights and accountability framework to encourage desirable behavior in the use of it what constitutes legal behavior is most often aligned with ethical behavior although this is not always the case most of the legislation discussed in this chapter was put into place to help diminish the possibility of unintended information disclosure most of the legislative acts discussed have at their core a mandate to protect the data of clients while at the same time increasing the reporting requirements of companies to official agencies both of these general items have at their core data management issues internal controls are a set of measures that an organization adopts to ensure that policies and procedures are not violated data is properly secured and reliable and operations can be carried out efficiently establishing a corporate wide and certainly it wide awareness of security privacy and reporting as it relates to the data an organization collects and processes is a critical task especially given the current regulatory environment intellectual property ip includes inventions inventive ideas designs patents and patent applications discov eries improvements trademarks designs and design rights written work and know how devised developed or written by an individual or set of individuals background intellectual property is ip that already exists before an activity takes place foreground intellectual property is ip that is generated during an activity patent provides an exclusive legal right for a set period of time to make use sell or import an invention copyright provides an exclusive legal right for a set period of time to reproduce and distribute a literary musical audiovisual or other work of authorship trademark provides an exclusive legal right to use a word symbol image sound or some other distinc tion element that identifies the source of origin in connection with certain goods or services another make use sell or import an invention 1 explain the motives behind organizational ethics and legislations 2 describe business situations in which an individual or businesses behavior would be considered a illegal and unethical b legal but unethical c illegal but ethical 3 explain the role of the organizational code of conduct in relation to local and international legislation 4 describe the importance of it governance and its relationship to legal and ethical practice in organizations 5 explain how an international business can be influenced by laws from a foreign land and how it may face liability from its customers for reasons beyond its control 6 explain the legal risks to an international business in data management 7 describe how an entity or individuals outside and within organizations can have legitimate power to access corporate data and therefore influence how it is managed 8 describe some of the legal challenges faced in protecting intellectual property and promoting innovation with emerging technology 9 suppose that you are a data administrator in a large european pharmaceutical manufacturer that has significant sales and marketing efforts in europe japan and the united states what data management issues would you have to be most concerned with 10 suppose that you have just joined a large financial services company as the head of it and are asked to create a formal code of ethics for it what steps would you take to research this task and what resources would you consider 11 access peter neumann inside risk article archives for the communications of the acm visit www csl sri com users neumann insiderisks html summarize in a few paragraphs a recent article from these archives dealing with legal and or ethical issues related to it 12 access the acm code of ethics and professional conduct and the bcs code of conduct and code of good practice when comparing the two discuss elements that are emphasized more or less in one code than another 13 access the singapore computer society code of conduct www scs org sg php compare this code with either the acm or bcs code and note any differences 14 consider the dreamhome case study described in chapter 11 produce a report for the director of the company outlining the legal and ethical issues that need to be considered and make any recommendations that you think are appropriate 15 consider the case studies described in appendix b produce a report for each case study outlining the legal and ethical issues that need to be considered and make any recommendations you think appropriate 16 suppose you are the chief information officer for a public university in europe what data management issues would you have to be concerned with and what governance policies and procedures would you have to con sider putting in place to ensure you comply with legal standards 17 describe the it governance mechanisms that could be implemented in your organization to improve how it and data is managed 18 describe with examples how legal frameworks in place to support transnational management of data can prove ineffective in protecting the privacy of business or consumers 19 most of the legislations described in this chapter were triggered by a problematic situation facing the society analyze the situations in your country and identify problems that in one way or another forced the introduction of the available known legislation 20 it governance plays a vital role in ensuring accountability and decision rights proper management and use of it resources helps organizations meet their objectives critically analyze how it governance mechanisms are applied in an organization of your choice chapter transaction management in chapter 2 we discussed the functions that a database management system dbms should provide among these are three closely related functions that are intended to ensure that the database is reliable and remains in a consistent state transaction support concurrency control services and recovery services this reliability and consistency must be maintained in the presence of failures of both hardware and software components and when multiple users are accessing the database in this chapter we concentrate on these three functions although each function can be discussed separately they are mutually depend ent both concurrency control and recovery are required to protect the database from data inconsistencies and data loss many dbmss allow users to undertake simultaneous operations on the database if these operations are not controlled the accesses may interfere with one another and the database can become inconsist ent to overcome this the dbms implements a concurrency control protocol that prevents database accesses from interfering with one another database recovery is the process of restoring the database to a correct state fol lowing a failure the failure may be the result of a system crash due to hardware or software errors a media failure such as a head crash or a software error in the application such as a logical error in the program that is accessing the database it may also be the result of unintentional or intentional corruption or destruction of data or facilities by system administrators or users whatever the underlying cause of the failure the dbms must be able to recover from the failure and restore the database to a consistent state structure of this chapter central to an understanding of both concurrency control and recovery is the notion of a transaction which we describe in section 1 in section 2 we discuss concurrency control and examine the protocols that can be used to prevent conflict in section 3 we discuss database recovery and examine the techniques that can be used to ensure the database remains in a consistent state in the presence of failures in section 4 we examine more advanced transaction models that have been proposed for transactions that are of a long duration from hours to possibly even months and have uncertain developments so that some actions cannot be foreseen at the beginning in section 5 we examine how oracle handles concurrency control and recovery in this chapter we consider transaction support concurrency control and recovery for a centralized dbms that is a dbms that consists of a single database later in chapter we consider these services for a distributed dbms that is a dbms that consists of multiple logically related databases distributed across a network 1 transaction support an action or series of actions carried out by a single user or application program that reads or updates the contents of the database a transaction is treated as a logical unit of work on the database it may be an entire program a part of a program or a single statement for example the sql statement insert or update and it may involve any number of operations on the figure 1 example transactions database in the database context the execution of an application program can be thought of as one or more transactions with nondatabase processing taking place in between to illustrate the concepts of a transaction we examine two relations from the instance of the dreamhome rental database shown in figure 4 3 staff staffno fname iname position sex dob salary branchno propertyforrent propertvno street city postcode type rooms rent ownerno staffno branchno a simple transaction against this database is to update the salary of a particular mem ber of staff given the staff number x at a high level we could write this transaction as shown in figure 1 a in this chapter we denote a database read or write opera tion on a data item x as read x or write x additional qualifiers may be added as necessary for example in figure 1 a we have used the notation read staffno x salary to indicate that we want to read the data item salary for the tuple with primary key value x in this example we have a transaction consisting of two database opera tions read and write and a nondatabase operation salary salary 1 1 a more complicated transaction is to delete the member of staff with a given staff number x as shown in figure 1 b in this case as well as having to delete the tuple in the staff relation we also need to find all the propertyforrent tuples that this member of staff managed and reassign them to a different member of staff newstaffno say if all these updates are not made referential integrity will be lost and the database will be in an inconsistent state a property will be managed by a member of staff who no longer exists in the database a transaction should always transform the database from one consistent state to another although we accept that consistency may be violated while the transaction is in progress for example during the transaction in figure 1 b there may be some moment when one tuple of propertyforrent contains the new newstaffno value and another still contains the old one x however at the end of the transaction all necessary tuples should have the new newstaffno value a transaction can have one of two outcomes if it completes successfully the transaction is said to have committed and the database reaches a new consistent state on the other hand if the transaction does not execute successfully the figure 2 state transition diagram for a transaction transaction is aborted if a transaction is aborted the database must be restored to the consistent state it was in before the transaction started such a transaction is rolled back or undone a committed transaction cannot be aborted if we decide that the committed transaction was a mistake we must perform another compensating transaction to reverse its effects as we discuss in section 4 2 however an aborted transaction that is rolled back can be restarted later and depending on the cause of the failure may successfully execute and commit at that time the dbms has no inherent way of knowing which updates are grouped together to form a single logical transaction it must therefore provide a method to allow the user to indicate the boundaries of a transaction the keywords begin transaction commit and rollback or their equivalent are available in many data manipulation languages to delimit transactions if these delimiters are not used the entire program is usually regarded as a single transaction with the dbms automatically performing a commit when the program terminates correctly and a rollback if it does not figure 2 shows the state transition diagram for a transaction note that in addition to the obvious states of active committed and aborted there are two other states while high level data parallel frameworks like mapreduce sim plify the design and implementation of large scale data processing systems they do not naturally or efficiently support many important data mining and machine learning algorithms and can lead to ineffi cient learning systems to help fill this critical void we introduced the graphlab abstraction which naturally expresses asynchronous dynamic graph parallel computation while ensuring data consis tency and achieving a high degree of parallel performance in the shared memory setting in this paper we extend the graphlab framework to the substantially more challenging distributed setting while preserving strong data consistency guarantees we develop graph based extensions to pipelined locking and data versioning to reduce network congestion and mitigate the effect of network latency we also introduce fault tolerance to the graphlab abstraction using the classic chandy lamport snapshot algorithm and demonstrate how it can be easily implemented by exploiting the graphlab abstraction itself finally we evaluate our distributed implementation of the graphlab abstraction on a large amazon deployment and show orders of magnitude performance gains over hadoop based implementations introduction with the exponential growth in the scale of machine learning and data mining mldm problems and increasing sophistication of mldm techniques there is an increasing need for systems that can execute mldm algorithms efficiently in parallel on large clusters simultaneously the availability of cloud computing services like amazon provide the promise of on demand access to afford able large scale computing and storage resources without substantial upfront investments unfortunately designing implementing and debugging the distributed mldm algorithms needed to fully utilize the cloud can be prohibitively challenging requiring mldm experts to address race conditions deadlocks distributed state and commu nication protocols while simultaneously developing mathematically complex models and algorithms nonetheless the demand for large scale computational and stor age resources has driven many to develop new parallel and distributed mldm systems targeted at individual mod els and applications this time consuming and often redundant effort slows the progress of the field as different research groups repeatedly solve the same parallel distributed computing problems therefore the mldm community needs a high level distributed abstraction that specifically targets the asynchronous dynamic graph parallel computation found in many mldm applications while hiding the complexities of parallel distributed system design unfortunately existing high level parallel abstractions e g mapreduce dryad and pregel fail to support these critical properties to help fill this void we introduced graphlab abstraction which directly targets asynchronous dynamic graph parallel computation in the shared memory setting in this paper we extend the multi core graphlab abstraction to the distributed setting and provide a formal description of the distributed execution model we then explore several methods to implement an efficient distributed execution model while preserving strict con sistency requirements to achieve this goal we incorporate data versioning to reduce network congestion and pipelined distributed locking to mitigate the effects of network latency to address the challenges of data locality and ingress we introduce the atom graph for rapidly placing graph structured data in the distributed setting we also add fault tolerance to the graphlab framework by adapting the classic chandy lamport snapshot algorithm and demonstrate how it can be easily implemented within the graphlab abstraction we conduct a comprehensive performance analysis of our optimized c implementation on the amazon elastic cloud computing service we show that applications created using graphlab outperform equivalent hadoop mapreduce implementations by and match the performance of carefully constructed mpi implementations our main contributions are the permission to make digital or hard copies of all or part of this work for following personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee articles from this volume were invited to present a summary of common properties of mldm algorithms and the limitations of existing large scale frameworks sec a modified version of the graphlab abstraction and execution their results at the international conference on very large data bases model tailored to the distributed setting sec august istanbul turkey proceedings of the vldb endowment vol no copyright vldb endowment two substantially different approaches to implementing the new distributed execution model sec chromatic engine uses graph coloring to achieve efficient sequentially consistent execution for static schedules locking engine uses pipelined distributed locking and la tency hiding to support dynamically prioritized execution fault tolerance through two snapshotting schemes sec implementations of three state of the art machine learning algo rithms on top of distributed graphlab sec an extensive evaluation of distributed graphlab using a pro cessor node cluster including comparisons to hadoop pregel and mpi implementations sec mldm algorithm properties in this section we describe several key properties of efficient large scale parallel mldm systems addressed by the graphlab abstraction and how other parallel frameworks fail to address these properties a summary of these properties and parallel frame works can be found in table graph structured computation many of the recent advances in mldm have focused on modeling the dependencies between data by modeling data dependencies we are able to extract more signal from noisy data for example modeling the dependencies between similar shoppers allows us to make better product recommendations than treating shoppers in isolation unfortunately data parallel abstractions like mapreduce are not generally well suited for the dependent computation typically required by more advanced mldm algorithms although it is often possible to map algorithms with computational dependencies into the mapreduce abstraction the resulting transformations can be challenging and may introduce substantial inefficiency as a consequence there has been a recent trend toward graph parallel abstractions like pregel and graphlab which naturally express computational dependencies these abstractions adopt a vertex centric model in which computation is defined as kernels that run on each vertex for instance pregel is a bulk syn chronous message passing abstraction where vertices communicate through messages on the other hand graphlab is a sequential shared memory abstraction where each vertex can read and write to data on adjacent vertices and edges the graphlab runtime is then responsible for ensuring a consistent parallel execution con sequently graphlab simplifies the design and implementation of graph parallel algorithms by freeing the user to focus on sequen tial computation rather than the parallel movement of data i e messaging asynchronous iterative computation many important mldm algorithms iteratively update a large set of parameters because of the underlying graph structure parameter updates on vertices or edges depend through the graph adjacency structure on the values of other parameters in contrast to synchronous systems which update all parameters simultaneously in parallel using parameter values from the previous time step as input asynchronous systems update parameters using the most recent parameter values as input as a consequence asynchronous systems provides many mldm algorithms with significant algorithmic benefits for example linear systems common to many mldm algorithms have been shown to converge faster when solved asynchronously additionally there are numerous other cases e g belief propagation expectation maximization and stochastic optimization where asynchronous procedures have been empirically shown to significantly outperform synchronous procedures in fig a we demonstrate how asyn chronous computation can substantially accelerate the convergence of pagerank synchronous computation incurs costly performance penalties since the runtime of each phase is determined by the slowest ma chine the poor performance of the slowest machine may be caused by a multitude of factors including load and network imbalances hardware variability and multi tenancy a principal concern in the cloud even in typical cluster settings each compute node may also provide other services e g distributed file systems imbalances in the utilization of these other services will result in substantial performance penalties if synchronous computation is used in addition variability in the complexity and convergence of the individual vertex kernels can produce additional variability in execution time even when the graph is uniformly partitioned for example natural graphs encountered in real world applications have power law degree distributions which can lead to highly skewed running times even with a random partition furthermore the actual work required for each vertex could depend on the data in a problem specific manner e g local rate of convergence while abstractions based on bulk data processing such as mapre duce and dryad were not designed for iterative computation recent projects such as spark extend mapreduce and other data parallel abstractions to the iterative setting however these abstractions still do not support asynchronous computation bulk synchronous parallel bsp abstractions such as pregel pic colo and bpgl do not naturally express asynchronicity on the other hand the shared memory graphlab abstraction was de signed to efficiently and naturally express the asynchronous iterative algorithms common to advanced mldm dynamic computation in many mldm algorithms iterative computation converges asymmetrically for example in parame ter optimization often a large number of parameters will quickly converge in a few iterations while the remaining parameters will converge slowly over many iterations in fig b we plot the distribution of updates required to reach convergence for pagerank surprisingly the majority of the vertices required only a single update while only about of the vertices required more than updates additionally prioritizing computation can further accelerate convergence as demonstrated by zhang et al for a variety of graph algorithms including pagerank if we update all parameters equally often we waste time recomputing parame ters that have effectively converged conversely by focusing early computation on more challenging parameters we can potentially accelerate convergence in fig c we empirically demonstrate how dynamic scheduling can accelerate convergence of loopy belief propagation a popular mldm algorithm several recent abstractions have incorporated forms of dynamic computation for example pregel supports a limited form of dynamic computation by allowing some vertices to skip computa tion on each super step other abstractions like pearce et al and graphlab allow the user to adaptively prioritize computation while both pregel and graphlab support dynamic computation only graphlab permits prioritization as well as the ability to adap tively pull information from adjacent vertices see sec for more details in this paper we relax some of the original graphlab scheduling requirements described in to enable efficient dis tributed fifo and priority scheduling serializability by ensuring that all parallel executions have an equivalent sequential execution serializability eliminates many chal lenges associated with designing implementing and testing parallel mldm algorithms in addition many algorithms converge faster if serializability is ensured and some even require serializability for correctness for instance dynamic als sec is unstable when allowed to race fig d gibbs sampling a very popular mldm algorithm requires serializability for statistical correctness enforce computation sparse async iterative prioritized model depend comp ordering consistency distributed mpi messaging yes yes yes n a no yes mapreduce par data flow no no extensions a no yes yes dryad par data flow yes no extensions b no yes yes pregel bpgl graphbsp yes no yes no yes yes piccolo distr map no no yes no partially c yes pearce et al graph visitor yes yes yes yes no no graphlab graphlab yes yes yes yes yes yes table comparison chart of large scale computation frameworks a describes and iterative extension of mapreduce b proposes an iterative extension for dryad c piccolo does not provide a mechanism to ensure consistency but instead exposes a mechanism for the user to attempt to recover from simultaneous writes 8000 sync pregel of vertices e async graphlab updates at convergence sweeps updates b dynamic pagerank sync pregel c i t r e v l a u async r o r f o r d i e dynamic async graphlab time a async vs sync pagerank c loopybp conv r o r r e g n i n i r e e b r a r not serializable m u t n serializable d als consistency figure a rate of convergence measured in l x error to the true pagerank vector versus time of the pagerank algorithm on a vertex edge web graph on processors b the distribution of update counts after running dynamic pagerank to convergence notice that the majority of the vertices converged in only a single update c rate of convergence of loopy belief prop agation on web spam detection d comparing serializable and non serializable racing execution of the dynamic als algorithm in sec on the netflix movie recommendation problem non serializable execution exhibits unstable convergence behavior an abstraction that enforces serializable computation eliminates much of the complexity introduced by concurrency allowing the mldm expert to focus on the algorithm and model design de bugging mathematical code in a concurrent program which has data corruption caused by data races is difficult and time consum ing surprisingly many asynchronous abstractions like do not ensure serializability or like piccolo provide only basic mech anisms to recover from data races graphlab supports a broad range of consistency settings allowing a program to choose the level of consistency needed for correctness in sec we describe several techniques we developed to enforce serializability in the distributed setting dist graphlab abstraction the graphlab abstraction consists of three main parts the data graph the update function and the sync operation the data graph sec represents user modifiable program state and stores both the mutable user defined data and encodes the sparse computational dependencies the update function sec represents the user computation and operate on the data graph by transforming data in small overlapping contexts called scopes finally the sync operation sec concurrently maintains global aggregates to ground the graphlab abstraction in a concrete problem we will use the pagerank algorithm as a running example e xample data graph the graphlab abstraction stores the program state as a directed graph called the data graph the data graph g v e d is a container that manages the user defined data d here we use the term data broadly to refer to model parameters algorithm state and even statistical data users can associate arbitrary data with each vertex d v v v and edge d u v u v e in the graph however as the graphlab abstraction is not dependent on edge directions we also use d u v to denote the data on both edge directions u v and v u finally while the graph data is mutable the structure is static and cannot be changed during execution example pagerank ex the data graph is directly obtained from the web graph where each vertex corresponds to a web page and each edge represents a link the vertex data d v stores r v the current estimate of the pagerank and the edge data d u v stores w u v the directed weight of the link update functions computation is encoded in the graphlab abstraction in the form of update functions an update function is a stateless procedure that modifies the data within the scope of a vertex and schedules the future execution of update functions on other vertices the scope of p age r ank the pagerank sively defines the rank of a webpage v r v α n α u links to v algorithm recur vertex v denoted by s v is the data stored in v as well as the data stored in all adjacent vertices and adjacent edges fig a a graphlab update function takes as an input a vertex v and its w u v r u scope s v in terms of the weighted w u v and returns the new versions of the data in the scope as well as a set vertices t ranks r u of the pages u that link to v as well as some probability α of randomly jumping to that page update f v s v the pagerank algorithm iterates eq until the pagerank changes by less than some small value ε s v t after executing an update function the modified data in s v is written back to the data graph the set of vertices u t are eventually executed by applying the update function f u s u following the execution semantics described later in sec rather than adopting a message passing or data flow model as in graphlab allows the user defined update functions complete freedom to read and modify any of the data on adjacent vertices and edges this simplifies user code and eliminates the need for the users to reason about the movement of data by controlling what vertices are returned in t and thus to be executed graphlab update functions can efficiently express adaptive computation for example an update function may choose to return schedule its neighbors only when it has made a substantial change to its local data there is an important difference between pregel and graphlab in how dynamic computation is expressed graphlab decouples the scheduling of future computation from the movement of data as a consequence graphlab update functions have access to data on adjacent vertices even if the adjacent vertices did not schedule the current update conversely pregel update functions are initiated by messages and can only access the data in the message limiting what can be expressed for instance dynamic pagerank is difficult to express in pregel since the pagerank computation for a given page requires the pagerank values of all adjacent pages even if some of the adjacent pages have not recently changed therefore the decision to send data pagerank values to neighboring vertices cannot be made by the sending vertex as required by pregel but instead must be made by the receiving vertex graphlab naturally expresses the pull model since adjacent vertices are only responsible for scheduling and update functions can directly read adjacent vertex values even if they have not changed example pagerank ex the update function for pagerank defined in alg computes a weighted sum of the current ranks of neighboring vertices and assigns it as the rank of the current vertex the algorithm is adaptive neighbors are scheduled for update only if the value of current vertex changes by more than a predefined threshold algorithm pagerank update function input vertex data r v from s v input edge data w u v u n v from s v input neighbor vertex data r u u n v from s v r r v old v α n r v save old pagerank foreach u n v do loop over neighbors r v r v α w u v r u if the pagerank changes sufficiently if r v schedule r old v neighbors ε then to be updated return u u n v output modified scope s v with new r v the graphlab execution model the graphlab execution model presented in alg follows a simple single loop semantics the input to the graphlab abstraction consists of the data graph g v e d an update function an initial set of vertices t to be executed while there are vertices re maining in t the algorithm removes line and executes line vertices adding any new vertices back into t line duplicate vertices are ignored the resulting data graph and global values are returned to the user on completion to enable a more efficient distributed execution we relax the execution ordering requirements of the shared memory graphlab abstraction and allow the graphlab run time to determine the best order to execute vertices for example removenext t line algorithm graphlab execution model input data graph g v e d input initial vertex set t while t is not empty do v v v removenext t t s v f v s v t t t output modified data graph g v e d may choose to return vertices in an order that minimizes network communication or latency see sec the only requirement imposed by the graphlab abstraction is that all vertices in t are eventually executed because many mldm applications benefit from prioritization the graphlab abstraction allows users to assign priorities to the vertices in t the graphlab run time may use these priorities in conjunction with system level objectives to optimize the order in which the vertices are executed ensuring serializability the graphlab abstraction presents a rich sequential model which is automatically translated into a parallel execution by allowing multiple processors to execute the same loop on the same graph removing and executing different vertices simultaneously to retain the sequential execution semantics we must ensure that overlap ping computation is not run simultaneously we introduce several consistency models that allow the runtime to optimize the parallel execution while maintaining serializability the graphlab runtime ensures a serializable execution a seri alizable execution implies that there exists a corresponding serial schedule of update functions that when executed by alg pro duces the same values in the data graph by ensuring serializability graphlab simplifies reasoning about highly asynchronous dynamic computation in the distributed setting a simple method to achieve serializability is to ensure that the scopes of concurrently executing update functions do not overlap in we call this the full consistency model see fig b however full consistency limits the potential parallelism since concurrently executing update functions must be at least two vertices apart see fig c however for many machine learning algorithms the update functions do not need full read write access to all of the data within the scope for instance the pagerank update in eq only requires read access to edges and neighboring vertices to provide greater parallelism while retaining serializability graphlab defines the edge consistency model the edge consistency model ensures each update function has exclusive read write access to its vertex and adjacent edges but read only access to adjacent vertices fig b as a consequence the edge consistency model increases parallelism by allowing update functions with slightly overlapping scopes to safely run in parallel see fig c finally the vertex consistency model allows all update functions to be run in parallel providing maximum parallelism sync operation and global values in many mldm algorithms it is necessary to maintain global statistics describing data stored in the data graph for example many statistical inference algorithms require tracking global convergence estimators to address this need the graphlab abstraction defines global values that may be read by update functions but are written using sync operations similar to aggregates in pregel the sync operation is an associative commutative sum z finalize v v map s v read scope s write vertex data d d d d d d d d d d d d d d d d d d d d d d d d d read d d d d write edge data d d d d d d d d d d a data graph d d d d d d d d read b consistency models d d write d d d d d c consistency and parallelism figure a the data graph and scope s d d d d d d d d d d d d gray cylinders represent the user defined vertex and edge data while the irregular region containing the vertices is the scope s of vertex an update function applied to vertex is able to read and modify all the data in s vertex data d d d and d and edge data d d and d b the read and write permissions for an update function executed on vertex under each of the consistency models under the full consistency model the update function has complete read write access to its entire scope under the edge consistency model the update function has only read access to adjacent vertices finally vertex consistency model only provides write access to the central vertex data c the trade off between consistency and parallelism the dark rectangles denote the write locked regions that cannot overlap update functions are executed on the dark vertices in parallel under the stronger consistency models fewer functions can run simultaneously defined over all the scopes in the graph unlike pregel the sync operation introduces a finalization phase finalize to support tasks like normalization which are common in mldm algorithms also in contrast to pregel where the aggregation runs after each super step the sync operation in graphlab runs continuously in the background to maintain updated estimates of the global value since every update function can access global values ensuring serializability of the sync operation with respect to update functions is costly and will generally require synchronizing and halting all computation just as graphlab has multiple consistency levels for update functions we similarly provide the option of consistent or inconsistent sync computations distributed graphlab design in this section we extend the shared memory system design of the graphlab abstraction to the substantially more challenging distributed setting and discuss the techniques required to achieve this goal an overview of the distributed design is illustrated in fig a because of the inherently random memory access patterns common to dynamic asynchronous graph algorithms we focus on the distributed in memory setting requiring the entire graph and all program state to reside in ram our distributed is written in c and extends the original open sourced shared memory graphlab implementation the distributed data graph efficiently implementing the data graph in the distributed set ting requires balancing computation communication and storage therefore we need to construct balanced partitioning of the data graph that minimize number of edges that cross between machines because the cloud setting enables the size of the cluster to vary with budget and performance demands we must be able to quickly load the data graph on varying sized cloud deployments to resolve these challenges we developed a graph representation based on two phased partitioning which can be efficiently load balanced on arbitrary cluster sizes the data graph is initially over partitioned using domain specific knowledge e g planar embedding or by using a distributed graph partitioning heuristic e g parmetis random hashing into k open source c reference implementation of the distributed graphlab framework is available at http graphlab org parts where k is much greater than the number of machines each part called an atom is stored as a separate file on a distributed storage system e g hdfs amazon each atom file is a simple binary compressed journal of graph generating commands such as addvertex vdata and addedge edata in addition each atom stores information regarding ghosts the set of vertices and edges adjacent to the partition boundary the connectivity structure and file locations of the k atoms is stored in a atom index file as a meta graph with k vertices corresponding to the atoms and edges encoding the connectivity of the atoms distributed loading is accomplished by performing a fast balanced partition of the meta graph over the number of physical machines each machine then constructs its local portion of the graph by playing back the journal from each of its assigned atoms the playback procedure also instantiates the ghost of the local partition in memory the ghosts are used as caches for their true counterparts across the network cache coherence is managed using a simple versioning system eliminating the transmission of unchanged or constant data e g edge weights the two stage partitioning technique allows the same graph par tition computation to be reused for different numbers of machines without requiring a full repartitioning step a study on the quality of the two stage partitioning scheme is beyond the scope of this paper though simple experiments using graphs obtained from suggest that the performance is comparable to direct partitioning distributed graphlab engines the distributed graphlab engine emulates the execution model defined in sec and is responsible for executing update functions and sync operations maintaining the set of scheduled vertices t and ensuring serializability with respect to the appropriate consis tency model see sec as discussed in sec the precise order in which vertices are removed from t is up to the implementa tion and can affect performance and expressiveness to evaluate this trade off we built the low overhead chromatic engine which exe cutes t partially asynchronously and the more expressive locking engine which is fully asynchronous and supports vertex priorities chromatic engine a classic technique to achieve a serializable parallel execution of a set of dependent tasks represented as vertices in a graph is to construct a vertex coloring that assigns a color to each vertex such that no adjacent vertices share the same color given a vertex coloring of the data graph we can satisfy the edge consistency model by executing synchronously all vertices of the same color in the vertex set t before proceeding to the next color we use the term color step in analogy to the super step in the bsp model to describe the process of updating all the vertices within a single color and communicating all changes the sync operation can then be run safely between color steps we can satisfy the other consistency models simply by changing how the vertices are colored the full consistency model is satisfied by constructing a second order vertex coloring i e no vertex shares the same color as any of its distance two neighbors the vertex con sistency model is satisfied by assigning all vertices the same color while optimal graph coloring is np hard in general a reasonable quality coloring can be constructed quickly using graph coloring heuristics e g greedy coloring furthermore many mldm prob lems produce graphs with trivial colorings for example many optimization problems in mldm are naturally expressed as bipar tite two colorable graphs while problems based upon template models can be easily colored using the template while the chromatic engine operates in synchronous color steps changes to ghost vertices and edges are communicated asyn chronously as they are made consequently the chromatic engine efficiently uses both network bandwidth and processor time within each color step however we must ensure that all modifications are communicated before moving to the next color and therefore we require a full communication barrier between color steps distributed locking engine while the chromatic engine satisfies the distributed graphlab ab straction defined in sec it does not provide sufficient scheduling flexibility for many interesting applications in addition it presup poses the availability of a graph coloring which may not always be readily available to overcome these limitations we introduce the distributed locking engine which extends the mutual exclusion technique used in the shared memory engine we achieve distributed mutual exclusion by associating a readers writer lock with each vertex the different consistency models can then be implemented using different locking protocols vertex consistency is achieved by acquiring a write lock on the central vertex of each requested scope edge consistency is achieved by acquiring a write lock on the central vertex and read locks on adjacent vertices finally full consistency is achieved by acquiring write locks on the central vertex and all adjacent vertices deadlocks are avoided by acquiring locks sequentially following a canonical order we use the ordering induced by machine id followed by vertex id owner v v since this allows all locks on a remote machine to be requested in a single message since the graph is partitioned we restrict each machine to only run updates on local vertices the ghost vertices edges ensure that the update have direct memory access to all information in the scope each worker thread on each machine evaluates the loop described in alg until the scheduler is empty termination is evaluated using the distributed consensus algorithm described in a naive implementation alg will perform poorly due to the latency of remote lock acquisition and data synchronization we therefore rely on several techniques to both reduce latency and hide its effects first the ghosting system provides caching capabilities eliminating the need to transmit or wait on data that has not changed remotely second all lock requests and synchronization calls are pipelined allowing each machine to request locks and data for many scopes simultaneously and then evaluate the update function only when the scope is ready algorithm naive locking engine thread loop while not done do get next vertex v from scheduler acquire locks and synchronize data for scope s v execute t s v f v s v on scope s v update scheduler on each machine for each machine p send t owner p release locks and push changes for scope s v algorithm pipelined locking engine thread loop while not done do if pipeline has ready vertex v then execute t update s scheduler v f v s v on each machine for each machine p send t owner p release locks and push changes to s v in background else wait on the pipeline pipelined locking and prefetching each machine maintains a pipeline of vertices for which locks have been requested but have not been fulfilled vertices that complete lock acquisition and data synchronization leave the pipeline and are executed by worker threads the local scheduler ensures that the pipeline is always filled to capacity an overview of the pipelined locking engine loop is shown in alg to implement the pipelining system regular readers writer locks cannot be used since they would halt the pipeline thread on con tention we therefore implemented a non blocking variation of the readers writer lock that operates through callbacks lock acquisi tion requests provide a pointer to a callback that is called once the request is fulfilled these callbacks are chained into a distributed continuation passing scheme that passes lock requests across ma chines in sequence since lock acquisition follows the total ordering described earlier deadlock free operation is guaranteed to fur ther reduce latency synchronization of locked data is performed immediately as each machine completes its local locks example to acquire a distributed edge consistent scope on a vertex v owned by machine with ghosts on machines and the system first sends a message to machine to acquire a local edge consistent scope on machine write lock on v read lock on neighbors once the locks are acquired the message is passed on to machine to again acquire a local edge consistent scope finally the message is sent to machine before returning to the owning machine to signal completion to evaluate the performance of the distributed pipelining system we constructed a three dimensional mesh of vertices each vertex is connected to immediately adjacent vertices along the axis directions as well as all diagonals producing over million edges the graph is partitioned using metis into atoms we interpret the graph as a binary markov random field and evaluate the runtime of iterations of loopy belief propagation varying the length of the pipeline from to and the number of cluster compute instance from machines processors to machines processors we observe in fig a that the distributed locking system provides strong nearly linear scalability in fig b we evaluate the efficacy of the pipelining system by increasing the pipeline length we find that increasing the length from to leads to a factor of three reduction in runtime number of machines e m maximum pipeline length b pipeline length figure a plots the runtime of the distributed locking en gine on a synthetic loopy belief propagation problem varying the number of machines with pipeline length b plots the runtime of the distributed locking engine on the same synthetic problem on machines cpus varying the pipeline length increasing pipeline length improves perfor mance with diminishing returns algorithm snapshot update on vertex v if v was already snapshotted then quit save d v a runtime e m i t i t n u r n u r machines machines machines save current vertex foreach u n v do loop over neighbors if u was not snapshotted then save data on edge d u v schedule u for a snapshot update mark v as snapshotted fault tolerance we introduce fault tolerance to the distributed graphlab frame work using a distributed checkpoint mechanism in the event of a failure the system is recovered from the last checkpoint we evalu ate two strategies to construct distributed snapshots a synchronous method that suspends all computation while the snapshot is con structed and an asynchronous method that incrementally constructs a snapshot without suspending execution synchronous snapshots are constructed by suspending execution of update functions flushing all communication channels and then saving all modified data since the last snapshot changes are written to journal files in a distributed file system and can be used to restart the execution at any previous snapshot unfortunately synchronous snapshots expose the graphlab en gine to the same inefficiencies of synchronous computation sec that graphlab is trying to address therefore we designed a fully asynchronous alternative based on the chandy lamport snap shot using the graphlab abstraction we designed and implemented a variant of the chandy lamport snapshot specifically tailored to the graphlab data graph and execution model the resulting algo rithm alg is expressed as an update function and guarantees a consistent snapshot under the following conditions edge consistency is used on all update functions schedule completes before the scope is unlocked the snapshot update is prioritized over other update functions which are satisfied with minimal changes to the graphlab engine the proof of correctness follows naturally from the original proof in with the machines and channels replaced by vertices and edges and messages corresponding to scope modifications both the synchronous and asynchronous snapshots are initiated at fixed intervals the choice of interval must balance the cost of constructing the checkpoint with the computation lost since the last x baseline async snapshot sync snapshot time elapsed a snapshot time elapsed x baseline d e t a d p u e c i t r e v async snapshot sync snapshot b snapshot with delay figure a the number of vertices updated vs time elapsed for iterations comparing asynchronous and synchronous snapshots synchronous snapshots completed in seconds have the characteristic flatline while asynchronous snapshots completed in seconds allow computation to proceed b same setup as in a but with a single machine fault lasting seconds as a result of the second delay the asynchronous snapshot incurs only a second penalty while the synchronous snapshot incurs a second penalty checkpoint in the event of a failure young et al derived a first order approximation to the optimal checkpoint interval t interval d e t a d p u e c i t r e v 2t checkpoint t mtbf where t checkpoint is the time it takes to complete the checkpoint and t mtbf is the mean time between failures for the cluster for instance using a cluster of machines a per machine mtbf of year and a checkpoint time of min leads to optimal checkpoint intervals of hrs therefore for the deployments considered in our experiments even taking pessimistic assumptions for t mtbf leads to checkpoint intervals that far exceed the runtime of our experiments and in fact also exceed the hadoop experiment runtimes this brings into question the emphasis on strong fault tolerance in hadoop better performance can be obtained by balancing fault tolerance costs against that of a job restart evaluation we evaluate the performance of the snapshotting algorithms on the same synthetic mesh problem described in the previous section running on machines processors we configure the implementation to issue exactly one snapshot in the middle of the second iteration in fig a we plot the number of up dates completed against time elapsed the effect of the synchronous snapshot and the asynchronous snapshot can be clearly observed synchronous snapshots stops execution while the asynchronous snapshot only slows down execution the benefits of asynchronous snapshots become more apparent in the multi tenancy setting where variation in system performance exacerbate the cost of synchronous operations we simulate this on amazon by halting one of the processes for seconds after snapshot begins in figures fig b we again plot the number of updates completed against time elapsed and we observe that the asynchronous snapshot is minimally affected by the simulated fail ure adding only seconds to the runtime while the synchronous snapshot experiences a full second increase in runtime system design in fig a we provide a high level overview of a graphlab system the user begins by constructing the atom graph representa tion on a distributed file system dfs if hashed partitioning is used the construction process is map reduceable where a map is performed over each vertex and edge and each reducer accumulates an atom file the atom journal format allows future changes to the graph to be appended without reprocessing all the data graphlab ini aliza on phase execu on phase distributed file system raw graph data raw graph data update fn mapreduce distributed cluster distributed exec thread graph builder file system tcp rpc comms file system parsing par oning atom index monitoring atom placement atom index atom atom atom atom atom file file file file collec on atom atom atom atom file file file file index atom atom atom atom construc on file file file file a system overview b locking engine design figure a a high level overview of the graphlab system in the initialization phase the atom file representation of the data graph is constructed in the graphlab execution phase the atom files are assigned to individual execution engines and are then loaded from the dfs b a block diagram of the parts of the distributed graphlab process each block in the diagram makes use of the blocks below it for more details see sec fig b provides a high level overview of the graphlab locking stances each with dual intel xeon quad engine implementation when graphlab is launched on a cluster core nehalem processors and gb of memory and connected by one instance of the graphlab program is executed on each machine a gigabit ethernet network all timings include data loading the graphlab processes are symmetric and directly communicate and are averaged over three or more runs on each node graphlab with each other using a custom asynchronous rpc protocol over spawns eight engine threads matching the number of cores nu tcp ip the first process has the additional responsibility of being a merous other threads are spawned for background communication master monitoring machine in fig a we present an aggregate summary of the parallel at launch the master process computes the placement of the atoms speedup of graphlab when run on to hpc machines on all based on the atom index following which all processes perform three applications in all cases speedup is measured relative to a parallel load of the atoms they were assigned each process is the four node deployment since single node experiments were not responsible for a partition of the distributed graph that is managed always feasible due to memory limitations no snapshots were within a local graph storage and provides distributed locks a cache constructed during the timing experiments since all experiments is used to provide access to remote graph data completed prior to the first snapshot under the optimal snapshot each process also contains a scheduler that manages the vertices interval hours as computed in sec to provide intuition in t that have been assigned to the process at runtime each ma regarding the snapshot cost in fig d we plot for each application chine local scheduler feeds vertices into a prefetch pipeline that the overhead of compiling a snapshot on a machine cluster collects the data and locks required to execute the vertex once all our principal findings are data and locks have been acquired the vertex is executed by a pool of worker threads vertex scheduling is decentralized with each machine managing the schedule for its local vertices and forward ing scheduling requests for remote vertices finally a distributed consensus algorithm is used to determine when all schedulers are empty due to the symmetric design of the distributed runtime there is no centralized bottleneck on equivalent tasks graphlab outperforms hadoop by and performance is comparable to tailored mpi implementations graphlab performance scaling improves with higher computa tion to communication ratios the graphlab abstraction more compactly expresses the netflix ner and coseg algorithms than mapreduce or mpi applications netflix movie recommendation we evaluated graphlab on three state of the art mldm appli the netflix movie recommendation task uses collaborative filter cations collaborative filtering for netflix movie recommendations ing to predict the movie ratings for each user based on the ratings of video co segmentation coseg and named entity recognition similar users we implemented the alternating least squares als ner each experiment was based on large real world problems algorithm a common algorithm in collaborative filtering the and datasets see table we used the chromatic engine for the input to als is a sparse users by movies matrix r containing the netflix and ner applications and the locking engine for the coseg movie ratings of each user the algorithm iteratively computes a application equivalent hadoop and mpi implementations were low rank matrix factorization also evaluated on the netflix and ner applications movies d unfortunately we could not directly compare against pregel since it is not publicly available and current open source implementations do not scale to even the smaller problems we considered while pregel exposes a vertex parallel abstraction it must still provide u u x d movies v access to the adjacent edges within update functions in the case of where u and v are rank d matrices the als algorithm alternates the problems considered here the computation demands that edges between computing the least squares solution for u and v while be bi directed resulting in an increase in graph storage complexity holding the other fixed both the quality of the approximation and for instance the movie harry potter connects to a very large the computational complexity depend on the magnitude of d higher number of users finally many pregel implementations of mldm d produces higher accuracy while increasing computational cost algorithms will require each vertex to transmit its own value to all collaborative filtering and the als algorithm are important tools in adjacent vertices unnecessarily expanding the amount of program mldm an effective solution for als can be extended to a broad state from o v to o e class of other applications experiments were performed on amazon elastic computing while als may not seem like a graph algorithm it can be repre cloud using up to high performance cluster hpc in sented elegantly using the graphlab abstraction the sparse matrix r e r r e u sparse gl engine gl engine gl engine remote graph cache update fn exec thread distributed graph local graph storage async rpc comm over tcp scope prefetch locks data pipeline distributed locks update fn exec thread scheduler e in h c a m ideal coseg e n i h ner e n i h c a m ideal d cycles d cycles o t e v i netflix ner c a m r e d cycles d cycles hadoop mpi graphlab netflix machines machines machines machines a overall scalability c netflix scaling with intensity d netflix comparisons figure a scalability of the three test applications with the largest input size coseg scales excellently due to very sparse graph and high computational intensity netflix with default input size scales moderately while ner is hindered by high network utilization see sec for a detailed discussion b average bandwidth utilization per cluster node netflix and coseg have very low bandwidth requirements while ner appears to saturate when machines c scalability of netflix varying the computation cost of the update function d runtime of netflix with graphlab hadoop and mpi implementations note the logarithmic scale graphlab outperforms hadoop by and is comparable to an mpi implementation see sec and sec for a detailed discussion r defines a bipartite graph connecting each user with the movies they rated the edge data contains the rating for a movie user pair the vertex data for users and movies contains the corresponding row in u and column in v respectively the graphlab update function recomputes the d length vector for each vertex by reading the d length vectors on adjacent vertices and then solving a least squares regression problem to predict the edge values since the graph is bipartite and two colorable and the edge consistency model is sufficient for serializability the chromatic engine is used the netflix task provides us with an opportunity to quantify the distributed chromatic engine overhead since we are able to directly control the computation communication ratio by manipulating d the dimensionality of the approximating matrices in eq in fig c we plot the speedup achieved for varying values of d and the corresponding number of cycles required per update extrapo lating to obtain the theoretically optimal runtime we estimated the overhead of distributed graphlab at machines cpus to be about for d and about for d note that this overhead includes graph loading and communication this provides us with a measurable objective for future optimizations next we compare against a hadoop and an mpi implementation in fig d d for all cases using between to machines the hadoop implementation is part of the mahout project and is widely used since fault tolerance was not needed during our exper iments we reduced the hadoop distributed filesystem hdfs replication factor to one a significant amount of our effort was then spent tuning the hadoop job parameters to improve performance however even so we find that graphlab performs between times faster than hadoop while some of the hadoop inefficiency may be attributed to java job scheduling and various design decisions graphlab also leads to a more efficient representation of the underlying algorithm we can observe that the map function of a hadoop als implementation performs no computation and its only purpose is to emit copies of the vertex data for every edge in the graph unnecessarily multiplying the amount of data that need to be tracked for example a user vertex that connects to movies must emit the data on the user vertex times once for each movie this results in the generation of a large amount of unnecessary network traffic and unnecessary hdfs writes this weakness ex tends beyond the mapreduce abstraction but also affects the graph message passing models such as pregel due to the lack of a scatter operation that would avoid sending same value multiple times to each machine comparatively the graphlab update function is simpler as users do not need to explicitly define the flow of informa tion synchronization of a modified vertex only requires as much o t e iv t t la e p s coseg la e r p p b r p u e m i t n u r u d e e m p d e e p s s 56 b overall network utilization communication as there are ghosts of the vertex in particular only machines that require the vertex data for computation will receive it and each machine receives each modified vertex data at most once even if the vertex has many neighbors our mpi implementation of als is highly optimized and uses synchronous mpi collective operations for communication the computation is broken into super steps that alternate between re computing the latent user and movies low rank matrices between super steps the new user and movie values are scattered using mpi alltoall to the machines that need them in the next super step as a consequence our mpi implementation of als is roughly equiv alent to an optimized pregel version of als with added support for parallel broadcasts surprisingly graphlab was able to outperform the mpi implementation we attribute the performance to the use of background asynchronous communication in graphlab finally we can evaluate the effect of enabling dynamic computa tion in fig a we plot the test error obtained over time using a dynamic update schedule as compared to a static bsp style update schedule this dynamic schedule is easily represented in graphlab while it is difficult to express using pregel messaging semantics we observe that a dynamic schedule converges much faster reaching a low test error in about half amount of work video co segmentation coseg video co segmentation automatically identifies and clusters spatio temporal segments of video fig a that share similar texture and color characteristics the resulting segmentation fig a can be used in scene understanding and other computer vision and robotics applications previous co segmentation methods have focused on processing frames in isolation instead we developed a joint co segmentation algorithm that processes all frames simultaneously and is able to model temporal stability we preprocessed frames of high resolution video by coars ening each frame to a regular grid of rectangular super pixels each super pixel stores the color and texture statistics for all the raw pixels in its domain the coseg algorithm predicts the best label e g sky building grass pavement trees for each super pixel using gaussian mixture model gmm in conjunction with loopy belief propagation lbp the gmm estimates the best label given the color and texture statistics in the super pixel the algorithm operates by connecting neighboring pixels in time and space into a large three dimensional grid and uses lbp to smooth the local estimates we combined the two algorithms to form an expectation maximization algorithm alternating between lbp to compute the label for each super pixel given the gmm and then updating the gmm given the labels from lbp the graphlab update function executes the lbp local iterative update we implement the state of the art adaptive update sched ule described by where updates that are expected to change vertex values significantly are prioritized we therefore make use of the locking engine with an approximate priority scheduler the parameters for the gmm are maintained using the sync operation to the best of our knowledge there are no other abstractions that provide the dynamic asynchronous scheduling as well as the sync reduction capabilities required by this application in fig a we demonstrate that the locking engine can achieve scalability and performance on the large million vertex graph used by this application resulting in a speedup with more machines we also observe from fig a that the locking engine provides nearly optimal weak scaling the runtime does not increase significantly as the size of the graph increases proportionately with the number of machines we can attribute this to the properties of the graph partition where the number of edges crossing machines increases linearly with the number of machines resulting in low communication volume while sec contains a limited evaluation of the pipelining system on a synthetic graph here we further investigate the behavior of the distributed lock implementation when run on a complete problem that makes use of all key aspects of graphlab both sync and dynamic prioritized scheduling the evaluation is performed on a small frame vertices problem using a node cluster and two different partitioning an optimal partition was constructed by evenly distributing frame blocks to each machine a worst case partition was constructed by striping frames across machines and consequently stressing the distributed lock implementation by forcing each scope acquisition is to grab at least one remote lock we also vary the maximum length of the pipeline results are plotted in fig b we demonstrate that increasing the length of the pipeline increases performance significantly and is able to compensate for poor partitioning rapidly bringing down the runtime of the problem just as in sec we observe diminishing returns with increasing pipeline length while pipelining violates the priority order rapid convergence is still achieved we conclude that for the video co segmentation task distributed graphlab provides excellent performance while being the only dis tributed graph abstraction that allows the use of dynamic prioritized scheduling in addition the pipelining system is an effective way to hide latency and to some extent a poor partitioning named entity recognition ner named entity recognition ner is the task of determining the type e g person place or thing of a noun phrase e g obama chicago or car from its context e g president lives near or bought a ner is used in many natural language processing applications as well as information retrieval in this application we obtained a large crawl of the web from the nell project and we counted the number of occurrences of each noun phrase in each context starting with a small seed set of pre labeled noun phrases the coem algorithm labels the remaining noun phrases and contexts see table b by alternating between estimating the best assignment to each noun phrase given the types of its contexts and estimating the type of each context given the types of its noun phrases the data graph for the ner problem is bipartite with one set of vertices corresponding to noun phrases and other corresponding to each contexts there is an edge between a noun phrase and a context if the noun phrase occurs in the context the vertices store the estimated distribution over types and the edges store the number of times the noun phrase appears in a context since the graph is food religion onion catholic garlic fremasonry noodles marxism blueberries catholic chr b ner types figure a coseg a frame from the original video sequence and the result of running the co segmentation algorithm b ner top words for several types two colorable and relatively dense the chromatic engine was used with random partitioning the lightweight floating point arithmetic in the ner computation in conjunction with the relatively dense graph structure and random partitioning is essentially the worst case for the current distributed graphlab design and thus allow us to evaluate the overhead of the distributed graphlab runtime from fig a we see that ner achieved only a modest im provement using more machines we attribute the poor scaling performance of ner to the large vertex data size bytes dense connectivity and poor partitioning random cut that resulted in substantial communication overhead per iteration fig b shows for each application the average number of bytes per second trans mitted by each machine with varying size deployments beyond machines ner saturates with each machine sending at a rate of over per second we evaluated our distributed graphlab implementation against a hadoop and an mpi implementation in fig c in addition to the optimizations listed in sec our hadoop implementation required the use of binary marshaling methods to obtain reasonable performance decreasing runtime by from baseline we demonstrate that graphlab implementation of ner was able to obtains a speedup over hadoop the reason for the performance gap is the same as that for the netflix evaluation since each vertex emits a copy of itself for each edge in the extremely large coem graph this corresponds to over gb of hdfs writes occurring between the map and reduce stage on the other hand our mpi implementation was able to outper form distributed graphlab by a healthy margin the coem task requires extremely little computation in comparison to the amount of data it touches we were able to evaluate that the ner update function requires fewer cycles per byte of data accessed as compared to the netflix problem at d the hardest netflix case evaluated the extremely poor computation to communication ratio stresses our communication implementation that is outperformed by mpi efficient communication layer furthermore fig b pro vides further evidence that we fail to fully saturate the network that offers further optimizations to eliminate inefficiencies in graphlab communication layer should bring us up to parity with the mpi implementation we conclude that while distributed graphlab is suitable for the ner task providing an effective abstraction further optimizations are needed to improve scalability and to bring performance closer to that of a dedicated mpi implementation cost evaluation to illustrate the monetary cost of using the alternative abstrac tions we plot the price runtime curve for the netflix application in fig b in log log scale all costs are computed using fine grained billing rather than the hourly billing used by amazon the price runtime curve demonstrates diminishing returns the cost of attaining reduced runtimes increases faster than linearly as a comparison we provide the price runtime curve for hadoop on the same application for the netflix application graphlab is about two orders of magnitude more cost effective than hadoop a coseg video frame exp verts edges vertex data netflix 8d edge data update o complexity deg shape partition engine bipartite random chromatic coseg o deg grid frames locking ner o deg bipartite random chromatic table experiment input sizes the vertex and edge data are measured in bytes and the d in netflix is the size of the latent dimension optimal partition hadoop worst case d a partition graphlab mpi nodes data graph size vertices pipeline length nodes b pipelined locking c ner comparisons d snapshot overhead figure a runtime of the coseg experiment as data set size is scaled proportionately with the number of machines ideally runtime is constant graphlab experiences an increase in runtime scaling from to machines b the performance effects of varying the length of the pipeline increasing the pipeline length has a small effect on performance when partitioning is good when partitioning is poor increasing the pipeline length improves performance to be comparable to that of optimal partitioning runtime for worst case partitioning at pipeline length is omitted due to excessive runtimes c runtime of the ner experiment with distributed graphlab hadoop and mpi implementations note the logarithmic scale graphlab outperforms hadoop by about when the number of machines is small and about when the number of machines is large the performance of distributed graphlab is comparable to the mpi implementation d for each application the overhead of performing a complete snapshot of the graph every v updates where v is the number of vertices in the graph when running on a machine cluster opinionated social media such as product reviews are now widely used by individuals and organizations for their decision making however due to the reason of profit or fame people try to game the system by opinion spamming e g writing fake reviews to promote or demote some target products for reviews to reflect genuine user experiences and opinions such spam reviews should be detected prior works on opinion spam focused on detecting fake reviews and individual fake reviewers however a fake reviewer group a group of reviewers who work collaboratively to write fake reviews is even more damaging as they can take total control of the sentiment on the target product due to its size this paper studies spam detection in the collaborative setting i e to discover fake reviewer groups the proposed method first uses a frequent itemset mining method to find a set of candidate groups it then uses several behavioral models derived from the collusion phenomenon among fake reviewers and relation models based on the relationships among groups individual reviewers and products they reviewed to detect fake reviewer groups additionally we also built a labeled dataset of fake reviewer groups although labeling individual fake reviews and reviewers is very hard to our surprise labeling fake reviewer groups is much easier we also note that the proposed technique departs from the traditional supervised learning approach for spam detection because of the inherent nature of our problem which makes the classic supervised learning approach less effective experimental results show that the proposed method outperforms multiple strong baselines including the state of the art supervised classification regression and learning to rank algorithms categories and subject descriptors h information systems human factors j computer applications social and behavioral sciences keywords opinion spam group opinion spam fake review detection introduction nowadays if one wants to buy a product most probably one will first read reviews of the product if he she finds that most reviews are positive he she is very likely to buy it however if most reviews are negative he she will almost certainly choose another product positive opinions can result in significant financial gains and fames for organizations and individuals this unfortunately gives strong incentives for opinion spamming which refers to human activities e g writing fake reviews that try to deliberately mislead readers by giving unfair reviews to some entities e g products in order to promote them or to damage their reputations as more and more individuals and organizations are using reviews for their decision making detecting such fake reviews becomes a pressing issue the problem has been widely reported in the news there are prior works on detecting fake reviews and individual fake reviewers or spammers however limited research has been done to detect fake reviewer or spammer groups which we also call spammer groups group spamming refers to a group of reviewers writing fake reviews together to promote or to demote some target products a spammer group can be highly damaging as it can take total control of the sentiment on a product because a group has many people to write fake reviews our experiments show that it is hard to detect spammer groups using review content features or even indicators for detecting abnormal behaviors of individual spammers because a group has more manpower to post reviews and thus each member may no longer appear to behave abnormally note that by a group of reviewers we mean a set of reviewer ids the actual reviewers behind the ids could be a single person with multiple ids sockpuppet multiple persons or a combination of both we do not distinguish them in this work before proceeding further let us see a spammer group found by our algorithm figures and show the reviews of a group of three reviewers the following suspicious patterns can be noted about this group i the group members all reviewed the same three products giving all star ratings ii they posted reviews within a small time window of days two of them posted in the same day iii each of them only reviewed the three products when our amazon review data was crawled iv they were among the early reviewers for the products to make a big impact all these patterns occurring together strongly suggest suspicious activities notice also none of the reviews themselves are similar to each other i e not duplicates or appear deceptive if we only look at the three reviewers individually they all appear genuine in fact out of reviews received helpfulness votes by amazon users indicating that the reviews are useful clearly these three reviewers have taken total control of the sentiment on the set of reviewed products in fact there is a fourth reviewer in the group due to space limitations we omit it here if a group of reviewers work together only once to promote or to demote a product it is hard to detect them based on their collective behavior they may be detected using the content of their reviews e g copying each other then the methods in are applicable however over the years opinion spamming has become a business people get paid to write fake reviews such people cannot just write a single review copyright is held by the international world wide web conference committee distribution of these papers is limited to classroom use and personal use by others www april lyon france acm http www nytimes com technology for a star a retailer gets star reviews html http www nytimes com technology finding fake reviews online html http www amazon com gp pdp profile http www amazon com gp pdp profile http www amazon com gp cdp member reviews www session fraud and bias in user ratings april lyon france as they would not make enough money that way instead they write many reviews about many products such collective behaviors of a group working together on a number of products can give them away this paper focuses on detecting such groups since reviewers in the group write reviews on multiple products the data mining technique frequent itemset mining fim can be used to find them however so discovered groups are only group spam candidates because many groups may be coincidental as some reviewers happen to review the same set of products due to similar tastes and popularity of the products e g many people review all apple products ipod iphone and ipad thus our focus is to identify true spammer groups from the candidate set one key difficulty for opinion spam detection is that it is very hard to manually label fake reviews or reviewers for model building because it is almost impossible to recognize spam by just reading each individual review in this work multiple experts were employed to create a labeled group opinion spammer dataset this research makes the following main contributions it produces a labeled group spam dataset to the best of our knowledge this is the first such dataset what was surprising and also encouraging to us was that unlike judging individual fake reviews or reviewers judging fake reviewer groups were considerably easier due to the group context and their collective behaviors we will discuss this in sec it proposes a novel relation based approach to detecting spammer groups with the labeled dataset the traditional approach of supervised learning can be applied however we show that this approach can be inferior due to the inherent nature of our particular problem i traditional learning assumes that individual instances are independent of one another however in our case groups are clearly not independent of one another as different groups may share members one consequence of this is that if a group i is found to be a spammer group then the other groups that share members with group i are likely to be spammer groups too the reverse may also hold ii it is hard for features used to represent each group in learning to consider each individual member behavior on each individual product i e a group can conceal a lot of internal details this results in severe information loss and consequently low accuracy we discuss these and other issues in greater detail in sec to exploit the relationships of groups individual members and products they reviewed a novel relation based approach is proposed which we call gsrank group spam rank to rank candidate groups based on their likelihoods for being spam a comprehensive evaluation has been conducted to evaluate gsrank experimental results show that it outperforms many strong baselines including the state of the art learning to rank supervised classification and regression algorithms of people found the following review helpful practically free music december this review is from audio xtract cd rom i can t believe for after rebate i got a program that gets me free unlimited music i was hoping it did half what was wow internet music december this review is from audio xtract cd rom i looked forever for a way to record internet music my way took a long time and many steps frustrtaing then i found audio xtract with more than songs downloaded in of people found the following review helpful yes it really works december this review is from audio xtract pro cd rom see my review for audio xtract this pro is even better this is the solution i ve been looking for after buying itunes of people found the following review helpful best music just got december this review is from audio xtract pro cd rom the other day i upgraded to this top notch product everyone who loves music needs to get it from internet of people found the following review helpful my kids love it december this review is from pond aquarium deluxe edition this was a bargain at better than the other ones that have no above water scenes my kids get a kick out of the of people found the following review helpful this is even better than december this review is from audio xtract pro cd rom let me tell you this has to be one of the coolest products ever on the market record internet radio stations at once of people found the following review helpful of people found the following review helpful for the price you december cool looks great december this review is from pond aquarium deluxe edition this review is from pond aquarium deluxe edition this is one of the coolest screensavers i have ever seen the fish we have this set up on the pc at home and it looks great move realistically the environments look real and the the fish and the scenes are really neat friends and family figure big john profile figure cletus profile figure jake profile of people found the following review helpful like a tape recorder december this review is from audio xtract cd rom this software really rocks i can set the program to record music all day long and just let it go i come home and my related work the problem of detecting review or opinion spam was introduced in which used supervised learning to detect individual fake reviews duplicate and near duplicate reviews which are almost certainly fake reviews were used as positive training data while found different types of behavior abnormalities of reviewers proposed a method based on unexpected class association rules and employed standard word and part of speech pos n gram features for supervised learning also used supervised learning with additional features used a graph based method to find fake store reviewers a distortion based method was proposed in none of them deal with group spam in we proposed an initial group spam detection method but it is much less effective than the proposed method in this paper in a wide field the most investigated spam activities have been in the domains of web and email web spam has two main types content spam and link spam link spam is spam on hyperlinks which does not exist in reviews as there is usually no link in them content spam adds irrelevant words in pages to fool search engines reviewers do not add irrelevant words in their reviews email spam usually refers to unsolicited commercial ads although exists ads in reviews are rare recent studies on spam also extended to blogs online tagging and social networks however their dynamics are different from those of product reviews they also do not study group spam other literature related to group activities include mining groups in wlan mobile users using network logs and community discovery based on interests sybil attacks in security create pseudo identities to subvert a reputation system in the online context pseudo identities in sybil attacks are known as sockpuppets indeed sockpuppets are possible in reviews and our method can deal with them lastly studied the usefulness or quality of reviews however opinion spam is a different concept as a low quality review may not be a spam or fake review building a reference dataset as mentioned earlier there was no labeled dataset for group opinion spam before this project to evaluate our method we built a labeled dataset using expert human judges opinion spam and labeling viability argues that classifying the concept spam is difficult research on web email blogs and even social spam all rely on manually labeled data for detection due to this inherent nature of the problems the closest that one can get to gold standards is by creating a manually labeled dataset using human experts we too built a group opinion spam dataset using human experts amazon dataset in this research we used product reviews from amazon which have also been used in the original www session fraud and bias in user ratings april lyon france crawl was done in updates were made in early for our study we only used reviews of manufactured products which are comprised of reviewers reviews and 392 products each review consisted of a title content star rating posting date and number of helpful feedbacks mining candidate spammer groups we use frequent itemset mining fim here in our context a set of items i is the set of all reviewer ids in our database each transaction t i labeling results we now report the labeling results and analyze the agreements among the judges spamicity we calculated the spamicity degree of spam of each group by assigning point for each spam judgment point for each borderline judgment and point for each non spam of reviewer ids who have reviewed a particular t i product i is the set thus each product generates a transaction of reviewer ids by mining frequent itemsets we find groups of reviewers who have reviewed multiple products together we found candidate groups with minimum support count and at least items reviewer ids per itemset group i e each group must have worked together on at least products itemsets groups with support lower than this are very likely to be due to random chance rather than true correlation and very low support also causes combinatorial explosion because the number of frequent itemsets grows exponentially for fim fim working on reviewer ids can also find sockpuppeted ids forming groups whenever the ids are used times to post reviews opinion spam signals we reviewed prior research on opinion spam and guidelines on consumer sites such as consumerist com lifehacker com and consumersearch and collected from these sources a list of spamming indicators or signals e g i having zero caveats ii full of empty adjectives iii purely glowing praises with no downsides iv being left within a short period of time of each other etc these signals were given to our judgment a group received and took the average of all labelers we call this average the spamicity score of the group based on the spamicities the groups can be ranked in our evaluation we will evaluate the proposed method to see whether it can rank similarly in practice one can also use a spamicity threshold to divide the candidate group set into two classes spam and non spam groups then supervised classification is applicable we will discuss these in detail in the experiment section agreement study previous studies have showed that labeling individual fake reviews and reviewers is hard to study the feasibility of labeling groups and also the judging quality we used fleiss multi rater kappa to measure the judges agreements we obtained κ which indicates close to perfect agreement based on the in this was very encouraging and also surprising considering that judging opinion spam in general is hard it tells us that labeling groups seems to be much easier than labeling individual fake reviews or reviewers we believe the reason is that unlike a single reviewer or review a group gives a good context for judging and comparison and similar behaviors among members often reveal strong signals this was confirmed by our judges who had domain expertise in reviews judges we believe that these signals and the additional information described below enhance their judging rather than bias them because judging spam reviews and reviewers is very challenging it is hard for anyone to know a large number of possible signals without substantial prior experiences these spamming behavior indicators for modeling or learning a set of effective spam indicators or features is needed this section proposes two sets of such indicators or behaviors which may indicate spamming activities signals on the web and research papers have been compiled by experts with extensive experiences and domain knowledge we also reminded our judges that these signals should be used at their group spam behavior indicators here we discuss group behaviors that may indicate spam discretion and encouraged them to use their own signals group time window gtw members in a spam group are to reduce the judges workload further for each group we also provided additional pieces of information as they are required by some of the above signals reviews with posting dates of each likely to have worked together in posting reviews for the target products during a short time interval we model the degree of active involvement of a group as its group time window gtw individual group member list of products reviewed by each member reviews of products given by non group members and ggtw max pp g gtw p pg whether group reviews were tagged with avp amazon verified purchase amazon tags each review with avp if the reviewer actually bought the product judges were also given access to our database for querying based on their needs labeling we employed expert judges employees of rediff shopping and ebay in for labeling our candidate groups the judges had domain expertise in feedbacks and reviews of products due to their nature of work in online shopping since there were too many patterns or candidate groups our judges could only manage to label of them as being spam non spam or borderline the judges were made to work in isolation to prevent any bias the labeling took around weeks we did not use amazon mechanical turk mturk for this labeling task because mturk is normally used to perform simple tasks which require human judgments however our task is highly challenging time consuming and also required the access to our database also we needed judges with good knowledge of the review domain thus we believe that mturk was not suitable http consumerist com how you spot fake online reviews html http lifehacker com hone your eye for fake online reviews http www consumersearch com blog how to spot fake user reviews pggtw p pgfpgl τ if pgfpgl τ otherwise where l g p and f g p are the latest and earliest dates of reviews posted for product p p g by reviewers of group g respectively p g is the set of all products reviewed by group g thus on a single gtw p g product p gives p the time window information this definition says that of group g a group g of reviewers posting reviews on a product p within a short burst of time is more prone to be spamming attaining a value close to groups working over a longer time interval thanτ get a value of as they are unlikely to have worked together τ is a parameter which we will estimate later the group time window gtw g considers all products reviewed by the group taking max over p p g so as to capture the worst behavior of the group for subsequent behaviors max is taken for the same reason group deviation gd a highly damaging group spam occurs when the ratings of the group members deviate a great deal from no agreement κ slight agreement κ fair agreement κ moderate agreement κ substantial agreement κ and almost perfect agreement for κ www session fraud and bias in user ratings april lyon france those of other genuine reviewers so as to change the sentiment one extreme worst case the group members are the only on a product the larger the deviation the worse the group is reviewers of the product completely controlling the sentiment on this behavior is modeled by group deviation gd on a star the product on the other hand if the total number of reviewers of rating scale with being the maximum possible deviation the product is very large then the impact of the group is small ggd max pgd ggsr avg pp g pp g pggsr p pgd rr gp gp where r gp and r gp are the average ratings for product p given by pggsr p m g p where gsr p members of group g and by other reviewers not in g respectively d g p is the deviation of the group on a single product p if there are no other reviewers who have reviewed the product p gp the set of all reviewers of product p for product p group size gs group collusion is also exhibited by its size for large groups the probability of members happening to be together by chance is small furthermore the larger the group the more damaging it is gs is easy to model we normalize it to max g i g p is the ratio of group size to m p r group content similarity gcs group connivance is also exhibited by content similarity duplicate or near duplicate reviews when spammers copy reviews among themselves so the victimized products have many reviews with similar content group content similarity gcs models this behavior ggcs max pp g pgcs g is the largest group size of all discovered groups ggs g max g i pgcs avg cosine pmcpmc i j jigmm where c m i group support count gsup support count of a group is the total number of products towards which the group has worked g ji p is the content of the review written by group member pairwise together groups with high support counts are more likely to be spam groups as the probability of a group of random people m similarity i g for of product review contents p cs g g among p captures group members the average for happen to have reviewed many products together is small gsup is modeled as follows we normalize it to with max p g i a product p by computing the cosine similarity group member content similarity gmcs another flavor of content similarity is exhibited when the members of a group g do not know one another and are contacted by a contracting agency since writing a new review every time is taxing a group member may copy or modify his her own previous reviews for similar products if multiple members of the group do this the group is more likely to be spamming this behavior can be expressed by group member content similarity gmcs as follows being the largest support count of all discovered groups ggsup max p i these eight group behaviors can be seen as group spamming features for learning from here on we refer the group behaviors as f g p g f when used in the context of features it is important to note that by no means do we say that whenever a group attains a feature f or a threshold value it is a spam mgcs m gmcs g gm g mgcs m avg jippp gji cosine pmcpmc group it is possible that a group of reviewers due to similar tastes coincidently review some similar products and form a coincidental group in some short time frame or may generate some deviation of ratings from the rest or may even have i j modified some of the contents of their previous reviews to update the group attains a value indicating spam on gmcs when all its members entirely copied their own reviews across different products in p g their reviews producing similar reviews the features just indicate the extent those group behaviors were exhibited the final cs m g m models the average pairwise similarity of member m g over all products in p g content group early time frame getf reports spammers usually review early to make the biggest impact similarly when group members are among the very first people to review a prediction of groups is done based on the learned models as we will see in spam groups sec all and feature features values f attained f are by strongly spam correlated groups with exceed those attained by other non spam groups by a large margin individual spam behavior indicators product they can totally hijack the sentiments on the products although group behaviors are important they hide a lot of details the group early time frame getf models this behavior about its members clearly individual members behaviors also getf g max pggtf pp g give signals for group spamming we now present the behaviors for individual members used in this work if papgl β pggtf papgl β otherwise can individual model ird rating as deviation ird like group deviation we where gtf g p captures the time frame as how early a group g reviews a product p l g p and a p are the latest date of review posted for product p p g pmird rr mp mp where r mp and r mp are the rating for product p given by reviewer m by group g and the date when p was made available for reviewing respectively β is a threshold say months later estimated which means that after β months gtf attains a value of as reviews posted then are not considered to be early any more since our experimental dataset does not have the exact date when each product was launched we use the date of the first review of the product as the value for a p group size ratio gsr the ratio of group size to the total number of reviewers for a product can also indicate spamming at and the average rating for p given by other reviewers respectively individual content similarity ics individual spammers may review a product multiple times posting duplicate or near duplicate reviews to increase the product popularity similar to gmcs we model ics of a reviewer m across all its reviews towards a product p as follows pmics avg cosine pmc the average is taken over all reviews on p posted by m www session fraud and bias in user ratings april lyon france gtw gcs gmcs statistically validating their correlation with group spam for this study we used the classification setting for spam detection a spamicity threshold of was employed to divide all candidate groups into two categories i e those with spamicity greater than as spam groups and others as non spam groups using this scheme we get non spam groups and spam groups in sec we will see that these features work well in general rather gd gsr gsup than just for this particular threshold note that the individual spam indicators in sec are not analyzed as there is no suitable labeled data for that however these indicators are similar to their group counterparts and are thus indirectly validated through the group indicators they also helped gsrank well sec statistical validation for a given feature f its effectiveness eff is defined with fpfeff spam fp non spam getf gs where f is the event that the corresponding behavior is exhibited to some extent let the null hypothesis be both spam and normal groups are equally likely to exhibit f and the alternate hypothesis spam groups are more likely to exhibit f than non figure behavioral distribution cumulative of spam solid and spam groups and are correlated with f thus demonstrating that f is observed among spam groups and is correlated is reduced to show that eff f we estimate the probabilities as follows fp spam gfg gg non spam dashed groups vs feature value g spam spam fp non spam gfg gg we use fisher exact test to test the hypothesis the test rejects the null hypothesis with p for each of the modeled behaviors this shows that spam groups are indeed characterized by the modeled behaviors furthermore since the modeled behaviors are all anomalous and fisher exact test verifies strong correlation of those behaviors with groups labeled as spam it also indirectly gives us a strong confidence that the majority of the class labels in the reference dataset are trustworthy behavioral distribution we now analyze the underlying distribution of spam and non spam groups across each behavioral feature dimension figure shows the cumulative behavioral distribution cbd against each value x attained by a feature f x as f f we plot the cumulative percentage of spam non spam groups having values of f x we note the following insights from the plots position cbd curves of non spam groups lean more towards the left boundary of the graph than those for spam groups across all features this implies that for a given cumulative percentage cp the corresponding feature value x n g non spam non spam individual early time frame ietf like getf we define ietf of a group member m as pmietf papml β if papml β otherwise where l m p denotes the latest date of review posted for a product p by member m individual member coupling in a group imc this behavior measures how closely a member works with the other members of the group if a member m almost posts at the same date as other group members then m is said to be tightly coupled with the group however if m posts at a date that is far away from the posting dates of the other members then m is not tightly coupled with the group we find the difference between the posting date of member m for product p and the average posting date of other members of the group for p to compute time we use the time when the first review was posted by the group for product p as the baseline individual member coupling imc is thus modeled as avgmgimc g pgfpmt pp pgfpgl mgavg for non spam groups is less than if cp x for i e spam groups for example in cbd of the gs feature pgfpmt i mgavg mgm i g then of members the non spam while groups of are the bounded spam groups by x n are bounded by x where l g p and f g p are the latest and earliest dates of reviews posted for product p p g i e members as another example we take cbd of gsup with cp we see that of products the non spam while by group g respectively and t m p is the actual posting date of reviewer m on product p groups of are spam bounded groups by are x n bounded i e by x note that ip addresses of reviewers may also be of use for group spam detection however ip information is privately held by proprietary firms and not publicly available we believe if ip addresses are also available additional features may be added which will make our proposed approach even more accurate empirical analysis to ensure that the proposed behavioral features are good indicators of group spamming this section analyzes them by i e products this shows that spamming groups usually work with more members and review more products as non spam groups are mostly coincidental we find that their feature values remain low for most groups indicating benign behaviors also we emphasize the term bounded by in the above description by no means do we claim that every spam dataset in sec of all candidate and max support we groups with multiply feature values of gs yielded max g and gsup by these i numbers to get the actual counts see equations and for details www session fraud and bias in user ratings april lyon france group in our database reviewed at least products and is example using the group spam member spam relation we infer comprised of at least members lastly since the leftward positioning of the cbd curve for x non spam n x cp groups due to the spamicity of a group based on the spamicity of its individual members and vice versa our ranking method called gsrank spam groups obtain higher feature values than non spam groups group spam rank is then presented to tie all these inferences for each modeled behavior f which solves an eigenvalue problem by aligning the group vector steep initial jumps these indicate that very few groups obtain significant feature values before jump abscissa for example we find that there are very few spam groups with gsup and to the dominant eigenvector before going to the details we first define some notations used in the following sub sections let p p i for gcs we find a majority of non spam groups in our database have minuscule content among their reviews gaps cbd curves of non spam groups are higher than those of spam groups and the gap separation margin refers to the relative discriminative potency gcs has the maximum gap and next in order are gsup and getf this result is not surprising as a group of people having a lot of content similarity in their reviews is highly suspicious of being spamming and hence gcs has good discriminative strength lastly we note again that the above statistics are inferred from the labeled groups by domain experts based on the data of crawled in by no means do we claim that the results can be generalized across any group of random people who happen to review similar products together owing to similar interests modeling relations with the group behavioral features separating spam and non spam groups by a large margin and the labeled data from sec the classic approach to detect spammer groups is to employ a supervised classification regression or learning to rank algorithm to classify or rank candidate groups all these existing methods are based on a set of features to represent each instance group in our case however as we indicated in the introduction section this feature based approach has some shortcomings for our task they assume that training and testing instances are drawn independently and identically iid from some distribution however in our case different groups instances can share members and may review some common products thus our data does not follow the iid assumption because many instances are related i e apart from group features the spamicity of a group is also affected by the other groups sharing its members the spamicity of the shared members the extent to which the reviewed products are spammed etc group features f g groups and members and m k graded over g j let and g m j m k be the set of all products be the spamicity of g j to signify which high p i is spammed spamicity and respectively also graded m over k and let p values i for groups and members and be the extent close to greater extent to v g which products are spammed additionally be let v p p i p group g and j member g and score v m vectors m k m the corresponding product group spam products model this model captures the relation among groups and products they target the extent a product p is spammed by various groups is related to i spam contribution to p by each group reviewing p and ii spamicity of each such group also spam contribution by a group with high spamicity counts more similarly the spamicity of a group is associated with i its own spam contribution to various products and ii the extent those products were spammed to express this relation we first compute the spam contribution have gtw p to time window a product of group p i by a group g j from sec we g activity over a product p d g deviation of ratings for p gtf early time frame of g behaviors spam on p infliction and are gsr symmetric p towards ratio of p group in cs the g g sense size content for that p higher similarity we values note of reviews that these indicate that g behavior on p is suspicious and also indicate that spam contribution to p by g is high thus the spam contribution by g j to p i can be expressed by the following function gpw ji pgdpggtw ijp ij pgcspggtf ij ijg pggsr ijp w pg w w p i g j p x g spam p i g j when g j did not inflicted across various review spamming p i the sum captures the dimensions and is normalized functions too by summation so that w and normalization for subsequent are used for contribution the same reason w pg denotes the corresponding contribution matrix group behaviors this f clearly only summarize e g by max avg leads to loss of information using computes the extent p i because spamicity contributions from members are not considered at each individual member level but are summarized max avg to represent the whole group behavior due to different group sizes and complex relationships it is not easy to design and include each individual member related features explicitly without some kind of summary it is also difficult to design features which can consider the extent to which each product is spammed by groups although our focus is on detecting spammer groups the underlying products being reviewed are clearly related below we propose a more effective model which can consider the inter relationship among products groups and group members in computing group spamicity specifically we model three binary relations group spam products member spam products and group spam member spam the overall idea is as follows we first model the three binary relations to account for how each entity affects the other we then draw inference of one entity from the other entity based on the corresponding binary relation for computed using lingpipe java api available at http alias i com lingpipe is spammed by various groups it sums the spam contribution weights it by the spamicity of by each group that group g j similarly w p i g j and updates the group spamicity by summing its spam contribution on all products weighted by the extent those products were spammed the relations can also be expressed as matrix equations ps i g vwvgsgpw i j j p gpg j gs j p vwvpsgpw i j i g t pg p i since g j spamicity was spammed and and w of degree g j p i extent to which p i employ a of summation spam to inflicted compute by g g j towards j p i further as spam contribution by a group with higher spamicity is more damaging the degree of spam contribution by a group is weighted by its spamicity in similarly for spam contribution w and p i spamicity of the is group weighted for by subsequent p i to account for the effective models too weighted summation is used for similar reasons the matrix equation also shows how the product vector can be inferred from the group vector using matrix w pg and vice versa using www session fraud and bias in user ratings april lyon france member spam product model algorithm spam by a group on a product is basically spam by individuals in gsrank input weight matrices w pg the group a group feature can only summarize spam of members in the group over the set of products they reviewed here we consider spam contributions of all group members exclusively like w output ranked list of candidate w mp spam and groups w gm initialize iterate v g g t by a member we employ m k towards w product to compute the spam contribution p i we model w as follows i v p ii v g w pg v g t v m w mp t v p pmw ik pmird ik pmics ik ietf pm ik iii v p w w mp gm t v m v m v v m g t w gm w pg v t g v p w mp iv until v v g g t t v v g t g t v g t w w m k p i m x p individual m k p i member if m k did not review spam contribution p i similar over the to spam dimensions w captures δ output the ranked list of groups in descending order of v g ird individual rating deviation of m towards p ics individual content similarity of reviews on p by m and ietf individual in line we first initialize all groups with spamicity the spamicity scale next we infer v p of over early time frame of spam infliction by m towards p similar to we compute the spamicity of m k from the current value of this completes v g and the then initial infer bootstrapping v m from the of so vectors updated v g v p line i contributions and like by summing its spam for the current iteration line ii then draws inferences v p based and v on m the group spam member spam model it first infers v g by members towards various we update we sum the individual p i to products reflect the w extent weighted it by was p spammed i contribution of each member because v m from v m w contains the recent update from line i and then weighted by its spamicity ms k p vwvpspmw ik i m pmp inference infers v m procedure from so updated across v the g this ordering is used to guide the iterations line iii then updates v p based i ps i m vwvmspmw ik k p t mp on the member spam product model first and defers the spam product inference of model v g until from the so last updated update v so p based on the group that v g k m gets the most updated value for the next iteration finally line iv performs group spam member spam model normalization to maintain an invariant state discussed later thus as the iterations progress the fact that each entity affects the clearly the spamicity of a group is related to the spamicity of its members and vice versa if a group consists of members with high other is taken into account as the score vectors v g spamicities then the group spam infliction is likely to be high similarly a member involved in spam groups of high spamicity affects its own spamicity we first compute the contribution of a member m g towards a group g from sec we see that the contribution is captured by imc degree of m coupling in g gs size of g with which m worked and gsup number of products towards which m worked with g we model it as follows are updated via the inferences drawn from each relation model the iterations progress until v g v m and v p contains the spamicity scores of all groups line outputs since the final v g ordering of spam groups according to the spamicity scores of the stable vector v g converges to the stable v g we now show the convergence of gsrank lemma gsrank seeks eigenvector and is an instance to of align an eigenvalue v g towards the dominant problem j k j k j j proof from line of gsrank we have mgw mgimc ggs gsup g v g t w gm v g w gm w mp w pg v g w g j m k g x m t w pg t w mp t w gm tv g w large g j groups m k the when individual m k g j contribution as gs is normalized over of a member diminishes for hence we use gs g j substituting in and letting z w gm w mp w pg we get to compute w v g t zt z v g t clearly this is an instance of power iteration for the eigenvalue using computes the spamicity of a group by summing up the spamicities of all its members m k problem of computing each weighted by his z corresponding to the the dominant group vector eigenvalue v g as the eigenvector of zt contribution to the members updates group the spamicity w g j m of k a since member groups by summing can share up the spamicities of all groups it worked with each weighted by its from and we can see how the two inferences for each entity are linked for example spamicity of groups v g own contribution to that group based m on and collective the accounted spamicity spam for behavior of by its the members on product products of v m matrices v line p line ii iii w gm and are based both on the linked gs j vwvmsmgw j k k g mgm k similar connections other entities thus all exist model for inferences v m and v p when are and inferred w pg from t in ms k g vwvgsmgw j k j m t gm j encoded in the final iterative inference of v g combined and gsrank ranking group spam using the relation models each entity is inferred twice once from each other entity as the two inferences for each entity are conditioned on other two entities they are thus complementary for example v g g in theorem gsrank converges proof as gsrank seeks eigenvector to show convergence to align it is v sufficient g towards the dominant to show that the certain stable vector number v g of iterations is aligned let to a the dominant zt z from eigenvector and line after iv a both of these is inferred inferences once complement from v p each and other then from because v m of gsrank we get group spamicity is related to both its collective spam activities on products and also the spamicities of its members this complementary connection is further explicitly shown in lemma since the relations are circularly defined to effectively rank the groups gsrank uses the iterative algorithm below t t g g av t g v av which when applied recursively gives t v g t t va g av j j g www session fraud and bias in user ratings april lyon france we diagonalizable note that a is v a g square can be matrix expressed of order as g assuming a to be о a convex combination о о of the g eigenvectors of a where g j θ arg max log gp spam θ θ j estimated j g v g α ii v vector d to compute where each x i p g j takes the values spam we treat g j attained by the x x as a i also v using i with let λ λ i v and denote the being eigenvalue corresponding to the eigenvector as each feature models a different behavior we can features to be independent and express p g k ith assume feature the f i the spam we dominant obtain eigenvalue vector pair of a then πp spam to compute p spam i e p spam for we discretize the range of values obtained by f i into v g t t av λ α t λ t α λ i t i a set of k intervals j since λ g i t i ii v av v g i kf i such that kf i p x i j λ α ii v p kf i spam then reduces to j j g g is spam whenever simply the fraction is dominant λ i λ i thus for large t interval kf i of x i spam lies in the groups interval whose kf i value and p kf of i f i spam lies in we used the popular discretization algorithm in to g t divide the value range of each feature into intervals to bootstrap the hill climbing search we used initial seeds τ i λ i λ α ii v and the stable vector v g v normalization l consecutive before each iteration normalization maintains an invariant v g is normalized state line iv between two iterations so that convergence is observed as a very months the final estimated values were τ months β and β ranking experiments to compare group spam ranking of gsrank we use regression small change max norm of less than δ in the value the difference as our of terminating of v v g g over consecutive we employ iterations l as it to is be a normalization and learning to rank as our baselines regression is a natural choice because the spamicity score of each group from the judges is a real value over sec the problem of ranking also prevents any overflow that might occur due to the geometric growth of components during each iteration spammer groups can be seen as optimizing the spamicity of each group as a regression target that is the learned function predicts complexity at each iteration of a with zero elements v g in so a it and takes t gsrank requires the multiplication o t e where e is the total number the spamicity score of each test group the test groups can then be is the number of non of iterations in terms ranked based on the values for this set of experiments we use the support vector regression svr system in svmlight of p g and m it takes o t g m p m p which is linear in the number of candidate groups discovered by fim the actual computation w gm however is quite fast since the matrices w pg learning to rank is our second baseline approach given the w mp training different samples rankings x x y n a learning of the to rank system takes as input k are quite sparse due to the power law distribution followed by reviews furthermore gsrank being an instance of power q y k samples generated by k queries iteration it can efficiently deal with large and sparse matrices as it does not compute a matrix decomposition experimental evaluation we now evaluate the proposed gsrank method we use the groups described in sec we first split groups into the development set d with groups randomly sampled for parameter estimation and the validation set v with groups for evaluation all evaluation metrics are averaged over fold cross validation cv on v below we first describe parameter estimation and then ranking and classification experiments parameter estimation our proposed behavioral model has two parameters τ and β which have to be estimated τ is the parameter of gtw i e the time interval beyond which members in a group are not likely to be working in collusion β is the parameter of getf which denotes the time interval beyond which reviews posted are not considered to be early anymore sec for this estimation we again use the classification setting the estimated parameters actually work well in general as we will see in the next two subsections let θ denote the two parameters we learn θ using a greedy hill climbing search to maximize the log likelihood of the set d a matrix a n x n learning q k each algorithm ranking learns y i is a a ranking permutation model of h x which x n based is then on used q i the to rank the test samples ranking spam groups u the u desired m based information on a query q in our case of need q denotes the question are these group spam to prepare training rankings we treat each feature f as a ranking function i e the groups are ranked in descending generates training order ranks of a values learning attained algorithm by each then f learns this the optimal ranking function given no other knowledge this is a reasonable approach since f f groups sec the rank produced f are strongly by each correlated with spam feature is thus based on a certain spamicity dimension none of the training ranks may be optimal a learning to rank method basically learns an optimal ranking vectorized function using with represented the combination with a vector of of f each group is the group spam features we ran two widely used learning to rank algorithms svmrank and rankboost for svmrank we used the system in rankboost was from for both systems their default parameters were applied we also experimented with ranknet in ranklib but its results are significantly poorer on our data thus its results are not included in addition we also experimented with the following baselines group spam feature sum gsfsum as each group feature f f an f obvious measures baseline spam although behavior naïve on a specific is to rank spam the dimension groups in descending order of the sum of all feature values over the field f is diagonalizable iff the sum of the dimensions of its eigenspaces is equal to n this can be shown to be equivalent to a being of full rank with n linearly independent eigenvectors the proof remains equally valid when a is defective not diagonalizable i e it has k g linearly independent eigenvectors and hence the summation in goes up to k convergence of gsrank is still guaranteed because of the following argument helpfulness score hs in many review sites readers can provide helpfulness feedback to each review it is reasonable to assume that spam reviews should get less helpfulness feedback hs uses the mean helpfulness score percentage of people who found a review helpful of reviews of each group to rank groups in ascending order of the scores lim t k i http www cs umass edu vdang ranklib html λ λ i t α ii v lim whenever t g t i i λ λ α ii v as k g using this threshold our implementation converges in iterations www session fraud and bias in user ratings april lyon france from figure we observe that gsrank performs the best at all top rank positions except at the bottom which are unimportant because they are most likely to be non spam since in each fold of cross validation the test set has only groups and out of the there are at most spam groups see table below paired t tests for rank positions k show that all the gsrank svr svmrank rankboost gsfsum hs 40 improvements of gsrank over other methods are significant at the confidence level of although regression is suitable for the task it did not perform as well as rankboost and svmrank and behave similarly to rankboost figure ndcg k comparisons ndcg for top rank positions and svmrank but performed slightly poorer gsfsum fared mediocrely as ranking based on summing all feature values is gsrank svr svmrank rankboost gsfsum hs 40 a the spamicity threshold of ξ unable to balance the weights of features because not all features are equal in discriminative strength hs performs poorly which reveals that while many genuine reviews may not be helpful spam reviews can be quite helpful deceptive thus helpfulness scores are not good for differentiating spam and non spam groups since in many applications the user wants to investigate a certain number of highly likely spam groups and ndcg does not give any guidance on how many are very likely to be spam we thus also use precision n to evaluate the rankings in this case we gsrank svr need to know which test groups are spam and non spam we can svmrank rankboost gsfsum hs use a threshold ξ on the spamicity to decide that which can reflect the user strictness for spam since in different applications the user may want to use different thresholds we use two thresholds in our experiments ξ and ξ that is if the spamicity value is ξ the group is regarded as spam otherwise non spam 40 80 these thresholds give us the following data distributions b the spamicity threshold of ξ figure precision n 40 80 rank positions all the improvements of gsrank over other methods are statistically significant at ξ ξ spam non spam the confidence level of based on paired t test table data distributions for the two spamicity thresholds ξ heuristic training rankings h in our preliminary study three heuristic rankings using feature mixtures were proposed to generate the training ranks for learning to rank methods we list them briefly here for details please see figure a and b show the precisions 40 60 80 and top rank positions for ξ and ξ respectively we can see that gsrank consistently outperforms all existing methods rankboost is the strongest among the existing methods h h h g g g g r h g gcs g gmcs g classification experiments if a spamicity threshold is applied to decide spam and non spam groups supervised classification can also be applied using the we thresholds use svm of ξ in svmlight and we with have linear the labeled kernel data and in table logistic regression lr in weka www cs waikato ac nz ml weka as the learning algorithms the commonly used measure auc area under the roc curve is employed for classification evaluation next we discuss the features that we consider in learning group spam features gsf f g g r r h h g g gs g gsup g gtw g gsr g getf g gd g using these three functions to generate the training ranks we ran the learning to rank methods we denote these methods and their results with rankboost_h and to compare rankings we first use normalized discounted cumulative gain ndcg as our evaluation metric ndcg is commonly used to evaluate retrieval algorithms with respect to an ideal ranking based on relevance it rewards rankings with the most relevant results at the top positions which is also our objective i e to rank those groups with the highest spamicities at the top the spamicity score for each group computed from judges sec thus can be regarded as the relevance score to generate the ideal spam ranking let r m be the relevance score of the mth ranked item ndcg k is defined as eight group features presented f in sec these are the proposed individual spammer features isf a set of features for detecting individual spammers was reported in using these features we represented each group with their average values of all the members of each group we want to see ndcg k dcg k k whether such individual spammer features are also effective for groups note that these features cover those in sec linguistic features of reviews lf in word and pos part of speech n gram features were shown to be effective for detecting individual fake reviews here we want to see whether such features are also effective for spam groups for each group we merged its reviews into one document and represented it with these linguistic features table a and b show the auc values of the two classification algorithms for different feature settings using fold cross m k log mr m where z k z dcg k is the discounted cumulative gain dcg of the ideal ranking of the top k results we report ndcg scores at various top positions up to in figure in our case r m refers to the score g applied m if computed needed where by each g is ranking the group algorithm ranked normalization at position m was to compute spamicity g z k m dcg k for the ideal ranking we use the from our expert judges www session fraud and bias in user ratings april lyon france gs rank gsf 87 85 isf 68 lf 62 69 68 gsf isf lf 83 b the spamicity threshold of ξ table auc results of different algorithms and feature sets all the improvements of gsrank over other methods are statistically significant at the confidence level of based on paired t test validation for ξ and ξ respectively it also includes the ranking algorithms in sec as we can also compute their auc values given the spam labels in the test data note that the relation based model of gsrank could not use other features than gsf features and the features in sec not shown in table here again we observe that gsrank is significantly better than all other algorithms with the confidence level using paired t test rankboost again performed the best among the existing methods individual spam features isf performed poorly this is understandable because they cannot represent group behaviors well linguistic features lf fared poorly too we believe it is because content based features are more useful if all reviews are about the same type of products the language used in fake and genuine reviews can have some subtle differences however reviewers in a group can review different types of products even if there are some linguistic differences among spam and non spam reviews the features become quite sparse and less effective due to a large number of product types and not so many groups we also see that combining all features table last row in each table improves auc slightly rankboost achieved auc 86 ξ and ξ which are still significantly lower than auc ξ and ξ for gsrank respectively finally we observe that the results for ξ are slightly better than those for ξ this is because with the threshold ξ the spam and non spam groups are well separated see table in summary we conclude that gsrank outperforms all baseline methods including regression learning to rank and classification this is important considering that gsrank is an unsupervised method this also shows that the relation based model used in gsrank is indeed effective in detecting opinion spammer groups conclusions this paper proposed to detect group spammers in product reviews the proposed method first used frequent itemset mining to find a set of candidate groups from which a labeled set of spammer groups was produced we found that although labeling individual fake reviews or reviewers is hard labeling groups is considerably easier we then proposed several behavior features derived from collusion among fake reviewers a novel relation based model called gsrank was presented which can consider relationships among groups individual reviewers and products they reviewed to detect spammer groups this model is very different from the traditional supervised learning approach to spam detection experimental results showed that gsrank significantly outperformed the state of the art supervised classification regression and learning to rank algorithms online social networking technologies enable individuals to simultaneously share information with any number of peers quantifying the causal effect of these mediums on the dis semination of information requires not only identification of who influences whom but also of whether individuals would still propagate information in the absence of social sig nals about that information we examine the role of social networks in online information diffusion with a large scale field experiment that randomizes exposure to signals about friends information sharing among million subjects in situ those who are exposed are significantly more likely to spread information and do so sooner than those who are not exposed we further examine the relative role of strong and weak ties in information propagation we show that although stronger ties are individually more influential it is the more abundant weak ties who are responsible for the propagation of novel information this suggests that weak ties may play a more dominant role in the dissemination of information online than currently believed categories and subject descriptors h models and principles user machine systems j social and behavioral sciences sociology general terms experimentation measurement human factors keywords social influence tie strength causality introduction social influence can play a crucial role in a range of behav ioral phenomena from the dissemination of information to the adoption of political opinions and technologies which are increasingly mediated through online systems part of this research was performed while the author was a student at the university of michigan copyright is held by the international world wide web conference com mittee distribution of these papers is limited to classroom use and personal use by others www april lyon france acm despite the wide availability of data from online social networks identifying influence remains a challenge indi viduals tend to engage in similar activities as their peers so it is often impossible to determine from observational data whether a correlation between two individuals behaviors ex ists because they are similar or because one person behav ior has influenced the other in the context of information diffusion two people may disseminate the same information as each other because they possess the same in formation sources such as web sites or television that they consume regularly moreover homophily the tendency of individuals with similar characteristics to associate with one another creates difficulties for measuring the relative role of strong and weak ties in information diffusion since peo ple are more similar to those with whom they interact of ten on one hand pairs of individuals who interact more often have greater opportunity to influence one an other and have more aligned interests increasing the chances of contagion however this commonality ampli fies the potential for confounds those who interact more often are more likely to have increasingly similar informa tion sources as a result inferences made from observa tional data may overstate the importance of strong ties in information spread conversely individuals who interact infrequently have more diverse social networks that provide access to novel information but because contact between such ties is intermittent and the individuals tend to be dissimilar any particular piece of information is less likely to flow across weak ties historical attempts to collect data on how often pairs of individuals communicate and where they get their information have been prone to biases further obscuring the empirical relationship between tie strength and diffusion confounding factors related to homophily can be addressed using controlled experiments but experimental work has thus far been confined to the spread of highly specific in formation within limited populations in order to understand how information spreads in a real world envi ronment we wish to examine a setting where a large pop ulation of individuals frequently exchange information with their peers facebook is the most widely used social net working service in the world with over million people using the service each month for example in the united states of adult internet users are on facebook www session information diffusion in social networks april lyon france those american users on average maintain of their real contagion event is a friend such data does not tell us about world contacts on the site and many of these individuals the relative importance of social networks in information dif regularly exchange news items with their contacts in fusion for example consider the spread of news in bradley addition interaction among users is well correlated with self greenberg classsic study of media contagion of reported intimacy thus facebook represents a broad respondents learned about the kennedy assassination via online population of individuals whose online personal net interpersonal ties despite the substantial word of mouth works reflect their real world connections making it an ideal spread it is clear that all of the respondents would have environment to study information contagion gotten the news at a slightly later point in time perhaps we use an experimental approach on facebook to mea from the very same media outlets as their contacts had sure the spread of information sharing behaviors the ex they not communicated with their peers therefore a com periment randomizes whether individuals are exposed via plete understanding of the importance of social networks in facebook to information about their friends sharing behav information diffusion not only requires us to identify sources ior thereby devising two worlds under which information of interpersonal contagion but also requires a counterfactual spreads one in which certain information can only be ac understanding of what would happen if certain interactions quired external to facebook and another in which informa did not take place tion can be acquired within or external to facebook by comparing the behavior of individuals within these two con ditions we can determine the causal effect of the medium on information sharing the remainder of this paper is organized as follows we facebook feed further motivate our study with additional related work in section our experimental design is described in section story then in section we discuss the causal effect of exposure story to content on the newsfeed and how friends sharing behav ior is correlated in time irrespective of social influence via the newsfeed furthermore we show that multiple sharing friends are predictive of sharing behavior regardless of expo story linking to page x sure on the feed and that additional friends do indeed have an increasing causal effect on the propensity to share in section we discuss how tie strength relates to influence and information diffusion we show that users are more likely to have the same information sources as their close friends and that simultaneously these close friends are more likely to influence subjects using the empirical distribution of tie strength in the network we go on to compute the overall effect of strong and weak ties on the spread of information in the network finally we discuss the implications of our work in section related work online networks are focused on sharing information and as such have been studied extensively in the context of in formation diffusion diffusion and influence have been mod eled in blogs email and sites such as twitter digg and flickr one particularly salient charac teristic of diffusion behavior is the correlation between the number of friends engaging in a behavior and the proba bility of adopting the behavior this relationship has been observed in many online contexts from the joining of live journal groups to the bookmarking of photos and the adoption of user created content however as anag nostopoulos et al point out individuals may be more likely to exhibit the same behavior as their friends because of homophily rather than as a result of peer influence sta tistical techniques such as permutation tests and matched sampling help control for confounds but ultimately can not resolve this fundamental problem not all diffusion studies must infer whether one individ ual influenced another for example leskovec et al study the explicit graph of product recommendations sun et al study cascading in page fanning and bakshy et al examine the exchange of user created content how ever in all these studies even if the source of a particular observable unobservable external correlation regular visitation to web sites e mail instant messaging user visits page x user shares page x on facebook figure causal relationships that explain diffusion like phenomena information presented in users news feeds and other sharing behavior on facebook com are observed external events that cause users to be exposed to information outside of facebook cannot be observed and may explain their sharing behavior our experiment blocks the causal relationship dashed arrow between the facebook newsfeed and user visitation by randomly removing stories about friends sharing behavior in subjects feeds thus our experiment allows us to compare situations where both influence via the feed and ex ternal correlations exist the feed condition to situ ations in which only external correlations exist the no feed condition experimental design and data facebook users primarily interact with information through an aggregated history of their friends recent activity sto ries called the news feed or simply feed for short some of these stories contain links to content on the web uniquely identified by urls our experiment evaluates how much exposure to a url on the feed increases an individual propensity to share that url beyond correlations that one might expect among facebook friends for example friends with whom a user interacts more often may be more likely to visit sites that the user also visits as a result those friends may be more likely to share the same url as the www session information diffusion in social networks april lyon france b figure an example of the facebook news feed interface for a hypothetical subject who has a link high lighted in red assigned to the a feed or b no feed condition user before she has the opportunity to share that content herself additional unobserved correlations may arise due to external influence via e mail instant messaging and other social networking sites these causal relationships are illus trated in figure from the figure one can see that all unobservable correlations can be identified by blocking the causal relationship between the facebook feed and sharing our experiment therefore randomizes subjects with respect to whether they receive social signals about friends sharing behavior of certain web pages via the facebook feed assignment procedure subject url pairs are randomly assigned at the time of display to either the no feed or the feed condition stories that contain links to a url assigned to the no feed condi tion for the subject are never displayed in the subject feed those assigned to the feed condition are not removed from the feed and appear in the subject feed as normal fig ure pairs are deterministically assigned to a condition at the time of display so any subsequent share of the same url by any of a subject friends is also assigned to the same con dition to improve the statistical power of our results twice as many pairs were assigned to the no feed condition be cause removal from the feed occurs on a subject url basis and we include only a small fraction of subject url pairs in the no feed condition a shared url is on average delivered to over of its potential targets all activity relating to subject url pairs assigned to ei ther experimental condition is logged including feed expo sures censored exposures and clicks to the url from the feed or other sources like messaging directed shares such as a link that is included in a private facebook message or explicitly posted on a friend wall are not affected by the assignment procedure if a subject url pair is assigned to an experimental condition and the subject clicks on con tent containing that url in any interface other than the feed that subject url pair is removed from the experiment our experiment which took place over the span of seven weeks includes subjects 888 urls and 168 unique subject url pairs ensuring data quality threats to data quality include using content that was or may have been previously seen by subjects on facebook prior to the experiment content that subjects may have seen through interfaces on facebook other than feed spam and malicious content we address these issues in a number of ways first we only consider content that was shared by the subjects friends only after the start of the experiment this enables our experiment to accurately capture the first time a subject is exposed to a link in the feed and ensures that urls in our experiment more accurately reflect content that is primarily being shared contemporaneously with the timing of the experiment we also exclude potential subject a www session information diffusion in social networks april lyon france demographic feature feed no feed of subjects gender female male unspecified age or younger 27 or older country top other united states 9 turkey great britain italy france 9 canada indonesia philippines germany mexico others table summary of demographic features of sub jects assigned to the feed n and no feed n condition some subjects may appear in both columns url pairs where the subject had previously clicked on the url via any interface on the site at any time up to two months prior to exposure or any interface other than the feed for content assigned to the no feed condition finally we use the facebook site integrity system 40 to classify and remove urls that may not reflect ordinary users purposeful intentions of distributing content to their friends population the experimental population consists of a random sample of all facebook users who visited the site between august to october and had at least one friend sharing a link at the time of the experiment there were approxi mately million facebook users logging in at least once a month our sample consists of approximately million of these users all facebook users report their age and gender and a user country of residence can be inferred from the ip address with which she accesses the site in our sample the median and average age of subjects is and respec tively subjects originate from countries and territories of which have one million or more subjects additional summary statistics are given in table and show that sub jects are assigned to the conditions in a balanced fashion evaluating outcomes the assignment procedure allows us to directly compare the overall probability that subjects share links they were or were not exposed to on the feed the causal effect of exposure via the facebook feed on sharing is simply the ex pected probability of sharing in the feed condition minus the expected probability in the no feed condition this quantity known as the average treatment effect on the treated or al ternatively the absolute risk increase can vary when con ditioning on other variables including the number of friends and tie strength which are analyzed in sections and al ternatively the difference in probabilities can be viewed as a ratio the relative risk ratio which quantifies how many times more likely an individual is to share as a result of being exposed to content on the feed although the assignment is completely random subjects and urls may differ in ways that impact our measurements for example certain users may be highly active on face book so that they are assigned to experimental conditions more often than other users if these users were to vary sig nificantly in terms of their information sharing propensities such as sharing or re sharing greater or fewer links than oth ers the disproportionate inclusion of these users may bias our measurements and threaten the population validity of our findings similarly very popular urls may also intro duce biases they may be more or less likely to be re shared because of their inherent appeal or more likely to be dis covered independently of facebook because of their relative popularity amongst friends to provide control for these biases we use bootstrapped averages clustered by the subject or url we find that in all of our analyses clustering by the url rather than the sub ject yields nearly identical probability estimates that have marginally wider confidence intervals so we have chosen to present our results using means and confidence inter vals clustered by url risk ratios are obtained using the bootstrapped confidence intervals of likelihood of shar ing in the feed and no feed conditions to compute the lower bound of the ratio we divide the lower bound of the prob ability of sharing in the feed condition by the upper bound for the no feed condition the upper bound of the ratio is computed by dividing the upper bound in the feed condition by the lower bound of the no feed condition the additive analog of the same procedure is used to obtain confidence intervals for probability differences how exposure to social signals affects diffusion we find that subjects who are exposed to signals about friends sharing behavior are several times more likely to share that same information and share sooner than those who are not exposed to measure the relative increase in sharing due to exposure we compute the risk ratio the like lihood of sharing in the feed condition divided by the likelihood of sharing in the no feed condition and find that individuals in the feed condition are times more likely share ci 72 although the probability of sharing upon exposure may appear small it is important to note that individuals have hundreds of con tacts online who may see their link and that on average one out of every urls that are clicked on in the feed condition are subsequently re shared temporal clustering contemporaneous behavior among connected individuals is commonly used as evidence for social influence processes e g 9 we find that subjects who share the same link as their friends typically do so within a time that is proximate to their friends sharing time even when no exposure occurs on facebook figure illustrates the cumulative distribution of information lags between the subject and their first sharing friend among www session information diffusion in social networks april 20 lyon france tion we first match the share time of each url in the feed condition with a share time of the url in the no feed con dition sampling urls in proportion to their relative abun y t i n e d e v i t a l u m u c dances in the data from this set of contrasts we find that the median sharing latency after a friend has already shared the content is hours in the feed condition compared to 20 hours when assigned to the no feed condition wilcoxon rank sum test p the presence of strong tempo ral clustering in both experimental conditions illustrates the problem with inferring influence processes from observations feed of temporally proximate behavior among connected individ no feed uals regardless of access to social signals within a particular online medium individuals can still acquire and share the 20 25 share time alter share time days same information as their friends albeit at a slightly later point in time a effect of multiple sharing friends classic models of social and biological contagion e g 23 predict that the likelihood of infection increases with y it n e d e v i t a l u m u c the number of infected contacts observational studies of online contagion 9 not only find evidence of temporal clustering but also observe a similar relationship between the likelihood of contagion and the number of in fected contacts however it is important to note that this correlation can have multiple causes that are unrelated to social influence processes for examle if a website is pop feed no feed ular among friends then a particularly interesting page is more likely to be shared by a users friends independent of one another the positive relationship between the num ber of sharing friends and likelihood of sharing may there 20 25 fore simply reflect heterogeneity in the interestingness of share time exposure time days the content which is clustered along the network the more b popular a page is for a group of friends the more likely it is that one would observe multiple friends sharing it figure temporal clustering in sharing the same link as a friend in the feed and no feed conditions a the difference in sharing time between a subject and their first sharing friend b the difference between the time at which a subject was first to exposed or was to be exposed to the link and the time at which they shared vertical lines indicate one day and one week we first show that consistent with prior observational studies the probability of sharing a link in the feed condi tion increases with the number of contacts who have already shared the link solid line figure but the presence of a similar relationship in the no feed condition grey line fig ure shows that an individual is more likely to exhibit the sharing behavior when multiple friends share even if she does not necessarily observe her friends behavior there fore when using observational data the na ıve conditional probability which is equivalent to the probability of shar ing in the feed condition does not directly give the proba subjects who had shared a url after their friends the top bility increase due to influence via multiple sharing friends panel shows the latency in sharing times between the subject rather such an estimate reflects a mixture of internal influ and their friend for users in the feed and no feed condition ence effects and external correlation while a larger proportion of users in the feed condition share our experiment allows us to directly measure the effect of a link within the first hour of their friends the distribution the feed relative to external factors computed as either the of sharing times is strikingly similar the bottom panel difference or ratio between the probability of sharing in the shows the differences in time between when subjects shared feed and no feed conditions figure while the differ and when they were or would have been first exposed to ence in sharing likelihood grows with the number of sharing their friends sharing behavior on the facebook feed the friends the relative risk ratio falls this contrast suggests horizontal axis is negative when a subject had shared a link that social information in the feed is most likely to influence after a friend but had not yet seen that link on the feed a user to share a link that many of her friends have shared from this comparison it is easy to see that users in the feed but the relative impact of that influence is highest for con condition are most likely to share a link immediately upon tent that few friends are sharing the decreasing relative exposure while those who share it without seeing it in their effect is consistent with the hypothesis that having multi feed will do so over a slightly longer period of time ple sharing friends is associated with greater redundancy in to evaluate how exposure on the facebook feed relates information exposure which may either be caused by ho to the speed at which urls appear to diffuse we consider mophily in visitation and sharing tendencies or external urls that were assigned to both the feed and no feed condi influence www session information diffusion in social networks april 20 lyon france condition feed no feed a b c figure users with more friends sharing a web link are themselves more likely to share a the probability of sharing for subjects that were feed and were not no feed exposed to content increases as a function of the number sharing friends b the causal effect of the feed is greater when subjects have more sharing friends c the multiplicative impact of the feed is greatest when few friends are sharing error bars represent the bootstrapped confidence intervals clustered on the url tie strength and influence next we examine the relationship between tie strength influence and information diversity by combining the ex perimental data with users online and offline interactions following arguments originally proposed by mark granovet ter seminal paper the strength of weak ties empirical work linking tie strength and diffusion often uti lize the number of mutual contacts as proxies of interaction frequency rather than using the number of mutual con tacts which can be large for pairs of individuals who no longer communicate e g former classmates we directly measure the strength of tie between a subject and her friend in terms of four types of interactions i the frequency of private online communication between the two users in the form of facebook ii the frequency of public on line interaction in the form of comments left by one user on another user posts iii the number of real world coin cidences captured on facebook in terms of both users be ing labeled by users as appearing in the same photograph and iv the number of online coincidences in terms of both users responding to the same facebook post with a com ment frequencies are computed using data from the three months directly prior to the experiment the distribution of tie strengths among subjects and their sharing friends can be seen in figure effect of tie strength we measure how the difference in the likelihood of sharing a url in the feed versus no feed conditions varies according to tie strength to simplify our estimate of the effect of tie strength we restrict our analysis to subjects with exactly one friend who had previously shared the link in both con ditions a subject is more likely to share a link when her quantify message and comment interactions as the number of communication events the subject received from their friend the number of messages and comments sent and the geometric mean of communications sent and re ceived yielded qualitatively similar results so we plot only the single directed measurement for the sake of clarity 020 005 number of sharing friends number of sharing friends number of sharing friends 96 type comments received messages received photo coincidences thread coincidences 20 40 tie strength figure tie strength distribution among friends displayed in subjects feeds using the four measure ments points are plotted up to the percentile note that the vertical axis is collapsed sharing friend is a strong tie figure for example sub jects who were exposed to a link shared by a friend from whom the subject received three comments are 83 times more likely to share than subjects exposed to a link shared by a friend from whom they received no comments for those who were not exposed the same comparison shows that subjects are 84 times more likely to share a link that was previously shared by the stronger tie the larger ef fect in the no feed condition suggests that tie strength is a stronger predictor of externally correlated activity than it is for influence on feed from figure it is also clear that individuals are more likely to be influenced by their stronger www session information diffusion in social networks april 20 lyon france condition 008 feed no feed 006 006 004 004 002 002 000 000 000 8 comments received messages received photo coincidences thread coincidences a 8 8 8 8 2 2 2 2 2 8 2 4 2 4 2 4 8 comments received messages received photo coincidences thread coincidences b figure 6 strong ties are more influential and weak ties expose friends to information they would not have otherwise shared a the increasing relationship between tie strength and the probability of sharing a link that a friend shared in the feed and no feed conditions b the multiplicative effect of feed diminishes with tie strength suggesting that exposure through strong ties may be redundant with external exposure while weak ties carry information one might otherwise not have been exposed to ties via the feed to share content that they would not have otherwise spread furthermore our results extend granovetter hypothesis that weak ties disseminate novel information into the con text of media contagion figure shows that the risk ratio of sharing between the feed and no feed conditions is highest for content shared by weak ties this suggests that weak ties consume and transmit information that one is unlikely to be exposed to otherwise thereby increasing the diversity of information propagated within the network 2 collective impact of ties strong ties may be individually more influential but how much diffusion occurs in aggregate through these ties de pends on the underlying distribution of tie strength i e figure using the experimental data we can estimate the amount of contagion on the feed generated by strong and weak ties the causal effect of exposure to information shared by friends with tie strength k is given by the average treatment effect on the treated atet k p k feed p k no feed to determine the collective impact of ties of strength k we multiply this quantity by the fraction of links displayed in all users feeds posted by friends of tie strength k denoted by f k in order to compare the impact of weak and strong ties we must set a cutoff value for the minimum amount of interaction required between two individuals in order to consider that tie strong setting the cutoff at k a single interaction provides the most generous classification of strong ties while preserving some meaningful distinction between strong and weak ties thereby giving the most in fluence credit to strong ties under this categorization of strong and weak ties the esti mated total fraction of sharing events that can be attributed to weak and strong ties is the average treatment effect on the treated weighted by the proportion of url exposures from each tie type t weak atet f t strong n atet i f i i we illustrate this comparison in figure and show that by a wide margin the majority of influence is generated by weak although we have shown that strong ties are individually more influential the effect of strong ties is not large enough to match the sheer abundance of weak ties 6 discussion social networks may influence an individual behavior but they also reflect the individual own activities inter ests and opinions these commonalities make it nearly im possible to determine from observational data whether any particular interaction mode of communication or social en vironment is responsible for the apparent spread of a behav ior through a network in the context of our study there are three possible mechanisms that may explain diffusion like phenomena an individual shares a link on facebook that for the purposes of this study it is not neces sary to model the effect of tie strength for users with multi ple sharing friends since stories of this kind only constitute 4 2 of links in the newsfeed and their inclusion would not dramatically alter the balance of aggregate influence by tie strength www session information diffusion in social networks april 20 lyon france find that the majority of influence results from exposure to weak individual weak ties which indicates that most information diffusion on facebook is driven by simple contagion this strong stands in contrast to prior studies of influence on the adop tion of products behaviors or opinions which center around the effect of having multiple or densely connected contacts weak who have adopted 6 13 our results suggest that in large online environments the low cost of disseminating in strong formation fosters diffusion dynamics that are different from situations where adoption is subject to positive externalities weak or carries a high cost because we are unable to observe interactions that occur strong outside of facebook a limitation of our study is that we can only fully identify causal effects within the site cor related sharing in the no feed condition may occur because weak friends independently visit and share the same page as one another or because one user is influenced to share via an ex strong ternal communication channel twitter is a social media giant famous for the exchange of short character messages called tweets in the scientific community the microblogging site is known for openness in sharing its data it provides a glance into its millions of users and billions of tweets through a streaming api which provides a sample of all tweets matching some parameters preset by the api user the api service has been used by many researchers compa nies and governmental institutions that want to extract knowledge in accordance with a diverse array of ques tions pertaining to social media the essential drawback of the twitter api is the lack of documentation concern ing what and how much data users get this leads re searchers to question whether the sampled data is a valid representation of the overall activity on twitter in this work we embark on answering this question by compar ing data collected using twitter sampled api service with data collected using the full albeit costly firehose stream that includes every single published tweet we compare both datasets using common statistical metrics as well as metrics that allow us to compare topics net works and locations of tweets the results of our work will help researchers and practitioners understand the implications of using the streaming api introduction twitter is a microblogging site where users exchange short character messages called tweets ranking as the most popular site in the world by the alexa rank in january of the site boasts million registered users pub lishing million tweets per day twitter platform for rapid communication is said to be a vital communication platform in recent events including hurricane the arab spring of campbell and several political campaigns tumasjan et al gayo avello metaxas and mustafaraj as a result twitter data has been coveted by both computer and social scientists to better un derstand human behavior and dynamics copyright c association for the advancement of artificial intelligence www aaai org all rights reserved www alexa com topsites www nytimes com interactive nyregion hurricane sandy html social media data is often difficult to obtain with most so cial media sites restricting access to their data twitter poli cies lie opposite to this the twitter streaming api is a capability provided by twitter that allows anyone to retrieve at most a sample of all the data by providing some pa rameters according to the documentation the sample will return at most of all the tweets produced on twitter at a given time once the number of tweets matching the given parameters eclipses of all the tweets on twitter twit ter will begin to sample the data returned to the user the methods that twitter employs to sample this data is currently unknown the streaming api takes three parameters key words words phrases or hashtags geographical boundary boxes and user id one way to overcome the limitation is to use the twit ter firehose a feed provided by twitter that allows access to of all public tweets a very substantial drawback of the firehose data is the restrictive cost another drawback is the sheer amount of resources required to retain the firehose data servers network availability and disk space conse quently researchers as well as decision makers in compa nies and government institutions are forced to decide be tween two versions of the api the freely available but lim ited streaming and the very expensive but comprehensive firehose version to the best of our knowledge no research has been done to assist those researchers and decision mak ers by answering the following how does the use of the streaming api affect common measures and metrics per formed on the data in this article we answer this question from different perspectives we begin the analysis by employing classic statistical measures commonly used to compare two sets of data based on unique characteristics of tweets we design and conduct additional comparative analysis by extracting top ics using a frequently used algorithm we compare how top ics differ between the two datasets as tweets are linked data we perform network measures of the two datasets be cause tweets can be geo tagged we compare the geograph ical distribution of geolocated tweets to better understand how sampling affects aggregated geographic information dev twitter com docs streaming apis a firehose b streaming api figure tag cloud of top terms from each dataset related work twitter streaming api has been used throughout the do main of social media and network analysis to generate un derstanding of how users behave on these platforms it has been used to collect data for topic modeling hong and davi son pozdnoukhov and kaiser network analy sis sofean and smith and statistical analysis of con tent mathioudakis and koudas among others re searchers reliance upon this data source is significant and these examples only provide a cursory glance at the tip of the iceberg due to the widespread use of twitter stream ing api in various scientific fields it is important that we understand how using a sub sample of the data generated affects these results from a statistical point of view the law of large num bers mean of a sample converges to the mean of the en tire population and the glivenko cantelli theorem the un known distribution x of an attribute in a population can be approximated with the observed distribution x guarantee satisfactory results from sampled data when the randomly selected sub sample is big enough from network algorith mic wasserman and faust perspective the question is more complicated previous efforts have delved into the topic of network sampling and how working with a restricted set of data can affect common network measures the prob lem was studied earlier in granovetter where the au thor proposes an algorithm to sample networks in a way that allows one to estimate basic network properties more re cently costenbader and valente and borgatti car ley and krackhardt have studied the affect of data error on common network centrality measures by randomly deleting and adding nodes and edges the authors discover that centrality measures are usually most resilient on dense networks in kossinets the authors study global properties of simulated random graphs to better understand data error in social networks leskovec and faloutsos proposes a strategy for sampling large graphs to preserve network measures in this work we compare the datasets by analyzing facets commonly used in the literature we start by comparing the top hashtags found in the tweets a feature of the text com monly used for analysis in tsur and rappoport the authors try to predict the magnitude of the number of tweets mentioning a particular hashtag using a regression model trained with features extracted from the text the authors find that the content of the idea behind the tag is vital to the count of the tweets employing it tweeting a hashtag automatically adds a tweet to a page showing tweets published by other tweeters containing that hashtag in yang et al the authors find that this communal property of hashtags along with the meaning of the tag itself drive the adoption of hash tags on twitter de choudhury et al studies the prop agation patterns of urls on sampled twitter data topic analysis can also be used to better understand the content of tweets kireyev palen and anderson drills the problem down to disaster related tweets discover ing two main types of topics informational and emotional finally yin et al hong et al pozdnoukhov and kaiser all study the problem of identifying topics in geographical twitter datasets proposing models to ex tract topics relevant to different geographical areas in the data joseph tan and carley studies how the topics users discuss drive their geolocation geolocation has become a prominent area in the study of social media data in wakamiya lee and sumiya the authors try to classify towns based upon the content of the geotagged tweets that originate from within the town de longueville smith and luraschi studies twitter use as a sensor for disaster information by study ing the geographical properties of users tweets the authors discover that twitter information is accurate in the later stages of a crisis for information dissemination and retrieval figure 2 raw tweet counts for each day from both the streaming api and the firehose the data from december january we col lected tweets from the twitter firehose matching any of the keywords geographical bounding boxes and users in ta ble during the same time period we collected tweets from the streaming api using tweettracker kumar et al with exactly the same parameters during the time we collected tweets from the streaming api and 280 tweets from the firehose the raw counts of tweets we received each day from both sources are shown in figure 2 one of the more interesting results in this dataset is that as the data in the firehose spikes the streaming api coverage is reduced one possible explanation for this phe nomenon could be that due to the western holidays observed at this time activity on twitter may have reduced causing the threshold to go down one of the key questions we ask in this work is how the amount of coverage affects measures commonly performed on twitter data here we define coverage as the ratio of data from the streaming api to data from the firehose to better understand the coverage of the streaming api for each day we construct a box and whisker plot to visualize the distri bution of daily coverage shown in figure in this period of time the streaming api receives on average of the data available on the firehose on any given day while this is much better than just of the tweets promised by the streaming api we have no reference point for the data in the tweets we received the most striking observation is the range of coverage rates see figure increase of absolute importance more global awareness or relative importance the overall num ber of tweets decreases result in lower coverage as well as fewer tweets to give the reader a sense for the top words in both datasets we include tag clouds for the top words in the streaming api and the firehose shown in figure table parameters used to collect data from syria coor dinates below the boundary box indicate the southwest and northeast corner respectively keywords geoboxes users syria assad aleppovolcano alawite homs hama tartous idlib damascus daraa aleppo syrianrevo 32 8 35 9 42 arabic word for syria figure distribution of coverage for the streaming data by day whiskers indicate extreme values statistical measures we investigate the statistical properties of the two datasets with the intent of understanding how well the characteris tics of the sampled data match those of the firehose we begin first by comparing the top hashtags in the tweets for different levels of coverage using a rank correlation statistic we continue to extract topics from the text matching topical content and comparing topical distribution to better under stand how sampling affects the results of this common pro cess performed on twitter data in both cases we compare our streaming data to random datasets obtained by sampling the data obtained through the firehose top hashtag analysis hashtags are an important communication device on twitter users employ them to annotate the content they produce allowing for other users to find their tweets and to facilitate interaction on the platform also adding a hashtag to a tweet is equivalent to joining a community of users discussing the same topic yang et al in addition hashtags are also used by twitter to calculate the trending topics of the day which encourages the user to post in these communities recently hashtags have become an important part of twitter analysis efron tsur and rappoport recuero and araujo for both the purpose of com munity formation and trend analysis it is important that our streaming dataset convey the same importance for hashtags as the firehose data here we compare the top hashtags in the two datasets using kendall т rank correlation coeffi houla figure 4 relationship between n number of top hashtags and the correlation coefficient т β cient agresti kendall т of top hashtags kendall т is a statistic which measures the correlation of two ordered lists by an alyzing the number of concordant pairs between them con sider two hashtags a and b if both lists rank a higher than b then this is considered a concordant pair otherwise it is counted as a discordant pair ties are handled using the т β statistic as follows т β p c p c p d where p c p d t f p c p d t s discordant but in the not streaming in the is pairs streaming the t data set f is of but the data concordant not set t in s of is the ties the pairs firehose in number the p d firehose of is and ties the n set of data found is the number negative of correlation pairs in total to the perfect т β value positive ranges correlation from perfect to understand the relationship between n and the result ing of curate т β correlation for representation n between т β we construct and of the a chart showing the value in steps of 10 to get an ac differences in correlation at each level of streaming coverage we select five days with differ ent levels of coverage as motivated by figure the mini mum december lower quartile december me dian december upper quartile december and the maximum december the results of this exper iment are shown in figure 4 here we see mixed results at small values of n indicating that the streaming data may not be good for finding the top hashtags at larger values of n we see that the streaming api does a better job of estimating the top hashtags in the firehose data comparison with random samples after seeing the re sults from the previous section we are left to wonder if the results are an artifact of using the streaming api or if we could have obtained the same results by any random sam pling would we obtain the same results with a random sam ple of equal size from the firehose data or does the stream figure random sampling of firehose data relationship between coefficient n for number different of top levels hashtags of coverage and т β the correlation ing api filtering mechanism give us an advantage to an swer this question we repeat the experiments for each day in the previous section this time instead of using stream ing api data we select tweets uniformly at random with out replacement until we have amassed the same number of tweets as we collected from the streaming api for that day we repeat this process times and obtain results as shown in figure here we see that the levels of coverage in the random large n and streaming data have however at smaller n we comparable see a much т different β values pic for ture showing the a random good capacity data gets for very finding high the т β top scores hashtags for n in 10 the dataset the streaming api data does not consistently find the top hashtags in some cases revealing reverse correlation with the firehose data at smaller n this could be indica tive of a filtering process in twitter streaming api which causes a misrepresentation of top hashtags in the data topic analysis topic models are statistical models which discover topics in a corpus topic modeling is especially useful in large data where it is too cumbersome to extract the topics manually due to the large volume of tweets published on twitter topic modeling has become central to many content based stud ies using twitter data kireyev palen and anderson pozdnoukhov and kaiser hong et al yin et al chae et al we compare the topics drawn from the streaming data with those drawn from the firehose data using a widely used topic modeling algorithm latent dirich let allocation lda blei ng and jordan latent dirichlet allocation is an algorithm for the automated dis covery of topics lda treats documents as a mixture of top ics and topics as a mixture of words each topic discovered by lda is represented by a probability distribution which conveys the affinity for a given word to that particular topic we analyze these distributions to understand the differences between the topics discovered in the two datasets to get a sense of how the topics found in the streaming data com pare with those found with random samples we compare with topics found by running lda on random subsamples of the firehose data topic discovery here we compare the topics generated using the firehose corpus with those generated using the streaming corpus lda takes in addition to the corpus three parameters as its input k the number of topics α a hyperparameter for the dirichlet prior topic distribution and a hyperparameter for the dirichlet prior word distri bution choosing optimal parameters is a very challenging problem and is not the focus of this work instead we focus on the similarity of the results given by lda using identical parameters on both the streaming and firehose corpus we set k as suggested by dumais et al and use priors of α k and 7 the software we used to discover the topics is the gensim software package reh urek ˇ and sojka to get an understanding of the topics dis covered at each level of streaming coverage we select the same days as we did for the comparison of kendall т topic comparison to understand the differences be tween the topics generated by lda we compute the dis tance in their probability distribution using the jensen shannon divergence metric lin jan since lda topics have no implicit orderings we first must match them based upon the similarity of the words in the distribution to do the matching we construct a weighted bipartite graph between the topics from the streaming api and the firehose treat ing each topic as a bag of words we use the jaccard score between topic t j f the as the words weight in a of streaming the edges topic in the t i graph s and a firehose d t i s t j f t i s t i s t j f 2 after constructing the graph we use the maximum weight matching algorithm proposed in galil to find the best matches between topics from the streaming and firehose data after making the ideal matches we then compute the jensen shannon divergence between the two topics treat ing each topic as a probability distribution we compute this as follows js t i s t j f t j f 2 kl t i s m kl t j f m where m divergence cover 2 t i s and t j f thomas and kl is the we kullback liebler compute the jensen shannon divergence for each matched pair and plot a histogram of the values in figure 6 we see a trend of higher divergence with lower coverage and lower divergence with higher coverage this shows that decreased coverage in the streaming data causes variance in the discovered topics comparison with random samples in order to get ad ditional perspective on the accuracy of the topics discov ered in the streaming data we compare the streaming data with data sampled randomly from the firehose as we did earlier to compare the correlation first we compute the average of the jensen shannon scores from the streaming data in figure 6 s we then repeat this process for each of the runs we use maximum likelihood with random data each estimation run casella called and x i berger next to estimate the parameters of the gaussian distribu tion from which these points originate ˆμ i x i and ˆσ i x i ˆμ 2 finally we compute the z score for s z sure of the difference s ˆμ ˆσ this score gives us a concrete mea between the streaming api data and the random samples results of this experiment including z scores are shown in figure 7 nonetheless we are still able to get topics from the streaming api that are close to those found in random data with higher levels of cover age a threshold of sigma is often used in the literature to indicate extreme values filliben and others section 6 with this threshold we see that overall we are able to get significantly better topics with the random data than with the streaming api on 4 of the 5 days network measures because twitter is a social network twitter data can be ana lyzed with methods from social network analysis wasser man and faust in addition to statistical measures possible 1 mode and 2 mode networks are user user retweet networks user hashtag content networks hash tag hashtag co occurrence networks for the purpose of this article we focus on user user retweet networks users who send tweets within a certain time period are the nodes in the network furthermore users that are retweeted within this time period are also nodes in this network re gardless of the time their original tweet was tweeted the networks created by this procedure are directed and not sym metric by design however bi directional links are possible in case a b and b a we ignore line weight created by multiple a b retweets and self loops yes some user retweet themselves for the network metrics the compari son is done on both the network and the node levels net works are analyzed using ora carley et al node level measures the node level comparison is accomplished by calculat ing measures at the user level and comparing these results we calculate three different centrality measures at the node level two of which degree centrality and betweenness centrality were defined by freeman as distinct intuitive conceptions of centrality freeman p degree centrality counts the number of neighbors in unweighted networks in particular we are interested in in degree cen trality as this reveals highly respected sources of informa tion in the retweet network where directed edges point to the source betweenness centrality freeman identi fies brokerage positions in the twitter networks that connect different communities with each other or funnel different in formation sources furthermore we calculate the potential reach which counts the number of nodes that are reachable in the network weighted with the path distance in our twit ter networks this is equivalent to the inverse in distance of d a min μ b μ 018 c median μ 018 μ 014 σ 019 σ 018 σ 020 σ 016 e max μ 016 σ 018 figure 6 the jensen shannon divergence of the matched topics at different levels of coverage the x axis is the binned diver gence no divergence was 15 the y axis is the count of each bin μ is the average divergence of the matched topics σ is the standard deviation a min s 024 ˆμ 017 ˆσ 002 z 500 b s 018 ˆμ 012 ˆσ 001 z 6 000 c median s 018 ˆμ 013 ˆσ 001 z 5 000 d s 014 ˆμ 013 ˆσ 001 z 1 000 e max s 016 ˆμ 013 ˆσ 001 z 000 figure 7 the distribution of average jensen shannon divergences in the random data blue curve with the single average obtained through the streaming data red vertical line z indicates the number of standard deviations the streaming data is from the mean of the random samples table 2 average centrality measures for twitter retweet net works for daily networks all is all days together measure k top k min max isfying when we look at the results of the individual days we can see that the matches have once again a broad range all as a function of the data coverage rate in borgatti carley and krackhardt the authors argue that network mea sures are stable for denser networks twitter data being very in degree 10 4 21 9 4 sparse causes the network metrics accuracy to be rather low in degree 53 4 36 potential reach 59 2 32 83 80 betweenness 8 in the case when the data sub sample is smaller however identifying key players correctly for a single day is reasonable and accuracy can be increased by using longer observation periods even more the potential reach metrics are quite stable for some days in the aggregated data reachable nodes sabidussi this approach results in network level measures a metric that finds sources of information users that poten tially can reach many other nodes on short path distances before calculating these measures we extract the main com ponent and delete all other nodes see next sub section in general centrality measures are used to identify important nodes therefore we calculate the number of top 10 and top nodes that can be correctly identified with the stream ing data table 2 shows the results for the average of daily networks the min max range as well as the aggregated net work including all days we complement our node level analysis by comparing vari ous metrics at the network level these metrics are reported in table and are calculated as follows since retweet net works create a lot of small disconnected components we focus only on the size of the largest component the size of the main component and the fact that all smaller compo nents contain less than 1 of the nodes justify our focus on the main component for this data therefore we reduce the networks to their largest component before we proceed with the calculations to describe the structure of the retweet net although we know from previous studies borgatti car works we calculate the clustering coefficient a measure for ley and krackhardt that there is a very low likelihood local density watts and strogatz we do not take all that the ranking will be correct when handling networks with possible triads of directed networks into account but treat missing data the accuracy of the daily results is not very sat the networks as undirected when calculating the clustering table comparison of network level social network analysis metrics firehose streaming api metrics avg day days avg day days nodes 6 590 719 2 466 4 894 9 links 10 173 36 76 37 6 d in 25 1 32 4 20 5 max d in 2 167 1 main comp 5 609 383 2 069 701 main comp 84 6 5 5 92 9 clust coef 029 053 033 050 dc in centr 059 042 085 043 bc centr 010 053 010 050 p reach centr 130 240 coefficient largest component the value of d the in highest that shows are unscaled retweeted the proportion in degree and max d value of nodes in i e shows in num the ber of unique users retweeting the same single user the final three lines of table are network centralization in dexes based on the node level measures that have been intro duced in the previous paragraph freeman freeman describes metric tral node as to the the all difference centralization other node of the values c value x of compared c a x network p to of the for the maximum any most given cen possible difference c x max i i c x p c x p i c x p c x p i 4 high centralization indicates a network with some nodes having very high node level values and many nodes with low values while low centralization is the result of evenly distributed node level measures we do not discuss all details of the individual results but focus on the differences between the two data sources first the coverage of nodes and links is similar to the coverage of tweets this is a good indicator that the sub sample is not biased to the specific twitter user e g high activity the smaller proportion of nodes with non zero in degree for the firehose shows us that the larger number of nodes includes many more peripheral nodes a low clustering co efficient implies that networks are hierarchical rather than interacting communities even though the centralization in dexes are rather similar there is one very interesting result when looking at the individual days the range of values is much higher for the streaming data as a result of the high coverage fluctuation further research will analyze whether we can use network metrics to better estimate how sufficient the sampled streaming data is geographic measures the final facet of the twitter data we compare is the geolo cation of the tweets geolocation is an important part of a table 4 geotagged tweet location by continent excluding boundary box from parameters continent firehose streaming error africa 5 33 10 2 64 antarctica 0 0 asia 26 30 4 15 europe 13 2 01 mid ocean 12 27 0 n america 32 27 5 oceania 1 98 15 1 0 s america 3 0 11 2 0 19 0 total 00 0 00 tweet and the study of the location of content and users is currently an active area of research cheng caverlee and lee wakamiya lee and sumiya we study how the geographic distribution of the geolocated tweets is affected by the sampling performed by the streaming api the number of geotagged tweets is low with only 16 739 geotagged tweets in the streaming data 3 17 and 579 in the firehose data 1 45 we notice that despite the dif ference in tweets collected on the whole we get 10 coverage of geotagged tweets we start by grouping the lo cations of tweets by continent and can find a strong asian bias due to the boundary box we used to collect the data from both sources shown in table 1 to better understand the distribution of geotagged tweets we repeat the same pro cess this time excluding tweets originating in the bound ary box set in the parameters after removing these tweets more than of geotagged tweets from both sources are excluded from the data and the streaming coverage level is reduced to 19 the distribution of tweets by continent is shown in table 4 here we see a more even representation of the tweets locations in asia and north america conclusion and future work in this work we ask whether data obtained through twit ter sampled streaming api is a sufficient representation of activity on twitter as a whole to answer this question we collected data with exactly the same parameters from both the free but limited streaming api and the unlimited but costly firehose we provide a methodology for comparing the two multifaceted sets of data and results of our analysis we started our analysis by understanding the coverage of the streaming api data finding that when the number of tweets matching the set of parameters increases the stream ing api coverage is reduced one way to mitigate this might be to create more specific parameter sets with dif ferent users bounding boxes and keywords this way we might be able to extract more data from the streaming api next we studied the statistical differences between the two datasets we used a common correlation coefficient to understand the differences between the top n hashtags in the two datasets we find that the streaming api data estimates the top hashtags for a large n well but is often misleading when n is small we also employed lda to extract topics from the text we compare the probability distribution of the words from the most closely matched topics and find that they are most similar when the coverage of the streaming api is greatest that is topical analysis is most accurate when we get more data from the streaming api the streaming api provides just one example of how sampling twitter data affects measures we leverage the firehose data to get additional samples to better understand the results from the streaming api in both of the above ex periments we compare the streaming data with 100 datasets sampled randomly from the firehose data we compare the statistical properties to find that the streaming api performs worse than randomly sampled data especially at low cov erage we find that in the case of top hashtag analysis the streaming api sometimes reveals negative correlation in the top hashtags while the randomly sampled data exhibits very high positive correlation with the firehose data in the case of lda we find a significant increase in the accuracy of lda with the randomly sampled data over the data from the streaming api both of these results indicate some bias in the way that the streaming api provides data to the user by analyzing retweet user user networks we were able to show that we can identify on average 60 of the top 100 key players when creating the networks based on one day of streaming api data aggregating some days of data can increase the accuracy substantially for network level measures first in depth analysis revealed interesting corre lation between network centralization indexes and the pro portion of data covered by the streaming api finally we inspect the properties of the geotagged tweets from both sources surprisingly we find that the streaming api almost returns the complete set of the geotagged tweets despite sampling we attribute this to the geographic bound ary box although the number of geotagged tweets is still very small in general 1 researchers using this informa tion can be confident that they work with an almost complete sample of twitter data when geographic boundary boxes are used for data collection when we remove the tweets col lected this way we see a much larger disparity in the tweets from both datasets even with this disparity we see a similar distribution based on continent overall we find that the results of using the streaming api depend strongly on the coverage and the type of anal ysis that the researcher wishes to perform this leads to the next question concerning the estimation of how much data we actually get in a certain time period we suggest that we found first evidence in different types of analysis that can help us to estimate the streaming api coverage uncovering the nuances of the streaming api will help researchers busi ness analysts and governmental institutions to better ground their scientific results based on twitter data looking forward we hope to find methods to compensate for the biases in the streaming api to provide a more accu rate picture of twitter activity to researchers provided fur ther access to twitter s firehose we will determine whether the methodology presented here will yield similar results for twitter data collected from other domains such as natural disaster protest and elections challenges and opportunities with big data alexandros labrinidis university of pittsburgh pittsburgh pa usa labrinid cs pitt edu h v jagadish university of michigan ann arbor mi usa jag umich edu abstract the promise of data driven decision making is now being recog nized broadly and there is growing enthusiasm for the notion of big data including the recent announcement from the white house about new funding initiatives across different agencies that target research for big data while the promise of big data is real for example it is estimated that google alone contributed billion dollars to the us economy in there is no clear consensus on what is big data in fact there have been many controversial state ments about big data such as size is the only thing that matters in this panel we will try to explore the controversies and debunk the myths surrounding big data 1 introduction it is hard to avoid mention of big data anywhere we turn to day there is broad recognition of the value of data and products obtained through analyzing it 1 popular news media now ap preciates the value of big data as evidenced by coverage in the economist 2 3 the new york times 5 and national pub lic radio 7 8 industry is abuzz with the promise of big data 6 government agencies have recently announced significant pro grams towards addressing challenges of big yet many have a very narrow interpretation of what that means and we lose track of the fact that there are multiple steps to the data analysis pipeline whether the data are big or small at each step there is work to be done and there are challenges with big data the first step is data acquisition some data sources such as sen sor networks can produce staggering amounts of raw data much of this data is of no interest and it can be filtered and compressed by orders of magnitude one challenge is to define these filters in such a way that they do not discard useful information for exam ple in considering news reports is it enough to retain only those that mention the name of a company of interest do we need the full report or just a snippet around the mentioned name the sec ond big challenge is to automatically generate the right metadata www whitehouse gov blog 29 big data big deal to describe what data is recorded and how it is recorded and mea sured this metadata is likely to be crucial to downstream analysis for example we may need to know the source for each report if we wish to examine duplicates frequently the information collected will not be in a format ready for analysis the second step is an information extraction process that pulls out the required information from the underlying sources and expresses it in a structured form suitable for analysis a news report will get reduced to a concrete structure such as a set of tuples or even a single class label to facilitate analysis fur thermore we are used to thinking of big data as always telling us the truth but this is actually far from reality we have to deal with erroneous data some news reports are inaccurate data analysis is considerably more challenging than simply lo cating identifying understanding and citing data for effective large scale analysis all of this has to happen in a completely au tomated manner this requires differences in data structure and semantics to be expressed in forms that are computer understand able and then robotically resolvable even for simpler analyses that depend on only one data set there remains an important question of suitable database design usually there will be many alterna tive ways in which to store the same information certain designs will have advantages over others for certain purposes and possibly drawbacks for other purposes mining requires integrated cleaned trustworthy and efficiently accessible data declarative query and mining interfaces scalable mining algorithms and big data computing environments a prob lem with current big data analysis is the lack of coordination be tween database systems which host the data and provide sql query ing with analytics packages that perform various forms of non sql processing such as data mining and statistical analyses to day s analysts are impeded by a tedious process of exporting data from the database performing a non sql process and bringing the data back having the ability to analyze big data is of limited value if users cannot understand the analysis ultimately a decision maker pro vided with the result of analysis has to interpret these results usu ally this involves examining all the assumptions made and retrac ing the analysis furthermore as we saw above there are many possible sources of error computer systems can have bugs models permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies almost always have assumptions and results can be based on erro neous data for all of these reasons users will try to understand and verify the results produced by the computer the computer bear this notice and the full citation on the first page to copy otherwise to system must make it easy for her to do so by providing supplemen republish to post on servers or to redistribute to lists requires prior specific permission and or a fee articles from this volume were invited to present their results at the international conference on very large data bases august istanbul turkey proceedings of the vldb endowment vol 5 no 12 tary information that explains how each result was derived and based upon precisely what inputs in short there is a multi step pipeline required to extract value from data heterogeneity incompleteness scale timeliness pri copyright vldb endowment 12 08 10 00 ________________ vacy and process complexity give rise to challenges at all phases of the pipeline furthermore this pipeline is not a simple linear flow rather there are frequent loops back as downstream steps suggest changes to upstream steps there is more than enough here that we in the database research community can work on 4 2 panel goals the panel has multiple goals first of all to identify if why big data is different from past very large database techniques and what are the most challenging aspects of big data secondly to determine how can the data management industry and academia collaborate towards solving big data challenges finally to con sider the role of the data management community within the big data solutions ecosystem in order to address these goals the panel will discuss the validity of the following claims big data is the same as scalable analytics big data problems are primarily at the application side big data problems are primarily at the systems level big data requires a cloud based platform the data management community is in danger of missing the big data train it is not possible to conduct big data research effectively without collaborating with people outside the data manage ment community all the big data problems can be reduced to map reduce problems the bulk of big data challenges are being addressed by in dustry the bulk of big data challenges are at the implementation level size is the only thing that matters for big data 3 panel composition 3 1 moderators alexandros labrinidis is an associate professor at the department of computer science of the university of pittsburgh and co director of the advanced data management technologies lab he is also an adjunct associate professor at carnegie mellon university cs dept he is currently the secretary treasurer for acm sigmod and has served as the editor of sigmod record he is the recipient of an nsf career award in h v jagadish is bernard a galler collegiate professor of electrical engineering and computer science and director of the software systems research laboratory at the university of michigan in ann arbor he is a fellow of the acm serves on the board of the computing research association and is founding editor in chief of the proceedings of the vldb endowment since 3 2 panelists susan davidson weiss professor and chair department of computer and information science founding co director center for bioinformatics and previously deputy dean school of engineering and applied sciences all at the university of pennsylvania also co founder greater philadelphia bioinformatics alliance johannes gehrke tisch university professor department of computer sciences cornell university he has received the cornell university provost s award for distinguished scholarship a humboldt research award from the alexander von humboldt foundation the ieee computer society technical achievement award and the blavatnik award for young scientists from the new york academy of sciences nick koudas professor department of computer science uni versity of toronto and president co founder sysomos he was named inventor of the year by the university of toronto raghu ramakrishnan technical fellow cto information services director cloud and information services lab microsoft he received the acm sigkdd innovation award in and the acm sigmod contributions award in he was elected fel low of the acm in and fellow of the ieee in multi label learning studies the problem where each example is represented by a single instance while associated with a set of labels simultaneously during the past decade significant amount of progresses have been made toward this emerging machine learning paradigm this paper aims to provide a timely review on this area with emphasis on state of the art multi label learning algorithms firstly fundamentals on multi label learning including formal definition and evaluation metrics are given secondly and primarily eight representative multi label learning algorithms are scrutinized under common notations with relevant analyses and discussions thirdly several related learning settings are briefly summarized as a conclusion online resources and open research problems on multi label learning are outlined for reference purposes index terms multi label learning label correlations problem transformation algorithm adaptation introduction t studied raditional machine supervised learning learning paradigms is one where of the each mostly real world object example is represented by a single instance feature vector and associated with a single label formally let x denote the instance space and y denote the label space the task of traditional supervised learning is to learn a function f x y from the training set x i express its semantics following the above consideration the paradigm of multi label learning naturally emerges in contrast to traditional supervised learning in multi label learning each object is also represented by a single instance while associated with a set of labels instead of a single label the task is to learn a function which can predict the proper y i label sets for unseen instances i m here x i x is an instance characterizing the early researches on multi label learning mainly focus properties features of an object and y i y is the corre on the problem of multi label text categorization sponding label characterizing its semantics therefore one during the past decade multi label learning fundamental assumption adopted by traditional supervised has gradually attracted significant attentions from machine learning is that each example belongs to only one concept learning and related communities and has been widely i e having unique semantic meaning applied to diverse problems from automatic annotation although traditional supervised learning is prevailing for multimedia contents to bioin and successful there are many learning tasks where the formatics web mining rule above simplifying assumption does not fit well as real mining information retrieval tag world objects might be complicated and have multiple recommendation etc specifically in recent six semantic meanings simultaneously to name a few in text years there are more than papers with categorization a news document could cover several top keyword multi label or multilabel in the title appearing ics such as sports london olympics ticket sales and torch in major machine learning related conferences including relay in music information retrieval a piece of symphony icml ecml pkdd ijcai aaai kdd icdm nips could convey various messages such as piano classical music this paper serves as a timely review on the emerging mozart and austria in automatic video annotation one area of multi label learning where its state of the art is video clip could be related to some scenarios such as urban presented in three parts in the first part section funda and building and so on mentals on multi label learning including formal definition to account for the multiple semantic meanings that learning framework key challenge threshold calibration one real world object might have one direct solution is and evaluation metrics example based label based theo to assign a set of proper labels to the object to explicitly retical results are given in the second and primary part m l zhang is with the school of computer science and engineering southeast university nanjing china and the key laboratory of computer network and information integration southeast university ministry of education china e mail zhangml seu edu cn z h zhou is with the national key laboratory for novel software technology nanjing university nanjing china e mail zhouzh lamda nju edu cn in a broad sense multi label learning can be regarded as one possible instantiation of multi target learning where each object is associated with multiple target variables multi dimensional out puts different types of target variables would give rise to different instantiations of multi target learning such as multi label learning binary targets multi dimensional classification categorical multi class targets multi output multivariate regression numerical tar gets and even learning with combined types of target variables manuscript received june revised jan accepted feb note that there have been some nice reviews on multi label date of publication mar date of current version july learning techniques compared to earlier attempts in recommended for acceptance by j pei this regard we strive to provide an enriched version with the fol for information on obtaining reprints of this article please send e mail to lowing enhancements a in depth descriptions on more algorithms reprints ieee org and reference the digital object identifier below b comprehensive introductions on latest progresses c succinct sum digital object identifier tkde marizations on related learning settings c ieee personal use is permitted but republication redistribution requires ieee permission see http www ieee org publications rights index html for more information ieee transactions on knowledge and data engineering vol no august section technical details of up to eight representative multi label algorithms are scrutinized under common nota tions with necessary analyses and discussions in the third part section several related learning settings are briefly summarized to conclude this review section online resources and possible lines of future researches on multi label learning are discussed t he if each example is confined to have only one single label however the generality of multi label learning inevitably makes the corresponding learning task much more difficult to solve actually the key challenge of learning from multi label data lies in the overwhelming size of output space i e the number of label sets grows exponentially as the number of class labels increases for example for a label space with class labels q the number of possible label sets p aradigm would exceed one million i e formal definition learning framework to cope with the challenge of exponential sized output space it is essential to facilitate the learning process by exploiting correlations or dependency among labels suppose x rd or zd denotes the d dimensional instance for example the probability of an image being anno space and y y y y q denotes the label space with tated with label brazil would be high if we know it has q possible class labels the task of multi label learning is labels rainforest and soccer a document is unlikely to be to learn a function h x from the multi label train labeled as entertainment if it is related to politics therefore ing set d x i y i i m for each multi label effective exploitation of the label correlations information is example x i y i x i x is a d dimensional feature vector deemed to be crucial for the success of multi label learning x with x x i x id for any and y i y is the set of labels associated unseen instance x x the multi label techniques existing strategies to label correlations exploita tion could among others be roughly categorized into three classifier h predicts h x y as the set of proper labels families based on the order of correlations that the learning for x techniques have considered to characterize the properties of any multi label data set several useful multi label indicators can be utilized the most natural way to measure the degree of multi labeledness is label cardinality lcard d first order strategy the task of multi label learning is the average number of labels per example m tackled in a label by label style and thus ignoring co m accordingly i y i i e existence of the other labels such as decomposing the multi label learning problem into a number of label density normalizes label cardinality by the number of independent binary classification problems one per possible labels in the label space lden d another popular multi labeledness measure y is lcard d label the prominent merit of first label diver order strategy lies in its conceptual simplicity and sity ldiv d y x x y d i e the number of high efficiency on the other hand the effectiveness distinct label sets appeared in the data set similarly label of the resulting approaches might be suboptimal due diversity can be normalized by the number of examples to to the ignorance of label correlations indicate the proportion of distinct label sets pldiv d d second order strategy the task of multi label learning ldiv d is tackled by considering pairwise relations between in most cases the model returned by a multi label learn labels such as the ranking between relevant label ing system corresponds to a real valued function f x y and irrelevant label or interaction r where f x y can be regarded as the confidence of y y between any pair of labels etc being the proper label of x specifically given a multi label as label correlations are exploited to some extent by example x y f should yield larger output on the rel second order strategy the resulting approaches can evant label y y and smaller output on the irrelevant label achieve good generalization performance however y y i e f x y f x y note that the multi label clas there are certain real world applications where label sifier h can be derived from the real valued function f correlations go beyond the second order assumption via h x y f x y t x y y where t x r acts as a thresholding function which dichotomizes the label space into relevant and irrelevant label sets for ease of reference table lists major notations used throughout this review along with their mathematical meanings key challenge it is evident that traditional supervised learning can be regarded as a degenerated version of multi label learning in this paper the term multi label learning is used in equiva lent sense as multi label classification since labels assigned to each instance are considered to be binary furthermore there are alternative multi label settings where other than a single instance each example is represented by a bag of instances or graphs or extra ontology knowledge might exist on the label space such as hierarchy struc ture to keep the review comprehensive yet well focused examples are assumed to adopt single instance representation and possess flat class labels high order strategy the task of multi label learning is tackled by considering high order relations among labels such as imposing all other labels influences on each label or addressing con nections among random subsets of labels etc apparently high order strategy has stronger correlation modeling capabilities than first order and second order strategies while on the other hand is computationally more demanding and less scalable in section a number of multi label learning algorithms adopting different strategies will be described in detail to better demonstrate the respective pros and cons of each strategy threshold calibration as mentioned in subsection a common practice in multi label learning is to return some real valued func tion f as the learned model in this case in zhang and zhou review on multi label learning algorithms table summary of major mathematical notations order to decide the proper label set for unseen instance x i e h x the real valued output f x y on each label should be calibrated against the thresholding function output t x generally threshold calibration can be accomplished with two strategies i e setting t as constant function or inducing t from the training examples for the first strategy as f x y takes value in r one straightforward choice is to use zero as the calibration constant another popular choice for calibration constant is when f x y represents the posterior probability of y being a proper label of x furthermore when all the unseen instances in the test set are available the calibration constant can be set to minimize the difference on certain multi label indica tor between the training set and test set notably the label cardinality for the second strategy a stacking style procedure would be used to determine the thresholding function one popular choice is to assume a linear model for t i e t x w f weight vector w and bias b the following linear least squares problem is solved based on the training set d w b min m i w here x i f x i b x i y k y i f x i y arg k min a r a y represents j y j y i f x i y j a y k the target output of the stacking model which bipartitions y into relevant and irrelevant labels for each training example with minimum misclassifications all the above threshold calibration strategies are general purpose techniques which could be used as a post processing step to any multi label learning algorithm returning real valued function f accordingly there also exist some ad hoc threshold calibration techniques which are specific to the learning algorithms and will be introduced as their inherent component in section instead of utilizing the thresholding function t an equiv alent mechanism to induce h from f is to specify the x b where f x number of relevant labels for each example with t x f x y f x y q t rq is a q dimensional stacking vector storing the learning system real valued outputs q such that h x y rank f on each label specifically to work out the q dimensional x y t x here rank f x y returns the rank of y when all class labels in y are sorted in descending order based on f x ieee transactions on knowledge and data engineering vol no august fig summary of major multi label evaluation metrics evaluation metrics brief taxonomy in traditional supervised learning generalization perfor mance of the learning system is evaluated with conven tional metrics such as accuracy f measure area under the roc curve auc etc however performance eval uation in multi label learning is much complicated than traditional single label setting as each example can be asso ciated with multiple labels simultaneously therefore a number of evaluation metrics specific to multi label learn ing are proposed which can be generally categorized into two groups i e example based metrics and label based metrics following the notations in table let s x i the subset accuracy evaluates the fraction of cor rectly classified examples i e the predicted label set is identical to the ground truth label set intuitively subset accuracy can be regarded as a multi label counterpart of the traditional accuracy metric and tends to be overly strict especially when the size of label space i e q is large hamming loss hloss h p p i q h x i y i y i i p be the test set and h be the learned multi label classifier example based metrics work by evaluating the learning system performance on each test exam ple separately and then returning the mean value across the test set different to the above example based met here stands for the symmetric difference between two sets the hamming loss evaluates the fraction of misclassified instance label pairs i e a relevant label is missed or an irrelevant is predicted note that when each example in s is associated with only one label hloss s rics label based metrics work by evaluating the learning system performance on each class label separately and then returning the macro micro averaged value across all class labels note that with respect to h the learning system gen eralization performance is measured from classification per spective however for either example based or label based metrics with respect to the real valued function f which is returned by most multi label learning systems as a com mon practice the generalization performance can also be measured from ranking perspective fig summarizes the major multi label evaluation metrics to be introduced next example based metrics following the notations in table six example based clas sification metrics can be defined based on the multi label classifier h h will be q times of the traditional misclassification rate accuracy exam j conceptually speaking macro averaging and micro ranking loss averaging assume equal weights for labels and exam rloss f p p i ples respectively it is not difficult to show that both accuracy macro y i y y f x i y h accuracy micro h and accuracy micro h hloss h hold note that the macro micro averaged to the set of test instances with without label y j the average precision evaluates the average fraction of relevant labels ranked higher than a particular label y y i x i y the second line of eq follows from the close relation between auc and the wilcoxon mann whitney statistic correspondingly the micro averaged auc can also be derived as for one error coverage and ranking loss the smaller the metric value the better the system performance with opti auc micro mal value of p ieee transactions on knowledge and data engineering vol no august algorithm should therefore be tested on a broad range of metrics instead of only on the one being optimized specifically recent theoretical studies show that classifiers aim at maximizing the subset accuracy would perform rather poor if evaluated in terms of hamming loss and vice versa as multi label metrics are usually non convex and dis continuous in practice most learning algorithms resort to optimizing convex surrogate multi label metrics recently the consistency of multi label learning has been studied i e whether the expected loss of a learned classifier converges to the bayes loss as the training set size increases specifically a necessary and sufficient condition for consistency of multi label learning based on surrogate loss functions is given which is intuitive and can be infor mally stated as that for a fixed distribution over x the set of classifiers yielding optimal surrogate loss must fall in the set of classifiers yielding optimal original multi label loss by focusing on ranking loss it is disclosed that none pairwise convex surrogate loss defined on label pairs is consistent with the ranking loss and some recent multi label approach is inconsistent even for deterministic multi label learning interestingly in contrast to this negative result a complementary positive result on con sistent multi label learning is reported for ranking loss minimization by using a reduction to the bipartite ranking problem simple univariate convex surrogate loss exponential or logistic defined on single labels is shown to be consistent with the ranking loss with explicit regret bounds and convergence rates l earning mathematical rigor we have chosen to rephrase and present each algorithm under common notations in this paper a simple categorization of multi label learning algorithms is adopted problem transformation methods this category of algo rithms tackle multi label learning problem by trans forming it into other well established learning scenarios representative algorithms include first order approaches binary relevance and high order approach classifier chains which transform the task of multi label learning into the task of binary classification second order approach calibrated label ranking which transforms the task of multi label learning into the task of label ranking and high order approach random k labelsets which transforms the task of multi label learning into the task of multi class classification algorithm adaptation methods this category of algorithms tackle multi label learning problem by adapting popular learning techniques to deal with multi label data directly representative algorithms include first order approach ml knn adapting lazy learning techniques first order approach ml dt adapting decision tree techniques second order approach rank svm adapting kernel techniques and second order approach cml adapting information theoretic techniques briefly the key philosophy of problem transformation methods is to fit data to algorithm while the key philosophy of algorithm adaptation methods is to fit algorithm to data fig summarizes the above mentioned algorithms to be detailed in the rest of this section a lgorithms problem transformation methods binary relevance simple categorization the basic idea of this algorithm is to decompose the multi algorithm development always stands as the core issue label learning problem into q independent binary classifi of machine learning researches with multi label learning cation problems where each binary classification problem being no exception during the past decade significant corresponds to a possible label in the label space amount of algorithms have been proposed to learning from following the notations in table for the j th class multi label data considering that it is infeasible to go label y j through all existing algorithms within limited space in this review we opt for scrutinizing a total of eight representative multi label learning algorithms here the representativeness of those selected algorithms are maintained with respect to the following criteria a broad spectrum each algorithm has unique characteristics covering a variety of algorithmic design strategies b primitive impact most algorithms lead to a number of follow up or related methods along its line of research and c favorable influence each algorithm is among the highly cited works in the multi label learning field as we try to keep the selection less biased with the above criteria one should notice that the eight algorithms to be detailed by no means exclude the importance of other meth ods furthermore for the sake of notational consistency and here deterministic multi label learning corresponds to the easier learning case where for any instance x x there exists a label subset y y such that the posteriori probability of observing y given x is greater than i e p y x according to google scholar statistics by january each paper for the eight algorithms has received at least citations with more than citations on average binary relevance firstly constructs a correspond ing binary training set by considering the relevance of each training example to y j d j x i φ y i y j i m where φ y i y j if y j y i otherwise after that some binary learning algorithm b is utilized to induce a binary classifier g j x r i e g j b d j therefore for any multi label training example x i y i instance x i will be involved in the learning process of q binary classifiers for relevant label y j y i x i is regarded as one positive instance in inducing for irrelevant label y k y i x i g j on the other hand is regarded as one neg ative instance the above training strategy is termed as cross training in for unseen instance x binary relevance predicts its associated label set y by querying labeling relevance on each individual binary classifier and then combing relevant labels y y j g j x j q zhang and zhou review on multi label learning algorithms fig categorization of representative multi label learning algorithms being reviewed note that when all the binary classifiers yield negative out puts the predicted label set y would be empty to avoid producing empty prediction the following t criterion rule can be applied y y j g j x j q y j j arg max j q g j x briefly when none of the binary classifiers yield positive predictions t criterion rule complements eq by includ ing the class label with greatest least negative output in addition to t criterion some other rules toward label set prediction based on the outputs of each binary classifier can be found in remarks the pseudo code of binary relevance is sum marized in fig it is a first order approach which builds classifiers for each label separately and offers the natural opportunity for parallel implementation the most promi nent advantage of binary relevance lies in its extremely straightforward way of handling multi label data steps which has been employed as the building block of many state of the art multi label learning techniques on the other hand binary relevance com pletely ignores potential correlations among labels and the binary classifier for each label may suffer from the issue fig pseudo code of binary relevance of class imbalance when q is large and label density i e lden d is low as shown in fig binary relevance has computational complexity o q f b d for testing of o q f b m d for training and classifier chains the basic idea of this algorithm is to transform the multi label learning problem into a chain of binary classification problems where subsequent binary classifiers in the chain is built upon the predictions of preceding ones for q possible class labels y y y q let τ q q be a permutation function which is used to specify an ordering over them i e y τ y τ y τ q for the j th label y τ j j q in the ordered list a corresponding binary training set is constructed by appending each instance with its relevance to those labels d τ j preceding x i pre i τ j y τ j φ y i y τ j i m where pre i τ j φ y i y τ φ y i y τ j here x i prei τ j concatenates vectors x i and prei τ j and prei τ j represents the binary assignment of those labels preceding y τ j on x i specifically prei τ after that in this paper computational complexity is mainly examined with respect to three factors which are common for all learning algorithms i e m number of training examples d dimensionality and q number of possible class labels furthermore for binary multi class learning algorithm b m embedded in problem transformation methods we denote instance complexity its testing training results complexity complexity reported in as as this f f b b paper d m d f m are f d m the q m d q worst case all computational and bounds its per in classifier chains binary assignment is represented by and without loss of generality binary assignment is represented by and in this paper for notational consistency ieee transactions on knowledge and data engineering vol no august fig pseudo code of classifier chains some binary learning algorithm b is utilized to induce a binary classifier g τ j concretely for each label pair y j y k pairwise compari son firstly constructs a corresponding binary training set by considering the relative relevance of each training example to y j and y k d jk x i ψ y i y j y k φ y i y j φ y i y k i m where ψ y i if φ y i y j and x j r i e g τ j b d τ j in other words g τ j determines whether y τ j is a relevant label or not y j y k φ y i y k if φ y i for unseen instance x its associated label set y is pre dicted by traversing the classifier chain iteratively let λx τ j y j and φ y i y k in other words only instances with distinct relevance to represent the predicted binary assignment y j and y k will be included in d jk after that some binary of y τ j on x which are recursively derived as follows λ x τ learning algorithm b is utilized to induce a binary classifier g τ g jk x r i e g jk b d jk therefore for any multi label training example x i λ x τ j sign sign x g τ j x λ x τ λ x τ j j q the learning process of y i y instance i y i x i will be involved in binary classifiers for any instance x x the learning system votes for y j if g jk x here sign is the signed function accordingly the and y k otherwise predicted label set corresponds to for unseen instance x calibrated label ranking firstly y y τ j λ x τ j j q feeds it to the q q trained binary classifiers the overall votes on each possible class label to obtain it is obvious that for the classifier chain obtained as above its effectiveness is largely affected by the ordering ζ x y j specified by τ to account for the effect of ordering an ensemble of classifier chains can be built with n random per mutations over the label space i e τ τ τ n for each permutation τ r r n instead of inducing one classifier chain by applying τ r directly on the orig inal training set d a modified training set d j k q k j g kj g x jk x j q based that on the above definition it is not difficult to verify r is used by ranked sampling d without replacement d r d or with replacement d r d remarks the pseudo code of classifier chains is summarized in fig it is a high order approach which considers correlations among labels in a random man ner compared to binary relevance classifier chains q j according ζ x y j q q here labels in y can be to their respective votes ties are broken arbitrarily thereafter some thresholding function should be further specified to bipartition the list of ranked labels into relevant and irrelevant label set to achieve this within the pairwise comparison framework calibrated label ranking incorpo rates a virtual label y v has the advantage of exploiting label correlations while loses the opportunity of parallel implementation due to its chaining property during the training phase classifier chains augments instance space with extra features from ground truth labeling i e prei τ j into each multi label training example x i y i conceptually speaking the virtual label serves as an artificial splitting point between x i relevant and irrelevant labels in other words y v is lower than y j y i while ranked considered higher than to y k be y i ranked in eq instead of keep ing extra features binary valued another possibility is to set them to the classifier probabilistic outputs when the in addition to the original q q binary classifiers q auxiliary binary classifiers will be induced one for each new label pair y j model returned by b e g naive bayes is capable of yielding posteriori probability i calibrated label ranking the basic idea of this algorithm is to transform the multi label learning problem into the label ranking problem where ranking among labels is fulfilled by techniques of pairwise comparison for q possible class labels y otherwise based on this the binary learning algorithm b is utilized to induce a binary classifier corresponding to the virtual label g a total of induced classifiers q q binary classifiers can be generated by pairwise comparison one for each label pair y zhang and zhou review on multi label learning algorithms fig pseudo code of calibrated label ranking furthermore the overall votes on virtual label can be computed as ζ multi class classifier is induced by the label powerset lp techniques lp is a straightforward approach to transforming multi label learning problem into multi class single label classi fication problem let σ y n be some injective function mapping from the power set of y to natural numbers and σ phase y be lp the firstly corresponding converts the inverse original function multi label in the training training set d into the following multi class training set by treating every distinct label set appearing in d as a new class by comparing eq to eq it is obvious that the training set d jv employed by calibrated label ranking is identical to the training set d j employed by binary classifier induction for unseen instance x lp predicts its associated label set y by firstly querying the prediction of multi class classifier and then mapping it back to the power set of y relevance therefore calibrated label ranking can be y σ regarded as an enriched version of pairwise comparison where the routine q q binary classifiers are enlarged with the q binary classifiers of binary relevance to facilitate learning remarks the pseudo code of calibrated label ranking is summarized in fig it is a second order approach which builds classifiers for any pair of class labels compared to previously introduced algorithms which construct binary classifiers in a one vs rest man ner calibrated label ranking constructs binary classifiers except those for virtual label in a one vs one manner and thus has the advantage of mitigating the negative influence of the class imbalance issue on the other hand the number of binary classifiers constructed by calibrated label ranking grows from linear scale to quadratic scale in terms of the number class labels i e q improvements on calibrated label ranking mostly focus on reducing the quadratic number of classifiers to be queried in testing phase by exact pruning or approximate pruning by exploiting idiosyncrasy of the underlying binary learning algorithm b such as dual representation for perceptron the quadratic number of classifiers can be induced more efficiently in training phase as shown in fig calibrated label ranking has computational complexity of o f b g y unfortunately lp has two major limitations in terms of practical feasibility a incompleteness as shown in eqs and lp is confined to predict label sets appearing in the training set i e unable to generalize to those outside y i y x be i m b inefficiency too many newly mapped classes when y in is large there might overly high complexity training examples for in training some newly g mapped y and d y extremely classes leading to few to keep lp simplicity while overcoming its two major drawbacks random k labelsets chooses to combine ensem ble learning with lp to learn from multi label data the key strategy is to invoke lp only on random k labelsets size k subset in y to guarantee computational efficiency and then ensemble a number of lp classifiers to achieve predictive completeness let y k represent the collection of all possible k labelsets in y y y where k l the k l th k labelset is denoted as yk l i e yk l training set can l be constructed q k similar to eq as well by a multi class shrinking the original label space y into y ds doesn t hold as shown in eqs and it suffices to estimate the prior probabilities as well as likelihoods for making predictions ml knn fulfills the above task via the frequency count y y j μ x y j τ x y j j q ing strategy firstly the prior probabilities are estimated in other words when the actual number of votes exceeds half of the maximum number of votes y j is regarded to by counting the number training examples associated with each label be relevant for an ensemble created by n k labelsets the maximum number of votes on each label is nk q on average p h j a rule of thumb setting for random k labelsets is k and n remarks the pseudo code of random k labelsets is summarized in fig it is a high order approach where the degree of label correlations is controlled by the size of k labelsets in addition to use k labelset another way to improve lp is to prune distinct label set in d appear ing less than a pre specified counting threshold although random k labelsets embeds ensemble learn ing as its inherent part to amend lp major drawbacks ensemble learning could be employed as a meta level strategy to facilitate multi label learning by encompass ing homogeneous or heterogeneous component multi label learners as shown in fig random k labelsets has computational complexity of o n f m m i y j m y i p h j p h j j q here is a smoothing parameter controlling the effect of uniform prior on the estimation generally takes the value of resulting in laplace smoothing secondly the estimation process for likelihoods is some what involved for the j th class label y d for testing algorithm adaptation methods multi label k nearest neighbor ml knn the basic idea of this algorithm is to adapt k nearest neighbor techniques to deal with multi label data where maximum a posteriori map rule is utilized to make prediction by reasoning with the labeling information embodied in the neighbors for unseen instance x let n x represent the set of its k nearest neighbors identified in d generally similarity between instances is measured with the euclidean distance for the j th class label ml knn chooses to calculate the following statistics thereafter by substituting eq prior probabilities and eq likelihoods into eq the predicted label set in eq naturally follows zhang and zhou review on multi label learning algorithms fig pseudo code of ml knn remarks the pseudo code of ml knn is summarized in fig it is a first order approach which reasons the rele vance of each label separately ml knn has the advantage of inheriting merits of both lazy learning and bayesian rea soning a decision boundary can be adaptively adjusted due to the varying neighbors identified for each unseen instance b the class imbalance issue can be largely miti gated due to the prior probabilities estimated for each class label there are other ways to make use of lazy learn ing for handling multi label data such as combining knn with ranking aggregation identifying knn in a label specific style expanding knn to cover the whole training set considering that ml knn is ignorant of exploiting label correlations several extensions have been proposed to provide patches to ml knn along this direction as shown in fig ml knn has computational complexity of o qmk for training and o md qk for testing multi label decision tree ml dt the basic idea of this algorithm is to adopt decision tree tech niques to deal with multi label data where an information gain criterion based on multi label entropy is utilized to build the decision tree recursively given any multi label data set t x i criterion c is met e g size of the child node is less than the pre specified threshold to instantiate ml dt the mechanism for computing multi label entropy i e mlent in eq needs to be specified a straightforward solution is to treat each subset y y as a new class and then resort to the conventional single label entropy mlent t however as the number of new classes grows exponen tially with respect to y many of them might not even appear in t and thus only have trivial estimated probabil ity i e p y to circumvent this issue ml dt assumes independence among labels and computes the multi label entropy in a decomposable way mlent t represents the fraction of examples in t with label y j note that eq can be regarded as a simplified version of eq under the label independence assumption and it holds that mlent t mlent t for unseen instance x it is fed to the learned decision tree by traversing along the paths until reaching a leaf node affiliated with a number of training examples t d then the predicted label set corresponds to y y j p j j q in other words if for one leaf node the majority of train ing examples falling into it have label y j any test instance allocated within the same leaf node will regard y j as its relevant label remarks the pseudo code of ml dt is summarized in y i i n fig it is a first order approach which assumes label inde with n examples the information gain achieved by dividing pendence in calculating multi label entropy one promi t along the l th feature at splitting value θ is nent advantage of ml dt lies in its high efficiency in ig t l θ mlent t ρ t ρ t inducing the decision tree model from multi label data possible improvements on multi label decision trees include mlent t ρ employing pruning strategy or ensemble learning tech where t x i y i x il θ i n niques as shown in fig ml dt has compu tational complexity of o mdq for training and o mq for t x i y i x il θ i n testing namely t t consists of examples l th feature less greater than θ with values on the ranking support vector machine rank svm starting from the root node i e t d ml dt identi the basic idea of this algorithm is to adapt maximum mar fies the feature and the corresponding splitting value which gin strategy to deal with multi label data where a set of maximizes the information gain in eq and then gener linear classifiers are optimized to minimize the empirical ates two child nodes with respect to t and t the above ranking loss and enabled to handle nonlinear cases with process is invoked recursively by treating either t or t kernel tricks as the new root node and terminates until some stopping let the learning system be composed of q linear clas without loss of generality here we assume that features are real sifiers w w j valued and the data set is bi partitioned by setting splitting point along each feature similar to eq information gain with respect to discrete valued features can be defined as well rd and b j b j j q where w j r are the weight vector and bias for the j th class label y j correspondingly rank svm defines the learning system margin on x i y i by considering its ranking ability on the ieee transactions on knowledge and data engineering vol no august fig pseudo code of ml dt example relevant and irrelevant labels min is the set of slack variables the objective in eq consists of two parts when the learning system is capable of properly ranking balanced by the trade off parameter c specifically the first every relevant irrelevant label pair for each training exam part corresponds to the margin of the learning system while ple eq will return positive margin in this ideal case the second parts corresponds to the surrogate ranking loss of we can rescale i m b i and thereafter the problem of maximizing the margin in eq can be expressed as the learning system implemented in hinge form note that surrogate ranking loss can be implemented in other ways such as the exponential form for neural network global error function note that eq is a standard quadratic programming qp problem with convex objective and linear constraints max w which can be tackled with any off the shelf qp solver furthermore to endow rank svm with nonlinear classifi subject to w cation ability one popular way is to solve eq in its dual i m y form via kernel trick more details on the dual formulation can be found in suppose we have sufficient training examples such that as discussed in subsection rank svm employs for each label pair satisfying the margin over hyperplanes for relevant irrelevant label pairs rank svm benefits from kernels to handle non to overcome the difficulty brought by the max operator linear classification problems and further variants can rank svm chooses to simplify eq by approximating be achieved firstly as shown in the empirical zhang and zhou review on multi label learning algorithms fig pseudo code of rank svm ranking loss considered in eq can be replaced with other loss structures such as hamming loss which can be cast as a general form of structured output classifica tion secondly the thresholding strategy can be accomplished with techniques other than stacking style procedure thirdly to avoid the problem of ker nel selection multiple kernel learning techniques can be employed to learn from multi label data as shown in fig let f qp log posterior probability function a b represent the time complex ity for a qp solver to solve eq with a variables and b constraints rank svm has computational complexity of o f qp dq q m for training and o dq for testing note that eq is a convex function over whose global maximum though not in closed form can be found by any off the shelf unconstrained opti mization method such as bfgs generally gradi ents of l d are needed by most numerical collective multi label classifier cml the basic idea of this algorithm is to adapt maximum entropy principle to deal with multi label data where correlations methods l d λ k among labels are encoded as constraints that the resulting distribution must satisfy for any multi label example x y let x y be the corre sponding random variables representation using binary label vector be specified in other ways x y represent the information entropy of x y yielding variants of cml 114 given their distribution p the principle of maximum for unseen instance x the predicted label set corre entropy assumes that the distribution best modeling the sponds to current state of knowledge is the one maximizing h p x y subject to a collection k of given facts y arg max y max p p y x x y note that exact inference with arg max is only tractable for subject to e p h p f k x y f k k k small label space otherwise pruning strategies need to be applied to significantly reduce the search space of arg max generally the fact is expressed as constraint on the expectation of some function over x y i e by imposing e p f k x y f k here e p is the expectation operator with respect to p while f k corresponds value estimated from training set e g m e g only considering label sets appearing in the training set remarks the pseudo code of cml is summarized in to the expected together with the normalization constraint x y d on fig it is a second order approach where correlations f k x y p i e between in k every label pair are considered via constraints e p the second order correlation studied by cml is the constrained optimization problem of eq can be carried out with standard lagrange multiplier tech niques accordingly the optimal solution is shown to fall within the gibbs distribution family p y x k k more general than that of rank svm as the latter only considers relevant irrelevant label pairs as a conditional random field crf model cml is interested in using the conditional probability distribution p y x in eq for classification interestingly p y x can be factored in vari ous ways such as p y x k k is the set of parameters to be deter mined and z x is the partition function serving as the nor each in q term in the product can j be the classifier p y j x pa by node j where y j each term in the product can be modeled by assuming gaussian prior i e λ k n param and its and efficient parents algorithms pa j in a directed graph exist when the directed eters in can be found by maximizing the following graph corresponds to multi dimensional bayesian network ieee transactions on knowledge and data engineering vol no august fig pseudo code of cml table summary of representative multi label learning algorithms being reviewed with restricted topology directed graphs are also found to be useful for modeling multiple fault diagno sis where y j including their basic idea label correlations computational complexity tested domains and optimized surrogate met indicates the good failing condition of one ric as shown in table surrogate hamming loss and of the device components on the other hand ranking loss are among the most popular metrics to be opti there have been some multi label generative models which mized and theoretical analyses on them have aim to model the joint probability distribution p x y been discussed in subsection furthermore note that as shown in fig let f unc a m represent the subset accuracy optimized by random k labelsets is only the time complexity for an unconstrained optimization measured with respect to the k labelset instead of the whole method to solve eq with a variables cml has com label space putational domains reported in table correspond to data types and on which the corresponding algorithm is shown to work well in the original literature however all those repre sentative multi label learning algorithms are general purpose and can be applied to various data types nevertheless the computational complexity of each learning algorithm complexity o dq of o f unc dq m for testing for training summary table summarizes properties of the eight multi label learning algorithms investigated in subsections and zhang and zhou review on multi label learning algorithms table online resources for multi label learning for does play a key factor on its suitability for different scales of data here the data scalability can be stud ied in terms of three main aspects including the num ber of training examples i e m the dimensionality i e d and the number of possible class labels i e q furthermore algorithms which append class labels as extra features to instance space 106 might not ben efit too much from this strategy when instance dimen sionality is much larger than the number of class labels i e d q as arguably the mostly studied supervised learning framework several algorithms in table employ binary classification as the intermediate step to learn from multi label data an initial and general attempt towards binary classification transformation comes from the famous adaboost mh algorithm where each multi label training example x i to accommodate all class labels based on multi label entropy related learning settings there are several learning settings related to multi label learning which are worth some discussion such as multi instance learning ordinal classification multi task learning and data streams classification multi instance learning studies the problem where each example is described by a bag of instances while asso ciated with a single binary label a bag is regarded to be positive iff at least one of its constituent instances is positive in contrast to multi label learning which models the object ambiguities complicated semantics in out put label space multi instance learning can be viewed onverted as modeling the object ambiguities in input instance space there are some initial attempt towards exploit ing multi instance representation for learning from multi label data ordinal classification studies the problem where a natural ordering exists among all the class labels in multi label learning we can accordingly assume an order ing of relevance on each class label to generalize the crisp membership y j into q binary j q it can be regarded as a high order approach where labels in y are treated as appending feature to x and would be related to each other via the shared instance x as far as the binary learning algorithm b is capable of capturing dependencies among features other ways towards binary classifica tion transformation can be fulfilled with techniques such as stacked aggregation or error correcting output codes ecoc in addition first order algorithm adaptation methods can not be simply regarded as binary relevance com bined with specific binary learners for example ml knn is more than binary relevance combined with knn as bayesian inference is employed to reason with neighboring information and ml dt is more than binary relevance combined with decision tree as a single decision tree instead of q decision trees is built therefore graded multi label learning accommodates the case where we can only provide vague ordinal instead of definite judgement on the label relevance existing work shows that graded multi label learning can be solved by transforming it into a set of ordinal classification prob lems one for each class label or a set of standard multi label learning problems one for each membership level ieee transactions on knowledge and data engineering vol no august multi task learning studies the problem where multiple tasks are trained in parallel such that training information of related tasks are used as an inductive bias to help improve the generalization performance of other tasks nonetheless there are some essential differences between multi task learning and multi label learning to be noticed firstly in multi label learning all the examples share the same feature space while in multi task learning the tasks can be in the same feature space or different feature spaces secondly in multi label learning the goal is to predict the label subset associated with an object while the purpose of multi task learning is to have multiple tasks to be learned well simultaneously and it does not concern on which task subset should be associated with an object if we take a label as a task since it generally assumes that every object is involved by all tasks thirdly in multi label learning it is not rare yet demanding to deal with large label space while in multi task learning it is not reasonable to con sider a large number of tasks nevertheless techniques for multi task learning might be used to benefit multi label learning data streams classification studies the problem where real world objects are generated online and pro cessed in a real time manner nowadays streaming data with multi label nature widely exist in real world scenar ios such as instant news emails microblogs etc as a usual challenge for streaming data analysis the key fac tor for effectively classifying multi label data streams is how to deal with the concept drift problem existing works model concept drift by updating the classifiers significantly whenever a new batch of examples arrive taking the fading assumption that the influence of past data grad ually declines as time evolves or maintaining a change detector alerting whenever a concept drift is detected conclusion in this paper the state of the art of multi label learning is reviewed in terms of paradigm formalization learn ing algorithms and related learning settings in particular instead of trying to go through all the learning tech niques within confined space which would lead to only abridged introductions we choose to elaborate the algo rithmic details of eight representative multi label learn ing algorithms with references to other related works some online resources for multi label learning are sum marized in table including academic activities tutorial workshops special issue publicly available software and data sets as discussed in section although the idea of exploiting label correlations have been employed by var ious multi label learning techniques there has not been any formal characterization on the underlying concept or any principled mechanism on the appropriate usage of label correlations recent researches indicate that correla tions among labels might be asymmetric i e the influence of one label to the other one is not necessarily be the same in the inverse direction or local i e different instances share different label correlations with few correlations being globally applicable nevertheless full understanding on label correlations especially for scenarios with large output space would remain as the holy grail for multi label learning as reviewed in section multi label learning algo rithms are introduced by focusing on their algorith mic properties one natural complement to this review would be conducting thorough experimental studies to get insights on the pros and cons of different multi label learning algorithms a recent attempt towards extensive experimental comparison can be found in where multi label learning algorithms are compared with respect to evaluation metrics interestingly while not surprisingly the best performing algorithm for both clas sification and ranking metrics turns out to be the one based on ensemble learning techniques i e random forest of predictive decision trees nevertheless empiri cal comparison across a broad range or within a focused type e g are worthwhile topic to be further explored shark is a new data analysis system that marries query processing with complex analytics on large clusters it leverages a novel dis tributed memory abstraction to provide a unified engine that can run sql queries and sophisticated analytics functions e g iterative machine learning at scale and efficiently recovers from failures mid query this allows shark to run sql queries up to faster than apache hive and machine learning programs more than faster than hadoop unlike previous systems shark shows that it is possible to achieve these speedups while retaining a mapreduce like execution engine and the fine grained fault tolerance proper ties that such engine provides it extends such an engine in sev eral ways including column oriented in memory storage and dy namic mid query replanning to effectively execute sql the result is a system that matches the speedups reported for mpp analytic databases over mapreduce while offering fault tolerance proper ties and complex analytics capabilities that they lack categories and subject descriptors h database management systems keywords databases data warehouse machine learning spark shark hadoop introduction modern data analysis faces a confluence of growing challenges first data volumes are expanding dramatically creating the need to scale out across clusters of hundreds of commodity machines second such high scale increases the incidence of faults and strag glers slow tasks complicating parallel database design third the complexity of data analysis has also grown modern data analysis employs sophisticated statistical methods such as machine learn ing algorithms that go well beyond the roll up and drill down ca pabilities of traditional enterprise data warehouse systems finally despite these increases in scale and complexity users still expect to be able to query data at interactive speeds to tackle the big data problem two major lines of systems have recently been explored the first consisting of mapreduce permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee sigmod june new york new york usa copyright acm and various generalizations offers a fine grained fault tol erance model suitable for large clusters where tasks on failed or slow nodes can be deterministically re executed on other nodes mapreduce is also fairly general it has been shown to be able to express many statistical and learning algorithms it also easily supports unstructured data and schema on read however mapreduce engines lack many of the features that make databases efficient and thus exhibit high latencies of tens of seconds to hours even systems that have significantly optimized mapreduce for sql queries such as google tenzing or that combine it with a traditional database on each node such as hadoopdb report a minimum latency of seconds as such mapreduce approaches have largely been dismissed for interactive speed queries and even google is developing new engines for such workloads instead most mpp analytic databases e g vertica greenplum teradata and several of the new low latency engines proposed for mapreduce environments e g google dremel cloudera im pala employ a coarser grained recovery model where an entire query has to be resubmitted if a machine fails this works well for short queries where a retry is inexpensive but faces significant challenges for long queries as clusters scale up in addition these systems often lack the rich analytics functions that are easy to implement in mapreduce such as machine learning and graph algorithms furthermore while it may be possible to implement some of these functions using udfs these algorithms are often ex pensive exacerbating the need for fault and straggler recovery for long queries thus most organizations tend to use other systems alongside mpp databases to perform complex analytics to provide an effective environment for big data analysis we believe that processing systems will need to support both sql and complex analytics efficiently and to provide fine grained fault re covery across both types of operations this paper describes a new system that meets these goals called shark shark is open source and compatible with apache hive and has already been used at web companies to speed up queries by shark builds on a recently proposed distributed shared memory abstraction called resilient distributed datasets rdds to perform most computations in memory while offering fine grained fault tolerance in memory computing is increasingly important in large scale analytics for two reasons first many complex analyt ics functions such as machine learning and graph algorithms are iterative scanning the data multiple times thus the fastest sys tems deployed for these applications are in memory second even traditional sql warehouse workloads exhibit strong temporal and spatial locality because more recent fact table data provides fault tolerance within a query but dremel is lim ited to aggregation trees instead of the more complex communica tion patterns in joins and small dimension tables are read disproportionately often a study of facebook hive warehouse and microsoft bing analyt ics cluster showed that over of queries in both systems could be served out of memory using just gb node as a cache even though each system manages more than pb of total data the main benefit of rdds is an efficient mechanism for fault recovery traditional main memory databases support fine grained updates to tables and replicate writes across the network for fault tolerance which is expensive on large commodity clusters in con trast rdds restrict the programming interface to coarse grained deterministic operators that affect multiple data items at once such asmap group by and join and recover from failures by tracking the lineage of each dataset and recomputing lost data this approach works well for data parallel relational queries and has also been shown to support machine learning and graph computation thus when a node fails shark can recover mid query by rerun ning the deterministic operations used to build lost data partitions on other nodes similar to mapreduce indeed it typically recovers within seconds by parallelizing this work across the cluster to run sql efficiently however we also had to extend the rdd execution model bringing in several concepts from traditional an alytical databases and some new ones we started with an exist ing implementation of rdds called spark and added several features first to store and process relational data efficiently we implemented in memory columnar storage and columnar compres sion this reduced both the data size and the processing time by as much as over naïvely storing the data in a spark program in its original format second to optimize sql queries based on the data characteristics even in the presence of analytics functions and udfs we extended spark with partial dag execution pde shark can reoptimize a running query after running the first few stages of its task dag choosing better join strategies or the right degree of parallelism based on observed statistics third we lever age other properties of the spark engine not present in traditional mapreduce systems such as control over data partitioning our implementation of shark is compatible with apache hive supporting all of hive sql dialect and udfs and allowing execution over unmodified hive data warehouses it augments sql with complex analytics functions written in spark using spark java scala or python apis these functions can be combined with sql in a single execution plan providing in memory data sharing and fast recovery across both types of processing experiments show that using rdds and the optimizations above shark can answer sql queries up to faster than hive runs iterative machine learning algorithms more than faster than hadoop and can recover from failures mid query within seconds shark speed is comparable to that of mpp databases in bench marks like pavlo et al comparison with mapreduce but it offers fine grained recovery and complex analytics features that these systems lack more fundamentally our work shows that mapreduce like exe cution models can be applied effectively to sql and offer a promis ing way to combine relational and complex analytics in addi tion we explore why current sql engines implemented on top of mapreduce runtimes such as hive are slow we show how a combination of enhancements in shark e g pde and engine properties that have not been optimized in mapreduce such as the overhead of launching tasks eliminate many of the bottlenecks in traditional mapreduce systems system overview as described in the previous section shark is a data analysis sys tem that supports both sql query processing and machine learning hdfs namenode figure shark architecture functions shark is compatible with apache hive enabling users to run hive queries much faster without any changes to either the queries or the data thanks to its hive compatibility shark can query data in any system that supports the hadoop storage api including hdfs and amazon it also supports a wide range of data formats such as text binary sequence files json and xml it inherits hive schema on read capability and nested data types in addition users can choose to load high value data into shark memory store for fast analytics as illustrated below create table tblproperties shark cache true as select from logs where date now figure shows the architecture of a shark cluster consisting of a single master node and a number of worker nodes with the ware house metadata stored in an external transactional database it is built on top of spark a modern mapreduce like cluster computing engine when a query is submitted to the master shark compiles the query into operator tree represented as rdds as we shall dis cuss in section these rdds are then translated by spark into a graph of tasks to execute on the worker nodes cluster resources can optionally be allocated by a resource man ager e g hadoop yarn or apache mesos that provides resource sharing and isolation between different computing frame works allowing shark to coexist with engines like hadoop in the remainder of this section we cover the basics of spark and the rdd programming model and then we describe how shark query plans are generated and executed spark spark is the mapreduce like cluster computing engine used by shark spark has several features that differentiate it from tradi tional mapreduce engines like dryad and hyracks it supports general com putation dags not just the two stage mapreduce topology it provides an in memory storage abstraction called resilient distributed datasets rdds that lets applications keep data in memory across queries and automatically reconstructs any data lost during failures the engine is optimized for low latency it can efficiently manage tasks as short as milliseconds on clusters of thousands of cores while engines like hadoop incur a la tency of seconds to launch each task metastore system catalog master process master node resource manager scheduler worker node spark runtime execution engine memstore resource manager daemon hdfs datanode worker node spark runtime execution engine memstore resource manager daemon hdfs datanode figure lineage graph for the rdds in our spark example oblongs represent rdds while circles show partitions within a dataset lineage is tracked at the granularity of partitions rdds are unique to spark and were essential to enabling mid query fault tolerance however the other differences are important engineering elements that contribute to shark performance in addition to these features we have also modified the spark engine for shark to support partial dag execution that is modi fication of the query plan dag after only some of the stages have finished based on statistics collected from these stages similar to we use this technique to optimize join algorithms and other as pects of the execution mid query as we shall discuss in section resilient distributed datasets rdds spark main abstraction is resilient distributed datasets rdds which are immutable partitioned collections that can be created through various data parallel operators e g map group by hash join each rdd is either a collection stored in an external storage system such as a file in hdfs or a derived dataset created by applying operators to other rdds for example given an rdd of visitid url pairs for visits to a website we might compute an rdd of url count pairs by applying a map operator to turn each event into an url pair and then a reduce to add the counts by url in spark native api rdd operations are invoked through a functional interface similar to dryadlinq in scala java or python for example the scala code for the query above is val visits spark hadoopfile hdfs val counts visits map v v url reducebykey a b a b rdds can contain arbitrary data types as elements since spark runs on the jvm these elements are java objects and are au tomatically partitioned across the cluster but they are immutable once created and they can only be created through spark deter ministic parallel operators these two restrictions however enable highly efficient fault recovery in particular instead of replicating each rdd across nodes for fault tolerance spark remembers the lineage of the rdd the graph of operators used to build it and recovers lost partitions by recomputing them from base data for example figure shows the lineage graph for the rdds com puted above if spark loses one of the partitions in the url rdd for example it can recompute it by rerunning the map on just the corresponding partition of the input file the rdd model offers several key benefits in our large scale in memory computing setting first rdds can be written at the speed of dram instead of the speed of the network because there is no assume that external files for rdds representing data do not change or that we can take a snapshot of a file when we create an rdd from it provide fault tolerance across shuffle operations like a par allel reduce the execution engine also saves the map side of the shuffle in memory on the source nodes spilling to disk if necessary for master recovery could also be added by reliabliy log ging the rdd lineage graph and the submitted jobs because this state is small but we have not implemented this yet need to replicate each byte written to another machine for fault tolerance dram in a modern server is over faster than even a gigabit network second spark can keep just one copy of each rdd partition in memory saving precious memory over a repli cated system since it can always recover lost data using lineage third when a node fails its lost rdd partitions can be rebuilt in parallel across the other nodes allowing speedy recovery fourth even if a node is just slow a straggler we can recompute nec essary partitions on other nodes because rdds are immutable so there are no consistency concerns with having two copies of a par tition these benefits make rdds attractive as the foundation for our relational processing in shark fault tolerance guarantees to summarize the benefits of rdds shark provides the following fault tolerance properties which have been difficult to support in traditional mpp database designs shark can tolerate the loss of any set of worker nodes the execution engine will re execute any lost tasks and recom pute any lost rdd partitions using lineage this is true even within a query spark will rerun any failed tasks or lost dependencies of new tasks without aborting the query recovery is parallelized across the cluster if a failed node contained rdd partitions these can be rebuilt in parallel on different nodes quickly recovering the lost data the deterministic nature of rdds also enables straggler mit igation if a task is slow the system can launch a speculative backup copy of it on another node as in mapreduce recovery works even for queries that combine sql and ma chine learning udfs section as these operations all com pile into a single rdd lineage graph executing sql over rdds shark runs sql queries over spark using a three step process sim ilar to traditional rdbmss query parsing logical plan generation and physical plan generation given a query shark uses the hive query compiler to parse the query and generate an abstract syntax tree the tree is then turned into a logical plan and basic logical optimization such as predi cate pushdown is applied up to this point shark and hive share an identical approach hive would then convert the operator into a physical plan consisting of multiple mapreduce stages in the case of shark its optimizer applies additional rule based optimizations such as pushing limit down to individual partitions and creates a physical plan consisting of transformations on rdds rather than mapreduce jobs we use a variety of operators already present in spark such as map and reduce as well as new operators we imple mented for shark such as broadcast joins spark master then exe cutes this graph using standard mapreduce scheduling techniques such as placing tasks close to their input data rerunning lost tasks and performing straggler mitigation while this basic approach makes it possible to run sql over spark doing it efficiently is challenging the prevalence of udfs and complex analytic functions in shark workload makes it diffi cult to determine an optimal query plan at compile time especially for new data that has not undergone etl in addition even with such a plan naïvely executing it over spark or other mapreduce runtimes can be inefficient in the next section we discuss sev eral extensions we made to spark to efficiently store relational data and run sql starting with a mechanism that allows for dynamic statistics driven re optimization at run time engine extensions in this section we describe our modifications to the spark engine to enable efficient execution of sql queries partial dag execution pde systems like shark and hive are frequently used to query fresh data that has not undergone a data loading process this precludes the use of static query optimization techniques that rely on accurate a priori data statistics such as statistics maintained by indices the lack of statistics for fresh data combined with the prevalent use of udfs requires dynamic approaches to query optimization to support dynamic query optimization in a distributed setting we extended spark to support partial dag execution pde a tech nique that allows dynamic alteration of query plans based on data statistics collected at run time we currently apply partial dag execution at blocking shuf fle operator boundaries where data is exchanged and repartitioned since these are typically the most expensive operations in shark by default spark materializes the output of each map task in memory before a shuffle spilling it to disk as necessary later reduce tasks fetch this output pde modifies this mechanism in two ways first it gathers cus tomizable statistics at global and per partition granularities while materializing map outputs second it allows the dag to be altered based on these statistics either by choosing different operators or altering their parameters such as their degrees of parallelism these statistics are customizable using a simple pluggable ac cumulator api some example statistics include partition sizes and record counts which can be used to detect skew lists of heavy hitters i e items that occur frequently in the dataset approximate histograms which can be used to estimate par titions data distributions these statistics are sent by each worker to the master where they are aggregated and presented to the optimizer for efficiency we use lossy compression to record the statistics limiting their size to kb per task for instance we encode partition sizes in bytes with logarithmic encoding which can represent sizes of up to gb using only one byte with at most error the master can then use these statistics to perform various run time optimizations as we shall discuss next partial dag execution complements existing adaptive query op timization techniques that typically run in a single node system as we can use existing techniques to dynamically optimize the local plan within each node and use pde to optimize the global structure of the plan at stage boundaries this fine grained statis tics collection and the optimizations that it enables differentiates pde from graph rewriting features in previous systems such as dryadlinq join optimization partial dag execution can be used to perform several run time op timizations for join queries figure illustrates two communication patterns for mapreduce style joins in shuffle join both join tables are hash partitioned by table table stage join result map join join result figure data flows for map join and shuffle join map join broadcasts the small table to all large table partitions while shuffle join repartitions and shuffles both tables the join key each reducer joins corresponding partitions using a local join algorithm which is chosen by each reducer based on run time statistics if one of a reducer input partitions is small then it constructs a hash table over the small partition and probes it using the large partition if both partitions are large then a symmetric hash join is performed by constructing hash tables over both inputs in map join also known as broadcast join a small input table is broadcast to all nodes where it is joined with each partition of a large table this approach can result in significant cost savings by avoiding an expensive repartitioning and shuffling phase map join is only worthwhile if some join inputs are small so shark uses partial dag execution to select the join strategy at run time based on its inputs exact sizes by using sizes of the join inputs gathered at run time this approach works well even with in put tables that have no prior statistics such as intermediate results run time statistics also inform the join tasks scheduling poli cies if the optimizer has a prior belief that a particular join input will be small it will schedule that task before other join inputs and decide to perform a map join if it observes that the task output is small this allows the query engine to avoid performing the pre shuffle partitioning of a large table once the optimizer has decided to perform a map join skew handling and degree of parallelism partial dag execution can also be used to determine operators degrees of parallelism and to mitigate skew the degree of parallelism for reduce tasks can have a large per formance impact launching too few reducers may overload re ducers network connections and exhaust their memories while launching too many may prolong the job due to task scheduling overhead hive performance is especially sensitive to the number of reduce tasks due to hadoop large scheduling overhead using partial dag execution shark can use individual parti tions sizes to determine the number of reducers at run time by co alescing many small fine grained partitions into fewer coarse par titions that are used by reduce tasks to mitigate skew fine grained partitions are assigned to coalesced partitions using a greedy bin packing heuristic that attempts to equalize coalesced partitions sizes this offers performance benefits especially when good bin packings exist somewhat surprisingly we discovered that shark can obtain sim ilar performance improvement simply by running a larger number of reduce tasks we attribute this to spark low scheduling and task launching overhead columnar memory store in memory computation is essential to low latency query answer ing given that memory throughput is orders of magnitude higher stage shuffle join than that of disks naïvely using spark memory store however can lead to undesirable performance for this reason shark imple ments a columnar memory store on top of spark native memory store in memory data representation affects both space footprint and read throughput a naïve approach is to simply cache the on disk data in its native format performing on demand deserialization in the query processor this deserialization becomes a major bottle neck in our studies we saw that modern commodity cpus can deserialize at a rate of only per second per core the approach taken by spark default memory store is to store data partitions as collections of jvm objects this avoids deserial ization since the query processor can directly use these objects but leads to significant storage space overheads common jvm imple mentations add to bytes of overhead per object for example storing mb of tpc h lineitem table as jvm objects uses ap proximately mb of memory while a serialized representation requires only mb nearly three times less space a more seri ous implication however is the effect on garbage collection gc with a b record size a gb heap can contain million ob jects the jvm garbage collection time correlates linearly with the number of objects in the heap so it could take minutes to perform a full gc on a large heap these unpredictable expensive garbage collections cause large variability in response times shark stores all columns of primitive types as jvm primitive arrays complex data types supported by hive such as map and array are serialized and concatenated into a single byte array each column creates only one jvm object leading to fast gcs and a compact data representation the space footprint of columnar data can be further reduced by cheap compression techniques at vir tually no cpu cost similar to columnar database systems e g c store shark implements cpu efficient compression schemes such as dictionary encoding run length encoding and bit packing columnar data representation also leads to better cache behavior especially for for analytical queries that frequently compute aggre gations on certain columns distributed data loading in addition to query execution shark also uses spark execution engine for distributed data loading during loading a table is split into small partitions each of which is loaded by a spark task the loading tasks use the data schema to extract individual fields from rows marshal a partition of data into its columnar representation and store those columns in memory each data loading task tracks metadata to decide whether each column in a partition should be compressed for example the loading task will compress a column using dictionary encoding if its number of distinct values is below a threshold this allows each task to choose the best compression scheme for each partition rather than conforming to a global compression scheme that might not be optimal for local partitions these local decisions do not require coordination among data loading tasks allowing the load phase to achieve a maximum degree of parallelism at the small cost of requiring each partition to maintain its own compression meta data it is important to clarify that an rdd lineage does not need to contain the compression scheme and metadata for each parti tion the compression scheme and metadata are simply byproducts of the rdd computation and can be deterministically recomputed along with the in memory data in the case of failures as a result shark can load data into memory at the aggregated throughput of the cpus processing incoming data pavlo et al showed that hadoop was able to perform data loading at to times the throughput of mpp databases tested using the same dataset used in shark provides the same through put as hadoop in loading data into hdfs shark is times faster than hadoop when loading data into its memory store data co partitioning in some warehouse workloads two tables are frequently joined to gether for example the tpc h benchmark frequently joins the lineitem and order tables a technique commonly used by mpp databases is to co partition the two tables based on their join key in the data loading process in distributed file systems like hdfs the storage system is schema agnostic which prevents data co partitioning shark allows co partitioning two tables on a com mon key for faster joins in subsequent queries this can be ac complished with the distribute by clause create table tblproperties shark cache true as select from lineitem distribute by create table tblproperties shark cache true copartition as select from order distribute by when joining two co partitioned tables shark optimizer con structs a dag that avoids the expensive shuffle and instead uses map tasks to perform the join partition statistics and map pruning typically data is stored using some logical clustering on one or more columns for example entries in a website traffic log data might be grouped by users physical locations because logs are first stored in data centers that have the best geographical proximity to users within each data center logs are append only and are stored in roughly chronological order as a less obvious case a news site logs might contain and timestamp columns that are strongly correlated for analytical queries it is typical to apply filter predicates or aggregations over such columns for example a daily warehouse report might describe how different visitor seg ments interact with the website this type of query naturally ap plies a predicate on timestamps and performs aggregations that are grouped by geographical location this pattern is even more fre quent for interactive data analysis during which drill down opera tions are frequently performed map pruning is the process of pruning data partitions based on their natural clustering columns since shark memory store splits data into small partitions each block contains only one or few log ical groups on such columns and shark can avoid scanning certain blocks of data if their values fall out of the query filter range to take advantage of these natural clusterings of columns shark memory store on each worker piggybacks the data loading process to collect statistics the information collected for each partition in cludes the range of each column and the distinct values if the num ber of distinct values is small i e enum columns the collected statistics are sent back to the master program and kept in memory for pruning partitions during query execution when a query is issued shark evaluates the query predicates against all partition statistics partitions that do not satisfy the pred icate are pruned and shark does not launch tasks to scan them we collected a sample of queries from the hive warehouse of a video analytics company and out of the queries we obtained at least of them contained predicates that shark can use for map pruning section provides more details on this workload machine learning support a key design goal of shark is to provide a single system capable of efficient sql query processing and sophisticated machine learn ing following the principle of pushing computation to data shark def logregress points rdd point vector var w vector d rand nextdouble for i to iterations val gradient points map p val denom exp p y w dot p x denom p y p x reduce w gradient w val users select from user u join comment c on c uid u uid val features users maprows row new vector row getint age row getstr country val trainedvector logregress features cache note that this distributed logistic regression implementation in shark looks remarkably similar to a program implemented for a single node in the scala language the user can conveniently mix the best parts of both sql and mapreduce style programming currently shark provides native support for scala java and python we have modified the scala shell to enable interactive execution of both sql and distributed machine learning algorithms because shark is built on top of the jvm it would be relatively straightfor ward to support other jvm languages such as clojure or jruby we have implemented a number of basic machine learning al gorithms including linear regression logistic regression and k means clustering in most cases the user only needs to supply a maprows function to perform feature extraction and can invoke the provided algorithms the above example demonstrates how machine learning compu tations can be performed on query results using rdds as the main data structure for query operators also enables one to use sql to query the results of machine learning computations in a single exe listing logistic regression example cution plan execution engine integration in addition to language integration another key benefit of using supports machine learning as a first class citizen this is enabled by the design decision to choose spark as the execution engine and rdd as the main data structure for operators in this section we explain shark language and execution engine integration for sql and machine learning other research projects have demonstrated that it is pos sible to express certain machine learning algorithms in sql and avoid moving data out of the database the implementation of those projects however involves a combination of sql udfs rdds as the data structure for operators is the execution engine in tegration this common abstraction allows machine learning com putations and sql queries to share workers and cached data with out the overhead of data movement because sql query processing is implemented using rdds lin eage is kept for the whole pipeline which enables end to end fault tolerance for the entire workflow if failures occur during the ma chine learning stage partitions on faulty nodes will automatically be recomputed based on their lineage and driver programs written in other languages the systems be come obscure and difficult to maintain in addition they may sacri fice performance by performing expensive parallel numerical com putations on traditional database engines that were not designed for such workloads contrast this with the approach taken by shark which offers in database analytics that push computation to data implementation while implementing shark we discovered that a number of engi neering details had significant performance impacts overall to improve the query processing speed one should minimize the tail latency of tasks and the cpu cost of processing each row but does so using a runtime that is optimized for such workloads memory based shuffle both spark and hadoop write map out and a programming model that is designed to express machine learn put files to disk hoping that they will remain in the os buffer cache ing algorithms when reduce tasks fetch them in practice we have found that the language integration extra system calls and file system journaling adds significant over head in addition the inability to control when buffer caches are in addition to executing a sql query and returning its results shark flushed leads to variability in shuffle tasks a query response time also allows queries to return the rdd representing the query plan is determined by the last task to finish and thus the increasing vari callers to shark can then invoke distributed computation over the ability leads to long tail latency which significantly hurts shuffle query result using the returned rdd performance we modified the shuffle phase to materialize map as an example of this integration listing illustrates a data outputs in memory with the option to spill them to disk analysis pipeline that performs logistic regression over a user database logistic regression a common classification algorithm searches for a hyperplane w that best separates two sets of points e g spam mers and non spammers the algorithm applies gradient descent optimization by starting with a randomized w vector and iteratively updating it by moving along gradients towards an optimum value the program begins by using to issue a sql query to retreive user information as a tablerdd it then performs feature extraction on the query rows and runs logistic regression over the extracted feature matrix each iteration of logregress applies a function of w to all data points to produce a set of gradients which are summed to produce a net gradient that is used to update w the highlighted map maprows and reduce functions are au tomatically parallelized by shark to execute across a cluster and temporary object creation it is easy to write a program that creates many temporary objects which can burden the jvm garbage collector for a parallel job a slow gc at one task may slow the entire job shark operators and rdd transformations are written in a way that minimizes temporary object creations bytecode compilation of expression evaluators in its current implementation shark sends the expression evaluators generated by the hive parser as part of the tasks to be executed on each row by profiling shark we discovered that for certain queries when data is served out of the memory store the majority of the cpu cy cles are wasted in interpreting these evaluators we are working on a compiler to transform these expression evaluators into jvm byte code which can further increase the execution engine throughput the master program simply collects the output of the reduce func specialized data structures using specialized data structures is tion to update w an optimization that we have yet to exploit for example java hash table is built for generic objects when the hash key is a prim itive type the use of specialized data structures can lead to more compact data representations and thus better cache behavior experiments we evaluated shark using four datasets pavlo et al benchmark tb of data reproducing pavlo et al comparison of mapreduce vs analytical dbmss tpc h dataset gb and tb datasets generated by the dbgen program real hive warehouse tb of sampled hive warehouse data from an early industrial user of shark machine learning dataset gb synthetic dataset to mea sure the performance of machine learning algorithms overall our results show that shark can perform more than faster than hive and hadoop even though we have yet to imple ment some of the performance optimizations mentioned in the pre vious section in particular shark provides comparable perfor mance gains to those reported for mpp databases in pavlo et al comparison in some cases where data fits in memory shark exceeds the performance reported for mpp databases we emphasize that we are not claiming that shark is funda mentally faster than mpp databases there is no reason why mpp engines could not implement the same processing optimizations as shark indeed our implementation has several disadvantages relative to commercial engines such as running on the jvm in stead we aim to show that it is possible to achieve comparable per formance while retaining a mapreduce like engine and the fine grained fault recovery features that such engines provide in addi tion shark can leverage this engine to perform machine learning functions on the same data which we believe will be essential for future analytics workloads methodology and cluster setup unless otherwise specified experiments were conducted on ama zon using nodes each node had virtual cores gb of memory and tb of local storage the cluster was running bit linux apache hadoop and apache hive for hadoop mapreduce the num ber of map tasks and the number of reduce tasks per node were set to matching the number of cores for hive we enabled jvm reuse between tasks and avoided merging small output files which would take an extra step after each query to perform the merge we executed each query six times discarded the first run and report the average of the remaining five runs we discard the first run in order to allow the jvm just in time compiler to optimize common code paths we believe that this more closely mirrors real world deployments where the jvm will be reused by many queries pavlo et al benchmarks pavlo et al compared hadoop versus mpp databases and showed that hadoop excelled at data ingress but performed unfavorably in query execution we reused the dataset and queries from their benchmarks to compare shark against hive the benchmark used two tables a gb node rankings table and a gb node uservisits table for our node cluster we recreated a gb rankings table containing billion rows and a tb uservisits table containing billion rows we ran the four queries in their experiments comparing shark with hive and report the results in figures and in this subsection we hand tuned hive number of reduce tasks to produce optimal results for shark shark disk hive selection d n o c e e m i t aggregation aggregation groueps groueps figure selection and aggregation query runtimes seconds from pavlo et al benchmark hive figure join query runtime seconds from pavlo benchmark hive despite this tuning shark outperformed hive in all cases by a wide margin selection query the first query was a simple selection on the rankings table select pageurl pagerank from rankings where pagerank x in vertica outperformed hadoop by a factor of because a clustered index was created for vertica even without a clustered index shark was able to execute this query faster than hive for in memory data and on data read from hdfs aggregation queries the pavlo et al benchmark ran two aggregation queries select sourceip sum adrevenue from uservisits group by sourceip select substr sourceip sum adrevenue from uservisits group by substr sourceip in our dataset the first query had two million groups and the sec ond had approximately one thousand groups shark and hive both applied task local aggregations and shuffled the data to parallelize the final merge aggregation again shark outperformed hive by a wide margin the benchmarked mpp databases perform local ag gregations on each node and then send all aggregates to a single query coordinator for the final merging this performed very well when the number of groups was small but performed worse with large number of groups the mpp databases chosen plan is similar to choosing a single reduce task for shark and hive join query the final query from pavlo et al involved joining the tb uservis its table with the gb rankings table shark disk shark copartitioned select into temp sourceip avg pagerank sum adrevenue as totalrevenue from rankings as r uservisits as uv where r pageurl uv desturl and uv visitdate between date and date group by uv sourceip again shark outperformed hive in all cases figure shows that for this query serving data out of memory did not provide much benefit over disk this is because the cost of the join step dominated the query processing co partitioning the two tables however provided significant benefits as it avoided shuffling tb of data during the join step data loading hadoop was shown by to excel at data loading as its data loading throughput was five to ten times higher than that of mpp databases as explained in section shark can be used to query data in hdfs directly which means its data ingress rate is at least as fast as hadoop after generating the tb uservisits table we measured the time to load it into hdfs and compared that with the time to load it into shark memory store we found the rate of data ingress was higher in shark memory store than that of hdfs micro benchmarks to understand the factors affecting shark performance we con ducted a sequence of micro benchmarks we generated gb and tb of data using the dbgen program provided by tpc h we chose this dataset because it contains tables and columns of varying cardinality and can be used to create a myriad of micro benchmarks for testing individual operators while performing experiments we found that hive and hadoop mapreduce were very sensitive to the number of reducers set for a job hive optimizer automatically sets the number of reducers based on the estimated data size however we found that hive optimizer frequently made the wrong decision leading to incredi bly long query execution times we hand tuned the number of re ducers for hive based on characteristics of the queries and through trial and error we report hive performance numbers for both optimizer determined and hand tuned numbers of reducers shark on the other hand was much less sensitive to the number of reducers and required minimal tuning aggregation performance we tested the performance of aggregations by running group by queries on the tpc h lineitem table for the gb dataset lineitem table contained million rows for the tb dataset it contained billion rows the queries were of the form select group by count from lineitem we chose to run one query with no group by column i e a sim ple count and three queries with group by aggregations ship mode groups receiptdate groups and shipmode million groups in gb and million groups in tb for both shark and hive aggregations were first performed on each partition and then the intermediate aggregated results were partitioned and sent to reduce tasks to produce the final aggrega tion as the number of groups becomes larger more data needs to be shuffled across the network figure compares the performance of shark and hive measur ing shark performance on both in memory data and data loaded static figure join strategies chosen by optimizers seconds from hdfs as can be seen in the figure shark was faster than hand tuned hive for queries with small numbers of groups and faster for queries with large numbers of groups where the shuffle phase domniated the total execution cost we were somewhat surprised by the performance gain observed for on disk data in shark after all both shark and hive had to read data from hdfs and deserialize it for query processing this difference however can be explained by shark very low task launching overhead optimized shuffle operator and other factors see section for more details join selection at run time in this experiment we tested how partial dag execution can im prove query performance through run time re optimization of query plans the query joined the lineitem and supplier tables from the tb tpc h dataset using a udf to select suppliers of interest based on their addresses in this specific instance the udf selected out of million suppliers figure summarizes these results select on l from lineitem l join supplier where lacking good selectivity estimation on the udf a static opti mizer would choose to perform a shuffle join on these two tables because the initial sizes of both tables are large leveraging partial dag execution after running the pre shuffle map stages for both tables shark dynamic optimizer realized that the filtered supplier table was small it decided to perform a map join replicating the filtered supplier table to all nodes and performing the join using only map tasks on lineitem to further improve the execution the optimizer can analyze the logical plan and infer that the probability of supplier table being small is much higher than that of lineitem since supplier is smaller initially and there is a filter predicate on supplier the optimizer chose to pre shuffle only the supplier table and avoided launching two waves of tasks on lineitem this combination of static query analysis and partial dag execution led to a performance im provement over a naïve statically chosen plan fault tolerance to measure shark performance in the presence of node failures we simulated failures and measured query performance before dur ing and after failure recovery figure summarizes fives runs of our failure recovery experiment which was performed on a node 4xlarge cluster we used a group by query on the gb lineitem table to mea sure query performance in the presence of faults after loading the lineitem data into shark memory store we killed a worker ma chine and re ran the query shark gracefully recovered from this failure and parallelized the reconstruction of lost partitions on the other nodes this recovery had a small performance impact but it was significantly cheaper than the cost of re loading the entire dataset and re executing the query vs secs adaptive static adaptive shark d n o c shark disk hive tuned hive shark shark disk hive tuned hive 5k tpc h tpc h figure aggregation queries on lineitem table x axis indicates the number of groups for each aggregation query e e m i t d n o c e e m i t shark shark disk hive figure real hive warehouse workloads figure compares the performance of shark and hive on these queries the result is very promising as shark was able to process these real life queries in sub second latency in all but one cases whereas it took hive to times longer to execute them a closer look into these queries suggests that this data exhibits the natural clustering properties mentioned in section the map pruning technique on average reduced the amount of data scanned by a factor of machine learning a key motivator of using sql in a mapreduce environment is the ability to perform sophisticated machine learning on big data we implemented two iterative machine learning algorithms logistic re gression and k means to compare the performance of shark versus running the same workflow in hive and hadoop the dataset was synthetically generated and contained billion rows and columns occupying gb of space thus the fea ture matrix contained billion points each with dimensions these machine learning experiments were performed on a node xlarge cluster data was initially stored in relational form in shark memory store and hdfs the workflow consisted of three steps select ing the data of interest from the warehouse using sql extract ing features and applying iterative algorithms in step both algorithms were run for iterations figures and show the time to execute a single iteration post recovery single failure no failures figure query time with failures seconds after this recovery subsequent queries operated against the re covered dataset albeit with fewer machines in figure the post recovery performance was marginally better than the pre failure performance we believe that this was a side effect of the jvm jit compiler as more of the scheduler code might have become compiled by the time the post recovery queries were run real hive warehouse queries an early industrial user provided us with a sample of their hive warehouse data and two years of query traces from their hive sys tem a leading video analytics company for content providers and publishers the user built most of their analytics stack based on hadoop the sample we obtained contained days of video ses sion data occupying tb of disk space when decompressed it consists of a single fact table containing columns with heavy use of complex data types such as array and struct the sampled query log contains analytical queries sorted in or der of frequency we filtered out queries that invoked proprietary udfs and picked four frequent queries that are prototypical of other queries in the complete trace these queries compute ag gregate video quality metrics over different audience segments query computes summary statistics in dimensions for users of a specific customer on a specific day query counts the number of sessions and distinct customer client combination grouped by countries with filter predi cates on eight columns query counts the number of sessions and distinct users for all but countries query computes summary statistics in dimensions group ing by a column and showing the top groups sorted in de scending order full reload d n o c e e m i t q3 within a mapreduce job the map tasks save their output in case a reduce task fails second many queries need to be compiled shark into multiple mapreduce steps and engines rely on replicated file hadoop binary systems such as hdfs to store the output of each step hadoop text for the first case we note that map outputs were stored on disk primarily as a convenience to ensure there is sufficient space to hold them in large batch jobs map outputs are not replicated across figure logistic regression per iteration runtime seconds nodes so they will still be lost if the mapper node fails thus if the outputs fit in memory it makes sense to store them in memory initially and only spill them to disk if they are large shark shuf shark fle implementation does this by default and sees far faster shuffle performance and no seeks when the outputs fit in ram this hadoop binary is often the case in aggregations and filtering queries that return a hadoop text much smaller output than their input another hardware trend that may improve performance even for large shuffles is ssds which would allow fast random access to a larger space than memory figure k means clustering per iteration runtime seconds for the second case engines that extend the mapreduce execu tion model to general task dags can run multi stage jobs without of logistic regression and k means respectively we implemented two versions of the algorithms for hadoop one storing input data materializing any outputs to hdfs many such engines have been proposed including dryad tenzing and spark as text in hdfs and the other using a serialized binary format the data format and layout while the naïve pure schema on read binary representation was more compact and had lower cpu cost approach to mapreduce incurs considerable processing costs many in record deserialization leading to improved performance our re systems use more efficient storage formats within the mapreduce sults show that shark is faster than hive and hadoop for lo model to speed up queries hive itself supports table partitions a gistic regression and faster for k means k means experienced basic index like system where it knows that certain key ranges are less speedup because it was computationally more expensive than contained in certain files so it can avoid scanning a whole table as logistic regression thus making the workflow more cpu bound well as column oriented representation of on disk data we go in the case of shark if data initially resided in its memory store further in shark by using fast in memory columnar representations step and were executed in roughly the same time it took to run one iteration of the machine learning algorithm if data was not within spark shark does this without modifying the spark runtime by simply representing a block of tuples as a single spark record loaded into the memory store the first iteration took seconds for one java object from spark perspective and choosing its own both algorithms subsequent iterations however reported numbers representation for the tuples within this object consistent with figures and in the case of hive and hadoop another feature of spark that helps shark but was not present in every iteration took the reported time because data was loaded from previous mapreduce runtimes is control over the data partitioning hdfs for every iteration across nodes section this lets us co partition tables discussion finally one capability of rdds that we do not yet exploit is ran dom reads while rdds only support coarse grained operations shark shows that it is possible to run fast relational queries in a fault tolerant manner using the fine grained deterministic task model introduced by mapreduce this design offers an effective way to for their writes read operations on them can be fine grained ac cessing just one record this would allow rdds to be used as indices tenzing can use such remote lookup reads for joins scale query processing to ever larger workloads and to combine execution strategies hive spends considerable time on sorting it with rich analytics in this section we consider two questions the data before each shuffle and writing the outputs of each mapre first why were previous mapreduce based systems such as hive duce stage to hdfs both limitations of the rigid one pass mapre slow and what gave shark its advantages second are there other duce model in hadoop more general runtime engines such as benefits to the fine grained task model we argue that fine grained spark alleviate some of these problems for instance spark sup tasks also help with multitenancy and elasticity as has been demon ports hash based distributed aggregation and general task dags strated in mapreduce systems to truly optimize the execution of relational queries however why are previous mapreduce based systems slow we found it necessary to select execution plans based on data statis tics this becomes difficult in the presence of udfs and complex conventional wisdom is that mapreduce is slower than mpp databases analytics functions which we seek to support as first class citizens for several reasons expensive data materialization for fault toler in shark to address this problem we proposed partial dag execu ance inferior data layout e g lack of indices and costlier exe tion pde which allows our modified version of spark to change cution strategies our exploration of hive confirms these the downstream portion of an execution graph once each stage com reasons but also shows that a combination of conceptually simple pletes based on data statistics pde goes beyond the runtime graph engineering changes to the engine e g in memory storage and rewriting features in previous systems such as dryadlinq more involved architectural changes e g partial dag execution by collecting fine grained statistics about ranges of keys and by can alleviate them we also find that a somewhat surprising variable allowing switches to a completely different join strategy such as not considered in detail in mapreduce systems the task schedul broadcast join instead of just selecting the number of reduce tasks ing overhead actually has a dramatic effect on performance and greatly improves load balancing if minimized intermediate outputs mapreduce based query engines such as hive materialize intermediate data to disk in two situations first like hadoop also benefit from the os buffer cache in serving map outputs but we found that the extra system calls and file system journalling from writing map outputs to files still adds overhead section task scheduling cost perhaps the most surprising engine prop erty that affected shark however was a purely engineering con cern the overhead of launching tasks traditional mapreduce sys tems such as hadoop were designed for multi hour batch jobs consisting of tasks that were several minutes long they launched each task in a separate os process and in some cases had a high latency to even submit a task for instance hadoop uses periodic heartbeats from each worker every seconds to assign tasks and sees overall task startup delays of seconds this was sufficient for batch workloads but clearly falls short for ad hoc queries spark avoids this problem by using a fast event driven rpc li brary to launch tasks and by reusing its worker processes it can launch thousands of tasks per second with only about ms of over head per task making task lengths of ms and mapreduce jobs of ms viable what surprised us is how much this affected query performance even in large multi minute queries sub second tasks allow the engine to balance work across nodes extremely well even when some nodes incur unpredictable delays e g network delays or jvm garbage collection they also help dramatically with skew consider for example a system that needs to run a hash aggregation on cores if the system launches reduce tasks the key range for each task needs to be carefully cho sen as any imbalance will slow down the entire job if it could split the work among tasks then the slowest task can be as much as slower than the average without affecting the job response time much after implementing skew aware partition selection in pde we were somewhat disappointed that it did not help compared to just having a higher number of reduce tasks in most workloads because spark could comfortably support thousands of such tasks however this property makes the engine highly robust to unex pected skew in this way spark stands in contrast to hadoop hive where us ing the wrong number of tasks was sometimes slower than an optimal plan and there has been considerable work to auto matically choose the number of reduce tasks figure shows how job execution times vary as the number of reduce tasks launched by hadoop and spark in a simple aggregation query on a node cluster since a spark job can launch thousands of reduce tasks without incurring much overhead partition data skew can be mitigated by always launching many tasks d n o c e e d n o c e e im t im t number of hadoop tasks number of spark tasks figure task launching overhead more fundamentally there are few reasons why sub second tasks should not be feasible even at higher scales than we have explored such as tens of thousands of nodes systems like dremel rou tinely run sub second multi thousand node jobs indeed even if a single master cannot keep up with the scheduling decisions the scheduling could be delegated across lieutenant masters for sub sets of the cluster fine grained tasks also offer many advantages over coarser grained execution graphs beyond load balancing such as faster recovery by spreading out lost tasks across more nodes and query elasticity we discuss some of these next other benefits of the fine grained task model while this paper has focused primarily on the fault tolerance ben efits of fine grained deterministic tasks the model also provides other attractive properties we wish to point out two benefits that have been explored in mapreduce based systems elasticity in traditional mpp databases a distributed query plan is selected once and the system needs to run at that level of par allelism for the whole duration of the query in a fine grained task system however nodes can appear or go away during a query and pending work will automatically be spread onto them this en ables the database engine to naturally be elastic if an administrator wishes to remove nodes from the engine e g in a virtualized cor porate data center the engine can simply treat those as failed or better yet proactively replicate their data to other nodes if given a few minutes warning similarly a database engine running on a cloud could scale up by requesting new vms if a query is expen sive amazon elastic mapreduce already supports resizing clusters at runtime multitenancy the same elasticity mentioned above enables dy namic resource sharing between users in some traditional mpp databases if an important query arrives while another large query is using most of the cluster there are few options beyond canceling the earlier query in systems based on fine grained tasks one can simply wait a few seconds for the current tasks from the first query to finish and start giving the nodes tasks from the second query for instance facebook and microsoft have developed fair sched ulers for hadoop and dryad that allow large historical queries compute intensive machine learning jobs and short ad hoc queries to safely coexist related work to the best of our knowledge shark is the only low latency system that can efficiently combine sql and machine learning workloads while supporting fine grained fault recovery we categorize large scale data analytics systems into three classes first systems like asterix tenzing scope chee tah and hive compile declarative queries into mapreduce style jobs although some of them modify the execution engine they are built on it is hard for these systems to achieve interactive query response times for reasons discussed in section second several projects aim to provide low latency engines us ing architectures resembling shared nothing parallel databases such projects include powerdrill and impala these systems do not support fine grained fault tolerance in case of mid query faults the entire query needs to be re executed google dremel does rerun lost tasks but it only supports an aggregation tree topol ogy for query execution and not the more complex shuffle dags required for large joins or distributed machine learning a third class of systems take a hybrid approach by combining a mapreduce like engine with relational databases hadoopdb connects multiple single node database systems using hadoop as the communication layer queries can be parallelized using hadoop mapreduce but within each mapreduce task data processing is pushed into the relational database system osprey is a middle ware layer that adds fault tolerance properties to parallel databases it does so by breaking a sql query into multiple small queries and sending them to parallel databases for execution shark presents a much simpler single system architecture that supports all of the properties of this third class of systems as well as statistical learn ing capabilities that hadoopdb and osprey lack the partial dag execution pde technique introduced by shark resembles adaptive query optimization techniques proposed in social networks play a fundamental role in the diffusion of infor mation however there are two different ways of how information reaches a person in a network information reaches us through con nections in our social networks as well as through the influence external out of network sources like the mainstream media while most present models of information adoption in networks assume information only passes from a node to node via the edges of the underlying network the recent availability of massive online social media data allows us to study this process in more detail we present a model in which information can reach a node via the links of the social network or through the influence of external sources we then develop an efficient model parameter fitting tech nique and apply the model to the emergence of url mentions in the twitter network using a complete one month trace of twitter we study how information reaches the nodes of the network we quantify the external influences over time and describe how these influences affect the information adoption we discover that the in formation tends to jump across the network which can only be explained as an effect of an unobservable external influence on the network we find that only about of the information volume in twitter can be attributed to network diffusion and the remaining is due to external events and factors outside the network categories and subject descriptors h database manage ment database applications data mining general terms algorithms theory experimentation keywords diffusion of innovations information cascades infor mation diffusion external influence twitter social networks introduction networks represent a fundamental medium for the emergence and diffusion of information for example we often think of information a rumor or a piece of content as being passed over the edges of the underlying social network this way informa tion spreads over the edges of the network like an epidemic however due to the emergence of mass media like newspapers tv stations and online news sites the information not only reaches us through the links of our social networks but also through the in permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee kdd august beijing china copyright acm figure our model of external influence a node denoted by a big circle is exposed to information through an external source governed by external activity λ ext t and by already infected neighbors governed by the internal hazard function λ int t with each new exposure x the probability of infection changes according to the exposure curve η x we infer both the external activity λ ext t as well as the exposure curve η x fluence of exogenous out of network sources from the early stages of research on news media and more generally information diffusion there has been the tension between global effects from the mass media and local effects carried by social structure traditionally it was hard to capture and study the effects of mass media and social networks simultaneously however the web blogs and social media changed the traditional picture of the di chotomy between the local effects carried by the links of social networks and the global influence from the mass media today mass media as well as the social networks both exist in the same web ecosystem which means that it is possible to collect mas sive online social media data and at the same time capture the ef fects of mass media as well as the influence arising from the social networks this allows us to study processes of information diffusion and emergence in much finer detail than ever before in this paper we ask the question how does information trans mitted by the mass media interact with the personal influence aris ing from social networks based on the complete one month twit ter data we study ways in which information reaches the nodes of the twitter network we analyze over billion tweets to discover mechanisms by which information appears and spreads through the twitter network in particular we contrast two main ways by which information emerges at the nodes of the network diffusion over the edges of the network and external influence when informa tion jumps across the network and appears at a seemingly ran dom node for example when information appears at a node that has no connections to nodes that have previously mentioned the in formation the emergence of information at that node can only be explained by the influence of some unobserved exogenous source however when information appears at a node with a neighbor that already tweeted it then it is not clear whether the node tweeted the information due to neighbor influence or due to the influence of the exogenous source thus the effect of internal and external in fections get confounded and the goal of the paper is to develop models that will allow us to separate the influence transmitted by social networks from the influence of the exogenous source effects of external influence on twitter users often post links to various webpages most often these are links to news articles blog posts funny videos or pictures generally there are two fun damental ways how users learn about these urls and tweet them one would be due to the exogenous out of the network effects for example one can imagine a scenario where one checks news on cnn com finds an interesting article and then posts a tweet with a url to the article in this case cnn is the external influence that caused that url to emerge onto a particular twitter user at contrast users can also come across urls by seeing them posted by other users that they follow this type of user to user exposure is what we refer to as internal influence or diffusion we find that both external and internal influence play significant role in the emergence of urls in the twitter network modeling the external influence in order to accurately model the emergence of content in twitter we need to consider the activity of the invisible out of network sources that also transmit information to the nodes of the twitter network via channels like tv newspa pers etc we present a probabilistic generative model of informa tion emergence in networks in which information can reach a node via the links of the social network or through the influence of the external source developing such a model is important for exam ple we simulated a purely non diffusive process that picks nodes of the twitter network at random and infects them after such process infects of the nodes about of infections falsely appear to be a result of diffusion i e the random process picks a node that has simply by chance an already infected neighbor thus instead of estimating the amount of internal influence at naive estimate would be due to the confounding of diffusion and external influence we aim to separate the two factors in our model figure we distinguish between exposures and infections an exposure event occurs when a node gets ex posed to information i and an infection event occurs when a node posts a tweet with information i exposures to information lead to an infection a node can get exposed to information in two dif ferent ways first a node u gets exposed to or becomes aware of information i whenever one of his neighbors in the social network posts a tweet containing i we call this an internal exposure the second way u can be exposed to i is through the activity of the external source we refer to this as external exposure we refer to the volume of external exposures over time as the event profile in order to establish the connection between exposures and infections we define the notion of the exposure curve that maps the number of times node u has been exposed to i into the probability of u getting infected distinguishing between exposures and infec tions and explicitly modeling the exposure curve allows us to cap ture rich effects for example during the diffusion of a news story the story may become stale and less relevant each time a user sees it so the probability of infection would decrease with each expo sure on the other hand exposures to a story about new technology may have the opposite effect with each exposure the user learns more about the technology so the probability of infection would in crease exposure curves allow us to model such diverse behaviors that our model is able to accurately estimate from the data furthermore we also develop an efficient parameter estimation technique we are given a network and a set of node infection times we then infer the event profile which quantifies the num ber of exposures generated by the external source over time we also infer the exposure curve that models the probability of infec tion as a function of the number of exposures of a node our model accurately distinguishes external influence from network diffusion we experiment with our model on twitter and find that we can accurately detect the occurrence external out of network events and the exposure curve inferred from our model is often more accurate than baseline methods we find even though we are study ing processes intrinsic to the twitter network only about of the content that appears in twitter can be attributed to the diffu sion through the edges of the twitter network we fit our model to different url that have appeared across twitter users and we use the inferred parameters of the model to provide insights into the mechanics of the emergence of these urls moreover we also perform per topic analysis and find that topics like politics and sports are most heavily driven by the external sources while en tertainment and technology are driven internally with only of exposures being external related work work on the diffusion of innovations provides a concep tual framework to study the emergence of information in networks conceptually we think of an often implicit network where each node is either active infected influenced or inactive and active nodes can then spread the contagion information disease along the edges of the underlying network a rich set of models has been developed that all try to describe different mechanisms by which the contagion spreads from the infected to an uninfected node however nearly all models only focus on the dif fusive part of the contagion adoption process while neglecting the external influence in this regard our work introduces an important dimension to the diffusion of innovations framework where we ex plicitly model the activity and influence of the external source external influence in networks has been considered in the case of the popularity of youtube videos authors considered a simple model of information diffusion on an implicit completely connected network and argued that since some videos became pop ular quicker than their model predicted the additional popularity must have been a result of external influence our approach differs significantly we directly consider the network and the effect of node to node interactions explicitly infer the activity of external source over time and use a much more realistic model of infor mation adoption that distinguishes between exposures to and the adoption of information our model builds on the notion of expo sure curves which was proposed and studied by romero et al recently it was also argued that it is the shape of exposure curves that stops the information from spreading we make a step forward by providing an inference method that infers the shape of such exposure curves simulations show that our method much more accurately infers the exposure curves than the methods previ ously proposed proposed model here we develop in detail our novel information diffusion model that incorporates both the spread of information from node to node along edges in the network as well as the external influences act ing on the network additionally our model reconciles the gap between a stream of exposures arriving in continuous time and a series of discrete decisions leading to infection modeling we refer to the amount of influence external sources have on the the external exposures the second source of expo network as a function of time as the event profile it is proportional sures to a particular single contagion for nodes in the network comes to the probability of any node receiving an external exposure at a from the external source acting on the network the fundamental particular time we use the term contagion to refer to a particular property of the problem we are trying to solve is that the external piece of information emerging in the twitter network and we say source cannot be observed the source varies in intensity over time a node is infected with a particular contagion when she first men and this function is called the event profile which we designate as tions tweets the contagion we model contagions as independent λ ext t specifically of each other which means we consider them one by one we illustrate our model in a node centric context in fig as λ ext t dt p i receives exposure t t dt sume a single contagion i e a piece of information as time pro gresses a node receives a stream of varying intensity of external exposures governed by the event profile λ ext t additionally its neighbors in the network also become infected by the contagion and each infected neighbor generates an internal exposure each exposure has a chance of infecting the node but with the arrival of each exposure the probability of infection changes according to the exposure curve η x eventually either the arrival of exposures will cease or the node will become infected and then expose to its neighbors our goal is to infer the number of exposures generated by the external source over time as well as the shape of the expo sure curve η x that governs the probability of node infection for any node i where t represents the amount of time since the contagion first appeared in the network a couple of things should be noted here first all nodes have the same probability of receiv ing an external exposure for any point in time second λ ext is not conditioned upon the node not already having received an external exposure this means that any node can receive an arbitrary num ber of external exposures we call λ ext the event profile because it describes an actual real world event that caused the information to arrive in the network and start spreading as the event progresses over time event efficacy in the network changes for example if our contagion is civil unrest in libya then every time their ruler gaddafi gives a speech or the rebels win a battle we would expect a modeling the internal exposures consider a single contagion in spike in the intensity of the external source and thus the even pro our model an internal exposure occurs when a neighbor of a node becomes infected and then an exposure is transmitted after a ran dom interval of time imagine a real world scenario in which the file λ ext as time passes without any new developments or as the event relevancy fades we expect λ ext decrease to however every time there is a new development we expect a spike in the ex social network is the twitter network and the contagion spreading across the network is a particular url if a neighbor writes a tweet ternal event profile λ ext we will infer λ ext non parametrically so we can quantify the relevancy of any event over its lifespan involving a particular url then a user sees their neighbor tweet in order to derive the distribution of exposures a node receives then and only then has the internal exposure propagated along the over time as a function of time we model the arrival of exposures as edge an infected node will expose each of its outgoing neighbors a binomial distribution consider we were to take the entire contin exactly once and the time it takes for each exposure to occur is uous time interval of the lifetime of the contagion and break it down sampled from some distribution universal to all edges in the net into smaller but finite time intervals then whether an exposure oc work therefore a hazard function is appropriate to model curred during each such subinterval is a bernoulli random variable this process hazard functions were originally developed in actu exposure vs no exposure with its own probability therefore the ary sciences and they describe a distribution of the length of time total number of exposures received in a time interval is a sum of it takes for an event to occur recently used hazard functions bernoulli random variables just as a binomial random variable is a as a basis for disease propagation in continuous time across social networks they are extremely effective at modeling discrete events sum of bernoulli random variables let say that λ ext is constant for all time and that time is discretized into finite intervals of length that happen over continuous time in this respect hazard functions δt then the probability that n external exposures have been re represent a principled way of occurrence of discrete events i e ceived after t time intervals is exactly a binomial distribution exposures as a function of continuous time specifically let λ int be the internal hazard function where λ int t dt p i exposes j t t dt i hasn t exposed j yet p exp t n n t δt λ ext δt n λ ext δt t n for any neighboring nodes i and j where t is the amount of time that has passed since node i was infected in our context λ int effectively models how long it takes a node to notice one of its set t t δt if we take the limit of as δt and t such that t does not change then this probability approaches neighbors becoming infected it is a function of the frequency with which nodes check up on each other for the twitter network each time a user logs in they are updated on all of their neighbors p exp the expected number of internal exposures a node i has received by lative time distribution t which we functions will define of exposures as λ i int t propagating is the sum along of the each cumu of the node inbound edges and can be derived as follows λ i int t p j exposed i before t j j is i inf neighbor where τ j exp t dt n n t λ ext dt n λ ext dt t dt n to relax the constraint that λ ext is constant we use the average of λ ext t over t p i t dt λ ext t exp n t dt n n t t where λ ext j j τ j λ int τ j ds t external and internal t exposures λ ext ds at the finally same users time are so if receiving we need both is i inf neighbor take into account both processes this would imply taking the convo lution of the two probabilities which would be computationally is the infection time of node j infeasible instead we use the average of λ ext t λ i int t t dt n λ ext t t dt t dt n effectively we approximated the flux of exposures as constant in time such that each interval of time has an equal probability of an exposure arriving so the sum of the events is a standard binomial random variable modeling the exposure curve we model the exposure curve as a parameterized equation recall that the exposure curve describes the probability of infection as a function of the number of exposures received more specifically if x is the current number of exposures the node has received and η x is the exposure curve then η x p node i is infected immediately after x th exposure we choose to parameterize η x as η x where ρ ρ and ρ parameterizing η x in this manor allows for several desirable properties first η so it is impossible to become infected by a contagion before being ex posed to it secondly this function is unimodal with an exponential tail so there is a critical mass of exposures when the contagion is most infectious followed by decay brought on by the idea becom ing overexposed tiresome lastly and most importantly ρ and ρ η x and ρ have important conceptual meanings ρ max x argmax x η x because of this we can think of ρ as a gen eral measure of how infectious a contagion is in the network and ρ as a measure of the contagion enduring relevancy fig shows several different forms of η x this parameterization is expres sive but any other parameterization for η x is also valid for the remainder of the paper we will discuss the model in the context of the η x parameterization presented above from exposures to infections in order to fit the parameters of the model to observed data we must now construct the probability functions to describe the model with the equations given above building the distribution of the infection time of a node i can be p symbol λ ext λ int i exp η x τ i n t table definition of symbols used in the model p exp i t dt n n t λ i int t λ ext t t t the exposure distribution n λ i int t λ ext t the exposure curve parameters ρ ρ the event profile internal hazard function infection time ρ name x exp t t ρ x governs the random amount of time it takes an infected node to expose its neighbors determines how the probability of infection changes with each exposure proportional to the probability of any node receiving an exposure at time t dt the probability that node i has received n exposures by time t the infection time of node i dt description n o i t c e f n i p ρ ρ exposures figure example exposure curves η x where η x is the probability of a node becoming infected upon its xth exposure to the contagion the parameters of η x are ρ and ρ done as follows let f i t p τ i t be the probability that node i has been infected by time t where τ i is the infection time of node i making use of the quantity p exp i n t f i t n p i has n exp p i inf i has n exp p exp i n t n η k n k while f i t is analogous to the cumulative distribution func tion of the infection probability it is important to note that it is not actually a distribution lim t f t as a result of lim x η x this is ideal because it implies that there is a non zero chance that a node will never become infected as should be the case inferring the model parameters next we develop a method of inferring the model parameters for a given network and the tract of a single contagion we fit the model to each contagion separately we are given the network and the infection times for each node that got infected with the contagion under consideration we then need to infer the event profile λ ext t for all t at which at least one node was infected and parameters of η x ρ and ρ of the exposure curve in all the number of parameters we are inferring is the number of unique node infection times plus the two parameters of η x our general strategy is to alternate back and forth from inferring λ ext t to inferring η x assuming we known one for certain while we infer the other until both functions converge below we first demonstrate how to infer the event profile when the exposure curve is known then we show t dt p i exposes j t t dt i hasn t exposed j yet η x p infected right after xth exposure λ ext n p exp i λ i int t λ ext t t dt p node exposed t t dt t δt n technical definition λ int λ t i int t λ ext t δt t t δt n δt how to infer the exposure curve with a known event profile finally we combine the two steps into a single algorithm inferring the event profile the following outlines a fast and ro bust method for inferring λ ext t given η x let s t be the number of nodes that are uninfected by the contagion currently under consideration at time t s t is a random variable whose ex pectation value is dependent on λ ext t η x and the underlying network the networks which we are interested in are sufficiently large so the quantity s t e s t is usually very small in mag nitude inferring at least this one λ ext node provides t was t us infected λ ext with ds a very then let straight forward define t k be the λ k method for kth time at which as λ ext t k to calculate s t s t k n p node i not infected by time t i n i n p exp i n t k n η k k n p exp i n n t k exp η y dy i n y λ k i int i the first approximation comes from treating the number of expo sures received by a node at any given time as a continuous real num ber instead of an integer this provides us with a closed form ex pression the second approximation comes from setting the num ber of exposures received by each node to be the expected number of exposures since the right hand side is monotonic it is strictly decreasing with respect to λ k exp λ t k η y dy we can solve for λ k using bisection search doing this for all t k gives us λ ext t k for each possible time and then we can use finite difference to get λ ext t k once the event profile has been inferred we must then update the exposure curve accordingly inferring the exposure curve now we assume we know λ ext t for all t k and we want to infer the exposure curve η x specif ically its parameters ρ and ρ our strategy in solving for these parameters will be to fix ρ and then solve for a ρ that maximizes the following approximation to the log likelihood making use of eq l η λ we ext λ have int log d f i t i i p exp i n log η k i ic n k where i is the set of all infected nodes ic is the set of all unin fected nodes and τ max n τ max is the time of the last observed infection the optimal ρ satisfies l so i p exp i n τ max n η k i ic n k ρ log η n i i n p exp i n τ i n log η k k i i n p i exp ρ n τ max dt ρ n k η k ρ i ic η k log η k f i t of the infections with the contagion have occurred doing this not only makes the runtime constant with respect to the duration of the contagion it also speeds up the algorithm in general at the price of only a negligible decrease in accuracy the algorithm scales linearly with the number of nodes that re ceived at least one exposure all nodes that received only external exposures and no internal exposures however are effectively iden tical and can be grouped into a single term for both the event profile inference and the exposure curve inference therefore in practice the runtime scales linearly with the number of nodes that received at least one internal exposure i e the union of outgoing neighbor hoods for all infected nodes for most real world social networks can the parameter ρ be solved iteratively using and initial value between and because p exp i is independent of ρ they only need to be calculated once this along with the iterations converg ing quickly makes this entire process very fast now we combine the event profile inference process with the exposure curve inference process to form a single algorithm that infers the entire model inferring all parameters if we use the previously mentioned method to infer η x using the actual ground truth λ ext t it works extremely well in fact coming up with contrived instances in which it breaks is difficult the same thing is true for using the event profile inference method with ground truth η x when nei ther ground truth function is known and we have to iterate back and forth between both methods however the results are not as stable both functions inference methods are sensitive to errors in the other function fortunately all that is needed to correct this is a slight modification simply put we fix ρ to some integer value and then iterate back and forth between the two methods then ρ and λ ext t converge to some values dependent on the fixed ρ and we calculate the log likelihood of the resulting inferred func tions we do this for all reasonable integer values of ρ and we choose the one with the optimal log likelihood algorithm gives the pseudocode algorithm model parameter inference t practical considerations since we infer the event profile λ ext in non parametric form the number of parameters in the model could potentially scale with the time duration of the contagion we would have to solve for λ ext for each node infection time t i t i times this can be prevented however by predetermining a set of then λ ˆt ext m t m m between only at these which set the event profile will be inferred times can be approximated using linear interpolation in practice we used m and we set each ˆt m at the time in which m t initialize for ρ λ ext ρ max final do l max initialize ρ while not converged do ρ solution to eq using ρ λ ext t λ ext t solution to eq using ρ ρ end while l log likelihood λ ext t ρ ρ if l l max then l final max l ρ end final if ρ end for λ ext t solution to eq using final final m final ground truth e r u o p x e f n i p baseline our method j exposure curve exposures η x figure experiments on synthetic data a e the model fit ted to a synthetic contagion on a scale free network with nodes the internal hazard function is λ int t t which in duces a raleigh unimodal distribution for the internal expo sure propagation time given just the number of infections a our model is able to infer all of b e f j the model fit ted to the same network but with the internal hazard function λ int t which induces a power law distribution for the internal exposure propagation time this implies the runtime scales slightly more than linearly with re spect to the number of infections we can infer the model parameters for most contagions well in side a minute a large portion of real world contagions in our ground truth scaled baseline our method t d event profile time λ ext ground truth e r u o p x e f n i p baseline our method e exposure exposures curve η x infections predicted infections t n u o c a all infections time ext infections predicted ext infections t n u o c b external infections time int infections predicted int infections t n u o c c internal infections time 250 ext infections predicted ext infections g external infections t n u o c time int infections predicted int infections h internal infections t n u o c time t t inferred by the model t given ground truth scaled baseline our method t i event profile time λ ext infections predicted infections t n u o c f all infections time 250 150 08 our baseline method new developments our baseline method t e r u o p x e 025 f 02 n i p 120 time hours exposures a event profile λ ext t b exposure curve η x figure the model fitted to a single contagion representing urls related to the tucson arizona shootings the green ver tical lines designate when four distinct developments related to the shooting event occurred dataset infects about nodes and rarely did the algorithm take more than seconds to converge for larger contagions some infecting thousands of nodes the runtime was minutes in all we used the algorithm to fit the model to more than real contagions and hundreds of synthetic contagions and we never encountered convergence issues experiments with our model well defined and with an algorithm for infer ring its parameters we now apply it to real as well as synthetic data first to establish the accuracy of the parameter inference algorithm we fit our model to synthetic data this allows for di rect comparison of ground truth to inferred parameters we exam ine a specific real world case study to better illustrate the model lastly we run a series of large scale experiments on the emergence of twitter urls the model reveals the underlying dynamics of information emergence on twitter experiments with synthetic data to test accuracy of the model parameter inference algorithm we run a series of experiments on simulated data for each experiment we first generate a large synthetic preferen tial attachment network we then choose values for η x λ ext t and λ int t at the start of the experiment all nodes are uninfected then using a small discrete time step δt we march forward in time and external exposures are sent to each node with probability λ ext t δt if a node becomes infected it will transmit exactly one exposure to each of its outbound neighbors and the time each outbound exposure takes to propagate is governed by λ int t δt with each exposure a node receives we sample a binary random variable with bias η x to determine whether the node will become infected upon that exposure once the experiment is complete the algorithm is given a set of node infection times the underlying net work and λ int t and its task is to infer η x and λ ext t baselines we compared our algorithm against common sense baselines for inferring η x we used the baseline of assuming internal exposures propagate immediately and that all exposures originate internally calculating η x k at each exposure count x k then boils down to counting the fraction of times a node becomes infected immediately after x k of its neighbors become infected note this is exactly the method of inferring η x used in the baseline for inferring λ ext t uses the number of infections that occur for each unit of time in which none of the node neighbors were previously infected we refer to these infections as external infections since an externally infected node by definition has no infected neighbors we know with certainty that all exposures the node received came from the event profile therefore the arrival of external infections over time should be indicative of the arrival of external exposures over time i e the event profile this how ever only provides a shape but not the scale of the event profile because without knowledge of the exposure curve η x we do not know how many exposures it takes to typically cause an infection thus the scale of the baseline λ ext t is usually to orders of magnitude larger experimental results we ran many different combinations of net work topologies exposure curves event profiles and internal haz ard functions overall we ran over different combinations on networks of nodes and the algorithm not only performed con sistently well but also did significantly better than the baselines we included the results of two such experiments in fig for the first experiment our algorithm is given a network and the data on figure a based only on this information it is able to infer data shown in figures b to e these figures illus trate various aspects of the inferred profile of the external influence i e the event profile and exposure curve against the ground truth and the baselines for the event profile not only is the scale of the baseline off by several orders of magnitude but it also places the peak of the event profile far too early on the other hand the event profile inferred using our algorithm very closely predicts the scale shape and the occurrence of the profile peak to the extent that the difference between the ground truth and inferred event pro file is negligible the situation is the same for inferring the ex posure curve in fig e the inferred η x almost exactly fits the ground truth whereas the baseline overestimates the exposure curve by more than for the second experiment fig b j we used a very pecu liar zig zag ground truth external influence profile fig b but the observations are still the same our model was able to infer all the quantities almost exactly the event profile inference shown in fig i is very accurate it resolves each of the peaks while the baseline besides being orders of magnitude off in scale only detects peaks we infer η x almost exactly as shown in fig j note that even though we test the algorithm on synthetic data the fact that the model works well is not at all trivial in particu lar from the model fitting point of view the effects of internal and external influence are confounded and the model estimation proce dure needs to separate them out in particular consider the contrast in the performance of the baseline approaches and the proposed model overall these experiments demonstrate the robustness of the model and allow us to move to the experiments on real data experiments using real data we now fit our model to a real data from the twitter network we study the emergence of urls on the twitter network urls emerge by twitter users mentioning them in their tweets through tweeting or re tweeting thus urls correspond to contagions posting a tweet mentioning a particular url corresponds to an in fection event twitter dataset to apply our model to a real world information diffusion network we collected complete twitter data for january which consists of billion tweets we focus on urls that have been tweeted by at least users as our contagions of study we found that contagions smaller than infections did not pro vide robust enough statistics for urls that were shortened we unshortened them and treated all urls that point to the same web address as one contagion we restricted our focus to urls in which we could classify as written in english to do this we extracted natural text from the html of the urls and then used a charac ter sequenced based classifier to determine their language we also removed urls that demonstrated blatant spamming behavior in all this resulted in different urls we constructed the network over which these urls propagate as follows first we took the union of all users that tweeted at least one of these urls then for each user in this set we used the twitter api to extract a list of the users that they follow when one user follows another he she can see all of their tweets include urls that they post and it is through this relationship that con tagions spread on twitter in all this created a node subgraph with edges we focus our study on urls as they clearly emerge due to external events for the internal hazard function λ int t empirical analysis indi cates that λ int t where t is in hours is a suitable choice this implies that the distribution of lag time between infections and exposures follows a power law with an exponent of a case study of the influence of external events we start our investigations on real data with an illustrative case study using in formation diffusion we aim to detect a sequence of external events that presumably caused bursts of activity on the twitter network we examined the tucson arizona shooting on january in which people were killed and others were injured and among the injured was u s congresswoman gabrielle giffords there were four key developments related to this event the shoot ing occurs jan the westboro baptist church announces plans to protest at the funerals of the victims jan arizona governor jan brewer signs emergency legis lation blocking the protest jan and an on line get well soon card is formed for gabrielle giffords that people can sign jan we collected all urls that were tweeted at least times that contained the word giffords we then gathered them into a single contagion given that we aggregated four separate sub stories we would expect that when we fit our model to the observed data the event profile would coincide with developments related to the real world event indeed this is the case as shown in fig the results of the model applied to the contagion are shown in fig additionally the time of each of the developments listed above is represented as a vertical green line in fig a our model clearly detects all four developments each of them is followed by a spike in the event profile within hours for the second two de velopments the spikes in the event profile are immediate also in teresting is how the baseline event profile differs from the model for example immediately after the development i e when the governor passed a new law the model infers two spikes in λ ext t whereas the baseline records only one in response to the law being passed many different groups began organizing counter protests to prevent the westboro baptist church from interfering with the funerals this created a second influx of urls from sources ex ternal to twitter facebook groups news sites etc which was completely missed by the baseline evaluation using google trends as a global alternative eval uation method we also performed the experiment where we ex tracted a set of mainstream media articles for which we were able to identify a single keyword w that adequately describes them e g swine flu for a bbc article on increase in northern ireland swine flu cases for each w we then queried google trends to ob tain the number of worldwide search traffic of query w over time this served as a proxy for the activity of the external source we compared the distance between the inferred event profile and the google trends ground truth overall we found that our model gives relative improvement in the distance of the inferred event profile when compared to the naive event profile estimation t external influence of different news categories we now proceed to an aggregate analysis of event profiles and external influence of different category of news we identified news sites that specify the article category within the url all together we identified url belonging to different news categories we then fit our model each url and infer the event profile as well as the exposure curve for each news category we then calculated the average ρ which is the maximum probability of infection for the exposure curve ρ which is the number of exposures at which the url is most infections the duration or lifetime over which the event profile was inferred and the number of expected total exter nal exposures each node receives from the url event the results are displayed in table the average value of ρ was ρ was the average duration of the contagions was hours and the average fraction of external infections was in the first column we show the maximum probabil ity of infection for the exposure curve notice that entertainment business and health appear to be the most infectious where art education and travel are the least infectious this seems reason able as news articles about topics such as art or education would be less likely to be retweeted compared to entertainment articles the second column describes upon which exposure the url is most in fectious world news which is more time sensitive reaches max imum infectiousness earlier compared to other topics after a user has received more than ρ exposures the probability of infection decreases so it makes sense that these topics which become irrel evant as time passes reach this point sooner contrast this with a topic like art that is naturally less temporally sensitive addi tionally we learn that topics with a smaller ρ tend to have shorter duration and topics with a larger ρ tend to have infections appear over a longer interval of time intuitively this makes sense as topics related to events world business get old sooner lastly the last column shows on average what percent of expo sures came from external sources versus from within the network politics appear to be the most externally driven topic while enter tainment is the most internally driven this consistent with the fact that the of the top users followed on twitter are entertainers global characteristics the distributions for both the ρ and ρ exposure curve parameters inferred across the entire url dataset can be found in figures a b interesting is for how low the values of ρ were inferred with a mode on the order of this implies that the people at least twitter users are very selective about the ideas they adopt additionally most of the inferred ρ parameters were small with ρ being the most common re call that a smaller ρ implies that the probability of infection begins to decrease with additional exposures sooner and from this we see evidence that users quickly fatigue of most diffusing contagions next for each url we went through every user that was in fected one by one for each user we plotted the order of infection of the user in relation to all other infections versus the fraction of expected exposures the user received from internal sources and the results can be found in figure c this plot demonstrates the in teresting time dynamics at play on average the first few users are infected almost purely externally but then there is a surge in inter nal exposures as a result the early infections are largely internally driven but as the contagion continues to spread the infections are driven more and more by external influences this initial surge in internally driven infections is also evident in the aggregated expo sure curve shown in fig upon each infection the expected number of exposures the user has received is recorded and divided by the inferred value of ρ this value shows how far along the node was in the exposure curve when the infection occurred and the apex of the exposure curve occurs when it is equal to as one might expect there is a high density of infections occurring at the apex what is interesting however is that there is also a dense group of infections happening early in the exposure curve at low probabilities this group is almost exclusively populated by internally infected users finally for each url we calculated the expected number of ex posures each user received during the emergence of the url and what fraction of these exposures came from an external source av eraging across all urls we found that of all exposures came from internal sources within the network while the other of the exposures were external we find this to be significant and clear evidence that external effects cannot be ignored within the past few years organizations in diverse indus tries have adopted mapreduce based systems for large scale data processing along with these new users important new workloads have emerged which feature many small short and increasingly interactive jobs in addition to the large long running batch jobs for which mapreduce was origi nally designed as interactive large scale query processing is a strength of the rdbms community it is important that lessons from that field be carried over and applied where possible in this new domain however these new workloads have not yet been described in the literature we fill this gap with an empirical analysis of mapreduce traces from six separate business critical deployments inside facebook and at cloudera customers in e commerce telecommunications media and retail our key contribution is a characteriza tion of new mapreduce workloads which are driven in part by interactive analysis and which make heavy use of query like programming frameworks on top of mapreduce these workloads display diverse behaviors which invalidate prior assumptions about mapreduce such as uniform data ac cess regular diurnal patterns and prevalence of large jobs a secondary contribution is a first step towards creating a tpc like data processing benchmark for mapreduce introduction many organizations depend on mapreduce to handle their large scale data processing needs as companies across di verse industries adopt mapreduce alongside parallel data bases new mapreduce workloads have emerged that fea ture many small short and increasingly interactive jobs these workloads depart from the original mapreduce use case targeting purely batch computations and shares se mantic similarities with large scale interactive query pro cessing an area of expertise of the rdbms community consequently recent studies on query like programming ex tensions for mapreduce and applying query opti mization techniques to mapreduce are likely to bring considerable benefit however integrating these ideas into business critical systems requires configu ration tuning and performance benchmarking against real life production mapreduce workloads knowledge of such workloads is currently limited to a handful of technology companies a cross workload comparison is thus far absent and use cases beyond the technology in dustry have not been described the increasing diversity of mapreduce operators create a pressing need to characterize industrial mapreduce workloads across multiple companies and industries arguably each commercial company is rightly advocat ing for their particular use cases or the particular problems that their products address therefore it falls to neutral researchers in academia to facilitate cross company collabo ration and mediate the release of cross industries data in this paper we present an empirical analysis of seven industrial mapreduce workload traces over long durations they come from production clusters at facebook an early adopter of the hadoop implementation of mapreduce and at e commerce telecommunications media and retail cus tomers of cloudera a leading enterprise hadoop vendor cumulatively these traces comprise over a year worth of data covering over two million jobs that moved approxi mately exabytes spread over machines table combined the traces offer an opportunity to survey emerg ing hadoop use cases across several industries cloudera customers and track the growth over time of a leading hadoop deployment facebook we believe this paper is the first study that looks at mapreduce use cases beyond the technology industry and the first comparison of multiple large scale industrial mapreduce workloads our methodology extends and breaks down each mapreduce workload into three conceptual components da ta temporal and compute patterns the key findings of our analysis are as follows there is a new class of mapreduce workloads for interac tive semi streaming analysis that notably differs from the original use case targeting purely batch computations there is a wide range of behavior within this workload permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies class such that we must exercise caution in regarding any aspect of workload dynamics as typical query like programatic frameworks on top of mapreduce bear this notice and the full citation on the first page to copy otherwise to such as hive and pig make up a considerable fraction of republish to post on servers or to redistribute to lists requires prior specific permission and or a fee articles from this volume were invited to present their results at the international conference on very large data bases august istanbul turkey proceedings of the vldb endowment vol no activity in all workloads we analyzed some prior assumptions about mapreduce such as uni form data access regular diurnal patterns and prevalence of large jobs no longer hold copyright vldb endowment 08 subsets of these observations have emerged in several studies that each looks at only one mapreduce workload identifying these characteristics across a rich and diverse set of workloads shows that the observations are applicable to a range of use cases we view this class of mapreduce workloads for interac tive semi streaming analysis as a natural extension of in teractive query processing their prominence arises from the ubiquitous ability to generate collect and archive data about both technology and physical systems as well as the growing statistical literacy across many industries to interactively explore these datasets and derive timely in sights the semantic proximity of this mapre duce workload to interactive query processing suggests that optimization techniques for one likely translate to the other at least in principle however the diversity of behavior even within this same mapreduce workload class complicates ef forts to develop generally applicable improvements conse quently ongoing mapreduce studies that draw on database management insights would benefit from checking workload assumptions against empirical measurements the broad spectrum of workloads analyzed allows us to identify the challenges associated with constructing a tpc style big data processing benchmark for mapreduce top concerns include the complexity of generating representative data and processing characteristics the lack of understand ing about how to scale down a production workload the difficulty of modeling workload characteristics that do not fit well known statistical distributions and the need to cover a diverse range of workload behavior the rest of the paper is organized as follows we re view prior work on workload related studies and de velop hypotheses about mapreduce behavior using existing mental models we then describe the mapreduce work load traces the next few sections present empirical evidence that describe properties of mapreduce workloads for interactive semi streaming analysis which depart from prior assumptions about mapreduce as a mostly batch pro cessing paradigm we discuss data access patterns workload arrival patterns and compute patterns we detail the challenges these workloads create for building a tpc style benchmark for mapreduce and close the paper by summarizing the findings reflecting on the broader implications of our study and highlighting future work prior work the desire for thorough system measurement predates the rise of mapreduce workload characterization studies have been invaluable in helping designers identify problems ana lyze causes and evaluate solutions workload characterization for database systems culmi nated in the tpc series of benchmarks which built on industrial consensus on representative behavior for trans actional processing workloads industry experience also re vealed specific properties of such workloads such as zipf distribution of data accesses and bimodal distribution of query sizes later in the paper we see that some of these properties also apply to the mapreduce workloads we analyzed the lack of comparable insights for mapreduce has hin dered the development of a tpc like mapreduce bench mark suite that has a similar level of industrial consen sus and representativeness as a stopgap alternative some mapreduce microbenchmarks aim to faciliate performance comparison for a small number of large scale stand alone jobs an approach adopted by a series of stud ies these microbenchmarks of stand alone jobs remain different from the perspective of tpc bench marks which views a workload as a complex superposition of many jobs of various types and sizes the workload perspective for mapreduce is slowly emerg ing albeit in point studies that focus on technology industry use cases one at a time the stand alone nature of these studies forms a part of an interest ing historical trend for workload based studies in general studies in the late and early capture system behavior for only one setting possibly due to the nascent nature of measurement tools at the time stud ies in the and early achieve greater general ity likely due to a combination of im proved measurement tools wide adoption of certain systems and better appreciation of what good system measurement enables stand alone studies have become common again in recent years likely the result of only a few organizations being able to afford large scale systems the above considerations create the pressing need to gen eralize beyond the initial point studies for mapreduce work loads as mapreduce use cases diversify and mis engineer ing opportunities proliferate system designers need to op timize for common behavior in addition to improving the particulars of individual use cases some studies amplified their breadth by working with isps or enterprise storage vendors i e interme diaries who interact with a large number of end customers the emergence of enterprise mapreduce vendors present us with similar opportunities to look beyond single point mapreduce workloads hypotheses on workload behavior one can develop hypotheses about workload behavior bas ed on prior work below are some key questions to ask about any mapreduce workload for optimizing the underlying storage system how uniform or skewed are the data accesses how much temporal locality exists for workload level provisioning and load shaping how regular or unpredictable is the cluster load how large are the bursts in the workload for job level scheduling and execution planning what are the common job types what are the size shape and duration of these jobs how frequently does each job type appear for optimizing query like programming frameworks what of cluster load comes from these frame works what are the common uses of each framework for performance comparison between systems how much variation exists between workloads can we distill features of a representative workload trace using the original mapreduce use case of data indexing in support of web search and the workload assumptions be hind common microbenchmarks of stand alone large scale machines length date jobs bytes moved cc a month tb cc b days tb jobs one would expect answers to the above to be some data access skew and temporal locality exists but there is no information to speculate on how much cc c month pb cc d months pb cc e days tb fb months pb the load is sculpted to fill a predictable web search diur nal with batch computations bursts are not a concern since fb months eb total year eb new load would be admitted conditioned on spare cluster capacity the workload is dominated by large scale jobs with fixed computation patterns that are repeatedly and regularly run we lack information to speculate how and how much query like programming frameworks are table summary of traces cc is short for cloudera customer fb is short for facebook bytes moved is computed by sum of input shuffle and output data sizes for all jobs used we expect small variation between different use cases and the representative features are already captured in publications on the web indexing use case and existing cc a cc b cc c cc d cc e microbenchmarks several recent studies offered single use case counter points to the above mental model the data b o j f o n o i t in this paper allow us to look across use cases from several industries to identify an alternate workload class what sur f prised us the most is the tremendous diversity within this workload class which precludes an easy characterization per job input size per job shuffle size per job output size of representative behavior and that some aspects of workload behavior are polar opposites of the original large scale data indexing use case which warrants efforts to revisit some mapreduce design assumptions workload traces overview we analyze seven workloads from various hadoop deploy ments all seven come from clusters that support business critical processes five are workloads from cloudera en terprise customers in e commerce telecommunications me dia and retail two others are facebook workloads on the same cluster across two different time periods these work loads offer an opportunity to survey hadoop use cases across several technology and traditional industries cloudera cus tomers and track the growth of a leading hadoop deploy ment facebook table provides details about these workloads the trace lengths are limited by the logistical feasibility of shipping the trace data for offsite analysis the cloudera customer workloads have raw logs approaching requiring us to set up specialized file transfer tools transferring raw logs is infeasible for the facebook workloads requiring us to query facebook internal monitoring tools combined the workloads contain over a year worth of trace data covering a significant amount of jobs and bytes processed by the clusters the data comes from standard logging tools in hadoop no additional tools were necessary the workload traces contain per job summaries for job id numerical key job name string input shuffle output data sizes bytes du ration submit time map reduce task time slot seconds map reduce task counts and input output file paths string we call each of the numerical characteristic a dimension of a job some traces have some data dimensions unavailable we obtained the cloudera traces by doing a time range selection of per job hadoop history logs based on the file timestamp the facebook traces come from a similar query on facebook internal log database the traces reflect no logging interruptions except for the cluster in cc d which was taken offline several times due to operational reasons kb mb gb tb kb mb gb tb figure data size for each workload showing input shuffle and output size per job there are some inaccuracies at trace start and termina tion due to partial information for jobs straddling the trace boundaries the length of our traces far exceeds the typical job length on these systems leading to negligible errors to capture weekly behavior for cc b and cc e we intentionally queried for days of data to allow for inaccuracies at trace boundaries data access patterns data manipulation is a key function of any data manage ment system so understanding data access patterns is cru cial query size data skew and access temporal locality are key concerns that impact performance for rdbms systems the mirror considerations exist for mapreduce specifi cally this section answers the following questions how uniformly or skewed are the data accesses how much temporal locality exists we begin by looking at per job data sizes the equivalent of query size skew in access frequencies and temporal locality in data accesses per job data sizes figure shows the distribution of per job input shuffle and output data sizes for each workload across the work loads the median per job input shuffle and output sizes differ by and orders of magnitude respectively most c a r kb mb gb tb fb fb b o j f kb mb gb tb kb mb gb tb kb mb gb tb per job input size f o n o i t c a r per job shuffle size per job output size cc b y c cc c cc d cc e fb input file rank by descending access frequency b o j f o n o i t c a r f input file size cc b cc c cc d cc e fb kb mb gb tb n e u q e r f e c c a e l i f d e r o t cc b cc c cc d cc e fb f kb mb gb tb input files size figure access patterns vs input file size showing cummulative fraction of jobs with input files of a certain size top and cummulative fraction of all stored bytes from input files of a certain size bottom cc b y c n cc c cc d cc e 000 000 output file rank by descending access frequency figure log log file access frequency vs rank show ing zipf distribution of same shape slope for all work loads jobs have input shuffle and output sizes in the mb to gb range thus benchmarks of tb and above captures only a narrow set of input shuffle and output patterns from to the facebook workloads per job input and shuffle size distributions shift right become larger by several orders of magnitude while the per job output size distribution shifts left becomes smaller raw and inter mediate data sets have grown while the final computation results have become smaller one possible explanation is that facebook customer base raw data has grown while the final metrics output to drive business decisions have remained the same skews in access frequency this section analyzes hdfs file access frequency and in tervals based on hashed file path names the fb and cc a traces do not contain path names and the fb trace contains path names for input only figure shows the distribution of hdfs file access fre quency sorted by rank according to non decreasing frequency note that the distributions are graphed on log log axes and form approximately straight lines this indicates that the file accesses follow a zipf like distribution i e a few files account for a very high number of accesses this obser vation challenges the design assumption in hdfs that all data sets should be treated equally i e stored on the same medium with the same data replication policies highly skewed data access frequencies suggest a tiered storage ar chitecture should be explored and any data caching policy that includes the frequently accessed files will bring considerable benefit further the slope parameters of the distributions are all approximately across workloads and for both inputs and outputs thus file access patterns are zipf like distributions of the same shape figure suggests the existence of common computation needs that lead to the same file access behavior across different industries the above observations indicate only that caching helps e t 000 y b f e u q o n o 000 e r f i t c a r e c c a e l i f b o j f o n o i t c a f cc b cc c cc d r cc e output file size kb mb gb tb kb mb gb tb figure access patterns vs output file size showing cummulative fraction of jobs with output files of a certain size top and cummulative fraction of all stored bytes from output files of a certain size bottom if there is no correlation between file sizes and access fre quencies maintaining cache hit rates would require caching a fixed fraction of bytes stored this design is not sustain able since caches intentionally trade capacity for perfor mance and cache capacity grows slower than full data ca pacity fortunately further analysis suggests more viable caching policies figures and show data access patterns plotted against input and output file sizes the distributions for fraction of jobs versus file size vary widely top graphs but converge in the upper right corner in particular of jobs ac cesses files of less than a few gbs note the log scale axis these files account for up to only of bytes stored bot d e r o t f output file size cc b e t cc c y b f o n o cc d i t c a r cc e e r f a o e n cc b o i t c c c cc c a r f cc d cc e fb sec min hr hrs input input re access interval e r f a sec min hr hrs figure data re accesses intervals showing interval between when an input file is re read top and when an output is re used as the input for another job bottom o n e cc b o i c c cc c t c a r f cc d cc e fb output input re access interval b o j f jobs whose input re access pre o n io t existing output c a r f jobs whose input re access pre existing input fb cc b cc c cc d cc e figure fraction of jobs that reads pre existing input path note that output path information is missing from fb tom graphs thus a viable cache policy is to cache files whose size is less than a threshold this policy allows cache capacity growth rates to be detached from the growth rate in data prior work has also observed zipf like distributed data access patterns for rdbms workloads culminating in the formulation of the rule i e of the data access go to of the data for mapreduce the rule is more complicated we need to consider both the input and output data sets and the size of each data set if we had just considered a zipf log log slope of we would have arrived at a rule figure and account for the size of data sets also and indicate that of jobs data accesses go to less than of the stored bytes for both input and output data sets depending on the workload the access patterns range from an rule to an rule access temporal locality further analysis also reveals temporal locality in the data accesses figure indicates the distribution of time intervals between data re accesses of the re accesses take place within hours thus a possible cache eviction policy is to evict entire files that have not been accessed for longer than a workload specific threshold duration any similar policy to least recently used lru would make sense figure further shows that up to of jobs involve data re accesses cc c cc d cc e while for other work loads the fraction is lower thus the same cache eviction policy potentially translates to different benefits for different workloads combined the observations in this section indicate that it will be non trivial to preserve for performance comparisons the data size skew in access frequency and access temporal locality of the data the analysis also reveals the tremen dous diversity across workloads only one numerical feature remains relatively fixed across workloads the shape param eter of the zipf like distribution for data access frequencies consequently we should be cautious in considering any as pect of workload behavior as being typical workload variation over time the temporal workload intensity variation has been an im portant concern for rdbms systems especially ones that back consumer facing systems subject to unexpected spikes in behavior the transactions or queries per second metric quantifies the maximum stress that the system can handle the analogous metric for mapreduce is more complicated as each job or query in mapreduce potentially involves different amounts of data and different amounts of compu tation on the data actual system occupancy depends on the combination of these multiple time varying dimensions with thus yet unknown correlation between the dimensions the empirical workload behavior over time has implica tions for provisioning and capacity planning as well as the ability to do load shaping or consolidate different workloads specifically this section tries to answer the following how regular or unpredictable is the cluster load how large are the bursts in the workload in the following we look at workload variation over a week quantify burstiness a common feature for all workloads and compute temporal correlations be tween different workload dimensions our analysis proceeds in four dimensions the job submission counts the aggregate input shuffle and output data size involved the aggregate map and reduce task times and the resulting system occupany in the number of active task slots weekly time series figure depicts the time series of four dimensions of workload behavior over a week the first three columns respectively represents the cumulative job counts amount of i o again counted from mapreduce api and compu tation time of the jobs submitted in that hour the last col umn shows cluster utilization which reflects how the cluster serviced the submitted workload described by the preceding columns and depends on the cluster hardware and execu tion environment figure shows all workloads contain a high amount of noise in all dimensions as neither the signal nor the noise models are known it is challenging to apply standard signal processing methods to quantify the signal to noise ratio of these time series further even though the number of jobs submitted is known it is challenging to predict how much i o and computation will result some workloads exhibit daily diurnal patterns revealed by fourier analysis and for some cases are visually identifi able e g jobs submission for fb utilization for cc e in section we combine this observation with several oth ers to speculate that there is an emerging class of interactive and semi streaming workloads submission rate jobs hr utilization slots su m tu w th f sa su m tu w th f sa su m tu w th f sa su m tu w th f sa compute i o tb hr task hrs hr cc a cc b cc c cc d cc e fb fb tu w th f sa su m tu w th f sa su m tu w th f sa su m tu w th f sa su m su m tu w th f sa su m tu w th f sa su m tu w th f sa 10000 30000 f sa su m tu w th f sa su m tu w th f sa su m tu w th 10000 tu w th f sa su m tu w th f sa su m tu w th f sa su m tu w th f sa su m 500 3000 2000 1000 500 su m tu w th f sa su m tu w th f sa su m tu w th f sa 3000 2000 400 50000 su m tu w th f sa su m tu w th f sa su m tu w th f sa su m tu w th f sa figure workload behavior over a week from left to right jobs submitted per hour aggregate i o i e input shuffle output size of jobs submitted aggregate map and reduce task time in task hours of jobs submitted cluster utilization in average active slots from top row to bottom showing cc a cc b cc c cc d cc e fb and fb workloads note that for cc c cc d and fb the utilization data is not available from the traces also note that some time axes are misaligned due to short week long trace lengths cc b and cc e or gaps from missing data in the trace cc d figure offers visual evidence to indicate the diversity of mapreduce workloads there is significant variation in the shape of the graphs for both different dimensions of the same workloads rows and for the same workload dimen sion across different workloads columns consequently for cluster management problems that involve workload varia tion over time scales such as load scheduling load shifting resource allocation or capacity planning approaches de signed for one workload may be suboptimal or even counter productive for another as mapreduce use cases diversify and increase in scale it becomes vital to develop workload management techniques that can target each specific work load burstiness figure also reveals bursty submission patterns across various dimensions burstiness is an often discussed prop 1000 erty of time varying signals but it is often not precisely mea sured one common way to attempt to measure it to use the peak to average ratio there are also domain specific met rics such as for bursty packet loss on wireless links here we extend the concept of peak to average ratio to quantify burstiness we start defining burstiness first by using the median rather than the arithmetic mean as the measure of aver age median is statistically robust against data outliers i e extreme but rare bursts for two given workloads with the same median load the one with higher peaks that is a higher peak to median ratio is more bursty we then observe that the peak to median ratio is the same as the percentile to median ratio while the median is sta tistically robust to outliers the percentile is not this implies that the or percentile should also be calculated we extend this line of thought and compute 01 r u o h f o n o i t c a r f cc a cc b cc c cc d cc e normalized task seconds per hour r u o h f o n o i t c a r f 01 100 normalized task seconds per hour fb fb sine sine figure workload burstiness showing cummulative distribution of task time sum of map time and reduce time per hour to allow comparison between workloads all values have been normalized by the median task time per hour for each workload for comparison we also show burstiness for artificial sine submit patterns scaled with min max range the same as mean sine and of mean sine the general nth percentile to median ratio for a workload we can graph this the x axis versus n vector on the of y axis values the with resultant n th percentile median graph can on be interpreted as a cumulative distribution of arrival rates per time unit normalized by the median arrival rate this graph is an indication of how bursty the time series is a more horizontal line corresponds to a more bursty workload a vertical line represents a workload with a constant arrival rate figure graphs this metric for one of the dimensions of our workloads we also graph two different sinusoidal signals to illustrate how common signals appear under this burstiness metric figure shows that for all workloads the highest and lowest submission rates are orders of magnitude from the median rate this indicates a level of burstiness far above the workloads examined by prior work which have more regular diurnal patterns for the workloads here scheduling and task placement policies will be essen tial under high load conversely mechanisms for conserving energy will be beneficial during periods of low utilization for the facebook workloads over a year the peak to median ratio dropped from to accompanied by more internal organizations adopting mapreduce this shows that multiplexing many workloads workloads from many organizations help decrease bustiness however the work load remains bursty time series correlations we also computed the correlation between the workload submission time series in all three dimensions specifically we compute three correlation values between the time varying vectors jobssubmitted t and datasizebytes t be tween jobssubmitted t and computet imet askseconds t and between datasizebytes t and computet imet asksec onds t where t represents time in hourly granularity and ranges over the entire trace duration jobs bytes jobs task seconds bytes task seconds figure correlation between different submission pat tern time series showing pair wise correlation between jobs per hour input shuffle output bytes per hour and map reduce task times per hour the results are in figure the average temporal correla tion between job submit and data size is for job submit and compute time it is for data size and compute time it is 62 the correlation between data size and compute time is by far the strongest we can visually verify this by the and columns for cc e in figure this indicates that mapreduce workloads remain data centric rather than compute centric also schedulers and load balancers need to consider dimensions beyond number of active jobs combined the observations in this section mean that max imum jobs per second is the wrong performance metric to evaluate these systems the nature of any workload bursts depends on the complex aggregate of data and compute needs of active jobs at the time as well as the scheduling placement and other workload management decisions that determine how quickly jobs drain from the system any efforts to develop a tpc like benchmark for mapreduce should consider a range of performance metrics and stress ing the system under realistic multi dimensional variations in workload intensity computation patterns previous sections looked at data and temporal patterns in the workload as computation is an equally important aspect of mapreduce this section identifies what are the common computation patterns for each workload specifi cally we answer questions related to optimizing query like programming frameworks what of cluster load come from these frameworks what are the common uses of each framework we also answer questions with regard to job level scheduling and execution planning what are the common job types what are the size shape and duration of these jobs how frequently does each job type appear in traditional rdbms one can quantify query types by the operator e g join select and the cardinality of the data processed for a particular query each operator can be characterized to consume a certain amount of resources based on the cardinality of the data they process the ana log to operators for mapreduce jobs are the map and reduce steps and the cardinality of the data is quantified in our analysis by the number of bytes of data for the map input intermediate shuffle and reduce output stages n o i t a l e r r o c fb fb cc a cc b cc c cc d cc e we consider two complementary ways of grouping mapre duce jobs by the job name strings submitted to mapre duce which gives us insights on the use of native mapre duce versus query like programatic frameworks on top of mapreduce for some frameworks this analysis also reveals the frequency of the particular query like operators that are used by the multi dimensional job description according to per job data sizes duration and task times which serve as a proxy to proprietary code and indicate the size shape and duration of each job type by job names job names are user supplied strings recorded by mapre duce some computation frameworks built on top of mapre duce such as hive pig and oozie generate the job names automatically mapreduce does not currently impose any structure on job names to simplify analysis we focus on the first word of job names ignoring any capitalization numbers or other symbols figure shows the most frequent first words in job names for each workload weighted by number of jobs the amount of i o and task time the fb trace does not have this information the top figure shows that the top handful of words account for a dominant majority of jobs when these names are weighted by i o hive queries such as insert and other data centric jobs such as data extractors domi nate when weighted by task time the pattern is similar unsurprising given the correlation between i o and task time figure also implies that each workload consists of only a small number of common computation types the rea son is that job names are either automatically generated or assigned by human operators using informal but common conventions thus jobs with names that begin with the same word likely perform similar computation the small number of computation types represent targets for static or even manual optimization this will greatly simplify work load management problems such as predicting job duration or resource use and optimizing scheduling placement or task granularity each workload services only a small number of mapre duce frameworks hive pig oozie or similar layers on top of mapreduce figure shows that for all workloads two frameworks account for a dominant majority of jobs there is ongoing research to achieve well behaved multiplexing be tween different frameworks the data here suggests that multiplexing between two or three frameworks already cov ers the majority of jobs in all workloads here we believe this observation will remain valid in the future as new frameworks develop enterprise mapreduce users are likely to converge on an evolving but small set of mature frame works for business critical computations figure also shows that for hive in particular select and insert form a large fraction of activity for several work loads only the fb workload contains a large fraction of hive queries beginning with from unfortunately this information is not available for pig also we see evidence of some direct migration of established rdbms use cases such as etl extract transform load and edw enterprise data warehouse this information gives us some idea with regard to good targets for query optimization however more direct infor mation on query text at the hive and pig level will be even oozie others figure the first word of job names for each work load weighted by the number of jobs beginning with each word top total i o in bytes middle and map reduce task time bottom for example of jobs in the fb workload have a name beginning with ad a further begin with insert of all i o and of total task time comes from jobs with names that begin with from middle and bottom the fb trace did not contain job names e t y b f o n io t c a r f b o j f o n o i t c a r f e im t k a t f o n o i t c a r f others others select others insert others others hourly hyperlocaldatae xtractor others select ad flow distcp parallel select etl cascade columnset flow bmdailyjob twitch identifier piglatin fb cc a cc b cc c cc d cc e identifier snapshot piglatin others others others insert others others others iteminquiry esb search si select ajax select item edw tr edw snapshot importjob queryresult snapshot edwsequence twitch edwsequence fb cc a cc b cc c cc d cc e others select others others others others distcp select listing others snapshot default identifier columnset stage flow etl bmdailyjob bmdailyjob semi twitch click snapshot flow identifier fb cc a cc b cc c cc d cc e insert insert insert from from from ad hive piglatin metrodataextra ctor tr piglatin piglatin select tr insert oozie snapshot select insert insert oozie piglatin pig select oozie flow piglatin bmdailyjob piglatin insert piglatin flow sywr tr tr insert insert also more beneficial for workflow management frameworks such blurs the definition of a straggler if the only task of as oozie it will be benefitial to have uuids to identify jobs a job runs slowly it becomes impossible to tell whether the belonging to the same workflow for native mapreduce task is inherently slow or abnormally slow the importance jobs it will be desirable for the job names to contain a uni of stragglers as a problem also requires re assessment any form convention of pre and postfixes such as dates com stragglers will seriously hamper jobs that have a single wave putation types steps in multi stage processing etc ob of tasks however if it is the case that stragglers occur ran taining information at that level will help translate insights domly with a fixed probability fewer tasks per job means from multi operator rdbms query execution planning to only a few jobs would be affected we do not yet know optimize multi job mapreduce workflows whether stragglers occur randomly interestingly map functions in some jobs aggregate data by multi dimensional job behavior another way to group jobs is by their multi dimensional behavior each job can be represented as a six dimensional vector described by input size shuffle size output size job duration map task time and reduce task time one way to group similarly behaving jobs is to find clusters of vectors close to each other in the six dimensional space we use a standard data clustering algorithm k means k means enables quick analysis of a large number of data points and facilitates intuitive labeling and interpretation of cluster cen ters we use a standard technique to choose k the number of job type clusters for each workload increment k until there is diminishing return in the decrease of intra cluster variance i e residual variance our previous work contains additional details of this methodology table summarizes our k means analysis results we have assigned labels using common terminology to describe the one or two data dimensions that separate job categories within a workload a system optimizer would use the full numerical descriptions of cluster centroids we see that jobs touching of total data make up of all jobs these jobs are capable of achieving in teractive latency for analysts i e durations of less than a minute the dominance of these jobs counters prior assump tions that mapreduce workloads consist of only jobs at tb scale and beyond the observations validate research efforts to improve the scheduling time and the interactive capability of large scale computation frameworks the dichotomy between very small and very large job has been identified previously for workload management of busi ness intelligence queries drawing on the lessons learned there poor management of a single large job potentially im pacts performance for a large number of small jobs the small big job dichotomy implies that the cluster should reduce functions in other jobs expand data and many jobs contain data transformations in either stage such data ra tios reverse the original intuition behind map functions as expansions i e maps and reduction functions as aggre gates i e reduces also map only jobs appear in all but two workloads they form to of all bytes and to of all task times in their respective workloads some are oozie launcher jobs and others are maintenance jobs that oper ate on very little data compared with other jobs map only jobs benefit less from datacenter networks optimized for shuffle patterns further fb and cc c both contain jobs that handle roughly the same amount of data as others but take consid erably longer to complete versus jobs in the same workload with comparable data sizes fb contains a job type that consumes only of gb of data but requires days to complete map only transform days these jobs have inherently low levels of parallelism and cannot take advan tage of parallelism on the cluster even if spare capacity is available comparing the fb and fb workloads in table shows that job types at facebook changed significantly over one year the small jobs remain and several kinds of map only jobs remain however the job profiles changed in sev eral dimensions thus for facebook any policy parameters need to be periodically revisited combined the analysis once again reveals the diversity across workloads even though small jobs dominate all seven workloads they are small in different ways for each work load further the breadth of job shape size and durations across workloads indicates that microbenchmarks of a hand ful of jobs capture only a small sliver of workload activity and a truly representative benchmark will need to involve a much larger range of job types be split into two tiers there should be a performance tier which handles the interactive and semi streaming com putations and likely benefits from optimizations for interac tive rdbms systems and a capacity tier which nec essarily trades performance for efficiency in using storage and computational capacity the capacity tier likely as sumes batch like semantics one can view such a setup as analogous to multiplexing oltp interactive transactional and olap potentially batch analytical workloads it is important to operate both parts of the cluster while simul taneously achieving performance and efficiency goals the dominance of small jobs complicates efforts to rein in stragglers tasks that execute significantly slower than other tasks in a job and delay job completion compar ing the job duration and task time columns indicate that towards a big data benchmark in light of the broad spectrum of industrial data presented in this paper it is natural to ask what implications we can draw with regard to building a tpc style benchmark for mapreduce and similar big data systems the work loads here are sufficient to characterize an emerging class of mapreduce workloads for interactive and semi streaming analysis however the diversity of behavior across the work loads we analyzed means we should be careful when deciding which aspects of this behavior are representative enough to include in a benchmark below we discuss some challenges associated with building a tpc style benchmark for mapre duce and other big data systems small jobs contain only a handful of small tasks sometimes data generation a single map task and a single reduce task having few the range of data set sizes skew in access frequency and comparable tasks makes it difficult to detect stragglers and temporal locality in data access all affect system perfor jobs input shuffle output duration map time reduce time label cc a mb mb sec small jobs gb gb gb min 100 410 transform tb 27 gb hrs min map only huge gb gb mb hrs min 181 transform and aggregate cc b kb kb sec small jobs gb gb gb min 392 transform small gb gb gb min 389 transform medium tb mb mb min aggregate and transform gb gb mb hrs min 092 aggregate cc c gb gb 200 mb min small jobs tb tb gb min 462 transform light reduce gb gb mb hrs min 930 aggregate tb tb tb min 886 transform heavy reduce gb gb gb hrs min 865 aggregate large tb gb gb hrs 871 long jobs tb gb gb hrs min 758 aggregate huge cc d gb mb mb sec 376 085 small jobs gb tb gb min 692 expand and aggregate gb tb gb min 011 transform and aggregate tb tb tb min expand and transform gb gb gb min 259 aggregate cc e mb 970 kb sec small jobs gb gb gb min transform large gb gb gb hrs 745 transform very large tb 200 b min map only summary tb gb tb hrs min map only transform fb kb 871 kb 0 small jobs kb 0 gb min 079 0 load data fast kb 0 gb hr min 0 load data slow kb 0 447 gb hr min 0 load data large kb 0 tb hrs min 0 load data huge gb gb mb min 760 aggregate fast tb mb gb min 76 aggregate and expand gb tb gb hr min 974 expand and aggregate gb gb 6 gb min 338 data transform 6 tb gb kb min 843 data summary fb 6 mb 600 b kb min small jobs gb 0 gb hrs 664 0 map only transform hrs 6 tb 0 tb min 081 0 map only transform min tb 0 gb hr min 0 map only aggregate gb 0 gb days 0 map only transform days tb gb gb min 112 387 aggregate gb 6 tb gb hrs transform hrs 0 tb tb tb hr 818 aggregate and transform tb tb gb hrs min 862 678 expand and aggregate 630 gb tb gb hrs 220 174 transform hrs table job types in each workload as identified by k means clustering with cluster sizes centers and labels map and reduce time are in task seconds i e a job with map tasks of seconds each has map time of task seconds note that the small jobs dominate all workloads mance a good benchmark should stress the system with realistic conditions in all these areas consequently a bench mark needs to pre generate data that accurately reflects the complex data access patterns of real life workloads processing generation the analysis in this paper reveals challenges in accurately generating a processing stream that reflects real life work loads such a processing stream needs to capture the size shape and sequence of jobs as well as the aggregate clus ter load variation over time it is non trivial to tease out the dependencies between various features of the processing stream and even harder to understand which ones we can omit for a large range of performance comparison scenarios mixing mapreduce and query like frameworks the heavy use of query like frameworks on top of mapre duce indicates that future cluster management systems need to efficiently multiplex jobs both written in the native mapre duce api and from query like frameworks such as hive pig and hbase thus a representative benchmark also needs to include both types of processing and multiplex them in re alistic mixes scaled down workloads the sheer data size involved in the workloads means that it is economically challenging to reproduce workload behav ior at production scale one can scale down workloads pro portional to cluster size however there are many ways to describe both cluster and workload size one could normal ize workload size parameters such as data size number of jobs or the processing per data against cluster size param eters such as number of nodes cpu capacity or available memory it is not clear yet what would be the best way to scale down a workload empirical models the workload behaviors we observed do not fit any well known statistical distributions the single exception being zipf distribution in data access frequency it is necessary for a benchmark to assume an empirical model of workloads i e the workload traces are the model this is a departure from some existing tpc benchmarking approaches where the targeted workload are such that some simple models can be used to generate data and the processing stream a true workload perspective the data in the paper indicate the shortcomings of mi crobenchmarks that execute a small number of jobs one at a time they are useful for diagnosing subcomponents of a system subject to very specific processing needs a big data benchmark should assume the perspective already reflected in tpc and treat a workload as a steady process ing stream involving the superposition of many processing types workload suites the workloads we analyzed exhibit a wide range of be havior if this diversity is preserved across more workloads we would be compelled to accpet that no single set of be haviors are representative in that case we would need to identify as small suite of workload classes that cover a large range of behavior the benchmark would then consist not of a single workload but a workload suite systems could trade optimized performance for one workload type against more average performance for another a stopgap tool we have developed and deployed statistical workload in jector for mapreduce https github com swimproject ucb swim wiki this is a set of new bsd licensed work load replay tools that partially address the above challenges the tools can pre populate hdfs using uniform synthetic data scaled to the number of nodes in the cluster and replay the workload using synthetic mapreduce jobs the work load replay methodology is further discussed in the swim repository already includes scaled down versions of the fb and fb workloads cloudera has allowed us to contact the end customers directly and seek permission to make public their traces we hope the replay tools can act as a stop gap while we progress towards a more thorough benchmark and the workload repository can contribute to a scientific approach to designing big data systems such as mapreduce 8 summary and conclusions to summarize the analysis results we directly answer the questions raised in section the observed behavior spans a wide range across workloads as we detail below for optimizing the underlying storage system skew in data accesses frequencies range between an and 8 rule temporal locality exists and of data re accesses occur on the range of minutes to hours for workload level provisioning and load shaping the cluster load is bursty and unpredictable peak to median ratio in cluster load range from to for job level scheduling and execution planning all workloads contain a range of job types with the most common being small jobs these jobs are small in all dimensions compared with other jobs in the same workload they involve of kb to gb of data exhibit a range of data patterns between the map and reduce stages and have durations of of seconds to a few minutes the small jobs form over of all jobs for all work loads the other job types appear with a wide range of frequencies for optimizing query like programming frameworks the cluster load that comes from these frameworks is up to and at least the frameworks are generally used for interactive data exploration and semi streaming analysis for hive the most commonly used operators are insert and select from is frequently used in only one workload additional tracing at the hive pig hbase level is required for performance comparison between systems a wide variation in behavior exists between work loads as the above data indicates there is sufficient diversity between workloads that we should be cautious in claiming any behavior as typical additional workload studies are required the analysis in this paper has several repercussions mapreduce has evolved to the point where performance claims should be qualified with the underlying workload assumptions e g by replaying a suite of workloads system engineers should regularly re assess design priorities subject to evolving use cases prerequisites to these efforts are workload replay tools and a public workload repository so that engineers can share insights across different enter prise mapreduce deployments future work should seek to improve analysis and mon itoring tools enterprise mapreduce monitoring tools should perform workload analysis automatically present gra phical results in a dashboard and ship only the anonymized and aggregated metrics for workload comparisons offsite most importantly tracing capabilities at the hive pig and hbase level should be improved an analysis of query text at that level will reveal further insights and expedite trans lating rdbms knowledge to optimize mapreduce and solve real life problems involving large scale data improved tools will facilitate the analysis of more work loads over longer time periods and for additional statistics this improves the quality and generality of the derived de sign insights and contributes to the overall efforts to identify common behavior the data in this paper indicate that we need to look at a broader range of use cases before we can build a truly representative big data benchmark we invite cluster operators and the broader data manage ment community to share additional knowledge about their mapreduce workloads to contribute retain the job history logs generated by existing hadoop tools run the tools at https github com swimprojectucb swim wiki analyz e historical cluster traces and synthesize represe ntative workload and share the results abstract record linkage is the process of matching records from several databases that refer to the same entities when applied on a single database this process is known as deduplication increasingly matched data are becoming important in many application areas because they can contain information that is not available otherwise or that is too costly to acquire removing duplicate records in a single database is a crucial step in the data cleaning process because duplicates can severely influence the outcomes of any subsequent data processing or data mining with the increasing size of today databases the complexity of the matching process becomes one of the major challenges for record linkage and deduplication in recent years various indexing techniques have been developed for record linkage and deduplication they are aimed at reducing the number of record pairs to be compared in the matching process by removing obvious nonmatching pairs while at the same time maintaining high matching quality this paper presents a survey of variations of 6 indexing techniques their complexity is analyzed and their performance and scalability is evaluated within an experimental framework using both synthetic and real data sets no such detailed survey has so far been published index terms data linkage data matching entity resolution index techniques blocking experimental evaluation scalability ç i ntroduction a s projects many businesses collect the author is with the research school of computer science college of engineering and computer science the australian national university csit building canberra act australia e mail peter christen anu edu au manuscript received sept revised mar accepted may published online 7 june recommended for acceptance by c clifton for information on obtaining reprints of this article please send e mail to tkde computer org and reference ieeecs log number tkde digital object identifier no 1109 tkde 4347 00 ß ieee published by the ieee computer society government agencies and research increasingly large amounts of data techniques that allow efficient processing analyzing and mining of such massive databases have in recent years attracted interest from both academia and industry one task that has been recognized to be of increasing importance in many application domains is the matching of records that relate to the same entities from several databases often information from multiple sources needs to be integrated and combined in order to improve data quality or to enrich data to facilitate more detailed data analysis the records to be matched frequently correspond to entities that refer to people such as clients or customers patients employees tax payers students or travelers the task of record linkage is now commonly used for improving data quality and integrity to allow reuse of existing data sources for new studies and to reduce costs and efforts in data acquisition in the health sector for example matched data can contain information that is required to improve health policies information that traditionally has been collected with time consuming and expensive survey methods linked data can also help in health surveillance systems to enrich data that are used for the detection of suspicious patterns such as outbreaks of contagious diseases statistical agencies have employed record linkage for several decades on a routinely basis to link census data for further analysis many businesses use deduplication and record linkage techniques with the aim to deduplicate their databases to improve data quality or compile mailing lists or to match their data across organizations for example for collaborative marketing or e commerce projects many government organizations are now increasingly employing record linkage for example within and between taxation offices and departments of social security to identify people who register for assistance multiple times or who work and collect unemployment benefits other domains where record linkage is of high interest are fraud and crime detection as well as national security security agencies and crime investigators increasingly rely on the ability to quickly access files for a particular individual under investigation or cross check records from disparate databases which may help to prevent crimes and terror by early intervention the problem of finding records that relate to the same entities not only applies to databases that contain informa tion about people other types of entities that sometimes need to be matched include records about businesses consumer products publications and bibliographic cita tions web pages web search results or genome sequences in bioinformatics for example record linkage techniques can help find genome sequences in large data collections that are similar to a new unknown sequence in the field of information retrieval it is important to remove duplicate documents such as web pages and bibliographic citations in the results returned by search engines in digital libraries or in automatic text indexing systems 6 7 another application of growing interest is finding and comparing consumer products from different online stores because product descriptions are often slightly varying matching them becomes challenging 8 in situations where unique entity identifiers or keys are available across all the databases to be linked the problem of ieee transactions on knowledge and data engineering vol no september fig outline of the general record linkage process the indexing step the topic of this survey generates candidate record pairs while the output of the comparison step is vectors containing numerical similarity values matching records at the entity level becomes trivial a simple database join is all that is required however in most cases no such unique identifiers are shared by all databases and more sophisticated linkage techniques are required these techniques can be broadly classified into deterministic probabilistic and learning based approaches while statisticians and health researchers commonly name the task of matching records as data or record linkage the term used in this paper the computer science and database communities refer to the same process as data or field matching data integration data scrubbing or cleaning 14 data cleansing duplicate detection information integration entity resolution reference reconciliation or as the merge purge problem in commercial processing of business mailing lists and customer databases record linkage is usually seen as a component of etl extraction transformation and loading tools two recent surveys have provided overviews of record linkage and deduplication techniques and chal lenges the record linkage process fig outlines the general steps involved in the linking of two databases because most real world data are dirty and contain noisy incomplete and incorrectly formatted infor mation a crucial first step in any record linkage or deduplication project is data cleaning and standardization it has been recognized that a lack of good quality data can be one of the biggest obstacles to successful record linkage the main task of data cleaning and standardiza tion is the conversion of the raw input data into well defined consistent forms as well as the resolution of inconsistencies in the way information is represented and encoded the second step indexing is the topic of this survey and will be discussed in more detail in section the indexing step generates pairs of candidate records that are compared in detail in the comparison step using a variety of comparison functions appropriate to the content of the record fields attributes approximate string comparisons which take typographical variations into account are commonly used on fields that for example contain name and address details while comparison functions specific for date age and numerical values are used for fields that contain such data several fields are normally compared for each record pair resulting in a vector that contains the numerical similarity values calculated for that pair using these similarity values the next step in the record linkage process is to classify the compared candidate record pairs into matches nonmatches and possible matches depending upon the decision model used 27 record pairs that were removed in the indexing step are classified as nonmatches without being compared explicitly the majority of recent research into record linkage has concentrated on improving the classification step and various classification techniques have been developed many of them are based on machine learning approaches 31 if record pairs are classified into possible matches a clerical review process is required where these pairs are manually assessed and classified into matches or nonmatches this is usually a time consuming cumbersome and error prone process especially when large databases are being linked or deduplicated measuring and evaluating the quality and complexity of a record linkage project is a final step in the record linkage process contributions while various indexing techniques for record linkage and deduplication have been developed in recent years so far no thorough theoretical or experimental survey of such techni ques has been published earlier surveys have compared four or less indexing techniques only it is therefore currently not clear which indexing technique is suitable for what type of data and what kind of record linkage or deduplication application the aim of this survey is to fill this gap and provide both researchers and practitioners with information about the characteristics of a variety of indexing techniques including their scalability to large data sets and their performance for data with different characteristics the contributions of this paper are a detailed discussion of six indexing techniques with a total of variations of them a theoretical analysis of their complexity and an empirical evaluation of these techniques within a common framework on a variety of both real and synthetic data sets the reminder of this paper is structured as follows in the following section the indexing step of the record linkage process is discussed in more detail the six indexing techniques are then presented in section followed by their experimental evaluation in section the results of these experiments are discussed in section an overview of related work is then provided in section 6 and the paper is concluded in section 7 with an outlook to future challenges and work in this area i ndexing for r ecord l inkage and d eduplication when two databases a and b are to be matched potentially each record from a needs to be compared with every record from b resulting in a maximum number of jajâjbj comparisons between two records with jáj denoting the number of records in a database similarly when dedupli cating a single database a the maximum number of possible christen a survey of indexing techniques for scalable record linkage and deduplication table example records and blocking keys comparisons is jaj â ðjaj à because each record in a potentially needs to be compared with all other records the performance bottleneck in a record linkage or deduplication system is usually the expensive detailed comparison of field attribute values between records making the naıve approach of comparing all pairs of records not feasible when the databases are large for example the matching of two databases with million records each would result one trillion possible record pair comparisons at the same time assuming there are no duplicate records in the databases to be matched i e one record in a can only be a true match to one record in b and vice versa then the maximum possible number of true matches will correspond to minðjaj jbjþ similarly for a deduplication the number of unique entities and thus true matches in a database is always smaller than or equal to the number of records in it therefore while the computational efforts of comparing records increase quadratically as databases are getting larger the number of potential true matches only increases linearly in the size of the databases given this discussion it is clear that the vast majority of comparisons will be between records that are not matches the aim of the indexing step is to reduce this large number of potential comparisons by removing as many record pairs as possible that correspond to nonmatches the traditional record linkage approach has employed an indexing technique commonly called blocking which splits the databases into nonoverlapping blocks such that only records within each block are compared with each other a blocking criterion commonly called ablocking key the term used in this paper is either based on a single record field attribute or the concatenation of values from several fields because real world data are often dirty and contain variations and errors 34 an important criteria for a good blocking key is that it can group similar values into the same block what constitutes a similar value depends upon the characteristics of the data to be matched similarity can refer to similar sounding or similar looking values based on phonetic or character shape characteristics for strings that contain personal names for example phonetic similarity can be obtained by using phonetic encoding functions such as soundex nysiis or double metaphone these func tions which are often language or domain specific are applied when the blocking key values bkvs are generated as an example table shows three different blocking keys and the resulting bkvs for four records the first one is made of soundex sndx encoded givenname gin values concatenated with full postcode pc values the second consists of the first two digits of postcode values how the blocking key values are generated is detailed in section the two highlighted bold pairs of bkvs illustrate that these records would be inserted into the same blocks concatenated with double metaphone dme encoded sur name surn values and the third is made of soundex encoded suburb name subn values concatenated with the last two digits of postcode values to illustrate the two components of each blocking key in table their values are separated by a hyphen however in real world applications they would be concatenated directly several important issues need to be considered when record fields are selected to be used as blocking keys the first issue is that the quality of the values in these fields will influence the quality of the generated candidate record pairs ideally fields containing the fewest errors variations or missing values should be chosen any error in a field value used to generate a bkv will potentially result in records being inserted into the wrong block thus leading to missing true matches one approach used to overcome errors and variations is to generate several blocking keys based on different record fields as is illustrated in table the hope is that records that refer to true matches have at least one bkv in common and will therefore be inserted into the same block a second issue that needs to be considered when defining blocking keys is that the frequency distribution of the values in the fields used for blocking keys will affect the size of the generated blocks often this will be the case even after phonetic or other encodings have been applied for example a field containing surnames in a database from the united kingdom us or australia will likely contain a large portion of records with the value smith which will results in a similarly large portion of records with the corresponding soundex encoding if m records in database a and n records in database b have the same bkv then m â n candidate record pairs will be generated from the corresponding block the largest blocks generated in the indexing step will dominate execution time of the compar ison step because they will contribute a large portion of the total number of candidate record pairs therefore it is of advantage to use fields that contain uniformly distributed values because they will result in blocks of equal sizes when blocking keys are defined there is also a tradeoff that needs to be considered on one hand having a large number of smaller blocks will result in fewer candidate record pairs that will be generated this will likely increase the number of true matches that are missed on the other hand blocking keys that result in larger blocks will generate an increased number of candidate record pairs that likely will cover more true matches at the cost of having to compare more candidate pairs as will be discussed in the following section some indexing techniques do allow explicit control of the size of the blocks that will be generated 1540 ieee transactions on knowledge and data engineering vol no september while for others the block sizes depend upon the character istics of the record fields used in blocking keys all indexing techniques discussed in the following section do require some form of blocking key to be defined the question of how to optimally choose record fields for blocking keys such that as many true matching pairs as possible are included in the set of candidate record pairs is orthogonal to the selection of an actual indexing technique traditionally blocking keys have been manually selected by domain experts according to their understanding of the databases to be matched or based on initial data exploration steps conducted in order to achieve an indexing that generates candidate record pairs of good quality many recently developed indexing techniques require various parameters to be set the optimal values of these parameters depend both upon the data to be matched such as distribution of values and error characteristics as well as the choice of blocking key used this makes it often difficult in practice to achieve a good indexing because time consuming manual parameter tuning followed by test linkages and careful evaluation is required additionally in many real world record linkage or deduplication applications no data that contain the known true match status of record pairs are available that can be used to assess linkage quality therefore it is often not known how many true matches are included in the set of candidate record pairs measures suitable for assessing record linkage quality and complexity will be discussed in section ideally an indexing technique for record linkage and deduplication should be robust with regard to the selected parameter values or not require parameters at all which would allow automated indexing i ndexing t echniques in this section the traditional blocking approach and five more recently developed indexing techniques and variations of them are discussed in more detail their complexity is analyzed as the estimated number of candidate record pairs that will be generated knowing this number together with a measured average time per record pair comparison shown in table will allow an estimate of the runtime of the comparison step given this step is often the most time consuming step in a record linkage or deduplication project such estimates will help users to predict how long a certain linkage or deduplication project will take the estimated number of candidate record pairs will be calculated for two different frequency distributions of bkvs the first assumes a uniform distribution of values resulting in each block containing the same number of records the fig example records with surname values and their soundex encodings used as bkvs and the corresponding inverted index data structure as used for traditional blocking second assumes that the frequencies of the bkvs follow zipf law a frequency distribution that is commonly found in data sets that contain values such as personal names 38 zipf law states that in a list of words ranked according to their frequencies the word at rank r has a relative frequency that corresponds to r for attributes such as postcode suburb name or age the frequency distribution of their values is likely somewhere between a uniform and a zipf like frequency distribution therefore assuming these two distributions should provide lower and upper limits of the number of candidate record pairs that can be expected when linking or deduplicating real world databases conceptually the indexing step of the record linkage process can be split into the following two phases build all records in the database or databases are read their bkvs are generated and records are inserted into appropriate index data structures for most indexing techniques an inverted index can be used the bkvs will become the keys of the inverted index and the record identifiers of all records that have the same bkv will be inserted into the same inverted index list fig illustrates this for a small example data set when linking two databases either a separate index data structure is built for each database or a single data structure with common key values is generated for the second case each record identifier needs to include a flag that indicates from which database the record originates the field values required in the comparison step need to be inserted into another data structure that allows efficient access to single random records when they are required for field comparisons this can be achieved using an appropriately indexed database or hash table retrieve for each block its list of record identifiers is retrieved from the inverted index and candidate record pairs are generated from this list for a record linkage all records in a block from one database will be paired with all records from the block with the same bkv from the other database while for a deduplica tion each record in a block will be paired with all other records in the same block for example from the block with key from fig the pairs and r7 will be generated the candidate record pairs are then compared in detail in the comparison step and the resulting vectors containing numerical similarity values are given to a classifier in the classification step christen a survey of indexing techniques for scalable record linkage and deduplication this survey mainly considers the build phase namely how different indexing techniques using the same blocking key definition are able to index records from data sets with different characteristics and how this in combination with various parameter settings affects the number and quality of the candidate record pairs generated the specific questions of interest are how many candidate pairs are generated and how many of them are true matches and how many true nonmatches the following notation will be used when discussing the complexity of indexing techniques n a jaj and n b jbj are the number of records in databases a and b respectively for simplicity it is assumed that only one blocking key definition is used and that the bkvs generated for both databases are the same i e if k a and k b are the sets of bkvs generated from a and b then k a k b while this is a rare scenario in most real world applications it provides an upper bound on the number of candidate record pairs because any bkv that is only generated by one of the two databases will not result in any candidate record pairs the number of different bkvs is denoted as b with b jk a b j and jáj denoting the number of elements in a set traditional blocking this technique has been used in record linkage since the all records that have the same bkv are inserted into the same block and only records within the same block are then compared with each other each record is inserted into one block only assuming a single blocking key definition as illustrated in fig traditional blocking can be implemented efficiently using a standard inverted index as described in the build phase above in the retrieve phase the identifiers of all records in the same block are retrieved and the corresponding candidate record pairs are generated while traditional blocking does not have any explicit parameters the way blocking keys are defined will influence the quality and number of candidate record pairs that are generated as discussed in section a major drawback of traditional blocking is that errors and variations in the record fields used to generate bkvs will lead to records being inserted into the wrong block this drawback can be over come by using several blocking key definitions based on different record fields or different encodings applied on the same record fields a second drawback of traditional blocking is that the sizes of the blocks generated depend upon the frequency distribution of the bkvs and thus it is difficult in practice to predict the total number of candidate record pairs that will be generated if a uniform distribution of field values is assumed that leads to uniformly distributed bkvs then all blocks will be of uniform size and contain n a b or n b b records respectively with b being the number of different bkvs in this situation the number of candidate record pairs generated for a record linkage equals u tbrl b â n b a â n b b n a b n b while for a deduplication the number of candidate record pairs generated equals u tbd b â n b a â n b a à n a n a b à for both situations this corresponds to a b fold reduction in the number of candidate pairs compared to the naıve approach of comparing all records with each other if a zipf frequency distribution of record field values is assumed that leads to zipf like distribution of bkvs then the size of the generated blocks will also follow a zipf like frequency distribution with b blocks the number of candidate record pairs generated for a record linkage in this situation equals z tbrl b â i h b â n a i h b â n b with h b n a n b b â b sum candidate h b being record p b the i pairs harmonic for generated a number of the partial harmonic deduplication equals the number of z tbd b â i h b â n a i h b â n a à a b b n a b h b â à i for a given number of blocks and a database or databases of a given size having blocks of uniform size will lead to the smallest number of candidate records pairs generated compared to any nonuniform distribution for example for two equal sized blocks each containing x records the number of candidate record pairs generated will equal â changing the distribution to blocks containing ðx þ and ðx à records respectively will result in ðx þ þ ðx à þ þ à þ â þ therefore every redistribution away from uniform block sizes will increase the number of candidate record pairs generated from this follows that u tbrl z tbrl and u tbd z tbd for the same number of blocks and same number of records in the database to be linked or deduplicated sorted neighborhood indexing this technique was first proposed in the mid its basic idea is to sort the database according to the bkvs and to sequentially move a window of a fixed number of recordsw w over the sorted values candidate record pairs are then generated only from records within a current window as illustrated in figs and there are two different approaches of how this technique can be implemented sorted array based approach in this first approach as originally proposed the bkvs are inserted into an array that is sorted alphabetically as shown in the left hand side of fig the window is then moved over this sorted array and candidate record pairs are generated from all records in the current window as illustrated in the right hand side of fig in case of a record linkage the bkvs from both databases will be 1542 ieee transactions on knowledge and data engineering vol no september inserted into one combined array and then sorted alphabe tically but candidate record pairs are generated in such a way that for each pair one record is selected from each of the two databases for a record linkage assuming the length of the sorted array is ðn a þ n b þ the total number of records in both databases then the number of window positions equals ðn a þ n b à w þ while for a deduplication the number of windows is ðn a à w þ as can be seen in the right hand side of fig most candidate record pairs are generated in several windows however each unique pair will only be compared once in the comparison step because the window size is fixed in this approach the number of candidate record pairs generated is independent of the frequency distribution of the bkvs and only depends upon the window size w and the size of the database if n a ðn a þ n b þ denotes the ratio of the number of records in database a over the number of records in both databases and n b ðn a þ n b à þ the corresponding ratio for database b then for a record linkage the number of unique candidate record pairs generated equals u snrl sa ð wþð wþþðn a þ n b à wþ â ð ððw à þ þ ððw à þþ þ ðn a þ n b à wþðw à þ a þ n b à wþðw à ðn a n a þ n n b b þ â þ a þ n b à wþðw à the first term in the first line equals to the number of candidate record pairs that are generated in the first window position while the remainder of the equation equals to the number of unique pairs generated in the remaining ðn a þ n b à wþ window positions assuming evenly mixed bkvs from a and b in the first window position there will be w records from database a that have to be compared to w records from database b for all following window positions with a likelihood of the newest record added to fig example sorted neighborhood technique based on an inverted index and with the same bkvs and window size as in fig fig example sorted neighborhood technique based on a sorted array with bkvs being the surname values from fig and the corresponding record identifiers and a window size w the window originates from database a and has to be compared with ðw à records in the current window that are from database b a similar calculation the term ððw à þ can be made when the newest record in a window originates from database b the total number of candidate record pairs generated depends quadratically upon the window size w and on the harmonic mean of the sizes of the two databases that are linked for a deduplication the number of unique candidate pairs generated duplicate pairs not counted equals u snd sa wðw à þ ðn a à ðw à n a w wþðw à à for both a record linkage and a deduplication the number of candidate record pairs generated is independent of the frequency distribution of the bkvs and therefore no analysis of zipf distributed values is required a major drawback of this approach is that if a small window size is chosen it might not be large enough to cover all records that have the same bkv for example there might be several thousand records with a surname value smith in a large database but with a window size of for example w not all of these records will be in the same current window and thus not all of them will be compared with each other one solution to this problem is to select blocking keys that are made of the concatenation of several record fields like surname and given name so that they have a large number of different values rather than employing encoding functions that group many similar field values together another problem with this approach is that the sorting of the bkvs is sensitive toward errors and variations in the first few positions of values for example if given names are used as bkvs then christina and kristina will very likely be too far away in the sorted array to be inserted into the same window even though they are very similar names and might refer to the same person this drawback can be overcome by employing several blocking key definitions based on different record fields or by defining blocking christen a survey of indexing techniques for scalable record linkage and deduplication keys based on reversed field values for example anitsirhc and anitsirk the build phase for this indexing approach also requires the sorting of the array which has a complexity of oðn log nþ with n ðn a þ n b þ for a record linkage and n n a for a deduplication inverted index based approach an alternative approach for the sorted neighborhood technique is illustrated in fig rather than inserting bkvs into a sorted array this approach utilizes an inverted index similar to traditional blocking the index keys contain the alphabetically sorted bkvs as is shown in the left hand side of fig the window is moved over these sorted bkvs and candidate record pairs are formed from all records in the corresponding index lists as illustrated in the right hand side of fig similar to the sorted array based approach most candidate record pairs are generated in several windows but each unique candidate pair will again only be compared once in the comparison step the number of generated candidate record pairs with this approach depends upon the number of record identifiers that are stored in the inverted index lists for a window size w this inverted index based approach reduces to traditional blocking as described in section for all window sizes w the generated candidate record pairs will therefore be a superset of the pairs generated by traditional blocking in general for two window sizes w i and w j with w i w j all candidate record pairs generated with window size w i will also be in the set of pairs generated with w j however the larger the window size is the larger the number of generated candidate record pairs becomes the number of window positions with this approach for both a record linkage and a deduplication is ðb à w þ with b being the number of different bkvs for a record linkage record identifiers from both databases will be inserted into a common inverted index data structure together with a flag stating if a record originates from database a or b assuming a uniform distribution of bkvs each inverted index list will contain n a b þ n b b record identifiers the number of unique candidate record pairs generated for a record linkage is u snrl ii w n b a â w n b b þ ðb à wþ â n a b â w n b b þ n b b â ðw à n b a n a n 4 n a n b b þ ðb à à n a n b þ ðb à à the first term in 7 corresponds to the number of candidate record pairs generated in the first window position while the second term corresponds to the ðb à wþ following window positions the first part of this second term refers to the candidate pairs that are generated between the record identifiers in the most recently added inverted index list in the current window that come from database a and the identifiers in the previous lists from database b while the second part refers to the pairs that are generated between the newest list from the index of database b and the previous lists from the index of database a if the window size is set to w 4 the above formula reduces to generating the same number of candidate record pairs as with traditional blocking because the term þ ðb à à 4 þ ðb à à 4 þ ðb à 4 b for a deduplication the number of unique candidate pairs generated equals u snd ii 4 w n b a â w n b a à þ ðb à wþ â n a b n a b 4 w â ðw à n b a þ n b a à w þ ðb à wþ â as can easily be verified with w 4 the above formula reduces to for bkvs that have a zipf like frequency distribution calculating the number of candidate record pairs is difficult because this number will depend upon the ordering of the inverted index lists in the worst case scenario the size of the inverted index lists in number of record identifiers they contain corresponds to the alphabetically sorted bkvs such that the longest list is the alphabetically first the second longest the alphabetically second and so on as a result the first window would contain the largest number of record identifiers following and 7 the number of unique candidate record pairs generated in this worst case scenario for a record linkage equals z snrl ii n a â a n b a à ðw à þ n a n a b à â þ 4 n a w h b i n b w h b i þ n a h b n b h b using 4 and 8 for a deduplication the number of candidate record pairs can be calculated as z snd ii ðj þ w à i n b h b n a ðj þ w à h b i â 4 þ n a w n a w h b i h b i à þ n a h b n a ðj þ w à h b i b ðj n þ a w à â h b ðj þ n a w à à the inverted index based sorted neighborhood approach has two main disadvantages first similarly to traditional blocking the largest blocks will dominate the number of candidate record pairs that are generated and therefore also dominate the time requirements of the comparison step the second disadvantage is that the sorting of the bkvs assumes that their beginning is error free otherwise 1544 ieee transactions on knowledge and data engineering vol no september similar values will not be close enough in the sorted keys of the inverted index and might therefore not be covered in the same window as is recommended with the sorted array based approach it is therefore good practice to define several blocking keys ideally based on different record fields and run this indexing approach using each of these blocking keys similar to traditional blocking for this sorted neighborhood approach it will also be of advantage to use preprocessing like phonetic encodings to group similar record values into the same blocks i e convert them into the same bkvs assuming that the number of bkvs is much smaller than the number of records in the database to be matched or deduplicated i e b ðn a þ n b þ or b n a respectively the sorting of the bkvs will be much faster than for the sorted array based approach because the sorting step has a complexity of oðb log bþ adaptive sorted neighborhood approach recent research has looked at how the sorted neighborhood indexing technique based on a sorted array can be improved the issue of having a fixed block size w which can result in missed true matches because not all same bkvs fit into one window as discussed in section has been addressed through an adaptive approach to dynamically set the window size depending upon the characteristics of the bkvs used in the sorted array the idea is to find values adjacent to each other that are significantly different from each other using an appropriate string similarity measure these so called boundary pairs of bkvs are then used to form blocks i e they mark the positions in the sorted array where one window ends and a new one starts this approach can therefore be seen as a combination of traditional blocking and the sorted neighborhood approach due to the adaptive nature of the approach where block sizes are determined by the similarities between bkvs a theoretical analysis of the number of generated candidate record pairs would depend upon the actual bkvs and therefore not be of general use this adaptive approach will however be evaluated experimentally in section 4 another recently developed approach generalizes tradi tional blocking and the sorted neighborhood technique and combines them into a sorted blocks method the authors of this approach show that traditional blocking and sorted neighborhood are two ends of a general approach with blocking corresponding to sorted neighborhood where the window is moved forward w positions rather than resulting in nonoverlapping blocks the proposed combined fig q gram based indexing with surnames used as bkvs index key values based on bigrams q 4 2 and calculated using a threshold set to t 4 0 8 the right hand side shows three of the resulting inverted index lists blocks with the common bkv highlighted in bold in the index key value column approach allows specification of the desired overlap and experimental results presented show that the sorted neigh borhood approach performs better than traditional blocking especially for small blocks q gram based indexing the aim of this technique is to index the database such that records that have a similar not just the same bkv will be inserted into the same block the basic idea is to create variations for each bkv using q grams substrings of lengths q and to insert record identifiers into more than one block each bkv is converted into a list of q grams and sublist combinations of theseq gram lists are then generated down to a certain minimum length which is determined by a user selected threshold t t for a bkv that contains k q grams all sublist combinations down to a minimum length of l 4 bk â tcþ will be created bá á ác denotes rounding to the next lower integer number these sublists are then converted back into strings and used as the actual key values into an inverted index as is illustrated in fig different from the inverted index used in traditional blocking is that each record identifier is generally inserted into several index lists according to the number of q gram sublists generated for its bkv with a threshold t 4 0 however each record identifier will be inserted into one inverted index list only and in this case q gram based indexing will generate the same candidate record pairs as traditional blocking fig illustrates q gram based indexing for three exam ple records q 4 2 bigrams and a threshold t 4 0 8 the bkv smith in the first record for example contains four k 4 4 bigrams sm mi it th assuming all letters have been converted into lower case beforehand and so the length l of the shortest sublists for this value can be calculated as l 4 â 0 8c 4 therefore four sublists each containing three bigrams will be generated for this bkv mi it th sm it th sm mi th and sm mi it each of these is generated by removing one of the four original bigrams these sublists will then be converted back into strings to form the actual key values used in the inverted index as is shown in fig the identifier of the record will therefore be inserted into the five inverted index lists with key values smmiitth miitth smitth smmith and smmiit with an even lower threshold t 0 75 sublists of length two would be generated recursively from the sublists of length three christen a survey of indexing techniques for scalable record linkage and deduplication table 2 number of bigram q 4 2 sublists generated according to for different threshold values t and different number of q grams k in the bkvs the number of sublists generated for a bkv depends both upon the number of q grams it consists of as well as the value of the threshold t lower values of t will lead to an increased number of shorter sublists and therefore a larger number of different index key values the longer a bkv is the more sublists will be generated for a bkv of length c characters there will be k 4 ðc à q þ q grams and with l 4 bk â tcþ the length of the shortest sublists a total of 4 sublists will be generated from this bkv from table 2 it can be seen that the value of grows exponentially with longer bkvs and as the threshold t is set to lower values the time required to generate the q gram sublists will therefore be dominated by the recursive generation of sublists for longer bkvs fig 6 shows the length frequency distributions of values from three record fields that are common in many databases that contain information about people as can be seen these distributions roughly follow poisson distributions with parameter therefore assuming bkv lengths that follow a poisson distribution it is possible to estimate the overhead of q gram based indexing compared to traditional blocking let v denote the average number of times each record identifier is inserted into an inverted index list block compared to being inserted into just one index list as is done with traditional blocking i e v 4 assuming that the average length of bkvs in q grams is their maximum length is l max k k i and the minimum sublist length threshold is t then v can be calculated as v 4 l max leà l l 4maxð1 blâtcþ i l this summation covers bkvs of lengths to l max and for each of them the number of sublists generated for this specific length is calculated by combining a poisson distribution with assuming there are b different bkvs the question now is how many index key values denoted with are generated by the q gram sublist generation process this number depends upon the characteristics of the data specifically the distribution of unique q grams in the bkvs one extreme situation would be that every bkv generates a set of unique index key values that are not shared with any other bkv and thus 4 v â b the other extreme situation would be where every index key value is equal to an existing bkv and thus 4 b assuming uniform frequency distribution of the bkvs can be used to estimate the number of candidate record pairs that will be generated for a record linkage with the two extreme situations described above this number will be vb â n a v vb n b v vb u qgrl which can be simplified to n a â b â n a b v â n b b v n b v b u qgrl n a n b b similar for a deduplication 2 can be used to estimate the number of candidate record pairs as n a u qgd 2 v n b a à n a v n a 2 b for bkvs that follow a zipf frequency distribution and 4 can be extended similarly to calculate the estimated number of candidate record pairs as was shown in an earlier study q gram based indexing can lead to candidate record pairs that cover more true matches than both traditional blocking and the sorted neighborhood indexing techniques the drawback how ever is that a much larger number of candidate record pairs will be generated leading to a more time consuming comparison step this will be confirmed in the experiments in sections 4 and a similar q grams based approach to indexing has been proposed within a database framework using q gram based similarity joins and filtering techniques to improve performance this approach was implemented completely within a relational database and using sql statements by generating auxiliary database tables that contain the q grams and their record identifiers v à fig 6 normalized length frequency distributions of record field values from an australian telephone directory containing around seven million records for comparison a poisson distribution with 4 6 is also shown 1546 ieee transactions on knowledge and data engineering vol no september 4 suffix array based indexing this technique has recently been proposed as an efficient domain independent approach to multisource information integration the basic idea is to insert the bkvs and their suffixes into a suffix array based inverted index a suffix array contains strings or sequences and their suffixes in an alphabetically sorted order indexing based on suffix arrays has successfully been used on both english and japanese bibliographic databases in this indexing technique only suffixes down to a minimum length l m are inserted into the suffix array for example for a bkv christen and l m 4 the values christen hristen risten and isten will be gener ated and the identifiers of all records that have this bkv will be inserted into the corresponding four inverted index lists fig 7 shows several other examples of this approach a bkv of length c characters will result in ðc à l m þ suffixes to be generated similar to q gram based indexing the identifier of a record will likely be inserted into several inverted index lists to limit the maximum size of blocks and thus the number of candidate records pairs to be generated a second parameter b m allows the maximum number of record identifiers in a block to be set blocks that contain more that b m record identifiers will be removed from the suffix array for example in fig 7 with b m 4 the block with suffix value rina will be removed because it contains four record identifiers to calculate the number of candidate record pairs that will be generated with suffix array based indexing similar to q gram based indexing the lengths of the bkvs needs to be estimated assuming a poisson distribution of the length of bkvs and following the average number of suffixes generated from a bkv can be calculated as v 4 with l m l m 4l m leà l ðl à l m þ being the maximum and the average length both in characters of all bkvs in the database to be matched or deduplicated assuming there are b unique bkvs the minimum number of suffix values generated would be b in the extreme situation where all bkvs are of length l m characters and thus no shorter suffixes are generated the other extreme situation would occur when each bkv generates suffixes that are unique in this situation assuming each bkv in average generates v suffixes a total of v â b unique suffix values will 4 fig 7 suffix array based indexing with given names used as bkvs a tables on the right hand side show the resulting sorted suffix array the minimum suffix block with suffix length value rina l m the two will be removed because it contains more than b m record identifiers be generated with the maximum size of each block being b m the number of candidate record pairs generated can be estimated as b â m u sarl bv â m for a record linkage and for a deduplication as b â b m ðb m 2 à u sad b â vb m ðb 2 m à these estimates assume each block contains exactlyb m record identifiers in practice it is very unlikely this will occur and thus less record pairs will be generated as can be seen in fig 7 one problem with suffix array based indexing is that errors and variations at the end of bkvs will result in records being inserted into different blocks potentially missing true matches to overcome this draw back a modification of the suffix generation process is to not only generate the true suffixes of bkvs but all substrings down to the minimum lengths of l m in a sliding window fashion for example for the bkv christen and l m 4 this approach would generate the substrings christen christe hristen christ hriste risten chris hrist riste and isten this approach is similar to q gram based indexing as described in section 3 it can better overcome errors and variations at different positions in the bkvs at the costs of creating more blocks and inserting record identifiers into a larger number of blocks compared to the original suffix array technique this proposed variation will be evaluated experimentally in section 4 3 4 robust suffix array based indexing an improvement upon the original suffix array based indexing technique has recently been proposed the idea is similar to adaptive blocking in that the inverted index lists of suffix values that are similar to each other in the sorted suffix array are merged an approximate string similarity measure is calculated for all pairs of neighbor ing suffix values and if the similarity of a pair is above a selected threshold t then their lists are merged to form a new larger block for example using the given name suffix values from fig 7 the normalized edit distance string measure and a minimum similarity of t 4 0 85 then the following suffix string pairs and their corresponding record identifier lists will be merged into one block each atherina and atherine with similarity 0 875 and resulting in list catherina and catherine with similarity 0 889 and resulting in list and therina and therine 4 4 and a maximum block size b m christen a survey of indexing techniques for scalable record linkage and deduplication with similarity 0 857 and resulting in list r3 as will be shown in the experimental evaluation in section 4 this indexing technique can lead to improved matching or deduplication results at the cost of larger blocks and thus more candidate record pairs that need to be compared a detailed analysis of the efficiency and time complexity of this approach has been presented elsewhere 3 5 canopy clustering this indexing technique is based on the idea of using a computationally cheap clustering approach to create high dimensional overlapping clusters from which blocks of candidate record pairs can then be generated clusters are created by calculating the similarities between bkvs using measures such as jaccard or tf idf cosine both of these measures are based on tokens which can be characters q grams or words they can be implemen ted efficiently using an inverted index which has tokens rather than the actual bkvs as index keys this inverted index data structure is built by converting bkvs into lists of tokens usually q grams with each unique token becoming a key in the inverted index all records that contain this token in their bkv will be added to the corresponding inverted index list if the tf idf cosine similarity is used additional information has to be calculated and stored in the index first for each unique token the number of records that contain this token is required this corresponds to the term frequency tf of the token and equals the number of record identifiers stored in a token s inverted index list second within the inverted index lists themselves the document frequency df of a token i e how often it appears in a bkv needs to be stored fig 8 shows an example of such an inverted index data structure with df and tf counts as required for the tf idf cosine similarity when all records in the database s have been inserted into the inverted index the tf and df counts can be normalized and the inverse document frequencies idf be calculated if jaccard similarity is used neither frequency information nor normalization is required once the inverted index data structure is built over lapping clusters called canopies can be generated for this initially all records are inserted into a pool of candidate records a canopy cluster is created by randomly selecting a record r c from this pool this record will become the centroid of a new cluster all records in the pool that are similar to r c according to the selected similarity measure are added into the current cluster the jaccard similarity between r c and any other record r x in the pool is calculated as s j 4 jtokenðr jtokenðr c c þ þ tokenðr tokenðr x x þj þj fig 8 canopy clustering example with bkvs based on surnames and their sorted bigram q 4 2 lists including df counts the tf and df counts in the inverted index data structure are used to calculate tf idf weights with the function tokenðrþ returning the tokens of the bkv of a record r and 0 s j when the tf idf cosine similarity measure is used the normalized tf and idf weight values as stored in the inverted index are included into the similarity calculations which makes this measure computationally more expensive once the simila rities between r c and all other records in the pool are calculated an overlapping cluster can be created in two different ways based on thresholds or nearest neighbors 3 5 threshold based approach in this originally proposed approach 44 two similarity thresholds are used to create the overlapping clusters all records r x that are within a loose similarity t l to r c are inserted into the current cluster e g all records with t l s j of these all records that are within a tight similarity threshold t t with t t t l will be removed from the pool of candidate records this process of randomly selecting a centroid record r c calculating the similarities between this and all other records in the pool and inserting records into clusters is repeated until no candidate records are left in the pool if t l 4 t t the clusters will not be overlapping which means each record will be inserted into one cluster only if both t l 4 and t t 4 i e exact similarity only canopy clustering will generate the same candidate record pairs as traditional blocking estimating the number of candidate record pairs that will be generated with this approach is difficult similar to q gram based indexing because this number depends upon the values of the thresholds t l and t t the similarities between bkvs and the frequency distribution of their tokens together these factors determine how many record identi fiers will be inserted into each cluster and how many will be removed from the pool of candidate records in each step of the algorithm the random selection of the records used as cluster centroids also results in a nondeterministic approach that can result in different clusters for each run and thus different numbers of candidate record pairs generated one extreme situation would occur when the similarity values between all bkvs calculated using their tokens as discussed above are larger than the t t threshold resulting in one single cluster only that contains all record identifiers the other extreme situation would occur when all bkvs are so different from each other that their similarities are below t l and thus each record is inserted only into the block that contains the record identifiers that have the same bkv in this second situation canopy clustering will generate the same candidate record pairs as traditional blocking assuming the number of clusters or blocks generated equals b and that all clusters contain the same number of record identifiers then the number of candidate record pairs generated depends upon into how many clusters each record will be inserted the smallest number of candidate 1548 ieee transactions on knowledge and data engineering vol no september pairs will be generated when each record is inserted into one cluster only on the other hand if each record is inserted into v clusters then based on and the number of candidate record pairs generated for a record linkage can be estimated as n a n b b u ccrl t n a n b b and for a deduplication using 2 and the estimated number of candidate record pairs generated is n a 2 u ccd t as can be seen for both a record linkage and a deduplication the upper bound depends quadratically on the number of times a record identifier is inserted into a cluster given that in reality the generated clusters will not be of uniform size the largest clusters will generate the largest numbers of candidate record pairs similar as with traditional blocking for bkvs that follow a zipf like distribution 3 and 4 can be extended with the overhead v similarly to 18 and above 3 5 2 nearest neighbor based approach an alternative to using two thresholds is to employ a nearest neighbor based approach to create the overlapping clusters the idea is to replace the two threshold parameters t l n b a à n a v n a 2 b v à and t t with two nearest neighbor parameters n l and n t with n l n t the first parameter n l corresponds to the number of record identifiers that are inserted into each cluster while n t is the number of record identifiers that are removed from the pool of candidate records in each step of the algorithm similar to the threshold based approach the process of creating overlapping clusters starts by randomly selecting a record r c from the pool of initially all records similarities are then calculated between the r c and the records r x that have tokens in common in the inverted index the n l records closest to r c are inserted into the current cluster and of these the n t records closest to r c are removed from the pool this approach will result in all clusters containing n l record identifiers independently of the frequency distribution of the bkvs therefore blocks of uniform size will be created allowing the calculation of the number of generated record pairs the number of clusters only depends upon the number of records in the database s to be matched or deduplicated and the values of n l and n t the number of clusters generated corresponds to n a n t and n b n t respectively and each cluster will contain n l records for a record linkage the number of candidate record pairs generated therefore equals u ccrl n 4 z ccrl n 4 n a n l n t â n b n n t l 4 n a n b t l while for a deduplication the number equals u ccd n 4 z ccd n 4 n a n l t n a n n t l à ð21þ the drawback of this approach is similar to the draw back of the sorted neighborhood technique based on a sorted array as discussed in section 3 2 if there are bkvs that are frequent like the surnames smith or meier the generated clusters might not be big enough to include all records with these bkvs and therefore true matches might be missed the solution is to use bkvs that are the concatenation of several record fields and that have a large number of different values compared to other indexing techniques canopy cluster ing using both the threshold and the nearest neighbor approach is not sensitive to errors and variations at the beginning of bkvs because the similarity measures used are independent of the order of where tokens appear in bkvs previous experiments have shown that using the nearest neighbor based approach can result in an increase in the number of true matches in the candidate records pairs that are generated compared to the threshold based approach and also in a higher robustness of the canopy clustering technique with regard to changes in parameter settings the experiments presented in sections 4 and 5 will confirm these statements 3 6 string map based indexing this indexing technique is based on mapping bkvs assumed to be strings to objects in a multidimensional euclidean space such that the distances between pairs of strings are preserved any string similarity measure that is a distance function such as edit distance can be used in the mapping process groups of similar strings are then generated by extracting objects in this space that are similar to each other the approach is based on a modification of the fastmap algorithm called stringmap that has a linear complexity in the number of strings to be mapped the first step of string map based indexing iterates over d dimensions for each dimension the algorithm finds two pivot strings that are used to form orthogonal directions ideally these two pivots are as far apart from each other as possible to find the two pivot strings an iterative farthest first selection process is used once the pivot strings have been selected for a dimension the coordinates of all other strings are calculated based on the directions of these pivot strings selecting an appropriate dimension d is based on using a heuristic approach that iterates over a range of dimensions and selects the one that minimizes a cost function dimensions between and 25 seem to achieve good results once all strings are mapped into a multidimensional space using a suitable index data structure the original implementation uses an r tree in the second step of this indexing approach the retrieve step clusters of similar objects that refer to similar strings are retrieved in the implementation of string map based indexing evaluated in the experiments the originally implemented r tree data structure has been replaced with a grid based index as reported the performance of most tree based index struc tures degrade rapidly with more than 15 to dimensions because nearly all objects in an index will be accessed when similarity searches are conducted the grid based index works by having a regular grid of dimensionality d implemented as an inverted index in each dimension the index keys are the coordinate value of the objects and all objects mapped into the same grid cell in a dimension are inserted into the same inverted index list christen a survey of indexing techniques for scalable record linkage and deduplication similar to canopy clustering based indexing overlap ping clusters can be extracted from the multidimensional grid index an object referring to a bkv is randomly table 3 data sets used in experiments picked from the pool of initially all objects in the grid based index and the objects in the same as well as in neighboring grid cells are retrieved from the index similar to canopy clustering either two thresholds t l or the number of nearest neighbors n l and t t and n t can be used to insert similar objects into clusters and remove objects from the pool with a similarity larger than t t or that are the n t nearest objects to the centroid object equations 18 to artificial 21 can be used to estimate the number of record pairs that data sets containing 000 5 000 000 000 and 100 000 records respectively were generated will be generated with string map based indexing and using either a threshold or a nearest neighbor based approach a variation of this mapping based indexing technique has recently been proposed with the basic idea being to first map records into a multi dimensional space followed by a mapping into a second lower dimensional metric space where edit distance calculations are performed using a kd tree and a nearest neighbor based similarity approach allows for efficient matching experiment showed a reduc tion in runtime of to percent compared to string map based indexing while at the same time keeping the matching accuracy 48 4 e xperimental available from http secondstring sourceforge net data cora contains bibliographic records of machine learning publications and restaurant contains records extracted from the fodor and zagat restaurant guides the cddb data set contains records of audio cds such as their title artist genre and year this last data set was recently used in the evaluation of a novel indexing technique 41 the true match status of all record pairs is available in all four of these data sets artificial data sets were generated using the febrl data generator 49 this generator first creates original records based on frequency tables that contain real name and address values as well as other personal attributes followed by the generation of duplicates of these records e valuation based on random modifications such as inserting deleting or substituting characters and swapping removing insert the aim of the experiments conducted was to evaluate the ing splitting or merging words the types and frequencies efficiency and performance of the presented indexing of these modifications are also based on real characteristics techniques within a common framework in order to the true match status of all record pairs is known the answer questions such as how do parameter values and original and duplicate records were then stored into one file the choice of the blocking key influence the number and each to facilitate their linkage quality of the candidate record pairs generated how do as shown in table 3 two series of artificial data sets different indexing techniques perform with different types were created the clean data contain percent original of data which indexing techniques show better scalability to larger databases all presented indexing techniques were implemented in python within the febrl open source record linkage system available from https sourceforge net projects febrl to facilitate repeatability of the presented results the evaluation program used for these experiments evalin dexing py will be published as part of the next version of febrl all experiments were conducted on an otherwise idle and 20 percent duplicate records with up to three duplicates for one original record a maximum of one modification per attribute and a maximum of three modifications per record the dirty data contain per cent original and percent duplicate records with up to nine duplicates per original record a maximum of three modifications per attribute and a maximum of mod ifications per record compute server with two 2 33 ghz quad core cpus and 4 2 quality and complexity measures gigabytes of main memory running linux 2 6 ubuntu 04 and using python 2 6 5 four measures are used to assess the complexity of the indexing step and the quality of the resulting candidate 4 test data sets two series of experiments record pairs the total number of matched were conducted the first using nonmatched record pairs are denoted with n m and four real data sets that have previously been used by the record linkage research community and the second using artificial data sets table 3 summarizes these data sets the aim of the first series of experiments was to investigate how different indexing techniques are able to handle various types of data while the second series was aimed at investigating the scalability of the different indexing techniques to larger data sets the first three real data sets were taken from the secondstring toolkit census contains records that were generated by the us census bureau based on real census respectively with n m and n n þ n m 4 n a â n b for the linkage of two databases and n m þ n n 4 n a ðn a à 2 for the dedu plication of one database the number of true matched and true nonmatched record pairs generated by an indexing technique is denoted with s m respectively with s m and s n þ s n n þ n n the reduction ratio rr 4 0 à s m reduction m of the comparison space n m i e þs n measures the the fraction of record pairs that are removed by an indexing technique the higher the rr value the less candidate record pairs are being generated however reduction ratio does not take the quality of the generated candidate record pairs into account how many are true matches or not þn n 1550 ieee transactions on knowledge and data engineering vol no september table 4 the labels used in the result figures the number of different parameter settings evaluated and the runtimes in milliseconds per candidate record pair required to build each of the evaluated indexing techniques pairs completeness matched record pairs pc generated 4 n s m m is by the number of true an indexing techni que divided by the total number of true matched pairs it measures how effective an indexing technique is in not removing true matched record pairs pc corre sponds to recall as used in information retrieval true finally matched pairs record quality pairs pq generated 4 s m s m is the number of by an indexing technique divided by the total number of record pairs generated a high pq value means an indexing technique is efficient and generates mostly true matched record pairs on the other hand a low pq value means a large number of nonmatches are also generated pq corre sponds to precision as used in information retrieval the f score or f measure the harmonic mean of pc and pq f 4 pcþpq pcãpq þs n will also be reported 4 3 experimental setup rather than trying to find optimal parameter settings for each combination of blocking key definition indexing technique and test data set a large number of settings were evaluated to provide a better understanding of the average performance and scalability of the different indexing techniques on different data sets because in many real world applications no training data are available that would allow optimal parameter tuning domain and record linkage experts are commonly tasked with finding the best settings experimentally for each data set three different blocking keys were defined using a variety of combinations of record fields string fields such as names and addresses were phoneti cally encoded using the double metaphone algorithm for example for the census data set one blocking key definition consisted of encoded surnames concatenated with initials and zipcodes while a second consisted of encoded given names concatenated with encoded suburb names due to space limitation not all blocking key definitions can be described in detail a large variety of parameter settings were evaluated four string similarity functions jaro winkler bigram edit distance and longest common substring were employed for the adaptive sorted neighborhood the robust suffix array and the string map based indexing techniques for the two nonadaptive sorted neighborhood techniques the window size was set to w 4 3 5 7 similarity thresholds were set to t 4 8 0 and q grams to q 1 4 for all indexing techniques that require these parameters for suffix array based indexing the minimum suffix length and maximum block sizes were set to l m 1 4 and b m 1 4 for canopy clustering both the jaccard and tf idf cosine simila rities were used in combination with global thresholds t t t l 1 4 0 8 0 8 0 or nearest neighbor parameters n t n l 1 4 10 10 the same threshold and nearest values were also used for string map based indexing the grid size for this technique was set to 100 and 1 000 and the mapping dimension to d 1 4 for each data set a total of parameter settings were evaluated table 4 summarizes the experimental setup and shows runtime results figs 10 and show for each indexing technique the average and standard deviation values over all blocking key definitions and combinations of parameter settings the presented results on these various data sets should therefore provide some indication of the performance and scalability of the different indexing techniques note that for the scalability experiments not all results are shown because either an indexing technique required more than the available gigabytes of main memory or its runtime was prohibitively slow to conduct many experiments 5 d iscussion looking at the runtime results shown in the right hand side of table 4 one can clearly see that the q gram based indexing technique is the overall slowest technique by a very large margin in average times slower than the second slowest technique this confirms earlier experi ments and as a result this technique is not suitable for linking or deduplicating large databases both string map based indexing approaches the suffix array based approaches and threshold based canopy clus tering also have fairly slow average and maximum runtimes on the other hand the more simpler approaches like traditional blocking and the array based sorted neighborhood approach are the overall fastest techniques among the other fastest techniques are the robust suffix array and adaptive sorted neighborhood approaches in the following discussion we will see if these fast indexing times christen a survey of indexing techniques for scalable record linkage and deduplication fig experimental results for the four real data sets average values and standard deviations are shown come at the cost of lower indexing quality i e lower pc and pq values fig shows the results for the four real data sets as can be seen in fig some indexing techniques have large variations in the reduction ratio rr they attain while others have a high rr independently of the data sets they are applied on the array based sorted neighborhood and the three suffix array based indexing techniques achieve nearly uniformly high rr values for all data sets because the size of the blocks generated by them is either independent of the data to be linked or deduplicated or limited to a maximum size determined by a parameter therefore because the number of candidate record pairs generated by these techniques can easily be calculated they can be useful for applications where a linkage or deduplica tion must be done within a certain amount of time on the other hand the large variations of rr values by other indexing techniques for some data sets are due to the varying sizes of the blocks generated by them for these techniques block sizes depend upon the frequency dis tribution of the bkvs the quality of the candidate record pairs generated by the different indexing techniques measured using pc pq and the f score is mostly influenced by the characteristics of the data set and the choice of blocking key definition this can be seen by the large standard deviations for some data sets in figs and all three measures differ more prominently between data sets than between indexing techniques for the census data set for example all techniques achieve a pc value above 0 8 while for the cddb data set none produces a pc value larger than 0 4 this highlights the need for the careful definition of blocking keys which needs to be done uniquely to each data set as the very low pq and f score results for cddb and restaurant show due to variations and errors in these data sets that cannot be overcome with appropriate blocking key definitions it might not be possible at all to achieve good quality indexing with traditional techniques among the techniques that attain the lowest pc and f score values are the suffix array based approaches this is because the high rr they achieve comes at a cost of low pq and f score values the highest performing technique with regard to pc is the inverted index based sorted neighborhood approach which is surprising given its sensitivity to errors and variations at the beginning of bkvs the overall efficiency of the different indexing techni ques is shown in fig as the f score of pc and pq surprisingly traditional blocking is the best performing technique for two of the four data sets closely followed by threshold based canopy clustering q gram based indexing and the adaptive sorted neighborhood technique this figure once more highlights that the definition of suitable blocking keys is one of the most crucial components in the indexing step for record linkage or deduplication and not the actual indexing technique employed moving on to the results achieved with the artificial data sets shown in figs 10 and as can be seen the rr 1552 ieee transactions on knowledge and data engineering vol no september fig 10 reduction ratio results for the artificial data sets average values and standard deviations are shown fig pairs completeness results for the artificial data sets average values and standard deviations are shown fig pairs quality results for the artificial data sets average values and standard deviations are shown fig f score results for the synthetic data sets average values and standard deviations are shown values for most indexing techniques stay high or get even higher as the data sets get larger this indicates a subquadratic increase in the number of candidate record pairs generated as the data sets get larger one exception is the threshold based string map indexing technique which not only has the lowest average rr values in general but also shows to be very sensitive toward parameter settings as expected the pc values are higher for the clean data sets compared to the dirty data sets because the bkvs for the former contain less errors and variations and thus more records were inserted into the correct blocks as figs 11 and show for several indexing techniques the pc or pq values drop significantly as the data sets get larger as can be seen having a constant pc value across data sets size comes at the cost of lower pq values and vice versa christen a survey of indexing techniques for scalable record linkage and deduplication the reason for the drop in pc values are the fixed window size for the sorted neighborhood approaches the fixed number of nearest neighbors for the canopy clustering and string map based approaches and the fixed maximum block size for the suffix array based techniques as the data sets get larger more records will have the same or similar bkvs and the fixed block size limits will result in records that do have the same or very similar bkvs not being included into the same block thereby missing increasingly more true matches with larger data sets on the other hand traditional blocking and the threshold based indexing techniques do not have maximum block sizes and thus all records with the same or similar bkvs are inserted into the same blocks without limitations the costs of being able to keep constant pc values with larger data sets are lower pq values this means that as data sets get larger the number of candidate record pairs that will be generated increases faster for threshold based techniques than for techniques that somehow limit the maximum block size this tradeoff between pc and pq is similar to the precision recall tradeoff in information retrieval one aspect of indexing techniques that is of importance to their practical use is their robustness with regard to parameter settings ideally an indexing technique should achieve a high rr and a high f score value for a large variety of parameter settings because otherwise a user needs to carefully tune the parameters of an indexing technique as fig shows the string map and suffix array based approaches have the largest standard devia tions in the f score values they achieve for effective parameter tuning some form of gold standard data where the true match status of record pairs is known must be available such data must have the same characteristics as the data to be linked or deduplicated as can be seen from fig traditional blocking the adaptive sorted neighborhood approach and threshold based canopy clustering achieve high f score values for all their parameter settings 6 r elated r esearch research into indexing for record linkage and deduplica tion can be classified into two categories the first category is to develop new and improve existing techniques with the aim of making them more scalable to large data sets while keeping a high linkage quality 33 41 48 the techniques presented in this survey are efforts toward this goal the second category of research into indexing is the development of techniques that can learn optimal blocking key definitions traditionally the choice of blocking keys is made manually by domain and record linkage experts recently two supervised machine learning based ap proaches to optimally select blocking keys have been proposed they either employ predicate based formulations of learnable blocking functions or use the sequential covering algorithm which discovers disjunctive sets of rules both approaches aim to define blocking keys such that the number of true matches in the candidate record pairs is maximized while keeping the total number of candidate pairs as small as possible both approaches rely on training examples i e pairs of true matches and nonmatches such training data are often not available in real world applications or they have to be prepared manually in principle these learning approaches can be employed with any of the indexing techniques presented in this survey another approach to reduce the computational efforts of the record linkage process is to minimize the number of costly string comparisons that need to be made between records one recently proposed technique is based on a matching tree that allows early match decisions without having to compare all attributes between two records while another technique sequentially assesses the attributes and stops the comparison of two records once a match decision can be made a large amount of work has also been conducted by the database community on similarity joins 51 57 where the aim is to facilitate efficient and scalable approximate joins for similarity measures such as edit or jaccard distance another avenue of work is to efficiently index uncertain databases as well as finding similarities between objects in uncertain databases 61 concurrently the information retrieval community has developed techniques to detect duplicate documents returned by web search queries 6 7 in this domain fast matching and scalability to very large data collections are of paramount importance recently factorization models have attracted a lot of research in the fields of in telligent information systems and machine learning they have shown excellent prediction capabilities in several important applications for example recommender systems the most well studied factorization model is matrix factorization srebro and jaakkola which allows us to predict the relation between two categorical variables tensor factorization models are an extension for relations over several categorical variables among the proposed tensor factorization approaches are tucker decomposition tucker parallel factor analysis harshman or pairwise interaction tensor factorization rendle and schmidt thieme for specific tasks specialized factorization models have been proposed that take noncategorical variables into account for example svd koren ste ma et al fpmc author address s rendle social network analysis university of konstanz email steffen rendle uni konstanz de permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation copyrights for components of this work owned by others than acm must be honored abstracting with credit is permit ted to copy otherwise to republish to post on servers to redistribute to lists or to use any component of this work in other works requires prior specific permission and or a fee permissions may be requested from the publications dept acm inc penn plaza suite new york ny usa fax 0481 or permissions acm org c acm doi 2168771 http doi acm org 2168771 acm transactions on intelligent systems and technology vol no article publication date may s rendle rendle et al for a set categorical variable timesvd koren and bptf xiong et al for an additional numerical variable for the basic matrix factorization model many learning and inference approaches have been studied among them stochastic gradient descent alternating least squares e g pil aszy et al variational bayes lim and teh and markov chain monto carlo mcmc inference salakhutdinov and mnih however for more complex factorization models only the most simple learning method of gradient descent is mostly available even though factorization models have a high prediction quality in many applica tions it is nontrivial to work with them for each problem that cannot be described with categorical variables a new specialized model has to be derived and a learning algorithm has to be developed and implemented this is very time consuming error prone and only applicable for experts in factorization models on the other hand in practice the typical approach in machine learning is to de scribe data with feature vectors a preprocessing step aka feature engineering and to apply a standard tool for example libsvm chang and lin for support vector machines a toolbox such as weka hall et al or a simple linear regression tool this approach is easy and applicable even for users without in depth knowledge about the underlying machine learning models and inference mechanisms in this article factorization machines fm rendle are presented fms com bine the high prediction accuracy of factorization models with the flexibility of feature engineering the input data for fms is described with real valued features exactly like in other machine learning approaches such as linear regression support vector machines etc however the internal model of fms uses factorized interactions be tween variables and thus it shares with other factorization models the high predic tion quality in sparse settings like in recommender systems it has been shown that fms can mimic most factorization models just by feature engineering rendle this article summarizes the recent research on fms including learning algorithms based on stochastic gradient descent alternating least squares and bayesian infer ence using mcmc fms and all presented algorithms are available in the publicly available software tool libfm with libfm applying factorization models is as easy as applying standard tools such as svms or linear regression the article is structured as follows the fm model and its learning algorithms that are available in libfm are introduced several examples for input data are given and the relation to specialized factorization models is shown the libfm software is briefly introduced and experiments are conducted factorization machine model let us assume that the data of a prediction problem is described by a design matrix x variables rn p where the ith row x i rp of x describes one case with p real valued ample and where alternatively y i one is the prediction can describe target of the ith case this setting as a set see figure for an ex s of tuples x y where again x rp is a feature vector and y is its corresponding target such a represen tation with data matrices and feature vectors is common in many machine learning approaches for example in linear regression or support vector machines svm factorization machines fm rendle model all nested interactions up to order d between the p input variables in x using factorized interaction parameters the factorization machine fm model of order d is defined as ˆy x w p w j x j p p x j x j j j j j k v j f v j f f acm transactions on intelligent systems and technology vol no article publication date may factorization machines with libfm fig example from rendle for representing a recommender problem with real valued feature vectors x every tion the features row represents a are grouped into feature indicators vector for the x i with its corresponding target active user blue active item y red i for easier interpreta other movies rated by the same user orange the time in months green and the last movie rated brown where k is the dimensionality of the factorization and the model parameters w w w p v v p k are w r w rp v rp k the first part of the fm model contains the unary interactions of each input variable two x j with the nested target exactly sums as in a linear regression model the contains all pairwise interactions of input variables the important difference to standard polynomial regression is second part with the that the that effect is of x j the x j interaction is not modeled parametrization w j j that the effect of pairwise by an v j v j interactions independent k f has v j f a v low j f parameter w j j but with a factorized which corresponds to the assumption rank this allows fms to estimate re liable parameters even in highly sparse data where standard models fail the relation of fms to standard machine learning models is discussed in more detail in section in section it will also be shown how fms can mimic other well known factoriza tion models including matrix factorization svd fpmc timesvd etc complexity let n z be the number of nonzero elements n z x δ x i j in a matrix x or vector x i j where δ is the indicator function δ b if b is true if b is false the fm model rendle to in equation can be computed in o k n z x because it is equivalent ˆy x w p w j x j j the number of model parameters of an fm is p k p and thus linear in the number of predictor variables size of the input feature vector and linear in the size of the factorization k acm transactions on intelligent systems and technology vol no article publication date may k f p j v j f x j p j j f j s rendle multilinearity an appealing property of fms is multilinearity that is for each model parameter θ the fm is a linear combination independent of the value of θ rendle et al of two functions g θ and h θ that are ˆy x g θ x θ h θ x θ with h θ ˆy x the definition of g θ x is omitted because in the following it is never used directly if its value should be computed the equation g θ x will be used instead expressiveness the fm model can express any pairwise interaction provided that k is chosen large enough this follows from the fact that any symmetric positive semidef inite matrix w can be decomposed into v vt e g cholesky decomposition let w be any pairwise interaction matrix that should express the interactions between two dis tinct variables in an fm w is symmetric and as the fm does not use the diagonal elements because j j in eq any value and especially also arbitrary large values for the diagonal elements are possible which will make w positive semidefi nite please note that this is a theoretical statement about the expressiveness in prac tice k p because the advantage of fms is the possibility to use a low rank ap proximation of w and thus fms can estimate interaction parameters even in highly sparse data see section for a comparison to polynomial regression which uses the full matrix w for modeling interactions higher order fms the fm model of order d eq can be extended by fac torizing ternary and higher order variable interactions the higher order fm model rendle reads ˆy x w x ˆy x θ h θ p d p p l k l l j l j j d i f i with model parameters w w j x j x j i v j i f j d r w rp l d vl rp k l also for higher order interactions the nested sums in eq can be decomposed for a more efficient computation in the remainder of this article we will deal only with second order fms because in sparse settings where factorization models are espe cially appealing typically higher order interactions are hard to estimate rendle and schmidt thieme nevertheless most formulas and algorithms can be transfered directly to higher order fms as they share the property of multilinearity with second order fms learning factorization machines three learning methods have been proposed for fms stochastic gradient descent sgd rendle alternating least squares als rendle et al and markov chain monte carlo mcmc inference freudenthaler et al all three of them are available in libfm acm transactions on intelligent systems and technology vol no article publication date may θ if θ is w x l if θ is w l x l j l v j f x j if θ is v l f factorization machines with libfm optimization tasks optimality of model parameters is usually defined with a loss function l where the task is to minimize the sum of opt s losses over argmin the observed data s x y s l ˆy x y note that we add the model parameters to the model equation and write ˆy x when we want to stress that ˆy depends on a certain choice of depending on the task a loss function can be chosen for example for regression least squares loss lls y or for binary classification y lc y y y y y lnσ y y where σ x is the sigmoid logistic function fms usually have a large number of model parameters especially if k is chosen large enough this makes them prone to overfitting to overcome this typically regularization is applied which can be motivated by maximum margin srebro et al or tikhonov regularization optreg s λ argmin where λ θ l ˆy x y λ θ x y s θ r is the regularization value for the model parameter θ it makes sense to use individual regularization parameters for different parts of the model in libfm model parameters can be grouped for example one group for the parameters describ ing the users one group for the items one for time etc see figure for an example of groups and each group uses an independent regularization value moreover each factorization layer can have an individual f k regularization as well again as the with unary groups regression in total coefficients the regularization w and w structure of libfm is λw π λv f π π f k where π p is a grouping of model parameters that means for example the regularization value for v l f would be λv f π l probabilistic interpretation both loss and regularization can also be motivated from a probabilistic point of view e g salakhutdinov and mnih the least squares loss corresponds to the assumption that the target y is gaussian distributed with the prediction as mean y x n ˆy x α for binary classification a bernoulli distribution is assumed y x bernoulli b ˆy x where b r is a link function typically the logistic function σ or the cumula tive distribution function cdf of a standard normal distribution regularization corresponds to gaussian priors on the model parameters θ μ θ λ θ n μ θ λ θ acm transactions on intelligent systems and technology vol no article publication date may e x s rendle fig graphical representation of the variables involved in a standard factorization machine a the variables are target y input features x model parameters b algorithm the priors to are find extended the prior by parameters hyperpriors automatically α α λ β freudenthaler w β w λ γ j v μ j f and which et hyperparameters al priors μ λ α allows the mcmc algorithm the prior mean tion the values graphical λ θ see μ θ eq should be grouped and organized the same way as the regulariza model for the probabilistic view can be seen in figure a the max imum a posteriori map estimator for this model with α the optimization criterion of eq μ θ is the same as gradients for direct optimization of the loss functions the derivatives are for least squares regression ˆy x or for classification ˆy x finally due to multilinearity of the fm model the partial derivative of the model equation with respect to θ corresponds to h θ eq x stochastic gradient descent sgd stochastic gradient descent sgd algorithms are very popular for optimizing factor ization models as they are simple work well with different loss functions and have low computational and storage complexity algorithm shows how fms can be optimized acm transactions on intelligent systems and technology vol no article publication date may θ lc ˆy x y θ lls ˆy x y θ lnσ θ ˆy x y θ ˆy x y ˆy x h θ σ ˆy x y ˆy x y θ y θ factorization machines with libfm algorithm stochastic gradient descent sgd input training data s regularization parameters λ learning rate η initialization σ output model parameters w w v w w v n σ repeat for x y sdo w w η end end end until stopping criterion is met with sgd rendle the algorithm iterates over cases x y s and performs updates on the model parameters θ θ η where η r is the learning rate or step size for gradient descent complexity the sgd algorithm for fms has a linear computational and constant storage complexity for one iteration over all training cases the runtime complexity of sgd is o k n hyperparameters for performing sgd there are several critical hyperparameters learning rate η the convergence of sgd depends largely on η if η is chosen too high the algorithm does not converge and if it is chosen too small convergence is slow typically η is the first hyperparameter that should be determined regularization λ as noted in section the generalization capabilities of fms and thus the prediction quality depends largely on the choice of the regularization λ the regularization values are typically searched on a separate holdout set for example using grid search as there are several regularization parameters see eq the grid has exponential size and thus this search is very time consuming to make the search more feasible the number of regularization parameters is usu ally reduced for example groups are dropped and all factor layers use the same regularization value initialization σ the parameters for the factorized interactions v have to be initial ized with nonconstant values in libfm the values are sampled from a zero mean normal distribution with standard deviation σ typically small values are used for σ sgd with adaptive regularization in rendle it has been shown how the regular ization values can be adapted automatically in sgd while the model parameters are learned libfm includes the adaptive regularization algorithm proposed there and extends it with groups acm transactions on intelligent systems and technology vol no article publication date may s rendle alternating least squares coordinate descent the optimization approach of sgd is based on iterating over cases rows of the train ing data and performing small steps in the direction of a smaller loss coordinate descent or alternating least squares als takes another approach by minimizing the loss per model parameter for least squares regression with regularization the op timal value θ for one model parameter θ given all remaining parameters θ can be calculated directly rendle et al as he error term residual of the ith case e i y i ˆy x i this allows us to derive a least squares learning algorithm see algorithm that it eratively solves a least squares problem per model parameter and updates each model parameter with the optimal local solution θ θ this is performed iteratively over all parameters until convergence complexity the main effort of als learning eq is computing the following two quantities i with a trivial implementation updating one model parameter would require comput ing the model equation the corresponding ˆy x i and the gradient h θ x i for each training case x i where parameter this v j f column this j is nonzero x i j for example for updating the model would have to would render the complexity be computed for each of the o p k n i δ x model i j k parameters n z x i in total in rendle et al it has been shown how one full iteration over all model parameters can be done efficiently see eq and q rn k such that in o n z x k by precomputing the caches e rn q i f p v l f x i l l which allows us to compute h quickly in o h v l f x i x i l q i f v l f x i l acm transactions on intelligent systems and technology vol no article publication date may n i n i h θ x i h θ λ θ x i factorization machines with libfm algorithm alternating least squares als input training data s regularization parameters λ initialization σ output model parameters w w v w w v n σ repeat ˆy predict all cases s e y ˆy w for l w p do w l update w l e end for f k do init q f for l p do v l f v l f update e q end end until stopping criterion is met now calculating θ cache value q and e n i for can the be done lth parameter in constant is extra in o time δ x see i l rendle also et al updating each however the speed up comes at the price of higher memory consumption for the caches the approach presented in rendle et al has an additional memory complexity of o nk because of the q cache libfm provides a more efficient imple mentation with only o n memory complexity for the q cache see algorithm the idea is that the model parameters are updated per layer f i e first all parameters v q of v the v same layer then have v v to be v present etc this and in each layer only the cache values means that libfm stores and updates only the q cache for one layer and thus the storage is o n and when changing the layer the q values of the new layer are computed initialized the initialization of the q values per layer has no negative effect on the overall computational complexity hyperparameters a clear advantage of als over sgd is that als has no learn ing rate as hyperparameter however two important hyperparameters remain regularization and initialization finding good regularization values is especially computational expensive classification the als algorithm described so far is restricted to least squares re gression and cannot solve classification tasks libfm contains classification capabil ities for als coordinate descent which is based on using a probit link function this approach is motivated from the probabilistic interpretation section and will be described at the end of the mcmc section markov chain monte carlo mcmc inference the bayesian model used so far can be seen in figure both als and sgd learn the best parameters which are used for a point estimate of ˆy mcmc is a bayesian inference technique that generates the distribution of ˆy by sampling for mcmc acm transactions on intelligent systems and technology vol no article publication date may factorization machines with libfm with pπ p j δ π j π complexity the gibbs sampler for mcmc inference is sketched in algorithm and has the same complexity as the als algorithm this follows directly from the ob servation that for both algorithms the same summations have to be computed for the conditional posterior distribution in mcmc and the expected value in als the overhead of mcmc is the inference over eqs and but even with a the straightforward h that is computing the posteriors implementation this is in o k n z x are hyperparameters automatically determined a major advantage this comes of mcmc at the is price that of the introducing regularization parameters values for h hyperpriors ber of regularization however the number of hyperpriors parameters h is smaller than the num insensitive to choices of in libfm the following trivial that is a and more importantly mcmc trivial choice for the values of values for is typically works well μ the only hyperparameter that remains are used α β α λ β λ γ and for mcmc is the initialization σ in gen eral here one can even use a value of which is not possible for als and sgd because mcmc posterior uncertainty will identify the factorization however choos ing a proper value can speed up the sampler one can usually see in the first few samples if an initialization σ is a good choice classification the mcmc algorithm solves regression tasks it can be extended for binary classification by mapping the normal distributed ˆy to a probability b ˆy that defines the bernoulli distribution for classification gelman et al that means the mcmc algorithm will predict the probability that a case is of the positive class libfm uses the cdf of a normal distribution as mapping that is b z z because the posteriors are then easy to sample from the only two changes that have to be made to algorithm for classification is that for prediction ˆy is transformed by and that instead of regressing to y the regression target y is sampled in each iteration from its posterior that has a truncated normal distribution y i x i y i n ˆy x i n ˆy x i δ y i δ y i if y i has if the negative class y i has the positive class sampling from this distribution is efficient robert as noted before als for regression can be seen as a simplification of mcmc where the model parameters are not sampled but their expectation value is taken in each update a classification option for als is available in libfm that follows the same idea and instead of sampling from the truncated normal as it is done in mcmc for classification with als the expected value of the truncated normal is computed summary an overview of the properties of the learning algorithms in libfm can be found in table i acm transactions on intelligent systems and technology vol no article publication date may related work and application of factorization machines first examples for input data are shown including how they relate to other special ized factorization models note that fms are not restricted to the choices presented here second other generic factorization models are compared to fms third fms are compared to polynomial regression acm transactions on intelligent systems and technology vol no article publication date may algorithm x runtime complexity storage complexity o o n o n regression yes yes yes classification yes yes yes hyperparameters samples ˆy test sgd als mcmc o k n z initialization regularization values λs learning rate η table i properties of the learning algorithms in lib x o k n z initialization regularization values λs x o k n z fm initialization hyperpriors insensitive factorization machines with libfm expressing factorization models with factorization machines in this section the generality of fms will be discussed by comparing them to other specialized state of the art factorization models this also shows how to apply fms by defining the input data i e features it is crucial to note that in practice only the feature vector x has to be defined the rest is done implicitly by the fm neither an explicit reformulation of the model equation nor developing of new prediction nor a learning algorithms is necessary the analysis of the fm model equation that is done in this section is just to show the theoretical relations to other models matrix factorization assume data about two categorical variables u e g a user and i e g an item should be used in a fm the straightforward way to describe a case u i u i is to use a feature vector x r u i with binary indicator variables that is u i x where the uth entry in the first part of x is the ith entry on the second part of x is and the rest is e g see the first two groups of figure using this data in an fm the fm will be exactly the same as a biased matrix factorization model fm pairwise interaction tensor factorization if three categorical variables should be described for example u i and t e g tags a straightforward representation with a feature vector would be x r u i t the difference between this fm and the original pitf is that this fm contains lower order interactions and shares the factors v between interactions besides this both approaches are identical svd and fpmc assume that there are two categorical variables e g u and i and one set categorical variable e g p l one simple representation of this data would be x s rendle where each for example of the m elements of the by m in the corresponding set l column l m e g is described by a nonzero value see the first three groups of figure with this data the fm will be equivalent to ˆy x ˆy u i l svd l m if implicit feedback is the same as the svd used model as koren input for l l salakhutdinov m the fm just sketched is almost and mnih tak acs et al the first part annotated with svd is exactly the same as the original svd koren the second part second line in eq contains some additional interactions similar to the if factorized sequential personalized information is markov used as chain input fpmc for l l rendle m the et al fm is very especially if the fm is optimized for ranking like fpmc almost all terms that are in the fm model but not in the fpmc model will vanish see rendle for details if social information is used as input e g friends the fm is similar to the social trust ensemble ste ma et al bptf and timesvd if time should be included the most simple approach is to treat time as a categorical variable e g each day is a level and apply the same encoding as in eq the fm with this data is similar to the time aware bptf model xiong et al the differences are that bptf uses a ternary parafac model over the three categorical variables user item time whereas fm uses fac torized pairwise interactions moreover bptf has an additional regularizer over the time variables in freudenthaler et al it has been shown that the fm works indeed better than the more complex bptf model another approach is to use a separate time variable per user i e making the user time interaction explicit the input data would be x r u i u t with binary indica tors for the user item and a user specific day indicator with this data the fm model would be equivalent to factorization machines with libfm nearest neighbor models when other numerical measurements are available for this example can be encoded other ratings in a feature r r vector the x same r i i user has given to items l l i etc i r l r m l m x the fm model with this data would be equivalent to ˆy x ˆy i r l r m l m w w i this model is similar to the factorized nearest neighbor model koren another possible approach to encode the data would be to use separate rating indi cators per item and one for the user that is x r i u i i that means the rating indicators in eq would be in a separate block for each item an fm of order d with this data would be equivalent to ˆy x ˆy i u r this approach is identical to the nonfactorized nearest neighbor model of koren it can be combined with the implicit feedback idea which results in the knn model koren attribute aware models there are several attempts to integrate attribute infor mation about users and items into recommender systems it is very simple to use such information in an fm a straightforward approach is to add the attributes of an item or user such as genre actor etc to the input vector x assume the input vector consists of such item attributes and an indicator variable for the user this is almost identical to the attribute aware approach in gantner et al that uses a linear regression to map item attributes to factors see the highlighted part attribute mapping of eq the only difference is that the fm contains biases as well as additional interactions between item attributes e g between genre and actors if the input vector of standard matrix factorization eq is extended by at tribute information of the user e g demographics and attributes of the item the fm acm transactions on intelligent systems and technology vol no article publication date may s rendle would correspond to the attribute aware model proposed in agarwal and chen again the difference is that the fm contains additional interactions within user and item attributes e g interaction between a user age and gender other generic factorization models there are other attempts for a more generic factorization model in agarwal and chen a matrix factorization model is extended with regression priors that is the mean of the normal distributed priors of factors is a linear regression model fms can mimic this approach because for any hierarchical model using normal distributed priors the mean value of the prior and thus also a linear regression model for the prior mean can be added as covariates to the feature vectors on the other hand mf with regression priors is much more restricted than fms because mf itself is limited to interactions of two categorical variables and thus the mf model with regression priors is not appropriate for tasks with interactions over more than two variables for example tag recommendation or context aware recommendation fms include pairwise interactions between any number of variables also not limited to categorical ones svdfeature chen et al is another generic factorization model similar to agarwal and chen in svdfeature a matrix factorization model is extended with linear regression terms for the factors and for the bias term however compared to fms it shares the same shortcomings of agarwal and chen only interactions between two categorical variables can be factorized that means it is not able to mimic state of the art context aware recommenders tag recommenders etc moreover for svdfeature only sgd learning has been proposed whereas libfm features mcmc inference which is much more simple to apply as there is no learning rate and the regularization values are automatically determined an advantage of svdfeature over libfm is that due to the more restrictive model equation it has an improved learning algorithm following koren for speeding up learning in factor regression terms relation to polynomial regression in rendle it has been shown that fms can be seen as polynomial regression or svm with inhomogeneous polynomial kernel using factorized parameter matrices polynomial regression of order d can be defined as ˆypr x w p w j x j p p w j j x j x j j j j j with model parameters w r w rp w rp p comparing this to the fm model eq one can see that fms use a factorization for pairwise interactions whereas polynomial regression uses an independent param eter sparse w j settings j per pairwise interaction this difference is crucial for the success of fms in for example recommender systems or other prediction problems in volving categorical variables of large domain fms can estimate pairwise interactions w because j j for pairs j j even if none or only little observation about the pair is present a low rank assumption the interaction of pair j j is made and j j have w j something j v j v j in common that is it is assumed that in polynomial re gression both pairs are completely independent a priori acm transactions on intelligent systems and technology vol no article publication date may factorization machines with libfm libfm software is an implementation of factorization machines it includes the sgd als and mcmc algorithms described in section for regression and classification tasks fms of order d are implemented data format the input data format of libfm is the same as for svm light joachims and lib svm chang and lin for very large scale data that does not fit into main mem ory libfm has a binary data format where only parts have to be kept in main memory a converter from the standard text format to the binary data format is available example all major options are available over an easy to use command line interface an exam ple call for learning a dataset with mcmc inference would be libfm method mcmc task r dim iter test test libfm train train libfm out test pred where should dim be included specifies the and factorization k n here k dimensions for the dimensionality if w should be of included v if w init stdev is the standard deviation for initialization that is σ of algorithm iter is the number of samples that are drawn parameter setup in the following a few practical hints for applying libfm to a prediction problem are given for inexperienced users it is advisable to use mcmc inference as it is the most simple one to work with when a predictive model for a new dataset is built one should start with a low factorization dimensionality e g k and first determine the standard deviation for initialization init stdev because proper values will speed up the mcmc sampler several values for init stdev should be tested e g the success can be quickly seen on the first few iterations by monitoring the training error or even better by using a holdout set for validation after an appropriate init stdev has been determined mcmc can be run with a large number of iterations and larger factorization dimensionality k the accuracy and convergence can be monitored on the output of libfm for mcmc no other hyperparameters have to be specified other methods als and sgd require more hyperparameters to tune see section ranking libfm also contains methods for optimizing an fm model with respect to ranking liu and yang based on pairwise classification rendle et al ranking is not available over the command line but can be embedded into existing software an example for embedding libfm is provided in the software tool tag recommender also available with source code with source code from http www libfm org acm transactions on intelligent systems and technology vol no article publication date may s rendle fig prediction error of lib fm with sgd and mcmc learning using mf see eq and knn see eq input data a compare to the mf approaches pmf sgd salakhutdinov and mnih bpmf mcmc salakhutdinov and mnih and mf sgd koren b compare to the corresponding knn approach of koren evaluation in section it was shown that fms are able to mimic many factorization models now this will be substantiated by comparing the libfm implementation empirically to several well studied factorization models the success will be measured by root mean square error rmse for regression tasks and measure for ranking tasks see gunawardana and shani for a summary of evaluation metrics for recommender systems rating prediction in recommender systems the most well studied data set is the netflix which includes about ratings of about users for items all our reported results are obtained on the netflix quiz set i e the same test set as on the public leaderboard from the netflix challenge matrix factorization mf the most successful approaches on netflix are based on matrix factorization e g jahrer et al koren tak acs et al for mf many different variations and learning approaches have been proposed for exam ple als pil aszy et al mcmc salakhutdinov and mnih variational bayes lim and teh stern et al but mostly sgd variants e g koren salakhutdinov and mnih thus even for the simple mf model the pre diction quality reported differ largely we want to investigate how good the learning methods of libfm are by setting up an fm with mf indicators see eq which is equivalent to biased mf with this setting all compared approaches share the same model but differ in learning algorithm and implementation figure a shows a comparison of libfm using sgd and to the sgd approaches of pmf salakhutdinov and mnih and the mf sgd approach of koren as well as the bpmf approach using mcmc inference salakhutdinov www netflixprize com sgd results are from rendle fm mcmc results from freudenthaler et al acm transactions on intelligent systems and technology vol no article publication date may factorization machines with libfm fig lib fm for context aware recommendation compared to multiverse recommendation karatzoglou et al compares lib fm for the task of tag recommendation to the best four approaches on task of the ecml pkdd discovery challenge and mnih for that have been reported libfm in koren with sgd for we the use related the regularization svd model values it can λ θ be seen that the mcmc approaches have the lowest error and the mcmc sampler of libfm outperforms the sampler of the bpmf model slightly nearest neighbor models traditionally nearest neighbor models have attracted a lot of research in the recommender system community e g linden et al sarwar et al zheng and xie on the netflix challenge the best perform ing neighborhood approaches are based on treating the similarities between items as model parameters which are learned that is knn eq koren and factor ized knn eq koren again we want to see how well libfm can mimic these models just by feature engineering we set up the input data for libfm such that the fm model corresponds to the knn and knn i e with additional implicit indicators as described in koren we use the same pruning protocol to restrict to neighbors and ported in koren for sgd we use figure b shows the that same libfm regularization with mcmc λ and θ as re sgd achieves comparable quality to the approach of koren context aware recommendation secondly libfm has been studied on the problem of context aware recommendation rendle et al in context aware recommendation besides the user and item there is other information about the rating event available for example the location of the user at the time of his rating the mood etc as fms can work with any number of features they can be applied easily to this task figure a shows a comparison of libfm using als and mcmc to the state of the art approach multiverse recom mendation karatzoglou et al which outperforms other context aware methods such as item splitting baltrunas and ricci and the multidimensional approach of adomavicius et al see karatzoglou et al and rendle et al for details about the experimental setup acm transactions on intelligent systems and technology vol no article publication date may s rendle tag recommendation the last experiment shows the applicability of libfm to ranking we compare libfm for the task of tag recommendation e g lipczak and milios using input data as in eq with this data libfm mimics the pitf model rendle and schmidt thieme which was the best performing on task of the ecml pkdd discovery challenge figure b shows a comparison of the prediction quality of libfm to pitf and the second to fourth best models of the discovery challenge relational classification marinho et al and the models of the third lipczak et al and fourth place zhang et al conclusion and future work factorization machines fm combine the flexibility of feature engineering with fac torization models this article summarizes the recent research on fms and presents three efficient inference methods based on sgd als and mcmc also extensions are presented among them classification for mcmc and als as well as grouping of variables the properties of fms have been discussed both theoretically in terms of complexity expressiveness and relations to other factorization models as well as with an empiri cal evaluation it has been shown that fms can mimic several specialized factorization models for certain fms are not restricted to these examples empirical results show that the prediction quality of the described inference algorithms for fms is compara ble to the best inference approaches for specialized models in the area of recommender systems in total that means that the generality of fms does not come to the prize of a low prediction accuracy or a high computational complexity all presented algorithms are implemented in the publicly available software tool libfm there are several directions for future work on fms first due to the generality of fms they are supposed to be interesting for a wide variety of prediction problems especially problems involving categorical variables with large domains might benefit from fms studying fms using libfm on such problems is highly interesting second the complexity of the inference methods for fms could be reduced because the algo rithms proposed so far make no use of repeating patterns in the input data which could be exploited for an additional speed up third the software implementation libfm could be extended by higher order interactions in the past few years several datasets with high dimensionality have become publicly avail able on the internet this fact has brought an interesting challenge to the research community since for the machine learning methods it is difficult to deal with a high number of input features to confront the problem of the high number of input features feature selection algo rithms have become indispensable components of the learning process feature selection is the process of detecting the relevant features and discarding the irrelevant ones a correct selection of the features can lead to an improvement of the inductive learner either in terms of learning speed generalization capacity or simplicity of the induced model moreover there v bolón canedo b n sánchez maroño a alonso betanzos department of computer science university of a coruña a coruña spain e mail veronica bolon udc es v bolón canedo et al are some other benefits associated with a smaller number of features a reduced measurement cost and hopefully a better understanding of the domain there are several situations that can hinder the process of feature selection such as the presence of irrelevant and redundant features noise in the data or interaction between attri butes in the presence of hundreds or thousands of features such as dna microarray analysis researchers notice that is common that a large number of features is not informative because they are either irrelevant or redundant with respect to the class concept moreover when the number of features is high but the number of samples is small machine learning gets particularly difficult since the search space will be sparsely populated and the model will not be able to distinguish correctly the relevant data and the noise there exist two major approaches in feature selection individual evaluation and subset evaluation individual evaluation is also known as feature ranking and assesses individ ual features by assigning them weights according to their degrees of relevance on the other hand subset evaluation produces candidate feature subsets based on a certain search strategy besides this classification feature selection methods can also be divided into three models filters wrappers and embedded methods with such a vast body of feature selection meth ods the need arises to find out some criteria that enable users to adequately decide which algorithm to use or not in certain situations this work reviews several feature selection methods in the literature and checks their performance in an artificial controlled experimental scenario contrasting the ability of the algorithms to select the relevant features and to discard the irrelevant ones without permit ting noise or redundancy to obstruct this process a scoring measure will be introduced to compute the degree of matching between the output given by the algorithm and the known optimal solution as well as the classification accuracy finally real experiments are presented in order to check if the conclusions extracted from this theoretical study can be extrapolated to real scenarios state of the art feature selection fs since it is an important activity in data preprocessing has been widely studied in the past years by the machine learning researchers this technique has found suc cess in many different real world applications like dna microarray analysis intrusion detection text categorization or information retrieval including image retrieval or music information retrieval there exist numerous papers and books proving the benefits of the feature selection process however most researchers agree that there is not a so called best method and their efforts are focused on finding a good method for a specific problem setting there fore new feature selection methods are constantly appearing using different strategies a combining several feature selection methods which could be done by using algorithms from the same approach such as two filters or coordinating algorithms from two differ ent approaches usually filters and wrappers b combining fs approaches with other techniques such as feature extraction or tree ensembles c reinterpreting existing algorithms sometimes to adapt them to specific problems d creating new methods to deal with still unresolved situations and e using an ensemble of feature selection techniques to ensure a better behavior bearing in mind the large amount of fs methods available it is easy to note that carrying out a comparative study is an arduous task another problem is to test the effectiveness of these fs methods when real datasets are employed usually without knowing the relevant features a review of feature selection methods in these cases the performance of the fs methods clearly relies on the performance of the learning method used afterward and it can vary notably from one method to another more over performance can be measured using many different metrics such as computer resources memory and time accuracy ratio of features selected etc besides datasets may include a great number of challenges multiple class output noisy data huge number of irrelevant features redundant or repeated features ratio number of samples number of features very close to zero and so on it can be noticed that a comparative study tackling all these consid erations could be unapproachable and therefore most of the interesting comparative studies are focused on the problem to be solved so for example the work in presents an empirical comparison of twelve feature selection methods evaluated on a benchmark of text classification problem instances the comparative study in is used for the detection of breast cancers in mammograms recent works analyze the application of fs in growing research areas such as educational data mining edm other comparative studies are devoted to a specific approach such as in where an experimental study of eight typical filter mutual information based feature selection algorithms on thirty three datasets is pre sented or in that evaluates the capability of the survival relieff algorithm srelieff and of a tuned srelieff approach to properly select the causative pair of attributes similarly there are works examining different fs methods to obtain good performance results using a specific classifier naive bayes in in or the theoretical review for svm in related to datasets challenges there are several works trying to face the problem of high dimensionality in both dimensions samples and features or in one of them i e a high number of features versus low number of samples most of these studies also tackle with the multiple class problems also the majority of current real datasets microarray text retrieval etc also present noisy data however no specific fs comparative studies deal ing with this complex problem were found in the literature although some interesting works have been proposed see for example focusing on nonlinear methods is worth to mention the study of guyon et al finally from a theoretical perspective in a survey of feature selection methods was presented providing some guidelines in selecting feature selection algorithms paving the way to build an integrated system for intelligent feature selection more experimental work on feature selection algorithms for comparative purposes can be found in some of which were performed over artificially generated data like the widely used parity led or monks problems several authors choose to use artificial data since the desired output is known therefore a feature selection algorithm can be evalu ated with independence of the classifier used although the final goal of a feature selection method is to test its effectiveness over a real dataset the first step should be on synthetic data the reason for this is twofold controlledexperimentscanbedevelopedbysystematicallyvaryingchosenexperimental conditions like adding more irrelevant features or noise in the input this fact facilitates to draw more useful conclusions and to test the strengths and weaknesses of the existing algorithms the main advantage of artificial scenarios is the knowledge of the set of optimal features that must be selected thus the degree of closeness to any of these solutions can be assessed in a confident way in this work several feature selection techniques will be tested over synthetic datasets covering a large suite of problems nonlinearity of the data noise in the inputs and in the target increasing number of irrelevant and redundant features etc although works study ing some of these problems can be found up to the authors knowledge a complete v bolón canedo et al table feature selection techniques method advantages disadvantages examples filter consistency based cfs lower computational cost than wrappers interact relieff fast m d good generalization ability information gain mrmr embedded fs percepton lower computational cost than wrappers svm rfe captures feature dependencies wrapper wrapper captures feature dependencies risk of overfitting wrapper svm classifier dependent selection study such as the one described inhere has not been carried out besides a very interesting problem since it is very probable in very datasets such as the alteration of the input variables has not been addressed elsewhere feature selection techniques with regard to the relationship between a feature selection algorithm and the inductive learn ing method used to infer a model three major approaches can be distinguished filters which rely on the general characteristics of training data and carry out the feature selection process as a pre processing step with independence of the induction algorithm wrappers which involve optimizing a predictor as a part of the selection process embedded methods which perform feature selection in the process of training and are usually specific to given learning machines table provides a summary of the characteristics of the three feature selection methods indicating the most prominent advantages and disadvantages as well as some examples of each technique that will be further explained within filters one can distinguish between univariate and multivariate methods univariate methods such as infogain are fast and scalable but ignore feature dependencies on the other hand multivariate filters such as cfs interact etc model feature dependencies but at the cost of being slower and less scalable than univariate techniques besides this classification feature selection methods can also be divided according to two approaches individual evaluation and subset evaluation individual evaluation is independence of the classifier interaction with the classifier interaction with the classifier no interaction with the classifier classifier dependent selection computationally expensive a review of feature selection methods also known as feature ranking and assesses individual features by assigning them weights according to their degrees of relevance on the other hand subset evaluation produces can didate feature subsets based on a certain search strategy each candidate subset is evaluated by a certain evaluation measure and compared with the previous best one with respect to this measure while the individual evaluation is incapable of removing redundant features because redundant features are likely to have similar rankings the subset evaluation approach can handle feature redundancy with feature relevance however methods in this framework can suffer from an inevitable problem caused by searching through feature subsets required in the subset generation step and thus both approaches will be studied in this research the feature selection methods included in this work are subsequently described according to how they combine the feature selection search with the construction of the classification model filter methods wrapper methods and embedded methods see table all of them are available in the weka tool environment or implemented in matlab these feature selection methods belong to different families of techniques and conform an heterogeneous suite of methods to carry out a broad and complete study filter methods correlation based feature selection cfs is a simple multivariate filter algorithm that ranks feature subsets according to a correlation based heuristic evaluation function the bias of the evaluation function is toward subsets that contain features that are highly correlated with the class and uncorrelated with each other irrelevant features should be ignored because they will have low correlation with the class redundant features should be screened out as they will be highly correlated with one or more of the remaining fea tures the acceptance of a feature will depend on the extent to which it predicts classes in areas of the instance space not already predicted by other features the consistency based filter evaluates the worth of a subset of features by the level of consistency in the class values when the training instances are projected onto the subset of attributes the interact algorithm is a subset filter based on symmetrical uncertainty su and the consistency contribution which is an indicator about how significantly the elim ination of a feature will affect consistency the algorithm consists of two major parts in the first part the features are ranked in descending order based on their su values in the second part features are evaluated one by one starting from the end of the ranked feature list if the consistency contribution of a feature is less than an established threshold the feature is removed otherwise it is selected the authors stated that this method can handle feature interaction and efficiently selects relevant features information gain is one of the most common attribute evaluation methods this univariate filter provides an ordered ranking of all the features and then a threshold is required in this work the threshold will be set up selecting the features which obtain a positive information gain value relieff is an extension of the original relief algorithm the original relief works by randomly sampling an instance from the data and then locating its nearest neighbor from the same and opposite class the values of the attributes of the nearest neighbors are compared to the sampled instance and used to update relevance scores for each attribute the rationale is that an useful attribute should differentiate between instances from different classes and have the same value for instances from the same class relieff adds the ability of dealing with multiclass problems and is also more robust and capable of dealing with incomplete and noisy data this method may be applied in v bolón canedo et al all situations has low bias includes interaction among features and may capture local dependencies which other methods miss the mrmr minimum redundancy maximum relevance method selects features that have the highest relevance with the target class and are also minimally redundant i e selects features that are maximally dissimilar to each other both optimization criteria maximum relevance and minimum redundancy are based on mutual information the m d filter is an extension of mrmr which uses a measure of monotone depen dence instead of mutual information to assess relevance and irrelevance one of its contributions is the inclusion of a free parameter λ that controls the relative emphasis given on relevance and redundancy in this work two values of lambda will be tested and when λ is equal to zero the effect of the redundancy disappears and the measure is based only on maximizing the relevance on the other hand when λ is equal to one it is more important to minimize the redundancy among variables these two values of λ were chosen because we are interested in checking the performance of the method when the effect of the redundancy disappears also the authors state that λ performs better than other λ values embedded methods svm rfe recursive feature elimination for support vector machines was introduced by guyon in this embedded method performs feature selection by iteratively train ing a svm classifier with the current set of features and removing the least important feature indicated by the svm two versions of this methods will be tested the original one using a linear kernel and an extension using a nonlinear kernel in order to solve more complex problems fs p feature selection perceptron is an embedded method based on a percep tron a perceptron is a type of artificial neural network that can be seen as the simplest kind of feedforward neural network a linear classifier the basic idea of this method con sists on training a perceptron in the context of supervised learning the interconnection weights are used as indicators of which features could be the most relevant and provide a ranking wrapper methods wrappersubseteval evaluates attribute sets by using a learning scheme cross val idation is used to estimate the accuracy of the learning scheme for a set of attributes the algorithm starts with the empty set of attributes and searches forward adding attri butes until performance does not improve further in this work two well known learning schemes will be used svm and artificial datasets as was stated in sect the first step to test the effectiveness of a feature selection method should be on synthetic data since the knowledge of the optimal features and the chance to modify the experimental conditions allows to draw more useful conclusions the datasets chosen for this study try to cover different problems increasing number of irrelevant features redundancy noise in the output alteration of the inputs nonlinearity of a review of feature selection methods table summary of the synthetic datasets used corr stands for correlation dataset no of features no of samples relevant features corr noise non no feat baseline linear no samples accuracy corral corral xor led led g g g g g g madelon g i means that the feature selection method must select only one feature within the i th group of features the data etc these factors complicate the task of the feature selection methods which are very affected by them as it will be shown afterward besides some of the datasets have a significantly higher number of features than samples which implies an added difficulty for the correct selection of the relevant features the synthetic datasets employed are subsequently described and table shows a sum mary of the different problems covered by them as well as the number of features and samples and the relevant attributes which should be selected by the feature selection meth ods besides the baseline accuracy is shown in last column which indicates the minimum achievable accuracy when all samples are assigned to the majority class corral the corral dataset has six binary features i e f f f f f f and its class value is f f f f feature f is irrelevant and f is correlated to the class label by corral was constructed by adding irrelevant binary features to the previous corral dataset the data for the added features were generated randomly both datasets corral and corral have samples that are formed by considering all possible val ues of the four relevant features and the correlated one the correct behavior for a given feature selection method is to select the four relevant features and to discard the irrelevant and correlated ones the correlated feature is redundant if the four relevant features are selected and besides it is correlated to the class label by so if one applies a classifier after the feature selection process a of error will be obtained xor xor has relevant binary features and irrelevant binary features randomly generated the class attribute takes binary values and the dataset consists of samples features f and f are correlated with the class value with xor operation i e class equals f f this is a hard dataset for the sake of feature selection because of the small ratio v bolón canedo et al between number of samples and number of features and due to its nonlinearity unlike corral dataset which is a multi variate dataset the parity problem is a classic problem where the output is f x x n if the number of x i is odd and f x x n otherwise the dataset is a modified version of the original parity dataset the target concept is the parity of three bits it contains features among which are relevant another are redundant repeated and other are irrelevant randomly generated the led problem the led problem is a simple classification task that consists of given the active leds on a seven segments display identifying the digit that the display is representing thus the classification task to be solved is described by seven binary attributes see fig and ten possible classes available c a in a attribute indicates that the led is active and a indicates that it is not active two versions of the led problem will be used the first one adding irrelevant attributes with random binary values and the second one adding irrelevant attri butes both versions contain samples the small number of samples was chosen because we are interested in dealing with datasets with a high number of features and a small sample size besides different levels of noise altered inputs have been added to the attributes of these two versions of the led dataset and in this manner the tolerance to different levels of noise of the feature selection methods tested will be checked note that as the attributes take binary values adding noise means assigning to the relevant features an incorrect value the monk problems rely on an artificial robot domain in which robots are described by six different attributes x x the learning task is a binary classification task x x x x x x x fig led scheme a review of feature selection methods the logical description of the class of the third problem is the following x x x x among the samples are misclassifications i e noise in the target and these three synthetic datasets and are challenging problems because of their high number of features around and the small number of samples besides of a high number of irrelevant attributes these characteristics reflect the problematic of micro array data and it is necessary to introduce some new definitions of multiclass relevancy features full class relevant fcr and partial class relevant pcr features specifically fcr denotes genes features that serve as candidate biomarkers for discriminating all can cer types however pcr are genes features that distinguish subsets of cancer types and are three class datasets with samples each class containing samples generated based on the approach described in each synthetic dataset consists of both relevant and irrelevant features the relevant features in each dataset are generated from a multivariate normal distribution using mean and covariance matrixes besides irrelevant features are added to each dataset where are drawn from a normal distribution of n and the other are sampled with a uniform distribution u is designed to contain only fcr and irrelevant features two groups of relevant genes are generated from a multivariate normal distribution with genes in each group genes in the same group are redundant with each other and the optimal gene subset for distinguishing the three classes consists of any two relevant genes from different groups is designed to contain fcr pcr and irrelevant features four groups of relevant i e fcr and pcr genes are generated from a multivariate normal distribution with genes in each group genes in each group are redundant to each other and in this dataset only genes in the first group are fcr genes while genes in the three last groups are pcr genes the optimal gene subset to distinguish all the three classes consists of four genes one fcr gene from the first group and three pcr genes each from one of the three remaining groups has been designed to contain only pcr and irrelevant features six groups of relevant genes are generated from a multivariate normal distribution with genes in each group genes in the same group are designed to be redundant to each other and the optimal gene subset to distinguish all the three classes thus consists of six genes with one from each group it has to be noted that the easiest dataset in order to detect relevant features is since it contains only fcr features and the hardest one is due to the fact that it contains only pcr genes which are more difficult to detect madelon the madelon dataset is a class problem originally proposed in the nips feature selection challenge the relevant features are situated on the vertices of a five dimensional hypercube five redundant features were added obtained by multiplying the useful features by a random matrix some of the previously defined features were repeated to create more features the other features are drawn from a gaussian distribution and labeled ran domly this dataset presents high dimensionality both in number of features and in number v bolón canedo et al of samples and the data were distorted by adding noise flipping labels shifting and rescaling for all these reasons it conforms a hard dataset for the sake of feature selection experimental settings twelve different feature selection methods are tested and compared in this work in order to draw useful conclusions as was mentioned in the introduction there exist two major approaches in feature selection individual evaluation and subset evaluation individual eval uation provides an ordered ranking of the features while subset evaluation produces a can didate feature subset when a ranking of the features is returned it is necessary to establish a threshold in order to discard the features less relevant for the algorithm unfortunately where to establish the threshold is not an easy to solve question belanche et al opted for discarding those weights associated to the ranking which were further than two variances from the mean on the other hand mejía lavalle et al use a threshold defined by the largest gap between two consecutive ranked attributes and other authors just studied the whole ranking paying more attention to the first ranked features however in this work it is impossible to use a threshold related to the weights associated to the ranking since some of the ranker methods svm rfe mrmr and m d eliminate chunks of features at a time and do not provide weights to solve this problem and for the sake of fairness in these experiments we heuristically set the following rules to decide the number of features that ranker methods should return according to the number of total features n if n select of features if n select of features if n select of features ifn select of features at this point it has to be clarified that the datasets sd due to their particularities will be analyzed in a different manner which will be explained in sect according to the rules showed above the number of features that will be returned by ranker methods is for the datasets corral and for the datasets corral xor and both versions of led and for madelon a scoring measure was defined in order to fairly compare the effectiveness showed by the different feature selection methods the measure presented is a index of success suc see which attempts to reward the selection of relevant features and to penalize the inclusion of irrelevant ones penalizing two situations the solution is incomplete there are relevant features lacking the solution is incorrect there are some irrelevant features suc r r t where r α i i t is the number of relevant features selected r t is the total number of relevant fea tures i is the number of irrelevant features selected and i t is the total number of irrelevant features the termα was introduced to ponder that choosing an irrelevant feature is better than missing a relevant one i e we prefer an incorrect solution rather than an incomplete one the parameter α is defined as α min r the method and is the maximum i t t note that the higher the success the better in the case of ranker methods and in order to be fair if all the optimal features are selected before any irrelevant feature the index of success will be due to the fact that the number a review of feature selection methods of features that ranker methods are forced to select is always larger than the number of relevant features as was explained above the evaluation of the feature selection methods is done by count ing the number of correct wrong features however it is also interesting and a common practice in the literature to see the classification accuracy obtained in a fold cross validation in order to check if the true model that is the one with an index of success of is also unique that is if is the only one that can achieve the best percentage of classi fication success for this purpose four well known classifiers based on different models were chosen naive bayes nb and svm experimental evi dence has shown that decision trees such as exhibit a degradation in the performance when faced with many irrelevant features similarly instance based learners such as are also very susceptible to irrelevant features it has been shown that the number of train ing instances needed to produce a predetermined level of performance for instance based learning increases exponentially with the number of irrelevant features present on the other hand algorithms such as naive bayes are robust with respect to irrelevant features degrading their performance very slowly when more irrelevant features are added however the performance of such algorithms deteriorates quickly by adding redundant features even if they are relevant to the concept finally svm can indeed suffer in high dimensional spaces where many features are irrelevant experimental results in this section the results after applying different feature selection methods over datasets will be presented grouped in different families which deal with situations such as presence of noise both in the inputs and in the class irrelevant features redundancy etc the behavior of the feature selection methods will be tested according to the proposed index of success see eq and the classification accuracy obtained by different classifiers it is necessary to note that all the feature selection methods tested in this work are determin istic i e the set of selected features is unique therefore it is not necessary to repeat the experiments and average them in all tables of this section the best index of success is highlighted in bold face while best accuracy for each classifier is shaded and best accuracy for all classifiers is also in bold face columns rel irr and suc refer to the evaluation via counting the number of correct features selected while the remaining columns show the classification accuracy obtained by four different classifiers it has to be noted that for the calculation of the index of success the redundant attributes selected have the same penalization than the irrelevant features the results presented in this section will be analyzed and discussed in sect after presenting some cases of study in sect dealing with correlation and redundancy corral two versions of this well known datasets were used corral the classic dataset and corral formed by adding irrelevant binary features the desired behavior of a feature selection method is to select the relevant features and to discard the irrelevant ones as well as detecting the correlated feature and not selecting it tables and show the results obtained over the datasets corral and corral respec tively over corral fs p was able to select the desired set of features which led to classification accuracy obtained by classifier regarding corral it is curious that the v bolón canedo et al table results for corral method rel c irr suc accuracy nb svm cfs consistency interact infogain relieff mrmr m d λ m d λ svm rfe svm rfe nonlinear fs p wrapper svm wrapper rel shows the relevant features selected c indicates if the correlated feature is selected or not irr means the number of irrelevant features selected and suc represents the index of success table results for corral method rel c irr suc accuracy nb svm cfs consistency interact infogain relieff mrmr m d λ m d λ svm rfe svm rfe non linear fs p wrapper svm wrapper rel shows the relevant features selected c indicates if the correlated feature is selected or not irr means the number of irrelevant features selected and suc represents the index of success best classification accuracy was obtained by svm rfe which has a index of success of but this fact can be explained because in this dataset there are some irrelevant features that are informative to the classifiers this fact will be further analyzed in sect dealing with nonlinearity xor and parity in this subsection two nonlinear problems will be tested xor contains relevant fea tures and irrelevant features while has relevant redundant and irrelevant a review of feature selection methods table results for xor method rel irr suc accuracy nb svm relieff mrmr m d λ m d λ svm rfe svm rfe non linear fs p wrapper svm wrapper rel shows the relevant features selected red indicates the number of redundant features selected irr means the number of irrelevant features selected and suc represents the index of success table results for method rel red irr suc accuracy nb svm relieff mrmr m d λ m d λ svm rfe svm rfe non linear fs p wrapper svm wrapper rel shows the relevant features selected red indicates the number of redundant features selected irr means the number of irrelevant features selected and suc represents the index of success features the ability of feature selection methods to deal with relevance irrelevance and redundancy will be checked over two nonlinear scenarios in the case of xor with the added handicap of a small ratio between number of samples and number of features for the sake of completeness svm and naive bayes will be applied over these two datasets however bearing in mind that those methods are linear classifiers a linear kernel is being used for svm and no good results are expected so they will not be the focus in the analysis in sect as can be seen in tables and the methods cfs consistency interact and info gain do not appear because they were not able to solve these nonlinear problems so they returned an empty subset of features in those cases the classifiers were not applied either because with no features the maximum achievable accuracy is known and it coincides with the baseline accuracy shown in table on the other hand the filter relieff and the embedded method svm rfe with a nonlinear kernel detected the relevant features both in xor and in achieving the best indices of success and leading to high classification accuracies v bolón canedo et al dealing with noise in the inputs led the led dataset consists of correctly identifying seven leds that represent numbers between and some irrelevant features were added forming the led dataset irrelevant fea tures and the led dataset irrelevant attributes in order to make these datasets more complex different levels of noise in the inputs and were added it has to be noted that as the attributes take binary values adding noise means assigning to the relevant features an incorrect value in tables and one can see detailed results of these experiments it is interesting to note that subset filters cfs consistency and interact and the ranker filter information gain which has a behavior similar to subset filters do not select any of the irrelevant features in any case at the expense of discarding some of the relevant ones especially with high levels of noise with regard to the classification accuracy it decreases as the level of noise increases as expected dealing with noise in the target in this subsection the problem which includes a of misclassifications i e noise in the target will be tested the relevant features are x x and x however as it was stated in for a feature selection algorithm it is easy to find the variables x and x which together yield the second conjunction in the expression seen in sect according to the experimental results presented in selecting those features can lead to a better perfor mance than selecting the three relevant ones this additional information can help to explain the fact that in table several algorithms selected only two of the relevant features studying the index of success in table one can see that only relieff achieved a value of the worst behavior was showed by mrmr since it selected the three irrelevant features as was justified above many methods selected only two of the relevant features and it can be considered a good comportment for classifier the best accuracy corresponds to relieff which also obtained the best result in terms of index of success dealing with a small ratio samples features and these synthetic datasets have a small ratio between number of samples and features which makes difficult the task of feature selection this is the problematic present in microarray data a hard challenge for machine learning researchers besides these particularities of the data there is a high number of irrelevant features for the task of gene classification and also the presence of redundant variables is a critical issue for these datasets besides of using the index of success and classification accuracy we will use the measures employed in which are more specific for this problem hence the performance of and will be also evaluated in terms of number of selected features opt x number of selected features within the optimal subset where x indicates the optimal number of features red number of redundant features irr number of irrelevant features for the ranker methods relieff mrmr m d svm rfe and fs p two different cardi nalities were tested the optimal number of features and since the subset methods have a similar cardinality it has to be noted that in this problem and for the calculation of the index of success redundant features are treated the same as irrelevant features in eq notice a review of feature selection methods table results for led dataset with different levels of noise n in inputs n method relevant irr no suc nb accuracy svm cfs consistency interact infogain relieff mrmr m d λ m d λ svm rfe svm rfe non linear fs p wrapper svm wrapper cfs consistency interact infogain relieff mrmr m d λ m d λ svm rfe svm rfe non linear fs p wrapper svm wrapper cfs consistency interact infogain relieff mrmr m d λ m d λ svm rfe svm rfe non linear 59 fs p wrapper svm wrapper v bolón canedo et al table continued n method relevant irr no suc accuracy nb svm cfs consistency interact infogain relieff mrmr m d λ m d λ svm rfe svm rfe non linear 59 fs p wrapper svm wrapper cfs consistency interact infogain relieff mrmr m d λ m d λ svm rfe svm rfe non linear fs p wrapper svm wrapper cfs consistency interact infogain relieff mrmr m d λ m d λ svm rfe svm rfe non linear fs p wrapper svm wrapper that the index of success is even with irrelevant features selected due to the high number of irrelevant features studying the selected features the subset filters and infogain which exhibits a similar behavior showed excellent results in all and also svm rfe obtained a review of feature selection methods table results for led dataset with different levels of noise n in inputs n method relevant irr no suc nb accuracy svm cfs consistency interact infogain relieff mrmr m d λ m d λ svm rfe svm rfe non linear fs p wrapper svm wrapper cfs consistency interact infogain relieff 99 92 mrmr m d λ m d λ svm rfe svm rfe non linear fs p 99 wrapper svm wrapper cfs consistency interact infogain relieff mrmr m d λ m d λ svm rfe 00 00 svm rfe non linear 00 00 00 00 fs p 00 00 00 00 wrapper svm 99 00 00 00 00 wrapper 00 00 00 00 v bolón canedo et al table continued n method relevant irr no suc accuracy nb svm cfs 00 00 00 00 consistency 00 00 00 00 interact 00 00 00 00 infogain 00 00 00 00 relieff 00 00 00 00 mrmr 71 00 00 00 00 m d λ 71 00 00 72 00 00 m d λ 71 00 00 00 00 svm rfe 00 00 00 00 svm rfe non linear 00 00 00 00 fs p 00 00 00 00 wrapper svm 72 00 00 00 00 wrapper 76 00 00 00 00 cfs 00 00 00 00 consistency 00 00 00 00 interact 00 00 00 00 infogain 00 00 00 00 relieff 71 00 00 00 00 mrmr 71 00 00 00 00 m d λ 71 00 00 00 00 m d λ 71 00 00 00 56 00 svm rfe 00 00 00 00 svm rfe non linear 00 00 00 00 fs p 71 00 00 00 00 wrapper svm 00 00 00 00 wrapper 00 00 00 00 cfs 00 00 00 00 consistency 00 00 00 00 interact 00 00 00 00 infogain 00 00 00 00 relieff 00 00 00 00 mrmr 00 00 00 00 m d λ 00 00 00 00 m d λ 00 00 00 00 svm rfe 00 00 00 00 svm rfe non linear 57 00 00 00 00 fs p 71 00 00 00 00 wrapper svm 00 00 00 00 wrapper 28 00 00 28 00 00 good results although the version with a nonlinear kernel could not been applied on these datasets due to memory complexity with respect to the classifiers svm achieves the highest accuracies a review of feature selection methods table results for method relevant irr no suc accuracy nb svm cfs consistency 93 interact 93 infogain 93 relieff 93 mrmr 92 m d λ 93 m d λ 93 88 81 svm rfe 93 88 svm rfe non linear 93 88 84 84 fs p 93 88 84 84 43 wrapper svm 93 wrapper 93 88 79 relevant features table results for madelon method relevant red no irr no suc accuracy nb svm cfs 92 consistency interact infogain relieff 84 88 mrmr 92 57 m d λ 29 m d λ 29 svm rfe 88 81 fs p 70 wrapper svm 63 08 wrapper 99 04 70 00 relevant features dealing with a complex dataset madelon madelon is a very complex artificial dataset which is distorted by adding noise flipping labels shifting and rescaling it is also a nonlinear problem so it conforms a challenge for feature selection researchers the desired behavior for a feature selection method is to select the relevant features and discard the redundant and irrelevant ones table shows the relevant features selected by the feature selection methods as well and the number of redundant and irrelevant features selected by them and the classification accuracy notice that for the calculation of index of success the redundant attributes selected stand for irrelevant features again the results for svm and naive bayes will not be analyzed since they are linear classifiers the best result in terms of index of success was obtained v bolón canedo et al fig svm rfe linear vs nonlinear kernel the vertical axis represents the index of success by the wrapper with selecting all the relevant features which also led to the best classification accuracy for cases of study after presenting the experimental results and before discussing and analyzing them in detail we will describe several cases of study in order to decide among similar methods these cases of study will be based on the index of success to make this analysis classifier independent case of study i linear vs nonlinear kernel for svm rfe as was stated in sect two different kernels were applied on the embedded method svm rfe a nonlinear kernel allows to solve nonlinear problems but at the expense of being more computationally demanding in fact svm rfe with a nonlinear kernel could not be applied on the datasets sd and madelon due to the space complexity in fig one can see a comparison of these two versions of the method note that as for both led and led datasets there are results for different levels of noise in the inputs we have opted for computing the average of the index of success as expected the linear kernel is not able to deal with nonlinear problems xor and on the other hand the nonlinear kernel achieves a poor result over corral dataset where the number of irrelevant features increases considerably in the remaining datasets the nonlinear kernel maintains or increases the performance of the linear kernel in these cases it is necessary to bear in mind that the nonlinear kernel raises the computational time requirements of the algorithms and it cannot be applied over high dimensional datasets such as madelon and the sd family for example over xor dataset it takes almost times more time to use the nonlinear kernel than the linear one the authors suggest to use the nonlinear kernel when there is some knowledge about the nonlinearity of the problem and to use the linear kernel in the remaining cases specially when dealing with large amounts of data case of study ii mrmr vs m d the filter method m d is an extension of mrmr which instead of mutual information uses a measure of dependence to assess relevance and irrelevance besides it included a free a review of feature selection methods fig mrmr vs m d the vertical axis represents the index of success parameter λ that controls the relative emphasis given on relevance and irrelevance in light of the above the authors think that it is interesting to compare the behaviors showed by these two methods over the artificial datasets studied in this work two values of lambda were tested and and it is also important to see the difference between them when λ is equal to zero the effect of the redundancy disappears and the measure is based only on maximizing the relevance on the other hand when λ is equal to one it is more important to minimize the redundancy for the sake of fairness note that for the sd family of datasets we considered the results achieved selecting features with regard to the different values of λ one can see in fig that the index of success is the same for most of the datasets tested out of however there is a important improvement in and when the value of λ is zero therefore the authors suggest to use this value of λ although the appropriate value of λ is not an easy to solve question that requires to be studied further and seems to be very dependent of the nature of data comparing m d and mrmr the latter performs better in two datasets and whereas m d is better in datasets madelon and the sd family in the remaining datasets the index of success achieved by both methods is the same in light of these results the authors recommend the use of m d except in datasets with high nonlinearity case of study iii subset filters subset evaluation produces candidate feature subsets based on a certain search strategy each candidate subset is evaluated by a certain evaluation measure and compared with the previ ous best one with respect to this measure this approach can handle feature redundancy with feature relevance besides of releasing the user from the task of choosing how many features to retain in fig one can see a comparison among the three subset filters studied in this work cfs interact and consistency based with regard to the index of success all the three methods show in general a very similar behavior although some differences have been found consistency based is slightly worse on datasets which present noise this can be explained because for this filter a pattern is considered inconsistent if there exist at least two instances such that they match all but their class labels and therefore the given subset of features is inconsistent and the features are discarded this case can happen when the data have been distorted with noise on the other hand cfs decays on madelon as each feature is treated individually this algorithm cannot identify really strong interactions v bolón canedo et al fig subset filters the vertical axis represents the index of success as the ones which may appear in parity problems remind that madelon is a generalization of a parity problem in light of the above the authors suggest to use interact case of study iv different levels of noise in the input figure shows an overview of the behavior of feature selection methods with regard to different levels of noise according to the index of success described in as we would have expected in general the index of success decreases when the level of noise increases and worse performances were obtained over led due to the higher number of irrelevant features it may seem strange that in some cases the index of success improves with higher levels of noise for example in led from to of noise but this fact can be explained by the random generation of the noise notice that the influence of each relevant feature is not the same in this problem so adding noise to one or another may cause different results in fact the first five features see fig are enough to distinguish among the ten digits therefore if these attributes are distorted the result may be altered several conclusions can be extracted from the graphs in fig regarding the wrapper model both versions tested degrade their results with the presence of noise both in led and led with respect to embedded methods two opposite behaviors have been observed on the one hand fs p achieved very promising results on both datasets without showing degradation as the level of noise increases in fact the index of success oscillates between 76 and on led fig and between 71 and 99 on led fig on the other hand svm rfe specially the version with the linear kernel deteriorates considerably its behavior with high levels of noise note that svm rfe with linear kernel obtained as index of success on led and on led which is the worst results for all the feature selection methods tested concerning the filter model mrmr and relieff are the methods that achieve the best indices of success being relieff slightly better in two cases led with and of noise these two filters obtain very good results without being very affected by noise on the contrary the subset filters cfs consistency and interact and information gain are affected by high levels of noise although they are robust to the addition of irrelevant features finally with respect to m d it attains constant results particularly on led and no significative differences have been found between the two values of λ tested it is curious the opposite behaviors of information gain and mrmr bearing in mind that both come from the information theory field however this fact can be explained because a review of feature selection methods fig results for led and led a led b led c legend 70 e c c u level of noise a 70 e c c u level of noise b c v bolón canedo et al table average of success for every feature selection method tested correlation non linear noise high dimension method corr sd mad av cfs 29 consistency interact 81 infogain relieff 93 mrmr 99 56 76 m d a 99 76 m d b 99 76 76 57 svm rfe 89 svm rfe c 66 64 n a n a n a fs p 93 wrap svm 57 83 wrap 99 29 n a a b c λ λ nonlinear stands for not applicable kernel information gain is a univariate measure that considers the entropy between a given fea ture and the class level on the other hand mrmr takes into account the mutual information among features the latter is a multivariate measure and therefore a better behavior is expected when noise is present in data because although some features may be affected by noise in a sample not all of them are supposed to suffer it this is why information gain obtains excellent results with low levels of noise but as it increases its performance decays until reaching an index of success with value to sum up the filters mrmr and relieff and the embedded method fs p are the methods most tolerant to noise in the inputs and the subsets filters cfs consistency and interact and information gain are the most affected by noise analysis and discussion in this section an analysis and discussion of the results presented in sect will be carried out trying to check which method is the best and to explain some behaviors showed in the experimental results in sect we start analyzing the index of success while in sect we will discuss the relation between index of success and classification accuracy focusing on the specific problems studied in this work analysis of index of success table shows the average of success for each feature selection method over each scenario and also an overall average for each method last column for and led100 only one result is presented respectively corresponding to the average of the results for the distinct levels of noise tested analogously for the sd family of datasets only the average result of the datasets is shown a review of feature selection methods we are interested in an analysis of the index of success regardless of the classification accuracy in order to check the behavior of the feature selection methods in a classifier inde pendent manner in light of the results shown in table the best method according to the index of success is the filter relieff followed by the filters mrmr and both versions of m d however the subset filters and information gain which has a similar behavior than those showed poor results regarding the embedded model fs p is slightly better than svm rfe and both of them are in the middle of the ranking finally wrapper methods turned out to be the worst option in this study since they achieved the poorest averages of success in light of the results presented in table the authors suggest some guidelines in complete ignorance of the particulars of data the authors suggest to use the filter relieff it detects relevance in a satisfactory manner even in complex datasets such as xor and it is tolerant to noise both in the inputs and in the output moreover due to the fact that it is a filter it has the implicit advantage of its low computational cost when dealing with high nonlinearity of data such as xor and svm rfe with a nonlinear kernel is an excellent choice since it is able to solve these complex problems however at the expense of being computationally more expensive than the remaining approaches seen in this work in the presence of altered inputs the best option is to use the embedded method fs p since it has proved to be very robust to noise a less expensive alternative is the use of the filters relieff or mrmr which also showed good behaviors over this scenario with low levels of noise up to the authors also suggest the use of the filter information gain when the goal is to select the smallest number of irrelevant features even at the expense of selecting fewer relevant features we suggest to employ one of the subset filters cfs consistency based or interact this kind of methods have the advantage of releasing the user from the task of deciding how many features to choose when dealing with datasets with a small ratio between number of samples and features and a high number of irrelevant attributes which is part of the problematics of microarray data the subset filters and information gain presented a promising behavior svm rfe performs also adequately but because of being an embedded method is computationally expensive especially in high dimensional datasets like these in general the authors suggest the use of filters specifically relieff since they carry out the feature selection process with independence of the induction algorithm and are faster than embedded and wrapper methods however in case of using another approach we suggest to use the embedded method fs p as was stated in table filters are the methods with the lower computational cost while wrappers are computationally the most expensive to illustrate this fact in fig one can see the execution time of the different feature selection methods over two datasets xor and led with no noise in order to be fair mrmr was not included in this study because it was executed in a different machine however one can expect a computational time similar to the one required by m d as expected the filter model achieves the lowest execution times always below second the embedded methods require more computational time specially svm rfe with a nonlin ear kernel in fact the time required by this method over led is especially high because this is a multiclass dataset a fact that also increases the computational time wrapper svm contrary to wrapper is a very demanding method particularly with led which needed almost h v bolón canedo et al xor led fig time in seconds for the datasets xor and led analysis of classification accuracy although the previous analysis is interesting because it is not classifier dependent one may want to see the classification accuracy in order to check if the desired subset of features is unique and if it is the best option for the sake of brevity only the two extreme values of noise for led and led datasets were included in this study table shows for each clas sifier and dataset the best accuracy obtained as well as the corresponding index of success and feature selection method employed in this manner it is easy to see at a glance if the best accuracy matches with the best index of success in fact this happens for all datasets except led with of noise where the inputs are clearly disturbed this may be explained because the irrelevant features randomly generated are adding some information useful to the classifier while the disturbed relevant features are not so informative classifier based on nearest neighbors seems to be the best match for the proposed index of success since it obtains the best result for classification when the index obtains also its best result specifically in out of datasets tested instance based learners are very susceptible to irrelevant features therefore when a feature selection method only selects the relevant features its index of success is high and also the classification accuracy obtained by this classifier it has to be also noted that is a nonlinear classifier therefore it is capable to correctly classify problems such as xor or achieving of classification accuracy when other methods like svm obtained poor results svm obtained the highest classification accuracy in out of datasets showed in table however it only coincides with the highest index of success in dataset this predictor takes advantage of the embedded method svm rfe and wrapper svm both methods using this classifier performance to select the features in fact the highest accura cies were obtained after applying one of those methods for all datasets except for led with of noise although the behavior of the classifiers is interesting one may want to focus on the prob lems studied in this work for dealing with correlation and redundancy two datasets were evaluated in this paper corral and corral see tables and focusing on corral the subset filters infogain and the wrappers selected only the correlated feature which leads to an accuracy of for all the classifiers except which apparently is not able to take advantage of the relation between this feature and the class when the four relevant features plus the correlated one are selected rows at previous tables one can see that the s f c n c t i o n n i a f f e f i e r m m l n e p f g o f i m v w w s s filters embedded wrappers l e d d r m f r s s c n v m v a review of feature selection methods 510 v bolón canedo et al correlated feature since it is correlated for of data is hindering the process of classifi cation preventing the predictors to correctly classify all samples fs p was the only method that selected the four relevant features and discarded the irrelevant and correlated ones was able to achieve a of classification accuracy while the other methods were not this fact is explained because of the complexity of the problem that may cause that a given classifier may not solve the problem satisfactorily even with the proper features regarding corral the highest accuracy 88 was obtained by svm having only one of the relevant features the correlated one and irrelevant ones this fact can seem surprising but it can be explained because the irrelevant features randomly generated are informative in this problem classifying only with the relevant feature and the correlated one svm achieves of classification accuracy therefore it is clear that the irrelevant features are adding some useful information to the learner in fact by randomly generating binary features and having only samples the probability that some of these irrelevant features could be correlated with the class is very high this situation happens again with wrapper and classifier while the remaining methods exhibit a similar behavior than on corral nonlinearity is a difficult problem to deal with in fact two of the classifiers employed in this work svm with linear kernel and naive bayes and several feature selection meth ods do not turn very good results this problematic is present in xor and datasets in tables and as we have said naive bayes and svm cannot deal with nonlin earity therefore they will not be the focus in this section on the other hand and only over xor achieve of classification accuracy when the desired features are selected over xor obtains also of prediction accuracy after applying its own wrapper even when it selected two extra irrelevant features it has to be noted that this classifier performs an embedded selection of the features therefore it may be using a subset of features smaller than the given by the feature selection method finally it needs to be remarked the improvement in svm rfe when using a nonlinear kernel for this kind of datasets svm rfe over xor did not select any of the relevant features which led to a classification accuracy below the baseline accuracy however the computational time when using a nonlinear kernel is almost four times the one needed using the classical svm rfe see table so when choosing one or the other it is a fact to bear in mind different levels of noise in the input features were tested over led table and led table datasets as expected the classification accuracy decreases when the level of noise increases it has to be noted that for both datasets selecting out of the rele vant features is enough to achieve classification accuracy because segments see fig are enough to distinguish the digits actually binary features allow to represent different states in fact when the level of noise is the first four methods miss the third feature which allows to distinguish between digit and and the performance decays in which cannot be ascribed to the level of noise this is a case of classification show ing that the true model is not unique on the other hand it is curious that in some cases such as relieff over led with of noise where it achieves an index of success of 93 selecting the relevant features the maximum classification accuracy obtained with these features was svm which is not the result expected this fact can be explained because of the high level of noise which corrupts the relevant features and makes the classification task very complex in those cases with high levels of noise wrappers appear to be a good alternative since they are classifier dependent and try to search for the best features to the given classifier to sum up the filters mrmr and relieff and the embedded method fs p are the methods most tolerant to noise in the inputs and the subsets filters cfs consistency and interact and information gain are the most affected by noise although wrappers are also a good choice if one is interested in maximizing the classification accuracy a review of feature selection methods table features selected by each algorithm on synthetic dataset opt red irr suc accuracy nb svm cfs 28 57 69 cons 76 00 00 66 int 00 81 66 00 ig 72 00 70 relieff a 00 44 00 relieff b 00 70 mrmr a mrmr b 54 68 00 78 svm rfe a 56 00 00 52 00 57 svm rfe b 88 00 76 00 92 00 fs p a fs p b 76 00 m d a 00 00 44 00 m d λ b 56 00 66 m d λ a 00 57 m d λ λ b 54 46 66 w svm 44 00 w 00 ranker a b selecting selecting methods the features optimal are tested number selecting of features the optimal number and features as cardinality dataset see table is studied to deal with noise in the target as was explained in sect there are evidences that features x and x are enough for certain classifier which in fact happens in the experiments presented in this work this is an example of the optimal feature subset being different than the subset of relevant features on the other hand again one can see the implicit capacity of to select features since it achieves the same result in cases where different subsets of features were selected although for mrmr some of the irrelevant features caused the incorrect classification of one extra feature this is not the case for classifier which achieves the highest accuracy only when the known set of relevant features is selected naive bayes and svm seem to be more affected by misclassifications since they obtain the worst results and do not take advantage of the best indices of success and tables and introduce the problematic of microarray data a small ratio between number of samples and features and a high number of redundant and irrelevant features in general the classification results are poor because this kind of prob lems are very difficult to solve since the classifiers tend to overfit moreover the accuracy decreases when the complexity of the dataset increases the embedded method svm rfe achieves very good results specially with the svm classifier cfs and interact filters also work satisfactorily together with naive bayes and classifiers the small ratio between number of samples and features prevents the use of wrappers which have the risk of overfitting due to the small sample size in fact one can see in tables and that the wrappers obtain high accuracies in conjunction with their corresponding classifiers but the performance decreases when using other classifiers regarding the classifiers svm achieves good results specially over svms have many mathematical features that make them v bolón canedo et al table features selected by each algorithm on synthetic dataset opt red irr suc nb accuracy svm cfs 64 00 84 00 72 00 81 cons 54 70 00 72 00 int 70 00 81 ig 69 76 00 relieff a 00 64 00 52 00 relieff b 54 00 70 mrmr a 54 64 00 00 57 mrmr b 00 70 44 00 68 00 svm rfe a 75 46 62 54 svm rfe b 57 69 84 00 fs p a 54 00 57 fs p b 52 00 68 00 m d a 56 00 56 00 m d λ b 54 64 00 68 00 m d λ a 46 69 56 00 69 m d λ λ b 52 00 62 00 w svm 44 00 00 w 72 00 46 34 ranker a b selecting selecting methods the features optimal are tested number selecting of features the optimal number and features as cardinality attractive for gene expression analysis including their flexibility in choosing a similarity function sparseness of solution when dealing with large datasets the ability to handle large feature spaces and the ability to identify outliers 78 naive bayes obtained also high accu racies specially over and this learner is robust with respect to irrelevant features although it deteriorates quickly by adding redundant features in fact it obtains the best accuracies when a small number of redundant features are present madelon table is a complex dataset which includes noise flipping labels and non linearity due to the latter naive bayes and svm cannot obtain satisfactory results so they will not be analyzed obtained its highest accuracy after applying its own wrapper as expected it is more surprising the behavior of which obtained the highest predic tion accuracy after applying methods that achieve poor indices of success however this fact can be explained because these methods selected a high number of redundant features these redundant features were built by multiplying the useful features by a random matrix therefore they are also informative real datasets in order to check if the behaviors showed by the different feature selection methods can be extrapolated to the real world two real datasets were chosen the first one colon cancer is colon cancer dataset is available on http datam a star edu sg datasets krbd a review of feature selection methods table features selected by each algorithm on synthetic dataset opt red irr suc nb accuracy svm cfs 64 00 00 73 70 cons 76 00 62 76 00 int 70 66 ig 62 73 relieff a 50 57 relieff b 56 00 69 68 00 mrmr a 62 62 66 mrmr b 50 52 00 66 svm rfe a 50 56 00 70 61 65 svm rfe b 70 fs p a 00 54 34 46 fs p b 61 45 56 00 m d a 52 00 00 54 m d λ b 50 45 57 50 54 m d λ a 52 00 67 42 67 m d λ λ b 54 67 66 67 00 70 67 w svm 00 61 61 81 w 68 00 50 67 00 ranker a b selecting selecting methods the features optimal are tested number selecting of features the optimal number and features as cardinality a microarray binary dataset with features and 62 samples very similar to the sd family of datasets introduced in sect which consists of detecting if a patient has colon cancer or not the second dataset called optical recognition of handwritten digits has 64 features and 620 samples it consists of identifying digits from to so it is a multiclass dataset as well as led dataset results obtained over these datasets can be seen in tables and when it comes to real datasets the only way to evaluate the feature selection performance is to compute the classification accuracy therefore the same four classifiers as above were employed it also became necessary to compare the results after applying feature selection with the result when no feature reduction was performed showed in the first row and labeled as original it must be clarified that the high dimensionality of these datasets either in number of samples or in number of features prevents the use of svm rfe with a nonlinear kernel therefore it does not appear in these real experiments the first thing one can note when studying table is that feature selection improves classification accuracy in some cases remarkably see naive bayes original and with any of the feature selection methods this is due to the fact that most genes measured in a dna microarray experiment are not relevant for an accurate distinction among different classes of the problem and therefore feature selection plays a crucial role in dna microarray analysis it is also noticeable that svm rfe leads to the highest classification accuracies and it also happened with sd datasets this fact is not surprising since this method was introducing by the authors in the context of gene selection for cancer classification 62 optdigits dataset is available on http archive ics uci edu ml v bolón canedo et al table results for colon cancer method no features accuracy nb svm original 82 53 77 42 cfs 83 consistency 85 48 85 48 88 71 82 interact 79 infogain 85 48 82 65 relieff 82 85 48 77 42 mrmr 67 74 50 00 69 64 m d λ 65 65 m d λ 65 65 svm rfe 88 71 00 fs p 60 83 87 87 82 82 wrapper svm 79 87 83 87 94 wrapper 95 74 85 48 65 table results for optical digits method no features nb accuracy svm original 64 71 61 cfs 38 53 68 consistency 81 87 69 84 77 86 41 interact 59 83 infogain 87 80 94 76 relieff 81 28 45 mrmr 41 82 m d λ 57 73 95 m d λ 73 95 90 svm rfe 90 62 90 76 78 fs p 90 62 89 52 85 96 wrapper svm 90 50 91 69 54 98 wrapper 91 55 90 93 98 08 96 89 with regard to table feature selection maintains or improves the classification perfor mance reducing the number of features needed in this case the number of features is not so high and we do not know a priori if there are some irrelevant features therefore a drastic improvement like the one over colon cancer dataset was not expected even then for three classifiers the best result was obtained after applying feature selection and for svm although the best result was achieved with no feature selection after using its own wrapper the result is very similar but reducing drastically the number of features obtained the highest accuracy after performing feature selection and it has to be reminded that this classifier is the one with the best relation performance index of success see table as it happened with led dataset with no noise see tables and cfs leads to good results the highest accuracy obtained a review of feature selection methods table summary method correlation and non noise noise no feat redundancy linearity inputs target no samples cfs consistency interact infogain relieff mrmr m d λ m d λ svm rfe svm rfenl fs p wrapper svm wrapper conclusions feature selection has been an active and fruitful field of research in machine learning the importance of it is beyond doubt and it has proven effective in increasing predictive accuracy and reducing complexity of machine learning models however choosing the appropriate feature selection method for a given scenario is not an easy to solve question in this paper a review of feature selection methods applied over synthetic datasets and real datasets was presented aimed at studying their performance with respect to several situations that can hinder the process of feature selection the suite of synthetic datasets chosen covers phe nomena such as presence of irrelevant and redundant features noise in the data or interaction between attributes a scenario with a small ratio between number of samples and features where most of the features are irrelevant was also tested it reflects the problematic of data sets such as microarray data a well known and hard challenge in the machine learning field where feature selection becomes indispensable within the feature selection field three major approaches were evaluated filters wrap pers and embedded methods to test the effectiveness of the studied methods an evaluation measure was introduced trying to reward the selection of the relevant features and to penalize the inclusion of the irrelevant ones besides four classifiers were selected to measure the effectiveness of the selected features and to check if the true model was also unique table shows the behavior of the different feature selection methods over the different problems studied where the larger the number of dots the better the behavior to decide which methods were the most suitable under a given situation it was computed a trade off between index of success and classification accuracy in light of these results relieff turned out to be the best option independently of the particulars of the data with the added benefit that it is a filter which is the model with the lowest computational cost however svm rfe with a nonlinear kernel showed outstanding results although its computational time is in some cases prohibitive in fact it could not be applied over some datasets wrap pers have proven to be an interesting choice in some domains nevertheless they must be applied together with their own classifiers and it has to be reminded that this is the model v bolón canedo et al with the highest computational cost in addition to this table provides some guidelines for specific problems besides an detailed analysis of the findings of this work some cases of study were also presented in order to decide among methods that showed similar behaviors and helping to find the adequacy of them on different situations finally the feature selection methods were also tested over two real datasets demonstrating the conclusions extracted from this theoretical study over real scenarios and proving the effectiveness of feature selection in light of the results presented in this work the authors suggest the use of filters par ticularly relieff since they are independent of the induction algorithm and are faster than embedded and wrapper methods as well as having a good generalization ability as future work we plan to extend this study to other scenarios such as regression problems or appli cation to image analysis in recent years twitter has become one of the largest online mi croblogging platforms with over unique visitors and around tweets per day microblogging streams have become in blog twitter com million tweets per day html permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee wsdm february seattle washington usa copyright acm 00 valuable sources for many kinds of analyses including online rep utation management news and trend detection and targeted mar keting and customer services searching and mining microblog streams offers interesting technical challenges because of the sheer volume of the data its dynamic nature the creative language usage and the length of individual posts in many microblog search scenarios the goal is to find out what people are saying about concepts such as products brands persons et cetera here it is important to be able to accurately retrieve tweets that are on topic including all possible naming and other lexical variants so it is common to manually construct lengthy keyword queries that hopefully capture all possible variants we propose an alternative approach namely to determine what a microblog post is about by automatically identifying concepts in them we take a concept to be any item that has a unique and unambiguous entry in a well known large scale knowledge source wikipedia little research exists on understanding and modeling the seman tics of individual microblog posts linking free text to knowledge resources on the other hand has received an increasing amount of attention in recent years starting from the domain of named entity recognition ner current approaches establish links not just to entity types but to the actual entities themselves in stead of merely identifying types we also aim to disambiguate the found concepts and link them to wikipedia articles with over million articles wikipedia has become a rich source of knowledge and a common target for linking automatic linking approaches us ing wikipedia have met with considerable success 25 28 most if not all of the linking methods assume that the input text is relatively clean and grammatically correct and that it pro vides sufficient context for the purposes of identifying concepts microblog posts are short noisy and full of shorthand and other ungrammatical text and provide very limited context for the words they contain hence it is not obvious that automatic con cept detection methods that have been shown to work well on news articles or web pages perform equally well on microblog posts we present a robust method for automatically mapping tweets to wikipedia articles to facilitate social media mining on a semantic level the first research question we address is what is the perfor mance of state of the art approaches for linking text to wikipedia in the context of microblog posts our proposed approach involves a two step method for semantic linking the first step is recall oriented where the aim is to obtain a ranked list of candidate con cepts in the next step we enhance precision and determine which of the candidate concepts to keep our second research question concerns a comparison of methods for the initial concept ranking step we consider lexical matching language modeling and other state of the art baselines and compare their effectiveness our third noisy tweet concepts concepts e g by applying some kind of filtering or weigh is it me or does google instant encourage you to pay more at tention to their ads and shop ping links ing or we can clean up the text e g by identifying segments key phrases or by weighing terms based on various heuristics includ ing stopwords document frequency et cetera our approach incor porates both it first determines a high recall ranking of concepts for each part of the tweet and next applies machine learning on the concepts to improve precision since we adopt a machine learn ing framework we can also include twitter specific features the method proposed by milne and witten 28 yields accurate results on relatively clean input texts such as those from wikipedia en tries and news articles for comparative purposes we include their method as one of the baselines and show that it does not perform well on tweets ferragina and scaiella propose an approach similar to 28 but with an explicit focus on short texts they in corporate a voting scheme as well as pruning of n grams unrelated to the input text since this method is geared towards short texts we include it as a baseline below twitter twitter provides its users facilities to share short text messages comprising a maximum of characters tweets are published publicly by default making twitter an enormous resource of casual information exchanges as twitter has grown novel language use and standards such as mentions to reference another user hash tags to refer to a topic and retweets similar to an e mail forward have emerged various authors have attempted to give meaning to text in gen eral or to text contained in tweets liu et al focus on ner on tweets and use a semi supervised learning framework to identify four types of entities benson et al try to match tweets to records these records are for example artist venue pairs and can be obtained from sources like music guides they train a model that extracts artists and venues from tweets and automat ically match these to the extracted records our approach is more general and aims at enriching tweets rather than extracting infor mation from them our approach could therefore simplify the task proposed in abel et al aim at contextualizing tweets a task very similar to ours after adding context the authors use the tweets to profile twitter users their approach depends on matching tweets to news articles followed by semantic enrichment based on the news arti cle content finally the semantically enriched tweets are used for user modeling for the second step semantic enrichment the au thors use opencalais our approach differs from their approach in that we do not assume tweets to be related to news articles making our approach more general mendes et al propose linked open social signals a frame work that includes annotating tweets with information from linked data their approach is rather straightforward and involves either looking up hashtag definitions or lexically matching strings to rec ognize dbpedia entities in tweets we include the former as a feature in our framework and evaluate the latter as a baseline kwak et al show that hashtags are good indicators to detect events and trending topics and laniado and mika explore the use of hashtags in twitter and the relation to freebase concepts using manual annotations they find that about half of the hashtags can be mapped to freebase concepts most of them being named entities in a few cases more general hashtags are mapped to con cepts assessors showed high agreement on the task of mapping tags to concepts the authors make the assumption that hashtags are mainly used to ground tweets an assumption we lift in our work enabling us to add semantics to tweets without hashtags pre vious work has shown that hashtag usage is quite low and differs a ds a nd a ttention does google google instant i s i t l inks m e more etc keep your eyes out for an ac tress called judi dench she a promising talent and i pre dict we ll be hearing more about her a a bout a ctress a n and and i be called dench for hearing h er i i predict j udi dench etc table example tweets with concepts recognized using lexi cal matching on wikipedia article titles research question concerns the second precision enhancing step we approach this as a machine learning problem and consider a broad set of features some of which have been proposed previ ously in the literature on semantic linking some newly introduced in addition to multiple features we also consider multiple machine learning algorithms and examine which of these are most effec tive for our problem finally we examine the relative effectiveness of the precision enhancing step on top of different initial concept ranking methods the paper focuses on the effectiveness of con cept detection methods in the setting of microblog posts in the conclusion to the paper we also discuss efficiency considerations our main contributions are i a robust successful method for linking tweets to wikipedia articles based on a combination of high recall concept ranking and high precision machine learning including state of the art machine learning algorithms ii insights into the influence of various features and machine learning algo rithms on the task and iii a reusable dataset with which we aim to facilitate follow up research the remainder of this paper is or ganized as follows in section we discuss related work followed by a description of our method in section we discuss the ex perimental setup and in section the experiments with which we answer our research questions we end with a concluding section related work in this section we review related work pertaining to semantic linking and to twitter in particular linking text links to a knowledge structure are often seen as a way of pro viding semantics to digital items the idea has been used for dif ferent media types such as text 28 and multimedia 34 and for different text genres such as news pages queries archives and radiology reports a simple and frequently taken approach for linking text to concepts is to perform lexical matching between parts of the text and the concept titles an approach related to keyword based interfaces to databases 38 however merely matching an input text with concept titles suffers from many drawbacks including ambiguity where different con cepts with the same label can be confused and a possible lack of specificity in which case less meaningful concepts are identi fied table shows two example tweets with concepts identified using lexical matching between word n grams in the tweets and wikipedia titles from these examples it is obvious that while rel evant concepts such as google instant and judi dench are identified this approach also retrieves many false positives in general such issues can be addressed on either the wiki pedia or on the textual side that is we can reduce the amount of combination of n gram and concept an instance represented by a high dimensional feature vector its label indicates the concept binary relevance to the tweet and hence the n gram the goal of the machine learning algorithm is to learn a function that outputs a relevance status for any new n gram and concept pair given a fea ture vector of this new instance in the remainder of this section we detail the used features in section we introduce the machine learning algorithms since we consider each wikipedia article to be a concept we have an obvious textual representation to use for extracting our tex tual features below we discern different parts of this represen tation which we call fields for feature extraction and concept ranking the fields we include are title the title of the article associated with concept c sentence the first sentence of the ar ticle paragraph the first paragraph of the article content the full contents of the article and anchor the aggregated an chor texts of all incoming links in wikipedia we employ several types of features each associated with either an n gram concept or their combination we also include a separate set of twitter specific features n gram features this set of features is solely based on information from an n gram and are listed in table first group here idf q indi cates the relative number of concepts in which q occurs which is defined as idf q log c df q where c indicates the to tal number of concepts and df q the number of concepts in which q occurs the subscript f denotes the field of the wikipedia articles used see above wig q indicates the weighted information gain which can be considered a predictor of the retrieval performance of a query it uses the set of all candidate concepts retrieved for this n gram c q and determines the relative probability of q occurring in these documents as compared to the collection formally wig q c q c c q log p q c log p q log p q snil q and sncl q are an indicator of the polysemy of an n gram and determine the number of wikipedia articles whose title matches part of q similarly we leverage the wikipedia anchors to determine the probability that q is used as an anchor in wiki pedia let c a q be the set of wikipedia articles that contain the n gram q as an anchor and df q the total number of wikipedia articles in which q appears keyphraseness q then deter mines the probability c a q df q similarly we determine this probability based on all occurrences also including multiple occurrences in an article linkprob q a lot per country and language huang et al analyze the semantics of hashtags in more detail and reveal that hashtagging in twitter is more commonly used to join public discussions than to organize content for future retrieval in order to verify the rel ative contribution of hashtags we include a feature that leverages the highest voted hashtag definition linking posts to concepts as we have seen in the previous section linking text to concepts is most commonly approached by lexically matching the input text with the concept labels i e the titles of the wikipedia articles due to the noisy nature of microblog posts this approach does not work well on our task as we will see below furthermore a large number of concept titles will match with any part of the tweet as illustrated by the example in table we therefore need a mechanism to improve precision not only by removing spurious target concepts but also by limiting the num ber of n grams used to generate a mapping we approach this in two steps the goal of the first step is obtain high recall so we gen erate a ranked list of candidate concepts for each n gram in a tweet moreover using n grams in this step also means we can keep track of which n gram links to which concept in the second step we aim to improve precision by applying supervised machine learn ing here each candidate concept c is classified as being relevant or not in the context of the tweet and the user in a way this setup is a form of learning to rerank alternatively we could forego the first step in which we obtain a set of candidate concepts and apply a form of learning to rank on all concepts for each n gram how ever since semantic linking is akin to known item finding and thus geared towards high precision with only a relatively small num ber of relevant concepts per tweet we reduce the set of candidate concepts prior to applying machine learning moreover limiting the set of concepts to be used as input for the machine learning al gorithm also reduces the number of feature vectors that need to be created decreasing the runtime concept ranking in order to be able to identify the part of the tweet that is the source for a semantic link we first identify word n grams in the tweet that is we extract all possible n grams from a tweet q where n q for each n gram q in this set of n grams q methods i q i exist we generate for c c a q n q c c c a ranked list of candidate concepts various creating a ranked list of concepts for an n gram and in our experiments below we compare three families of ap proaches that are further detailed in section they include lexical matching language modeling and other state of the art methods machine learning once we have obtained a ranked list of candidate concepts for each n gram we turn to concept selection in this stage we need to decide which of the concepts are most viable we use super vised machine learning that takes as input a set of labeled examples tweet to concept mappings and several features of these examples detailed below for training each n gram q in each tweet q is associated with a set of concepts c q where n q c is the count of q in the content of the textual repre sentation of c concept features table second group lists the concept related features this set relates to the knowledge we have of each candidate concept such as the number of other wikipedia articles linking to or from it the number of associated categories the number of redirect pages pointing to it and the number of terms or characters in its title the last two features in this set capture the relative popularity of an wikipedia article and are based on wikipedia access logs that is http dammit lt wikistats n q c and a set of associated relevance assessments for the concepts the latter is created by considering all concepts that an annotator identified for the tweet if a concept was not se lected by any annotator we consider it to be non relevant for q section further details the specifics of the manual annotations as we are performing semantic linking at the tweet level instead of doing named entity resolution it is sufficient to have a map ping from tweets to concepts for our task so we consider each n gram features len q q number of terms in the n gram q idf f q inverse document frequency of q in representation f where f title anchor content wig q weighted information gain using top retrieved concepts keyphraseness q probability that q is used as an anchor text in wikipedia linkprob q probability that q is used as an anchor text in wikipedia all occurrences snil q number of articles whose title equals a sub n gram of q sncl q number of articles whose title match a sub n gram of q concept features inlinks c number of wikipedia articles linking to c outlinks c number of wikipedia articles linking from c gen c function of depth of c in the wikipedia category hierarchy cat c number of categories associated with c redirect c number of redirect pages linking to c wlen c number of terms in the title of c clen c number of characters in the title of c wikstats c number of times c was visited in wikistatswk c number of times c was visited in the seven days before the tweet was posted n gram concept features tf f c q n f q c f relative phrase frequency of q in representation f of c normalized by length of f where f title anchor first sentence first paragraph content pos n c q pos n q c position of nth occurrence of q in c normalized by length of c spr c q distance between the first and last occurrence of q in c tf idf c q importance of q for c ridf c q residual idf difference between expected and observed idf χ c q test of independence between q in c and all concepts nct c q does q contain the title of c tcn c q does the title of c contain q ten c q does the title of c equal q score c q language modeling score of c with respect to q rank c q retrieval rank of c with respect to q commonness c q probability of c being the target of a link with anchor text q tweet features twct c q does q contain the title of c tctw c q does the title of c contain q tetw c q does the title of c equal q tagdef q q number of times q appears in the hashtag definition of any hashtag in tweet q url q q number of times q appears in a webpage linked to by q table features used grouped by type more detailed descriptions in section we determine for each wikipedia article the normalized frequency with which it was visited in wikstats c and the fre quency with which it was visited in the seven days before the tweet was posted wikstatswk c n gram concept features this set of features considers the combination of an n gram and a concept table third group here we first consider the relative frequency of occurrence of q in separate concept fields the position of the first occurrence of the n gram the distance between the first and last occurrence and various ir based measures of these ridf is the difference between expected and observed idf for a concept which is defined as ridf c q log for the features score c q and rank c q we adopt a language modeling framework in which a query is viewed as hav ing been generated from a multinomial language model and each document is scored according to the likelihood that the words in the query were generated by randomly sampling the document lan guage model the word probabilities are estimated from the document itself using maximum likelihood estimation and com bined with background collection statistics to overcome zero prob ability and data sparsity issues a process known as smoothing we calculate the score for a concept c c according to the probability that it was generated by the n gram p c q which can be rewritten using bayes rule as p c q p q c p c p q here for a fixed n gram q the term p q is the same for all concepts and can c df q log exp n q c c be ignored for ranking purposes the term p c indicates the prior probability of selecting a concept which we assume to be uniform we consider each n gram to be a phrase and determine the proba we also consider whether the title of the wikipedia article matches q in any way bility p q c using the following estimate manual annotations p q c n q μ c c μp q in order to obtain manual annotations both for training and eval uation we have asked two volunteers to manually annotate tweets each containing terms on average they were pre this is an estimate using bayes smoothing with a dirichlet prior where p q indicates the probability of observing q in a large back ground collection here μ is a hyperparameter that controls the in fluence of the background corpus and c is the length of the textual representation of the concept finally we consider the prior probability that c is the target of a link with anchor text q in wikipedia sented with an annotation interface with which they could search through wikipedia articles using any of the fields defined above the annotation guidelines specified that the annotator should iden tify concepts contained in meant by or relevant to the tweet they could also indicate that an entire tweet was either ambiguous where multiple target concepts exist or erroneous when no relevant con cept could be assigned out of the tweets were labeled as not being in either of these two categories and kept for further anal commonness c q l q c c where l q c l q c denotes the set of all links with anchor text q and target ysis for these the annotators identified concepts per tweet on average in order to facilitate follow up research we make all annotations and derived data available c evaluation tweet features finally we consider features related to the entire tweet table we approach the task of linking tweets to concepts as a ranking problem given a tweet the goal of a system implementing a solu tion to this problem is to return a ranked list of concepts meant by last group including three kinds of lexical matches we also fetch the webpage linked to by the tweet if any and determine the or contained in it where a higher rank indicates a higher degree of relevance of the concept to the tweet the best performing method frequency of occurrence of q in them url q q finally we use the tagdef to lookup the highest voted hashtag definition of all hashtags in the tweet if any and determine the frequency of puts the most relevant concepts towards the top of the ranking our method identifies relevant concepts for each tweet constituent n grams and since the semantic linking task is defined at the tweet in occurrence of q in them tagdef q q stead of the n gram level we need a way to aggregate potential du plicate concepts in our experiments we aggregate each duplicate experimental setup in order to answer the research questions introduced in section we have designed several experiments in this section we detail our experimental setup including how we sample tweets the manual annotations the wikipedia version and how we evaluate linking tweets to concepts concept if any by naively summing the confidence scores for each source n gram future work should indicate whether more elabo rate methods perform better the manual annotations described above are then used to determine the relevance status of each of the concepts with respect to a tweet a concept is considered rele vant if it was linked by an annotator if it was not selected by any annotator we consider it to be non relevant tweets the evaluation measures we employ include precision at rank r precision r prec recall mean reciprocal rank mrr out of a sample of tweets pear analytics classified and mean average precision map together these measures 40 as containing pointless babble with another 55 as merely provide a succinct summary of the quality of the retrieved concepts conversational so a significant portion of all tweets are non we use the top 50 returned concepts for evaluation to test for sta informative and only a small fraction contains topics of general tistical significance we use a paired two sided t test with pairing interest in order to account for this possible bias we randomly at the run level we indicate the best performing run using bold sample users from the verified accounts twitter list and for each face and a significant improvement with and for p of these users we retrieve the last tweets the twitter users and p respectively and similarly a significant decrease in this list can be considered influential and their tweets are more in performance with and again for p and p likely to be picked up by other twitter users than from a random respectively unless indicated otherwise we test for significance sample of twitter users as to preprocessing we record all urls against the top most row in a table mentions and hashtags in each tweet urls are removed from each tweet whereas mentions and hashtags are kept without the baselines leading and respectively we employ three sets of baselines to which we compare our ap wikipedia proach and to which we apply supervised machine learning they include i lexical matching of the n grams with the concepts ii as our target for linking tweets we use a dump of wikipedia a language modeling baseline and iii a set of other methods aug that is dated oct in this particular snapshot we have mented with using solely the commonness c q feature articles proper redirects disambigua tion pages and 71 hyperlinks between articles for the lexical match anchor field we include not only the anchor texts found in intra wikipedia hyperlinks but also the titles of any redirect pages point ing to an article as to retrieval we use the entire wikipedia doc ument collection as background corpus and set μ cf eq to the average length of a wikipedia article as our first baseline we consider a simple heuristic which is commonly used and select concepts whose title lexically matches any n gram in the tweet similar to the nct c q and ten c q features described in section we subsequently rank the concepts based on the language modeling score of their http tagdef com http twitter com help verified http ilps science uva nl resources adding semantics to microblog posts similar to returning wikipedia disambiguation pages moreover the precise algorithmic details are not made public machine learning methods for all machine learning algorithms we perform fold cross validation at the tweet level in order to reduce the possibility of errors being caused by artifacts in the data and to verify the gener alizability to unseen data the reported scores are averaged over all testing folds we experiment with multiple machine learning algo rithms in order to confirm that our results are generally valid i e not dependent on any specific algorithm following milne and wit ten 28 we include a naive bayes nb support vector machines svm and a decision tree classifier random forests rfs are a very efficient alternative to since they are i relatively insensitive to parameter settings ii resistant to overfitting and iii easily parallelizable it is an ensemble based decision tree classifier based on bagging in which a learning algorithm is applied multiple times to a subset of the instances and the results averaged in this case for each iteration a bootstrap sample is taken and a full tree is constructed for each node of the tree m features are randomly selected to obtain the best split this process reduces overfitting by averaging classifiers that are trained on different subsets of the data with the same underlying distribution we set m to roughly of the size of the feature set i e m besides m rf has one additional parameter the number of iterations k we set k but also report on the effect of varying this setting on the linking effectiveness in recent years gradient boosted regression trees gbrts have been established as the de facto state of the art learning paradigm for web search ranking 29 it is a point wise learning to rank algorithm that predicts the relevance score of a result to a query by minimizing a loss function e g the squared loss us ing stochastic gradient descent it is similar to rf in that it is also based on tree averaging in this case however many low depth trees instead of full ones are sequentially created each with a bias towards instances that are responsible for the current regres sion error let q i be the feature vector associated with q i y i the associated label i e relevance status and t q i the current pre diction for q i then assume we have a continuous convex and differentiable loss function l t q t q n that reaches its minimum if t q i y i for all q i gbrt performs gradient de scent in the instance space where the current prediction is updated with a gradient step t q i t q i α t q l i during each iteration here α denotes the learning rate for our experiments we use the the square loss which is defined as l associated wikipedia article given the tweet this method is the most naive yet most commonly used approach in sum it returns concepts for which consecutive terms in the title of the wikipedia article are contained in the n gram an example is given in table as a variant we apply the same approach to the anchor field we also apply an heuristic to handle n grams this method starts with the largest n grams in the tweet and checks to see if it lexically matches with a concept if it does it discards any smaller constituent n grams if the n gram doesn t match it recurses with its constituent n grams retrieval this baseline builds on the approach described by eq and is similar to using a search engine and performing a search within wikipedia that is we use language modeling to determine the similarity of a tweet with the concepts in particular we explore two different tweet representations and three different concept fields resulting in six retrieval baseline runs for the tweet representation we try either the full tweet or the constituent n grams in the case of ranking concepts for the entire tweets we assume independence between the individual terms t q p c q p c t q n t q the probability p t c is again smoothed using bayes smoothing with a dirichlet prior in this case formulated as p t c p t c μ n t c t μp t as to the concept fields we use either the full wikipedia article its title or its incoming anchor texts to combine the rankings produced by each constituent n gram of a tweet we use combmnz combmnz is a result list merg ing method and a variant of combsum which sums a document scores from all lists where it was retrieved combmnz multiplies the combsum score by the number of lists that contained the par ticular document formally let q n t c q k be the term n grams in the tweet and c c k the concept rankings corresponding to each n gram then combsum c k c c k p c q k combmnz c combsum c k c c k other methods the final set of baselines that we consider comprises three estab lished methods including the one proposed by milne and witten 28 denoted m w which represents the state of the art in auto matic linking approaches we use the algorithm and best performing settings as described in 28 trained on our version of wikipedia we also include a novel service provided by dbpedia called db pedia spotlight the third baseline in this set tagme is pro vided by ferragina and scaiella these first three baselines in this set do not perform optimally when linking individual n grams therefore we use the entire cleaned tweet as input our last concept ranking method in this set cmns corresponds to the commonness c q feature detailed by eq this method scores each concept based on the relative frequency with which the n gram is used as an anchor text for that particular concept we ex clude opencalais from this set since this webservice only recog nizes entity types without performing any kind of disambiguation http dbpedia org spotlight so gbrt depends on three param eters the learning rate α the depth of the tree d and the number of iterations k we set α and k and following hastie et al we set d finally mohan et al 29 show that since rf is resistant to over fitting and also often outperforms gbrt the rf predictions can be used as starting point for gbrt by doing so gbrt starts at a point relatively close to the global minimum and is able to fur ther improve the already good predictions we also include this approach labeled igbrt results and discussion in this section we answer the research questions that we iden tified in section by presenting comparing and discussing the results of the baselines and our method n i t q i y i r prec recall mrr map r prec recall mrr map lexical match spotlight 2688 4215 anchor 0422 0796 title 0160 0338 m w 3033 4255 tagme 4643 6339 cmns 5271 7080 lexical match longest n gram anchor table results for the third set of baselines title 1335 2345 r prec recall mrr map table first set of baseline results obtained using lexical matching on titles or anchor texts first group and applying the longest n gram heuristic second group lex match 1344 2408 2440 1405 nb 2302 2021 4372 2658 2575 r prec recall mrr map svm 2337 3878 rf 4091 6078 tweet content 1173 2255 gbrt 4034 5964 igbrt 4082 6053 title 1185 2106 anchor 2147 3793 table results for the best lexical matching run with subse quent machine learning n grams content 1939 3910 title 1552 2795 anchor simply applying the commonness c q i e cmns concept ranking method however achieves the highest scores so far on all metrics this relatively simple approach is able to retrieve over table retrieval results using two tweet representations and three fields significance is tested against line first group and line second group 75 of the relevant concepts and place the first relevant concept around rank on average in conclusion the results on the three baseline sets show that the task of linking tweets to concepts can be addressed relatively successfully using a retrieval based approach outperforms an ap establishing a baseline proach based on mere lexical matching using the cmns method obtains the highest performance in terms of both precision and re since the machine learning step takes as input a ranked list of call concepts for each n gram in a tweet the ideal baseline method should have a high recall with sufficient precision table shows applying machine learning the results of using lexical matching to obtain a ranked list of con in this section we present the results of our approach i e apply cepts merely matching parts of the tweet with titles or anchors ing machine learning to further improve the results of the concept does not perform well note that this is a commonly taken ap ranking step that is we take the best performing run per base proach but these results indicate that it fails on tweets when we line set and apply the machine learning approaches introduced in apply the longest n grams heuristic we observe a notable increase section in particular we select the lexical match longest in concept ranking performance in terms of recall and precision n gram for the first set and the n grams retrieval for the second we now turn to the results for the retrieval baseline using dif set both using the anchor field finally we select the cmns run ferent fields and ways of handling the tweet as introduced earlier from the last set table first group shows the results for ranking concepts based in table we show the results when we apply machine learning on the entire tweet assuming independence between the terms in to the lexical matching baseline all machine learning algorithms the tweet this is a rather naive approach but performs remarkably are able to significantly improve precision mrr and map only well especially when we only use the anchor texts of each wiki nb and do not significantly improve in terms of recall here pedia article we obtain a value of i e the first returned rf obtains the highest scores overall although the differences with concept is relevant for almost of the tweets moreover the gbrt and igbrt are minimal and not significant average rank of of the first relevant concept lies around table table shows the results of the retrieval baseline augmented by second group shows the results when we generate a ranking for the machine learning step almost all machine learning algorithms each constituent n gram and apply combmnz to merge these we are able to improve over the retrieval baseline except for recall observe the following first recall improves significantly using n furthermore igbrt achieves the highest scores on most metrics grams and the anchor or content field second both map and closely followed by rf mrr increase significantly using the anchor field finally in table we show the results of applying machine table shows the results of the last set of baselines dbpedia learning to the cmns baseline of all the baselines cmns ob spotlight and the learning to link approach by milne and witten tains the highest recall and precision levels it does so by returning 28 achieve comparable results that slightly improve over the best relatively few concepts per n gram we note that nb and do language modeling baseline the performance results for m w not perform well in this case their performance drops significantly are much lower than the results reported in 28 which can be at for all metrics indeed recall is significantly worse for nb tributed to the different nature of tweets as compared to wikipedia and svm mainly due to the fact that a lot of concepts are iden and or news articles the tagme system designed especially for tified as non relevant by these algorithms interestingly im short texts fares much better achieving marked improvements proves significantly for svm while r prec is significantly worse r prec recall mrr map retrieval 6686 4003 2810 nb 3212 4911 c4 3822 2530 2341 svm 2419 4138 rf 4197 6195 gbrt 3978 6012 igbrt 4209 6229 table results of applying machine learning applied to the best performing retrieval baseline p1 r prec recall mrr map cmns 5271 7080 nb 4091 5853 c4 5209 3774 3528 svm 4670 6846 rf 5739 7676 gbrt 5624 7568 igbrt 5715 7644 table results of applying machine learning with as input the best performing other baseline run i e commonness cmns than cmns in many cases svm is able to push a single relevant concept to the top of the ranking but fails to do so for all relevant concepts rf gbrt and igbrt obtain significant improvements over cmns for all metrics rf outperforms all other methods in sum we find that the iterative machine learning methods ob tain the best improvements overall and that they are able to sig nificantly improve precision in all cases in the remainder of this section we perform a more detailed analysis of the obtained results we base these analyses mainly on the best performing method i e machine learning using rf on the cmns concept ranking which we denote cmns rf error analysis figure shows a per tweet plot of the difference in terms of aver age precision ap between cmns and cmns rf as this figure indicates most tweets are helped by applying machine learning except for about a fifth of them the tweets that are hurt most are mainly tweets with only a single relevant concept that is retrieved by cmns but classified as not relevant by cmns rf when we look at the errors being made in general we observe that there are three typical cases of errors first there are concepts that are clearly relevant but that were missed by the annotators in some other cases the concepts identified by the annotators do not always seem relevant in a future study we intend to investigate this further mainly by performing a post hoc relevance analysis of annotated tweets second we find that in some cases the machine learning algorithms seem to focus on a particular sense of an n gram e g in the case of st patrick most algorithms return various churches instead of the arguably more common concept saint patrick s day finally it also occurs that a non content bearing term in the tweet is being linked such as the n gram wow that gets linked to the concept world of warcraft in future work we plan on tackling these two classes of errors e g by in cluding a post ranking filter that first tries to match longer n grams before moving on to shorter ones or by including more features that cmns rf cmns gbrt cmns igbrt figure effect of varying k on map figure shows the effect of varying the number of iterations using the rf gbrt and igbrt machine learning algorithms on map both rf and igbrt start at roughly the same level since igbrt uses the rf predictions as initialization weights both algorithms also reach their optimum after approximately iterations indi cating that the iterative learning process can be stopped quite early gbrt is not able to reach the same level of performance as igbrt it takes over iterations to approach the others scores feature analysis finally we zoom in on the relative effectiveness of each of the features the top features with the highest discriminative power measured using information gain are in decreasing order 00 80 60 40 00 40 60 80 00 figure per tweet difference in ap between cmns and cmns rf where a positive value indicates an improvement over cmns are related to the entire tweet this way the remainder of the tweet might assist in disambiguating a particular n gram parameter settings in this section we consider the various parameters that are asso ciated with the machine learning algorithms for all of these we have selected the top ranked concepts returned by the concept rank ing step up to a maximum of 50 per n gram varying this number does not significantly alter the obtained results recall that rf gbrt and igbrt have several parameters asso ciated with them including the number of iterations and the learn ing rate in the case of gradient boosting there exists an inherent trade off between the learning rate and the number of iterations ideally the learning rate needs to be infinitesimally small and the number of iterations extremely large to obtain the true global min imum in our case varying the learning rate parameter α only in fluences the obtained end results very little 70 65 60 p a m 55 50 45 40 300 700 iterations ten c q twct c q redirect c linkprob q and idf anchor q interestingly this is a mix between features that are based solely on the n gram the concept and their combination the binary feature that indicates whether the tweet contains the concept title is also a strong indicator for the relevance of a concept regarding a tweet in a follow up experiment we rank all features by their informa tion gain and add each of them incrementally to the set of used fea tures starting with the features with the highest values we keep the number of features rf selects at each iteration at m and thus start with selecting the highest ranked features table shows the results cmns rf achieves map comparable to cmns on which this ranking is based after adding the feature tcn c q when cmns is subsequently added as a feature we observe an improvement of in retrieval performance after idf content q q we truncate the table map remains relatively constant around 65 after adding this feature almost all of the features shown here can table map of cmns rf after incrementally adding fea tures proportional to their information gain truncated to show only the top features be computed quite easily either from an inverted index or from a cache that contains the link structure and anchor information of wikipedia the url q q and wig q features are the most costly features in this list for the first an http connection needs to be made and the web page content fetched parsed and matched with the n gram the second is a function over the n gram and the top ranked concepts for that n gram in a situation where the running time is of importance we recommend removing these par ticular features future work includes the following first although our method is not language dependent in any way the manual annotations are indeed language specific wikipedia on the other hand already contains numerous manually curated inter language links that we could use for this purpose second we already mentioned a post hoc evaluation of our semantic linking method for future work in section we also acknowledge that our sample of tweets based conclusion and future work on authoritative users is comparatively small and might be bi ased therefore we intend to apply the best performing methods microblogging streams have become an invaluable resource for to a much larger random sample of microblog posts to see how it marketing search information dissemination and online reputa performs there further in this paper we have focused on a domain tion management searching and mining microblog streams offers independent way of obtaining high recall concept candidate rank interesting challenges and in this paper we have presented a suc ings we believe however that including additional information cessful semantic linking method for microblog posts the iden such as from ner could further improve semantic linking perfor tified concepts i e wikipedia articles can subsequently be used mance for future work we also intend to consider bootstrapping or for e g social media mining or advanced search result presenta co training in which the concepts with the highest confidence are tion our novel method uses machine learning and is based on a fed back as new training material finally we note that wikipedia high recall concept ranking and a high precision concept selection contains a few thousand links to twitter in the articles external step using a purpose built test collection we have shown that it links sections and we intend to investigate to what extent we can significantly outperforms other methods including various recently use this information for semantic linking proposed approaches moreover the concept selection step can be applied to any method that returns concepts for an input text our results show that this step in particular using random forests or gradient boosted regression trees can significantly improve a weak baseline especially in terms of precision it is even able to improve when the concept ranking performance is already strong on its own we have focused mainly on the effectiveness of semantic link ing in the setting of microblog posts as opposed to the efficiency since both best performing machine learning algorithms are eas ily parallelizable the bulk of the processing happens during feature extraction from the results obtained during feature analysis we note that not all features are equally important and that a minimal easily computable set can already obtain good performance more over our analysis has shown that only a relatively small number of iterations is needed to achieve optimal performance we finally note that in the cases where a real time analysis of a stream of mi croblog posts is required merely using the low cost cmns feature already obtains very good performance one of the main goals of computer vision is to enable computers to replicate the basic functions of human vision such as motion perception and scene understanding to achieve the goal of intelligent motion perception much effort has been spent on visual object tracking which is one of the most important and challenging research topics in computer vision essentially the core of visual object tracking is to robustly estimate the motion state i e location orientation size etc of a target object in each frame of an input image sequence in recent years a large body of research on visual object tracking has been published in the literature research interest in visual object tracking comes from the fact that it has a wide range of real world applications including visual surveillance traffic flow monitoring video compression and human computer interaction for example visual object tracking is successfully applied to monitor human activities in residential areas parking lots and banks e g w system haritaoglu et al and vsam project collins et al in the field of traffic transportation visual object tracking is also widely used to cope with traffic flow monitoring coifman et al traffic accident detection tai et al pedestrian counting masoud and papanikolopoulos and so on moreover visual object tracking is utilized by the mpeg video compression standard sikora to automatically detect and track moving objects in videos as a result more encoding bytes are assigned to moving objects while fewer encoding bytes are for redundant backgrounds visual object tracking also has several human computer interaction applications such as hand gesture recognition pavlovie et al and mobile video conferencing paschalakis and bober note that all these applications heavily rely on the information provided by a robust visual object tracking method if such information is not available these applications would be no longer valid therefore robust visual object tracking is a key issue to making these applications viable overview of visual object tracking in general a typical visual object tracking system is composed of four modules object initialization appearance modeling motion estimation and object localization i object initialization this may be manual or automatic manual initialization is performed by users to annotate object locations with bounding boxes or ellipses in contrast automatic initialization is usually achieved by object detectors e g face or human detectors ii appearance modeling this generally consists of two components visual repre sentation and statistical modeling visual representation focuses on how to con struct robust object descriptors using different types of visual features statistical modeling concentrates on how to build effective mathematical models for object identification using statistical learning techniques iii motion estimation this is formulated as a dynamic state estimation problem evolution x t h denotes f x t function v t the measurement and v t z t is the h x evolution t w t where process x t is the current state f is the state noise z t is the current observation of motion estimation is function and usually completed w by t is the measurement noise utilizing predictors such the task as linear regression techniques ellis et al kalman filters kalman or particle filters isard and blake arulampalam et al iv object localization this is performed by a greedy search or maximum a posterior estimation based on motion estimation acm transactions on intelligent systems and technology vol no article publication date september a survey of appearance models in visual object tracking fig illustration of complicated appearance changes in visual object tracking challenges in developing robust appearance models many issues have made robust visual object tracking very challenging including i low quality camera sensors e g low frame rate low resolution low bit depth and color distortion ii challenging factors e g nonrigid object tracking small size ob ject tracking tracking a varying number of objects and complicated pose estimation iii real time processing requirements iv object tracking across cameras with non overlapping views javed et al and v object appearance variations as shown in figure caused by several complicated factors e g environmental illumination changes rapid camera motions full occlusion noise disturbance nonrigid shape defor mation out of plane object rotation and pose variation these challenges may cause tracking degradations and even failures in order to deal with these challenges researchers have proposed a wide range of appearance models using different visual representations and or statistical modeling techniques these appearance models usually focus on different problems in visual object tracking and thus have different properties and characteristics typically they attempt to answer the following questions what characterstic and or properties should be tracked e g bounding box ellipse contour articulation block interest point and silhouette as shown in figure what visual representations are appropriate and robust for visual object tracking what are the advantages or disadvantages of different visual representations for different tracking tasks which types of statistical learning schemes are suitable for visual object tracking what are the properties or characteristics of these statistical learning schemes during visual object tracking how should the camera object motion be modeled in the tracking process the answers to these questions rely heavily on the specific context environment of the tracking task and the tracking information available to users consequently it is necessary to categorize these appearance models into several task specific categories and discuss in detail the representative appearance models of each category motivated by this consideration we provide a survey to help readers acquire valuable tracking knowledge and choose the most suitable appearance model for their particular tracking acm transactions on intelligent systems and technology vol no article publication date september x li et al fig illustration of object tracking forms a bounding box b ellipse c contour d articulation block e interest point f silhouette fig the organization of this survey tasks furthermore we examine several interesting issues for developing new appear ance models organization of this survey figure shows the organization of this survey which is composed of two modules visual representation and statistical modeling the visual represen tation module concentrates on how to robustly describe the spatiotemporal characteristics of object appearance in this module a variety of visual representa tions are discussed as illustrated by the tree structured taxonomy in the left part of figure these visual representations can capture various visual information at differ ent levels i e local and global typically the local visual representations encode the local statistical information e g interest point of an image region while the global vi sual representations reflect the global statistical characteristics e g color histogram of an image region for a clear illustration of this module a detailed literature review of visual representations is given in section acm transactions on intelligent systems and technology vol no article publication date september a survey of appearance models in visual object tracking table i summary of related literature surveys authors topic journal conference title ger onimo et al pedestrian detection ieee trans pami candamo et al human behavior recognition ieee trans intell transport syst cannons visual tracking tech rep zhan et al crowd analysis machine vision app kang and deng intelligent visual surveillance ieee acis int conf comput inf sci yilmaz et al visual object tracking acm comput sur forsyth et al human motion analysis found trends comput graph vis sun et al vehicle detection ieee trans pami hu et al object motion and behaviors ieee trans syst man cybern c appl rev arulampalam et al bayesian tracking ieee trans signal process as shown in the right part of figure the statistical modeling module is inspired by the tracking by detection idea and thus focuses on using different types of statisti cal learning schemes to learn a robust statistical model for object detection including generative discriminative and hybrid generative discriminative ones in this module various tracking by detection methods based on different statistical modeling tech niques are designed to facilitate different statistical properties of the object non object class for a clear illustration of this module a detailed literature review of statistical modeling schemes for tracking by detection is given in section moreover a number of source codes and video datasets for visual object tracking are examined to make them easier for readers to conduct tracking experiments in section finally the survey is concluded in section in particular we additionally address several interesting issues for the future research in section main differences from other related surveys in the recent literature several related surveys e g ger onimo et al candamo et al cannons zhan et al kang and deng yilmaz et al forsyth et al sun et al hu et al arulampalam et al of visual object tracking have been made to investigate the state of the art tracking algorithms and their potential applications as listed in table i among these surveys the topics of the surveys cannons yilmaz et al are closely related to this article specifically both of the surveys cannons yilmaz et al focus on low level tracking techniques using different visual features or statistical learning techniques and thereby give very comprehensive and specific technical contributions the main differences between these two surveys cannons yilmaz et al and this survey are summarized as follows first this survey focuses on the appear ance modeling for visual object tracking in comparison the surveys of cannons and yilmaz et al concern all the modules shown in figure hence this survey is more intensive while theirs are more comprehensive second this survey provides a more detailed analysis of various appearance models third the survey of yilmaz et al splits visual object tracking into three categories point tracking kernel track ing and silhouette tracking see figure yilmaz et al for details the survey of cannons gives a very detailed and comprehensive review of each tracking issue in visual object tracking in contrast to these two surveys this survey is formulated as a general module based architecture shown in figure that enables readers to easily grasp the key points of visual object tracking fourth this survey investigates a large number of state of the art appearance models which make use of novel visual features and statistical learning techniques in comparison the other surveys cannons acm transactions on intelligent systems and technology vol no article publication date september x li et al yilmaz et al pay more attention to classic and fundamental appearance models used for visual object tracking contributions of this survey the contributions of this survey are as follows first we review the literature of vi sual representations from a feature construction viewpoint specifically we hierar chically categorize visual representations into local and global features second we take a tracking by detection criterion for reviewing the existing statistical modeling schemes according to the model construction mechanisms these statistical model ing schemes are roughly classified into three categories generative discriminative and hybrid generative discriminative for each category different types of statistical learning techniques for object detection are reviewed and discussed third we provide a detailed discussion on each type of visual representations or statistical learning tech niques with their properties finally we examine the existing benchmark resources for visual object tracking including source codes and databases visual representation global visual representation a global visual representation reflects the global statistical characteristics of object appearance typically it can be investigated in the following main aspects i raw pixel representation ii optical flow representation iii histogram representation iv covariance representation v wavelet filtering based representation and vi ac tive contour representation table ii lists several representative tracking methods using global visual representations i e rows raw pixel representation as the most fundamental features in computer vision raw pixel values are widely used in visual object tracking because of their simplicity and efficiency raw pixel representation directly utilizes the raw color or intensity values of the image pixels to represent the object regions such a representation is simple and efficient for fast object tracking in the literature raw pixel representations are usually constructed in the following two forms vector based silveira and malis ho et al li et al ross et al and matrix based li et al wen et al hu et al wang et al li et al the vector based representation directly flattens an image region into a high dimensional vector and often suffers from a small sample size problem motivated by attempting to alleviate the small sample size problem the matrix based representation directly utilizes matrices or higher order tensors as the basic data units for object description due to its relatively low dimensional property however raw pixel information alone is not enough for robust visual object track ing researchers attempt to embed other visual cues e g shape or texture into the raw pixel representation typically the color features are enriched by fusing other visual information such as edge wang et al and texture allili and ziou optical flow representation in principle optical flow represents a dense field of displacement vectors of all the pixels inside an image region and is commonly used to capture the spatiotemporal motion information of an object typically optical flow has two branches constant brightness constraint cbc optical flow lucas and kanade horn and schunck werlberger et al sethi and jain salari and sethi santner et al and non brightness constraint nbc optical flow black and anandan sawhney and ayer hager and belhumeur bergen et al irani wu and fan the cbc optical flow has a constraint on brightness constancy while the nbc optical flow deals with the situations with varying lighting conditions acm transactions on intelligent systems and technology vol no article publication date september a survey of appearance models in visual object tracking table ii summary of representative visual representations item no references global local visual representations ho et al li et al ross et al global vector based raw pixel representation li et al global matrix based raw pixel representation wang et al global multi cue raw pixel representation i e color position edge werlberger et al santner et al global optical flow representation constant brightness constraint black and anandan wu and fan global optical flow representation non brightness constraint bradski comaniciu et al zhao et al global color histogram representation georgescu and meer global multi cue spatial color histogram representation i e joint histogram in x y r g b adam et al global multi cue spatial color histogram representation i e patch division histogram haralick et al gelzinis et al global multi cue spatial texture histogram representation i e gray level co occurrence matrix haritaoglu and flickner ning et al multi cue shape texture histogram global representation i e color gradient texture porikli et al wu et al global affine invariant covariance representation li et al hong et al wu et al hu et al global log euclidean covariance representation he et al li et al global wavelet filtering based representation paragios and deriche cremers allili and ziou sun et al global active contour representation lin et al local local feature based represnetation local templates tang and tao zhou et al local local feature based represnetation sift features donoser and bischof tran and davis local local feature based represnetation mser features he et al local local feature based represnetation surf features grabner et al kim local local feature based represnetation corner features local feature based represnetation feature pools of harr hog lbp etc collins et al grabner and bischof yu et al local toyama and hager mahadevan and vasconcelos yang et al fan et al local local feature based representations saliency detection based features 22 ren and malik wang et al local local feature based represnetation segmentation based features acm transactions on intelligent systems and technology vol no article publication date september x li et al histogram representation histogram representations are popular in visual object tracking because of their effectiveness and efficiency in capturing the distribution characteristics of visual features inside the object regions in general they have two branches single cue and multi cue i a single cue histogram representation often constructs a histogram to capture the distribution information inside an object region for example bradski uses a color histogram in the hue saturation value hsv color space for object representation and then embeds the color histogram into a continuously adaptive mean shift camshift framework for object tracking however the direct use of color histogram may result in the loss of spatial information following the work in bradski comaniciu et al utilize a spatially weighted color histogram in the rgb color space for visual representation and subsequently embed the spatially weighted color histogram into a mean shift based tracking framework for object state inference zhao et al convert the problem of object tracking into that of matching the rgb color distributions across frames as a result the task of object localization is taken by using a fast differential emd earth mover distance to compute the similarity between the color distribution of the learned target and the color distribution of a candidate region ii a multi cue histogram representation aims to encode more information to en hance the robustness of visual representation typically it contains three main components a spatial color b spatial texture c shape texture a spatial color two strategies are adopted including joint spatial color mod eling and patch division the goal of joint spatial color modeling is to describe the distribution properties of object appearance in a joint spatial color space e g x y r g b yang et al georgescu and meer birchfield and rangarajan the patch division strategy is to encode the spa tial information into the appearance models by splitting the tracking region into a set of patches adam et al nejhum et al by consider ing the geometric relationship between patches it is capable of capturing the spatial layout information for example adam et al construct a patch division visual representation with a histogram based feature de scription for object tracking as shown in figure the final tracking posi tion is determined by combining the vote maps of all patches represented by grayscale histograms the combination mechanism can eliminate the in fluence of the outlier vote maps caused by occlusion for the computational efficiency porikli introduces a novel concept of an integral histogram to compute the histograms of all possible target regions in a cartesian data space this greatly accelerates the speed of histogram matching in the pro cess of mean shift tracking b spatial texture an estimate of the joint spatial texture probability is made to capture the distribution information on object appearance for exam ple haralick et al propose a spatial texture histogram represen tation called gray level co occurrence matrix glcm which encodes the co occurrence information on pairwise intensities in a specified direction and distance note that the glcm in haralick et al needs to tune differ ent distance parameter values before selecting the best distance parameter value by experimental evaluations following haralick et al gelzinis et al propose a glcm based histogram representation that does not need to carefully select an appropriate distance parameter value the pro posed histogram representation gathers the information on the co occurrence matrices computed for several distance parameter values acm transactions on intelligent systems and technology vol no article publication date september a survey of appearance models in visual object tracking fig illustration of patch division visual representation adam et al ieee the left part shows the previous and current frames and the right part displays the patchwise histogram matching process between two image regions c shape texture the shape or texture information on object appearance is in corporated into the histogram representation for robust visual object track ing for instance haritaoglu and flickner incorporate the gradient or edge information into the color histogram based visual representation similar to haritaoglu and flickner wang and yagi construct a visual representation using color and shape cues the color cues are com posed of color histograms in three different color spaces rgb hsv and nor malized rg the shape cue is described by gradient orientation histograms to exploit the textural information of the object ning et al propose a joint color texture histogram for visual representation the local binary pattern lbp technique is employed to identify the key points in the object regions using the identified key points they build a confidence mask for joint color texture feature selection covariance representation in order to capture the correlation information of object appearance covariance matrix representations are proposed for visual representa tion porikli et al tuzel et al according to the riemannian metrics li et al hu et al the covariance matrix representations can be divided into two branches affine invariant riemannian metric based and log euclidean riemannian metric based i the affine invariant riemannian metric porikli et al tuzel et al is based on the following distance measure ρ c c d j ln λ j c c where λ j c matrices eigenvector c and following c c d j are the λ j c the x work j generalized c in x j porikli j eigenvalues d et al of the two covariance and x and j is the jth generalized tuzel et al austvoll and kwolek use the covariance matrix inside a region to detect whether the feature occlusion events take place the detection task can be completed by comparing the covariance matrix based distance measures in a particular window around the occluded key point acm transactions on intelligent systems and technology vol no article publication date september x li et al fig illustration of an active contour representation the left part shows the signed distance map of a human contour and the right part displays the contour tracking result ii the log euclidean riemannian metric arsigny et al formulates the distance measure between two covariance matrices in a euclidean vector space mathematically the log euclidean riemannian metric for two covariance matrices is the matrix c i and logarithm c j is formulated operator for as the d c descriptive i c j log c convenience i log c the j where log covariance matrices under the log euclidean riemannian metric are referred to as the log euclidean covariance matrices inspired by arsigny et al li et al employ the log euclidean covariance matrices of image features for visual representation since the log euclidean covariance matrices lie in a euclidean vector space their mean can be easily computed as the standard arithmetic mean due to this linear property classic subspace learning techniques e g principal component analysis can be directly applied onto the log euclidean covariance matrices following the work in li et al and hu et al wu et al extend the tracking problem of using log euclidean co variance matrices to that of using higher order tensors and aim to incrementally learn a low dimensional covariance tensor representation inspired by li et al and hu et al hong et al propose a simplified covariance region descriptor called sigma set which comprises the lower triangular matrix square root obtained by cholesky factorization of the covariance matrix used in li et al the proposed covariance region descriptor characterizes the second order statistics of object appearance by a set of vectors meanwhile it retains the advantages of the region covariance descriptor porikli et al such as low dimensionality robustness to noise and illumination variations and good discriminative power wavelet filtering based representation in principle a wavelet filtering based representation takes advantage of wavelet transforms to filter the object region in different scales or directions for instance he et al utilize a ga bor wavelet transform gwt for visual representation specifically an object is represented by several feature points with high gwt coefficients moreover li et al propose a tracking algorithm based on three layer simplified biologically inspired sbi features i e image layer layer and layer through the flattening operations on the four gabor energy maps in the layer a unified sbi feature vector is returned to encode the rich spatial frequency information active contour representation in order to track the nonrigid objects active contour representations have been widely used in recent years paragios and deriche cremers allili and ziou vaswani et al sun et al typically an active contour representation shown in figure is defined as a signed distance acm transactions on intelligent systems and technology vol no article publication date september a survey of appearance models in visual object tracking map x y x y c d x y c x y r out d x y c x y r in where r in c and d x and y c r out is a respectively function denote the regions returning the smallest inside and outside the contour euclidean distance from point x y to the contour c moreover an active contour representation is associated with an energy function which comprises three terms internal energy external energy and shape energy the internal energy term reflects the internal constraints on the object contour e g the curvature based evolution force the external energy term measures the likelihood of the image data belonging to the foreground object class and the shape energy characterizes the shape prior constraints on the object contour discussion without feature extraction the raw pixel representation is simple and efficient for visual object tracking since only considering the color information on object appearance the raw pixel representation is susceptible to complicated appear ance changes caused by illumination variation the constant brightness constraint cbc optical flow captures the field information on the translational vectors of each pixel in a region with the potential assumption of locally unchanged brightness however the cbc assumption is often invalid in the complicated situations caused by image noise illumination fluctuation and local defor mation to address this issue the non brightness constraint optical flow is developed to introduce more geometric constraints on the contextual relationship of pixels the single cue histogram representation is capable of efficiently encoding the sta tistical distribution information of visual features within the object regions due to its weakness in characterizing the spatial structural information of tracked objects it is often affected by background distractions with similar colors to the tracked objects in order to capture more spatial information the spatial color histogram representation is introduced for visual object tracking usually it encodes the spatial information by either modeling object appearance in a joint spatial color feature space or taking a patch division strategy however the preceding histogram representations do not consider the shape or texture information of object appearance as a consequence it is difficult to distinguish the object from the background with similar color distribu tions to alleviate this issue the shape texture histogram representation is proposed to integrate shape or texture information e g gradient or edge into the histogram rep resentation leading to the robustness of object appearance variations in illumination and pose the advantages of using the covariance matrix representation are as follows i it can capture the intrinsic self correlation properties of object appearance ii it provides an effective way of fusing different image features from different modalities iii it is low dimensional leading to the computational efficiency iv it allows for comparing regions of different sizes or shapes v it is easy to implement vi it is robust to illumination changes occlusion and shape deformations the disadvantages of using the covariance matrix representation are as follows i it is sensitive to noisy corruption because of taking pixel wise statistics ii it loses much useful information such as texture shape and location a wavelet filtering based representation is to encode the local texture information of object appearance by wavelet transform which is a convolution with various wavelet filters as a result the wavelet filtering based representation is capable of character izing the statistical properties of object appearance in multiple scales and directions e g gabor filtering acm transactions on intelligent systems and technology vol no article publication date september x li et al fig illustration of several local features extracted by using the software which can be downloaded at http www robots ox ac uk vgg research affine and http www klab caltech edu harel share gbvs php an active contour representation is designed to cope with the problem of nonrigid object tracking usually the active contour representation adopts the signed distance map to implicitly encode the boundary information of an object on the basis of level set evolution the active contour representation can precisely segment the object with a complicated shape local feature based visual representation as shown in figure local feature based visual representations mainly utilize interest points or saliency detection to encode the object appearance information in general the local features based on the interest points can be mainly categorized into seven classes local template based segmentation based sift based mser based surf based corner feature based feature pool based and saliency detection based several representative tracking methods using local feature based visual representations are listed in rows 22 of table ii local template based in general local template based visual representations repre sent an object region using a set of part templates in contrast to the global template based visual representation they are able to cope with partial occlusions effectively and model shape articulations flexibly for instance a hierarchical part template shape model is proposed for human detection and segmentation lin et al the shape model is associated with a part template tree that decomposes a human body into a set of part templates by hierarchically matching the part templates with a test image the proposed part template shape model can generate a reliable set of detection hypotheses which are then put into a global optimization framework for final human localization segmentation based typically a segmentation based visual representation incor porates the image segmentation cues e g object boundary ren and malik into the process of object tracking which leads to reliable tracking results another alternative is based on superpixel segmentation which aims to group pixels into perceptually meaningful atomic regions for example wang et al construct a local template based visual representation with the superpixel segmentation as shown in figure specifically the surrounding region of an object is segmented into several superpixels each of which corresponds to a local template by building a local acm transactions on intelligent systems and technology vol no article publication date september a survey of appearance models in visual object tracking fig illustration of the local template based visual representation using superpixels template dictionary based on the mean shift clustering an object state is predicted by associating the superpixels of a candidate sample with the local templates in the dictionary sift based usually a sift based visual representation directly makes use of the sift features inside an object region to describe the structural information of ob ject appearance usually there are two types of sift based visual representations i individual sift point based and ii sift graph based for i zhou et al set up a sift point based visual representation and combine this visual represen tation with the mean shift for object tracking specifically sift features are used to find the correspondences between the regions of interest across frames meanwhile the mean shift procedure is implemented to conduct a similarity search via color his tograms by using a mutual support mechanism between sift and the mean shift the tracking algorithm is able to achieve a consistent and stable tracking perfor mance however the tracking algorithm may suffer from background clutter which may lead to a one to many sift feature matching in this situation the mean shift and sift feature matching may make mutually contradictory decisions for ii the sift graph based visual representations are based on the underlying geometric con textual relationship among sift feature points for example tang and tao construct a relational graph using sift based attributes for object representation the graph is based on the stable sift features which persistently appear in sev eral consecutive frames however such stable sift features are unlikely to exist in complex situations such as shape deformation and illumination changes mser based an mser based visual representation needs to extract the mser maximally stable extremal region features for visual representation sivic et al subsequently tran and davis construct a probabilistic pixelwise oc cupancy map for each mser feature and then perform the mser feature matching for object tracking similar to tran and davis donoser and bischof also use mser features for visual representation to improve the stability of mser features they take temporal information across frames into consideration surf based with the scale invariant and rotation invariant properties the surf speeded up robust feature is a variant of sift bay et al it has similar properties to those of sift in terms of repeatability distinctiveness and robustness but its computational speed is much faster inspired by this fact he et al develop a tracking algorithm using a surf based visual representation by judging the compatibility of local surf features with global object motion the tracking algorithm is robust to appearance changes and background clutters acm transactions on intelligent systems and technology vol no article publication date september x li et al corner feature based typically a corner feature based visual representation makes use of corner features inside an object region to describe the structural properties of object appearance and then matches these corner features across frames for object lo calization for instance kim utilizes corner features for visual representation and then performs dynamic multilevel corner feature grouping to generate a set of corner point trajectories as a result the spatiotemporal characteristics of object appearance can be well captured moreover grabner et al explore the intrinsic differences between the object and non object corner features by building a boosting discriminative model for corner feature classification local feature pool based recently local feature pool based visual representations have been widely used in ensemble learning based object tracking usually they need to set up a huge feature pool i e a large number of various features for constructing a set of weak learners which are used for discriminative feature selection therefore different kinds of visual features e g color local binary pattern collins et al histogram of oriented gradients collins et al liu and yu yu et al gabor features with gabor wavelets nguyen and smeulders and haar like features with haar wavelets babenko et al can be used by fssl in an inde pendent or interleaving manner for example collins et al set up a color fea ture pool whose elements are linear combinations of the following rgb components α the discriminative β γ α β γ color features from this as pool a result grabner an object and bischof is localized by construct selecting an ensemble classifier by learning several weak classifiers trained from the haar like features viola and jones histograms of oriented gradient hog dalal and triggs and local binary patterns lbp ojala et al babenko et al utilize the haar like features to construct a weak classifier and then apply an online multiple instance boosting to learn a strong ensemble classifier for object tracking saliency detection based in principle saliency detection is inspired by the focus of attention foa theory palmer wolfe 1994 to simulate the human perception mechanism for capturing the salient information of an image such salient infor mation is helpful for visual object tracking due to its distinctness and robustness based on saliency detection researchers apply the biological vision theory to visual object tracking toyama and hager mahadevan and vasconcelos more recently yang et al construct an attentional visual representation method based on the spatial selection this visual representation method takes a two stage strategy for spatial selective attention at the first stage a pool of attentional regions ars are extracted as the salient image regions at the second stage discriminative learning is performed to select several discriminative attentional regions for visual representation finally the task of object tracking is taken by matching the ars between two consecutive frames discussion the aforementioned local feature based representations use local templates segmentation sift mser surf corner points local feature pools or saliency detection respectively due to the use of different features these represen tations have different properties and characteristics by representing an object re gion using a set of part templates the local template based visual representations are able to encode the local spatial layout information of object appearance resulting in the robustness to partial occlusions with the power of image segmentation the segmentation based visual representations are capable of well capturing the intrinsic structural information e g object boundaries and superpixels of object appearance leading to reliable tracking results in challenging situations since the sift features are invariant to image scaling partial occlusion illumination change and camera viewpoint change the sift based representation is robust to appearance changes in acm transactions on intelligent systems and technology vol no article publication date september a survey of appearance models in visual object tracking illumination shape deformation and partial occlusion however it cannot encode pre cise information on the objects such as size orientation and pose the mser based representation attempts to find several maximally stable extremal regions for feature matching across frames hence it can tolerate pixel noise but suffers from illumination changes the surf based representation is on the basis of the speeded up robust features which has the properties of scale invariance rotation invariance and com putational efficiency the corner point representation aims to discover a set of corner features for feature matching therefore it is suitable for tracking objects e g cars or trucks with plenty of corner points and sensitive to the influence of nonrigid shape deformation and noise the feature pool based representation is strongly correlated with feature selection based ensemble learning that needs a number of local features e g color texture and shape due to the use of many features the process of feature extraction and feature selection is computationally slow the saliency detection based representation aims to find a pool of discriminative salient regions for a particular ob ject by matching the salient regions across frames object localization can be achieved however its drawback is to rely heavily on salient region detection which is sensitive to noise or drastic illumination variation discussion on global and local visual representations in general the global visual representations are simple and computationally efficient for fast object tracking due to the imposed global geometric constraints the global visual representations are susceptible to global appearance changes e g caused by illumination variation or out of plane rotation to deal with complicated appearance changes a multi cue strategy is taken by the global features to incorporate multiple types of visual information e g position shape texture and geometric structure into the appearance models in contrast the local visual representations are able to capture the local structural object appearance consequently the local visual representations are robust to global appearance changes caused by illumination variation shape deformation rotation and partial occlusion since they require the keypoint detection the interest point based lo cal visual representations often suffer from noise disturbance and background distrac tion moreover the local feature pool based visual representations which are typically required by discriminative feature selection need a huge number of local features e g color texture and shape resulting in a very high computational cost inspired by the biological vision the local visual representations using biological features attempt to capture the salient or intrinsic structural information inside the object regions this salient information is relatively stable during the process of visual object tracking however salient region features rely heavily on salient region detection which may be susceptible to noise or drastic illumination variation leading to potentially many feature mismatches across frames statistical modeling for tracking by detection recently visual object tracking has been posed as a tracking by detection problem where statistical modeling is dynamically performed to support object detection according to the model construction mechanism statistical modeling is classified into three categories including generative discriminative and hybrid generative discriminative the generative appearance models mainly concentrate on how to accurately fit the data from the object class however it is very difficult to verify the correctness of the specified model in practice besides the local optima are always obtained during the course of parameter estimation e g expectation maximization by introducing online update mechanisms they incrementally learn visual representations for the acm transactions on intelligent systems and technology vol no article publication date september x li et al table iii summary of representative tracking by detection appearance models based on generative learning techniques item no references mixture kernel density subspace used generative models estimation learning learning techniques mckenna et al gaussian mixture model gmm in the hue saturation color space color based gmm spatial color appearance model using gmm spatial color mixture of gaussians smog yu and wu wang et al spatio color gmm three component mixture models w component s component l component jepson et al zhou et al wsl mean shift using a spatially weighted comaniciu et al leichter et al color driven color histogram mean shift using multiple reference color histograms leichter et al shape integration affine kernel fitting using color and boundary cues collins yang et al scale aware mean shift considering scale changes nguyen et al scale aware em based maximum likelihood estimation for kernel based tracking yilmaz non symmetric kernel asymmetric kernel mean shift shen et al global mode seeking annealed mean shift han et al sequential kernel based tracking sequential kernel density estimation black and jepson ho et al ross et al wen et al wang et al vector based linear subspace learning principal component analysis partial least square analysis wang et al li et al wen et al hu et al linear tensor based learning subspace component tensor analysis principle subspace analysis continued acm transactions on intelligent systems and technology vol no article publication date september a survey of appearance models in visual object tracking table iii continued item no mixture models kernel density estimation subspace learning used generative learning techniques references local linear embedding kernel principle component analysis lim et al chin and suter nonlinear subspace learning mei and ling li et al zhang et al jia et al bao et al sparse representation l sparse approximation li et al metric weighted least square regression li et al non sparse representation dct representation signal compression lee and kriegman fan et al kwon and lee bi subspace or multi subspace learning multiple subspaces hou et al sclaroff and isidoro matthews and baker active appearance shape and appearance models mesh fitting foreground object region information while ignoring the influence of the background as a result they often suffer from distractions caused by the background regions with similar appearance to the object class table iii lists representative tracking by detection methods based on generative learning techniques in comparison discriminative appearance models pose visual object tracking as a binary classification issue they aim to maximize the separability between the object and non object regions discriminately moreover they focus on discovering highly in formative features for visual object tracking for computational consideration online variants are proposed to incrementally learn discriminative classification functions for the purpose of object or non object predictions thus they can achieve effective and ef ficient predictive performances nevertheless a major limitation of the discriminative appearance models is to rely heavily on training sample selection e g by self learning or co learning table iv lists representative tracking by detection methods based on discriminative learning techniques the generative and discriminative appearance models have their own advantages and disadvantages and are complementary to each other to a certain extent therefore researchers propose hybrid generative discriminative appearance models hgdams to fuse the useful information from the generative and the discriminative models due to taking a heuristic fusion strategy hgdams cannot guarantee that the performance of the hybrid models after information fusion is better than those of the individual models in addition hgdams may add more constraints and introduce more parame ters leading to more inflexibility in practice table v lists representative tracking by detection methods based on hybrid generative discriminative learning techniques mixture generative appearance models typically this type of generative appearance model adaptively learns several compo nents for capturing the spatiotemporal diversity of object appearance they can be classified into two categories wsl mixture models and gaussian mixture models acm transactions on intelligent systems and technology vol no article publication date september x li et al acm transactions on intelligent systems and technology vol no article publication date september a survey of appearance models in visual object tracking acm transactions on intelligent systems and technology vol no article publication date september x li et al table v summary of representative tracking by detection methods using hybrid generative discriminative learning techniques item no references combination single layer combination multi layer used learning techniques kelm et al multi conditional learning lin et al combination of pca and fisher lda grabner et al combination of boosting and robust pca yang et al discriminative subspace learning using positive and negative data everingham and zisserman shen et al combination of a tree structured classifier and a lambertian lighting model combination of svm learning and kernel density estimation lei et al three layer combination of relevance vector machine and gmm learner combination layer classifier combination layer decision combination layer yu et al combination of the constellation model and fisher kernels wsl mixture models in principle the wsl mixture model jepson et al contains the following three components w component s component and l component these three components characterize the interframe variations the stable structure for all past observations and outliers such as occluded pixels re spectively as a variant of jepson et al another wsl mixture model zhou et al is proposed that directly employs the pixelwise intensities as visual fea tures instead of using the filter responses e g jepson et al moreover the l component is discarded in modeling the occlusion using robust statistics and an f component is added as a fixed template that is observed most often gaussian mixture models in essence the gaussian mixture models mckenna et al stauffer and grimson han and davis yu and wu wang et al utilize a set of gaussian distributions to approximate the underlying density function of object appearance as shown in figure for instance an object appearance model han and davis using a mixture of gaussian density func tions is proposed for automatically determining the number of density functions and their associated parameters including mean covariance and weight rectangular features are introduced by averaging the corresponding intensities of neighboring pixels e g or in each color channel to capture a spatialtemporal de scription of the tracked objects wang et al present a spatial color mixture of gaussians referred to as smog appearance model which can simultaneously encode both spatial layout and color information to enhance its robustness and stability wang et al further integrate multiple cues into the smog appearance model including three features of edge points their spatial distribution gradient intensity and size however it is difficult for the gaussian mixture models to select the correct number of components for example adaptively determining the com ponent number k in a gmm is a difficult task in practice as a result the mixture models often use ad hoc or heuristic criteria for selecting k leading to the tracking inflexibility acm transactions on intelligent systems and technology vol no article publication date september a survey of appearance models in visual object tracking fig illustration of the mode seeking process by mean shift kernel based generative appearance models kgams kernel based generative appearance models kgams utilize kernel density estimation to construct kernel based visual representations and then carry out the mean shift for object localization as shown in figure according to the mechanisms used for kernel construction or mode seeking they may be split into the following six branches color driven kgams shape integration kgams scale aware kgams nonsymmetric kgams kgams by global mode seeking and sequential kernel learning kgams color driven kgams typically a color driven kgam comaniciu et al builds a color histogram based visual representation regularized by a spatially smooth isotropic kernel using the bhattacharyya coefficient as the similarity metric a mean shift procedure is performed for object localization by finding the basin of attraction of the local maxima however the tracker comaniciu et al only considers color information and therefore ignores other useful information such as edge and shape resulting in the sensitivity to background clutters and occlusions another color driven kgam leichter et al is developed to handle multiview color variations by constructing the convex hull of multiple view specific reference color histograms shape integration kgams in general shape integration kgams aim to build a ker nel density function in the joint color shape space for example a shape integration kgam leichter et al is proposed to capture the spatiotemporal properties of object appearance using color and boundary cues it is based on two spatially normalized and rotationally symmetric kernels for describing the information about the color and object boundary acm transactions on intelligent systems and technology vol no article publication date september fig illustration of gaussian mixture generative appearance models 22 x li et al scale aware kgams in essence scale aware kgams are for capturing the spa tiotemporal distribution information on object appearance at multiple scales for instance a scale aware kgam collins using the difference of gaussian based mean shift features is presented to cope with the problem of kernel scale selection by detecting local maxima of the difference of gaussian dog scale space filters formulated as follows dog x σ x exp 2σ x where σ is a scaling factor based on a new probabilistic interpretation another scale aware kgam nguyen et al is proposed to solve a maximum likelihood problem which treats the coordinates for the pixels as random variables as a result the problem of kernel scale selection is converted to that of maximum likelihood optimization in the joint spatial color space nonsymmetric kgams conventional kgams use a symmetric kernel e g a circle or an ellipse leading to a large estimation bias in the process of estimating the complicated underlying density function to address this issue a nonsymmetric kgam yilmaz is developed based on the asymmetric kernel mean shift with adaptively varying the scale and orientation of the kernel in contrast to the symmetric mean shift only requiring the image coordinate estimate the nonsymmetric kgam needs to simultaneously estimate the image coordinates the scales and the orientations in a few number of mean shift iterations introducing asymmetric kernels can generate a more accurate representation of the underlying density so that the estimation bias is reduced furthermore the asymmetric kernel is just a generalization of the previous radially symmetric and anisotropic kernels kgams by global mode seeking due to the local optimization property of the mean shift large interframe object translations lead to tracking degradations or even failures in order to tackle this problem shen et al propose an annealed mean shift algorithm motivated by the success of the annealed importance sampling which is essentially a way of assigning the weights to the states obtained by multiple simulated annealing runs neal here the states correspond to the object positions while the simulated annealing runs are associated with different bandwidths for the kernel density estimation the proposed annealed mean shift algorithm aims to make a progressive position evolution of the mean shift as the bandwidths monotonically decrease i e the convergence position of mean shift with the last bandwidth works as the initial position of the mean shift with the next bandwidth and finally seeks the global mode sequential kernel learning kgams batch mode kernel density estimation needs to store the nonparametric representations of the kernel densities leading to a high computational and memory complexity to address this issue han et al develop a sequential kernel density approximation skde algorithm for real time visual object tracking the skde algorithm sequentially learns a nonparametric representation of the kernel density and propagates the density modes over time discussion the color driven kernel based tracking algorithms mainly take the color information into consideration however complicated factors may give rise to drastic tracking degradations including scale changes background clutters occlusions and rapid object movements to address this issue various algorithmic extensions have been made the aim of scale aware tracking algorithms is to capture the multiscale spatial layout information of object appearance thus they are capable of effectively completing the tracking task under the circumstance of drastic scaling changes moreover the edge or shape information is very helpful for accurate object localization or resisting background distraction motivated acm transactions on intelligent systems and technology vol no article publication date september exp a survey of appearance models in visual object tracking fig illustration of linear pca subspace models the left part shows a candidate sample and the right part displays a linear combination of eigenbasis samples by this consideration shape driven kernel based tracking algorithms have been developed to integrate the edge or shape information into the kernel design process normally the kernel based tracking algorithms utilize symmetric kernels e g a circle or an ellipse for object tracking resulting in a large estimation bias for complicated underlying density functions to tackle this problem nonsymmetric kernel based tracking algorithms are proposed to construct a better representation of the underlying density conventional kernel based tracking algorithms tend to pursue the local model seeking resulting in tracking degradations or even failures due to their local optimization properties to address this issue researchers borrow ideas from both simulated annealing and annealed importance sampling to obtain a feasible solution to global mode seeking in practice the factors of computational complexity and memory consumption have a great effect on real time kernel based tracking algorithms thus sequential techniques for kernel density estimation have been developed for online kernel based tracking subspace learning based generative appearance models slgams in visual object tracking a target is usually associated with several underlying sub spaces each of which is spanned by a set of basis templates for convenience let τ denote space mathematically the target and a the a target a n τ can denote be linearly the basis represented templates in of the an underlying following form sub τ c t where c a c a c n a n a a a n c c c n tive appearance c c n models is the slgams coefficient focus vector on therefore how to effectively subspace obtain learning based these underlying genera subspaces and their associated basis templates by using various techniques for sub space analysis for instance some slgams utilize eigenvalue decomposition or linear regression for subspace analysis and others construct multiple subspaces to model the distribution characteristics of object appearance according to the used techniques for subspace analysis they can be categorized into two types conventional and unconven tional slgams conventional subspace models in general conventional subspace models can be split into the following two branches linear subspace models and nonlinear subspace models linear subspace models in recent years linear subspace models lsms have been widely applied to visual object tracking according to the dimension of the used feature space lsl can be divided into i lower order lsms and ii higher order lsms the lower order lsms black and jepson ho et al li et al skocaj and leonardis wen et al need to construct vector based subspace models e g eigenspace by principal component analysis shown in figure while the higher order lsms needs to build matrix based or tensor based subspace models acm transactions on intelligent systems and technology vol no article publication date september x li et al e g eigenspace by principal component analysis and tensor eigenspace by tensor analysis for i several incremental principal component analysis pca algorithms are proposed to make linear subspace models more efficient for instance an incre mental robust pca algorithm li et al is developed to incorporate robust analysis into the process of subspace learning similar to li et al skocaj and leonardis embed the robust analysis technique into the incremental subspace learning framework which makes a sequential update of the principal subspace the learning framework considers the weighted influence of both individual images and individual pixels within an image unlike the aforementioned robust pca algorithm based on weighted residual errors the incremental subspace learning algorithms levy and lindenbaum brand utilize incremental singular value decom position svd to obtain a closed form solution to subspace learning however these incremental pca algorithms cannot update the sample mean during subspace learn ing to address this issue a subspace model based on r svd i e rank r singular value decomposition is built with a sample mean update ross et al more over wang et al apply partial least square analysis to learn a low dimensional feature subspace for object tracking in theory the partial least square analysis is capable of modeling relations between sets of variables driven by a small number of latent factors leading to robust object tracking results for ii a set of higher order lsms are proposed to address the small sample size problem where the number of samples is far smaller than the dimension of samples therefore many researchers have begun to build matrix based or tensor based subspace models for instance wang et al directly analyze the image matrices and construct a based appearance model for object tracking in addition to the foreground information they also consider background information to avoid the distractions from the background clutters moreover li et al and wen et al take advantage of online tensor decomposition to construct a tensor based appearance model for robust visual object tracking nonlinear subspace models if the training data lie on an underlying nonlinear man ifold the lsm based tracking algorithms may fail therefore researchers attempt to employ nonlinear subspace learning to capture the underlying geometric infor mation from target samples for the robust human tracking a nonlinear subspace model lim et al is built using nonlinear dimension reduction techniques i e local linear embedding as a nonlinear generalization of pca a nonlinear subspace model chin and suter based on kernel principal component analysis kpca is constructed to capture the kernelized eigenspace information from target samples unconventional subspace models in general unconventional subspace models can also be used for visual object tracking roughly they can be divided into three cate gories sparse nonsparse representation autoregressive modeling and multi subspace learning sparse nonsparse representation typically a set of target samples is associated with an underlying subspace spanned by several templates the likelihood of a can didate sample belonging to the object class is often determined by the residual be tween the candidate samples and the reconstructed samples derived from a linear representation tion procedure is to adopted ensure a to sparse obtain linear a sparse representation linear representation an l regularized solution optimiza mei and ling based on the sparse representation technique in mei and ling jia et al propose a tracking method that further improves the tracking accu racy by using the block division spatial pooling schemes e g average pooling max pooling and alignment pooling moreover zhang et al present a multitask acm transactions on intelligent systems and technology vol no article publication date september a survey of appearance models in visual object tracking 25 sparse tion cost optimization function instead framework of treating based on test a l samples p q regularized independently least square the framework minimiza explores the interdependencies between test group sparsity problem when p q the samples framework by degenerates solving an l p q to regularized the popular l model to tracker achieve li et mei a al real time and ling based performance on compressive of the l tracker sensing mei is built and by ling solving an a subspace orthogo nal matching pursuit omp optimization problem i e random projections which is about 000 times faster than that of mei and ling similar to li et al zhang et al make use of compressive sensing random projections to generate a low dimensional compressive feature descriptor leading to a real time tracking performance alternatively bao et al take advantage of the popular accelerated square minimization proximal problem gradient which apg has approach a quadratic to optimize convergence the l property regularized to ensure least the real time tracking performance another way of improving the efficiency of the l process tracker of mei evaluating and ling test samples is to mei reduce et al the number this of task l minimizations in the is accomplished by estimating the minimal error bound of the likelihood function in particle filtering resulting in a moderate improvement in tracking efficiency from a viewpoint of sig nal compression li et al construct a compact dct object representation based on a dct subspace spanned by cosine basis functions with the power of fast fourier transform fft the proposed dct object representation is capable of ef ficiently adapting to spatiotemporal appearance variations during tracking leading to robust tracking results in complicated situations on the other hand the sparsity of the linear representation is unnecessary for robust object tracking as long as an adequate number of template samples are pro vided as pointed out in li et al therefore a nonsparse metric weighted linear representation with a closed form solution is proposed to effectively and efficiently model the intrinsic appearance properties of the tracked object li et al autoregressive modeling since tracking is a time dependent process the target samples from adjacent frames are mutually correlated to characterize the time dependency across frames a variety of appearance models are proposed in recent years for instance a dynamical statistical shape representation is proposed to cap ture the temporal correlation information on human silhouettes from consecutive frames cremers the proposed representation learns a linear autoregressive shape model where the current silhouette is linearly constrained by the previous silhouettes the learned shape model is then integrated into the level set evolution process resulting in robust segmentation results multi subspace learning in order to capture the distribution diversity of target samples several efforts establish the double or multiple subspaces for visual rep resentation for example fan et al present a bi subspace model for visual tracking the model simultaneously considers two visual cues color appearance and texture appearance subsequently the model uses a co training strategy to exchange information between two visual cues for video based recognition and tracking lee and kriegman present a generic appearance model that seeks to set up a face appearance manifold consisting of several submanifolds each submanifold cor responds to a face pose subspace furthermore kwon and lee construct a set of basic observation models each of which is associated with a specific appear ance manifold of a tracked object by combining these basic observation models a compound observation model is obtained resulting in a robustness to combinatorial appearance changes acm transactions on intelligent systems and technology vol no article publication date september x li et al active appearance models aams usually aams hou et al sclaroff and isidoro matthews and baker need to incorporate two components a shape and b appearance for a the shape of an aam can be expressed as a linear coordinates combination n i of the p i i v of a base shape and several shape vectors where vertices the making shape up the denotes mesh x for y b x the y x appearance v n y v i i that such are that the of the aam can be represented as a linear combination of a base appearance appearance pixel minimize lying the inside images following the a base i x m cost i mesh function such that therefore for a x the given a x a test m i image λ i a i x the a where x aam and x needs several is a to model fitting x a x m λ i a i x i w x p i where aam w x p denotes a piecewise affine warp that transforms a pixel x into discussion the lower order linear subspace models lsms usually learn vector based visual representations for visual object tracking for tracking efficiency several incremental lsms e g incremental pca are developed for online visual object track ing since the vector based visual representations suffer from the small sample size problem researchers construct higher order matrix based or tensor based visual rep resentations however these lsms potentially assume that object appearance sam ples lie on an underlying linear manifold in practice this assumption is often violated because of complex extrinsic intrinsic appearance changes motivated by this consid eration nonlinear subspace models are developed for visual representation however the problem with these nonlinear subspace models is that they are computationally expensive due to the nonlinear subspace learning e g nonlinear dimension reduction in recent years unconventional subspace models have been proposed for visual object tracking these models either enforce the sparsity constraints on the linear represen tation solution or have different assumptions of subspace properties however the sparsity constrained linear representation typically induces a high optimization com plexity which motivates researchers to develop an efficient optimization method e g apg and omp for a real time tracking performance without the conventional single subspace assumption bi subspace or multi subspace algorithms are proposed to more precisely model the distribution diversity of the target samples but at the cost of an additional computation boosting based discriminative appearance models in the last decade boosting based discriminative appearance models bdams have been widely used in visual object tracking because of their powerful discriminative learning capabilities according to the learning strategies employed they can be categorized into self learning and co learning bdams typically the self learning bdams utilize the discriminative information from a single source to guide the task of object non object classification while the co learning bdams exploit the multisource discriminative information for object detection more specifically the self learning bdams first train a classifier over the data from the previous frames and subsequently use the trained classifier to evaluate possible object regions at the current frame after object localization a set of so called positive and negative samples are selected to update the classifier these positive and negative samples are labeled by the previously trained classifier due to tracking errors the training samples obtained in the tracking process may be polluted by noise therefore the labels for the training acm transactions on intelligent systems and technology vol no article publication date september a survey of appearance models in visual object tracking fig illustration of online boosting for feature selection samples are unreliable as the tracking process proceeds the tracking error may be accumulated possibly resulting in the drift problem in contrast the co learning bdams often takes a semisupervised strategy for object non object classification e g co training by building multiple classifiers on the other hand bdams also take different strategies for visual representation that is single instance and multi instance ones the single instance bdams require precise object localization if a precise object localization is not available these tracking algorithms may use suboptimal positive samples to update their corresponding object or non object discriminative classifiers which may lead to a model drift problem more over object detection or tracking has its own inherent ambiguity that is precise object locations may be unknown even for human labelers to deal with this ambiguity the multi instance bdams are proposed to represent an object by a set of image patches around the tracker location thus they can be further classified into single instance or multi instance bdams self learning single instance bdams based on online boosting oza and russell researchers have developed a variety of computer vision applications such as object detection viola and jones and visual object tracking grabner et al grabner and bischof in these applications the variants of boosting are invented to satisfy different demands conventional bdams as shown in figure the conventional bdams first make a discriminative evaluation of each feature from a candidate feature pool and then select the top ranked features to conduct the tracking process grabner et al grabner and bischof to accelerate the feature selection process liu and yu utilize gradient based feature selection to construct a bdam but this bdam requires an initial set of weak classifiers to be given in advance leading to difficulty in general object tracking these bdams often perform poorly in capturing the correlation information between features leading to the redundancy of selected features and the failure to compensate for the tracking error caused by other features to address this issue a feature weighting strategy is adopted to attach all the fea tures from the feature pool with different weights and then perform weighted fusion for object tracking for instance avidan constructs a confidence map by pixel classification using an ensemble of online learned weak classifiers which are trained by a feature weighting based boosting approach since it needs to store and compute acm transactions on intelligent systems and technology vol no article publication date september 28 x li et al all the features during feature selection the feature weighting based boosting ap proach is computationally expensive furthermore parag et al build a feature weighting based bdam for object tracking where the weak classifiers themselves are adaptively modified to adapt to scene changes namely the parameters of the weak classifiers are adaptively changed instead of replaced when the new data arrive the common property of the feature weighting based bdams is that they depend on a fixed number of weak classifiers however this property may restrict the flexibility of the trackers in practice dynamic ensemble based bdams the conventional bdams need to construct a fixed number of weak learners in advance and select these weak learners itera tively as the boosting procedure proceeds however due to the time varying prop erty of visual object tracking they are incapable of effectively adapting to dynamic object appearance changes to address this problem a dynamic ensemble based bdam visentini et al is proposed to dynamically construct and update the set of weak classifiers according to the ensemble error value noise insensitive bdams to make visual object tracking more robust to noise corrup tion a set of bdams are proposed in the literature for instance leistner et al point out that the convex loss functions typically used in boosting are highly sen sitive to random noise to enhance robustness leistner et al develop a generic bdam called online gradientboost which contains a set of noise insen sitive loss functions in essence this bdam is an extension of the gradientboost algorithm friedman and works similarly to the anyboost algorithm mason et al particle filtering integration based bdams to make visual object tracking more efficient researchers embed feature selection into the particle filtering process for example wang et al and okuma et al propose two online feature selection based bdams using particle filtering which generate the candidate state set of a tracked object and the classification results of adaboost is used to determine the final state transfer learning based bdams typically most existing bdams have an underly ing assumption that the training samples collected from the current frame follow a similar distribution to those from the last frame however this assumption is often violated when the drift problem takes place to address the drift problem a number of novel bdams wu et al luo et al are proposed to categorize the samples into two classes auxiliary samples obtained in the last frames and target samples generated in the current frame by exploring the intrinsic proximity relationships among these samples the proposed bdams are capable of effectively transferring the discriminative information on auxiliary samples to the discriminative learning process using the current target samples leading to robust tracking results co learning single instance bdams in general the self learning bdams suffer from the model drift problem due to their error accumulation caused by using the self learning strategy in order to address this problem researchers adopt the semisuper vised learning techniques zhu for visual object tracking for instance grabner et al develop a bdam based on semisupervised online boosting its main idea is to formulate the boosting update process in a semi supervised manner as a fused de cision of a given prior and an online classier as illustrated in figure subsequently liu et al make use of the co training strategy to online learn each weak clas sifier in boosting instead of only the final strong classifier the co training strategy dynamically generates a series of unlabeled samples for progressively modifying the weak classifiers leading to robustness to environmental changes it is proven that the co training strategy can minimize the boosting error bound in theory acm transactions on intelligent systems and technology vol no article publication date september a survey of appearance models in visual object tracking 29 fig illustration of single instance multi instance learning the left part shows the tracking result the middle part displays the positive and negative samples used by the single instance learning and the right part exhibits the positive and negative sample bags used by the multi instance learning multi instance bdams to deal with the underlying ambiguity of object localiza tion multiple instance learning is used for object tracking as illustrated in figure in principle it represents an object by a set of image patches around the tracker location self learning multi instance bdams for example babenko et al represent an object by a set of image patches which correspond to an instance bag with each instance being an image patch based on online multiple instance boosting a tracking system is developed to characterize the ambiguity of object localization in an online manner the tracking system assumes that all positively labeled instances are truly positive but this assumption is sometimes violated in practice furthermore the tracking system trains the weak classifiers based only on the current frame and is likely to be overfitting instead of equally treating the samples in each bag babenko et al zhang et al propose an online weighted multiple instance tracker which incorporates the sample importance information i e the samples closer to the current tracker location are of greater importance into the online multi instance boosting learning process resulting in robust tracking results to characterize the cumulative loss of the weak classifiers across multiple frames instead of the current frame li et al propose an online multi instance bdam using the strong convex the evaluating proposed elastic the multiple net cumulative regularizer instance loss instead of learning the online of the mil algorithm l algorithm regularizer of o has and t a further cumulative with t prove being regret that the number of boosting iterations acm transactions on intelligent systems and technology vol no article publication date september fig illustration of a typical co learning problem x li et al co learning multi instance bdams zeisl et al and li et al combine the advantages of semisupervised learning and multiple instance learning in the process of designing a bdam semisupervised learning can incorporate more prior information and multiple instance learning focuses on the uncertainty about where to select positive samples for model updating discussion as mentioned previously bdams can be roughly classified into self learning based and co learning based ones self learning based bdams adopt the self learning strategy to learn object non object classifiers they utilize previously learnt classifiers to select positive and negative training samples and then update the current classifiers with the selected training samples as a result tracking errors may be gradually accumulated in order to tackle this problem co learning based bdams are developed to capture the discriminative information from many unlabeled samples in each frame they generally employ semisupervised co learning techniques to update the classifiers with both labeled and unlabeled samples in an interleaved manner resulting in more robust tracking results on the other hand conventional bdams take a single instance strategy for visual representation that is one image patch for each object the drawback of this single instance visual representation is to rely heavily on exact object localization without which the tracking performance could be greatly degraded because of the suboptimal training sample selection to address this issue mil is introduced to visual object tracking it takes into account the inherent ambiguity of object localization represent ing an object by a set of image patches around the tracker location as a result the mil based tracking algorithms can achieve robust tracking results but may lose accu racy if the image patches do not precisely capture the object appearance information however all bdams need to construct a huge local feature pool for feature selection leading to a low computational speed additionally they usually obtain a local optimal solution to object tracking because of their focus on local features rather than global features svm based discriminative appearance models sdams sdams aim to learn margin based discriminative svm classifiers for maximizing in terclass separability sdams are able to discover and remember informative samples as support vectors for object non object classification resulting in a strong discriminative power effective kernel selection and efficient kernel computation play an importance role in designing robust sdams according to the used learning mechanisms sdams are typically based on self learning sdams and co learning sdams self learning sdams in principle the self learning sdams are to construct svm classifiers for object non object classification in a self learning fashion for example avidan proposes an offline sdam for distinguishing a target vehicle from a background since the sdam needs substantial prior training data in advance ex tending the algorithm to general object tracking is a difficult task following the work in avidan williams et al propose a probabilistic formulation based sdam which allows for propagating observation distributions over time despite its robustness the proposed sdam needs to fully encode the appearance variation information which is impractical in the tracking process tian et al utilize an ensemble of linear svm classifiers to construct a sdam these classifiers can be adaptively weighted according to their discriminative abilities during different periods resulting in robustness to large appearance variations these sdams need to heuristically select positive and negative samples surrounding the current tracker location to update the object non object svm classifier acm transactions on intelligent systems and technology vol no article publication date september a survey of appearance models in visual object tracking to avoid the heuristic and unreliable step of training sample selection usually requiring accurate estimation of object location two strategies are adopted in the literature one is based on a structured output support vector machine svm hare et al yao et al and the other is based on ranking svm bai and tang the key idea of these two strategies is to integrate the structured constraints e g relative ranking or voc overlap ratio between samples into the max margin optimization problem for instance hare et al propose an sdam based on a kernelized structured svm which involves an infinite number of structured loss i e voc overlap ratio based constraints in the structured output spaces in addition bai and tang therefore pose visual object tracking as a weakly supervised ranking problem which captures the relative proximity relationships between sam ples towards the true target samples co learning sdams in general the co learning sdams rely on semisuper vised multikernel learning to construct svm classifiers for object non object clas sification for instance tang et al adopt the co training svm technique to design a semisupervised tracker the disadvantage of this tracker is that it requires several initial frames to generate adequate labeled samples resulting in the inflex ibility in practice lu et al and yang et al design svm classifiers using multikernel learning mkl for visual object tracking mkl aims to learn an optimal linear combination of different kernels based on different features including the color information and spatial pyramid histogram of visual words discussion with the power of max margin learning the sdams have a good generalization capability of distinguishing foreground and background resulting in an effective svm classifier for object localization however the process of constructing the sdams requires a set of reliable labeled training samples which is a difficult task due to the influence of some complicated factors such as noisy corruptions occlusions illumination changes etc therefore most existing sdams take a heuristic strategy for training sample collection e g spatial distance based or classification score based which may lead to the instability or even drift of the tracking process to address this issue the structured svm is applied to model the structural relationships i e voc overlap ratio between samples resulting in a good tracking performance in terms of generalization and robustness to noise during tracking a hard assignment of a sample to a class label usually leads to the classification error accumulation to alleviate the issue the ranking svm a weakly supervised learning method is also introduced into the tracking process where the relative ranking information between samples is incorporated into the constraints of max margin learning the common point of the preceding sdams is to take a self learning strategy for object non object classification without considering the discriminative information from unlabeled data or multiple information sources motivated by this the co learning sdams are developed to integrate such discriminative information into the svm learn ing process by semisupervised multikernel learning randomized learning based discriminative appearance models rldams more shotton recently et al randomized lepetit learning and fua techniques and ferns e g random ozuysal forest breiman et al have been successfully introduced into the vision community in principle randomized learning techniques can build a diverse classifier ensemble by performing random input selec tion and random feature selection in contrast to boosting and svm they are more com putationally efficient and easier to extend for handling multiclass learning problems in particular they can be parallelized so that multicore and gpu implementations e g acm transactions on intelligent systems and technology vol no article publication date september 32 x li et al sharp can be performed to greatly reduce runtime however their tracking performance is unstable for different scenes because of their random feature selection inspired by randomized learning a variety of rldams are proposed in the field of visual object tracking including online random forests saffari et al santner et al random naive bayes classifiers godec et al and miforests leistner et al for instance godec et al develop a visual object tracking algorithm based on online random naive bayes classifiers due to the low computational and memory costs of random naive bayes classifiers the developed tracking algorithm has a powerful real time capability for processing long duration video sequences in contrast to online random forests saffari et al the random naive bayes classifiers have a higher computational efficiency and faster convergence in the training phase moreover leistner et al present an rldam named miforests which uses multiple instance learning to construct randomized trees and represents the hidden class labels inside target bags as random variables 7 discriminant analysis based discriminative appearance models dadams discriminant analysis is a powerful tool for supervised subspace learning in princi ple its goal is to find a low dimensional subspace with a high interclass separability according to the learning schemes used it can be split into two branches conventional discriminant analysis and graph driven discriminant analysis in general conventional dadams are formulated in a vector space while graph driven dadams utilize graphs for supervised subspace learning 7 conventional dadams typically conventional discriminant analysis techniques can be divided into one of the following two main branches unimodal dadams in principle unimodal dadams have a potential assumption that the data for the object class follow a unimodal gaussian distribution for in stance lin et al build a dadam based on incremental fisher linear discrim inant analysis iflda this dadam models the object class as a single gaussian distribution and models the background class as a mixture of gaussian distributions nguyen and smeulders use linear discriminant analysis lda for discrimina tive learning in the local texture feature space obtained by gabor filtering however there is a potential assumption that the distributions of the object and the back ground classes are approximately gaussian ones with an equal covariance li et al construct a dadam using the incremental on the image matri ces since matrix operations are directly made on these matrices the dadam is computationally efficient moreover another way of constructing unimodal dadams is by discriminant metric learning which aims to linearly map the original feature space to a new metric space by a linear projection wang et al jiang et al after discriminant metric learning the similarity between intraclass samples are minimized while the distance between interclass samples are maximized re sulting in an effective similarity measure for robust object tracking note that the above dadams are incapable of dealing well with the object and background classes having multimodal distributions multimodal dadams in essence multimodal dadams model the object class and the background class as a mixture of gaussian distributions for example xu et al take advantage of adaptive subclass discriminant analysis sda i e an extension to the basic sda zhu and martinez for object tracking the adaptive sda first partitions data samples into several subclasses by a nearest neighbor clustering and then runs the traditional lda for each subclass acm transactions on intelligent systems and technology vol no article publication date september a survey of appearance models in visual object tracking fig illustration of transductive learning a shows the decision hyperplane obtained by the conventional supervised learning and b displays the decision hyperplane further adjusted by the unlabeled samples of transductive learning 7 graph driven dadams researchers utilize the generalized graph based discrim inative learning i e graph embedding and graph transductive learning to construct a set of dadams for visual object tracking typically these dadams mainly have the following two branches graph embedding based dadams in principle the goal of graph embedding based dadams is to set up a graph based discriminative model which utilizes the graph based techniques to embed the high dimensional samples into a discriminative low dimensional space for the object non object classification for instance zhang et al design a dadam based on graph embedding based lda which makes a basic assumption that the background class is irregularly distributed with multiple modalities while the object class follows a single gaussian distribution however this basic assumption does not hold true in the case of complex intrinsic and extrinsic object appearance changes graph transductive learning based dadams in general graph transductive learning based dadams aim to utilize the power of graph based semisupervised transductive learning for the likelihood evaluation of the candidate samples belong ing to the object class they make use of the intrinsic topological information between the labeled and unlabeled samples to discover an appropriate decision hyperplane for object non object classification as shown in figure for instance zha et al develop a tracker based on graph based transductive learning the tracker utilizes the labeled samples to maximize the interclass separability and the unla beled samples to capture the underlying geometric structure of the samples 7 discussion the goal of dadams is to learn a decision hyperplane to separate the object class from the background class however the traditional dadams perform poorly when both the object class and the background class have multimodal sta tistical distributions to overcome this limitation multimodal discriminant analysis is adopted to explore the training data distributions by data clustering to make a nonlin ear extension to the conventional dadams graph based dadams are proposed these dadams try to formulate the problem of discriminant analysis as that of graph learn ing such as graph embedding and graph transductive learning however a drawback is that these algorithms need to retain a large amount of labeled unlabeled samples for graph learning leading to their impracticality for real tracking applications acm transactions on intelligent systems and technology vol no article publication date september 34 x li et al codebook learning based discriminative appearance models cldams in principle cldams need to constructed the foreground and background codebooks to adaptively capture the dynamic appearance information from the foreground and background recently yang et al constructed two codebooks of image patches using two different features rgb and lbp features leading to the robustness in handling occlusion scaling and rotation to capture more discriminative information an adaptive class specific codebook gall et al is built for instance tracking the codebook encodes the information on spatial distribution and appearance of object parts and can be converted to a more instance specific codebook in a probabilistic way i e probabilistic votes for the object instance inspired by the tracking by detection idea andriluka et al establish object specific codebooks which are constructed by clustering local features i e shape context feature descriptors and hessian laplace interest points extracted from a set of training images these codebooks are then embedded into a part based model for pedestrian detection therefore cldams often consider the discriminative information not only from the background but also from other object instances however it is very difficult to construct a universal codebook for different scenes or objects as a result it is necessary to collect different training samples for different scenes or objects leading to inflexibility in practice in addition determining the codebook size is a difficult task in practice hybrid generative discriminative appearance models hgdams as discussed in ulusoy and bishop the generative and the discriminative mod els have their own advantages and disadvantages and are complementary to each other to some extent consequently much effort has been made to propose a variety of hybrid generative discriminative models for combining the benefits of both the genera tive and the discriminative models in visual object tracking these hybrid generative discriminative models aim to combine the generative and the discriminative models in a single layer or multilayer manner hgdams via single layer combination hgdams via single layer combination aim to fuse the generative and the discriminative models at the same layer they attempt to fuse the confidence scores of the generative and the discriminative models to generate better tracking results than just using them individually typically they have two kinds of combination mechanisms decision level combination and intermediate level combination hgdams via decision level combination in principle such hgdams focus on how to effectively fuse the confidence scores from the generative and the discriminative models for instance a linear fusion strategy kelm et al is taken to combine the log likelihood of discriminative and generative models for pixel classification it is pointed out kelm et al that the performance of the combined generative discriminative models is associated with a balance between the purely generative and purely discriminative ones in addition lin et al propose a hgdam that is a generalized version of the fisher linear discriminant analysis this hgdam con sists of two components the observation submodel and the discriminative submodel hgdams via intermediate level combination in principle the hgdams via intermediate level combination aim to simultaneously utilize both low level features and high level confidence scores from the generative and the discriminative models for instance yang et al impose three data driven constraints on the proposed object appearance model negative data bottom up pairwise data constraints and adaptation dynamics as a result the object appearance model can greatly ameliorate the problem of adaptation drift and can achieve good tracking acm transactions on intelligent systems and technology vol no article publication date september a survey of appearance models in visual object tracking performances in various nonstationary scenes furthermore grabner et al propose a hgdam based on a boosting algorithm called eigenboosting which re quires visual features to be discriminative with reconstructive abilities at the same time in principle eigenboosting aims to minimize a modified boosting error function in which the generative information i e eigenimages generated from haar like binary basis functions using robust pca is integrated as a multiplicative prior hgdams via multilayer combination in principle the goal of the hgdams via multilayer combination is to combine the information from the generative and discriminative models at multiple layers in general such hgdams can be divided into two classes hgdams via sequential combination and hgdams via interleaving combination hgdams via sequential combination in principle the hgdams via sequential combination aim to fuse the benefits of the generative and discriminative models in a sequential manner namely they use the decision output of one model as the input of the other model for example everingham and zisserman combine generative and discriminative head models a discriminative tree structured classifier is trained to make efficient detection and pose estimation over a large pose space with three degrees of freedom subsequently a generative head model is used for the identity verification moreover shen et al develop a generalized kernel based hgdam which learns a dynamic visual representation by online svm learning subsequently the learned visual representation is incorporated into the standard ms tracking pro cedure furthermore lei et al propose a hgdam using sequential bayesian learning the proposed tracking algorithm consists of three modules in the first module a fast relevance vector machine algorithm is used to learn a discriminative classifier in the second module a sequential gaussian mixture model is learned for visual representation in the third module a model combination mechanism with a three level hierarchy is discussed including the learner combination at level one classifier combination at level two and decision combination at level three hgdams via interleaving combination in principle the goal of the hgdams via interleaving combination is to combine the discriminative generative information in a multilayer interleaving manner namely the decision output of one model is used to guide the learning task of the other model and vice versa for instance yu et al utilize a co training strategy to combine the information from a svm classifier and a generative multi subspace model lee and kriegman in a multilayer interleaving manner benchmark resources for visual object tracking to evaluate the performance of various tracking algorithms one needs the same test video dataset the ground truth and the implementation of the competing tracking algorithms table vi lists the current major resources available to the public another important issue is how to evaluate tracking algorithms in a qualitative or quantitative manner typically qualitative evaluation is based on intuitive perception by human namely if the calculated target regions cover more true object regions and contain fewer non object pixels the tracking algorithms are considered to achieve better tracking performances otherwise the tracking algorithms perform worse for a clear illustration a qualitative comparison of several representative visual representations is provided in table vii in terms of computational speed as well as handling occlu sion illumination variation and shape deformation capabilities moreover table viii provides a qualitative comparison of several representative statistical modeling based acm transactions on intelligent systems and technology vol no article publication date september x li et al acm transactions on intelligent systems and technology vol no article publication date september a survey of appearance models in visual object tracking acm transactions on intelligent systems and technology vol no article publication date september 38 x li et al table vii qualitative comparison of visual representations item no visual representations what to track shape deformation speed occlusion illumination vector based raw pixel representation ross et al rectangle high matrix based raw pixel representation li et al rectangle high multi cue raw pixel representation i e color position edge wang et al rectangle moderate multi cue spatial color histogram representation i e joint histogram in x y r g b georgescu and meer rectangle high multi cue spatial color histogram representation i e patch division histogram adam et al rectangle high covariance representation porikli et al li et al hu et al wu et al 7 rectangle moderate wavelet filtering based li et al representation rectangle slow cremers sun et al active contour representation contour slow local feature based represnetation local templates lin et al rectangle moderate local feature based represnetation mser features tran and davis irregular regions slow zhou et al local feature based represnetation sift features interest points slow local feature based represnetation surf features he et al interest points moderate local feature based represnetation corner features kim interest points moderate local feature based represnetation saliency detection based features fan et al note symbols saliency patches slow and mean that the visual representation can or cannot cope with the situations of occlusions illumination changes and shape deformations respectively acm transactions on intelligent systems and technology vol no article publication date september a survey of appearance models in visual object tracking 58 table viii qualitative comparison of representative statistical modeling based appearance models item no statistical modeling based appearance models domain speed memory usage adaptability online discriminability linear subspace models manifold learning fast low strong weak nonlinear subspace models manifold learning slow high weak moderate mixture models density parametric estimation moderate low strong moderate kernel based models density nonparametric estimation fast low weak weak boosting based appearance models ensemble learning moderate low strong strong svm based appearance models maximum margin learning slow high strong strong 7 classifier ensemble randomized learning based on random based appearance input selection and models random feature selection fast high strong weak discriminant analysis based appearance models supervised subspace learning fast low strong weak codebook learning based appearance models vector quantization slow high strong strong appearance models in terms of computational speed memory usage online adaptabil ity and discriminability in contrast a quantitative evaluation relies heavily on the ground truth annotation if objects of interest are annotated with bounding boxes a quantitative evaluation is performed by computing the positional errors of four corners between the tracked bounding boxes and the ground truth alternatively the overlapping ratio between the tracked bounding boxes or quantitative evaluation r and ellipses a g is is the difficult ground and truth time the ellipses a a t t task and the a a g g of where ground a t ground truth can be calculated for the is the tracked bounding box or ellipse truth annotation with bounding boxes or consuming consequently researchers take a point based annotation strategy for the quantitative evaluation specifically they either record object center locations as the ground truth for simplicity and efficiency or mark several points within the object regions by hand as the ground truth for accuracy e g seven mark points are used in the dudek face sequence ross et al this way we can compute the positional residuals between the tracking results and the ground truth for the quantitative evaluation conclusion and future directions in this work we have presented a survey of appearance models for visual object tracking the presented survey takes a module based organization to review the literature of two important modules in appearance models visual represen tations and statistical modeling schemes for tracking by detection as shown in figure the visual representations focus more on how to robustly describe the acm transactions on intelligent systems and technology vol no article 58 publication date september 58 40 x li et al spatiotemporalcharacteristics of object appearance while the statistical modeling schemes for tracking by detection put more emphasis on how to capture the gener ative discriminative statistical information of the object regions these two modules are closely related and interleaved with each other in practice powerful appearance models depend on not only effective visual representations but also robust statistical models in spite of great progress in appearance models in recent years there are still several issues remaining to be addressed balance between tracking robustness and tracking accuracy existing appearance models are incapable of simultaneously guaranteeing tracking robustness and track ing accuracy to improve the tracking accuracy more visual features and geometric constraints are incorporated into the appearance models resulting in a precise object localization in the situations of particular appearance variations however these vi sual features and geometric constraints can also lower the generalization capabilities of the appearance models in the aspect of undergoing other appearance variations on the other hand to improve the tracking robustness the appearance models relax some constraints on a precise object localization thus allowing for more ambiguity of the object localization thus balancing tracking robustness and tracking accuracy is an interesting research topic balance between simple and robust visual features in computer vision designing both simple and robust visual features is one of the most fundamental and important problems in general simple visual features have a small number of components as a result they are computationally efficient but have a low discriminability in contrast robust visual features often have a large number of components consequently they are computationally expensive and have sophisticated parameter settings therefore how to keep a good balance between simplicity and robustness plays an important role in visual object tracking and information fusion appearance models are computationally efficient and simple to implement due to the information loss of to projections appearance models cannot accurately estimate the poses of the tracked objects lead ing to sensitivity to occlusion and out of plane rotation in contrast appearance models are capable of precisely characterizing the pose of the tracked objects re sulting in robustness to occlusion and out of plane rotation however appearance models require a large parameter search space for pose estimation resulting in expensive computational costs therefore combining the advantages of and appearance models is a challenging research topic to accelerate the pose estimation process of the appearance models a possible solution is to use the tracking results of the appearance models as the initialization of the appearance models how ever how to effectively transfer from tracking to tracking is still an unsolved problem intelligent vision models inspired by the biological vision a number of high level salient region features are proposed to capture the salient semantic information of an input image these salient region features are relatively stable during the process of tracking while they rely heavily on salient region detection which may be affected by noise or drastic illumination variation unreliable saliency detection leads to many feature mismatches across frames consequently it is necessary to build an intelligent vision model that can robustly track these salient region features across frames like what human vision offers camera network tracking typically the appearance models are based on a single camera which provides a very limited visual information of the tracked objects in recent years several appearance models using multiple overlapping cameras networks are a natural way to represent social biological technological and information systems nodes in such networks organize into densely linked groups that are commonly referred to as network communities clusters or modules there are many reasons why nodes in networks organize into densely linked clusters for example society is organized into social groups families villages and associations on the world wide web topically related pages link more densely among themselves and in metabolic networks densely linked clusters of nodes are related to functional units such as pathways and cycles in community detection one aims to identify sets of nodes that correspond to communities one way to formalize the process of community detection is to think of a scoring function that quantifies how much the connectivity pattern of a given set of nodes resembles the connectivity structure of a network community most scoring functions like modularity 25 and conductance are based on the intuition that sets of nodes that have many connections between its members correspond to communities once the scoring function is defined then one applies a procedure to find sets of nodes with high score such sets of nodes are then extracted and referred to as network communities identifying such communities in networks 7 16 29 has proven to be a challenging task due to several reasons there exist a plethora of definitions scoring func tions and methods for extracting network communities even if we would agree on a single common structural definition i e a single scoring function the algorithmic for malizations of community detection lead to np hard problems 29 and the lack of reliable ground truth makes the evaluation of extracted communities and comparison of algorithms extremely difficult currently the performance of community detection methods is often evaluated by manual inspection for each detected community an effort is made to interpret it as a real community by identifying a common property or external attribute shared by all the members of the com munity for example when examining communities in a scientific collaboration network we might by manual inspection discover that many of detected communities correspond to groups of scientists working in common areas of science 25 such anecdotal evaluation procedures require extensive manual effort are non comprehensive and limited to small networks a possible solution to this problem would be to find a reliable definition of explic itly labeled gold standard ground truth communities using such ground truth communities would allow for quantitative and large scale evaluation and comparison of network com munity detection methods such ability would enable the field to move beyond the current standard of anecdotal evaluation of communities to a comprehensive evaluation of commu nity detection methods based on their performance to extract the ground truth furthermore it would allow for the development of new community detection methods and improve the understanding of how communities manifest themselves in networks present work structure and function in this paper we define a robust notion of ground truth communities we achieve this by distinguishing between structural and functional definitions of communities we argue that defining and evaluating network communities the goal of network community detection is to extract functional communities based on the connectivity structure of the nodes in the network we identify networks with explicitly labeled functional communities and then present a methodology that allows us to evaluate different structural definitions of communities generally after some community detection algorithm identifies communities based on the network structure the essential next step is to interpret the communities by identifying a common external property or a function that the members of a given community share and around which the community organizes for example given a protein protein interaction network of a cell one first identifies communities based on the structure of the network and then examines that these communities correspond to real functional units of a cell thus the goal of community detection is to identify sets of nodes with a common often external latent unobserved function based on the connectivity structure of the network in this context a common function can be a common role affiliation or attribute in our protein interaction network example above such common function of nodes would be belonging to the same functional unit or in a social network common function would be belonging to the same social circle thus community detection methods identify communities based on the network struc ture while the detected communities are then evaluated based on their function thus we distinguish between structural and functional definitions of communities structural defin itions are based on the structure of the connectivity between a set of nodes e g a set of nodes with high modularity score 25 on the other hand functional definitions of network communities are based on common function or role that the community members share e g proteins of the same functional unit generally the basic premise behind the network com munity detection is that functional communities have distinct structural patterns and thus one may be able to identify them based on the network structure present work networks with ground truth communities our goal here is to obtain high quality labels of ground truth communities so that we can then devise a methodology to compare and evaluate various structural definitions of network communities while explicitly labeled structural communities are nearly impossible to obtain our main insight here is that there exist networks where functional communities are explicitly declared in the data thus we use sets of nodes with a common function to define ground truth communities we gathered networks from a number of different domains and research areas where nodes explicitly state their ground truth functional community memberships the size of the networks ranges from hundreds of thousand to hundreds of millions of nodes and edges the networks represent a wide range of edge densities numbers of explicitly defined communities as well as sizes and amounts of community overlap our collection consists of social collaboration and information networks for each of which we find a robust functional definition of ground truth for example in online social net works like orkut livejournal and friendster we consider explicitly defined interest based groups e g fans of pop singer lady gaga students of the same school as ground truth func tional communities nodes in these networks explicitly join such groups that organize around specific topics interests and affiliations we also consider the product co purchasing network from amazon where we define ground truth using hierarchically nested product categories here all members i e products of the same ground truth community share a common function or purpose last in the scientific collaboration network of dblp we use j yang j leskovec publication venues as proxies for ground truth research communities our reasoning here is that in scientific collaboration networks real communities would correspond to areas of science thus we use journals and conferences as proxies for heavily overlapping scientific communities present work methodology and findings the availability of ground truth allows us to examine how well various structural definitions of network communities correspond to functional communities i e ground truth commu nities a good structural definition of a community would be such that it would correspond to connectivity patterns that correspond to functional communities our experiments show a clear connection between functional and structural definitions we show that functional communities exhibit distinct connectivity patterns this means that we can evaluate different structural definitions based on their ability to identify ground truth communities we study commonly used structural definitions of communities and examine their quality sensitivity and robustness each such definition corresponds to a scoring function that scores a given set of nodes how community like it is i e a scoring function assigns high score to sets of nodes that closely resemble functional communities by comparing correlations of scores that different structural definitions assign to ground truth communi ties we find that the definitions naturally group into four distinct classes these classes correspond to definitions that consider only internal community connectivity only external connectivity of the nodes to the rest of the network both internal and external community connectivity and network modularity we then consider an axiomatic approach and define four intuitive properties that communi ties would ideally have intuitively a good community is cohesive compact and internally well connected while being also well separated from the rest of the network this allows us to characterize which connectivity patterns a given structural definition detects and which ones it misses next we also investigate the robustness of community scoring functions based on four types of randomized perturbation strategies overall evaluation shows that among the scoring functions considered here those that are based on triadic closure and the conductance score best capture the structure of ground truth communities last we also investigate a task of discovering all members of a community given a single member node we extend the local spectral clustering algorithm into a parameter free community detection method that scales to networks of hundreds of millions of nodes our method recovers ground truth communities with relative improvement in the score over the current local graph partitioning methods to the best of our knowledge our work is the first to use social and information networks with explicit community memberships to define an evaluation methodology for comparing network community detection methods based on their accuracy on real data we believe that the present work will bring more rigor to the standard for the evaluation of community detection methods all our datasets can be downloaded at http snap stanford edu the rest of the paper is organized as follows section describes the datasets and defines the notions of ground truth communities in each dataset section shows the distribution of the properties of ground truth communities and the structural characteristics of ground truth communities section describes the structural definitions of communities that we consider in this paper and discusses the relationship among the definitions in sect we evaluate the structural definitions of communities on two aspects first we study what connectivity patterns various definitions prefer and which they penalize second we evaluate the robust ness of community structure by using a set of randomized community perturbation strategies defining and evaluating network communities section considers the problem of identifying ground truth communities from seed nodes section 7 discusses related work we conclude in sect last we also note that a shorter version of this paper appeared at the ieee international conference on data mining icdm 38 ground truth communities we begin by explaining the intuition behind the definition the ground truth communities we distinguish between structural and functional definitions of communities a structural definition of communities is a set of nodes with a particular connectivity structure e g set of nodes with high edge density or set of nodes with high modularity score a functional definition of communities is a set of nodes with a common function which can be common role affiliation or attribute with these two definitions of communities community detection process generally fol lows a two step procedure first one discovers communities based on a structural definition and then one argues that the discovered communities correspond to functional communi ties for example palla et al identified structural communities by identifying sets of overlapping k cliques on protein protein interaction networks then they found that these structurally defined communities of proteins correspond to functional modules of proteins in this example communities are extracted based on the structural definition and then evaluated based on the functional definition by arguing that belonging to the same functional module is the common function of nodes an issue with this approach is that it is ad hoc and that the evaluation of extracted structural communities is manual each extracted community has to be manually inspected our approach takes the different direction we first identify large scale datasets where functional communities are already labeled and then we evaluate community detection meth ods based on their ability to extract ground truth functional communities overall we consider large social collaboration and information networks where for each network we have a graph and a set of functionally defined ground truth communities members of these ground truth communities share a common function property or purpose networks that we study come from a wide range of domains and sizes table lists the networks and their properties ground truth communities in social networks first we consider three online social networks the livejournal blogging community the friendster online network and the orkut social network in these networks users table social collaboration and information networks with explicit ground truth communities dataset n e c s a livejournal 0m 34 9m 40 friendster 72 32 orkut 0m 34 86 95 ning nets 7 0m 5m 46 89 92 amazon 92m 49 732 99 86 14 83 dblp 79 56 n number of nodes e number of edges c number of communities s average community size and a community memberships per node ning statistics are aggregated over different subnetworks j yang j leskovec create explicit functional groups to which other users then join such groups serve as orga nizing principles of nodes in social networks groups range from small to very large and are created based on specific topics interests hobbies and geographical regions for example livejournal categorizes communities into the following types culture entertainment expres sion fandom life style life support gaming sports student life and technology there are over communities in livejournal with stanford in their name and they range from communities based on different classes student ethnic communities departments activity and interest based groups varsity teams etc overall there are over three hundred thousand explicitly defined communities in livejournal similarly users in friendster as well as in orkut define topic based communities that others then join both networks have more than a million explicitly defined groups and each user can join to one or more such groups we consider each group as a ground truth community last we have a set of different online social networks that are all hosted by the ning platform it is important to note that each ning network is a separate social network an independent web site with a separate user community for example the nba team dallas mavericks and diabetes patients network tudiabetes all use ning to host their separate online social networks after joining a specific network users then create and join groups for example in tudiabetes ning network groups form around specific types of diabetes parenting children with diabetes different geographical regions age groups and similar note that these are exactly the properties around which we expect communities to form in a network of diabetes patients again we use such explicitly defined functional groups as ground truth communities as we see in table ground truth communities in social networks are quite diverse for example communities in friendster are about twice smaller than communities in ning or livejournal communities in orkut overlap heavily as people are members of many communities at the same time while for example in friendster most nodes do not belong to any community ground truth communities in product networks the second type of a network we consider is the amazon product co purchasing network the nodes of the network represent products and edges link commonly co purchased prod ucts each product i e node belongs to one or more hierarchically organized product cate gories and products from the same category define a group which we view as a ground truth community note that here the definition of ground truth is somewhat different in this case nodes that belong to a common ground truth community share a common function or purpose ground truth communities in product networks table are larger than in social net works and include around nodes on the average given the hierarchical categorization of products we also note that an average product belongs to 14 categories i e ground truth communities ground truth communities in collaboration networks finally we also consider the dblp scientific collaboration network where nodes repre sent authors and edges connect authors that have co authored a paper to define ground truth in this setting we reason as follows functional communities in a scientific domain cor respond to people working in common areas and subareas of science however note that publication venues serve as good proxies for scientific areas people publishing in the same defining and evaluating network communities conference form a scientific community thus we use publication venues i e conferences journals as ground truth communities which serve as proxies for highly overlapping scien tific communities around which the collaboration network then organizes ground truth communities in the dblp network table are the largest and moderately overlap with nodes being part of about different communities on the average to conclude we note that all our networks and the corresponding ground truths are com plete and publicly available at http snap stanford edu for each of these networks we iden tified a sensible way of defining ground truth communities that serve as organizational units of these networks we were careful to define ground truth communities based on common affiliation social circle role activity interest function or some other property around which networks organize into communities 14 even though our networks come from very dif ferent domains and have very different motivation for the formation of communities the results we present here are consistent and robust our work is consistent with the premise that is implicit in all network community literature members of real communities share some latent unobserved property or affiliation that serves as an organizing principle of the nodes and makes them well connected in the network here we use these groups around which communities organize to explicitly define ground truth and as we will later see the ground truth communities exhibit connectivity patterns that match our intuition of communities as densely connected sets of nodes data preprocessing to represent all networks in a consistent way we consider each network as an unweighted undirected static graph because members of the group may be disconnected in the network we consider each connected component of the group as a separate ground truth community however we allow ground truth communities to be nested and to overlap i e nodes can be members of multiple communities at once characteristics of ground truth communities in this section we examine properties of ground truth communities first we study size and overlap distributions of communities and then proceed to examine finer structural properties of ground truth communities global properties of ground truth communities we start by analyzing the distribution of the properties of ground truth communities figure gives the distributions complementary cdf of community sizes which are the number of the nodes in the communities notice that all distributions are heavily skewed with most ground truth communities being small while large communities also exist for example largest social communities contain between one and ten thousand people while product communities can be even larger to get a sense of how much communities overlap we also examine how many communities a node belong to figure plots the distribution of the number of community memberships that a node belongs to again we observe heavy tails with most nodes belonging to only a small number of communities and few nodes belonging to many we further examine the properties of community overlaps we focus on characterizing the overlap between pairs of ground truth communities we show the distribution of the fig node membership distribution complementary cumulative distribution function of the node mem berships the number of communities nodes belong to absolute overlap sizes the number of the nodes in the overlap in fig we observe that the distributions follow a power law as also observed by palla et al on detected rather than ground truth communities last we also study on the relative size of community overlaps relative sizes of over laps are of our interest as they can characterize how ground truth communities overlap do ground truth communities overlap in a nested structure or do they overlap only for a small fraction of members we measure the fraction f of the size of the overlap a b between two fig relative size of community overlaps histogram of the fraction of the relative overlap size when ground truth communities a b overlap a b then the fraction of the relative overlap size is a b min a b where min x y is the smaller of x and y communities a b to the size of the smaller community min a b f a b min a b if the fraction of overlap is close to the network has a nested structure where the smaller community is included by the larger community on the other hand f being close to means that most communities are non overlapping we plot the histogram of the overlap fraction in fig the amazon network shows high probability at f because the ground truth communities form a nested structure by construction in social networks and j yang j leskovec the dblp network most overlaps take a small fraction of individual communities which is reasonable as each community has its own special interests structural characteristics of ground truth communities in this subsection we show that ground truth communities that we defined have distinct connectivity properties we show that our ground truth communities which are sets of nodes sharing common functions or properties i e functional communities also exhibit distinctive structural properties the experiments confirm the premise that the functional communities exhibit distinct structural connectivity patterns and can thus be discovered based on the network connectivity structure we compare structural properties of a ground truth community c i to those of a set of nodes that do not form a ground truth community with the goal to establish how ground truth communities structurally differ from non communities for each ground truth community c i we sample a non community c i make our experiments realistic we a set of nodes outside c add three constraints to c i i to which we compare c i to c i has the same number of nodes as c i c i members is connected of c i have the same distribution of shortest path distances as c i the last constraint is an approximation for the ideal that we want c i and c i to have simi lar compactness or connectedness to achieve these constraints we proceed as follows we take a node u c i uniformly at random and compute the distance histogram h u k v c i from u k d u v k that is the number of other member nodes who are k hop away then we pick u c i from which we generate c i by adding h u k nodes c i from the k hop neighbors of u for example if h u h u h u contains u neighbors of u and hop neighbors of u at the same choose the nodes that are connected to at least one of the other members of time we only the connectedness of c i c i to guarantee we then measure or c i that has n s the structural properties of c i and c i for a set of nodes s s c i member nodes and m s edges among its member nodes we measure the following clustering coefficient ccf is the average clustering coefficient between the member nodes of s average degree avgdeg is the average number of node degree to other member nodes s n s edge density density is the fraction of pairs of member nodes that have an edge s n s n s cohesiveness captures the intuition that a good community should be internally well and evenly connected i e it should be relatively hard to split a community into two subcom munities we capture this intuition by defining cohesiveness as the conductance of the internal cut max s s φ s where φ s is the conductance of s measured in the induced subgraph by s we will precisely define conductance later but intuitively the more cohesive the community the more edges have to be cut in order to further split the community and thus the higher the conductance score of the internal cut table shows the ratio between the average value of c i and that of c i for the proper ties we observe that groups show higher clustering coefficient higher average degree 39 higher edge density and higher cohesiveness than sets of randomly chosen defining and evaluating network communities table comparison between ground truth communities functional communities and sets of randomly chosen connected nodes dataset ccf avgdeg density cohesiveness livejournal 18 79 54 68 friendster 82 61 24 orkut 47 45 ning 48 39 amazon 06 29 25 66 dblp 09 06 average 18 51 39 02 ratio between the community scores of a ground truth community over those of a set of connected nodes with the same size and the same distance distribution ccf clustering coefficient avgdeg average degree of a node to other member nodes density the fraction of pairs of members that have an edge cohesiveness the highest conductance among possible internal cuts values higher than mean that ground truth communities achieve higher score than corresponding non community sets of nodes connected sets of nodes this shows that the members of functional communities tend to be more cohesively and densely connected and thus exhibit distinct connectivity patterns community scoring functions in community detection one aims to identify sets of nodes that correspond to communities one way to formalize this process is to design ascoring functionthat for a set of nodes outputs a quality score that characterizes how much the connectivity structure of a given set of nodes resembles that of a community the idea then is that given a community scoring function one can then find sets of nodes with high score and consider these sets as communities in practice nearly all scoring functions build on the intuition that communities are sets of nodes with many connections between the members and few connections from the members to the rest of the network there are many possible ways to mathematically formalize this intuition we gather commonly used scoring functions or equivalently structural definitions of network communities some scoring functions are well known in the literature while others are proposed here for the first time given a set of nodes s we consider a function f s that characterizes the community quality of a given set of nodes s let g v e be an undirected graph with n v nodes and m e edges let s be the set of nodes where n s is the number of nodes in s n s s m s the number of edges in s m s u v e u s v s c s the number of edges on the boundary of s c s u v e u s v s and d u is the degree of node u we consider scoring functions f s that capture the notion of quality of a network community s the experiments we will present later reveal that scoring functions naturally group into the following four classes a scoring functions based on internal connectivity internal density f s n s m s is the internal edge density of the node set s edges inside f s m s n s is the number of edges between the members of s average degree f s fraction over median degree n s s fomd is the average internal degree of the members of s j yang j leskovec f s higher than u u s u v v s d d m where n s m is the fraction of nodes of d m is the median value of d u in v s that have internal degree triangle participation ratio tpr f s u u s v w v w s u v e u w e v w e belong to a triad n s is the fraction of nodes in s that b scoring functions based on external connectivity expansion measures the number of edges per node that point outside the cluster f s n c s s cut ratio is the fraction of existing edges out of all possible edges leaving the cluster f s n s n n c s s c scoring functions that combine internal and external connectivity conductance f s outside the cluster c s s measures the fraction of total edge volume that points normalized cut f s c s c s s m m c s s 31 maximum odf out degree fraction f s max u s c s c s u v e v outside s d u s is the maximum fraction of edges of a node in s that point average odf f s n s u v e v u s d u s is the average fraction of edges of nodes in s that point out of s flake odf f s u u s u v e v s d u n s is the fraction of nodes in s that have fewer edges pointing inside than to the outside of the cluster d scoring function based on a network model modularity f s between nodes in s and m e m s e m s the s expected is the difference number between of m s the number of edges such edges in a random graph with identical degree sequence 24 experimental result four classes of scoring functions we examine relationship of the community scoring functions we introduced for each of the ground truth communities in our networks we compute a score using each of the scoring functions we then create a correlation matrix of scoring functions and threshold it figure shows connections between scoring functions with correlation on the livejournal network we observe that scores naturally group into four clusters this means that scoring func tions of the same cluster return heavily correlated values and quantify the same aspect of connectivity structure overall none of the scoring functions are negatively correlated which means that none of them systematically disagree interestingly modularity is not correlated with any other scoring function average degree is the most correlated at correlation we observe very similar results in all other network datasets that we considered in this study the experiment demonstrates that even though many different structural definitions of communities have been proposed these definitions are heavily correlated essentially there are only different structural notions of network communities as revealed by fig for brevity in the rest of the paper we present results for representative scoring functions defining and evaluating network communities fig correlations of community scoring functions two scoring functions are connected by an edge if the values output by scoring functions are correlated with correlation coefficient notice four distinct classes of scoring functions denoted as blue nodes in fig from the two large clusters and from the two small clusters we also note that here we computed the values of the scores on ground truth communi ties in reality the aim of community detection is to find sets of nodes that maximize a given scoring function exact maximization of these functions is typically np hard and leads to its own set of interesting problems refer to for discussion evaluation of community scoring functions the second main purpose of our paper is to develop an evaluation methodology for network community detection based on ground truth communities we now aim to compare and eval uate different community scoring functions we focus on two aspects of community scoring functions how well the community scoring function can detect communities sect and how robust the community scoring function is to noise in network structure as well as node labeling sect 2 detecting communities our goal is to rank different structural definitions of network communities i e community scoring functions by their ability to detect ground truth communities we adopt the following axiomatic approach first we define four community goodness metrics that formalize the intuition that good communities are both compact and well connected internally while being relatively well separated from the rest of the network the difference between community scoring functions from sect and the goodness metrics defined above is that a community scoring function quantifies how community like a set is while a goodness metric in an axiomatic way quantifies a desirable property of a community a set with high goodness metric does not necessarily correspond to a community but a set with high community score should have a high value on one or more goodness metrics in other words the goodness metrics shed light on various in many cases mutually exclusive aspects of the network community structure j yang j leskovec using the notation from sect we define four goodness metrics g s for a node set s separability captures the intuition that good communities are well separated from the rest of the network 31 meaning that they have relatively few edges pointing from set s to the rest of the network separability measures the ratio between the internal and the external density builds number on of intuition edges of that s g s good communities m c s s are well connected it measures the fraction of the edges out of all possible edges that appear between the nodes in s g s cohesiveness n s n m s 2 s characterizes the internal structure of the community intuitively a good community should be internally well and evenly connected i e it should be relatively hard to split a community into two subcommunities we characterize this by the conductance of the internal cut formally g s max s s φ s where φ s is the conductance of s measured in the induced subgraph by s conductance essentially corresponds to the ratio of the edges in s that point outside the set and the edges inside the set s a good community should have high cohesiveness high internal conductance as it should require deleting many edges before the community would be internally split into disconnected components clustering coefficient is based on the premise that network communities are manifestations of locally inhomogeneous distributions of edges because pairs of nodes with common neighbors are more likely to be connected with each other experimental setup we are interested in quantifying how good are the communities chosen by a particular scoring function f s by evaluating their goodness metric we formulate our experiments as follows for each of networks we have a set of ground truth communities s i for each community scoring function f s we rank the ground truth communities by the decreasing score f s i we measure the cumulative running average value of the goodness metric g s of the top k ground truth communities under the ordering induced by f s i the intuition for the experiments is the following a perfect community scoring function would rank the communities in the decreasing order of the goodness metric and thus the cumulative running average of the goodness metric would decrease monotonically with k whereas if a hypothetical community scoring function would randomly rank the communi ties then the cumulative running average would be a constant function of k 2 experimental results we found qualitatively similar results on all our datasets in this section we only present results for the livejournal network results are representative for all other networks we point the reader to the appendix of the paper for a complete set of results figs and figure shows the results by plotting the cumulative running average of separability score for livejournal ground truth communities ranked by each of the six community scoring functions curve u presents the upper bound i e it plots the cumulative running average of separability when ground truth communities are ordered by decreasing separability if the scoring function would order communities exactly by their value of the goodness score then optimal curve u would be achieved we observe that conductance c and cut ratio cr give near optimal performance i e they nearly perfectly order the ground truth communities by separability on the other hand clustering coefficient fig cumulative average of goodness metrics for livejournal communities ranked by each of the six representative scoring functions c conductance and t tpr with high and monotonically decreasing values perform best we observe triad participation ratio t and modularity m score ground truth communities in the inverse order of separability especially for k which means that they both prefer densely linked sets of nodes similarly fig c and d shows the cumulative running average of community density cohesiveness and clustering coefficient we observe that all scoring functions except modu larity rank denser more cohesive and more clustered ground truth communities higher for the density metric the fraction over median degree d score performs best for high values of k followed by conductance c and flake odf f in terms of cohesiveness and clustering coefficient the triad participation ratio t score gives by far the best results in all cases the only exception is the modularity which ranks the communities in nearly reverse order of the goodness metric the cumulative running average increases as a function of k we note that these are all well known issues of modularity but they get further attenuated when tested on ground truth communities the curves in fig illustrate the ability of the scoring functions to rank communities for the livejournal communities to further quantify this we perform the following experiment for a given goodness metric g and for each scoring function f we measure the rank of each scoring function in comparison with other scoring functions at every value of k for example in fig the rank at k 100 of conductance is cut ratio 2 flake odf fomd modularity and tpr for every k we rank the scores and compute the average rank over j yang j leskovec table average scoring function rank for each goodness metric scoring function separability density cohesiveness clustering conductance c flake odf f fomd d 2 9 2 9 tpr t 2 2 2 modularity m 4 5 5 7 9 cut ratio cr 2 2 5 5 conductance gives the highest separability while triad participation ratio tpr scores best on the remaining metrics best performing scores are bolded all values of k which quantifies the ability of the scoring function to identify communities with high goodness metric table shows the average rank for each score and each goodness metric an average rank of means that a particular score always outperforms other scores while rank of means that the score gives worst ranking out of all scores we observe that conductance c per forms best in terms of separability but relatively bad in the other three metrics for density cohesiveness and clustering coefficient triad participation ratio t is the best perhaps not surprisingly triad participation ratio scores badly on separability of ground truth communi ties thus conductance is able to identify well separated communities but performs poorly in identifying dense and cohesive sets of nodes with high clustering coefficient on the other hand triad participation ratio gives the worst performance in terms of separability but scores the best for the other three metrics we conclude that depending on the network different structural notions of network com munities might be appropriate when the network contains well separated non overlapping communities conductance is the best scoring function when the network contains dense heavily overlapping communities then the triad participation ratio defines the most appropri ate notion of a community further research is needed to identify most appropriate structural definitions of communities for various types of networks and functional communities for example in social networks we have both identity based and bond based communities 28 and they may in fact have different structural signatures figures and in the appendix show the results for all the networks where we find similar trends interestingly in figs and we also observe that the average goodness metric of the top k communities remains flat but then quickly degrades we observe the same pattern in all our datasets thus for the remainder of the paper we focus our attention on a set of the top 5 000 communities of each network based on the average rank over the scores 5 2 robustness to perturbation in this subsection we evaluate community scoring functions using a set of perturbation strategies we develop a set of strategies to generate randomized perturbations of ground truth communities which allows us to investigate robustness and sensitivity of community scoring functions intuitively a good community scoring function should be such that it is stable under small perturbations of the ground truth community but degrades quickly with larger perturbations our reasoning is as follows we desire a community scoring function that scores well when evaluated on a ground truth community but scores low when evaluated on a perturbed defining and evaluating network communities community in other words an ideal community scoring function should give a maximal value when evaluated on the ground truth community if we consider a slightly perturbed ground truth community i e a node set that differs very slightly from the ground truth community we would want the score to be nearly as good as the score of the original ground truth community this would mean that the scoring function is robust to noise however if the ground truth community is perturbed so much that it resembles a random set of nodes then a good scoring function should give it a low score 5 2 community perturbation strategies we proceed by defining a set of community perturbation strategies to vary the amount of perturbation each perturbation strategy has a single parameter p that controls the intensity of the perturbation given p and a ground truth community defined by its members s the com munity perturbation starts with s and then modifies it i e changes its members by executing the perturbation strategy p s times we define the following perturbation strategies nodeswap perturbation is based on the mechanism where the community memberships diffuse from the original community through the network we achieve this by picking a random edge u v where u s and v s and then swap the memberships i e remove u from s and add v note that nodeswap preserves the size of s but if v is not connected to the nodes in s then nodeswap makes s disconnected random takes community members and replaces them with random non members we pick a random node u s and a random v s and then swap the memberships like nodeswap random maintains the size of s but may disconnect s generally random will degrade the quality of s faster than nodeswap since nodeswap only affects the fringe of the community expand perturbation grows the membership set s by expanding it at the boundary we pick a random edge u v where u s and v s and add v to s adding v to s will generally decrease the quality of the community expand preserves the connectedness of s but increases the size of s shrink removes members from the community boundary we pick a random edge u v where u s v s and remove u from s shrink will decrease the quality of s and result in a smaller community while preserving connectedness for a given s let h s p denote a perturbed version of the community generated by the perturbation h of intensity p we now quantify the difference of the score between the unperturbed ground truth com munity and its perturbation we use the z score which measures in the units of standard deviation how much the scoring function changes as a function of perturbation intensity p for ground truth community s i the z score z f h p of community scoring function f under perturbation strategy h with intensity p is as follows z f h p e i f var s i i f h s i p where e i f h s i p var i are the mean and the variance over communities s i and f h s i p is the community score of perturbed s i under perturbation h with intensity p to measure f h s i p we run trials of h s i p and compute the average value of f z score is the difference between the average community score of true communities f s i and the average community scores of perturbed communities f h s i p normalized by the standard deviation of community scores of perturbed communities since f h s i p are independent j yang j leskovec for each i e i f h s i p follows a normal distribution by the central limit theorem thus p z z f h p gives the probability that e i f h s i p e i f s i where z is a standard normal random variable we measure f so that lower values mean better communities i e we add a negative sign to tpr modularity and fomd high z scores mean that e i f s i is likely to be smaller than e i f h s i p and that s i is better than h s i p in terms of f 5 2 2 experimental results for each of the community scoring functions we measure z score for perturbation intensity p ranging between and this means that we randomly swap between and 60 of the community members and measure the z score for each scoring function for small p small z scores are desirable since they indicate that the scoring function is robust to noise for high perturbation intensities p high z scores are preferred because this suggests that the community scoring function is sensitive i e as the community becomes more random we want the scoring function to significantly increase its value figure 7 shows the z scores as a function of perturbation intensity p in the livejournal online social network we plot the z score for each of the community scoring functions as expected the z scores increase with p which means that as the community gets more 4 e r c 2 m t d f 2 4 5 cr 1 2 4 5 odeswap c m t o c z 1 e d f cr perturbation intensity perturbation intensity andom 2 r o c z 4 4 2 2 a n b r 1 2 c t 1 2 4 5 d shrink fig 7 z scores as a function of the perturbation intensity c conductance and t triad participation ratio best detect the perturbations of livejournal ground truth communities 2 5 1 m f cr d 1 2 4 5 c e xpand 2 c t e r o c z m f 6 cr d 4 e r o c z 1 5 1 2 5 perturbation intensity perturbation intensity defining and evaluating network communities perturbed the value of the score tends to decrease the faster the increase the more sensitive and thus the better the score for example in livejournal under the nodeswap perturbation conductance c exhibits the highest z score after p 2 and it has the steepest curve triad participation ratio t also exhibits desirable behavior on the other hand modularity m score does not change much as we perturb the ground truth communities this means that modularity is not good at distinguishing true communities from randomized sets of nodes figure 12 and in the appendix give the same plot for all other networks we observe similar results interestingly modularity m fails to achieve increasing z score as a function of perturba tion intensity p in all the networks except the ning networks we note that the key difference in ning is the size of networks ning networks contain 3 000 nodes in average whereas the other 5 networks contain at least almost a million nodes thus our results show that while modularity works well in distinguishing true communities in small networks it fails in large networks interestingly this observation is consistent with the theoretical limitation of modularity known as the resolution limit fortunato and barthélemy proved that modularity is unable to distinguish communities when the network is too large compared to community sizes and this explains the failure of modularity for 5 large networks other than ning since the size of communities is generally less than 100 regardless of the network size table 1 and reference 20 most communities are too small for modularity in these large scale networks 5 2 3 sensitivity of community scoring functions we also quantify the sensitivity of community scoring functions by computing the increase of the z score between small p and large perturbations p 2 as noted above we prefer a community scoring function with fast increase of the z score as the community perturbation intensity increases table 4 displays the difference of the z score between a large and a small perturbation z f h 2 z f h we compute the average increment across all the networks a high value of increment means that the score is both robust and sensitive the score is robust because at small perturbation p 05 it maintains low z value while at large perturbation p 2 it has high z value and thus the overall z score increment is high conductance is the most robust score under nodeswap and shrink the triad participa tion ratio t is the most robust under random and expand in both cases conductance fol lows triad participation ratio closely we note that the clique percolation method cpm 26 which is the state of the art overlapping community detection method implicitly optimizes table 4 average absolute increment of the z score between small and large community perturbations scoring function nodeswap random expand shrink conductance c 1 06 1 59 50 45 flake odf f 51 1 41 fomd d 18 57 19 12 tpr t 1 85 74 modularity m 14 15 cut ratio cr 53 83 43 best performing scores are bolded j yang j leskovec the triad participation ratio cpm will add a node to a community only if the node forms a clique i e a triangle in the community and the triad participation ratio of the detected communities will be 1 5 2 4 bias of scoring functions the experiments so far revealed surprisingly large differences in the robustness of different community scoring functions interestingly we also observed that modularity prefers large communities score increases under expand to further investigate the bias of different community scoring functions on the size of the underlying network community we perform the following experiment we measure how the z score changes as a function of the size of the community while keeping perturbation intensity constant in particular we calculate the z score of each ground truth community s i at perturbation intensity level p 2 and plot it as a function of the community size s i figure 8 shows the results for the livejournal communities since p 2 represents relatively large perturbation high z scores are desirable we observe that under nodeswap conductance is the most robust score and that as the community size increases robustness of conductance slightly decreases for random and expand the triad participation ratio score performs best over the whole range of network community sizes generally best performing scores tend to be more sensitive on small communities the exception is modularity the results for modularity are consistent with the resolution limit mentioned previously the z score of modularity is very close to for communities smaller than 100 members i e modularity cannot distinguish a community and a perturbed community when the community is smaller than around 100 nodes due to resolution limit however for large communities modularity score shows high z scores except under the expand perturbation where it favors larger null communities in most networks we note that we find similar results in other networks figs 14 and 15 in the appendix 6 discovering communities from a seed node now we focus on the task of inferring communities given a single seed node we consider two tasks that build on two different viewpoints the first task is motivated by a community centric view where we discover all members of community s given a single member s the second task is motivated by a node centric view where we want to discover all communities that a single node belongs to this means we discover both the number of communities belongs to and the members of these communities 6 1 proposed method we extend the local spectral clustering algorithm 4 32 into a scalable parameter free com munity detection method the benefits of our method are first the method requires no input parameters and is able to automatically detect the number of communities as well as the members of those communities second the computational cost of our method is propor tional to the size of the detected community not the size of the network thus our method is scalable to networks with hundreds of millions of nodes our method algorithm 1 builds on the findings in sect 5 first we aim to find sets of well connected nodes around node we achieve this by defining a local partitioning method based on random walks starting from a single seed node 3 in particular we use the pagerank nibble random walk method that computes the pagerank vector with error ε in time o 1 ε independent of the network size 4 the nodes with high pagerank scores from correspond to the well connected nodes around moreover the random walk is truncated as it sets pagerank scores r u to for nodes u with r u ε for some small j yang j leskovec fig 9 two community scoring functions f conductance and f triad participation ratio evaluated on a set s k of top k nodes with highest random walk proximity score to seed node 1 local correspond optima to of detected f s k communities denoted by stars for conductance and a square for 2 f tpr f f s k f s k constant ε 3 this way the computational cost is proportional to the size of the detected community and not the size of the network after the pagerank nibble assigns the proximity scoresr u 3 1 10 2 10 3 k we sort the nodes in decreasing proximity r u and proceed to the second step of our algorithm which extends the approach of spielman and teng 32 we evaluate the community score on a set s k of all the nodes up to kth one note that by construction s k 1 s k this means that for a chosen community scoring function f we compute f s k of the set s k that is composed of the top k nodes with the highest random walk score r u the local minima of the function f s k then correspond to extracted communities we detect local minima of we measure f s k at some f point s k using k the following heuristic for increasingk 1 2 candidate point for a local minimum f s k will if f s k stop decreasing and this k becomes our keeps increasing after k and eventually becomes higher than down again before it αf s k we take reaches αf s k k as a valid local minimum however if we discard the candidate k we experimented f s k goes with several values of α and found that α 1 2 gives good results across all the datasets for example fig 9 plots f s k for two community scoring functions f conductance and f triad participation ratio we identify the local optima denoted by stars and squares and use the nodes in the corresponding sets s k as the detected communities note that our method can detect multiple communities that the seed node belongs to by identifying different local minima of f s k however we assume that the communities are nested smaller communities are contained in the larger ones even though the ground truth communities may not necessarily follow such a nested structure also note that our method is parameter free our method differs from local graph clustering approaches 3 32 in two aspects first instead of sweeping only using conductance we consider sweeping using other scoring functions second we find the local optima of the sweep curve instead of the global optimum this change gives a large improvement over the conventional local spectral clustering approaches 3 32 6 2 detecting a community from a single member we first consider the task where we aim to reconstruct a single ground truth community s based on one member node for each community s we pick a random member node as a seed node and compare the community we detect from with the ground truth community s starting from node we generate a sweep curve f s k let k be the value of k where ________________ defining and evaluating network communities table 5 performance of our 6 methods and 2 baselines lc cpm at detecting communities from a seed node score c f d t m cr lc cpm lj 64 64 62 57 15 61 54 43 fs 23 22 24 25 24 18 13 14 orkut 19 19 18 20 09 20 13 ning 24 19 10 19 08 19 17 amazon 87 75 73 79 06 85 74 85 dblp 61 61 65 66 04 61 46 53 avg 46 43 42 44 13 42 37 36 avg prec 50 53 52 55 13 53 49 38 avg rec 60 47 51 47 71 49 65 69 best performing scores are bolded f s k achieves the first local minima we then use the set s k as the detected community now given the ground truth community s and the detected community s k we evaluate how well detected s k corresponds to ground truth s for this purpose we compute precision recall and the score between s k and s for all metrics the score of 1 means that detected s k perfectly matches ground truth s we consider 6 community scoring functions f and compare the performance of our method to two standard community detection methods local spectral clustering lc 3 and the 3 clique clique percolation method cpm 26 table 5 shows the performance of the proposed method for each scoring function and for the two baselines first 5 rows show the score for each of the datasets and the last 3 rows show the average score precision and recall over all the datasets we observe that the conductance c gives the best average score and outperforms all other scores on livejournal lj orkut amazon and ning for friendster fs and dblp the triad participation ratio t performs best this agrees with our intuition that for networks like livejournal that have fewer community overlaps scoring functions that focus on good sepa rability perform well in networks where nodes belong to multiple communities like dblp where authors publish at multiple venues the triad participation ratio t performs best we also note that the average score of conductance is 46 while the baselines cpm and lc achieve score of only 36 and 37 respectively note this is 10 absolute and 30 relative improvement over the baselines last we observe that some methods detect larger communities than necessary higher recall lower precision modularity m most severely overestimates community size con ductance c and both baselines cr and cpm exhibit similar behavior but to a lesser extent on the contrary flake odf f fraction over median d triad participation ratio t and cut ratio cr tend to underestimate the community size higher precision than recall 6 3 detecting all communities that a seed node belongs to we also explore the second task where we want to detect all the communities to which a given seed node belongs in this task we are given a node that is a member of multiple communities but we do not know which and how many communities belongs to we detect multiple communities by detecting all the local minima and not just the first one of the sweep curve this way our method detects both the number and the members of communities ________________ j yang j leskovec table 6 average f score between detected communities and the ground truth communities to which a seed node belongs to when the seed node belongs to g different communities network g number of communities 1 2 3 4 5 all nodes lj 52 59 52 42 38 53 fs 13 10 08 05 02 13 orkut 21 17 13 11 10 20 ning nets 11 09 07 06 05 11 amazon 59 73 69 66 55 61 dblp 34 24 20 21 0 16 0 33 for each dataset we sample a node detect communities s ˆ j and compare them to the ground truth communities s i that node belongs to to measure correspondence between the true and the detected communities we match ground truth communities to detected communities by the hungarian matching method 17 we then compute the average score over the matched pairs we use conductance as the community scoring function and report results in table 6 note that this task is harder than the previous one as here we aim to discover multiple com munities simultaneously whereas the previous task evaluated our method for each ground truth community here we first sample node and then search for the communities s i that s belongs to therefore larger ground truth communities will be included in s i more often since larger ground truth communities are less well separated 20 this makes the task harder table 6 reports the average score as a function of the number of communities g that the seed node s belongs to given that this is a harder task we observe overall lower values of the f score moreover we also expect the performance to decrease as node s belongs to more and more communities in fact we observe that the performance degrades with increasing the number of communities g interestingly in livejournal and amazon it appears to be easier to detect communities of nodes that belong to 2 communities than to detect a community of a node that belongs to only a single community this is due to the fact that single community nodes reside on the border of the community and consequently conductance produces communities that are too small 20 7 related work generally there are two approaches toward understanding the characteristics of network community structure and the community scoring functions i e objective functions for com munity detection first way is theoretical analysis which has been performed for a few most widely used functions such as modularity 11 and conductance 3 13 for example gleich and seshadhri 13 mathematically proved the existence of node sets with high conductance in networks with high clustering coefficient fortunato and barthelemy 11 showed that mod ularity may not detect communities that have too few edges compared to the total number of edges in the network more general theoretical analysis includes the work of meilˇa 22 which studied the axiomatic criterion for community scores such as stability 34 although these theoretical attempts provide rigorous results they cover only a few different scoring functions ________________ defining and evaluating network communities to consider a broader range of community scoring functions therefore we take a different way an empirical analysis which allows us to evaluate any community scoring function empirical approach analyzes how community scoring functions behave on real world networks one recent example is by leskovec et al 19 which evaluated a wide range of community scoring functions on large scale real world networks however there are two crucial differences in our work here first leskovec et al 19 used detected communi ties by the local spectral method 3 for the evaluation using communities detected by a specific community detection method would introduce a bias introduced by the detection method 1 in this paper our evaluation is free from such bias as we adopt ground truth communities which are explicitly declared by individual nodes second leskovec et al 19 provides qualitative evaluation by showing the network community profile plot 20 for each community scoring function here we aim to quantify the robustness and sensitivity of community scoring functions to compare which scoring functions are better than others we also note that empirical approach can be done with synthetic benchmarks such as in shi et al 30 however using synthetic benchmarks generates further biases due to natural deficiencies of synthetically generate networks another related line of research is data driven analysis of community detection algorithms for example ahn et al 2 employed community quality metrics based on the purity of node attributes to evaluate the performance of community detection methods abrahao et al 1 showed that the communities detected by different communities exhibit fundamentally different structural properties in this paper however we use data driven evaluation metric to assess the performance of community detection on ground truth communities our quality metrics directly focus on the correspondence between the detected and the ground truth communities as in lin et al 21 sun et al 33 8 conclusion the lack of reliable ground truth gold standard communities has made network community detection a very challenging task in this paper we studied a set of different large social collaboration and information networks in which we defined the notion of ground truth communities by nodes explicitly stating their group memberships we developed an evaluation methodology for comparing network community detection algorithms based on their accuracy on real data and compared different definitions of network communities and examined their robustness our results demonstrate large differences in behavior of community scoring functions last we also studied the problem of community detection from a single seed node we examined class of scalable parameter free community detection methods based on random walks and found that our methods reliably detect ground truth communities the availability of ground truth communities allows for a range of interesting future direc tions for example further examining the connectivity structure of ground truth communities could lead to novel community detection methods 37 39 40 overall we believe that the present work will bring more rigor to the evaluation of network community detection and the datasets publicly released as a part of this work will benefit the research community acknowledgments this research has been supported in part by nsf iis cns career iis iis aro muri darpa xdata darpa graphs arl ahpcrc okawa foun dation docomo boeing allyes volkswagen intel alfred p sloan fellowship and the microsoft faculty fellowship the development of a city gradually fosters different functional re gions such as educational areas and business districts in this paper we propose a framework titled drof that discovers regions of different functions in a city using both human mobility among re gions and points of interests pois located in a region specifically we segment a city into disjointed regions according to major roads such as highways and urban express ways we infer the functions of each region using a topic based inference model which regards a region as a document a function as a topic categories of pois e g restaurants and shopping malls as metadata like authors af filiations and key words and human mobility patterns when peo ple reach leave a region and where people come from and leave for as words as a result a region is represented by a distribution of functions and a function is featured by a distribution of mobility patterns we further identify the intensity of each function in differ ent locations the results generated by our framework can benefit a variety of applications including urban planning location choos ing for a business and social recommendations we evaluated our method using large scale and real world datasets consisting of two poi datasets of beijing in and and two month gps trajectory datasets representing human mobility generated by over taxicabs in beijing in and respectively the re sults justify the advantages of our approach over baseline methods solely using pois or human mobility categories and subject descriptors h database management data mining spatial databases and gis general terms algorithms design experimentation performance keywords functional regions urban computing taxi trajectories human mo bility introduction the step of urbanization and modern civilization leads to differ ent functional regions in a city e g residential areas business dis tricts and educational areas which support different needs of peo ple urban lives and serve as a valuable organizing technique for framing detailed knowledge of a metropolitan these regions may be artificially designed by urban planners or naturally formulated according to people actual lifestyle and would change functions and territories with the development of a city in this paper we aim to discover regions of different functions in urban areas using human mobility and points of interests pois here a city is partitioned into individual regions by major roads like high way and ring roads refer to figure a human mo bility is represented by people movement trajectories which can be cell tower traces in a cellular network or trajectories of driving routes or a sequence of posts like geo tweets geo tagged pho tos or check ins in location based services a poi is associ ated with a coordinate and a category like restaurants and shopping malls specifically we fill regions that could have similar func tions with the same color in figure a and identify the function ality intensity of each function in different locations for example figure b shows the functionality intensity of developed commer cial entertainment a kind of function areas in beijing where the darker part suggests a higher intensity this step is motivated by the following observations sometimes only a part of a region con tributes to a function on the other hand a function could be for mulated across several individual regions e g a shopping street finally each function is titled with some tags in a semi manual way based on the output of our method permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee kdd august beijing china copyright acm 186 b figure the objectives of this paper a functional regions b intensity of a function discovering regions of different functions can enable a variety of valuable applications first it can provide people with a quick understanding of a complex city like new york city tokyo and paris and social recommendations for example tourists can easily a differentiate some scenic areas from business districts given these functional regions thereby reducing effort for trip planning local people can also expand their knowledge about a city by finding re gions that have similar functions e g entertainment areas it is very common that local people may not well understand each part of a metropolitan even if they have been in the metropolitan for a few years second these functional regions can calibrate the ur ban planning of a city and contribute to the future planning to some extent it is not surprising that a city did not evolve as its origi nal planning given the complexity of urban planning itself and the difficulty in predicting the development of a city third these func tional regions would also benefit location choosing for a business and advertisement for instance when building a supermarket we need to consider the distance to the residential areas and the adver tisement for a training course could be better put considering the geospatial intensity of the educational function to identify the functions of a region we need to take into account both pois located in a region and human mobility among these re gions due to the following two aspects poi data on the one hand pois feature the function of a re gion for example a region containing a number of universities and schools has a high probability to be an educational area on the other hand a region usually contains a variety of pois thereby having compound functions instead of a single function some re gions may serve as both business districts and entertainment ar eas in a city in addition the information from poi data cannot differentiate the quality of different venues and reflect the inter actions between functional regions for instance restaurants are everywhere in a city however they could denote different func tions some small restaurants were built just for satisfying local residences daily needs while a few famous restaurants attracting many people might be regarded as a feature of an entertainment area as a result sometimes two regions sharing a similar distribu tion of pois could still have different functions human mobility the functions of a region have a strong correla tion with the traveling behavior of people who visit the region the knowledge that human mobility contributes to reveal the functions of a region mainly lies in two folds one is when people arrive at and leave a region the other is where people come from and leave for intuitively in a workday people usually leave a residential area in the morning and return in the evening the major time when peo ple visit an entertainment area however is the evening of workdays or the whole day of non workdays furthermore regions of differ ent functions are correlated in the context of human mobility for instance people reaching an entertainment area have a high prob ability from a working area in a workday and a residential area in non workdays as a result two regions are more likely to have similar functions if people travel to the two regions from similar functional regions or leave for similar ones the research reported in this paper is a step towards urban com puting which enables smart cities by understanding urban dynam ics the contribution of this paper consists of three points we propose a topic model based method to identify the func tions of individual regions which are obtained using morpho logical image segmentation approach the proposed method regards a region as a document deems a function as a topic uses human mobility related to the region as words and treats pois located in a region as metadata like titles authors af filiations and key words as a result a region is represented by a distribution of topics functions and each topic is a de noted by a distribution of words this model fits the research intuitively and reduces the data sparseness problem we infer the territory of these functions by clustering regions territory identification discovery of region topics mobility patterns topic distribution figure framework for discovering the functional regions into groups according to the topic distribution of each region regions from the same cluster have similar functions and different clusters represent different functions then for each cluster of regions we identify the functionality intensity in the regions belonging to the cluster using kernel density estimation which employs human mobility as samples we evaluated our method using large scale and real world datasets consisting of two poi datasets of beijing in year and and two month gps trajectory datasets representing human mobility generated by over taxi cabs in beijing in and respectively the results justify the advantages of our approach over baseline meth ods solely using poi or human mobility in addition these powerful datasets allow us to study not only the functional regions in a city but also the evolving of the city across years in accordance with the framework of our work presented in fig ure the rest of this paper is organized as follows section dis covers the distributions of functions for each region which consists of a map segmentation and an analogy from topic model of docu ments section identifies the aggregated functional regions based on the discovered distribution of functions pertaining to each region and estimates the intensity of each function evaluation results are reported in section and related works are categorized in section finally we briefly conclude the paper in section discovery of region topics in this section we first segment the urban area of a city into region units in terms of major road networks and then infer the distribution of functions in each region unit using a topic model based method map segmentation a road network is usually comprised of some major roads like highways and ring roads which naturally partition a city into re gions for example as shown in figure the red segments de note freeways and city expressways in beijing and blue segments represent urban arterial roads the three kinds of roads are associ ated with a road level and respectively in a road network database forming a natural segmentation of the urban area of bei jing we term each segmented region as a formal region in the rest of this paper following the definition proposed in intrinsically a formal region is a basic unit carrying social economic functions due to the following reasons first people live in formal regions and pois fall in regions second formal regions as the origin and destination of a trip are the root cause of human mobility in short people travel among formal regions in our method we choose the raster based model to represent the road network and utilize morphological image processing tech end road networks region annotation map segmentation region clusters functionality pois basic regions intensity estimation human mobility region aggregation topic discovery km a mobility pattern m is a triple extracted from a transition given a transiton tr tr r o tr r d tr t l tr t a we obtain two mobility patterns the leaving mobility pattern m l tr r o tr r d tr t l and the arriving mobility pattern m a tr r o tr r d tr t a definition transition cuboids a transition cuboid c is an r r t cuboid where r is the number of regions and t is the number of time bins since there exist two types of mobility patterns we define two types of transition cuboids leaving cuboid c l and arriving cuboid c a the cell with index i j k of the figure beijing road network red level blue level niques to address the task of map segmentation typically in a leaving cuboid records the number of mobility patterns that leave r i for r j at time t k i e geographical information system gis there are two models to represent spatial data vector based model and raster based model vector based model uses geometric primitives such as points lines c l i j k m l x y z x r i y r j z t k similarly and polygons to represent spatial objects referenced by cartesian coordinates while raster based model quantizes an area into small discrete grid cells both of the two models have advantages and disadvantages depending on the specific applications for instance vector based method is more powerful for precisely finding shortest paths whereas it requires intensive computation when performing topological analysis such as the problem of map simplification which is proved to be np complete on the other hand raster based model is more computational efficient and succinct for terri torial analysis but the accuracy is limited by the number of cells c a i j k m a x y z x r i y r j z t k we project each trajectory representing human mobility on the seg mented region units turning a trajectory into a transition then we discretize time of day into time bins in each of which we deposit the transitions and formulate mobility patterns here we do not differ entiate different weekdays but differ the time bins in weekdays from those in weekends for example setting hours as a bin we will have bins for weekdays and for weekends in total later two transition cuboids are built using the mobility patterns used for discretizing the road networks concepts of topic models probabilistic topic models have been specifically a raster based map is a binary image e g stands successfully used for extracting the hidden thematic structure in for road segments and stands for blank space in order to remove large archives of documents in this model each document of the unnecessary details such as the lanes of a road and the over a corpus exhibits multiple topics and each word of a document sup passes see figure a we first perform a dilation operation to ports a certain topic given all the words of each document in a thicken the roads as a result we can fill the small holes and smooth corpus as observations a topic model is trained to infer the hidden out the unnecessary details as shown in figure b second we thematic structure behind the observations latent dirichlet allo obtain the skeleton of the road networks by performing a thinning cation lda is a generative model that includes hidden variables operation based on the algorithm proposed in as depicted in the intuition behind this model is that documents are represented figure c this operation recovers the size of a region which was as random mixtures over latent topics where each topic is charac reduced by the dilation operation while keeping the connectivity terized by a distribution over words let α and η be the prior between regions the last step is to perform a connected com parameters for the dirichlet document topic distribution and topic ponent labeling ccl that finds individual regions by clustering word distribution respectively assume there are k topics and β is labeled grids using the method proposed in figure d a k v matrix where v is the number of words in the vocabu presents the results lary all the words in the corpus d each β k is a distribution over topic discovery the vocabulary the topic proportions for the dth document are θ d where θ d k is the topic proportion for topic k in the dth document preliminary the topic assignments for the dth document are z d where z d n is the topic assignment for the nth word in the dth document finally definition transition a transition tr is a quater nion containing the following four items origin region tr r o leaving time tr t l destination region tr r d and arrival time tr t a here tr r o and tr r d are spatial features while the oth the observed words for document d are w d where w d n is the nth word in document d which is an element from the fixed vocabulary using the above notations as presented in figure the genera tive process can be described as follows ers are temporal features for each topic k draw β k dir η a original d b step dilation step ccl figure map segmentation d efinition c step thinning m obility p attern regard arriving cuboid region r as a document v leaving cuboid authors affiliation and key words α θ d z d n w d n n d β k words figure analogy between mobility patterns and words based on transition cuboids λ k k v v i v f k η figure graphic model of lda given the dth document d in corpus d draw θ d r n r j r t arriving matrix of r r n r t dir α for the nth word in the dth document w d n a draw z d n mult θ d b draw w d n mult β z d n here dir is the dirichlet distribution and mult is the multi nomial distribution the estimation of lda can be implemented using the em algorithm and the most commonly used inference method of lda is gibbs sampling see for a variational infer ence method and detailed discussions of lda topic modeling as shown in table we draw an analogy between discovering functions of a region and the topic discovery of a document specif ically we regard a formal region as a document and a function as a topic in other words a region having multiple functions is just like a document containing a variety of topics meanwhile we deem the mobility patterns associated with a region as words and pois as metadata of a document as a functional region is characterized by its agglomeration of activities its intraregional transport infras tructure mobility of people and inputs within its interaction bor ders table analogy from region functions to document topics transition cuboids vocabulary formal regions documents function of a region topic of a document mobility patterns words poi feature vector metadata of a document figure further details the analogy using an example in our method given the mobility dataset we build the arriving and leav ing cuboids respectively according to definition for a specific region r i the mobility patterns associated with r i are counted by c a r i t and c l i r t which are two slices ex tracted from arriving cuboid and leaving cuboid termed as arriving matrix and leaving matrix respectively the right part of figure shows a document we compose for region r where a cell in the matrices represents a specific mobility pattern and the numbers in the cell denote the occurrences of the pattern for example in the right most column of the arriving matrix the cell containing means on average the mobility that went to r from r j in time bin t k occurred times per day a poi is recorded with a tuple in a poi database consisting of a poi category as listed in table name and a geo position latitude longitude for each formal re gion r the number of pois in each poi category can be counted the frequency density v i of the ith poi category in r is calculated by v i number area of of region the pois r measured of the ith by poi grid cells category and the poi feature vector of r is denoted byx r v v v f where f is the number of poi categories and the last is a default feature to account for the mean value of each topic as explained in is a vector with the same length as the poi feature vector t k r j r t t r n r i r r t t k leaving matrix of r r n r n r j r t r j t k t r n r i r r t t k α k θ k z r n m r n φ k β x r r k figure dmr based topic model the poi feature vector is regarded as the metadata of each region which is an analogue of the observed features such as au thor email institution of a document using the basic lda model region topics can be discovered us ing the mobility patterns however as stated in section the re gion topics are products of both the pois and mobility patterns in order to combine the information from both of them we utilize a more advanced topic model based on lda and dirichlet multino mial regression dmr the dmr based topic model for simplicity dmr in the rest of the paper takes into account the influence of the observable meta data in a document by using a flexible framework which supports arbitrary features compared to other models designed for spe cific data such as author topic model and topic over time model a member in the supervised lda family of topic models dmr achieves similar or improved performance while is more computa tional efficient and succinct in implementation as presented in figure the generative process of the dmr model is for each region topic k a draw λ k n n b draw β k dir η given the rth region a for each region topic k let α r k b draw θ r dir α r exp xt r λ k c for the nth mobility pattern in the rth region m r n ii i draw draw z m r n r n mult θ mult β r z r n here n is the gaussian distribution with σ as a hyper parameter and λ k table poi category taxonomy code poi category code poi category car service banking and insurance service car sales corporate business car repair street furniture motorcycle service entrance bridge café tea bar public utilities sports stationery shop chinese restaurant living service foreign restaurant sports fastfood restaurant hospital shopping mall hotel convenience store scenic spot electronic products store residence supermarket governmental agencies and public organizations furniture building materi als market science and education pub bar transportation facilities theaters the nth observed mobility pattern of region r is denoted as m r n other notations are similar to the previous lda model this model can also be estimated using em and inferred using gibbs sampling different from the basic lda model here the dirichlet prior α is now specified to individual regions α r based on the observed poi features of each region therefore for different combination of poi category distributions the resulting α values are distinct thus the thematic region topic distributions extracted from the data are induced by both the poi features and mobility patterns as a result by applying dmr given the mobility patterns and poi features we obtain the topic assignment for each region and the mobility pattern distribution of each topic territory identification region aggregation this step aggregates similar formal regions in terms of region topic distributions by performing a clustering algorithm regions from the same cluster have similar functions and different clusters represent different functions for region r after parameter esti mations based on the dmr model the topic distribution is a k dimensional vector θ r θ r θ r θ r k where θ r k is the proportion of topic k for region r we perform the k means cluster ing method on the k dimensional points θ r r r the number of clusters can be predefined according to the needs of an application or determined using the average silhouette value as the criterion the silhouette value of a point i in the dataset de noted by i is in the range of where i close to means that the point is appropriately clustered and very distant from its neighboring clusters i close to indicates that the point is not distinctly in one cluster or another i close to means the point is probably assigned to the wrong cluster the average silhouette value of a cluster measures how tightly the data in this cluster are grouped and the average silhouette of the entire dataset reflects how appropriately all the data has been clustered in practice we per form cross validation on the dataset for different k multiple times and choose an appropriate k with the maximum overall silhouette value consequently we aggregate the formal regions into k clus ters each of which is termed as a functional region functionality intensity estimation on the one hand the functionality of a functional region is gener ally not uniformly distributed within the entire region on the other hand sometimes the core functional area may span multiple for mal regions and may have an irregular shape e g a hot shopping street crossing several formal regions in order to reveal the de gree of functionality and glean the essential territory of a functional region we estimate the functionality intensity for each aggregated functional region a cluster of formal regions intuitively the number of visits implicitly reflects the popularity of a certain functional region in other words people mobility patterns imply the functionality intensity as a result we feed the origin and destination of each mobility represented by latitude and longitude into a kernel density estimation kde model to in fer the functionality intensity in a functional region note that the real place that an individual visited may not be the destination that we can obtain from a mobility dataset for example the drop off points of taxi trajectories may not be people final destinations like a shopping mall however the pick up drop off points should not be too far from the really visited locations according to common sense knowledge the farther distance a location to the drop off point the lower probability that people would visit the location given n points x x x n located in a spatial space kde estimates the intensity at location by a kernel density es timator defined as λ n i k d i r where d i is the distance from x i to r is the bandwidth and k is the kernel function whose value decays with the increasing of d i r such as the gaussian function quartic function conic and negative exponential the choice of the bandwidth usually deter mines the smoothness of the estimated density a large r achieves smoother estimation while a small r reveals more detailed peaks and valleys in our case we choose the gaussian function as the kernel function i e k d i r exp i and the bandwidthr is determined according to mise criterion region annotation in this step given the results we obtained we try to annotate each cluster of regions with some semantic terms which could contribute to the understanding of its real functions note that region annota tion is a very challenging problem in both traditional urban planning and document processing essentially this annotation problem is the visualization problem of topic model which is listed as a future direction of topic modeling in the recent survey paper by blei a compromised method so far is to use the most frequent words in a discovered topic to annotate a document but in our case listing the frequent mobility patterns analogue to words is far from enough to name a functional region in our method we annotate a functional region by considering the following aspects the poi configuration in a functional re gion we compute an average poi feature vector across the regions in functional region according to the average frequency density in the calculated poi feature vector we rank each poi category in a functional region termed as internal ranking and rank all the func tional regions for each poi category termed as external ranking we will give an example in the experiment as shown in table the most frequent mobility patterns of each functional region the functionality intensity we study the representative pois lo cated in each functionality kernel e g a function region could be an educational area if its kernel is full of universities and schools the human labeled regions people may know the functions of poi category code i o p figure poi counts for each category in and a few well known regions e g the region contains the forbidden city is an area of historic interests after clustering the human labeled regions will help us understand other regions in a cluster refer to the experiments for the detailed results and analysis evaluation settings we use the following datasets for the evaluation a poi data two beijing poi datasets in year and the count of each poi category see table is presented in figure b mobility data two gps trajectory datasets generated by bei jing taxis in and with the statistics shown in table we only choose the occupied trips identified by the information of a taxi meter from the data actually there are over cities in this world having over taxicabs e g beijing has over taxis the taxi trips represent a significant portion of people urban mobility according to the report of beijing transportation bureau the taxi trips occupy over percent of traffic flows on road sur faces of course other mobility datasets such as mobile phone traces can be used in our framework c road networks we have the road networks of beijing in and with statistics shown in table table statistics of taxi trajectories and road networks year t e i r o t c e j a r taxis occupied trips effective days average trip distance km average trip duration min 597 average sampling interval sec d a o r road segments percentage of major roads segmented formal regions size of vocabulary non items 901 we implement our method on a bit server with a quad core cpu and ram we train our model with topics for iterations optimize the parameters every iterations for k means clustering we incorporate the average silhouette value to determine the k and use the average results based on a fold cross validation the efficiency on average is presented in table table overall efficiency of drof operation time min map segmentation building transition cuboids estimate topic model iterations region aggregation total we compare our method with two baselines a the tf idf based clustering which solely uses the poi data this method employs the term frequency inverse document frequency tf idf to measure the importance of a poi in a formal region specifically for a given formal region we formulate a poi vector v v v f where v i is the tf idf value of the i th poi category given by v i n i n log r r the i th poi category r where n i is the number of pois belonging to the i th category and n is the number of pois located in this region the idf term is cal culated by computing the quotient of the number of regions divided by the number of regions which have the i th poi category and taking the logarithm of that quotient later a k means clustering is used to cluster the formal regions into k functional regions b lda based topic model which uses only the mobility data sim ilar to our analogy from regions to documents this method feeds the mobility patterns the analogue to words into an lda model later we perform the k means clustering similar to the method we used when grouping all the formal regions based on their topic dis tributions learned from lda the parameters such as number of it erations number of topics are set in accordance with our drof as the number of poi categories usually has the same scale with that of the topics applying the lda model solely to pois as words will not reduce the dimension of words we carry out the following three studies to evaluate the effective ness of our framework though it is very difficult we invite some local people who have been in beijing for over years and request them to label two representative regions for each kind of function we check whether the regions having the same labels are assigned into the same functional region and whether the regions with dif ferent labels are improperly clustered into one functional region we find the evolving of beijing by comparing the results of and and identify whether the differences make sense we match our results against the land use planning of beijing results discovered functional regions figure shows the aggregated functional regions discovered by different methods where different colors indicate different func tions note that in different figures the same color may stand for different functions as a result tf idf based method forms clusters c c while lda based method and our method drof form functional regions the tf idf method performs the worst among the three ap proaches for example as shown in figure a region b is an university which should be clustered with region a another uni versity and region d a high school meanwhile region f the forbidden city is not distinguished from other commercial areas like region e xidan another example is the wangjing area c which is an emerging residential area with some companies and many living services like apartments shopping malls and restau rants unfortunately the tf idf method improperly divides this area into many small functional regions as this method only consid ers poi distributions basically the lda based method and drof have a similar out put of functional regions however there still exist several exem plary regions where drof outperforms lda obviously for exam ple regionf in figure b is a developing commercial entertainment area in wangjing area but lda aggregates it with the forbidden city region f in figure a which is a region of historical in terests area b china agricultural university and area e ts inghua university are typical science and education areas where lda fails to correctly cluster them together area a around sanl itun is a well known diplomatic district of beijing which is mixed with a developing commercial area c region d is a park where road c drof figure functional regions discovered by different methods table overall poi feature vector and ranking of functional regions by drof fd frequency density ir internal ranking poi fd ir fd ir fd ir fd ir fd ir fd ir fd ir fd ir fd ir carserv carsale 061 006 carrepa 062 062 motserv 003 caf tea 226 052 stastor 072 livserv 322 345 sports 092 080 hospital 222 246 194 hotel 058 211 scespo 032 012 residen 638 797 gov pub 276 375 sci edu 084 530 trasfac 397 364 bank fina 383 copbusi 348 977 strfur entr bri 210 097 pubuti 285 314 chires 399 370 forres 054 063 fasres 046 057 shopmal 724 476 convstor 370 234 e stor 056 037 063 supmar 055 furbuil 086 065 151 142 pub bar 179 053 031 071 theater 011 001 001 006 025 007 002 002 sparse developed residential areas c this region cluster is clearly a mature residential area with the most residential building living services hospitals hotels in this kind of areas an adequate num ber of services supports the people living such as the restaurants shopping malls banking services schools sports centers emerging residential areas c lda fails to put it with other park mountain areas the green re gions shown in figure b the lda method only using human mobility overlooks the feature of pois thereby drops behind the drof shown in figure c region annotation table shows the average poi feature vector of each region clus ter c this area is annotated as the emerg ing residential area since it has a balanced poi configuration such as living services residential buildings sports centers hospitals and some companies etc figure compares the arriving leaving transitions of region clusters c and c with that of other clusters respectively where the x axes are time of day by hour and y axes are the functional regions that come from left subfigures and leave for right subfig ures both c and c follow the trend that most leaving transitions in the morning go to work and most arriving transitions in the early evening go back home which is a typical pattern for the residential area however in terms of the absolute quantity c is much lower than c which shows that there are more people living in c developed commercial entertainment areas c they are typi cal entertainment areas with the highest external rank of theaters foreign restaurants and café tea bars moreover there are a great many shopping malls chinese restaurant and convenience stores figure shows the difference of arriving transitions of c devel oped commercial areas on weekdays and weekends it clear that the people reach this kind of areas from the residential regions c much earlier on weekends about 11am than on weekdays with regard to the other region clusters since the frequency den c remember that drof generated clusters and the corre sponding internal and external rankings where the external rank is sities of pois are much lower than the above functional regions we identify their semantic functions with more consideration on the represented by the depth of the color darkest lightest clearly functionality intensity and frequent mobility patterns derived for region cluster c c c c c are more mature and more devel oped areas as compared to other clusters since they have more high each functional region in addition to the poi configurations developing commercial business entertainment areas c ranked poi categories which are annotated as follows diplomatic and embassy areas c the poi configuration of this cluster is similar to cluster c and c but the most characteristic poi in terms of the absolute quantity c is less than c while more than category in this functional region is the governmental agencies and c public organizations with a significant higher frequency density than other functional regions actually most of the embassies are located in these areas which are well configured for the diplomatic function e g they have the highest external rank of pub bars and transportation facilities and the second highest rank of residential buildings hospitals and hotels among all the clusters education and science areas c a certain number of shopping malls restaurants and banking services feature this cluster as a developing commercial business entertainment functional region either of them is possible in the meantime the functionality intensity provides another corrobora tion for this annotation as depicted in figure a the core of this functional regions is the new cbd of beijing regions under construction c as analyzed above for region c this region cluster contains the this region will potentially become regions or since the poi con maximum number of science and education pois e g tsinghua figuration produces a rudiment of the commercial residential area university and beijing university in addition the biggest elec with related supporting services figure validates the degree of tronic market in china zhongguancun known as the silicon val development with respect to c ley in china is located in this functional region c and c areas of historic interests c if we only consider the poi config road b c a d e f a tf idf sparse e b lda f b c d road a sparse b functional region c figure functionality intensity of functional regions uration the characteristic of this cluster does not reveal obviously which contains some public utilities entrance bridges government organizations science and education spots however by consid ering the functionality intensity estimated by the mobility patterns we are easy to find that they are places of historic interests in bei jing as shown in figure b the famous historical places like forbidden city and temple of heaven are located in these areas nature and parks c these areas have the fewest pois in most of the poi categories actually a lot of forests and mountains cover this cluster e g the xishan forest park century forest park baiwang moutain etc figure shows that people come to this functional region following the similar temporal patterns with c the historical areas but the diversity and quantity are reasonably weaker than c since many pois in c are very famous scenic spots results in different years we apply our method to the data road network poi and mo bility of and respectively the discovered functional regions in are similar with while is slightly different in some regions figure shows the detailed comparison around the east areas of the forbidden city for example region a qianmen street becomes a developed commercial area from a nature park area the reason is that this region was developing in after a major repair since similar to region a region b close to the new cbd of beijing becomes a developing commercial area from an under construction area intriguingly region c becomes an un der construction area in from an developing commercial area we later found the truth that the tallest building of beijing will rise up in this region in leading to a relocation work in calibration for urban planning the discovered functional regions provide calibration and refer ence for urban planning for example figure presents the com parison between the governmental land use planning a functional region c figure transitions of c a arriving c arriving b leaving d leaving and c figure transitions of c a arriving c arriving c figure the east area of forbidden city in and and the results of drof in this area forms an emerging residential area as planned by the government while some small regions become developing commercial areas such as a b and c after years development related work urban computing with taxicabs urban computing is emerging as a concept where every sensor device person vehicle building and street in urban areas can be used as a component to probe city dynamics and further enable a city wide computing for serving peo ple and their cities the increasing availability of gps embedded taxicabs provides us with an unprecedented wealth to understand human mobility in a city thereby enabling a variety of novel ur ban computing research recently for example ge et al and yuan et al respectively study the strategies for improving a c2 c6 figure transitions of c a arriving e arriving c arriving b leaving d leaving and c b c0 c2 c3 c6 c8 c0 c2 c3 c4 c6 c8 c0 c1 c2 c3 c4 c6 c7 c8 figure transitions of c c0 c1 c2 c3 c4 c6 c7 c8 c b leaving d leaving c1 a arriving weekdays c5 b arriving weekends c5 f leaving c7 c a b and c b a a b c c a b figure a governmental land use planning b discovered functional regions in taxi drivers income by analyzing the pick up and drop off behavior of taxicabs in different locations aims to find the practically fastest driving route to a destination according to a large number of taxi trajectories and zheng et al glean the problematic urban planning in a city using the corresponding taxi trajectories based on the traffic flow represented by taxi trajectories the technology for detecting anomalies in urban areas has been reported in the work presented in this paper is also a step towards urban computing different from the above mentioned research however we focus on the discovery of functional regions in a city which we have never seen before in this research theme discovery of functional regions functional regions have been studied in traditional fields of gis and urban planning for years as the discovery of them can benefit policy making resource alloca tion and the related research gives a good survey on the related literatures which are mainly based on clustering algorithms some algorithms classify regions in urban area based on remote sensor data as thoroughly compared in other network based cluster ing algorithms e g spectral clustering however employ interac tion data such as the economic transactions and people movement between regions recently a brunch of work aims to study the geographic distri bution of some topic in terms of user generated social media for example yin et al study the distributions of some geographi cal topics like beach hiking and sunset in usa using geo tagged photos acquired from flickr pozdnoukhov et al explore the space time structure of topical content from a large number of geo tweets the social media generated in a geo region is still used as static features to feature a region on the other hand a few litera tures have reported that human mobility can describe the functions of regions for instance qi et al observe that the getting on off amount of taxi passengers in a region can depict the social activity dynamics in the region our work is different from the research mentioned above in the following aspects first to the best of our knowledge our method is the first one that simultaneously considers static features pois of a region and interactions human mobility between regions when identifying functional regions second rather than directly using some clustering algorithm we propose a topic model based solu tion which represents a region with a distribution of functions the function distribution is more practical than a single function for a region moreover it reduces the data sparseness problem before clustering regions we justify the advantage of our method over just using clustering approach in the experiments conclusion this paper proposes a framework titled drof for discovering regions of different functions e g educational areas entertainment areas and regions of historic interests in a city using both human mobility and pois the discovered functional regions help people easily understand a complex metropolitan benefiting a variety of applications such as urban planning location choosing for a busi ness advertisement casting and social recommendations we eval uated this framework with a two year beijing poi dataset and and gps trajectory datasets generated by over taxis in year and according to the extensive studies drof outperforms two baselines solely using pois or mobility data in ef fectively finding functional regions we also compared the results of drof in with that of discovering the evolving of beijing in addition by matching the discovered functional regions against beijing land use planning we do not only val idate our framework but also find interesting results in the future we will further study the effectiveness of our method changing over the scale of the data we use at the same time we are going to employ or add other mobility data sources such as cell tower traces and check ins in location based services discussed in the paper are able to deal with rdfs or skos as well although database schemas are different from ontologies by not providing explicit semantics for their data these are also similar since they both provide a vocabulary of terms and somewhat constrain the meaning of terms used in the vocabulary moreover in real life situations schemas and ontologies have both well defined and obscure entities and structures hence they often share similar matching solutions therefore we discuss in this paper approaches that come from semantic web and artificial intelligence as well as from databases overcoming semantic heterogeneity is typically achieved in two steps namely matching entities to determine an alignment i e a set of correspondences and interpreting an alignment according to application needs such as data translation or query answering we focus only on the matching step ontology matching is a solution to the semantic hetero geneity problem it finds correspondences between seman tically related entities of ontologies these correspondences can be used for various tasks such as ontology merging query answering or data translation thus matching ontologies enables the knowledge and data expressed with respect to the matched ontologies to interoperate diverse solutions for matching have been proposed in the last decades several recent surveys and books have been written on the as well as evaluations of the recent years indicate the field of ontology matching has made a measurable improvement p shvaiko is with taslab informatica trentina via g gilli trento italy e mail pavel shvaiko infotn it the speed of which is albeit slowing down in order to achieve similar or better results in the forthcoming years j euzenat is with inria lig inria grenoble rhoˆne alpes actions have to be taken we believe this can be done avenue de l europe montbonnot saint martin france e mail jerome euzenat inria fr manuscript received feb revised nov accepted nov through addressing we identify as specifically promising challenges that published online dec recommended for acceptance by c clifton for information on obtaining reprints of this article please send e mail to progress of information and communication tech nologies has made available a huge amount of disparate information the problem of managing heterogeneity among various information resources is increasing for example most of the database research self assessment reports recognize that the thorny question of semantic heterogeneity that is of handling variations in meaning or ambiguity in entity interpretation remains open as a consequence various solutions have been proposed to facilitate dealing with this situation and specifically to automate integration of distributed information sources among these semantic technologies have attracted parti cular attention in this paper we focus on a kind of semantic technologies namely ontology matching an ontology typically provides a vocabulary that de scribes a domain of interest and a specification of the meaning of terms used in the vocabulary depending on the precision of this specification the notion of ontology encompasses several data and conceptual models includ ing sets of terms classifications thesauri database sche mas or fully axiomatized theories when several competing ontologies are used in different applications most often these applications cannot immediately inter operate in this paper we consider ontologies expressed in owl as a typical example of a knowledge representation language on which most of the issues can be illustrated owl is succeeding to a large degree as a knowledge representation standard for instance used for building knowledge systems however several matching systems i large scale matching evaluation ii efficiency of matching techniques tkde computer org and reference ieeecs log number tkde digital object identifier no tkde see http www ontologymatching org for more details on the topic ß ieee published by the ieee computer society shvaiko and euzenat ontology matching state of the art and future challenges fig two simple ontologies and an alignment fig the ontology matching operation iii matching with background knowledge iv matcher selection combination and tuning v user involvement vi explanation of matching results vii social and collaborative matching and viii alignment management infrastructure and support this paper is an expanded and updated version of an earlier invited conference paper the first contribution of this work is a review of the state of the art backed up with analytical and experimental comparisons its second contribution is an in depth discussion of the challenges in the field of the recent advances made in the areas of each of the challenges and an outline of potentially useful approaches to tackle the challenges identified the remainder of the paper is organized as follows section presents the basics of ontology matching section outlines some ontology matching applications sections and discuss the state of the art in ontology matching together with analytical and experimental comparisons section overviews the challenges of the field while sections and discuss them in detail finally section provides the major conclusions t he o ntology m atching p roblem in this section we first discuss a motivating example section and then we provide some basics of ontology matching section motivating example in order to illustrate the matching problem let us use the two simple ontologies and of fig classes are shown in rectangles with rounded corners e g in book being a specialization subclass of product while relations are shown without the latter such as price being an attribute defined on the integer domain and creator being a property albert camus la chute is a shared instance correspondences are shown as thick arrows that link an entity from with an entity from they are annotated with the relation that is expressed by the correspondence for example person in is less general v than human in assume that an e commerce company acquires another one technically this acquisition requires the integration of their information sources and hence of the ontologies of these companies the documents or instance data of both companies are stored according to ontologies and respectively in our example these ontologies contain subsumption statements property specifications and in stance descriptions the first step in integrating ontologies is matching which identifies correspondences namely the candidate entities to be merged or to have subsumption relationships under an integrated ontology once the correspondences between two ontologies have been deter mined they may be used for instance for generating query expressions that automatically translate instances of these ontologies under an integrated ontology for example the attributes with labels title in and in are the candidates to be merged while the class with label mono graph in should be subsumed by the class product in problem statement there have been different formalizations of the matching operation and its result we follow the work in that provided a unified account over the previous works the matching operation determines an alignment for a pair of ontologies and hence given a pair of ontologies which can be very simple and contain one entity each the matching task is that of finding an alignment between these ontologies there are some other parameters that can extend the definition of matching namely the use of an input alignment a which is to be extended the matching parameters for instance weights or thresholds and external resources such as common knowledge and domain specific thesauri see fig we use interchangeably the terms matching operation thereby focussing on the input and the result matching task thereby focussing on the goal and the insertion of the task in a wider context and matching process thereby focussing on its internals it can be useful to specifically consider matching more than two ontologies within the same process though this is out of the scope of this paper an alignment is a set of correspondences between entities belonging to the matched ontologies alignments can be of various cardinalities one to one m one to many n many to one or n m many to many given two ontologies a correspondence is a uple hid e e ri ieee transactions on knowledge and data engineering vol no january such that a simple keyword like request for a map generation such id is an identifier for the given correspondence e and e are entities e g classes and properties the first and the second ontology respectively r is a relation e g equivalence more ðwþ disjointness ð þ holding between e the correspondence hid e as hydrography trento january this request is a of set of terms covering spatial trento and temporal january aspects to be addressed while looking for a specific general theme that is of hydrography handling such a request r holds between example hid e ri asserts that the ontology entities e and e the relation for involves interpreting at runtime the user query and creating an alignment between the relevant gi resources such as those having up to date january topography and hydrography maps of trento in order to ultimately compose these into a single one technically alignments are used in such a setting for query expansion for what concerns thematic part e g hydrography standard match ing technology can be widely reused while the spatial and temporal counterparts that constitute the specificity of gi applications have not received enough attention so far in the ontology matching field with exceptions such as and hence this gap will have to be covered in future r ecent http www iconclass nl http www rijksmuseum nl collectie index jsp lang en http oaei ontologymatching org and e book monograph wi asserts that book in is more general w than monograph in correspon dences have some associated metadata such as the corre spondence author name a frequently used metadata element is a confidence in the correspondence typically in the range the higher the confidence the higher the likelihood that the relation holds a pplications ontology matching is an important operation in traditional applications e g ontology evolution ontology integra tion data integration and data warehouses these applications are characterized by heterogeneous models e g database schemas or ontologies that are analyzed and matched manually or semi automatically at design time in such applications matching is a prerequisite to running the actual system there are some emerging applications that can be characterized by their dynamics such as peer to peer information sharing web service composition search and query answering such applications contrary to traditional ones require ultimately a runtime matching operation and take advantage of more explicit conceptual models a detailed description of these applications as well as of the requirements they pose to matching can be found in we illustrate only some of these applications with the help of two short real world examples in order to facilitate the comprehension of the forthcoming material cultural heritage a typical situation consists of having several large thesauri such as entities and the aria collection terms from the rijksmuseum the documents indexed by these thesauri are illuminated manu scripts and masterpieces i e image data the labels are gloss like i e sentences or phrases describing the concept since they have to capture what is depicted on a masterpiece examples of labels from iconclass include city view and landscape with man made constructions and earth world as celestial body in contrast to iconclass aria uses simple terms as labels examples of these include landscapes personifi cations and wild animals matching between these thesauri that can be performed at design time is required in order to enable an integrated access to the masterpieces of both collections specifically alignments can be used as naviga tion links within a multifaceted browser to access a collection via thesauri it was not originally indexed with geo information gi a typical situation at a urban planning department of a public administration consists of m atching s ystems we now review several state of the art matching systems sections and that appeared in the recent years and have not been covered by the previous surveys section among the several dozens of systems that have appeared in these recent years we selected some which have repeatedly participated to the ontology alignment evalua tion initiative oaei see section in order to have a basis for comparisons and have corresponding archival publications hence the complete account of these works is also available an overview of the considered systems is presented in table the first half of the table provides a general outlook over the systems the input column presents the input format used by the systems the output column describes the cardinality of the computed alignment see section the gui column shows if a system is equipped with a graphical user interface and the operation column describes the ways in which a system can process alignments the second half of the table classifies the available matching methods depending on which kind of data the algorithms work on strings terminological structure structural data instances extensional or models semantics strings and structures are found in the ontology descriptions e g labels comments attributes and their types relations of entities with other entities instances constitutes the actual population of an ontology models are the result of semantic interpretation and usually use logic reasoning to deduce correspondences table illustrates particular matching methods employed by the systems under consideration below we discuss these systems in more details sambo linko pings u sambo is a system for matching and merging biomedical ontologies it handles ontologies in owl and outputs alignments between concepts and relations the system uses various similarity based matchers including shvaiko and euzenat ontology matching state of the art and future challenges table analytical comparison of the recent matching systems terminological n gram edit distance comparison of the lists of words of which the terms are composed the results of these matchers are combined via a weighted sum with predefined weights structural through an iterative algorithm that checks if two concepts occur in similar positions with respect to is a or part of hierarchies relative to already matched concepts with the intuition that the concepts under consideration are likely to be similar as well background knowledge based using a relationship between the matched entities in unified medical language system umls and a corpus of knowledge collected from the published literature exploited through a naive bayes classifier the results produced by these matchers are combined based on user defined weights then filtering based on thresholds is applied to come up with an alignment suggestion which is further displayed to the user for feedback approval rejection or modification once match ing has been accomplished the system can merge the matched ontologies compute the consequences check the newly created ontology for consistency etc sambo has been subsequently extended into a toolkit for evaluation of ontology matching strategies called kitamo falcon southeast u falcon is an automatic divide and conquer approach to ontology matching it handles ontologies in rdfs and owl it has been designed with the goal of dealing with large ontologies of thousands of entities the approach operates in three phases partitioning ontologies matching blocks and discovering alignments the first phase starts with a structure based partitioning to separate entities classes and properties of each ontology into a set of small clusters partitioning is based on structural proximities between classes and properties e g how closely are the classes in the hierarchies of rdfs sub classof relations and on an extension of the rock agglomerative clustering algorithm then it constructs blocks out of these clusters in the second phase the blocks from distinct ontologies are matched based on anchors pairs of entities matched in advance i e the more anchors are found between two blocks the more similar the blocks are in turn the anchors are discovered by matching entities with the help of the i sub string comparison technique the block pairs with high similarities are selected based on a cutoff threshold notice that each block is just a small fragment of an ontology finally in the third phase the results of the so called v doc a linguistic matcher and gmo an iterative structural matcher techniques are combined via sequential composi tion to discover alignments between the matched block pairs ultimately the output alignment is extracted through a greedy selection dssim open u poznan u of economics dssim is an agent based ontology matching framework the system handles large scale ontologies in owl and skos and computes alignments with equivalence and subsumption relations between concepts and properties it uses the dempster shafer theory in the context of query answering specifically each agent builds a belief for the correctness of a particular correspondence hypoth esis then these beliefs are combined into a single more coherent view in order to improve correspondence quality the ontologies are initially partitioned into fragments each concept or property of a first ontology fragment is viewed as a query which is expanded based on hypernyms from wordnet viewed as background knowledge these hypernyms are used as variables in the hypothesis to enhance the beliefs the expanded concepts and properties ieee transactions on knowledge and data engineering vol no january are matched syntactically to the similar concepts and properties of the second ontology in order to identify a relevant graph fragment of the second ontology then the query graph of the first ontology is matched against the relevant graph fragment of the second ontology for that purpose various terminological similarity measures are used such as monger elkan and jaccard distances which are combined using dempster rule similarities are viewed as different experts in the evidence theory and are used to assess quantitative similarity values converted into belief mass functions that populate the similarity matrices the resulting correspondences are selected based on the highest belief function over the combined evidences eventual conflicts among beliefs are resolved by using a fuzzy voting approach equipped with four ad hoc if then rules the system does not have a dedicated user interface but uses that of the aqua question answering system able to handle natural language queries rimom tsinghua u hong kong u of science and technology rimom is a dynamic multistrategy ontology matching framework it extends a previous version of the system that focused on combining multiple matching strate gies through risk minimization of bayesian decision the new version quantitatively estimates the similarity characteristics for each matching task these characteristics are used for dynamicly selecting and combining the multiple matching methods two basic matching methods are employed linguistic similarity edit distance over entity labels vector distance among comments and in stances of entities and structural similarity a variation of similarity flooding implemented as three similarity propagation strategies concept to concept property to property and concept to property in turn the strategy selection uses label and structure similarity factors obtained as a preprocessing of the ontologies to be matched in order to determine what information should be employed in the matching process specifically the strategy selection dynamically regulates the concrete feature selection for linguistic matching the combination of weights for similarity combination and the choice of the concrete similarity propagation strategy after similarity propagation the matching process concludes with align ment refinement and extraction of the final result asmov infotech soft inc u of miami automatic semantic matching of ontologies with verifica tion asmov is an automatic approach for ontology matching which targets information integration for bioinfor matics overall the approach can be summarized in two steps similarity calculation and semantic verification it takes as input two owl ontologies and an optional input alignment and returns as output an n m alignment between ontology entities classes and proper ties in the first step it uses lexical string equality a variation of levenshtein distance structural weighted sum of the domain and range similarities and extensional matchers to iteratively compute similarity measures be tween two ontologies which are then aggregated into a single one as a weighted average it also uses several sources of general and domain specific background knowledge such as wordnet and umls to provide more evidence for similarity computation then it derives an alignment and checks it for inconsistency consistency checking is pattern based i e that instead of using a complete solver the system recognizes sets of correspondences that are proved to lead to an inconsistency the semantic verification process examines five types of patterns e g disjoint subsumption contradiction subsumption incompleteness this matching process is repeated with the obtained alignment as input until no new correspondences are found anchor flood toyohashi u of technology the anchor flood approach aims at handling efficiently particularly large ontologies it inputs ontologies in rdfs and owl and outputs alignments the system starts with a pair of similar concepts from two ontologies called an anchor e g all exactly matched normalized concepts are considered as anchors then it gradually proceeds by analyzing the neighbors i e superconcepts subconcepts siblings of each anchor thereby building small segments fragments out of the ontologies to be matched the size of the segments is defined dynamically starting from an anchor and exploring the neighboring concepts until either all the collected concepts are explored or no new matching pairs are found the system focuses on local segment to segment comparisons thus it does not consider the entire ontologies which improves the system scalability it outputs a set of correspondences between concepts and properties of the semantically connected segments for determining the correspondences between segments the approach relies on terminological wordnet and winkler based string metrics and structur al similarity measures which are further aggregated by also considering probable misalignments the similarity between two concepts is determined by the ratio of the number of terminologically similar direct superconcepts on the number of total direct superconcepts retrieved local matching pairs are considered as anchors for further processing the process is repeated until there is no more matching pairs to be processed agreementmaker u of illinois at chicago agreementmaker is a system comprising a wide range of automatic matchers an extensible and modular architec ture a multipurpose user interface a set of evaluation strategies and various manual e g visual comparison and semiautomatic features e g user feedback it has been designed to handle large scale ontologies based on the requirements coming from various domains such as the geospatial and biomedical domains the system handles ontologies in xml rdfs owl and outputs m n n m alignments in general the matching process is organized into two modules similarity computation and alignment selection the system combines matchers using three layers the matchers of the first layer compare concept features such as labels comments instances which are represented as tfáidf vectors used with a cosine similarity metric other string based measures e g edit distance substrings may be used as well shvaiko and euzenat ontology matching state of the art and future challenges the second layer uses structural ontology properties and includes two matchers called descendants similarity inheritance if two nodes are matched with high similarity then the similarity between the descendants of those nodes should increase and siblings similarity contribution which uses the relationships between sibling concepts at the third layer a linear weighted combination is computed over the results coming from the first two layers whose results are further pruned based on thresholds and desired output cardinalities of the correspondences the system has a sophisticated user interface deeply integrated with the evaluation of ontology alignment quality being an integral part of the matching process thus empowering users with more control over it analytical summary the following can be observed concerning the considered systems sections and see also table the approaches equally pursue the development of generic matchers e g falcon rimom anchor flood as well as those focusing on particular applica tion domains e g sambo asmov that target primarily biomedical applications most of the systems under consideration declare to be able to handle efficiently large scale ontologies i e tens of thousands of entities see some experi mental comparisons in section this is often achieved through employing various ontology par titioning and anchor based strategies such as in falcon dssim or anchor flood although all systems can deal with owl being an oaei requirement many of them can be applied to rdfs or skos most of the systems focus on discovering alignments but yet several systems are able to discover n m alignments moreover most of the systems focus on computing equivalence relations with the exception of dssim which is also able to compute subsumption relations many systems are not equipped with a graphical user interface with several exceptions such as sambo dssim and agreementmaker semantic and extensional methods are still rarely employed by the matching systems in fact most of the approaches are quite often based only on terminological and structural methods many systems have focussed on combining and extending the known methods for example the most popular of these are variations of edit distance and wordnet matchers as well as iterative similarity propagation as adaptation of the similarity flooding algorithm thus the focus was not on inventing fundamentally new methods but rather on adapting and extending the existing methods r ecent m atching e valuations we provide a comparative experimental review of the matching systems described previously section in order to observe and measure empirically the progress made in the field we base our analysis on the ontology alignment evaluation initiative and more precisely its campaigns oaei is a coordinated international initiative that organizes annual evaluations of the increasing number of matching systems it proposes matching tasks to participants and their results are evaluated with measures inspired from information retrieval these are precision which is a measure of correctness recall which is a measure of completeness and f measure which aggregates them we consider here the three oldest test cases of oaei in order to have a substantial set of data for comparison as well as diversity in tasks from automatically generated test cases to expressive ontologies these are benchmarks section web directories section and anatomy section participants were allowed to use one algo rithm and the same set of parameters for all the test cases beside parameters the input of the algorithms must be two ontologies to be matched and any general purpose resource available to everyone i e resources designed especially for the test cases were not allowed see for further details benchmarks the goal of the benchmark test case is to provide a stable and detailed picture of each matching algorithm for that purpose the algorithms are run on systematically gener ated test cases test data the domain of this test case is bibliographic references it aims at comparing an owl dl ontology containing more than entities with its variations most of the variations are obtained by discarding features of the original ontology other variations select either unrelated ontologies or other available ontologies on the same topic evaluation results a comparative summary of the best results of oaei on the benchmarks is shown in fig edna is a simple edit distance algorithm on labels which is used as a baseline for we maximized the results of the two best systems fujitsu and promptdiff the two best systems of the last several years are asmov and lily their results are very comparable a notable progress has been made between and by falcon and the results of were repeated in by both asmov and lily fig benchmarks comparison of matching quality results in more systems are mentioned in the figure with respect to those presented in section the results of these systems are given for the completeness of the presentation see for further details ieee transactions on knowledge and data engineering vol no january directory the directory test case aims at providing a challenging task in the domain of large directories constructed from google yahoo and looksmart web directories these directories have vague terminology and modeling principles thus the matching tasks incorporate the typical uncontrolled open web modeling and terminological errors the test case was built following the methodology test data the data set is presented as taxonomies where the nodes of the web directories are modeled as classes and the classification relation connecting the nodes is modeled as rdfs subclassof there are more than node matching tasks where each node matching task is composed from the paths to the root of the nodes in the web directories evaluation results a comparison of the results in for the top systems of each year based on the highest f measure is shown in fig a key observation is that from to we can measure a continuous improvement of the results while in and the participating systems have either maintained or decreased their f measure values the quality of the best f measure result of achieved by asmov is higher than the best f measure of demonstrated by dssim it is higher than that of by falcon it equals to that of prior and is still lower than the best f measure of by ola anatomy the focus of this test case is to confront existing matching technology with expressive and realistic ontologies in the biomedical domain two of its specificities are the specia lized vocabulary of the domain and the usage of owl modeling capabilities test data the ontologies are part of the open biomedical ontologies obo designed from the nci thesaurus classes describing the human anatomy published by the national cancer institute and the adult mouse anatomical dictionary classes this test case has been used since while in and it was run on a different test data which we do not consider here and focus on the more recent results instead fig directory comparison of matching quality results in more systems are mentioned in the figure with respect to those presented in section the results of these systems are given for the completeness of the presentation see for further details evaluation results a comparison of the results in for the top systems of each year based on the highest f measure is shown in fig we can make two key observations the first one is that a baseline label matcher based on string equality already provides quite good results with f measure of the second one is that in all the years the best f measure remained stable around of however some progress have been made in terms of efficiency i e the runtime reduced from days and hours to minutes and seconds for example the best runtime result of in belongs to anchor flood its f measure was while in and the competition was clearly dominated by the aoas and sambo systems that were heavily exploiting background knowledge umls in turn in the best result showed by sobom was obtained without using any background knowledge finally in the best result was shown by agreementmaker experimental summary as we can see from the previous sections sections and various sets of systems participate on various test cases but not necessarily on all of these not all the systems participated every year which prohibits measuring com prehensively the progress of each system over the years in table when available we report the f measure obtained by these systems and the respective progress or regress made from table we conclude that individual systems improve over the years on the same test cases an exception includes rimom on the directory test case what can be explained by the new release of the system which still required tuning see better matching quality on one task is not achieved at the expense of another task on the average the overall average improvements made by the individual systems on the test cases under consid erations reach percent increase or percentage points by dssim in the recent years fig anatomy comparison of matching quality results in more systems are mentioned in the figure with respect to those presented in section the results of these systems are given for the completeness of the presentation see for further details shvaiko and euzenat ontology matching state of the art and future challenges table the progress made by some systems over the recent years for each year we report the f measure indicator obtained by the systems on three test cases benchmarks directory and anatomy the empty cells mean that the corresponding systems did not participate on a test case in a particular year the æ column stands for the progress regress made over the years calculated as a percentage increase between the first and the last participation e g for sambo on benchmarks resulting in þ the last column shows the average progress made by the systems over different years on different test cases calculated as the average over b d a e g for agreementmaker this results in þ an average progress over the oaei participants that support a similar or a stronger growth in the forthcoming have been made in the three test cases considered from the years some specific actions have to be taken in particular early years to the recent years is of percent in terms of we propose to guide the evolution of the ontology matching f measure the average of all progression reported in field by addressing some specific challenges table i e an increase of percentage points on f with respect to the third question we offer eight measure moreover on the anatomy test case the runtime challenges for ontology matching see table the improved times on average from mn about challenges under consideration are among the major ontol hours in to mn in see for an in ogy matching topics of the recent conferences in semantic depth discussion thus measurable progress is observed web artificial intelligence and databases in terms of effectiveness and efficiency made by the if the design of matchers consists of tuning further automatic ontology matching systems in the recent years similarity measures or issuing other combinations of at present in the database community there are no well matchers it is not to be expected a revolutionary progress established benchmarks for comparing schema matching but most likely only an incremental one as section also tools however there are many recent database schema suggests other open issues are the computation of matching tools and more generally model management expressive alignments e g correspondences across classes infrastructures e g coma agreementmaker and properties oriented alignments with none gerome harmony that are able also to process quivalence relations or cross lingual matching ontologies and hence might be interested to test them within oaei as actually already happens though mod estly on the other hand oaei has to consider including database schema matching tasks involving xml and relational schemas in order to improve the cross fertilization between these communities 72 such issues are gradually progressing within the ontology matching field in the first years of oaei it was not possible to test such alignments because there was not enough matching systems able to produce them only recently oriented matching data sets were introduced and there are more systems able to produce complex corre spondences moreover we consider these issues as too t oward the c hallenges specific with respect to the other challenges discussed so we did not retain them as challenges after years of work on ontology matching several ques tions arise is the field still making progress is this progress significant enough to pursue further research if so what are the particularly promising directions the previous section showed that the field is indeed making measurable progress but the speed of this progress is slowing down and becomes harder to determine also breakthroughs can come from either completely different settings or classes of systems particularly adapted to specific applications we can seek for such improvements from recovering background knowledge section for example from the linked open data cloud as it represents a large and continuously growing source of knowledge another source of quality gains is expected from the working environment the need for matching has risen in many different fields in which matching is performed hence work on involving which have diverging demands for instance design time users in matching section or social and collaborative matching with correct and complete alignments e g required when two banks merge versus runtime matching with approximate alignments e g acceptable in query answering on the web this calls for more precise and table applications versus challenges more versatile evaluations of matchers the second question interrogates the significance of the obtained results this question requires measurements as well the oaei evaluations measured the progress of percentage points in the recent five six years section this is a sufficient result compared to other fields of computer science to peruse further research into it also we can see from the benchmarks that after a few years the improvement rate is slowing down hence in order to the checkmarks indicate the primarily impact of the challenges under consideration on two broad types of applications ieee transactions on knowledge and data engineering vol no january matching section may provide surprising results the l arge s cale m atching e valuation challenges have been selected by focusing on pragmatic issues that should help consolidating the available work in the field bringing tangible results in the short medium period thus leaving most of the theoretical and less promising directions aside for example in we also identified as challenges uncertainty in ontology matching and reasoning with alignments 76 these are challenging theoretical issues but they have a long term impact hence we do not discuss them here the growth of matching approaches makes the issues of their evaluation and comparison more severe in fact there are many issues to be addressed in order to empirically prove the matching technology mature and reliable the challenge large tests involving and entities per ontology are to be designed and conducted in turn this raises the issues of a wider automation for acquisition of reference alignments e g by another point worth mentioning is the rise of linked data minimizing the human effort while increasing an evaluation and the subsequent need for data interlinking data set size ontology matching can take advantage of linked data as we believe that the point of large scale evaluation is of an external source of information for ontology matching prime importance though there are some other issues this is fully relevant to the matching with background around ontology matching evaluation to be addressed as knowledge challenge conversely data interlinking can well benefit from ontology matching by using correspondences to focus the search for potential instance level links oaei since reacted to this need by hosting a specific instance matching track however data interlinking is a more specific topic which is out of scope of this paper the challenges are articulated as follows we start with the issue of evaluating ontology matching section since this theme has had a large impact on the development of matchers in recent years and it shows their practical more accurate evaluation quality measures beside precision and recall are needed application specific measures have to be developed in order to assess whether the result of matching e g f measure of or percent is good enough for a particular application such as navigation among collections of masterpieces in the cultural heritage settings section or web service matching this usefulness then the next three challenges sections should help quantifying more precisely the useful and are concerned with creating better more effective ness and differences between matching systems in and efficient automatic matching technology and cover some hard metrics such as development time respectively such aspects as efficiency of ontology interoperability benchmarks testing the ability of matching techniques section involving background exchanging data without loss of information be knowledge section matcher selection combination and tween the ontology matching tools have to be self configuration section these problems have designed and conducted become prevalent with the advent of applications requiring a methodology and test cases allowing for a run time matchers in turn sections and consider comparative evaluation of instance based matching matchers and alignments in their relation with users and approaches have to be designed and conducted respectively cover how and where to involve users of the matching technology section and what explanations of matching results are required section moreover users can be considered collectively when working collaboratively on alignments section this in turn requires an alignment infrastructure for sharing and recent advances oaei campaigns gave some prelimin ary evidence of the scalability characteristics of the ontology matching technology for example most of the test cases of oaei dealt with thousands of matching tasks with an exception of the very large cross lingual resources test case reusing alignments section solving these problems of oaei similar observations can be made as well would bring ontology matching closer to final users and with respect to individual matching evaluations e g more prone to fill their needs to understand better the most pressing issues for the below we summarize the recent advances along the three different types of applications section table crosses the challenges identified and two broad types of applica previous issues tions i e those requiring design time and runtime half of initial steps toward better evaluation measures have the challenges are largely important for both design and already been done by proposing semantic versions runtime applications while the other half is primarily of precision and recall implemented in and important either to one or another type of applications in the alignment api in turn an early thereby showing commonalities and specificities of these attempt to introduce application specific measures for example efficiency of ontology matching techniques is was taken in the library test case a variation of the vital for runtime applications while involving background knowledge matcher selection and self configuration are crucial for improving quality precision recall of matching results in both design and runtime applications each of the challenges is articulated in three parts definition of the challenge overview of recent advances that complement those discussed in section and discussion of potential approaches to tackle the challenge cultural heritage case in section of oaei this is similar to the task based evaluation used in ontology learning despite efforts on metamatching systems on com posing matchers on the alignment api and in the project promoting automation of evaluations in particular for ontology under consideration semantic evaluation at large scale http www seals project eu shvaiko and euzenat ontology matching state of the art and future challenges www linkeddata org matching the topic of interoperability between in overall the challenge is to come up with scalable matching tools remains largely unaddressed ontology matching reference solutions the theme of a comparative evaluation of instance recent advances as section indicates the issue of based matching approaches is in its infancy some efficiency was addressed explicitly by many recent systems test cases that have been used in the past can be however for instance in the anatomy track of oaei found in while a recent approach only a few systems such as falcon section took toward a benchmark for instance matching was several minutes to complete this matching task while other proposed and implemented in oaei and systems took much more time hours and even days in oaei 58 96 oaei anchor flood section managed to solve it discussion a plausible step toward large scale ontology in seconds in the very large cross lingual resources test matching evaluation was taken within the very large cross case of oaei only dssim section took part out of lingual resources test case of oaei in particular it involved matching among the following three resources wordnet which is a lexical database for english dbpedia which is a collection of things each tied to an article in the english language wikipedia gtaa participants though the input files were manually split into fragments and then the matching system was applied on the pairs of these fragments discussion efficiency issues can be tackled through a number of strategies including which is a dutch thesaurus used by the netherlands institute for sound and vision to index tv programs the number of entities involved from each of the resources are 180 and 160 respectively oaei made one step further by having specific instance matching track where the whole linked open data was involved finding a large scale real world test case is not enough parallelization of matching tasks e g cluster comput ing distribution of matching tasks over peers with available computational resources approximation of matching results which over time become better e g more complete modularization of ontologies yielding smaller more for running an evaluation a reference alignment against targeted matching tasks which the results provided by matching systems has to be optimization of existing and empirically proved to created a typical approach here is to build it manually be useful matching methods however the number of possible correspondences grows quadratically with the number of entities to be compared this often makes the manual construction of the reference correspondences demanding to the point of being infeasible to our knowledge the first two items above remain largely unaddressed so far and thus will have to be covered in future there are tasks such as matching very large cross lingual resources of oaei which the existing matching for large scale matching tasks a semiautomatic approach technology cannot handle automatically the resources were to the construction of reference alignment has been too large more computing power does not necessarily proposed in which can be used as a starting point improve matching quality but at least at the beginning it it remains difficult to know which matcher fits best to would accelerate the first run and the analysis of the which task or application to this end the notion of hardness bottlenecks to this end the approaches taken in the larkc for matching identifying the degree of difficulty of a project to realize the strategies mentioned previously particular test would be useful this would allow for e g through divide conquer swap strategy which extends automatically generating tests with particular characteris the traditional approach of divide and conquer with an tics of required hardness this would also allow for iterative procedure whose result converges toward com defining test profiles specifying data set characteristics pleteness over time should be looked for and adapted to and measures for different types of applications ontology matching the existing work mainly focused on the last three items below we give insights on potential e fficiency of m atching t echniques further developments of the themes of approximation modularization and optimization besides quality the efficiency of matchers is of prime the complexity of matching in a pairwise set up is importance in dynamic applications especially when a usually proportional to the size of the ontologies under user cannot wait too long for the system to respond or when consideration and the number of matching algorithms memory is limited current ontology matchers are mostly employed a straightforward approach here is to reduce design time tools which are usually not optimized for the number of pairwise comparisons in favor of a resource consumption n incomplete top down strategy as implemented in the challenge the execution time indicates efficiency qom or to avoid using computationally expensive properties of matchers however good execution time can matching methods such as in rimom by suppressing be achieved by using a large amount of main memory or the structure based strategies and by applying only a bandwidth taking on par the other computational resources simple version of the linguistic based strategies such as cpu another worthwhile direction to avoid exhaustive thus usage of main memory should also be measured or pairwise comparisons which appears to be particularly improved moreover we can expect the need for matching promising when handling large ontologies is based on on handheld computers or smartphones in the near future segment based approaches e g coma and an chor flood section thus targeting at matching only the similarly enough segments this theme has to be further ieee transactions on knowledge and data engineering vol no january and more systematically developed it is also worth investigating how to automatically partition large ontolo gies into proper segments the efficiency of the integration of various matchers can be improved by minimizing with the help of clustering such as in porsche and xclust the target search space for a source ontology entity optimizations are worth performing only once the underlying basic techniques are stable for example in the case of s match the matching problem was reduced to the validity problem for the propositional calculus the basic version of s match used a standard satisfiability procedure of once it has been realized that the approach is promising based on preliminary evaluations the efficiency problems were tackled specifi fig use of background knowledge in scarlet the process is cally for some frequent practical cases e g when the propositional formula encoding a matching problem appears to be horn satisfiability can be tested in linear made of two steps finding an ontology referring to the concepts to be matched inferring a relation between these concepts in function of those of the background ontology time while a standard propositional satisfiability solver would require quadratic time finally the logmap approach uses an incomplete reasoner as well as a number of optimizations to obtain the results faster thereby exploiting several strategies to improve efficiency m atching with http www org using domain specific ontologies e g in the field of anatomy upper level ontologies 114 or all the ontologies available on the semantic web such as in the work on scarlet in addition the work on s match discussed an automatic approach to deal with the lack of background b ackground k nowledge knowledge in matching tasks by using semantic matching one source of difficulty for matching is that ontologies are designed in a particular context with some background knowledge which often do not become part of the final ontology specification the challenge matching can be performed by discover ing a common context or background knowledge for the ontologies and use it to extract relations between ontology entities this context can take different forms such as a set of resources web pages pictures etc which have been annotated with the concepts from an ontology which provides common anchors to the ontologies to be matched the difficulty is a matter of balance adding context provides new information and hence helps increasing recall but this new information may also generate incorrect matches which decreases precision recent advances various strategies have been used to iteratively on top of s match the work in discussed the use of umls instead of wordnet as a source of background knowledge in medical applications the techniques mentioned above have helped improving the results of matchers in various cases for instance fig shows two entities from the and thesauri that had to be matched in the food test case of oaei when considering concepts beef and food the use of background knowledge found on the web such as the ontology helps deduce that beef is less general than food the same result can be also obtained with the help of wordnet since beef is a hyponym is a kind of food thus multiple sources of background knowledge can simulta neously help discussion the techniques mentioned before can undergo different variations based on deal with the lack of background knowledge in particular the way background knowledge sources are identi declaring missing axioms manually as a prematch effort cupid coma a cluster based approach proposed in or using partial input alignments sambo reusing previous matches coma more generally storing and sharing existing alignments can be used for composing alignments which helps solving part of the matching problem using the web as background knowledge and specifically exploiting linked data as background knowledge or the work on search engine weighted approximate matching using domain specific corpora of schemas and fied to be useful e g if there are enough entities in common for a particular matching task the way background knowledge sources are se lected i e given multiple sources such as domain specific ontologies and upper level ontologies identified in the former step selecting one or a combination of these to use the way ontology entities are matched against the background knowledge sources e g by employing simple string based techniques or more sophisti cated matchers the way the obtained results are combined or aggregated e g by majority voting mappings 111 or schema covers http www fao org aims htm http www nal usda gov http rdf vrp examples tap rdf shvaiko and euzenat ontology matching state of the art and future challenges once the necessary knowledge has been recovered e g through a composition of several auxiliary resources the issue is how to maintain it several alternatives can be explored including extending privately or locally general purpose resources such as wordnet or schema org toward specific domain knowledge sharing the recov ered knowledge publicly as linked open data the insights provided above have to be systematically investigated combined in a complementary fashion and evaluated this is particulary important in dynamic set tings where the matching input is often shallow especially when dealing with fragmented descriptions and therefore incorporates fewer clues to this end it is vital to identify the minimal background knowledge necessary e g a part of tap in the example of fig to resolve a particular problem with sufficiently good results m atcher s election c ombination and t uning many matchers are now available as oaei campaigns indicate section there is no single matcher that clearly dominates others often these perform well in some cases and not so well in some other cases both for design and runtime matching it is necessary to be able to take advantage of the best configuration of matchers the challenge there is evidence from oaei section that matchers do not necessarily find the same correct correspondences usually several competing matchers are applied to the same pair of entities in order to increase evidence toward a potential match or mismatch this requires to solve several important problems selecting matchers and combining them and self configuring or tuning matchers on top of this for dynamic applications it is necessary to perform matcher combination and self tuning at runtime and thus efficiency of the configuration search strategies becomes critical as the number of available matchers increases the problem of their selection will become more critical e g when the task will be to handle more than matchers within one system recent advances the problem of matcher selection has been addressed for example through analytic hierarchy process ad hoc rules or a graphical matching process editor often the matcher selection is tackled by setting appropriate weights in to matchers that are predefined in a pool of usually at most several dozens of matchers and to be further aggregated so far mostly design time toolboxes allow to do this manually another approach involves ontology metamatching i e a framework for combining a set of selected ontology matchers instead of least square linear regression as in the work in uses a machine learning technique called boosting the adaboost algorithm in order to select matchers from a pool to be further used in combination multiagent techniques have also been used for that purpose e g exploits the max sum algorithm to maximize the utility of a set of agents while uses argumentation schemes to combine matching results the work in proposed an approach to tune a library of schema matchers at design time given a particular matching task it automatically tunes a matching system by choosing suitable matchers and the best parameters to be used such as thresholds the work in discussed consensus building after many methods have been used discussion the above mentioned problems share com mon characteristics the search space is very large and the decision is made involving multiple criteria resol ving these two problems simultaneously at runtime makes ontology matching even harder the work on evaluation section can be used in order to assess the strengths and the weaknesses of individual matchers by comparing their results with task requirements often there are many different constraints and requirements applied to the matching tasks e g correctness completeness execution time main memory thereby involving multidecision criteria the main issue is the semiautomatic combination of matchers by looking for complementarities balancing the weaknesses and reinfor cing the strengths of the components for example the aggregation is usually performed following a predefined aggregation function such as a weighted average novel ways of performing aggregation with provable qualities of alignments have to be looked for in order to go beyond the incremental progress that we observed in the recent years for example one of the plausible directions to pursue was investigated in which proposed to use a decision tree as an aggregation function where the nodes represent the similarity measures and edges are used as conditions on the results such a decision tree represents a plan whose elementary operations are matching algo rithms further issues to be addressed include investigat ing the automatic generation of decision trees based on an application domain in the web setting it is natural that applications are constantly changing their characteristics therefore ap proaches that attempt to tune and adapt automatically matching solutions to the settings in which an application operates are of high importance this may involve the runtime reconfiguration of a matcher by finding its most appropriate parameters such as thresholds weights and coefficients the above mentioned work in also contributed to the theme of tuning specifically since edges in the decision tree are used as conditions these can be viewed as thresholds personalized to each matcher thus various ways of encoding the matcher combination and tuning problem have to be explored and developed further u ser i nvolvement in traditional applications the result of matching performed at design time is screened by human users before being accepted however the overwhelming size of data may render this task difficult in dynamic applications users are generally not ontology matching specialists who can be asked to inspect the alignments hence in both cases user involvement becomes crucial the challenge is to design ways of involving users so that they can help the matching process without being lost in the amount of results the issue is both for design and ieee transactions on knowledge and data engineering vol no january runtime matching to design interaction schemes which are burdenless to the user at design time interaction should be both natural and complete at runtime it should be hidden in the user task recent advances so far there have only been few studies on how to involve users in ontology matching the works in proposed to use query logs to enhance match candidate generation several efforts were dedicated to design time matcher interaction such as in some recent works have focussed on the ergonomic aspect of elaborating alignments either for designing them manually or for checking and correcting them e g through learning 135 specifically the work in proposed a graphical visualization of alignments based on cognitive studies in turn the work in provided an environment for manually designing complex alignments through the use of connected perspective that allows for quickly deemphasizing non relevant aspects of the ontolo gies being matched while keeping the connections between relevant entities the work in provided the clip tool that allows for explicitly specifying structural transforma tions by means of a visual language in addition to value couplings to be associated with correspondences discussion with the development of interactive ap proaches the issues of their usability will become more prominent this includes scalability of visualization and better user interfaces in general which are expected to bring higher quality gains than more accurate matching algorithms an interesting trend to follow concerning user involve ment relies on final users in order to learn from them given a matching task what is the best system configuration to approach that task moreover for dynamic applications only the final user can help this can be exploited by adjusting matching system parameters section or by experiment ing with alignment selection strategies in order to facilitate this matching tools have to be configurable and customiz able users themselves could improve these tools thereby arriving to the exact solution that best fits their needs and preferences when users are given this freedom by working on tool customization they can also provide useful feedback to system designers involving final users in an active manner in a matching project would increase its impact as users who recognize the actual need also have promising ideas on how to approach it when these lead users want something that is not available on the market high benefits may be expected from such endeavors technically a basic premise underlying user interaction design is that users of a matching system should be able to influence the search for an optimal alignment on various levels via unified interfaces for example by recommend ing relevant background knowledge in advance by influencing the selection and weighting of the various matching components by criticizing aspects of intermediate results and by determining whether the final result is good enough to be put to use little attention has been devoted so far to the realization of interfaces that actually allow users to become active in these ways systems should be developed on the basis of continual tests with final users and the ultimate success criterion will be the extent to which the system has value for them finally as more systems will become equipped with guis see table we expect that evaluation of usability and customizability of such systems will become more prominent e g included as evaluation indicators of the oaei campaigns e xplanation of m atching r esults in order to better edit alignments thereby providing feedback to the system users need to understand them it is often not sufficient that a matcher returns an alignment for users to understand it immediately in order for matching systems to gain a wider acceptance and to be trusted by users it will be necessary that they provide explanations of their results to users or to other programs that exploit them notice that the issues of trustworthiness and provenance become particularly important in the web settings that enable social and collaborative matching section the challenge is to provide explanations in a simple yet clear and precise way to the user in order to facilitate informed decision making in particular many sophisti cated techniques used by matching systems e g machine learning or discrete optimization do not yield simple or symbolic explanations recent advances there are only a few matching systems able to provide an explanation for their results 142 the solutions proposed so far focus on default explanations explaining basic matchers explaining the matching process and negotiating alignments by argumentation more recently the work in introduced the notion of a matchability score computed via a synthetic workload which quantifies how well on average a given schema matches future schemas using the matchability score different types of matching mistakes such as missing a correspondence or predicting a wrong correspondence can be analyzed e g by looking into the most likely general reasons leading to them once the matchability score has been computed for all the entities they are ranked by increasing scores a matchability report is generated for each entity by grouping incorrect matches based on their under lying general reasons and by displaying the reason an example to illustrate the reason and a suggestion of revisions to be made thereby guiding users in revising correspondences by addressing the reported mistakes finally provided a mapping design wizard that uses data examples to systematically assist integration engineers in explaining and refining alignments toward a desired specification the key intuition behind it is that integration engineers usually understand better their data than align ments hence such data examples are used in explanations of nuances to clarify possible variations in interpretation of alignments by including these examples into a small number of automatically generated yes or no questions discussion few works have addressed the theme of explanations in ontology matching therefore the direc tions pursued in those works are worth considering in addition an interactive environment is still needed to help users accept or revise the suggested correspondences see section in this respect it would be also useful to exploit shvaiko and euzenat ontology matching state of the art and future challenges the abstraction techniques extensively and build on top have been analyzed a standard volunteering scheme and a of the work on explanations in recommender systems scheme in which users have to pay by answering first in the longer term it would be helpful to standardize several questions in order to use a desired service explanations of matching results in order to facilitate the discussion a promising way of tackling the matching interaction of matching systems with other applications task is by taking advantage of the network effect if it is too cumbersome for one person to come up with a correct s ocial and c ollaborative m atching in an open environment like the web social support has been the key in solving hard and large problems this approach can be also applied to ontology matching the challenge matching could be improved through social interaction this may be obtained with the help of people explicitly arguing about correspondences or by implicitly voting each time a correspondence is used during an interaction this calls for algorithms able to rank a massive amount of correspondences the incompleteness and inconsistency of alignments will have to be dealt with in a satisfactory way other issues include understanding what tasks are relatively easy for humans but difficult for machines how to individuate and deal with malicious users and which incentive schemes promise to facilitate user participation in establishing alignments collaboratively recent advances the work in extended the notion of ontology matching to community driven ontology matching and discussed early experiments in which a community of people can share alignments over the web reuse them as well as argue about them by using alignment between several pairs of ontologies this can be more easily resolved by many people together namely each person has to do a very small amount of work each person can improve on what has been done by others and errors remain in minority thus the process and the dynamics of crowdsourcing and collaborative ontology matching in the context of various applications should be studied and formalized some steps in this direction have already been taken within the reference alignment consensus building sessions of the ontology matching workshops in general the experi ences with collaborative knowledge construction and in particular with the collaborative development of ontol ogies 151 as well as with the community information management systems should be monitored and whenever promising adapted for collaborative matching the success of the social and collaborative matching techniques will largely depend on the creation of a critical mass of users in communities of interest that actually use them similar to what happens to any data on the web once an alignment has been established either annotations technically this meant among other things manually or automatically it should be publicly shared extending correspondences with metadata covering the thereby enabling its further reuse section in turn this correspondence author name application domain his or requires an adequate support for handling trustworthiness her trust value in a community etc and provenance of alignments section the work in proposed a model the mapping ontology for representing correspondences called map pings collected from the user community and the metadata left network right corre sponding adjacency matrix build on a novel observation that overlaps between communities are densely connected this is in sharp contrast with present com munity detection methods which implicitly assume that overlaps between communities are sparsely connected and thus cannot prop erly extract overlapping communities in networks in this paper we develop a model based community detection algorithm that can detect densely overlapping hierarchically nested as well as non overlapping communities in massive networks we evaluate our al gorithm on large social collaboration and information networks with ground truth community information experiments show state of the art performance both in terms of the quality of detected com munities as well as in speed and scalability of our algorithm categories and subject descriptors h database manage ment database applications data mining general terms algorithms theory experimentation keywords network communities overlapping community detec tion matrix factorization communities are often interpreted as organizational units in social networks functional units in biochemical networks ecological niches in food web networks or scientific disci plines in citation and collaboration networks even though methods for identifying overlapping as well as hierar chically nested communities in networks have been considered in the past identifying meaningful communities in large networks has proven to be a challenging task most methods have trouble scaling to large networks and the lack of reliable ground truth makes evaluation of detected communities surprisingly difficult thus while networks have been extensively studied and the existence and properties of communities in small networks is by now well understood it is still not clear how to identify realistic overlapping communities in very large net works that are increasingly common present work empirical observations our work starts with a novel and in retrospective very intuitive observation that over introduction a large body of work in computer science statistics applied mathematics and statistical physics has been devoted to identify ing community structure in complex networks see for surveys of this area a community also referred to as a mod ule or a cluster is intuitively thought of as a group of nodes with more interactions amongst its members than between its members and the remainder of the network such groups of nodes i e laps of communities tend to be more densely connected than the non overlapping parts in particular we empirically ob serve that the more communities a pair of nodes shares the more likely they are connected in the network for example people sharing multiple hobbies i e interest based communities have a higher chance of becoming friends researchers with many common interests i e many common scientific communities are more likely to work and publish together even though intuitive our observation is very subtle and repre sents a radical new view of networks communities and has impor tant consequences for network community detection to permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are put our observation in the context we first give a quick overview of recent developments in the network community detection tra not made or distributed for profit or commercial advantage and that copies ditionally the emergence of communities in networks has been un bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee wsdm february rome italy copyright acm derstood through the strength of weak ties theory this theory led researchers to conceptualize networks as consisting of dense clusters that are linked by a small number of weak ties figure a graph partitioning modularity optimization b sparse overlaps as well as betweenness centrality based community detection methods all assume such view of network communities and thus aim to identify edges that can be cut in order to separate the net work into a set of non overlapping clusters in social as well as other types of networks nodes can belong to multiple communities simultaneously which leads to overlapping community structure however we noticed that practically all present overlapping community detection methods for example make a hidden and so far undocumented assumption that community overlaps are less densely connected than the non overlapping parts of communities figure b this leads to an unnatural modeling assumption that the more communities a pair of nodes shares the less likely it is they are connected fig ure b illustrates the unnatural structure of community overlaps emerging under such assumption in contrast we find an increasing relationship between the num ber of shared communities and the probability of nodes being con nected by an edge a direct consequence of this observa tion is that parts of the network where communities overlap tend to be more densely connected figure c even though very natural the observation stands in sharp contrast to present defini tions of network communities more importantly today commu nity detection methods for example cannot correctly identify such dense community overlaps present methods either mistakenly identify the overlap as a separate community or merge two overlapping communities into a single one present work large scale community detection via matrix fac torization building on the above observation the goal of this work is to detect communities in a given large unlabeled undirected net work this means that for every node in a given large undirected network we aim to discover the communities it belongs to to achieve this we develop a novel community detection method that allows for discovering any combination of densely overlapping non overlapping as well as hierarchically nested communities we build on models of affiliation networks and develop the b igclam cluster affiliation model for big networks in b ig c lam communities arise due to shared community affiliations of nodes we explicitly model the affiliation strength of each node to each community we assign each node community pair a non negative latent factor which represents the degree of membership of a node to the community we then model the probability of an edge between a pair of nodes in the network as a function of the shared community affiliations we identify network communities by fitting the b ig c lam model to a given large undirected network our goal is to estimate non negative latent factors that model the membership strength of each node to each community by combining the state of the art non negative matrix factorization methods with block stochastic gradient descent we achieve gains both in the quality of de tected communities as well as in scalability of the method we improve by a factor of the size of the largest networks that over lapping community detection methods could process in the past an additional contribution of our work is improved evaluation so far community detection methods have mostly been evaluated anecdotally on small networks in contrast we identify social collaboration information and biological networks with explicitly labeled ground truth communities this allows for quantita tive evaluating by assessing how well detected communities corre spond to the ground truth communities experiments reveal that bigclam discovers overlapping as well as non overlapping community structure more accurately than present state of the art methods moreover b ig c lam scales well beyond the current overlapping community detection methods experi c ments show that b ig lam achieves near linear running time while other methods exhibit quadratic or exponential running time we process networks of more than million edges which improves by a factor of the size of the largest networks that overlapping community detection methods could process in the past bigclam improves over the current state of the art in both the scalability as well as the quality of detected communities code as well as all the data are available at http snap stanford edu related work our b ig c lam is an example of a bipartite affiliation network model affiliation networks have been extensively stud ied in sociology as a metaphor of classical social theory con cerning the intersection of persons with groups where it has been recognized that communities arise due to shared group affiliations in affiliation network models nodes of the social network are affiliated with communities they belong to and the links of the un derlying social network are then derived based on the node com munity affiliations whereas classical models assume binary node community affiliations in our model we also consider the strength of an affiliation which provides additional modeling flexibility bigclam formulates community detection as a variant of non negative matrix factorization nmf similar to nmf we aim to learn factors that can recover the adjacency matrix of a given network however bigclam has two important improve ments first most of nmf research pays relatively little atten tion to interpreting the latent factors the primary goal there is to estimate the missing entries of the matrix e g as in the netflix competition on the other hand bigclam aims to learn latent factors which represent community affiliations of nodes second instead of using a gaussian distribution or logistic link function we optimize the model likelihood of explaining the links of the observed network our formulation of likelihood al lows us to compute a gradient of the factor matrix in near constant time which is significant improvement over existing nmf meth ods where the complexity of computing such gradient is linear in the number of rows of the matrix i e nodes of the network in practice computing the gradient in near constant time makes our algorithm about times faster in terms of scalability most overlapping community detection methods scale to networks with at most thousands of nodes the largest network processed with overlapping community detection methods is a mobile phone network of nodes and million edges non overlapping community detection algorithms which solve a simpler problem have been applied to networks with millions of nodes our methods presented here can process networks with tens of millions of edges while also obtaining state of the art quality of detected communities empirical observation we motivate the development of our model by empirically study ing the structure of communities and community overlaps in net works we fist describe the network datasets with explicit ground truth communities and then present our empirical findings networks with ground truth communities to study the connec tivity structure of community overlaps we now describe networks with explicitly labeled ground truth communities to define such ground truth we collected large social information and collabo ration networks where nodes explicitly state their community mem berships defining ground truth communities will also help us later in evaluating the performance various methods section table dataset statistics n number of nodes e number of edges c number of communities s average community size a community memberships per node m denotes a million and k denotes one thousand on average of all communi ties overlap with at least one other community first we briefly describe the networks the first net works are online social networks the livejournal blogging com munity the friendster online network the orkut social network and the youtube social network users in these networks create groups which other users then join such groups are formed over specific interests hobbies affiliations and geographical regions for instance livejournal categorizes communities into the follow ing types culture entertainment expression fandom life style life support gaming sports student life and technology there are over communities with stanford in their name and they range from communities based around different classes student ethnic communities departments activity and interest based groups var sity teams etc we use such user defined groups as ground truth communities a user can belong to zero one or more ground truth communities and thus ground truth communities can overlap the largest network among these online social networks is friend ster which has million nodes billion edges and million ground truth communities we also consider the amazon product co purchasing network where the nodes represent products and edges connect commonly co purchased products each product i e node belongs to one or more hierarchically nested product categories we use each product category to define a ground truth community members of the same community share a common function or a role ground truth com munities in the amazon network can be overlapping or hierarchi cally nested last we also use the collaboration network of dblp where nodes represent authors actors and edges connect nodes that have co authored a paper since research communities stem around conferences or journals we use publication venues as ground truth communities in dblp the networks we consider show a nice range of scale in all mea sures table the size of networks ranges from hundreds of thou sands to hundreds of millions of nodes and edges and the number of ground truth communities varies from hundreds to millions last the networks represent a wide range of edge densities numbers of explicit communities as well as amounts of community overlap in our previous work we found the above definitions of ground truth to be reliable and robust in particular while the networks we consider here come from a variety of domains span a wide range of network sizes and edge densities we find our obser vations and results to be consistent and robust across all of them the consistency and robustness of results make us confident in our methodology and empirical observations in order to express all networks in a consistent way we repre sent each network as an unweighted undirected static graph be cause members of the same group may be disconnected in the net work we treat each connected component of the group as a sepa rate ground truth community we allow ground truth communities to overlap because a node can belong to multiple groups at once are available at http snap stanford edu dataset livejournal friendster orkut 93 youtube dblp amazon m m m m m m n m m 93 m m m m e m m k k k k c 72 86 86 s a k number of shared memberships livejournal friendster orkut youtube dblp k number of shared memberships amazon a edge probability orkut b orkut figure a normalized edge probability as a function of com mon memberships k probabilities are scaled so that maximum value over k is one b edge probability in the orkut network plotted as an absolute value we conclude overlaps are more densely connected than single communities observation community overlaps are dense having defined ground truth communities we now empirically study the structure of ground truth communities we find that ground truth communi ties heavily overlap on average of all communities overlap with at least one other community and only of community members belong to only that community we thus examine the structure of community overlaps by measuring the probability of a pair of nodes being connected given that they belong to k common communities i e the nodes reside in the overlap of same k com munities figure a plots this probability for all six datasets for visualization we scale each probability curve so that the maximum value of each curve over k is under the current assumption that overlaps are less dense than non overlaps the probability curves would decrease as k increases in contrast we notice an increasing relationship for all datasets i e the more communities a pair of nodes has in common the higher the probability of an edge this means that nodes residing in overlaps are more densely connected each other than the nodes in a single community to demonstrate how the edge probability changes as k increases we plot the edge probability without scaling measured in the orkut network as a function of the number of common communities k in figure b similar to all large networks orkut is extremely sparsely connected the background probability of a random pair of nodes being connected is the increase in edge prob ability is highly significant for example if a pair of nodes has communities in common the probability of an edge is nearly the edge probability increases by times from to as soon as the pair share two communities overall in all the datasets we consistently observe similar and robust behavior the probability of a pair of nodes being con nected approaches as the number of common communities in creases while in online social networks the edge probability exhibits a diminishing returns like growth in dblp it appears to follow a threshold like behavior discussion in retrospective the above observation is very intu itive and thus so much more surprising for pairs of nodes that belong to multiple common communities edges often exist due to one dominant reason thus nodes in the overlaps will have higher chance of being connected because they belong to multiple com munities many examples to support this for example people sharing multiple hobbies or belonging to several common institu tions have a higher chance of becoming friends researchers with many common interests are more likely to work together and proteins involved in multiple common functional modules are more likely to interact the observation that the probability of an edge increases as a function of the number of shared communities means that nodes in the overlap of two or more communities are more likely to be a b a community affiliation network a b f ub u b weight for affiliation figure a bipartite community affiliation graph cir cles communities squares nodes of the underlying network edges indicate node community memberships edges with zero weight are not shown b each affiliation edge from node u to community c has strength f uc f ua connected thus our finding suggests communities overlap as il lustrated in figure c where the overlap of the two communities is more densely connected than each single community however we note that our finding is in sharp contrast to the currently predom inant view of network communities which is based on two funda mental social network theories triadic closure and strength of weak ties this leads to the picture of network communi ties as illustrated in figure a which suggests that homophily in networks operates in small pockets where nodes gather in dense non overlapping clusters extending these two theories to the over lapping communities leads to the unnatural structure of community overlaps as illustrated in figure b community overlaps are less densely connected than the groups themselves our results show the contrary as a consequence this means that present overlapping community detection methods which rely on the assump tion of sparse overlaps fail to correctly identify dense community overlaps they would either merge two overlapping communities into a single cluster or identify the overlap as a separate cluster last we also note that the observation that community overlaps are denser than communities themselves nicely extends the notion of homophily in networks the strength of weak ties suggests that homophily in networks operates in small pockets where inside the pocket nodes link strongly among themselves and weakly to other pockets our work extends the understanding of homophily we are discovering pluralistic homophily where the similarity of one node to another is the number of shared affiliations not just their similarity along a single dimension this view of tie forma tion is consistent with the works of simmel on the web of affil iations and feld on focused organization of social ties in both of these views networks consist of overlapping tiles or social circles that serve as organizing principles of nodes in networks cluster affiliation model next we present the cluster affiliation model for big networks bigclam a probabilistic generative model for graphs that re liably captures the organization of networks based on community affiliations our model has three main ingredients the first ingredient is based on breiger work which recog nized that communities arise due to shared group affiliations we represent node community memberships with a bipartite affiliation network that links nodes of the social network to com munities that they belong to figure a the second ingredient stems from the fact that people tend to be involved in communities to various degrees therefore we assume that each affiliation edge in the bipartite affiliation network has a nonnegative weight the higher the node weight of the affiliation to the community the more likely is the node to be connected to other members in the community the last ingredient of our model is based on the fact that when people share multiple community affiliations e g co workers who attended the same university the links between them stem for one dominant reason i e shared community this means that for each community a pair of nodes shares we get an independent chance of connecting the nodes thus naturally the more communities a pair of nodes shares the higher the probability of being connected figure illustrates our model we start with a bipartite graph where the nodes at the bottom represent the nodes of the social network g the nodes on the top represent communities c and the edges m indicate node community affiliations we denote the bipartite affiliation network as b v c m the flexibility of the affiliation network allows us to model a wide range of network community structures figure illustrates the structure of the network as well as the corresponding node community affiliation network figure a shows an affiliation graph of a network with two non overlapping communities the affiliation graph in figure c represents hierarchical community structure where communities a and c are nested inside community b finally figure b shows an example of overlapping commu nities these three very different examples demonstrate that the flexibility of the affiliation network structure allows b igclam to simultaneously model any combination of non overlapping hierar chically nested as well as overlapping communities in networks from node community affiliations to the edges of the network to generate a network g v e given a bipartite community affil iation b v c m we need to specify the process that generates the edges e of g given the affiliation network b we consider a simple parameterization where we assign a nonnegative weight f uc between node u v and community c c f uc means no affiliation given f we assume that each community c connects its member nodes depending on the value of f in par ticular each community c connects its member nodes u v with probability exp f uc f vc each community c creates edges independently however if a pair of nodes gets connected multiple times the duplicate edges are not included in the graph g v e since each community c connects u v independently with proba bility is shared exp communities exp f c f uc uc f f vc vc the edge and thus probability increasing between u and v in the number of definition let f be a nonnegative matrix where f uc is a weight between node u v and community c c given f the bigclam generates a graph g v e by creating edge u v between a pair of nodes u v v with probability p u v p u v exp f u f v t where f u is a weight vector for node u f u f u the process in eq suggests the following probabilistic inter pretation assume an undirected weighted network where pairs of nodes have a latent interaction of non negative strength x uv how ever we only observe an undirected unweighted version of network g v e where a pair of nodes u v is connected if the correspond ing x uv now consider that nodes u v generate an interaction of strength x uv c within each community c using a poisson distri bution with mean f uc f vc then the total amount of interaction x uv between x uv nodes u x c and uv v is the uv c x c uv sum of x p ois f uc f vc c then p same ois as due p u c f to uc the v f in additivity vc eq and of the poisson random variable x uv the edge probability p x uv is the a non overlapping b overlapping c nested figure bigclam allows for rich modeling of network communities a non overlapping b overlapping c nested in a we assume that nodes in two communities connect with small prob ε refer to the discussion in the main text p x uv p x uv exp f uc f vc c loss function d and exp as a link function we can represent the problem as follows f ˆ argmin note that node u with higher f uc f d a f ff t is more likely to be connected to other members of c as x uv c will have a higher mean note that this process naturally generates an increasing relation ship between edge probability and the number of shared commu nities this is due to the fact that nodes that share multiple com munity memberships receive multiple chances to create a link for example pairs of purple nodes in the overlap of communities a and b in figure a get two chances to create an edge first they the benefit of using matrix factorization approach is increased scal ability overlapping community detection methods have been de veloped to analyze small networks and most methods rely on combinatorial optimization which is hard to scale on the other hand for nonnegative matrix factorization many efficient techniques exist b ig create an edge with probability e f ua c lam modifies the existing nmf methods and f va due to the mem bership to community a and then also an edge with probability adapts them to large networks while nmf methods use l norm as an objective function l e f ub norm is not suitable for modeling bi f vb due to membership to community b the edge probability between these nodes is e f ua f va f ub f vb if they were to reside in the non overlapping region of a they would be linked with probability e f ua nary adjacency matrices instead bigclam employs log likelihood as a loss function additional benefit is that for sparsely connected networks which real networks are our formulation al e f ua f va f ub f vb f va which is smaller than lows for near constant time gradient computation l takes linear time which in practice speeds up our algorithm for a factor of ε community in the formulation of equation bigclam does not allow for the edges between the nodes u and v that do not share any common communities since for such nodes f uc solving the optimization problem to solve the problem in eq we adopt a block coordinate gradient ascent algorithm 21 in f vc for all c to allow for edges between nodes that do not share any community affiliations we assume an additional community particular we update f u for each u with the other f v fixed i e we update the memberships of one node with fixing the membership of all other nodes the main reason is that if we fix all f v called the ε community which connects any pair of nodes with a very small probability ε we find that setting ε to be the back ground edge probability between a random pair of nodes ε e v v works well in practice for all our experiments we set ε community detection now that we defined the b ig then the problem of updating f u becomes a convex optimization problem we solve the following subproblem for each u argmax f uc l f u where l f u log exp f u f v t f u f v t c lam model we explain how to v n u v n u detect network communities using the model given an unlabeled undirected network g v e we aim to detect k communities by fitting matrix the f ˆ b ig rn k c lam to i e the finding underlying the most network likely g affiliation by maximizing factor where n u is a set of neighbors of u to solve this convex prob lem we use projected gradient ascent the gradient can be com puted straightforwardly the likelihood l f log p g f of the underlying g f ˆ argmax l f u f f v v n u exp f u f v t f v l f where l f we compute a step size using backtracking line search after update we project f u f uc into a space of nonnegative vectors by setting u v e log exp f u f v t f u f v t u v e max f uc for a large network with more than a million nodes this coor dinate ascent is not very scalable as making a single step of coor for now we assume the number of communities k is given we dinate ascent i e computing l f u will describe later how to automatically estimate k the optimization problem of eq can be viewed as a variant of nonnegative matrix factorization nmf where we learn f rn k that best approximates the adjacency matrix a of a given network g by representing a negative log likelihood l f as a takes linear time o n however we reduce the complexity to o n u by com puting and l f u v n u f v efficiently in particular we notice v n u v v n u f v f v f u f v exp f u f t v v n u by storing v f v we can compute v n u f u in time o n u given that real world networks are extremely sparse n u n we can update f u for a single node u in near constant time we iteratively update f u for each u and stop the iter ation if the likelihood does not increase increase less than 001 after we update f u for all u in practice this speeds up our algo rithm for two orders of magnitude and makes it practical to run it on networks with millions of nodes and edges determining community affiliations after we learn f ˆ we still have to determine whether u belongs to community c or not from the value of f uc to achieve this we ignore the membership of node u to communityc if f uc is below some threshold δ otherwise f uc δ we regard u as belonging to c we set δ so that if two nodes belong to community c then their edge probability is higher than the background edge probability ε see section ε exp solving this inequality we set the value of δ log ε note we also experimented with other values of δ and found that our choice for δ gives overall good performance initialization to initialize f we use locally minimal neighbor hoods neighborhoods n u of node u is a community of u and its neighbors and n u is locally minimal if n u has lower conductance than all the n v for nodes v who are connected to u recently gleich et al empirically showed that the locally minimal neighborhoods are good seed sets for community detec tion algorithms for a node u who belongs to a locally minimal neighborhood k we initialize f u k otherwise f u k choosing the number of communities to find the number of communities k we adopt the approach used in we reserve of node pairs as a hold out set varying k we fit the b ig clam model with k communities on the of node pairs and then evaluate the likelihood of b ig on the hold out set the k with the maximum hold out likelihood will be chosen as the number of communities when the network is too small e g has less than edges we use k that achieves the smallest value of the bayes information criterion bic k c lam f ˆ nk log e implementational details since the objective function of our optimization problem is not the l norm the methods for least squares nmf such as multiplicative update or alternating least squares are not applicable we experimented with the cyclic coordinate descent method ccd which optimizes f uc for each u and each c by the newton method but the method con verged slower than our block coordinate ascent method the main reason for this is that the number of subproblems that we have to solve in ccd grows linearly with k the number of communities in matrix factorization usually k the rank of f is assumed to be a very small constant 21 however in our problem k increases as the size of the underlying network grows connection to other affiliation network models last we also briefly describe the connection between b ig and other affil iation network models in particular we consider the agm which can also model densely overlapping network community structure similarly to b ig c lam agm generates g v e given a bipartite community affiliation b v c m in contrast to big c lam c lam agm assigns a single parameter p c to every community c given b v c m and p c agm models the edge probability p u v as follows f t v p u v where c uv is a set of communities that u and v have in common one can also detect community structure by fitting agm to a given network g v e i e finding affiliation graph b and pa rameters p c by maximizing the log likelihood argmax p p c u v e log p u v log p u v u v e this results in a combinatorial optimization problem that is very hard to solve solving the problem requires a combinatorial search over all possible affiliation graphs b however there is an expo nential number k of possible affiliation graphs b we now show that fitting b ig c lam eq can also be derived by relaxing the fitting problem of agm eq into a continuous optimization problem we begin by stating eq in a new form p u v c c uv c m uc m vc where m uc p c p c is an indicator variable whether node u belongs to com munity c by replacing p c exp α c with α c we can express the equation as a linear form of m and α c p u v exp c m uc α c m vc we then further simplify the equation by letting m uc p u v exp m u mt v note that we did not use any approximation so far so the maximum likelihood tion problem estimation m uc of the model is still a combinatorial means optimiza that if node u belongs to c it would be connected to other member nodes in c with the factor as the level of participation of u in community c which then determines edge m uc probability of u to other nodes in c basically we can replace with a continuous membership f uc which can be any nonneg ative number this way we actually model a level of participation of each node in a particular community as members with the higher value of f uc will be more likely to connect to other members of c p u v exp f u f v t now we transform the problem of eq into a continuous opti mization problem f ˆ argmax f u v e log exp f u f v t f u f v t u v e in other words we can view the optimization problem of b ig clam as a continuous relaxation of the combinatorial optimization problem of fitting agm b ig c lam can be considered as a relaxed version of agm in the sense that it models community affiliation as continuous variables with b ig c lam finding the most proba ble community affiliation is equivalent to factorizing the adjacency matrix of the underlying network with nonnegative factors experiments we proceed by evaluating the performance of bigclam and comparing it to the state of the art community detection methods on a range of networks from a number of different domains and research areas α c therefore we can interpret α c c c uv m uc p c α c m uc α c m uc experiments on synthetic networks using synthetic networks we investigate the scalability and con vergence of the bigclam optimization problem convergence of b ig c lam non negative matrix factorization is non convex which means that gradient based approaches do not guarantee to find an optimal solution to verify that our fitting algorithm does not suffer too much from local optima we conduct the following experiment on synthetic networks we generated synthetic networks using the agm model for each of these networks we then fit bigclam using different random starting points and attempt to recover the true community affiliations in of cases our fitting algorithm finds true communities with reliable accuracy score of node community memberships higher than 85 and in of cases our algorithm discovers the communities almost perfectly score this result sug gests that the optimization space has several local optima which almost equivalent to the global optimum scalability of b ig c lam we also evaluate the scalability of b ig clam by measuring the running time on the networks of increas ing sizes for comparison we compare the runtime of the following overlapping community detection methods nmf least squares non negative matrix factorization we solve the is an following adjacency problem matrix of argmax a given f uk network a f f we used t a f projected where a gradient descent as we do with b ig c lam bigclam naive bigclamwithout the optimization in eq lc link clustering method cpm clique percolation method mmsb mixed membership stochastic blockmodel link clustering clique percolation method and mixed mem bership stochastic blockmodels are considered the state of the art overlapping community detection methods we used the imple mentation of lc and cpm in the stanford network analysis plat for mmsb we used publicly available lda r package for cpm we use the clique size k for cpm for mmsb we set the number of communities to detect to k we also consider nmf and b ig c lam naive so that we can compare the performance gain due to the optimization described in eq figure shows the results nmf bigclam naive and mmsb scale to networks of around lc and cpm scale to networks of about and then their runtime becomes prohibitively large on the other hand b ig c lam can process networks with hundreds of thousands of nodes within minutes this means that big clam can easily process networks to times larger than other approaches and while also more accurately detecting com munities last note that the optimization of bigclam defined in eq speeds up the algorithm for around times and is thus essential for making bigclam scale to large networks experiments using real ground truth we also examine the performance of b ig c lam using the net works with ground truth communities that we described in sec tion in these networks nodes explicitly state their ground truth community memberships which allows us to quantify the accu racy of community detection methods by evaluating the level of correspondence between detected and ground truth communities experimental setup we are given an unlabeled undirected net work g with known ground truth communities c we aim to dis http snap stanford edu snap bigclam nmf c e s bigclam naive lc cpm mmsb e m i t number of nodes figure algorithm runtime comparison b ig c lam runs to faster than competing approaches figure sampling subnetworks of g cover communities c ˆ such that discovered communities c ˆ closely match the ground truth communities c even though our algorithm can process the networks described in table all the baseline methods do not scale to networks of such size to allow for comparison between our and the baseline methods we use the following evaluation scenario where the goal is to obtain a large set of relatively small subnetworks with over lapping community structure to obtain one such subnetwork we pick a random node u in the given graph g that belongs to at least two communities we then take the subnetwork to be the induced subgraph of g consisting of all the nodes that share at least one ground truth community membership with u figure illustrates how a subnetwork right is created from g v e left based on the red node u note that on average 95 of all ground truth com munities overlap which means that this procedure does not bias towards overlapping communities in our experiments we created different subnetworks for each of the six datasets baselines for comparison for baselines we choose three most prominent overlapping community detection methods link clus tering lc clique percolation method cpm and the mixed membership stochastic block model mmsb these methods have a number of parameters that need to be set for cpm we set the clique size k since the number of commu nities discovered by cpm with k best approximates the true number of communities for mmsb we have to set the number of communities k as an input parameter we use the bayes informa tion criterion to choose k while we require hard community memberships mmsb returns stochastic node memberships to each of the k communities thus we assign a node to a community if the corresponding stochastic membership is non zero we also con sidered infomap which is the state of the art non overlapping community detection method we omit the results as the perfor mance of the method was not competitive evaluation metrics the availability of ground truth communi ties allows us to quantitatively evalute the performance of commu nity detection algorithm without ground truth such evaluation is simply not possible for evaluation we use metrics that quantify the level of correspondence between the detected and the ground truth communities given a network g v e we consider a set of ˆ ground truth communities c and a set of detected communities c where community each c ground truth ˆ i quantify the level c ˆ is of correspondence community c i c and each detected defined by a of set c ˆ of to its c member we consider nodes to average score termine which c i to compute the score we need c ˆ to de we define score to be the average of the score of the best matching ground truth community to each detected community and the score of the best matching detected community to each ground truth community c corresponds to which c ˆ i where the best matching g and g is defined as follows g i argmax j c i c ˆ j g i argmax c ˆ i j c j and c i c ˆ j is the harmonic mean of precision and recall omega index 13 is the accuracy on estimating the number of communities that each pair of nodes shares where v share c and uv is c ˆ the uv is set the of set ground truth of detected communities communities that that u they and share normalized mutual information adopts the criterion used in information theory to compare the detected communities and the ground truth communities normalized mutual informa tion has been proposed as a performance metric for community detection refer to for details accuracy in the number of communities is the relative accu racy between the detected and the true number of communities c c ˆ for all metrics higher values mean more accurately detected communities i e detected node community memberships better cor respond to ground truth node community memberships maximum value of is obtained when the detected communities perfectly cor respond to the ground truth communities results on ground truth communities for each community de tection method and each dataset we measure the average value of the evaluation metrics over the subnetworks sampled using the procedure described above then for each evaluation metric separately we scale the scores of the methods so that the best per forming community detection method achieves the score of fi nally we compute the composite performance by summing up the normalized scores if a method outperforms all the other method in all the scores then its composite performance is figure displays the composite performance of the methods over all six networks on average the composite performance of bigclam is which is higher than that of link clustering higher than that of cpm and higher than that of mmsb the absolute average value of omega in dex of bigclam over the networks is which is higher than link clustering higher than cpm and higher than mmsb in terms of absolute values of scores b ig c lam archives the average score of average omega index of mutual information of and accuracy of the num ber of communities of c c c i c v c i u v v ˆ c g i c uv ˆ c c ˆ i c ˆ ˆ c uv c g i ˆ c i c overall b ig lam gives superior overall performance this means that while b ig c lam is two orders of magnitude more scal able than competing approaches it also achieves superior perfor mance in the quality of detected communities on out of net works bigclam performs best by a big margin however we note that on dblp and amazon mmsb is the winning method mostly due to bigclam scoring very badly on a single individual met ric number of communities on dblp ω index on amazon this occurs due to the fact that b ig c lam uses a single parameter ε to model the edge probability between all pairs of different commu nities ε community in section while mmsb uses one param eter for each pair of communities with more parameters mmsb can fit these networks better note that b ig c lam could be easily extended to include a distinct parameter for the edge probability between each pair of communities experiments on networks in ahn et al we further evaluate bigclam using performance benchmarks from ahn et al for this experiment we adopt exactly the same data evaluation metrics and experimental setup as in note that these networks do not contain information about ground truth com munities however nodes in these communities contain attributes and used purity metrics as surrogates for the quality of de tected communities the idea behind evaluation metrics here is that good communities have low diversity of member nodes features experimental setup we use the same seven different networks as in biological networks a network of wikipedia pages and a word association network for further details about these datasets refer to we also adopt the same data driven measures defined in community coverage overlap coverage community qual ity overlap quality all networks are small so we apply the com munity detection methods to full networks moreover the met rics are heavily biased towards methods that find a large number of communities so we fit bigclam using the same number of com munities as detected by lc i e the algorithm developed in results following we compute the composite performance by normalizing the scores the same way as we did in the experiments with ground truth communities figure shows the composite per formance of the four methods the bigclam achieves best com posite performance in networks and the second best in three net works in all these cases mmsb slightly outperforms bigclam due to b ig c lam bad performance on the overlap coverage met ric overlap coverage is defined as the average number of commu nities that a node belongs to this metric is extremely ill posed since assigning nodes to more communities always improves the score since any non zero stochastic membership found by mmsb is regarded as a valid community membership the mmsb achieves extremely high score on the overlap coverage metric neverthe less on average the bigclam achieves a composite performance score of outperforming link clustering by clique percolation by and mmsb by experiments on large networks in addition to better accuracy another strength of bigclam is its scalability to test this we apply b ig c lam to large real world networks we were able to run bigclam on full networks from table livejournal youtube amazon and dblp to reduce the memory requirements of our method we aim to find sparse latent factors we achieve this by adding l regulariza tion term to eq and optimize argmax f uc l f λ f uc u c methods e c n a m r o f r e p e t i o p m o c l link clustering c clique percolation mixed membership stochastic block model b bigclam l c m b m l c m b l c m b l c m b l c m b l c m b figure performance of detecting ground truth communities while being to times faster than competing approaches b ig c lam also achieves overall best performance in the accuracy of detected communities e c n a m r o f r e p e t i o p m o c l c m b l c m b l c m b l c m b l c m b l c m b l c m b ppi ppi ap ms ppi lc ppi all metabolic philosophers word association measures overlap quality community quality overlap coverage community coverage network methods l link clustering c clique percolation m mixed membership stochastic block models b bigclam figure experiments on the data and evaluation metrics used in ahn et al n number of the nodes e number of the edges since l regularization introduces sparsity to matrix f we only need to keep track of latent factors with non zero value which de creases the memory requirements of our method we use λ for amazon youtube and dblp and λ for livejournal we update f u solving eq for multiple nodes in parallel with threads it takes about one day to fit b ig to the livejour nal network nodes edges as our baselines from the previous experiments do not scale to these networks we consider two well known graph partitioning methods as baselines metis and graclus for graclus and metis we set the number of communities to detect k to be the number of ground truth communities and use the same k for b ig c lam c lam as well similarly to experiments in figure we measure the accuracy of detected communities using f score and omega index nmi is omitted as all the methods perform the same moreover notice that grund truth communities in our data are partially annotated as some nodes might not indicate their memberships this means it is important to quantify the recall of a given method we define re call as the average recall of best matching detected communities recall c c ˆ where rc c i c ˆ j is the recall of c ˆ j under the best matching g since the two baselines graculus and metis perform very sim ilarly in all metrics we take just the best value among the two in each case rather than showing the result of baselines separately for each network and each score we pick the best score x among the two baselines and compute the relative improvement of b ig c lam over the x i e score bigclam x table shows the relative im provement of bigclam over the baselines for example 21 for x c c i c rc c i ˆ c g i table relative improvement of bigclam over metis and graclus in detecting communities in large scale networks pos itive value indicates that b ig c lam outperforms the baselines f in livejournal means that bigclam achieves 21 higher f score than the best baseline metis in this case overall bigclam outperforms the baselines in nearly all cases on average b ig c lam achieves higher omega index higher f score and higher average recall which means that bigclam achieves relative improvement on average among the three scores furthermore b ig c lam outperforms the base lines in every measure and every network the absolute value of the scores of b ig c lam is omega index 13 f score and recall overall the results emphasize the need for a scalable and accurate overlapping community detection method as graph partitioning methods fail to detect overlapping communities results demonstrate that bigclam could be the needed solution conclusion in this paper we developed a novel large scale community detec tion method that accurately discovers the overlapping community structure of real world networks we identified a set of networks where nodes explicitly state their ground truth community mem bership and studied the connectivity of ground truth communities and their overlaps we observed that the overlaps of communi ties are more densely connected than the non overlapping parts of the acquisition of knowledge is always of use to the intellect because it may thus drive out useless things and retain the good for nothing can be loved or hated unless it is first known leonardo da vinci authors are listed alphabetically corresponding author permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page copyrights for components of this work owned by others than the author must be honored abstracting with credit is permitted to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee request permissions from permissions acm org kdd august new york ny usa copyright is held by the owner author publication rights licensed to acm acm http dx doi org 1145 in recent years several large scale knowledge bases kbs have been constructed including academic projects such as yago nell dbpedia and elementary deep dive as well as commercial projects such as those by walmart and others see section for a detailed discussion of related work these knowledge repositories store millions of facts about the world such as information about people places and things generically referred to as entities despite their seemingly large size these repositories are still far from complete for example consider freebase the largest open source knowledge base of people in freebase have no known place of birth and have no known furthermore coverage for less common relations predicates can be even lower previous approaches for building knowledge bases primar ily relied on direct contributions from human volunteers as well as integration of existing repositories of structured knowledge e g wikipedia infoboxes however these meth ods are more likely to yield head content namely frequently mentioned properties of frequently mentioned entities suh et al also observed that wikipedia growth has essen tially plateaued hence unsolicited contributions from hu man volunteers may yield a limited amount of knowledge going forward therefore we believe a new approach is necessary to further scale up knowledge base construction such an approach should automatically extract facts from the whole web to augment the knowledge we collect from human input and structured data sources unfortunately standard methods for this task cf often produce very noisy unreliable facts to alleviate the amount of noise in the automatically extracted data the new approach should automatically leverage already cataloged knowledge to build prior models of fact correctness in this paper we propose a new way of automatically con structing a web scale probabilistic knowledge base which we call the knowledge vault or kv for short like many other knowledge bases kv stores information in the form www bing com blogs b search archive 21 satorii aspx www google com insidesearch features search knowledge html www insidefacebook com facebook builds knowledge graph with info modules on community pages current as of october cf freebase data is publicly available at https developers google com freebase data data of rdf triples subject predicate object an example is sources considerably improves precision at a given re m people person m call level where m is the freebase id for barack obama and m is the id for honolulu associated with each such triple is a confidence score representing the probabil ity that kv believes the triple is correct overview kv contains three major components entity types and predicates come from a fixed ontology which is similar to that used in other systems such as yago nell deepdive and various systems partici pating in the tac kbp slot filling competition 22 knowl edge bases that use a fixed ontology should be contrasted with open information extraction open ie approaches extractors these systems extract triples from a huge number of web sources each extractor assigns a con fidence score to an extracted triple representing un certainty about the identity of the relation and its cor responding arguments such as reverb which work at the lexical level open ie systems usually have multiple redundant facts that are worded differently such as barack obama was born in honolulu and obama place of birth honolulu in graph based priors these systems learn the prior probability of each possible triple based on triples stored in an existing kb contrast kv separates facts about the world from their lex ical representation this makes kv a structured repository of knowledge that is language independent the contributions of this paper are threefold first the knowledge fusion this system computes the prob ability of a triple being true based on agreement be tween different extractors and priors knowledge vault is different from previous works on au tomatic knowledge base construction as it combines noisy extractions from the web together with prior knowledge which is derived from existing knowledge bases in this pa per we use freebase as our source of prior data this approach is analogous to techniques used in speech recog nition which combine noisy acoustic signals with priors de rived from a language model kv prior model can help overcome errors due to the extraction process as well as errors in the sources themselves for example suppose an extractor returns a fact claiming that barack obama was born in kenya and suppose for illustration purposes that the true place of birth of obama was not already known in freebase our prior model can use related facts about obama such as his profession being us president to infer that this new fact is unlikely to be true the error could be due to mistaking barack obama for his father entity res abstractly we can view the kv problem as follows we are trying to construct a weighted labeled graph which we can view as a very sparse e p e matrix g where e is the number of entities p is the number of pred icates and g p o if there is a link of type p from to o and g p o otherwise we want to compute pr g p o for candidate p o triples where the probability is conditional on different sources of information when using extractions we condition on text features about the triple when using graph based priors we condition on known edges in the freebase graph obviously we exclude the edge we are trying to predict finally in knowledge fusion we condition on both text extractions and prior edges we describe each of three components in more detail in the following sections before that we discuss our training and test procedure which is common to all three approaches olution or co reference resolution error or it could be due to an erroneous statement on a spammy web site source error second kv is much bigger than other comparable kbs see table in particular kv has triples of which have a confidence of or higher and have a confidence of or higher this is about times more than the largest previous comparable system deepdive which has confident facts ce zhang personal communi cation to create a knowledge base of such size we extract facts from a large variety of sources of web data including free text html dom trees html web tables and hu man annotations of web pages note that about of the confident triples were not previously in freebase so we are extracting new knowledge not contained in the prior third we perform a detailed comparison of the quality and coverage of different extraction methods as well as dif ferent prior methods we also demonstrate the benefits of using multiple extraction sources and systems finally we evaluate the validity of the closed world assumption which is often used to automatically evaluate newly extracted facts given an existing knowledge base see section in the following sections we describe the components of kv in more detail we then study the performance of each part of the system in isolation and in combination and show that fusion of multiple complementary systems and evaluation protocol using the methods to be described in section we extract about candidate triples covering different types of relations and different types of entities about of these facts have an estimated probability of being true above we call these confident facts the resulting kb is much larger than other automatically constructed kbs as summarized in table to evaluate the quality of our methods we randomly split this data into a training set of the data and a test set 20 of the data we infer labels for these triples using the method described below to ensure that certain common predicates e g relating to geographical containment did not dominate the performance measures we took at most instances of each predicate when creating the test set we then pooled the samples from each predicate to get a more balanced test set if the test set contains the triple p o then the training set is guaranteed not to contain the same triple however it may contain p o or p o or p o for example suppose is barack obama p is father of and o is sasha obama then the training set may contain the fact that barack is the father of malia obama or that barack lives in the same place as sasha obama etc in graph terminology we are leaving out edges at random from the training set and asking how well we can predict their presence or absence name entity types entity instances relation types confident facts relation instances knowledge vault kv deepdive nell 271 19m prospera n a 350 000 9 freebase 000 knowledge graph kg 000 table comparison of knowledge bases kv deepdive nell and prospera rely solely on extraction freebase and kg rely on human curation and structured sources and uses both strategies confident facts means with a probability of being true at or above 9 ace zhang u wisconsin private communication bbryan ccore facts kiesel http www mpi inf mpg de yago naga yago downloads html cmu private communication dthis is the number of non redundant base triples excluding reverse predicates and lazy triples derived from flattening cvts ehttp insidesearch blogspot com get smarter answers from html complex value types an alternative approach to constructing the test set would have been to leave out all edges emanating from a particu lar node however in such a case the graph based models would have no signal to leverage for example suppose we omitted all facts about barack obama and asked the sys tem to predict where he lives and who his children are this would be possible given text extractions but impossi ble given just a prior graph of facts a compromise would be to omit all edges of a given type for example we could omit connections to all his children but leave in other re lations however we think the random sampling scenario more accurately reflects our actual use case which consists of growing an existing kb where arbitrary facts may be missing local closed world assumption lcwa all the components of our system use supervised machine learning methods to fit probabilistic binary classifiers which can compute the probability of a triple being true we give the details on how these classifiers are constructed in the following sections here we describe how we determine the labels we use the same procedure for the training and test set for p o triples that are in freebase we assume the label is true for triples that do not occur in freebase we could assume the label is false corresponding to a closed world assumption but this would be rather dangerous since we know that freebase is very incomplete so instead we make use a somewhat more refined heuristic that we call the local closed world assumption to explain this heuristic let us define o p as the set of existing object values for a given and p this set will be a singleton for functional single valued predicates such as place of birth but can have multiple values for general relations such as children of course the set can also be empty now given a candidate triple p o we assign its label as follows if p o o p we say the triple is correct if p o o p but o p we say the triple is incorrect because we assume the kb is locally complete for this subject predicate pair if o p is empty we do not label the triple and we throw it out of our training test set this heuristic is also used in previous works such as we empirically evaluate its adequacy in section by com paring to human labeled data there are more sophisticated methods for training models that don t make this assump tion such as 36 but we leave the integration of such methods into kv to future work fact extraction from the web in this section we summarize the extractors that we use to build kv and then we evaluate their relative performance extraction methods text documents txt we use relatively standard methods for relation extraction from text see for a recent overview but we do so at a much larger scale than previous systems we first run a suite of standard nlp tools over each doc ument these perform named entity recognition part of speech tagging dependency parsing co reference resolution within each document and entity linkage which maps mentions of proper nouns and their co references to the cor responding entities in the kb the in house named entity linkage system we use is similar to the methods described in next we train relation extractors using distant supervi sion specifically for each predicate of interest we ex tract a seed set of entity pairs that have this predicate from an existing kb for example if the predicate is the pairs could be barackobama michelleobama and bill clinton hillaryclinton we then find examples of sen tences in which this pair is mentioned and extract features patterns either from the surface text or the dependency parse from all of these sentences the features that we use are similar to those described in in a bootstrapping phase we look for more examples of sentences with these patterns occurring between pairs of en tities of the correct type we use the local closed world assumption to derive labels for the resulting set of extrac tions once we have a labeled training set we fit a binary classifier we use logistic regression for each predicate inde pendently in parallel using a mapreduce framework we have trained extractors for predicates which is much more than previous machine reading systems html trees dom a somewhat different way to extract information from web pages is to parse their dom trees these can either come from text pages or from deep web sources where data are stored in underlying databases and queried by fill ing html forms these sources together generate more than pages of data in dom tree format to extract triples from dom trees we train classifiers as in the text case ex cept that we derive features connecting two entities from the dom trees instead of from the text specifically we use the lexicalized path along the tree between the two entities as a feature vector the score of the extracted triples is the output of the classifier html tables tbl there are over tables on the web that contain rela tional information as opposed to just being used for visual formatting unfortunately fact extraction techniques developed for text and trees do not work very well for ta bles because the relation between two entities is usually contained in the column header rather than being close by in the text tree instead we use the following heuristic technique first we perform named entity linkage as in the text case then we attempt to identify the relation that is expressed in each column of the table by looking at the en tities in each column and reasoning about which predicate each column could correspond to by matching to freebase as in standard schema matching methods ambiguous columns are discarded the score of the extracted triple re flects the confidence returned by the named entity linkage system human annotated pages ano there are a large number of webpages where the web master has added manual annotations following ontologies from schema org microformats org opengraphprotocol org etc in this paper we use schema org annotations many of these annotations are related to events or products etc such information is not currently stored in the knowledge vault so instead in this paper we focus on a small sub set of different predicates mostly related to people we define a manual mapping from schema org to the freebase schema for these different predicates the score of the ex tracted triple reflects the confidence returned by the named entity linkage system the same one we use for txt triples fusing the extractors we have described different fact extraction methods a simple way to combine these signals is to construct a feature vector f t for each extracted triple t p o and then to apply a binary classifier to compute pr t f t for simplicity and speed we fit a separate classifier for each predicate the feature vector is composed of two numbers for each extractor the square of the number of sources that the extractor extracted this triple from and the mean score of motivation for using n where n is the number of sources is to reduce the effect of very commonly expressed facts such as the birth place of barack obama re sults are similar if we use log n note that we per figure true probability vs estimated probability for each triple in kv the extractions from this extractor averaging over sources or if the system did not produce this triple the classifier learns a different weight for each compo nent of this feature vector and hence can learn the relative reliabilities of each system in addition since we fit a sep arate classifier per predicate we can model their different reliabilities too the labels for training the fusion system come from ap plying the local closed world assumption to the training set since this is a very low dimensional classification problem we initially used a linear logistic regression model how ever we observed considerably better performance by using boosted decision stumps 35 this kind of classifier can learn to quantize the features into bins and thus learn a non linear decision boundary calibration of the probability estimates the confidence scores from each extractor and or the fused system are not necessarily on the same scale and cannot necessarily be interpreted as probabilities to alle viate this problem we adopt the standard technique known as platt scaling named after which consists of fitting a logistic regression model to the scores using a separate validation set figure shows that our fused probability estimates are well calibrated in the following sense if we collect all the triples that have a predicted probability of 9 then we find that about of them are indeed true each individual extractor is also calibrated results not shown comparison of the methods using the four extractors described earlier applied to a very large web corpus we extract about triples ta ble shows the number of triples from each system we see that the dom system extracts the largest number of triples overall about 2b of which about or 8 are high confidence with a probability of being true at or above 9 see the penultimate column of table the tbl system extracts the least number of triples overall about 9 one reason for this is that very few columns in webtables only according to map to a corresponding free base predicate the ano and txt systems both produce hundreds of millions of triples in addition to measuring the number of triples at different confidence levels it is interesting to consider the area under the roc curve auc score this score is equal to the prob ability that a classifier will rank a randomly chosen positive form de duplication of sources before running the extraction pipelines system 9 frac 9 auc tbl 9 4m ano 4m 002 txt dom fused ex table performance of different extraction sys tems figure predicted probability of each triple vs the number of systems that predicted it solid blue line correct true triples dotted red line incorrect false triples instance higher than a randomly chosen negative one we computed the auc scores for the different extraction meth ods on different test sets namely only using the extractions produced by each system obviously the test sets were dis tinct from the training sets the test set for computing the auc score for the fused extractors was the union of all the test sets of the individual systems we see that the dom system has the highest auc score so although it produces a large number of low confidence triples the system knows that these are likely to be false the table also illustrates the benefits of fusing multiple ex tractors we get about more high confidence triples while maintaining a high auc score see the last row of the table not surprisingly however the performance of the fusion system is dominated by that of the dom system in section we shall show much greater gains from fusion when we combine graph priors with extractors the beneficial effects of adding more evi dence figure shows how the overall predicted probability of each triple changes as more systems extract it when no systems extract a given triple we rely on our prior model described in section averaging over all the triples we see that the prior probability for the true triples is about whereas the prior probability for the false triples is close to as we accumulate more evidence in favor of the triple our belief in its correctness increases to near 0 for the true triples for the false triples our belief also increases although it stays well below 0 figure predicted probability of each triple vs the number of unique web sources that contain this triple axis truncated at for clarity figure shows how the probability of a triple increases with the number of sources where the triple is seen again our final belief in true triples is much higher than in false triples to prevent over counting of evidence we only count each triple once per domain as opposed to once per url for example if we extract a triple asserting that barack obama was born in kenya from myblogger com and myblogger com we only count this once graph based priors as mentioned in the introduction facts extracted from the web can be unreliable a good way to combat this is to use prior knowledge derived from other kinds of data in this paper we exploit existing triples in freebase to fit prior models which can assign a probability to any possible triple even if there is no corresponding evidence for this fact on the web cf this can be thought of as link predic tion in a graph that is we observe a set of existing edges representing predicates that connect different entities and we want to predict which other edges are likely to exist we have tried two different approaches to solving this problem which we describe below path ranking algorithm pra one way to perform link prediction is to use the path ranking algorithm of similar to distant supervision we start with a set of pairs of entities that are connected by some predicate p pra then performs a random walk on the graph starting at all the subject source nodes paths that reach the object target nodes are considered successful for example the algorithm learns that pairs x y which are connected by a marriedto edge often also have a path of the form x parentof z parentof y since if two people share a common child they are likely to be married the quality of these paths can be measured in terms of their support and precision as in association rule mining cf the paths that pra learns can be interpreted as rules for example consider the task of predicting where someone went to college the algorithm discovers several useful rules shown in table in english the first rule says a person x is likely to have attended school s if x was drafted from p r w path 0 03 0 sports drafted athlete drafted sports sports league draft pick school 0 05 0 0 people person sibling people sibling relationship sibling people person education education education institution 0 06 0 0 people person spouse people marriage spouse people person education education education institution 0 04 0 29 0 02 people person parents people person education education education institution 0 05 0 21 0 02 85 people person children people person education education education institution 0 13 0 0 people person place of birth location location people born here people person education education education institution 0 05 0 04 0 34 74 type object type type type instance people person education education education institution 0 04 0 03 0 people person profession people profession people with this profession people person education education education institution table some of the paths learned by pra for predicting where someone went to college rules are sorted by decreasing precision column headers is the harmonic mean of precision and recall p is the precision r is the recall w is the weight given to this feature by logistic regression sports team t and t is from school s the second rule says a person is likely to attend the same school as their sibling since multiple rules or paths might apply for any given pair of entities we can combine them by fitting a binary classifier we use logistic regression in pra the features are the probabilities of reaching o from s following differ ent types of paths and the labels are derived using the local closed world assumption we can fit a classifier for each predicate independently in parallel we have trained prior predictors for predicates using freebase as train ing data at test time given a new p o triple we look up all the paths for predicate p chosen by the learned model and perform a walk on the training graph from to o via each such path this gives us a feature value that can be plugged in to the classifier the overall auc is 0 884 which is less than that of the fused extractor system 0 927 but is still surprisingly high neural network model mlp an alternative approach to building the prior model is to view the link prediction problem as matrix or rather tensor completion in particular the original kb can be viewed as a very sparse e p e matrix g where e is the number of entities p is the number of predicates and g p o if there is a link of type p from to o and g p o 0 otherwise we can perform a low rank decomposition of this tensor by associating a latent low dimensional vector to each entity and predicate and then computing the elementwise inner product pr g p o σ model requires o ke parameters where m is the number of layers in the tensor w in this paper we considered a simpler approach where we associate one vector per predicate as in equation but then use a standard multi layer perceptron mlp to capture interaction terms more precisely our model has the form pr g p o σ βtf a u where a is a l matrix where the term arises from the k dimensional u w p v o and w p and v o representing the first layer weights after the embeddings and β is a l vector representing the second layer weights we set l k this has only o l lk ke kp parameters but achieves essentially the same performance as the one in equation on their dataset having established that our mlp model is comparable to the state of the art we applied it to the kv data set sur prisingly we find that the neural model has about the same performance as pra when evaluated using roc curves the auc for the mlp model is 0 882 and for pra is 0 884 to illustrate that the neural network model learns a mean ingful semantic representation of the entities and predi cates we can compute the nearest neighbors of various items in the a k dimensional space it is known from previous work e g 27 that related entities cluster together in the space so here we focus on predicates the results are shown in table we see that the model learns to put semanti cally related but not necessarily similar predicates near each other for example we see that the closest predicates k in the w embedding space to the children predicate are parents spouse and birth place k fusing the priors we can combine the different priors together using the fu sion method described in section the only difference is the features that we use since we no longer have any ex tractions insead the feature vector contains the vector of confidence values from each prior system plus indicator val ues specifying if the prior was able to predict or not this lets us distinguish a missing prediction from a prediction score of 0 0 we train a boosted classifier using these sig nals and calibrate it with platt scaling as before fusing precisely reported an 9 accuracy on the sub set of freebase data they have worked with 043 entities 13 relations when they replaced entities such as barack obama by their constituting words barack and obama apply ing the same technique of replacing entities with consituting words our simpler model got an accuracy of where σ x e x is the sigmoid or logistic function and k is the number of hidden dimensions here u u sk w pk v ok w p and v o are k dimensional vectors which embed the discrete tokens into a low dimensional semantic space if we ignore the sigmoid transform needed to produce binary responses this is equivalent to the parafac method of tensor decomposition a more powerful model was recently proposed in this associates a different tensor with each relation and hence has the form pr g p o σ β p t f u t w p m v o where f is a nonlinear function such as tanh β p is a k vector and w p m is a k k matrix unfortunately this predicate neighbor neighbor neighbor children 0 parents 0 spouse 0 8 birth place birth date children 25 gender 29 parents edu end job start 61 edu end 74 job end table nearest neighbors for some predicates in the embedding space learned by the neural net work numbers represent squared euclidean dis tance edu start and edu end represent the start and end dates of someone attending a school or col lege similarly job start and job end represent the start and end dates of someone holding a particular job figure roc curves for the fused extractor fused prior and fused prior extractor the numbers in the legend are the auc scores the two prior methods helps performance since they have complementary strengths and weaknesses different induc tive biases the auc of the fused system is 0 911 fusing extractors and priors we have described several different fact extraction meth ods and several different priors we can combine all these systems together using the fusion method described in sec tion figure shows the benefits of fusion quantitatively we see that combining prior and extractor together results in a significant boost in performance to more clearly illustrate the effect of adding the prior figure plots the number of triples in each confidence bin for the fused extractor the fused prior and the overall system we see that compared with considering only extrac tions combining priors and extractors increases the number of high confidence facts those with a probability greater than 0 9 from about to about of these about 33 are new facts that were not yet in freebase figure illustrates another interesting effect when we combine prior and extractor the number of triples about which we are uncertain i e the predicted probability falling in the range of 7 has gone down some of these triples we now believe to be true as we discussed previously but many we now believe to be false this is a visual illustration that the prior can reduce the false positive rate figure number of triples in kv in each confi dence bin we now give a qualitative example of the benefits of com bining the prior with the extractor the extraction pipeline extracted the following triple 7 barry richter m people person edu edu edu institution universty of wisconsin madison m the fused extraction confidence for this triple was just 0 14 since it was based on the following two rather indirect statements 8 in the fall of richter accepted a scholarship to the university of wisconsin where he played for four years and earned numerous individual accolades the polar caps cause has been helped by the impact of knowledgable coaches such as andringa byce and former uw teammates chris tancill and barry richter however we know from freebase that barry richter was born and raised in madison wi this increases our prior belief that he went to school there resulting in a final fused belief of 0 61 evaluating lcwa so far we have been relying on the local closed world assumption lcwa to train and test our system however we know that this is just an approximation to the truth for example freebase often lists the top or so actors for any given movie but it is unreasonable to assume that this list is complete since most movies have a cast of 20 actors this can result in false negatives if our system predicts the the predicate is a conjunction of two primi tive predicates people person education and educa tion education institution obtained by passing through a complex value type cvt node aka an anonymous or blank node representing the temporal event that barry attended madison http www legendsofhockey net legendsofhockey jsp searchplayer jsp player and http host madison com sports high school hockey numbers dwindling for once mighty madison high school hockey programs 11df html labels prior extractor prior ex lcwa 0 943 0 872 0 959 human 0 843 0 852 0 869 table auc scores for the fused prior extractor and prior extractor using different labels on the test set name of an actor that is not on the list conversely but less frequently freebase can contain errors which can result in false positives to assess the severity of this problem we manually la beled a subset of our balanced test set using an in house team of raters this subset consisted of triples for different predicates we asked each rater to evaluate each such triple and to determine based on their own research which can include web searches looking at wikipedia etc whether each triple is true or false or unknown we discarded the triples with unknown labels we then computed the performance of our systems on this test set using both lcwa labels and the human la bels in both cases the system was trained on our full training set i e of 6b using lcwa labels the results are shown in table we see that the performance on the human labeled data is lower although not by that much indirectly justifying our use of the lcwa 7 related work there is a growing body of work on automatic knowledge base construction this literature can be clustered into main groups approaches such as yago dbpedia and freebase which are built on wikipedia infoboxes and other structured data sources approaches such as reverb ollie and pris matic 13 which use open information schema less ex traction techniques applied to the entire web approaches such as nell readtheweb 8 prospera and deepdive elementary which extract information from the entire web but use a fixed ontology schema and ap proaches such as probase 47 which construct taxonomies is a hierarchies as opposed to general kbs with multiple types of predicates the knowledge vault is most similar to methods of the third kind which extract facts in the form of disambiguated triples from the entire web the main difference from this prior work is that we fuse together facts extracted from text with prior knowledge derived from the freebase graph there is also a large body of work on link prediction in graphs this can be thought as creating a joint probability model over a large set of binary random variables where g p o if and only if there is a link of type p from to o the literature can be clustered into three main kinds of methods methods that directly model the correlation between the variables using discrete markov random fields e g or continuous relaxations thereof e g 34 methods that use latent variables to model the correlations indirectly using either discrete factors e g or contin uous factors e g 11 20 and methods that approximate the correlation using algorithmic approaches such as random walks in the knowledge vault we currently employ graph priors of the second and third kind in particular our neural tensor model is a continuous latent variable model which is sim ilar to but slightly different from see section for a discussion our pra model is similar to the method de scribed in except it is trained on freebase instead of on nell in addition it uses a more scalable implementation another related literature is on the topic of probabilistic databases see e g 43 kv is a probabilistic database and it can support simple queries such as barackobama bornin which returns a distribution over places where kv thinks obama was born however we do not yet sup port sophisticated queries such as join or select finally there is a small set of papers on representing un certainty in information extraction systems see e g 25 kv also represents uncertainty in the facts it has ex tracted indeed we show that its uncertainty estimates are well calibrated we also show how they change as a function of the amount of evidence see figure 8 discussion although knowledge vault is a large repository of useful knowledge there are still many ways in which it can be improved we discuss some of these issues below modeling mutual exclusion between facts currently for reasons of scalability we treat each fact as an inde pendent binary random variable that is either true or false however in reality many triples are correlated for exam ple for a functional relation such as born in we know there can only be one true value so the p o i triples represent ing different values o i for the same subject and predicate p become correlated due to the mutual exclusion constraint a simple way to handle this is to collect together all candidate values and to force the distribution over them to sum to possibly allowing for some extra probability mass to ac count for the fact that the true value might not be amongst the extracted set of candidates this is similar to the no tion of an x tuple in probabilistic databases prelimi nary experiments of this kind did not work very well since the different o i often represent the same entity at different levels of granularity for example we might have a fact that obama was born in honolulu and another one stating he was born in hawaii these are not mutually exclusive so the naive approach does not work we are currently inves tigating more sophisticated methods modeling soft correlation between facts for some kinds of relations there will be soft constraints on their val ues for example we know that people usually have between 0 and children there is of course a long tail to this distri bution but it would still be surprising and indicative of a potential error if we extracted different children for one person similarly we expect the date of birth of a person to be about to years earlier than the date of birth of their child preliminary experiments using joint gaussian models to represent correlations amongst numerical values show some promise but we still need to fully integrate this kind of joint prior into kv values can be represented at multiple levels of ab straction we can represent the world at different levels of granularity for example we can say that obama is born in honolulu or in hawaii or in the usa when matching extracted facts with those stored in freebase we use prior geographical knowledge to reason about compatibility for example if we extract that obama was born in hawaii and we already know he was born in honolulu we consider this a correct extraction in the future we would like to gener alize this approach to other kinds of values for example if we extract that obama profession is politician and we already know his profession is president we should regard the extracted fact as true since it is implied by what we already know dealing with correlated sources in figure we showed how our belief in a triple increased as we saw it extracted from more sources this is of course problematic if we have duplicated or correlated sources currently we have a very simple solution to this based on counting each domain only once in the future we plan to deploy more sophisticated copy detection mechanisms such as those in some facts are only temporarily true in some cases the truth about a fact can change for example google current ceo is larry page but from to it was eric schmidt both facts are correct but only during the specified time interval for this reason freebase allows some facts to be annotated with beginning and end dates by use of the cvt compound value type construct which rep resents n ary relations via auxiliary nodes an alternative approach is to reify the pairwise relations and add extra assertions to them as in the system 19 in the future we plan to extend kv to model such temporal facts however this is non trivial since the duration of a fact is not necessarily related to the timestamp of the correspond ing source cf 21 adding new entities and relations in addition to miss ing facts there are many entities that are mentioned on the web but are not in freebase and hence not in kv ei ther in order to represent such information we need to automatically create new entities cf this is work in progress furthermore there are many relations that are mentioned on the web but cannot be represented in the freebase schema to capture such facts we need to extend the schema but we need to do so in a controlled way to avoid the problems faced by open ie systems which have many redundant and synonymous relations see for one possible approach to this problem knowledge representation issues the rdf triple for mat seems adequate for representing factual assertions as suming a suitably rich schema but it might be less ap propriate for other kinds of knowledge e g representing the difference between running and jogging or between jazz music and blues there will always be a long tail of con cepts that are difficult to capture in any fixed ontology our neural network is one possible way to provide semantically plausible generalizations but extending it to represent richer forms of knowledge is left to future work inherent upper bounds on the potential amount of knowledge that we can extract the goal of kv is to become a large scale repository of all of human knowledge however even if we had a perfect machine reading system not all of human knowledge is available on the web in particular common sense knowledge may be hard to acquire from text sources however we may be able to acquire such knowledge using crowdsourcing techniques vouchers such online discussions provide a new means to sense the public interests and generate feedback in real time and are mostly appealing compared to generic media such as radio or tv broadcasting another example is flickr a public picture sharing site which received 8 million photos per day on average from february to march 35 assuming the size of each photo is megabytes mb this requires terabytes tb storage every single day indeed as an old saying states a picture is worth a thousand words the billions of pictures on flicker are a treasure tank for us to explore the human society social events public affairs disasters and so on only if we have the power to harness the enormous amount of data the above examples demonstrate the rise of big data applications where data collection has grown tremen dously and is beyond the ability of commonly used software tools to capture manage and process within a tolerable elapsed time the most fundamental challenge for big data applications is to explore the large volumes of data and extract useful information or knowledge for future actions in many situations the knowledge extraction process has to be very efficient and close to real time because storing all observed data is nearly infeasible for example the square kilometer array ska 17 in radio astronomy consists of 000 to meter dishes in a central km area it provides times more sensitive vision than any existing radio telescopes answering fundamental questions about the universe however with a gigabytes gb second data volume the data x wu is with the school of computer science and information engineering hefei university of technology china and the department of computer science university of vermont x zhu is with the department of computer electrical engineering and generated from the ska are exceptionally large although researchers have confirmed that interesting patterns such as transient radio anomalies 41 can be discovered from computer science florida atlantic university boca raton fl g q wu is with the school of computer science and information engineering hefei university of technology china w ding is with the computer science department university of massachusetts boston morrissey blvd boston ma the ska data existing methods can only work in an offline fashion and are incapable of handling this big data scenario in real time as a result the unprecedented data volumes require an effective data analysis and prediction manuscript received 25 jan revised 13 apr accepted june platform to achieve fast response and real time classifica published online june recommended for acceptance by x he for information on obtaining reprints of this article please send e mail to tkde computer org and reference ieeecs log number tkde digital object identifier no 10 1109 tkde yan mo won the nobel prize in literature this is probably the most controversial nobel prize of this category searching on google with yan mo nobel prize resulted in 050 000 web pointers on the internet as of january for all praises as well as criticisms said mo recently i am grateful what types of praises and criticisms has mo actually received over his year writing career as comments keep coming on the internet and in various news media can we summarize all types of opinions in different media in a real time fashion including updated cross referenced discussions by critics this type of summarization program is an excellent example for big data processing as the information comes from multiple heterogeneous autonomous sources with complex and evolving relationships and keeps growing along with the above example the era of big data has arrived 34 29 every day quintillion bytes of data are created and percent of the data in the world today were produced within the past two years our capability for data generation has never been so powerful and enormous ever since the invention of the information technology in the early century as another example on october the first presidential debate between president barack obama and governor mitt romney triggered more than 10 million tweets within hours among all these tweets the specific moments that generated the most discussions actually revealed the public interests such as the discussions about medicare and tion for such big data the remainder of the paper is structured as follows in section we propose a hace theorem to model big data characteristics section summarizes the key challenges for 14 ß ieee published by the ieee computer society 98 ieee transactions on knowledge and data engineering vol no january big data mining some key research initiatives and the authors national research projects in this field are outlined in section 4 related work is discussed in section 5 and we conclude the paper in section b ig d ata c haracteristics hace t heorem hace theorem big data starts with large volume heterogeneous autonomous sources with distributed and decentralized control and seeks to explore complex and evolving relationships among data these characteristics make it an extreme challenge for discovering useful knowledge from the big data in a naıve sense we can imagine that a number of blind men are trying to size up a giant elephant see fig which will be the big data in this context the goal of each blind man is to draw a picture or conclusion of the elephant according to the part of information he collects during the process because each person view is limited to his local region it is not surprising that the blind men will each conclude independently that the elephant feels like a rope a hose or a wall depending on the region each of them is limited to to make the problem even more complicated let us assume that the elephant is growing rapidly and its pose changes constantly and each blind man may have his own possible unreliable and inaccu rate information sources that tell him about biased knowledge about the elephant e g one blind man may exchange his feeling about the elephant with another blind man where the exchanged knowledge is inherently biased exploring the big data in this scenario is equivalent to aggregating heterogeneous information from different sources blind men to help draw a best possible picture to reveal the genuine gesture of the elephant in a real time fashion indeed this task is not as simple as asking each blind man to describe his feelings about the elephant and then getting an expert to draw one single picture with a combined view concerning that each individual may speak a different language heterogeneous and diverse information sources and they may even have privacy concerns about the messages they deliberate in the information exchange process huge data with heterogeneous and diverse dimensionality one of the fundamental characteristics of the big data is the huge volume of data represented by heterogeneous and fig the blind men and the giant elephant the localized limited view of each blind man leads to a biased conclusion diverse dimensionalities this is because different informa tion collectors prefer their own schemata or protocols for data recording and the nature of different applications also results in diverse data representations for example each single human being in a biomedical world can be represented by using simple demographic information such as gender age family disease history and so on for x ray examination and ct scan of each individual images or videos are used to represent the results because they provide visual information for doctors to carry detailed examinations for a dna or genomic related test micro array expression images and sequences are used to represent the genetic code information because this is the way that our current techniques acquire the data under such circumstances the heterogeneous features refer to the different types of representations for the same individuals and the diverse features refer to the variety of the features involved to represent each single observation imagine that different organizations or health practitioners may have their own schemata to represent each patient the data heterogeneity and diverse dimensionality issues become major challenges if we are trying to enable data aggregation by combining data from all sources autonomous sources with distributed and decentralized control autonomous data sources with distributed and decentra lized controls are a main characteristic of big data applications being autonomous each data source is able to generate and collect information without involving or relying on any centralized control this is similar to the world wide web www setting where each web server provides a certain amount of information and each server is able to fully function without necessarily relying on other servers on the other hand the enormous volumes of the data also make an application vulnerable to attacks or malfunctions if the whole system has to rely on any centralized control unit for major big data related applica tions such as google flicker facebook and walmart a large number of server farms are deployed all over the world to ensure nonstop services and quick responses for local markets such autonomous sources are not only the solutions of the technical designs but also the results of the legislation and the regulation rules in different countries regions for example asian markets of walmart are inherently different from its north american markets in terms of seasonal promotions top sell items and customer behaviors more specifically the local government regula tions also impact on the wholesale management process and result in restructured data representations and data warehouses for local markets complex and evolving relationships while the volume of the big data increases so do the complexity and the relationships underneath the data in an early stage of data centralized information systems the focus is on finding best feature values to represent each observation this is similar to using a number of data fields such as age gender income education background and so on to characterize each individual this type of sample feature representation inherently treats each individual as an independent entity without considering their social connections which is one of the most important factors of wu et al data mining with big data the human society our friend circles may be formed based on the common hobbies or people are connected by biological relationships such social connections commonly exist not only in our daily activities but also are very popular in cyberworlds for example major social network sites such as facebook or twitter are mainly characterized by social functions such as friend connections and followers in twitter the correlations between individuals inherently complicate the whole data representation and any reasoning process on the data in the sample feature representation individuals are regarded similar if they share similar feature values whereas in the sample feature relationship repre sentation two individuals can be linked together through their social connections even though they might share nothing in common in the feature domains at all in a dynamic world the features used to represent the indivi duals and the social ties used to represent our connections may also evolve with respect to temporal spatial and other factors such a complication is becoming part of the reality for big data applications where the key is to take the complex nonlinear many to many data relationships along with the evolving changes into consideration to discover useful patterns from big data collections d ata m ining c hallenges with b ig d ata for an intelligent learning database system to handle big data the essential key is to scale up to the exceptionally large volume of data and provide treatments for the characteristics featured by the aforementioned hace theorem fig shows a conceptual view of the big data processing framework which includes three tiers from inside out with considerations on data accessing and computing tier i data privacy and domain knowledge tier ii and big data mining algorithms tier iii the challenges at tier i focus on data accessing and arithmetic computing procedures because big data are often stored at different locations and data volumes may fig a big data processing framework the research challenges form a three tier structure and center around the big data mining platform tier i which focuses on low level data accessing and computing challenges on information sharing and privacy and big data application domains and knowledge form tier ii which concentrates on high level semantics application domain knowledge and user privacy issues the outmost circle shows tier iii challenges on actual mining algorithms continuously grow an effective computing platform will have to take distributed large scale data storage into consideration for computing for example typical data mining algorithms require all data to be loaded into the main memory this however is becoming a clear technical barrier for big data because moving data across different locations is expensive e g subject to intensive network communication and other io costs even if we do have a super large main memory to hold all data for computing the challenges at tier ii center around semantics and domain knowledge for different big data applications such information can provide additional benefits to the mining process as well as add technical barriers to the big data access tier i and mining algorithms tier iii for example depending on different domain applications the data privacy and information sharing mechanisms between data producers and data consumers can be significantly differ ent sharing sensor network data for applications like water quality monitoring may not be discouraged whereas releasing and sharing mobile users location information is clearly not acceptable for majority if not all applications in addition to the above privacy issues the application domains can also provide additional information to benefit or guide big data mining algorithm designs for example in market basket transactions data each transaction is considered independent and the discovered knowledge is typically represented by finding highly correlated items possibly with respect to different temporal and or spatial restrictions in a social network on the other hand users are linked and share dependency structures the knowledge is then represented by user communities leaders in each group and social influence modeling and so on therefore understanding semantics and application knowledge is important for both low level data access and for high level mining algorithm designs at tier iii the data mining challenges concentrate on algorithm designs in tackling the difficulties raised by the big data volumes distributed data distributions and by complex and dynamic data characteristics the circle at tier iii contains three stages first sparse heterogeneous uncertain incomplete and multisource data are prepro cessed by data fusion techniques second complex and dynamic data are mined after preprocessing third the global knowledge obtained by local learning and model fusion is tested and relevant information is fedback to the preprocessing stage then the model and parameters are adjusted according to the feedback in the whole process information sharing is not only a promise of smooth development of each stage but also a purpose of big data processing in the following we elaborate challenges with respect to the three tier framework in fig tier i big data mining platform in typical data mining systems the mining procedures require computational intensive computing units for data analysis and comparisons a computing platform is therefore needed to have efficient access to at least two types of resources data and computing processors for small scale data mining tasks a single desktop computer which contains hard disk and cpu processors is sufficient ieee transactions on knowledge and data engineering vol no january to fulfill the data mining goals indeed many data mining algorithm are designed for this type of problem settings for medium scale data mining tasks data are typically large and possibly distributed and cannot be fit into the main memory common solutions are to rely on parallel computing 43 33 or collective mining to sample and aggregate data from different sources and then use parallel computing programming such as the message passing interface to carry out the mining process for big data mining because data scale is far beyond the capacity that a single personal computer pc can handle a typical big data processing framework will rely on cluster computers with a high performance computing platform with a data mining task being deployed by running some parallel programming tools such as mapreduce or enterprise control language ecl on a large number of computing nodes i e clusters the role of the software component is to make sure that a single data mining task such as finding the best match of a query from a database with billions of records is split into many small tasks each of which is running on one or multiple computing nodes for example as of this writing the world most powerful super computer titan which is deployed at oak ridge national laboratory in tennessee contains 688 nodes each with a core cpu such a big data system which blends both hardware and software components is hardly available without key industrial stockholders support in fact for decades companies have been making business decisions based on transactional data stored in relational databases big data mining offers opportunities to go beyond traditional relational databases to rely on less structured data weblogs social media e mail sensors and photographs that can be mined for useful information major business intelligence companies such ibm oracle teradata and so on have all featured their own products to help customers acquire and organize these diverse data sources and coordinate with customers existing data to find new insights and capitalize on hidden relationships 2 tier ii big data semantics and application knowledge semantics and application knowledge in big data refer to numerous aspects related to the regulations policies user knowledge and domain information the two most important issues at this tier include data sharing and privacy and 2 domain and application knowledge the former provides answers to resolve concerns on how data are maintained accessed and shared whereas the latter focuses on answering questions like what are the under lying applications and what are the knowledge or patterns users intend to discover from the data 2 information sharing and data privacy information sharing is an ultimate goal for all systems involving multiple parties 24 while the motivation for sharing is clear a real world concern is that big data applications are related to sensitive information such as banking transactions and medical records simple data exchanges or transmissions do not resolve privacy con cerns 19 25 for example knowing people locations and their preferences one can enable a variety of useful location based services but public disclosure of an individual locations movements over time can have serious consequences for privacy to protect privacy two common approaches are to restrict access to the data such as adding certification or access control to the data entries so sensitive information is accessible by a limited group of users only and 2 anonymize data fields such that sensitive information cannot be pinpointed to an indivi dual record 15 for the first approach common chal lenges are to design secured certification or access control mechanisms such that no sensitive information can be misconducted by unauthorized individuals for data anonymization the main objective is to inject randomness into the data to ensure a number of privacy goals for example the most common k anonymity privacy measure is to ensure that each individual in the database must be indistinguishable from k à others common anonymiza tion approaches are to use suppression generalization perturbation and permutation to generate an altered version of the data which is in fact some uncertain data one of the major benefits of the data annomization based information sharing approaches is that once anonymized data can be freely shared across different parties without involving restrictive access controls this naturally leads to another research area namely privacy preserving data mining 30 where multiple parties each holding some sensitive data are trying to achieve a common data mining goal without sharing any sensitive information inside the data this privacy preserving mining goal in practice can be solved through two types of approaches including using special communication protocols such as yao protocol to request the distributions of the whole data set rather than requesting the actual values of each record or 2 designing special data mining methods to derive knowledge from anonymized data this is inherently similar to the uncertain data mining methods 2 2 domain and application knowledge domain and application knowledge provides essential information for designing big data mining algorithms and systems in a simple case domain knowledge can help identify right features for modeling the underlying data e g blood glucose level is clearly a better feature than body mass in diagnosing type ii diabetes the domain and application knowledge can also help design achievable business objectives by using big data analytical techniques for example stock market data are a typical domain that constantly generates a large quantity of information such as bids buys and puts in every single second the market continuously evolves and is impacted by different factors such as domestic and international news government reports and natural disasters and so on an appealing big data mining task is to design a big data mining system to predict the movement of the market in the next one or two minutes such systems even if the prediction accuracy is just slightly better than random guess will bring significant business values to the developers 9 without correct domain knowledge it is a clear challenge to find effective matrices measures to characterize the market movement and such knowledge is often beyond the mind of the data miners although some recent research has shown that using social networks such as twitter it is wu et al data mining with big data possible to predict the stock market upward downward trends 7 with good accuracies tier iii big data mining algorithms local learning and model fusion for multiple information sources as big data applications are featured with autonomous sources and decentralized controls aggregating distributed data sources to a centralized site for mining is system atically prohibitive due to the potential transmission cost and privacy concerns on the other hand although we can always carry out mining activities at each distributed site the biased view of the data collected at each site often leads to biased decisions or models just like the elephant and blind men case under such a circumstance a big data mining system has to enable an information exchange and fusion mechanism to ensure that all distributed sites or information sources can work together to achieve a global optimization goal model mining and correlations are the key steps to ensure that models or patterns discovered from multiple information sources can be consolidated to meet the global mining objective more specifically the global mining can be featured with a two step local mining and global correlation process at data model and at knowledge levels at the data level each local site can calculate the data statistics based on the local data sources and exchange the statistics between sites to achieve a global data distribution view at the model or pattern level each site can carry out local mining activities with respect to the localized data to discover local patterns by exchanging patterns between multiple sources new global patterns can be synthetized by aggregating patterns across all sites at the knowledge level model correlation analysis investigates the relevance between models gener ated from different data sources to determine how relevant the data sources are correlated with each other and how to form accurate decisions based on models built from autonomous sources 3 2 mining from sparse uncertain and incomplete data spare uncertain and incomplete data are defining features for big data applications being sparse the number of data points is too few for drawing reliable conclusions this is normally a complication of the data dimensionality issues where data in a high dimensional space such as more than 000 dimensions do not show clear trends or distribu tions for most machine learning and data mining algorithms high dimensional spare data significantly de teriorate the reliability of the models derived from the data common approaches are to employ dimension reduction or feature selection to reduce the data dimensions or to carefully include additional samples to alleviate the data scarcity such as generic unsupervised learning methods in data mining uncertain data are a special type of data reality where each data field is no longer deterministic but is subject to some random error distributions this is mainly linked to domain specific applications with inaccurate data readings and collections for example data produced from gps equipment are inherently uncertain mainly because the technology barrier of the device limits the precision of the data to certain levels such as meter as a result each recording location is represented by a mean value plus a variance to indicate expected errors for data privacy related applications 36 users may intentionally inject randomness errors into the data to remain anonymous this is similar to the situation that an individual may not feel comfortable to let you know his her exact income but will be fine to provide a rough range like for uncertain data the major challenge is that each data item is represented as sample distributions but not as a single value so most existing data mining algorithms cannot be directly applied common solutions are to take the data distributions into consideration to estimate model parameters for example error aware data mining utilizes the mean and the variance values with respect to each single data item to build a naıve bayes model for classification similar approaches have also been applied for decision trees or database queries incomplete data refer to the missing of data field values for some samples the missing values can be caused by different realities such as the malfunction of a sensor node or some systematic policies to intentionally skip some values e g dropping some sensor node readings to save power for transmission while most modern data mining algorithms have in built solutions to handle missing values such as ignoring data fields with missing values data imputation is an estab lished research field that seeks to impute missing values to produce improved models compared to the ones built from the original data many imputation methods 20 exist for this purpose and the major approaches are to fill most frequently observed values or to build learning models to predict possible values for each data field based on the observed values of a given instance 3 3 3 mining complex and dynamic data the rise of big data is driven by the rapid increasing of complex data and their changes in volumes and in nature 6 documents posted on www servers internet back bones social networks communication networks and transportation networks and so on are all featured with complex data while complex dependency structures underneath the data raise the difficulty for our learning systems they also offer exciting opportunities that simple data representations are incapable of achieving for example researchers have successfully used twitter a well known social networking site to detect events such as earthquakes and major social activities with nearly real time speed and very high accuracy in addition by summarizing the queries users submitted to the search engines which are all over the world it is now possible to build an early warning system for detecting fast spreading flu outbreaks making use of complex data is a major challenge for big data applications because any two parties in a complex network are potentially interested to each other with a social connection such a connection is quadratic with respect to the number of nodes in the network so a million node network may be subject to one trillion connections for a large social network site like facebook the number of active users has already reached billion and analyzing such an enormous network is a big challenge for big data mining if we take daily user actions interactions into consideration the scale of diffi culty will be even more astonishing 102 ieee transactions on knowledge and data engineering vol no january inspired by the above challenges many data mining methods have been developed to find interesting knowl edge from big data with complex relationships and dynamically changing volumes for example finding communities and tracing their dynamically evolving rela tionships are essential for understanding and managing complex systems 3 10 discovering outliers in a social network 8 is the first step to identify spammers and provide safe networking environments to our society if only facing with huge amounts of structured data users can solve the problem simply by purchasing more storage or improving storage efficiency however big data complexity is represented in many aspects including complex heterogeneous data types complex intrinsic semantic associations in data and complex relationship networks among data that is to say the value of big data is in its complexity complex heterogeneous data types in big data data types include structured data unstructured data and semistruc tured data and so on specifically there are tabular data relational databases text hyper text image audio and video data and so on the existing data models include key value stores bigtable clones document databases and graph databases which are listed in an ascending order of the complexity of these data models traditional data models are incapable of handling complex data in the context of big data currently there is no acknowledged effective and efficient data model to handle big data complex intrinsic semantic associations in data news on the web comments on twitter pictures on flicker and clips of video on youtube may discuss about an academic award winning event at the same time there is no doubt that there are strong semantic associations in these data mining complex semantic associations from text image video data will significantly help improve application system performance such as search engines or recommendation systems however in the context of big data it is a great challenge to efficiently describe semantic features and to build semantic association models to bridge the semantic gap of various heterogeneous data sources complex relationship networks in data in the context of big data there exist relationships between individuals on the internet individuals are webpages and the pages linking to each other via hyperlinks form a complex network there also exist social relationships between individuals forming complex social networks such as big relationship data from facebook twitter linkedin and other social media 5 13 including call detail records cdr devices and sensors information 44 gps and geocoded map data massive image files transferred by the manage file transfer protocol web text and click stream data 2 scientific information e mail and so on to deal with complex relationship networks emerging research efforts have begun to address the issues of structure and evolution crowds and interac tion and information and communication the emergence of big data has also spawned new computer architectures for real time data intensive proces sing such as the open source apache hadoop project that runs on high performance clusters the size or complexity of the big data including transaction and interaction data sets exceeds a regular technical capability in capturing mana ging and processing these data within reasonable cost and time limits in the context of big data real time processing for complex data is a very challenging task 4 r esearch i nitiatives and p rojects to tackle the big data challenges and seize the opportunities afforded by the new data driven resolu tion the us national science foundation nsf under president obama administration big data initiative announced the bigdata solicitation in such a federal initiative has resulted in a number of winning projects to investigate the foundations for big data management led by the university of washington analytical approaches for genomics based massive data computation led by brown university large scale machine learning techniques for high dimensional data sets that may be as large as 500 000 dimensions led by carnegie mellon university social analytics for large scale scientific literatures led by rutgers university and several others these projects seek to develop methods algorithms frameworks and research infrastructures that allow us to bring the massive amounts of data down to a human manageable and interpretable scale other coun tries such as the national natural science foundation of china nsfc are also catching up with national grants on big data research meanwhile since the authors have taken the lead in the following national projects that all involve big data components integrating and mining biodata from multiple sources in biological networks sponsored by the us national science foundation medium grant no ccf october 30 september issues and significance we have integrated and mined biodata from multiple sources to decipher and utilize the structure of biological networks to shed new insights on the functions of biological systems we address the theoretical underpinnings and current and future enabling technologies for integrating and mining biological networks we have expanded and integrated the techniques and methods in information acquisition transmission and processing for information networks we have developed methods for semantic based data integra tion automated hypothesis generation from mined data and automated scalable analytical tools to evaluate simulation results and refine models big data fast response real time classification of big data stream sponsored by the australian research council arc grant no 1 january dec issues and significance we propose to build a stream based big data analytic framework for fast response and real time decision making the key challenges and research issues include designing big data sampling mechanisms to reduce big data volumes to a manageable size for processing building prediction models from big data streams such models can adaptively adjust to wu et al data mining with big data the dynamic changing of the data as well as accurately predict the trend of the data in the future and a knowledge indexing framework to ensure real time data monitoring and classification for big data applications pattern matching and mining with wildcards and length constraints sponsored by the national natural science foundation of china grant nos phase 1 1 january december and phase 2 1 january december issues and significance we perform a systematic investigation on pattern matching pattern mining with wildcards and application problems as follows exploration of the np hard complexity of the matching and mining problems multiple pattern matching with wildcards approximate pattern matching and mining and application of our research onto ubiquitous personalized information processing and bioin formatics key technologies for integration and mining of multiple heterogeneous data sources sponsored by the national high technology research and devel opment program program of china grant no 1 january december issues and significance we have performed an investigation on the availability and statistical regularities of multisource massive and dynamic information including cross media search based on information extraction sampling uncertain informa tion querying and cross domain and cross platform information polymerization to break through the limitations of traditional data mining methods we have studied heterogeneous information discovery and mining in complex inline data mining in data streams multigranularity knowledge discovery from massive multisource data distribution regula rities of massive knowledge quality fusion of massive knowledge group influence and interactions in social networks sponsored by the national basic research program of china grant no 1 january december issues and significance we have studied group influence and interactions in social networks including employing group influence and information diffusion models and deliberating group interaction rules in social networks using dynamic game theory studying interactive individual selection and effect evaluations under social networks affected by group emotion and analyzing emotional interactions and influence among individuals and groups and establishing an interactive influence model and its computing methods for social network groups to reveal the interactive influence effects and evolution of social networks w 5 r elated ork 5 1 big data mining platforms tier i due to the multisource massive heterogeneous and dynamic characteristics of application data involved in a distributed environment one of the most important characteristics of big data is to carry out computing on the petabyte pb even the exabyte eb level data with a complex computing process therefore utilizing a parallel computing infrastructure its corresponding programming language support and software models to efficiently analyze and mine the distributed data are the critical goals for big data processing to change from quantity to quality currently big data processing mainly depends on parallel programming models like mapreduce as well as providing a cloud computing platform of big data services for the public mapreduce is a batch oriented parallel computing model there is still a certain gap in perfor mance with relational databases improving the perfor mance of mapreduce and enhancing the real time nature of large scale data processing have received a significant amount of attention with mapreduce parallel program ming being applied to many machine learning and data mining algorithms data mining algorithms usually need to scan through the training data for obtaining the statistics to solve or optimize model parameters it calls for intensive computing to access the large scale data frequently to improve the efficiency of algorithms chu et al proposed a general purpose parallel programming method which is applicable to a large number of machine learning algo rithms based on the simple mapreduce programming model on multicore processors ten classical data mining algorithms are realized in the framework including locally weighted linear regression k means logistic regression naive bayes linear support vector machines the indepen dent variable analysis gaussian discriminant analysis expectation maximization and back propagation neural networks 14 with the analysis of these classical machine learning algorithms we argue that the computational operations in the algorithm learning process could be transformed into a summation operation on a number of training data sets summation operations could be per formed on different subsets independently and achieve penalization executed easily on the mapreduce program ming platform therefore a large scale data set could be divided into several subsets and assigned to multiple mapper nodes then various summation operations could be performed on the mapper nodes to collect intermediate results finally learning algorithms are executed in parallel through merging summation on reduce nodes ranger et al proposed a mapreduce based application programming interface phoenix which supports parallel programming in the environment of multicore and multi processor systems and realized three data mining algo rithms including k means principal component analysis and linear regression gillick et al 22 improved the mapreduce implementation mechanism in hadoop evaluated the algorithms performance of single pass learning iterative learning and query based learning in the mapreduce framework studied data sharing between computing nodes involved in parallel learning algorithms distributed data storage and then showed that the mapreduce mechanisms suitable for large scale data 104 ieee transactions on knowledge and data engineering vol no 1 january mining by testing series of standard data mining tasks on medium size clusters papadimitriou and sun 38 pro posed a distributed collaborative aggregation disco framework using practical distributed data preprocessing and collaborative aggregation techniques the implementa tion on hadoop in an open source mapreduce project showed that disco has perfect scalability and can process and analyze massive data sets with hundreds of gb to improve the weak scalability of traditional analysis software and poor analysis capabilities of hadoop systems das et al conducted a study of the integration of r open source statistical analysis software and hadoop the in depth integration pushes data computation to parallel processing which enables powerful deep analysis capabil ities for hadoop wegener et al 47 achieved the integration of weka an open source machine learning and data mining software tool and mapreduce standard weka tools can only run on a single machine with a limitation of 1 gb memory after algorithm parallelization weka breaks through the limitations and improves performance by taking the advantage of parallel computing to handle more than 100 gb data on mapreduce clusters ghoting et al 21 proposed hadoop ml on which developers can easily build task parallel or data parallel machine learning and data mining algorithms on program blocks under the language runtime environment 5 2 big data semantics and application knowledge tier ii in privacy protection of massive data ye et al proposed a multilayer rough set model which can accurately describe the granularity change produced by different levels of generalization and provide a theoretical foundation for measuring the data effectiveness criteria in the anonymization process and designed a dynamic mechanism for balancing privacy and data utility to solve the optimal generalization refinement order for classifica tion a recent paper on confidentiality protection in big data 4 summarizes a number of methods for protecting public release data including aggregation such as k anonymity i diversity etc suppression i e deleting sensitive values data swapping i e switching values of sensitive data records to prevent users from matching adding random noise or simply replacing the whole original data values at a high risk of disclosure with values synthetically generated from simulated distributions for applications involving big data and tremendous data volumes it is often the case that data are physically distributed at different locations which means that users no longer physically possess the storage of their data to carry out big data mining having an efficient and effective data access mechanism is vital especially for users who intend to hire a third party such as data miners or data auditors to process their data under such a circumstance users privacy restrictions may include 1 no local data copies or downloading 2 all analysis must be deployed based on the existing data storage systems without violating existing privacy settings and many others in wang et al a privacy preserving public auditing mechanism for large scale data storage such as cloud computing systems has been proposed the public key based mechanism is used to enable third party auditing tpa so users can safely allow a third party to analyze their data without breaching the security settings or compromising the data privacy for most big data applications privacy concerns focus on excluding the third party such as data miners from directly accessing the original data common solutions are to rely on some privacy preserving approaches or encryp tion mechanisms to protect the data a recent effort by lorch et al indicates that users data access patterns can also have severe data privacy issues and lead to disclosures of geographically co located users or users with common interests e g two users searching for the same map locations are likely to be geographically colocated in their system namely shround users data access patterns from the servers are hidden by using virtual disks as a result it can support a variety of big data applications such as microblog search and social network queries without compromising the user privacy 5 3 big data mining algorithms tier iii to adapt to the multisource massive dynamic big data researchers have expanded existing data mining methods in many ways including the efficiency improvement of single source knowledge discovery methods 11 designing a data mining mechanism from a multisource perspective as well as the study of dynamic data mining methods and the analysis of stream data 12 the main motivation for discovering knowledge from massive data is improving the efficiency of single source mining methods on the basis of gradual improvement of computer hardware functions researchers continue to explore ways to improve the efficiency of knowledge discovery algo rithms to make them better for massive data because massive data are typically collected from different data sources the knowledge discovery of the massive data must be performed using a multisource mining mechanism as real world data often come as a data stream or a characteristic flow a well established mechanism is needed to discover knowledge and master the evolution of knowl edge in the dynamic data source therefore the massive heterogeneous and real time characteristics of multisource data provide essential differences between single source knowledge discovery and multisource data mining wu et al 51 proposed and established the theory of local pattern analysis which has laid a foundation for global knowledge discovery in multisource data mining this theory provides a solution not only for the problem of full search but also for finding global models that traditional mining methods cannot find local pattern analysis of data processing can avoid putting different data sources together to carry out centralized computing data streams are widely used in financial analysis online trading medical testing and so on static knowledge discovery methods cannot adapt to the characteristics of dynamic data streams such as continuity variability rapidity and infinity and can easily lead to the loss of useful information therefore effective theoretical and technical frameworks are needed to support data stream mining 18 57 knowledge evolution is a common phenomenon in real world systems for example the clinician s treatment programs will constantly adjust with the conditions of the patient such as family economic status health insurance the course of treatment treatment effects and distribution wu et al data mining with big data of cardiovascular and other chronic epidemiological changes with the passage of time in the knowledge discovery process concept drifting aims to analyze the phenomenon of implicit target concept changes or even fundamental changes triggered by dynamics and context in data streams according to different types of concept drifts knowledge evolution can take forms of mutation drift progressive drift and data distribution drift based on single features multiple features and streaming features 6 c onclusions driven by real world applications and key industrial stakeholders and initialized by national funding agencies managing and mining big data have shown to be a challenging yet very compelling task while the term big data literally concerns about data volumes our hace theorem suggests that the key characteristics of the big data are 1 huge with heterogeneous and diverse data sources 2 autonomous with distributed and decentralized control and 3 complex and evolving in data and knowledge associations such combined characteristics suggest that big data require a big mind to consolidate data for maximum values 27 to explore big data we have analyzed several chal lenges at the data model and system levels to support big data mining high performance computing platforms are required which impose systematic designs to unleash the full power of the big data at the data level the autonomous information sources and the variety of the data collection environments often result in data with complicated conditions such as missing uncertain values in other situations privacy concerns noise and errors can be introduced into the data to produce altered data copies developing a safe and sound information sharing protocol is a major challenge at the model level the key challenge is to generate global models by combining locally discovered patterns to form a unifying view this requires carefully designed algorithms to analyze model correlations between distributed sites and fuse decisions from multiple sources to gain a best model out of the big data at the system level the essential challenge is that a big data mining framework needs to consider complex relationships between samples models and data sources along with their evolving changes with time and other possible factors a system needs to be carefully designed so that unstructured data can be linked through their complex relationships to form useful patterns and the growth of data volumes and item relationships should help form legitimate patterns to predict the trend and future we regard big data as an emerging trend and the need for big data mining is arising in all science and engineering domains with big data technologies we will hopefully be able to provide most relevant and most accurate social sensing feedback to better understand our society at real time we can further stimulate the participation of the public audiences in the data production circle for societal and economical events the era of big data has arrived in growing numbers scholars are integrating social media tools like blogs twitter and mendeley into their professional communications the online public nature of these tools exposes and reifies scholarly processes once hidden and ephemeral metrics based on this activities could inform broader faster measures of impact complementing traditional citation metrics this study explores the properties of these social media based metrics or altmetrics sampling articles published by the public library of science we find that that different indicators vary greatly in activity around of sampled articles are cited in wikipedia while close to have been included in at least one mendeley library there is however an encouraging diversity a quarter of articles have nonzero data from five or more different sources correlation and factor analysis suggest citation and altmetrics indicators track related but distinct impacts with neither able to describe the complete picture of scholarly use alone there are moderate correlations between mendeley and web of science citation but many altmetric indicators seem to measure impact mostly orthogonal to citation articles cluster in ways that suggest five different impact flavors capturing impacts of different types on different audiences for instance some articles may be heavily read and saved by scholars but seldom cited together these findings encourage more research into altmetrics as complements to traditional citation measures introduction social media tools are beginning to affect the research workflow growing numbers of scholars discuss and share the research literature on twitter organize it in social reference managers like mendeley and zotero and review it in blogs article comments and post publication peer review services like faculty of while this incorporation of social media into the research workflow may improve the overall responsiveness and timeliness of scholarly communication it also has a powerful secondary advantage exposing and fixing scholarly processes once hidden and ephemeral the daily work of scholars is moving online as it does the background of scholarship the dog eared manuscripts and hallway conversations is pushed out on to the stage traditionally this stage has been occupied almost exclusively by citations researchers have turned to these pellets of peer recognition to help track the flow of scholarly ideas and have been rewarded with an impressive and growing understanding of how scholarship is transmitted and adopted however citation tracking has never been able to follow the less visible but often more important threads of invisible colleges woven through personal connections and informal communications the increasing online reification of these ethereal threads gives us a chance to fill this gap building a deeper richer map of scholarly information flows so called alternative metrics or altmetrics build on information from social media use and could be employed side by side with citations one tracking formal acknowledged influence and the tracking the unintentional and informal scientific street cred altmetrics could deliver information about impact on diverse audiences like clinicians practitioners and the general public as well as help to track the use of diverse research products like datasets software and blog posts the future then could see altmetrics and traditional bibliometrics presented together as complementary tools presenting a nuanced multidimensional view of multiple research impacts at multiple time scales before this can be done however it is important to describe and characterize social media tools as sources for metrics scholarly users of tools like mendeley and twitter remain a minority among their often skeptical peers is there in fact enough data available to construct meaningful metrics how is data distributed across tools users and time how do altmetrics relate to accepted citation measures there is a need for exploratory research to answer these and other questions presenting early descriptive findings to guide more focused inquiry this paper presents the results of our study investigating these questions literature review although citation metrics have become increasingly important in the measure of research impact they are not without flaws citations take time to accumulate the citation latency of even high impact articles may by years or even longer citations have been used to measure only one type of research product the peer reviewed article this leaves out other important products such as datasets similarly citations measure impact only on that minority of an article audience with the means and inclination to cite it in the literature leaving out clinicians the general public and even most scientists by some estimates only about to of scientists in the united states have authored a refereed article the most popular citation metric thomson scientific journal impact factor has been a lightning rod for critics who have argued that it fails to correlate with impact at the article level is arbitrary and irreproducible and is easy to game the shortcomings of citation metrics are not news scientometricians and others have for decades worked to create more realistically diverse measures of research impact painstakingly gathering diverse indicators including patents acknowledgements doctoral committee membership and many others the web brought with it the potential for yet more metrics gathered electronically and at scale swapping citation indexes for search engines practitioners of webometrics gathered indicators from hyperlinks and online mentions on webpages syllabi presentations and other online resources at the same time researchers began to investigate the usage records created by online article delivery as yet another source of impact evidence https arxiv org html https arxiv org html these new approaches however have not completely filled the gap between citation metrics and real world impact webometric approaches that rely on search engines are fundamentally limited by terms of use restrictions on automated mining of results consequently webometrics must be compiled manually on a custom basis this approach does not work at large scale usage metrics are automatically gathered but also scale poorly for a similar reason publishers are unwilling to release data for widespread use however there is growing interest in a new type of web based metric that may scale more effectively and present an even broader picture of scholarly impact metrics based on social media activities social media tools are becoming increasingly important in scholars workflows as several recent studies have made clear report that of uk academics frequently use web in novel forms of scholarly communications while find that of scholars have social media accounts carpenter et al report that around of uk doctoral students use and value twitter for research and also report that social media tools are affecting the scholarly workflow these tools include social reference managers twitter blogs bookmarking services and more importantly these tools do not create new types of scholarly practice so much as they facilitateexisting practice social reference managers like mendeley for example are an extension of paper based bibliography collections academics have maintained for centuries while twitter facilitates the sort of informal conference chats that have long vivified the academy invisible colleges consequently many have suggested that these tools open a valuable window on heretofore hidden scholarly processes finally fulfilling the early web promise to give substance to modes of influence which have historically been backgrounded in narratives of science some suggest using social media to assess scholars authority while others propose social media as a sort of soft peer review social media tools typically offer open apis facilitating large scale use unlike most search engines and most journal publishers offering potential for a scientometrics more recently altmetrics has been used to describe alternative metrics based on social media with proponents touting their speed richness and breadth as these rallying calls have gone out several tools have emerged to gather altmetrics including in approximate order of appearance plos article level metrics readermeter citedin total impact altmetric com and sciencecard a growing body of research has begun to test the claims of altmetrics value examining two main questions first how much altmetrics data exists and how is it distributed and second what do altmetrics measure and how does it correlate with citation inquiries into the first question have tended to find scholarly use of social media relatively rare but significant and growing examinations of journal based commenting systems or rapid responses uncover a very skewed distribution half of sampled journals in have not attracted a single comment on any article though a few active journals attracted comments on most articles although only an estimated of scholars actively use twitter this number is growing steadily and around of sampled articles from the arxiv preprint repository have been tweeted usage of scientific terms on twitter is modest but reveals interesting usage patterns social reference manager applications have attracted significant use mendeley has been particularly successful advertising on their homepage that over million users have collected million documents over of nature and science articles are included in mendeley collections although this high number no doubt in part reflecting the popularity of these publications in contrast find only of physics articles bookmarked in citeulike researchers have also looked beyond counts of social media users and uses to assess types of use do social media really host scholarship or just idle chatter what do social media uses or citations mean twitter for instance is often perceived as a place to do little more than discuss one lunch this so called cheese sandwich problem seems to be significantly overstated however as several studies have presented evidence of scholars using the service to enrich academic conferences as well as cite scholarly literature as many as of tweets from scholars contain scholarly content examinations of social reference managers have focused more on the potential uses of available data demonstrating value in collaborative filtering approaches and use of tag based folksonomies post publication peer review service faculty of has attracted several studies of its article rankings with inconsistent findings reports that the reviewers failed to spot many significant articles while emphasize that reviewers uncovered many hidden gems several studies have attempted to relate the unknown impacts of social media to the more accepted yardstick of citation citations in wikipedia correlate with those in the journal citation report though slightly over citing high impact journals similarly while discussions in scholarly blogs pull from diverse sources they tend to focus on articles published in highly cited journals in a sample of economics blogs report correlation between pageviews and per year citation of blogs contributors is several studies have compared traditional citation with usage or bookmarking in social reference managers suggesting weak to moderate correlation correlations of and are reported between mendeley users and web of science citations of science and nature articles and for citeulike as well as a correlation of between mendeley users and citations for top biology papers using a more heterogeneous dataset of physics papers from journals reports a correlation of between saves into citeulike and web of science citations although the top paper was the same for both metrics data from the open access publisher public library of science plos article level metrics alm reveal a correlation of between citeulike saves and scopus citations more recently examined articles in the journal of medical internet research that had attracted links from tweets or twitter citations articles in the most tweeted quartile after one week were eleven times more likely to be in the most cited quartile after two years for the top most tweeted articles in a sample from arxiv repository log of tweet counts correlates weakly with log of citation counts r methods data collection in november we collected identifiers for all articles published to date in the seven journals of open access publisher public library of science plos and gathered a set of approximately million altmetrics events defined as a specific action on a specific article each event was of a certain event type such as delicious bookmark or pdf download using common scientometrics terminology event types could also be called indicators we obtained counts for a diverse set of event types as described in table and table using a combination techniques including searching the plos alm api writing custom scripts to search apis of external data sources and writing a custom script to crawl and download plos comments in november and again in december we gathered thomson reuters web of science citation counts manually through the web of science website interface table api data on articles event level data delicious bookmarks includes author timestamp tags yes wikipedia counts of links out to articles counts only https arxiv org html https arxiv org html mendeley counts of users bookmarking articles includes location discipline of users https arxiv org html counts only facebook counts of clicks likes shares and comments counts only twitter via backtweets linking tweets includes author timestamp text yes citeulike viaplosalm saves into reference library includes author timestamp tags yes nature blogs viaplosalm linking blog posts incl timestamp author url yes postgenomic blogs viaplosalm linking blog posts incl timestamp author url yes research blogging blogs viaplosalm linking blog posts incl timestamp author url yes scopus citations viaplosalm counts of citations counts only crossref citations viaplosalm articles citing includes bibliographic information yes pubmed central citations viaplosalm counts of citations includes pubmed ids counts only monthly pdf html and xml views downloads viaplosalm counts of pageviews and downloads by month counts only plos comments count comments on articles incl timestamp author yes web of science citations articles citing includes bibliographic information yes faculty of rankings numerical ratings counts only table citation and altmetrics data collected for this study an denotes metrics no longer available for information on access methods and rate limits see supplemental materials normalization and transformation simply comparing raw counts is highly affected by confounding trends such as the length of time it took for various metrics to accumulate and changes in overall user bases of various tools over the seven year period of our sample therefore we need a way to isolate altmetrics activity on articles from other trends in order to compare apples to apples to do this we used a technique to express event counts as a percentage of what count we would expect for that article compared to other articles published in the same journal and at around the same time more precisely we smooth the number of events based on the average metric value within a given time window days before and after the publication date of the current article we apply a standard digital signal processing technique to avoid inducing noise from outliers suddenly appearing then disappearing in the averaging window weighting the points by a hamming window entered on the current publication date these weights are shown in figure figure hamming window used for weighting metric values included in smoothed averages centered on article publication date and extending days before and after because different communities have different levels of social media adoption and readership levels see http www plosone org static journalstatistics action we needed to also normalize by journal when we normalized an article we only included other articles from the same journal in the normalization window the normalized values were set to missing for metric values without at least nonzero metric points in the same journal in the surrounding year finally we know that the distribution of these counts is heavily skewed with most articles receiving ones or zeroes depending on the metric and just a few with very high counts we log transformed the normalized counts to even out the distributions adding to all counts since there is no log of zero since evaluation scores are not count data they were not log transformed statistical analysis data was analyzed using the r statistics environment version with the following libraries clvalid ggdendro gparotation gplots nfactors plyr psych rms rweka signal sqldf xtable and zoo https arxiv org html correlations between metrics and citation count were computed on unnormalized data for articles published in we applied the spearman nonparametric ranking correlation method to estimate correlations the overall correlation matrix was computed across all articles calculating pearson correlation coefficients on pairwise complete normalized and transformed indicator values first order exploratory factor analysis was performed with the fa function in the psych library on this overall correlation matrix using the minimum residual minres solution and a promax oblique rotation we used k means cluster analysis to group papers based on patterns of impact represented by a subselection of indicators k means cluster centers were calculated for plos one articles published before cluster membership was assigned for plos one papers published in based on euclidian distance to these centers and rules to assign these same cluster memberships were derived using the weka jrip algorithm through rweka evaluation of the accuracy of rules to predict k means cluster membership was done through cross validation using the function data and code availability datasets and statistics scripts are availableongithub https github com jasonpriem and from the dryad digital repository citation to be included pending manuscript acceptance under a cczero http creativecommons org publicdomain zero waiver results and discussion in this section we first present an overview for the dataset followed by results and discussion structured around our four main research questions how much and what kind of altmetrics data exist how are altmetrics distributed over time how do altmetrics relate to one another and to traditional citations can we cluster articles of different impact types using altmetrics dataset attributes we collected altmetrics data for plos publications between and editorials perspectives etc were excluded from the analysis dataset resulting in research articles the research articles were distributed across the seven plos journals as shown in table pbio pcbi pgen pmed pntd pone ppat na plos biology https arxiv org html plos plos computational biology plos plos neglected genetics medicine tropical diseases plos one plos pathogens 176 145 873 210 110 4003 436 423 5596 sum 21096 table distribution of articles by journal and publication date as shown previously in table we collected data from a wide variety of sources there were two main types of data events and event counts events were actions applied to articles such as bookmarks or tweets events included metadata such as timestamps and creator identifiers this supported fine grained examination of for instance number of events within one week of article publication or mean number of event creators per article event counts were simply the total number of events of that type to date one particularly distinct type of event was downloads separated into pdf html and xml the latter is mostly for machine consumption and is consequently disregarded in most of the following analysis html downloads are likely browser views in almost all cases downloads are binned into monthly download events whose magnitude represents the number of views that month how much and what kind of altmetrics data exist unsurprisingly nearly every article had at least some pdf and html downloads according to the plos article level metrics website these statistics do not include robot or crawler downloads a full of articles had at least one bookmark on mendeley while had been bookmarked on citeulike this finding is in keeping with earlier suggestions that the mendeley community is significantly more active than the citeulike community in december of plos research articles had received one citation in web of science and of articles had citations from papers in pubmed central a free source of citation information by december more than of research articles had received at least one citation in web of science for plos one https arxiv org html as shown in figure other types of events had less activity ten to twelve percent of articles had been bookmarked on delicious tweeted shared on facebook or received a comment on the plos website although note that actual tweet counts are likely higher since twitter data was incomplete about of articles were the topic of a blog post or had received an rating about of articles had been cited on wikipedia liked on facebook or commented on in facebook there are two groups evident in the activity graph with the separation occurring between facebook of articles shared and saves into citeulike of articles this may be explained by the novel kinds of work required by some event types the high activity group tracks activities reading saving citing that are requirements of traditional scholarly communication in contrast to the low activity group tracking new communication modes unrewarded in the traditional publication cycle https arxiv org html figure proportion of articles with at least one event by metric attention across the metrics was not concentrated on only a few papers we defined an engaged indicator for a given paper as an indicator having a value of at least one for that paper for example an article with no events except pdf views and a single tweet would have two engaged indicators counting engaged indicators per article is a good way to see whether attention to articles is spread out over multiple systems figure shows a histogram of papers by number of engaged indicators citations are represented here with just the web of science source half of the papers in our dataset had at least four engaged indicators usually html page views pdf downloads maybe saves into mendeley and a citation a quarter of the papers had at least five engaged indicators in our dataset of papers received attention from six or more event types figure histogram of papers by the number of metrics with at least one event as described in table some of the indicators had associated qualitative information while others are simply event counts collected qualitative information differed between services with some sources returning much richer information this is in keeping with previous findings that social tags on research articles add a third layer of perception besides the author and indexer perspectives figure demonstrates this https arxiv org html for a single exemplar article comparing tagclouds of tags returned by plos metadata delicious and citeulike https arxiv org html figure illustration of tag metadata captured by plos delicious and citeulike for one article in figure the number of tweets and bookmarks by content creators roughly approximates a straight line on a log log plot this familiar distribution indicates that a few power users post the lion share of events citeulike saves delicious bookmarks or tweets on articles in the dataset while the majority of users reside in the long tail contributing just a few events figure histogram of number of distinct plos related events from each creator by service log log scale plos one the bars that drop below the line represent the value of zero how are metrics distributed over time we first examined the delay between article publication and different types of events following in the footsteps of earlier such investigations of download and citation latency as well as more recent examinations of latency between article publication and tweets figure shows the results when we look at the timing of events in aggregate across the lifecycle of an article we can see that the rates of bookmarking commenting and sharing decay more quickly after publication than website views and pdf downloads citeulike saves do not drop off quite as quickly which may be related to its prominent position within scholars document workflows this bears additional investigation https arxiv org html https arxiv org html figure total number of events each week aggregated across all articles vs time after article publication by metric type for metric types with timestamped events log scale we next examined the distribution of events by article publication date and event type as shown in figure here three main distributions are visible citations pageviews and wikipedia citations all show linear relationships between article age and number of events the older an article is the more events it has suggesting that events are accumulating steadily if more slowly over time social reference managers citeulike and mendeley as well as delicious bookmarks and ratings fall into a second distribution in which the number of events is relatively unaffected by articles ages this is likely due to a much smaller half life in these environments as suggested in figure after just a year articles have accumulated about as much activity in these environments as they ever will finally a third distribution is displayed by twitter facebook plos comments and blogs in all these cases data are somewhat confused due to changes in adoption of the services and factors relating to data collection and quality for example plos discontinued posting editorial comments in plos one blogging networks come and go and our twitter source only goes back one year this demonstrates the challenge of gathering data from sources that can appear become popular evolve and decline all in a few years and underscores the importance of conditioning and normalizing raw data https arxiv org html https arxiv org html 18 differences in behavior across journal can be observed in figure for example articles published in plos medicine green were cited often relative to articles in plos computational biology light orange in contrast articles in plos computational biology were saved in mendeley or citeulike more often than articles in plos medicine figure average metric levels over time by journal averages are displayed for metric sources with events across at least articles in the surrounding year in the given journal finally figure looks more closely at a specific article illustrating how events reveal patterns over time and in relationship to one another unsurprisingly delicious citeulike and plos comments are active immediately after the article publication interestingly though in this case a second wave of activity arrived around four months later possibly initiated by readers alerted to the article by its first external citation which may reported slightly late here due to issues of granularity in journal publication dates although beyond the scope of this paper this sort of data could be aggregated from thousands of individual articles to identify and classify patterns of dissemination these in turn could inform powerful predictive models https arxiv org html https arxiv org html 18 figure an illustrative example of altmetric activity timelines for the article doi journal pone each point represents a single event results how do altmetrics relate to one another and to citations in exploring the potential of altmetrics it is of great importance to see how metrics relate to one another in particular it is useful to establish the relationship between altmetrics and the relatively well researched metric of traditional citation counts to do this we first examined correlations between normalized event counts of different types then we performed factor analysis before turning to examine the altmetrics citation relationship more closely correlation figure presents pearson correlations between all gathered metrics this graphic is quite similar to the visualization of plos article level metrics correlations in however it differs in that it contains data from sources not reported by plos and that correlations are on log transformed counts that have been normalized by metric type date and journal a few trends are immediately apparent citation measures cluster closely together joined by social reference managers mendeley and citeulike this is unsurprising given the overwhelmingly scholarly audience of these tools and their integration in the citing process pdf and html downloads correlate at moderate to high levels with almost every other indicator this too is to be expected given that most types of use both require that someone has looked at the article and tend to encourage others to do so is notable in its relative lack of correlation with anything but citations and social reference managers finally it is interesting that most indicators correlate better with web of science citation counts than with counts this may be because later counts are higher overcoming noise from low early counts figure pearson correlations of all journals all years normalized and transformed https arxiv org html factors dimensions of impact to examine the intercorrelation among indicators we turned to factor analysis exploratory factor analysis is a statistical approach to distill variability shared across observed variables in terms of unobserved variables called factors factors often facilitate an understanding of the underlying structure of data and can be used for dimension reduction we chose to explore a solution with six factors based on scree plots and the size of our dataset the rotated first order factors are given in table with loadings larger than or less than factors in table are presented in the order of factor extraction the factor that describes the most shared variation is the column furthest to the left some of the loadings are greater than one this is not unexpected since the factors are oblique and thus the loadings in the pattern matrix represent regression coefficients rather than correlations correlations between attributes and the factors are given in the structure matrix in table the six factors accounted for of the total common variance after examining the relative loadings of variables on the factors post hoc we interpreted and named the six impact factors as impact signal reflected through citations page views and shares facebook hosted discussion plos hosted comments social reference manager saves and pdf downloads the factors must be interpreted in the context of the other factors in the order of factor extraction common citation variability was identified first and is represented by the citation factor from the remaining variability a commonality was detected between html page views twitter mentions delicious bookmarks etc this was extracted as the second factor which we have dubbed page views and shares the remaining factors are interpreted similarly for example the sixth factor pdf downloads needs to be interpreted as the variability shared between pdf downloads html page views and web of science cites that was not captured by any of the earlier factors interestingly the indicator did not have shared variability with any of the derived factors citations pageviews and shares https arxiv org html 18 facebook hosted discussion plos hosted comments social ref saves pdf downloads web of science cites thru web of science cites thru scopus cites pubmed central cites crossref cites pdf downloads html pageviews mendeley saves citeulike saves plos comments plos comment responses delicious bookmarks blog mentions facebook comments facebook likes facebook shares facebook clicks rating wikipedia cites twitter mentions table loading of variables onto exploratory rotated factors factors are given in the order of extraction and should be interpreted in this context because the underlying dimensions of impact are likely intercorrelated we chose a factoring algorithm that allowed the derived factors to be correlated with one another correlation patterns can be seen in figure we found the citation factor was correlated with both the pdf download factor and the factor representing social reference manager saves indeed social reference manager saves were relatively well correlated with all of the derived factors the page views and shares signal was particularly correlated with facebook hosted discussions and plos hosted comments 2018 https arxiv org html https arxiv org html 18 figure correlation between derived factors based upon the loading matrix future work incorporating additional altmetrics streams would facilitate a more nuanced understanding of factor structure second order factors might also be interesting it appears there may be a higher order dimension around citing and pdf access another dimension around social engagement and a third bridging signal related to social reference management relation to citation although the validity of citation counts as a measure of research impact has not gone unchallenged citation has become a gold standard for impact in both theory and practice consequently it is important to examine altmetrics against this existing standard however high correlation with citations is not essential or even perhaps desirable if altmetrics do in fact track important forms of impact not reflected in the citation record we would expect low to moderate correlations figures and present correlations between all gathered indicators and web of science citation counts each using a different method to address noise from differing publication dates figure includes correlations of all research papers with indicator values that have been normalized by date and journal and transformed as described above figure reports correlations on a restricted sample only articles published in in this case the indicator values were left unnormalized and untransformed so spearman correlations were used because of high skew in the untransformed data both and focus show correlations for three journals plos one very high volume interdisciplinary plos pathogens medium volume specialist and plos biology low volume high impact the patterns in the two figures are quite similar with the largest difference being much higher correlations between web of science citations and citation counts from other indicators this may be because figure includes many older articles these have had enough time to accumulate a large proportion of their lifetime citation counts informing a more reliable signal since the two figures are similar this discussion will focus on results reported in figure except when noted otherwise all three journals sampled display moderately strong relationships between citation count and pdf html download count r of the altmetrics indicators mendeley and citeulike displayed the highest correlations to web of science counts for all three journals r interestingly for all three journals mendeley bookmark counts correlate more closely to web of science citations counts than expert ratings of what is more in the age restricted sample figure the correlations between mendeley and web of science citations rivaled or surpassed those of scopus pubmed and crossref citations for all three journals correlation patterns across the three journals differ in many ways as well plos one correlations are weaker almost across the board with negligible or zero values for altmetrics other than mendeley the strongest correlations are from plos biology with plos pathogens tending to fall in between this difference may be related to the number of articles a journal publishes per year plos biology is likely read in its entirety by many knowledgeable scholars who in turn probably decide to share cite or discuss only the best articles there may be a sort of ongoing curation from the same consistent group of experts likely to cite plos one on the other hand likely publishes too many articles across too many different fields for any expert curator to personally rank consequently altmetrics data is fragmented inconsistent and likely reflects impact on populations less likely to cite plos pathogens lies somewhere in between both in size and correlations between citations and altmetrics this hypothesis is but one possible explanation and deserves further investigation the important finding here is that the correlation between altmetrics and citations appears quite sensitive to the publishing journal 2018 https arxiv org html https arxiv org html 18 figure pearson correlations coefficients for normalized and transformed citation counts with other normalized transformed indicators on articles published all years by journal figure spearman correlations coefficients for unnormalized citation counts with other metrics on articles published in by journal can we cluster articles of different impact types using altmetrics factor analysis helps us understand which metrics are similar and how many different dimensions of impact signal are represented in our data we would also like to know how these signals are reflected in different types of papers for that we clustered the papers based on the patterns of attention they ve received across various metrics to ground these results most closely to our observed data we clustered based on selected variables rather than factor scores based on correlations factor analysis results and popularity we chose five metrics to be representative of impact metric dimensions web of science citation counts mendeley saves html page views score and a composite we call sharecombo we constructed the sharecombo variable to keep collinearity and number of clustering dimensions low it consists of the aggregation of events from four sources facebook shares delicious bookmarks blog mentions and mentions on twitter we used normalized metric values from papers published in plos one before to derive the clusters based on scree plots stability metrics and interpretability we directed the clustering algorithm to identify five cluster centers 2018 https arxiv org html https arxiv org html 18 figure cluster centers columns represent the centers of clusters rows show indicators a dark blue cell in a given cluster indicates that the cluster centers on a relatively high standardized value z score for the given indicator grey cells represent low values the derived clusters were quite stable across random starting locations the centers of the clusters are shown in figure the center of cluster a is a paper with a high number of citations from web of science and a low score whereas the center of cluster b is a paper with a relatively few citations that has received quite a few saves into mendeley clusters a and b varied somewhat across runs clusters c d and e were quite stable the clusters were relatively distinct and non overlapping in some projections as demonstrated by the existence of simple rules to accurately predict cluster membership as described at the end of this section the clusters of impact patterns could be considered the impact flavor of the research article by analyzing patterns in what people are reading bookmarking sharing discussing and citing online we might identify what kind what flavor of impact a research output is making in a way that citations alone cannot rival the goal is not to compare flavors one flavor is not objectively better than another however recognizing different types of contributions might help us appreciate scholarly products for the particular needs they meet we might consider papers in cluster d for example to be expert picks papers with high evaluations and relatively high citation counts cluster c could be dubbed popular hits given the high degree of social media attention they received papers in cluster a were viewed and cited often whereas those in cluster b are heavily included in reference managers but rarely cited cluster e included papers that didn t receive much attention in any of the variables in this analysis though it is worth remembering they may have made impact in ways not captured in this analysis we do not claim this analysis reports canonical impact flavors more research is needed to identify stable reproducible validated clusters of research impact however this does illustrate what it might look like to begin describing research impact of papers and other scholarly product types with a full flavor palette cluster exemplars exemplar membership in each cluster is illustrated in table exemplars were chosen as papers with metric values closest to the center of each cluster there are few obvious emergent patterns save that cluster c titles unsurprisingly suggest articles of particular appeal to a general interest audience future work might more systematically examine articles for internal features like topic keywords reading level article length and so on that predict cluster membership cluster articles in cluster examples a read and cited phylogeny in aid of the present and novel microbial lineages diversity in bacillus contact networks in a wildlife livestock host community identifying high risk individuals in the transmission of bovine tb among badgers and cattle refuge or reservoir the potential impacts of the biofuel crop miscanthus x giganteus on a major pest of maize b read saved and shared vision and foraging in cormorants more like herons than hawks rich pickings near large communal roosts favor gang foraging by juvenile common ravens corvus corax wind waves and wing loading morphological specialization may limit range expansion of endangered albatrosses c popular hit cognitive processes associated with sequential tool use in new caledonian crows claims of potential expansion throughout the u s by invasive python species are contradicted by ecological niche models why men matter mating patterns drive evolution of human lifespan d expert pick an inhibitory sex pheromone tastes bitter for drosophila males substantial alterations of the cutaneous bacterial biota in psoriatic lesions community analysis of chronic wound bacteria using rrna gene based pyrosequencing impact of diabetes and antibiotics on chronic wound microbiota e not much attention using these metrics effects of endolithic parasitism on invasive and indigenous mussels in a variable physical environment sexual conflict and sexually antagonistic coevolution in an annual plant antibiotic treatment of the tick vector amblyomma americanum reduced reproductive fitness table exemplar members of each cluster from the field of ecology 2018 https arxiv org html identifying clusters with rules clusters would be most useful if they could be simply defined and operationalized through a simple set of rules this would allow researchers or evaluators to characterize the impact flavor of an article simply by examining its pattern of activity to illustrate this concept and explore the distinctness of the clusters we attempted to derive simple rules for accurately predicting the k means cluster membership the k means centers described above were derived on papers published before we now used these centers to assign cluster membership to the plos one papers in our dataset published during it was for these papers that we derived associative rules to predict the k means assigned cluster membership five simple rules listed below were sufficient to assign the identical cluster membership to euclidean distance algorithms with misclassification error kappa cluster d if rating and html pageviews else cluster c if html pageviews and tweets else cluster a if web of science citations or web of science citations and mendeley saves else cluster b if mendeley saves or mendeley saves and tweets else cluster e the confusion matrix from the rule classification evaluation given in table provides some insight into which of the clusters were easily mistaken for one another papers which had been classified as cluster b read saved and shared were classified relatively often by the simple rules as members of cluster a read and cited or cluster e not much attention whereas classification into cluster d expert pick was very consistent across k means and the simple rules cluster c cluster d cluster e cluster b cluster a cluster c 18 cluster d cluster e cluster b cluster a table confusion matrix cross classifications of k means clusters reported in rows vs rule based classifications reported in columns conclusion this descriptive study presents a great many findings but three are perhaps especially salient first there is no shortage of data from altmetrics sources although different indicators vary greatly in activity around of sampled articles have been included in at least one mendeley library and a quarter of articles have nonzero data from five or more different sources second altmetrics and citations track forms of impact that are distinct but related neither approach is able to describe the complete picture of scholarly use alone there are moderate correlations between mendeley and web of science citation comparable to that between web of science and scopus but many altmetric indicators seem mostly orthogonal to citation third articles cluster in ways that suggest several different impact flavors that capture impacts on different audiences and of different types for instance some articles cluster b may be heavily read and saved by scholars but seldom cited limitations these results are subject to several important limitations first data quality is a challenge since different services come and go over time this is a major source of noise in the impact signal and while our normalization approach is helpful in reducing this noise it is not perfect second our sample should be carefully considered before attempting to generalize results in particular plos publishes only open access journals exposing each article to a much broader audience and a much wider range of potential interactions than a closed access publisher would the corpus of articles is also dominated particularly in later years by the huge volume published by plos one this unique publication has a volume and scope dwarfing that of traditional journals inescapably giving rise to similarly unique patterns of readership there are also limitations in our sampling of indicators although this sample is diverse it is not complete google scholar citations for instance are conspicuously absent and likely would have correlated more closely with many altmetric indicators than citations from web of science also given the growing scholarly use social media it is likely that many counts are low compared to what we would see if replicating the study today third counts for altmetric indicators may be relatively easy to game this is more a limitation for future applications since few of these counts are today important enough to fabricate however it is a limitation that will need to be addressed before altmetrics can be used for serious evaluation indeed experience suggests that the more attention is paid to these metrics the greater problem gaming will become this has certainly been the case with citations which are routinely gamed by academics to varying degrees of success experience at google and wikipedia however suggest that careful data mining is an effective technique for stopping gaming while community driven policing at social sites like digg and reddit have also been effective the social science research network ssrn a scholarly preprint repository has also reported success with these approaches finally it is important to emphasize that that while new metrics are useful as alternatives to citation based measure for some purposes they are by no means a replacement instead they should be deployed alongside one another complementing each other strengths future research and applications much work to expand this research will center around reducing noise that obscures the impact signal or more accurately isolating and identifying different types of impacts on different audiences most obviously this will involve investigating altmetrics in other contexts with more sources and especially sampling from different journals and publishers this also must involve observational interview content analytic and ethnographic studies these are crucial to understand what the events informing alternative metrics actually mean following the examples set by early investigators of citation these investigations of context should also expand refine and validate clusters or impact flavors perhaps by asking readers or expert judges to identify article features influencing different patterns of use metrics must move beyond simply reporting counts and take network properties into effect as google does with pagerank a tweet from a highly connected expert scholar should mean something different from one authored by a casual observer research should continue to examine whether early altmetrics counts can predict later citation https arxiv org html 18 2018 https arxiv org html investigation should also expand to examine altmetrics for scholarly products other than articles such as datasets software and blog posts since creators of these products may find recognition via traditional citation difficult altmetrics could be crucial in evaluating and promoting web aware scholarship however it must be emphasized that much additional research is needed before altmetrics can be relied upon in high stakes evaluation like tenure and promotion decisions tools to gather altmetrics data will be essential to fuel this additional research as well as to support low stakes experimentation with altmetrics as source of formative evaluation data it is essential that these tools supply open data and make their collection procedures completely transparent avoiding the much lamented opacity closedness and consequent irreproducibility of thomson impact factor several promising open source altmetrics tools have begun to appear including citedin readermeter sciencecard and total impact in the future tools like these may allow researchers to keep a live cv showing up to date indicators of their works impact next to each product they have produced funding organizations could keep timely portfolios of their grants impact across different populations and communities tenure promotion and hiring committees could assemble research teams whose members have different sets of impact specialties just as sport managers might assemble teams of players with different physical and mental skill sets altmetrics also have great potential for supporting personalized recommender systems citation based versions of these have been created before but suffer from the long lag between an idea insemination and citation altmetrics could solve this problem sufficiently advanced recommendation tools allow the technical act of publishing to be decoupled from relevance and quality filtering allowing every researchers to essentially consume their own private journal peer reviewed organically by their scholarly network while many of the data sources described in this study will no doubt disappear over time scholars presence on the social web will not invisible colleges are not a fad the social web is merely the latest in a series of technologies improving the reach and density of these communities which have always lay at the heart of scholarship in cronin argued it is clear that we will soon have access to a critical mass of web based digital objects and usage statistics with which to develop multi dimensional models of scholars communication behaviors seven years later our data suggest that he was correct if you would like to write for this or any other emerald publication then please use our emerald for authors service information about how to choose which publication to write for and submission guidelines are available for all please visit www emeraldinsight com authors for more information about emerald www emeraldinsight com emerald is a global publisher linking research and practice to the benefit of society the company manages a portfolio of more than journals and over books and book series volumes as well as providing an extensive range of online products and additional customer resources and services emerald is both counter and transfer compliant the organization is a partner of the committee on publication ethics cope and also works with portico and the lockss initiative for digital archive preservation related content and download information correct at time of download oir received march accepted august online information review vol no pp q emerald group publishing limited doi the current issue and full text archive of this journal is available at www emeraldinsight com htm framework for a global quality evaluation of a website a lvaro rocha university fernando pessoa porto portugal abstract purpose this paper aims to propose a high level structure for a global quality evaluation of a website this structure is based on the characteristics sub characteristics and attributes of three main dimensions content service and technical quality that will substantiate the development of broad website quality evaluation comparison and improvement methodologies according to particular sectors of activity and evaluator perspective design methodology approach based on the literature and the author experience a framework is proposed for a global quality evaluation of a website findings considering the results of some studies as well as the systematisation of the knowledge available in several bibliographies website quality can be grouped into three main dimensions content quality service quality and technical quality there has not yet been an evaluation methodology that focuses on these three main website quality dimensions in a broad and transversal sense originality value the paper presents an innovative high level structure for a global quality evaluation of a website based on three dimensions not previously considered together keywords website evaluation content quality service quality technical quality service quality assurance paper type research paper introduction with the increasing number of websites and considerable investment in them website quality evaluation has become an important activity naik and tripathy organisations invest time and money to develop and maintain their website quality these websites should establish an effective information and communication channel between organisations and their clients in some cases they are part of the offered product since they make useful services available to clients grigoroudis et al a website should clearly reflect the quality efforts made by the organisation because it establishes an important connection with clients modern websites show a significant range of aspects complexity of structure and diversity of offered services kappel et al as in all information systems website evaluation is an important development and operational factor that may lead to the improvement of their users satisfaction grigoroudis et al and to the optimisation of invested resources cheung and lee in this research we propose an innovative high level structure for a global quality evaluation of a website that structure can become a platform for the development of specific website quality evaluation comparison and improvement methodologies according to different sectors of activity we will thus introduce in the following sections the concept of quality propose three main dimensions for website quality and identify methods for the quality evaluation in each of these dimensions we will propose the structure for a global quality evaluation of a website and finally we will offer some conclusions software quality people look for quality in each object they create and software is no exception software is one of the strategic assets in the information society with the internet boom and the following exponential increase in contents and services made available through websites a quality revolution quickly spread throughout the whole world naik and tripathy aspects related to the quality of websites have therefore become relevant to many sectors of activity several contributions to the field of website quality and different schools of thought have primarily focused on the definition of quality its structure and how it can be measured e g jung et al mich et al in this paper we adopt the definition of quality published in the most recent international organisation for standardisation iso standard for software quality because it agrees with our purposes because of its breadth and completeness and because of the prestige of that organisation we therefore understand quality as the capability of a software product to satisfy stated and implied needs when used under specified conditions iso iec p dimensions of website quality content and services are the reasons for the existence of a website which is built by application of techniques and technologies thus considering the results of some studies conducted and or supervised by the author e g rocha and victor as well as the systematisation of the knowledge available in several bibliographies we can group website quality in three main dimensions figure content quality service quality and technical quality this is an innovative conception of website quality in the first dimension the main concern is content quality rather than its existence as this should be a technical quality concern in content quality attributes such as accuracy completeness relevance opportunity consistency coherence updates orthography and syntax may be evaluated in the second dimension the focus is the quality of services offered in websites in service quality attributes such as security reliability privacy performance efficiency accuracy opportunity availability response time time saving empathy reputation and personalisation may be evaluated figure main dimensions for website quality quality evaluation of a website oir finally the third dimension focuses on the technical quality of websites i e on quality attributes that are usually found in quality standards for software such as iso iec iso iec and its successor iso iec iso thus attributes such as navigation map path search engine download time of pages browser compatibility broken links and accessibility may be evaluated by searching the literature and through our experience we have observed that there is no evaluation methodology that focuses on these three main website quality dimensions in a broad integrated and transversal sense content quality evaluation content quality evaluation primarily employs methodologies based on the likert scale which evaluates the quality of these contents amongst respondents users linguists and experts in the content presented in websites some studies related to this dimension are worth mentioning such as bernstam et al caro et al hargrave et al moraga et al parker et al and richard et al a possible structure for a content quality measuring instrument could have a similar format to the one presented by moraga et al table i already aligned with the iso standard iso iec which is the most relevant contribution that we found in the literature for this dimension to classify each attribute the analysers should carefully read and analyse either the content of every webpage in the website or every webpage content until a certain predefined level of depth is reached analysers can classify webpage content quality in a five point likert scale bad mediocre reasonable good very good service quality evaluation online service quality evaluation which includes for instance in the health area healthcare scheduling prescription renewal or drug acquisition usually employs evaluation methodologies for back office procedures and or users satisfaction with services available on websites some studies related to this dimension are worth mentioning such as al momani and noor arshad et al cernea et al hamadi li and suomi parasuraman et al and zhao a possible structure for a service quality measurement instrument could have the format presented in table ii based on the service quality scale developed by parasuraman et al which is the most relevant contribution that we found in the literature for this dimension the variables second column are grouped by characteristics first column analysers can classify websites services quality in a five point likert scale completely disagree disagree do not agree or disagree agree completely agree technical quality evaluation the technical dimension is related to how the content and services are assembled and made available on a website technical quality evaluation is based on software quality models or standards and on methods focused on usability methods developed through studies in the area of point of view category characteristic subcharacteristic inherent intrinsic accuracy credibility objectivity reputation traceability currency expiration completeness consistency accessibility compliance confidentiality efficiency precision understandability system dependent operational availability accessibility interactive ease of operation customer support verifiability confidentiality portability recoverability contextual validity reliability scope value added applicability flexibility novelty relevancy novelty timeliness specialisation usefulness efficiency effectiveness traceability compliance precision representational concise representation consistent representation understandability interpretability amount of data documentation organisation attractiveness readability source moraga et al human computer interaction the first group of methodologies include amongst others iso iec iso iec and iso iec iso standards the second group includes an approach that emerged with the hypermedia nature of the internet and the relevance of interface conception to speed access to information and quality evaluation of a website table i structure for content quality measurement oir table ii service quality measurement structure characteristic attribute efficiency this site makes it easy to find what i need it makes it easy to get anywhere on the site it enables me to complete a transaction quickly information at this site is well organised it loads its pages fast this site is simple to use this site enables me to get on to it quickly this site is well organised fulfilment it delivers orders when promised this site makes items available for delivery within a suitable time frame it quickly delivers what i order it sends out the items ordered it has in stock the items the company claims to have it is truthful about its offerings it makes accurate promises about delivery of products system availability this site is always available for business this site launches and runs right away this site does not crash pages at this site do not freeze after i enter my order information privacy it protects information about my web shopping behaviour it does not share my personal information with other sites this site protects information about my credit card responsiveness it provides me with convenient options for returning items this site handles product returns well this site offers a meaningful guarantee it tells me what to do if my transaction is not processed it takes care of problems promptly compensation this site compensates me for problems it creates it compensates me when what i ordered doesn t arrive on time it picks up items i want to return from my home or business contact this site provides a telephone number to reach the company this site has customer service representatives available online it offers the ability to speak to a live person if there is a problem source parasuraman et al globally improve human computer interaction and includes standards such as iso iec iso iec this approach defines quality according to usability considering the users point of view e g jung et al obeso technical quality has received more attention from researchers than the other two dimensions and several methodologies have been proposed for its evaluation amongst those we consider the work developed by olsina and other works that followed its lead e g machado and rocha reis the most relevant since they base their methodologies on iso iec iso iec and its high level quality characteristics that interest website users usability functionality reliability and efficiency technical quality measurement can also be classified using a three or five point likert scale structure for a global quality evaluation of a website bearing in mind the three main dimensions for website quality defined innovatively in this paper resulting from literature synthesis and based on our experience as website users and website engineering researchers we now propose a high level structure to evaluate the global quality of a website in a broad transversal and detailed way this structure is organised according to the three main website quality dimensions comprising characteristics which in their turn consist of attributes as shown in figure characteristics can sometimes consist of more than one level of sub characteristics website quality evaluation comparison and improvement methodologies developed through the proposed structure should be designed to incorporate adjustments to the activity sector in which they are applied since the suitable structure for quality sub characteristics and attributes generally differs among activity sectors and to incorporate adjustments to the type of evaluators users experts or owners since the suitable quality characteristics generally differ among them for example evaluation methodologies from a user perspective should not consider the characteristics of portability and maintainability in the technical dimension simultaneously the methodologies must be configured without overlap between the characteristics sub characteristics and attributes of the three dimensions conclusions and future work in this paper we proposed an innovative high level structure for global quality evaluation of a website we can highlight a few aspects as a conclusion website quality is strategically important for organisations and for the satisfaction of their clients website quality should be based on the quality measurement of three main dimensions content service and technical a structure based in these three dimensions characteristics sub characteristics and attributes will substantiate a broad integrated transversal and detailed quality evaluation of a website a good evaluation comparison and improvement methodology for website quality should comprise the three mentioned quality dimensions and allow adjustment to a specific activity sector and to a type of evaluator the next step in our study will be the development of an evaluation comparison and improvement methodology for the quality of institutional and hospital websites based on the high level structure proposed in this paper and will be built and validated with help of web engineering experts and hospital website users the need for this methodology is justified by the fact that we do not know of any that provide a broad and detailed assessment integrating the three main quality dimensions of a website content service and technical this study aims to propose a set of metrics in order to assess reactivity dialogic communication and stakeholder engagement popularity commitment and virality stakeholders mood and social legitimacy on corporate facebook pages these metrics can offer a better understanding and measurability of this social media social network online communication management tool design methodology approach three theories dialogic stakeholders and legitimacy were considered in the development of these metrics empirical evidence was collected from a sample of european companies then ten active companies were used to validate the proposed metrics on facebook findings the constructed set of metrics was found to be valid and efficiently usable according to the principles of the applied theories moreover all the proposed metrics could be adapted for such sites as google þ research limitations implications limitations can only be identified within the validation process as the metrics were only applied to ten representative companies from the eurozone practical implications the proposed metrics will help users marketing pr communication professionals and company managers to measure their and their competitors popularity commitment virality metrics which reflect stakeholder engagement and the mood of stakeholders and use content analysis in order to measure social legitimacy via csr information disclosure on facebook thus the online reputation of a company can be practically measured originality value this paper is the first proposing metrics to assess stakeholder engagement and social legitimacy on a corporate facebook page that can be used in both academic and professional circles to a gain a better understanding of corporate online communication via facebook keywords corporate facebook and google þ metrics stakeholder engagement corporate dialogue online reputation voluntary disclosure social legitimacy paper type research paper introduction recent changes in the business environment have prompted scholars to pay particular attention to social networking sites sns as a new medium of business communication in the growing body of literature on sns several papers have focused on facebook in particular mayer and puller lewis et al hughes et al facebook and most sns offer a strong platform for creating empirical this study has been carried out with the financial support of the spanish national r d plan through research project econ feder a set of metrics received march first revision accepted august online information review vol no pp q emerald group publishing limited doi oir oir studies because of their public features although their measurement is not easy the advent of the internet not only reorganised the way in which companies collect information but also redefined stakeholders expectations a study of governments showed that web applications are creating new features of innovation and improved transparency meijer and thaens and in some sense their stakeholders are recognised as partners and co creators not just consumers chua et al companies use social media for communication purposes with stakeholders the field of marketing and pr is generally well studied within the topic of communication for exmaple how firms support their brands michaelidou et al e marketer breslauer and smith the goal is to identify facebook metrics to help the measurement of this online sns channel for professionals focusing on stakeholders corporate social responsibility and social legitimacy this study provides insights into some specifically corporate facebook practices and offers new measurement metrics of popularity commitment and virality stakeholders mood and content analysis thus it also examines the online reputation of a firm the aim was to develop a new dimension of measurements from which new findings and implications can be drawn they not only can be used by practitioners but can also help scholars identify how to use facebook as a tool for online reputation management for these reasons the paper seeks to address the following research questions based on three theories as explained in the corporate facebook metrics section how can reactivity and dialogic communication be measured on corporate facebook how can stakeholder engagement be measured how can stakeholders mood be measured on corporate facebook how can social legitimacy be measured on corporate facebook previous research social networking is part of web which means the user is no longer just a consumer of the content but also actively participates in creating and shaping it this new stage of the internet opened the way to communicate collaborate and share content easily and fast online basically with anyone some relevant studies deal with web chua et al and the internet calero et al hariri in general or focus on a particular issue such as the measurement of urls for different purposes such as advertisements saikat and bin cheng and information propagation zheng et al esrock and leichty granovetter pioneered the direction of social network research as a scholarly discipline the growth of social network analysis as an academic field has coincided with the popularity of social network sites web based services such as facebook that allow individualstohavetheirownprofile havealistofotherusers andviewandtraversetheir list of connections boyd and ellison researchers have long recognised the potential of online communication technologies for improving network research rogers watts works related to social network analysis offer models measured by social graphs sala et al or large scale data clustering becker et al some others report on the usage of sns virtual communities or social networking in general hsiao royo vela and casamassima or focus on a special channel or sector the channels include blogs as a relationship building tool seltzer and mitrook a facebook channel ellison et al hughes et al junco lever lewis et al waters et al myspace thelwall or twitter account fischer and reuber hughes et al lovejoy et al etc the sector can be for example the tourist industry illum et al isacsson and gretzel litvin et al xiang et al ye et al government bonso n et al rodriguez domınguez et al towner and dulio the financial sector bonso n and flores or e banking services sa nchez franco and martın velicia beyond those the fortune esrock and leichty pettigrew and reber and eurostoxx companies bonso n and ratkai have also been analysed problem definition none of the cited works dealt with corporate facebook metrics although there has been ongoing debate about how facebook can be successfully integrated into companies communication strategies this is why we decided to conduct this research in order to provide a set of metrics both for further research and professional purposes the suggested metrics make possible the measurement of stakeholder engagement popularity commitment virality and the mood of stakeholders while content analysis is used to determine which elements influence the online reputation of a firm all the suggested metrics are public making it easy to assess competitors this leads to the conclusion that both academics and professionals managers can also benefit from this research the theoretical foundations for this study are drawn from communication management and the social sciences in general because the online presence of a company provides stakeholders with access to essential corporate information and thanks to the new technologies provides the possibility of symmetrical communication dialogic symmetrical communication pearson suggested that ethical communication management should be more dialogical than monologue both include an element of reactivity and the desire to provide immediate information pettigrew and reber in particular journalists as stakeholders expect this functionality from web platforms pettigrew and reber journalists iterate that a poor web presence can reduce the press coverage of a company nielsen so companies need to keep up with developing technology pettigrew and reber and integrate this knowledge and these tools into their communication strategy roper argued that symmetrical communication can lead to greater power for the organisation than asymmetrical communication according to habermas a dialogue cannot be dominated by one side and it should be cooperative and communicative as pettigrew and reber stated one of the features of a dialogic act is its power in this work the foucauldian approach is used which can be described as power which exists in a network of relationships rather than the kind of power that influences from the top down so it can be understood as a rejection of common power concepts according to foucault the power is not monopolised which could only have a negative aspect because the power can exclude dialogues but on the other hand it can also create them this mechanism raises the possibility of resistance the a set of metrics oir question naturally arises of how to create conditions for dialogue we suggest that corporate facebook can be one solution dialogic theory suggests that it is not the outcome that is important within the communication but the process itself kent and taylor heath et al kelleher pettigrew and reber it is more about open and negotiated discussion than agreement kent and taylor technology itself can neither create nor destroy relationships according to kent and taylor ovaitt and mitra also argued that the internet in some cases can be the only way to reach some traditionally isolated groups and entire communities paine raised the prospect of monitoring internet coverage in the same way that press coverage can be monitored pestana and daniels described the changes over time the first step was the measurement of outputs in traditional media one to many the second decade was the measurement of outputs and external data in traditional and digital media one to many and the last decade from till the present is the measurement of both outputs and outcomes in traditional digital and social media many to many this question of measurement raises the first research question how can reactivity and dialogic communication be measured on corporate facebook stakeholders from a stakeholder viewpoint companies should try to achieve different goals according to the stakeholder groups according to guthrie et al companies are expected to report to their stakeholders on their activities and this kind of organisational accountability is more than a simple economic or financial act the term accountability was defined by mulgan as the responsibility of performance from one party to another an et al suggest that accounting information disclosure can be considered accountability various authors deegan and samkin gray et al have also highlighted that for example the loyalty of stakeholders can be beneficial for the entrepreneur and this can be based on a good relationship this study measures how companies communicate with their stakeholders as if they are a single group and the mood of this communication takagi et al and bollen et al argued that the stakeholders mood is very important from the companies viewpoint these ideas raise the second and the third research questions how can stakeholder engagement be measured and how can stakeholders mood be measured on corporate facebook social legitimacy and voluntary disclosure legitimacy theory suggests that a social contract exists between the company and society deegan deegan and samkin society is a wider category than the stakeholders because it contains entities who are not stakeholders of the company so this means that a business should conduct its activities in a manner which is socially acceptable not only following the expectations for example of the investors this also requires continuous change from the company side to survive from this viewpoint organisations should voluntarily report both financial and non financial information according to the expectations of society pfeffer and salancik the banking sector was one of the first where the regulators required online transparency bonso n et al bonso n and flores this kind of financial and non financial disclosure can also be called corporate social responsibility csr several authors suchman garriga and mele claasen and roloff have linked csr and legitimacy according to claasen and roloff one of the ways to attain legitimacy can be by engaging in highly visible philanthropic activities and communicating them to a wide audience this raises the fourth research question how can social legitimacy be measured on corporate facebook corporate facebook metrics this section proposes a set of metrics answering the research questions based on the theories applied reactivity dialogues and stakeholder engagement is how can reactivity and dialogic communication be measured on corporate facebook or in other words how can the stakeholders engagement be measured these two questions cover different aspects of the same problem based on different theories but the same set of metrics can be used to provide an answer according to the amount of public quantitative information which is offered by facebook popularity commitment and virality can be measured as explained in table i to offer a better view of reactivity dialogues and stakeholder engagement popularity is measured by the likes on facebook commitment refers to the number of comments and virality is measured by the shares on facebook according to the taxonomy selected for the research stakeholders mood is how can the stakeholders mood be measured on corporate facebook in order to answer this research question the comments can be classified into three categories indicating the mood of stakeholders positive negative and neutral table ii within the framework of a qualitative examination a growing body of literature shows the importance of the mood of stakeholders takagi et al bollen et al social legitimacy and voluntary disclosure is how can social legitimacy be measured on corporate facebook for the purpose of answering this research question again a qualitative analysis of content name sign formula measures popularity number of posts with likes total posts percentage of the total posts that have been liked total likes total number of posts average number of likes per post number of fansþ popularity of messages among fans commitment number of posts with comments total posts percentage of the total posts that have been commented on total comments total posts average number of comments per post number of fansþ commitment of fans virality number of posts with shares total posts percentage of the total posts that have been shared total shares total posts average number of shares per post number of fansþ virality of messages among fans a set of metrics table i measurement of reactivity dialogic communication and stakeholder engagement oir can be conducted among the posts categorising them into seven groups table iii content analysis is one of the most widely used techniques ettredge et al bonso n et al gallego alvarez et al in studies that examine the information provided by businesses or institutions this kind of analysis typically leads to a disclosure index numerical indicator which shows the quantity of information with the level of disclosure on the communication channel analysed from the viewpoint of voluntary disclosure and legitimacy theory the main focus must be on corporate social responsibility information disclosure the inter american development bank provides a definition csr is a business approach that views respect for ethics people communities and the environment as an integral strategy that increases value added and thus improves the competitive position of a firm csr information can be divided into four categories environmental social financial and governance an analysis of content is needed and the following categories are proposed beyond csr for the purpose of gaining a complex view marketing customer support customer services and others methodology and metrics validation three different stages were defined during the process the first stage was to identify patterns by empirical experimentation observing eurostoxx companies over the years the chosen set of companies covers supersectors and countries this led to the creation of the proposed metrics they were designed after reviewing more than a name formula measures table ii stakeholders mood by comments with positive negative the ratio between the positive measurement of comments and neutral mood negative and neutral moods stakeholders mood expressed in comments sign measures number of wall posts about these topics environmental issues social human resources career issues financial reporting financial transparency issues table iii governance measurement of social marketing marketing selling products legitimacy and voluntary cs customer support customer service disclosure other not covered by the above categories hundred active accounts on facebook paying special attention to the available public information the goal was the creation of metrics that can be effective and available to everybody this means that the basic variables are all public so there is no need to be an administrator of the facebook page examined this feature makes the proposed metrics advantageous both to researchers and professionals at the second stage all the data was collected and examined for the purpose of validation during the validation process companies pirelli c spa from italy wartsila renault adp and biomerieux from france basf lufthansa bmw and henkel from germany and indra sistemas from spain chosen from a study by bonso n and ratkai covering the most active and engaged firms were analysed the purpose was validation and familiarity with the values average minimum maximum etc of these metrics the last posts from the company were considered for analysis the companies on average needed days to post times on their wall which shows the activity level of the channel channel activity bonso n and flores bonso n et al and the number of fans ragland bonso n et al can be considered very basic variables of corporate facebook as mulvihill stated fan growth is an important thing but the real aim is to get more impressions within the facebook news feed which led us to the creation of the proposed metrics finally at the third stage the necessary modifications were conducted to improve the metrics reactivity dialogues and stakeholder engagement the feedback from stakeholders also has value according to gorry and westbrook some companies have already integrated feedback into their organisational processes examples include ritz carlton fiskars vaseline levi strauss harley davidson starbucks and charles schwab gorry and westbrook found that as a business grows in size and complexity its knowledge of its customers decreases three metrics were used to measure the quantity of feedback from stakeholders popularity commitment and virality the term virality was created to show the effectiveness of viral messages on facebook fortunately there is already a tool to measure virality which is the share button it shows how many times a wall post was shared with someone for the ten companies analysed per cent of the wall posts were liked per cent were commented upon and per cent were shared the average number of likes per post was more than the average number of comments per post was while the average number of shares per post was only table iv it can be seen that the set of metrics of reactivity dialogues and stakeholder engagement are valid however in the cases was used in order to offer the possibility of of p a better c comparison and v multiplication by as the original results were close to zero a set of metrics table v validation of content analysis oir table iv validation of the metrics of reactivity dialogues and stakeholder engagement stakeholders mood the feedback from stakeholders can be analysed not only quantitatively but qualitatively as well negative conversations also understood as negative feedback are the main reasons why so many companies fear getting involved in social media ralphs after deep analysis of ten firms the results do not confirm the fear of negative feedback on average only per cent of the comments were negative johnson proposes that individuals can have a negative impact on reputation however the fear of negative feedback is not justified on average per cent of the comments were mainly positive per cent positive and per cent neutral social legitimacy and voluntary disclosure the content analysis table v shows that on average per cent of the wall posts were related to corporate social responsibility this result is lower than their use for marketing purposes but an encouraging number of csr posts can be expected to help gain social legitimacy the disclosure index of the marketing selling type of content was per cent which shows the highest ratio remarkably the smallest ratio of the wall posts on average was for customer service per cent on average per cent of posts fell into the category of other as on average per cent of the content can be classified by the proposed categories and only per cent falls into the other category this classification can therefore be accepted as valid discussion and implications day suggests that web presence is more important than service access or content as stuart recognised sns are tools with high potential although their impact and value are not easy to measure this paper by means of the proposition of metrics offers some practical implications for both practitioners and researchers the benefits of this study for practitioners are the availability of monitoring competitors practices assessing the performance of community managers p p p c c c v v v average 646 minimum median maximum 70 80 18 csr csr4 marketing cs others average minimum 00 00 median 00 00 maximum 00 assessing stakeholders participation and their opinions assessing marketing strategies on facebook and other sns where the metrics are adaptable assessing communication strategies and assessing online reputation of the company or products and services for researchers these metrics can contribute to theory validations and interpretations first for dialogic theory habermas heath et al kent and taylor pearson pestana and daniels pettigrew and reber roper measuring popularity commitment and virality is useful to examine the reactivity and dialogues as the proposed metrics can be considered to indicate levels of engagement of the dialogue second for stakeholders theory deegan and samkin gray et al guthrie et al in addition to engagement level metrics the mood of stakeholders is also an interesting metric because the theory states that a good relationship is essential in order to achieve different corporate goals the engagement level was also measured by popularity commitment and virality as for dialogues third from a legitimacy theory perspective claasen and roloff deegan deegan and samkin voluntary information disclosure an et al mulgan pfeffer and salancik can help a company to gain more transparency bonso n et al bonso n and flores theunissen and noordin and this way more trust both from society and stakeholders the suggested metrics can also be applied in an evolutionary historical analysis for example trying to confirm whether this new field of research focusing on different markets sectors sns channels etc behaves according to the expectations of for example the follower effect lyabert furthermore these kinds of analysis can lead the researchers to predict some general corporate communication trends of the future and compare the behaviour of different markets based on experimental studies using the suggested metrics actual comparisons with other studies in most cases are difficult to conduct because of the previous lack of measurement tools however in some special cases similar research was found these are compared below grouped according to the implicated theories reactivity dialogues and stakeholder engagement heath et al suggest that all dialogue has common elements including as many stakeholders as possible engaging them as human beings and not as representatives of stakeholders focusing on listening and answering and constructing an environment platform that allows stakeholders to speak sincerely these kinds of dialogues are considered by various authors to represent a risk or danger saunders leitch and neilson heath et al mersham et al stated that losing control is not something that managers are willing to do and yet there is a requirement to establish a dialogue according to theunissen and noordin transparency and trust are essential for effective dialogue and a set of metrics oir organisations should understand these features of dialogical communication the likes comments and shares on facebook can be considered dialogue as seen earlier the number of likes is higher than the number of comments and the number of shares this result supports the findings of another study by bonso n and ratkai the reason may be that it is faster and easier to press the like button than to comment on something it is obvious that activating the audience is a difficult part of corporate dialogue this background highlights the c popularity and v metrics commitment which measured the and virality reason for the close to zero results for the p active population of the audience regarding forcing us to change the formula with a multiplication by stakeholders mood a study by cone shows that per cent of those interviewed agreed that the role of companies in sns is to collect feedback it was found that the stakeholders mainly just express positive or neutral feelings towards the company negative comments were scarcely found the research of shu and chuang also found that people who use these sites express positive attitudes about them which our results can confirm in our sample the most criticised company was indra sistemas with per cent negative comments although the majority of these comments came from one person possibly an ex employee of course clever companies can learn from negative feedback as gap did in they wanted to launch a new logo for the brand but after the discussions on their corporate facebook page they decided to bring back the old logo ralphs they posted that they had heard loud and clear that the customers did not like the new logo so they brought back the blue box immediately social legitimacy and voluntary disclosure the topic of communication was analysed in order to identify whether it could be considered as financial and non financial voluntary disclosure for this reason besides the usual content analysis marketing purposes etc corporate social responsibility was also addressed as a topic according to the iso standard csr is the responsibility of an organisation for the impact of its decisions and activities on society and the environment through transparent and ethical behaviour european commission so csr was considered as voluntary disclosure by a company in order to gain social legitimacy the disclosure index of csr is per cent which means that per cent of posts can be considered somehow related to corporate social responsibility the ratio is good especially if the comparison is made with customer support another study bonso n and ratkai also reported on the usage of csr of around per cent which was also confirmed with our results in the case of customer support a higher disclosure index was expected cone found that per cent of those interviewed indicated virtual customer service as the role of companies within sns in our opinion telecommunications companies use sns for customer support more frequently simply because their products and services are easily adaptable to offer a virtual service telefo nica for example reported that sns have provided considerable opportunities to get closer to stakeholders and listen to their needs and expectations a remarkably high ratio of content fell into the category of other in some cases these posts were about events on the facebook page or a poll not related to any of the mentioned contents or were just simply outside the rest of the categories so as a result in the future a better classification can be developed conclusion we began this work as a search for practical solutions after noticing the lack of online corporate communication measurement and the overlap between professional and academic fields during the process it became clear to us that we have to design the metrics in a way that makes them suitable for everyone who is interested in online corporate communication and reputation our most important contribution is proposing the set of validated metrics based on theories which also can contribute to theory validation and interpretation in this research paper four research questions were addressed and answered see table vi based on theories the proposed metrics were able to measure the reactivity and dialogic communication with stakeholders and their engagement the mood of stakeholders and the messages from the company for the purpose of assessing social legitimacy by means of voluntary csr disclosure on corporate facebook all the proposed metrics could be adapted for sites such as google þ by changing the names of the functions where necessary for example in the case of google þ the equivalent of likes is þ but apart from that the other metric names are applicable the quantitative information shows that the posts are more liked per cent than commented on per cent or shared per cent future research is needed to see why this is but the reason may be the difficulty in encouraging the participation of stakeholders in other words stakeholder engagement although those stakeholders who gave some form of feedback by means of liking commenting or sharing to the company have made an effort in order to express their opinion it takes less energy and time to click on the like button than to put their thoughts into a logical order and write them down the qualitative information shows that fear of negative feedback does not seem to be borne out as per cent of the comments were mainly positive the content analysis shows that communication for marketing purposes per cent comprises the biggest part but csr information per cent also has a significant role so companies do seem to care about social legitimacy and according to what can be measured relational theory metrics reactivity and dialogic communication dialogic theory popularity commitment virality rq2 stakeholder engagement stakeholder theory popularity commitment virality rq3 stakeholders mood stakeholder theory mood of comments positive negative neutral voluntary disclosure legitimacy theory content analysis highlighting csr a set of metrics table vi review of research questions oir the so called follower effect lyabert a greater importance for csr information within communication on sns can be expected limitations as the goal of this paper was to suggest and validate new metrics of measurement on corporate facebook and the applied metrics were found to be valid and usable according to the principles of the applied theories limitations can only be found within the sample the ten representative companies were adequate for the validation process but results may vary from the possible results using a bigger or otherwise different sample future research future research could examine a broader number of companies through the use of the proposed metrics in future research consideration can also be given to identifying different stakeholder groups and analysing the complex relationships and communications with these individually identified groups effort should be made to include facebook events and facebook polls in the content analysis a media type analysis could also be undertaken good practice would be to analyse not only the comments from stakeholders but also the wall posts from them in most but not all of the cases the companies allow publication of the stakeholders posts on their pages moreover to get a better understanding of stakeholders mood this kind of practice could be used as a measure of reactivity dialogic communication and stakeholder engagement if there was also analysis of whether the company replied to the feedback or not subject areas are defined by elsevier in scopus on the base of journal classifications the delineations in terms of subject areas provided by scopus are continuously improved by scimago using among other things the categorizations of the web of science and medline lopez illescas de moya anegon moed lópez illescas noyons visser de moya anegón moed however journal classifications remain a reduction of the complexity of inter journal and inter discipline relations which cannot be expected to match one by one boyack klavans rafols leydesdorff the use of index terms e g mesh terms provided by discipline specific data bases e g medline us national library of medicine where papers are classified to subject areas on a paper by paper basis might be more appropriate than journal classifications bornmann mutz neuhaus daniel leydesdorff but these classifications have not been available for large scaled discipline overlapping analyses hitherto recently the third edition of the world report available at http www scimagoir com pdf pdf was published with a new excellence indicator added this indicator can be traced back to the methodological developments of bornmann and leydesdorff and leydesdorff bornmann mutz and opthof the excellence indicator provides the percentage of papers published by an institution belonging to the top papers in terms of numbers of citations normalized for the same field of publications and the same publication year tijssen visser and van leeuwen and tijssen and van leeuwen argued that the top of papers with the highest citation counts in a publication set can be considered as highly cited see also lewison thornicroft szmukler tansella for example an excellence indicator of for an institution means that of its papers belong to the top most highly cited papers among those published in the same year and subject area e g biochemistry genetics molecular biology immunology microbiology scimago uses an inclusive definition of the top when a set of documents has the same number of citations as the last document of the core these documents are all considered as part of the top set in some cases the top set is thus larger than but this is usually within the rounding of the first decimal the indicator can be considered as an item oriented field normalized citation score because each paper in an institutional publication set is analyzed whether it belongs to the top of papers in the set of papers covered by scopus with the same publication year and subject area however different from normalizations based on average values lundberg opthof leydesdorff waltman van eck van leeuwen visser van raan the top can be considered as a non parametric statistics this non parametric approach accounts for the prevailing skewedness of citation distributions albarrán crespo ortuño ruiz castillo seglen the excellence indicator has two advantages first the percentage for an institution the observed number can be compared with the reference value expected value of the expected number in the top for a set of papers selected at random would be agarwal searls bornmann mutz accordingly institutions in the world report with percentages above perform above expectation or in other words above the reference standard and institutions with percentages below perform below expectation the percentages of different institutions and their deviations from can be compared directly with one another since these ratios were already normalized for respective publication years and subject areas secondly the excellence indicator allows for testing whether the difference between the institution percentage and the expected value of or the percentage difference between two institutions are statistically significant the statistical significance test analyzes whether the difference e g between the observed and expected institution number of top papers which is reached on the base of a sample e g papers published between and is valid in all likelihood for all ever published papers of the institute in question covered by scopus bornmann et al if the test is statistically significant the difference does not seem to be a random event but can be interpreted beyond the analyzed sample data the appropriate test is the z test for two independent proportions sheskin pp 643 this test can be used for evaluating both the degree to which an observed number differs from the expected number and whether the observed numbers for two institutions differ respectively bornmann leydesdorff in general the test statistics can be formulated as follows z pp pp nn and where n n are the numbers of all papers published by institutions and under the column output in the world report and p are the values of the excellence indicators of institutions and furthermore and p where t p tt nn and t the numbers of top papers of institutions and these numbers can be calculated on the base of output and excellence indicator in the case of testing observed versus expected for the same set n is the observed value of the excellence indicator and p n p the expected value which is for stochastic reasons of n an absolute value of z larger than indicates statistical significance of the difference between the two proportions at the five percent level p the critical value for a test at the one percent level p is if a reader of the world report conducts a series of tests for many institutions a higher significance level than five percent may have to be chosen there is a possibility of family wise accumulation of type i errors leydesdorff et al for example at the position in the scimago institutions rankings university of california los angeles ucla has an output of papers with an excellence rate of stanford university follows at the position with papers and a excellence rate using the above formulas z the difference between these two institutions thus is not statistically significant a calculator is provided at http www leydesdorff net xls in which one can fill out this test for the comparison of any two institutions and also for each institution on whether it scores significantly above or below expectation assuming that of the papers are for stochastic reasons in the top set as the interpretations and calculations described in this letter to the editor show the simple percentage of top papers for an institution the new excellence indicator offers already a lot of possibilities for the comparison of an institution against an expectation or reference standard and with other institutions by using non parametric statistics for testing significance leydesdorff and bornmann further developed this statistics to the integrated impact indicator which allows for more refinement of the choices but this measure is perhaps less intuitively easy to understand than the top for a non specialist audience bornmann leydesdorff in press we analyze whether preferential attachment in scientific coauthorship networks is differ ent for authors with different forms of centrality using a complete database for the scientific specialty of research about steel structures we show that betweenness centrality of an existing node is a significantly better predictor of preferential attachment by new entrants than degree or closeness centrality during the growth of a network preferential attachment shifts from local degree centrality to betweenness centrality as a global measure an interpretation is that supervisors of phd projects and postdocs broker between new entrants and the already existing network and thus become focal to preferential attachment because of this mediation scholarly networks can be expected to develop differently from networks which are predicated on preferential attachment to nodes with high degree centrality elsevier ltd all rights reserved introduction collaboration is one of the defining features of modern science in recent decades milojevic persson glänzel danell wagner although the concept is perhaps difficult to define woolgar hara solomon kim and sonnenwald suggested that collaboration presumes at least two common elements working together for a common goal and sharing knowledge in our opinion collaboration can be considered as a social process bordons gómez milojevic shrum genuth chompalov as milojevic at p formulated the most commonly used methods for studying collaboration networks have been bibliometrics bordons gómez glänzel glänzel schubert social network analysis network science barabási et al kretschmer newman wagner wagner leydesdorff qualitative methods of observation and interviews hara et al shrum et al and surveys birnholtz lee bozeman in academia co authorship is the most visible and accessible indicator of scientific collaboration abbasi altmann hwang and has thus been frequently used to measure collaborative activity milojevic especially in biblio metric borgman furner and network analysis studies milojevic bibliometric studies of co authorship have emphasized the effects of collaboration on scientific productivity publications and citations as well as on organizational and institutional aspects of collaboration applied to different units of analysis authors institutions and countries milojevic on the other hand network studies have focused primarily on the mechanisms in the formation of collaboration a previous version of this paper abbasi hossain was presented at the international conference on knowledge based and intelligent information engineering systems kaiserslautern germany september corresponding author tel e mail addresses alireza abbasi sydney edu au a abbasi liaquat hossain sydney edu au l hossain loet leydesdorff net l leydesdorff see front matter elsevier ltd all rights reserved doi j joi journal homepage www elsevier com locate joi journal of informetrics contents lists available at sciverse sciencedirect journal of informetrics a abbasi et al journal of informetrics networks and understanding the underlying structures and processes leading to the observed structures milojevic for example see abbasi altmann hossain for example moody indicated that authors with many collaborators and high scientific prestige gain connections from authors that are newly entering the network more than their colleagues recently some studies used collaboration networks to study network dynamics barabási albert barabási et al newman in order to reveal the existence of specific network topologies and preferential attachment as a structuring mechanism milojevic barabási and albert originally proposed preferential attachment as a key mechanism in the development and evolution of networks new nodes attach preferentially to existing nodes that are already well connected in other words to nodes with a high degree centrality this suggests that the evolution and expansion of networks not only depend on the growth of network adding more nodes and links to the network but also follows a specific scale free pattern whereas many networks e g world wide web citation networks follow this model of competition it has remained a matter of debate whether the model applies to social networks newman wagner leydesdorff scientific collaboration networks are a complex kind of social networks since both the numbers of authors nodes and co authorship links among them are growing over time additionally the structure of the network the way the authors are connected and the positions of authors in the network may vary over time analysis of the attachment behavior of authors as nodes in terms of the nodes positional properties may help to explore the dynamics of structural change and evolutionary behavior in scientific collaboration networks in this paper we present a study of a collaboration co authorship network and investigate how authors behave during evolution expansion of this co authorship network we focus on a specific field of science namely research about steel research with which one of us is intimately familiar to investigate authors attachments to specific positions in the network the positions are indicated using centrality metrics from social network analysis studies the three main standard centrality measures degree closeness and betweenness reflect different positions and consequently roles of the actors in a network in other words we hypothesize that authors attach differently to authors who are already well connected high degree centrality close to all others high closeness centrality or well bridging brokering between authors high betweenness centrality can the general notion of preferential attachment thus be refined we envisage extending this model in search of the favorable positions of researchers in their collaboration networks which give them capacity to attract more co authors in a next stage during evolution of a collaboration network attachments new links can happen between new authors that is authors added in a next period and already existing authors among new authors among existing authors who were not connected previously and among existing authors who already had at least one previous collaboration our objective is to find these behavioral attachment patterns particularly identifying which characteristics of existing authors attract new authors or cause new authors to attach to them in particular we investigate the following research questions how do authors behave during the evolution of their collaboration network do positions roles of existing authors in a coauthorship network associate with the number of new authors collaborating with them at a next moment of time what types of positions are most attractive for preferential attachment after reviewing the literature on social network analysis and preferential attachment in section we describe data sources and our collection methods in addition to the measures that will be used in section section provides the results of our analysis and finally the paper ends with conclusions and a discussion of implications of this study social network analysis and preferential attachment social network analysis a social network is a set of individuals or groups each of which has connections of some kind to some or all of the others in the language of social network that is graph analysis the people or groups are called vertices actors or nodes and the connections are edges ties or links both actors and ties can be defined in different ways depending on the research questions of interest an actor can be a single person a team or a company a tie could be a friendship between two people collaboration or common member between two teams or a business relationship between companies social network analysis has produced many results concerning social influence social groupings inequality disease propagation communication of information and indeed almost every topic that has interested century sociology newman social network analysis enables us to study the networks and their participants nodes and relations among them social network analysts argue that networks operate on many levels from friends up to the level of nations the networks play a critical role in determining the way problems are solved organizations are run markets evolve and the extent to which individuals succeed in achieving their goals social networks have been analyzed to identify areas of strengths and weaknesses within and among research organizations businesses and nations as well as to direct scientific development and funding policies owen smith riccaboni pammolli powell sonnenwald fig an example of co authorship network of an academic community adapted from abbasi et al in a scientific collaboration network nodes are authors and ties links are co authorship relations among them a tie exists between each two authors scholars if they have at least one co authored publication in general scientific collaboration co authorship networks can be represented as a graph fig shows an example the nodes actors and vertices of the graph represent authors and the links ties and edges between each two nodes indicate a co authorship relationship between them the weights of links denote the number of publications that two authors co authors have jointly published recently the analysis of networks and particularly the dynamics in the evolution of large networks has become of greater interest to more authors given the increasing evidence that networks obey unexpected scaling laws albert jeong barabási barabási albert that can be interpreted as signatures of deviation from randomness jeong néda barabási there have been efforts resulting in a class of models that view networks as evolving dynamical systems rather than as static these approaches look for universalities in the dynamics governing network evolution jeong et al most models of evolving network are based on two ingredients barabási albert growth and preferential attachment the growth hypothesis suggests that networks tend to expand by the addition of both new nodes and links between the nodes while the hypothesis of preferential attachment states that new nodes attach preferentially to existing old nodes that are already well connected in other words a new node is connected to some old nodes in the network based on its number of links that is degree centrality these models indicate that as barabási and albert p formulated the development of large networks is governed by robust self organizing phenomena that go beyond the particulars of the individual systems preferential attachment the preferential attachment process is based on the principle that the rich get richer or more generally cumulative advantage this mechanism was originally proposed by yule and is therefore known as leading to the yule distribution and is also known as the matthew effect which was originally formulated by merton the mechanism was elaborated by price who used the terminology of cumulative advantage all these processes with different names are based on a general mechanism through which a relatively favorable position can be considered as a resource to generate further gains diprete eirich the terminology of preferential attachment itself was originally used by barabási and albert who made the concept basic to the emerging network science according to newman the application of preferential attachment to processes in the new network science helps to model a quantitative mechanism or mechanisms by which a network forms usually in an effort to explain how the observed structure of the network arises the specification of a dynamic mechanism makes it into one of the most important classes among the network models the focus is thus on modeling the network generation and its evolution rather than modeling the network topology kronegger mali ferligoj doreian data and measures data sample for our analysis we used a portion of a large longitudinal dataset which has been used to study the evolutionary dynamics of scientific collaboration networks of a research field indicated here as steel structures abbasi hossain uddin rasmussen to construct the dataset we extracted publications using the string steel structure in the titles keywords or abstracts in the top specified journals of the field shortlisted by one of the authors as an expert of the field and restricting the search to publications in english a abbasi et al journal of informetrics a abbasi et al journal of informetrics after extracting metadata of these publications from one of the main sources of bibliometric data we imported the information into a relational database upon comparison with our originally data we found the affiliation information to be messy with several fields missing for some of publication and with different spellings of names of institutions cities and countries in the address information therefore in a second step we carefully undertook manual checks using google to fill out the missing fields additionally we merged the universities and departments that had different names e g misspellings or using abbreviations in the originally extracted records the database was thus made complete so that the author names are disambiguated for this study we use only the publications published between and after cleaning the publication data the resulting database contained publications reflecting the contributions of authors from institutes in countries measures a common method used to understand networks and their nodes in a static design is to evaluate the location of nodes in the network in terms of strategic positions node centrality concepts and measures help determine the importance of a node in a network bavelas was the pioneer in this field who initially investigated formal properties of centrality as a relation between structural centrality and influence in group process he proposed several centrality concepts later freeman argued that centrality is an important structural factor influencing leadership satisfaction and efficiency to quantify the importance of an actor in a social network various centrality measures have been proposed over the years scott degree centrality the simplest and easiest way of measuring a node centrality is by counting the number of other nodes connected directly to this node this degree of a node can be regarded as a measure of local centrality scott it is worth to note that a central node is not necessarily at the center of the network physically the degree centrality of node k i e p k is defined as follows c d n p k a p i p k i where n is the number of nodes in the network and a p i p k if and only if node i and k i e p i and p k are connected a p i p k otherwise the concept of node centrality originated in the sociometric literature of the star scott which is a central node with many direct connections to other nodes the simplest and easiest way of measuring node centrality is accordingly by the degree of the different nodes in the network a node in a position with high degree centrality can influence the group by withholding or distorting information in transmissions bavelas freeman thus degree centrality reflects the node position and role in terms of popularity and activity of the node freeman through knowing more people furthermore nodes with high degree centrality could be identified as the informal leaders of the group krackhardt closeness centrality freeman proposed closeness as a measure of global centrality in terms of the distances among various nodes sabidussi originally had suggested this concept in his work as a sum distance that is the sum of the geodesic distances the shortest path between any particular pair of nodes in a network to all other nodes in the network by simply calculating the sum of distances of a node to others we will have farness how far the node is from other nodes thus one needs to use the inverse of the farness as a measure of closeness so a node can be considered as globally central if it lies at the shortest distance from many other nodes in other words it is close to many of the other nodes in the network in unconnected networks every node is at an infinite distance from at least one other node and the closeness centrality of all nodes is then to solve this problem freeman proposed another way for calculating closeness of a node as the sum of the reciprocal distances of that node to all other nodes so closeness centrality of node k i e p k is defined by freeman as follows c c p k n d p i p k i where d p i p k is the geodesic distance shortest paths linking p i and p k a node with the nearest position on average to all others can most efficiently obtain information and disseminate information quickly through the network thus closeness centrality is a proxy for the independence and efficiency for communicating with other nodes in the network at www scopus com a abbasi et al journal of informetrics table authors and their co authorship links statistics over time new entries frequencies cumulative frequencies year of publications of authors of links avg links au year of publications of authors of links avg link au 117 283 265 509 53 818 359 1187 658 91 betweenness centrality another global measure of centrality is betweenness which was also proposed by freeman one considers the number of times a particular node lies between the various other nodes in the network betweenness centrality of a node is defined as the portion of the number of shortest paths between all pairs of nodes that pass through the given node divided by the number of shortest path between any pair of nodes regardless of passing through the given node borgatti more precisely the betweenness of node k i e p k is formulated as follows c b n p k i j g ij p g ij k i j k where g ij is the geodesic distance shortest paths linking p i and p j and g ij p k is the geodesic distance linking p i and p j that contains p k nodes with high betweenness centrality play the role of a broker or gatekeeper to connect the nodes and sub groups so they can most frequently control information flows in the network burt due to dependency of others on nodes with high betweenness centrality the latter is often considered as an indicator of the power and influence these actors have in a group or organization krackhardt analysis and results table indicates the growth of the coauthorship network in this set by showing the number of publications the number of authors the number of links among them and the average links per author during network evolution between and it also shows the cumulative number of publications authors and links for each year the number of links reflects the sum of the frequency of collaborations among each pair of co authors the results indicate that the growth of the number of new links is higher than the growth of the number of new authors during the period the number of authors has been almost doubled from authors in to authors in but the number of links has increased more than three times from links in to links in this increase reflects the new links collaborations among existing authors in each period in addition to the new links with and among the new authors for all years after the number of links is larger than the number of authors and the proportion of the number of new links per new author increases albeit with fluctuations during the period under study interestingly the average number of links per author almost doubled from 02 in to in this shows the increasing trend of collaborations among authors in this field over time although is the most productive year of the field with the largest number of publications the number of new authors the number of new links and also the average number of links per author is the highest in using cumulative numbers of authors and links over time right side of table the number of authors and links among them is increasing rapidly and also the average number of links per author increases continuously from in to in the attachment behavior of authors and links in order to answer our first research question of how nodes authors behave during the evolution of the coauthorship network we evaluate different forms of attachments between new and existing authors and within these two subsets since during evolution of the co authorship network both new authors and new links are adding to the network we investigate authors and links attachment behavior first separately over time a abbasi et al journal of informetrics fig authors collaboration network red diamonds existing authors in blue circles newly added researchers in for interpretation of the references to color in this figure legend the reader is referred to the web version of the article the attachment behavior of authors as an example fig shows the co authorship network for the year which includes existing authors in red diamonds inside the oval this is the co authorship network in the year and the newly attached authors in blue circles the thickness of the links is proportionate to the number of collaborations between each pair of co authors the co authorship network shows just a few links between new authors and existing authors when compared to grouping among the new authors or existing authors thus there are many new authors who are not connected to any of the previously existing authors table shows the number of authors the number of new authors and the number of new authors who has attached to at least i another new author and ii an existing author it also shows the number of existing old authors up to the previous year who have attached to at least i a new author ii another existing author and iii any author that is independently of whether this is a new or old author since in a co authorship network we ignore single authored publications a new author will be added to the network because of connecting to either an existing author or another new author we did not add this frequency of attachment between a new author and any author as it is by definition for example the last row of table shows that of the newly attached authors in out of authors have been attached to at least another new author but only existing authors out of authors up to have been attached to at least one of the newly attached authors in the results show that a minority of the new authors attach to existing authors while most of them attach to other new authors studying attachment behavior of existing authors interestingly shows relatively few of them connect to any other authors no matter whether he is a new or existing author this rate decreases as the network grows over time the attachments of existing authors however are almost equally attached to other existing authors or new authors different types of new links attachments during the evolution of a co authorship network several types of new links may form i among new authors ii between new authors and existing authors iii among existing authors who had no collaboration link before and iv table authors nodes frequency over time year cumulative of authors new authors new author attached to at least old authors attached to at least a new author an old author a new author an old author any author 251 95 160 42 293 89 49 53 90 19 53 366 90 88 22 96 2595 295 99 145 91 92 a abbasi et al journal of informetrics 409 table different links types frequency over time year cumulative of links of new of new links links among new authors between new and old authors among old authors not connected among old co authors 547 83 18 35 87 40 1030 14 65 69 9 33 1773 359 9 165 21 658 18 4129 202 34 72 30 29 among existing authors already linked i e the new links at time t among existing authors who had at least one collaboration at any time before t table shows the number of links collaborations and new links per year followed by the frequency of four different types of new links the results indicate that most of the new links attachments occur among newly added authors and then between new and existing authors and among existing old co authors respectively although very few disconnected existing authors attach to each other existing co authors tend to coauthor more frequently it follows from table that existing authors collaborate either to new authors most possibly in supervision relations or with previous coauthors the previous collaboration can be expected to have generated trust among these authors which in turn facilitates their new collaboration the number of new links among new authors and between new and existing authors is almost equal with the exception of when there was an exceptionally high number of links among new authors preferential attachment behavior during network evolution in order to answer our second and third research questions about the behaviors of new authors attachments to existing authors based on their positional characteristics first we calculate existing centrality measures i e degree closeness and betweenness for each year for all authors and correlate these values with the frequencies of new authors and links attached to them in the year thereafter using spearman rank correlations we measure the correlations between existing authors centrality measures and the numbers of attached authors and links to them in the following year between and table shows that the existing authors centrality measures positively and significantly correlate with the numbers of new authors attaching to them except for degree and closeness centrality measures in and results of the correlation test not only support the preferential attachment process for this scientific collaboration network new authors prefer to attach to well connected authors having high degree centrality but also asserts that the new authors prefer to attach to the authors who are close to all other authors in the co authorship network having high closeness centrality and the authors who entertain the role of brokering and bridging in the network having high betweenness centrality we also examined the correlations between the existing authors centrality measures and the number of links attached to them the number of new links attach to an existing author considers both the number of newly attached authors and also recurrent collaborations with other existing nodes no matter the other existing author was connected before or not however the results of correlation test were virtually similar to those in table the results reveal that the correlation between betweenness centrality of existing authors and their attachment frequency the number of new authors and links attached to them in the following year is always significant and much higher than the degree and closeness centrality measures during the evolution of this collaboration network over time in other words table spearman correlation between the existing authors centrality measures and their attachment frequency in each period centrality measures number of new authors attached to the existing authors in the next year number of authors 480 1296 2595 degree centrality p 146 closeness centrality p 082 175 132 123 105 118 betweenness centrality p 232 292 correlation is significant at the level tailed correlation is significant at the level tailed 410 a abbasi et al journal of informetrics table spearman correlation between the existing authors centrality measures and their co author frequency in each period centrality measures number of all co authors of the existing authors in the next year number of authors 480 815 1296 1904 2595 degree centrality p 946 978 closeness centrality p 683 603 betweenness centrality p 508 488 correlation is significant at the level tailed authors with high betweenness centrality attract more new co authors than the well connected authors or the authors who are close to all others it is worth to note that looking at each centrality measure values over time the correlation between the number of newly attached authors and degree centrality remains almost constant with some fluctuation but for closeness centrality the correlation is fluctuating and for betweenness centrality it is increasing over time therefore we may infer that as the collaboration network grows betweenness centrality becomes increasingly important for attachments or in other words authors with high betweenness centrality gain more power and influence to attract new co authors an increasing number of authors prefer to attach to the existing authors who are controlling the flow of information communication by having a brokering or bridging role in the collaboration network in table we provide the rank order correlations between existing authors centrality measures for each year and their number of co authors in the next year the results indicate that the correlation coefficient is highest for degree centrality as expected because of the accumulative design followed by closeness centrality and thereafter betweenness centrality these results show that authors who have a larger number of co authors can be expected to have many co authors in the following period furthermore authors with higher closeness centrality measures can be expected to have more co authors in the following period than the authors with higher betweenness centrality measure while the coefficient values follow an increasing trend over time for degree centrality this trend is negative for closeness and is approximately stable for betweenness centrality ignoring the first period one can consider that authors degree centrality is always increasing over the evolution of their co authorship network considering an accumulative design but authors closeness and betweenness measures vary in each period as these are global measures which depend on the topology of the network in each period therefore it follows from the design that the correlation coefficients for degree centrality are very high and increasing furthermore in order to see how the position of existing authors in terms of these centrality measures have an impact on the number of newly attached authors to them we compared the average numbers of new authors relative to authors with low and high centrality measures the mean of each centrality measure is used as a threshold for dividing authors into two categories having low or high centrality measure table shows the average number of new authors for each category in each period the results in table show that authors with high values for degree centrality have on average larger number of new co authors compared to low degree authors this is the same for the authors with high closeness and betweenness authors with on average high betweenness centrality values have the largest number of new co authors in each period this effect increases over time furthermore authors with low degree centrality have the lowest number of new co authors on average in each period these results not only confirm our previous findings that the betweenness centrality of authors is more important than their degree or closeness but also show a large gap in preferential attachment between authors with high betweenness centrality on the one side and high degree or closeness centrality on the other table comparing authors average number of new authors attached to the existing authors in each period centrality measures average mean of the number new authors attached to the existing authors in the next year 2003 2004 degree centrality low 14 04 03 04 03 high 23 18 18 closeness centrality low 15 07 05 05 high 18 12 15 18 betweenness centrality low 15 06 07 07 06 05 high 29 37 33 57 79 85 a abbasi et al journal of informetrics 412 5 discussion and conclusion in order to investigate the attachment behavior of authors during the temporal evolution of their co authorship networks we examined whether central positions and roles in the collaboration network generate further gains in attractivity to new nodes network science has introduced centrality measures as proxies for specific positions and roles of the nodes in a network in this study of the evolution of the co authorship relations among researchers in steel structure between and we assessed the extent to which the main centrality measures i e degree closeness and betweenness associated with the expectation of new co authorships the results show that all three centrality values of existing authors correlate to the attachment frequency of new authors to them however more authors prefer to attach to authors who have higher betweenness centrality rather than those with higher degree or closeness centrality in other words during network evolution existing authors who have the power of controlling the communication and information flow that is higher betweenness centrality attract more new co authors than authors who have more co authorship links degree centrality or those who have more direct connection to all other nodes in the network closeness centrality our results also indicate that a relatively small number of new authors attach to the existing authors during the evolution of scientific collaboration networks new authors not necessarily attach to existing authors and thus add to the coherence in the network we find that also few existing authors have a new collaboration to any author in the following year on average this percentage is almost equal to that for new authors existing authors prefer to coauthor with others with whom they already coauthored before furthermore we found that authors rarely initiate coauthorship relations with other authors who have already published in their domain but prefer to have collaboration with new authors probably a large proportion of this category consists of collaborations between supervisors and their students in a co authorship network authors with high betweenness centrality seem to be supervisors since they usually have publications with both other colleagues and their graduate students during their academic life this gives them the brokering bridging role to connect students to other colleagues or students one of our questions was to examine whether preferential attachment played a role in the temporal evolution of scientific collaboration networks following previous studies in which preferential attachment was found in the evolution of a large number of network types including scientific collaboration networks whereas this was shown our main novel contribution however is that we found another property of authors than their degree namely the brokering role based on their position in their of co authorship networks i e betweenness centrality to be driving the coauthorship network betweenness centrality indicates the preferential attachment process among authors during the network evolution more than the number of links one has accumulated i e degree centrality a limitation of this study remains that we studied the single case of one field namely research about steel structures our contribution therefore provides mainly a hypothesis in order to generalize these findings one would need to investigate other scientific domains if this relationship between betweenness centrality in coauthorship networks and new entrants to the field is systematic it might help policy and decision makers to identify key actors who facilitate the flow of information by attaching new actors during the evolution and expansion of the networks this helps to control the distribution of resources information dissemination and propagation based on a typology of network positions acknowledgments the authors appreciate the anonymous reviewers for their positive and useful comments on the early drafts of this paper attitudes towards consumption have shifted in recent years and brought increasing concern over ecological soci etal and developmental impact a growing concern about climate change and a yearning for social embeddedness by localness and communal consumption albinsson perera belk botsman rogers have made the collaborative consumption sharing economy the peer to peer based activity of obtaining giving or sharing the access to goods and services coordinated through community based online services an appealing alternative for consumers past literature shows that people are turned away from ethical consumption because of economical and institutional reasons bray johns kilburn eckhardt belk devinney yet with the develop ment of new ways of consumption through the sharing economy such as collaborative consumption cc these issues are addressed and potentially overcome the sharing economy is an emerging economic technological phenom enon that is fuelled by developments in information and communications technology ict growing consumer awareness proliferation of collaborative web communities as well as social commerce sharing botsman rogers received april 17 revised march accepted march kaplan haenlein wang zhang we consider the sharing economy as an umbrella concept that v library c wileyonlinelibrary com asis t wileyonlinelibrary com asis t published published doi online online asi doi in june asi wiley online in wiley library online encompasses several ict developments and technologies among others cc which endorses sharing the consumption journal journal of of the the association association for for information information science science and and technology technology 9 of goods and services through online platforms in this study we explore how continued participation is motivated in the part of the sharing economy that is concerned with cc namely sharing the consumption of goods and services through activities such as renting swapping or trading this includes services such as zipcar as well as couchsurfing and airbnb forbes geron has estimated that revenue flowing through the sharing economy directly into people wallets will surpass 5 billion with growth exceeding se is referring to only cc and microwork at the same time investors regard the sharing economy as the new mega trend investing hundreds of millions into related start ups alsever further the rise of the sharing economy is predicted to have a major societal impact and thus holds relevance to both practitioners and policy makers eu environment for instance a potential change in e commerce patterns may have a significant impact on online sales which makes it important to examine the role and effects of cc in an online consumption context despite a growing practical importance there is a lack of quantitative studies on motivational factors that affect consumers attitudes and intentions towards cc the context is of especially great interest since participation in cc communities and services is generally characterized as driven by obligation to do good for other people and for the environment such as sharing helping others and engaging in sustainable behavior prothero etal sacks however cc may also provide economic benefits saving money facilitating access to resources and free riding which constitute more individualistic reasons for participating for these reasons there exists a real practical problem of how cc could become more widespread in particular the possible discrepancy between motivations and their effect on attitudes and behavior war rants an interesting context for research bray et al kollmuss agyeman this article explores people motivations to participate in cc we explore how cc can be defined in more detail in the section the sharing economy as a technological phenom enon but we mainly consider cc to be based on access over ownership the use of online services as well as monetary and nonmonetary transactions such as sharing swapping trading and renting see botsman rogers we adopt the lens of intrinsic and extrinsic motivations in attitude formation and use intentions related to cc see e g deci ryan lindenberg the research model and hypotheses were developed as a triangulation of three sources a self determination theory classification of motivations into intrinsic and extrinsic motivations deci ryan lindenberg b previous studies on parallel sharing economies related phenomena hennig thurau henning sattler lakhani wolf nov naaman ye and c context specific adjust ments the article is structured as follows the next section presents the theoretical framework and background for our hypotheses the subsequent section then outlines data and methods followed by the results the article concludes with a discussion on implications and avenues for future research background this section gives an overview of how cc is positioned in the sharing economy as a technological phenomenon as a first step we present our mapping of platforms to better understand the overall cc landscape we then unravel the contextual understanding of the term sharing within the sharing economy and the characteristics it is assigned such as the common traits of social dynamics and collectiv ism versus individual reputation the sharing economy as a technological phenomenon the development of information technologies alongside the growth of web has enabled the development of online platforms that promote user generated content sharing and collaboration kaplan haenlein classical examples of these include open source software repositories e g sourceforge and github collaborative online ency clopedias e g wikipedia and other content sharing sites e g youtube instagram or even peer to peer file sharing e g the pirate bay more recent examples are peer to peer financing such as microloans e g kiva and crowdfunding services e g kickstarter these four examples open source software online collaboration file sharing and peer to peer financing are considered as dif ferent instances of the phenomenon we label the sharing economy the phenomenon of the sharing economy thus emerges from a number of technological developments that have simplified sharing of both physical and nonphysical goods and services through the availability of various infor mation systems on the internet we will thus view the sharing economy primarily through the lens of informa tion technology we argue that although these different instances open source online collaboration file sharing peer to peer financing of the sharing economy seem superficially differ ent they share a number of common aspects to begin with all have origins and growth stemming from the tech driven culture of silicon valley this is easily attributed to open source and content sharing services but as reported by for example sacks this is also where the first largest and most successful cc services have emerged in the last few years more importantly the various instances of the sharing economy also share the characteristics of online collaboration online sharing social commerce and some form of underlying ideology such as collective purpose or a common good as will be discussed in the section aspects of the sharing economy all of these characteristics can also be attributed to cc services in this article also cc is mainly positioned as a category of this contemporary technology driven sharing economy in our view this is an interesting and relevant approach to cc because almost all practical cc activities are mediated journal journal of the of the association association for for information information science science and and technology technology september doi doi asi asi seen by various information systems as we will outline here in table notably some services facilitate multiple therefore we study cc mainly as a technological phenom types of activities such as renting as well as purchasing and enon as opposed to for example the perspective of an thus belong to more than one category emerging consumer culture we position our study in the the mapping of cc platforms revealed that the literature on technology participation and adoption as well activities may be separated into two main categories of as content contribution we view cc as not just consumption exchange access over ownership and transfer of ownership but as an activity where both the contribution and use of however it is possible for a platform to facilitate both resources are intertwined through peer to peer networks modes of exchange this occurs when the platform has more the consumer related literature is also relevant for than one type of trading activity such as lending access example cc could be viewed from perspective of sharing over ownership and donating transfer of ownership e g belk borrowing e g jenkins et al causing an overlap between the main categories out of the reuse and remix culture e g lessig charity platforms were identified as facilitating access over e g hibbert horne strahilevitz myers ownership while provided the transfer of ownership a second hand markets sustainable consumption e g total of platforms had overlapping categories young hwang mcdonald oates and for access over ownership is the most common mode of instance even anticonsumption ozanne ballantine exchange access over ownership means that users may we note however that although framing cc in the offer and share their goods and services to other users for a context of consumer studies is of course complementary it limited time through peer to peer sharing activities such as is beyond the scope of this work renting and lending see bardhi eckhardt most we define the term cc broadly as the peer to peer based common was renting for example monjoujou rents out activity of obtaining giving or sharing access to goods and childrens toys for a duration of 15 30 or days other services coordinated through community based online ser examples are airbnb and renttherunway where goods vices this definition was formed by the combination of and services can be accessed by users for a certain amount of previous considerations as well as by the mapping of time and often for a fee another example berlin based cc websites of the websites were identified by sys drivenow is a paid car sharing service where a user may tematically going through all the categories i e transport book any of the designated cars randomly distributed equipment children etc of the directory on collaborative throughout the city and when the user is done he may park consumption org this contains a collection of various types the car anywhere within the assigned city area of websites that relate their business to the sharing economy alternatively the transfer of ownership passes ownership cc the directory is continuously updated by adding ccs from one user to another through swapping donating and that are just starting out and also updated by removing those purchasing of primarily second hand goods for instance ccs that have halted operations services such as swapstyle or resecond help users to swap to qualify for the mapping the cc must be an online unwanted clothes other examples are zilch and thredup website a mobile app or a combination that is continuously swapping or donating are the most popular categories fol used and maintained by the users however a website that lowed by the least popular category namely purchasing used advertises a standalone and purely offline activity such as a goods an overview of the mapping can be seen in flea market would not qualify the evaluation of each table website was made by alphabetically and systematically furthermore this analysis sheds light on numerous going through the directory opening the website then aspects of the sharing economy but particularly on the mul reading and examining its content and if necessary signing tiplicity of the term sharing we want to emphasize that our up for an account to look at any additional features the definition of the sharing economy differs slightly from those mapping placed the ccs in different categories that of other scholars belk as well as some other described the mode of exchange sharing new purchase definitions of sharing economy lessig sacks second hand purchase renting donating swapping and or collaborative consumption belk lending or borrowing an overview of the mapping can be botsman rogers table overview of mapping of collaborative consumption services mode of exchange trading activity monetary transaction market allotment example access over ownership renting yes platforms renttherunway com lending no platforms couchsurfing com transfer of ownership swapping no platforms swapstyle com donating no platforms freegive co uk purchasing used goods yes platforms thedup com journal of the association for information science and technology doi asi journal of the association for information science and technology september doi asi moreover cc operates through technological platforms such as a website or mobile app yet relies heavily on social dynamics for the actual sharing and collaboration in fact wiertz and de ruyter propose that firms that own and operate such online platforms do not control the actual sharing at all instead the development is led by social dynamics such as enjoyment and self marketing of a community lin and lu wasko faraj therefore sharing economy and in particular cc platforms act merely as economical technological coordination providers this resembles for example github and torrent trackers which do not necessarily have control of the content distributed exchanged and coordinated collaborative consumption communi ties represent such coordinating centers in the context of cc in summary this article suggests that cc is a peer to peer based activity of obtaining giving or sharing access to goods and services coordinated through community based online services this is based on existing definitions that are combined and refined with the findings from the mapping of the platforms nevertheless there remains a difficulty in defining this phenomenon because of the wide variations in existing terminology a definition should include cc socioeconomical as well as technological aspects taking into account that it manifests varying degrees of digital and physical exchange in this way cc also affords several equally important perspectives for analysis however mainstream media have merely defined cc as an economic model based on sharing swapping trading or renting products and services enabling access over ownership botsman another previous schol arly definition restricts cc only to nonmonetary transac tions the acquisition and distribution of a resource for a fee or other compensation belk p however this is where the definitions diverge based on whether monetary exchange is allowed as a part of cc moreover as we discussed earlier publicly available list ings of cc services include a variety of services that have different features and modes of exchange including mon etary transactions in this article we have primarily inves tigated cc as a technological development and have viewed it from the perspective of research on peer to peer technologies such open source software repositories e g sourceforge and github collaborative online encyclope dias e g wikipedia and other content sharing sites e g youtube instagram or even peer to peer file sharing e g the pirate bay this approach provides a solid bridge to tie the cc phenomenon into the existing literature both conceptually and in terms of theory aspects of the sharing economy in the following four sections we take a more detailed look at the characteristics of the sharing economy namely online collaboration social commerce the notion of sharing online and consumer ideology collaboration online the growing use of information tech nologies in the web era has increased the amount of user generated content and also the manner in which infor mation is created and consumed online kaplan haenlein nov the peer to peer platform has grown into an essential tool for the purposes of such information cre ation and consumption the term peer to peer is commonly associated with file sharing however it also refers to the larger phenomenon of collaborative activities between users online such as consumer to consumer exchanges in fact rodrigues and druschel describe the peer to peer platform as a system in which content generation is highly distributed and decentralized as a result of the organic growth and strong user self organization moreover an essential aspect of this type of platforms is the focus on collaboration kaplan haenlein rodrigues druschel in which for example open software proj ects may be gathered and facilitated a particularly well known example is wikipedia where online users work together to produce content by sharing knowledge in addi tion studies on participation motives in open source soft ware oss projects lakhani wolf oreg nov roberts hann slaughter suggest that participation is influenced by a variety of factors such as reputation enjoyment and both intrinsic and extrinsic moti vation see also wasko faraj social commerce online social commerce rests on peer to peer interaction as it is a form of commerce that is mediated by social media and uses social media to support social interactions and user contributions to assist activities in the buying and selling of products and services online and offline wang zhang p social commerce and social shopping are often used interchangeably although social shopping is a subcategory of social commerce stephen toubia and is more related to the social influence exerted by peers on purchasing decisions wang zhang on the other hand group deals that are obtained via social buying services such as groupon seem to mostly be motivated by saving money social commerce thus relies on platforms with peer to peer interaction which in turn rely on users being motivated to continue using and engaging through social networking sites sns sns and social commerce share common ground as both involve peer to peer interaction on social media although the latter also include mercantile features ellison boyd wang zhang the motiva tion of users to continue participating in social commerce is multifaceted and often relies on the perception of individual enjoyment also through relatedness and economic benefits nevertheless wang and zhang assert that social commerce is moving beyond individual enjoyment and cen tering on economic concern for instance a pertinent form of social commerce is the consumer self coordination of group deals for pursuit of economic gains wang zhang kozinets also proposes that consumers are empowered through peer to peer sharing in an online journal journal of the of the association association for for information information science science and and technology technology september doi doi asi asi reputation commerce setting in which they turn to their social networks and self fulfillment this is much like social to retrieve information about products rather than commer commerce and online sharing that are also driven by enjoy cial sources the role of marketers is thus reduced while the ment economic incentive reputation yet additionally role of users is induced to be both a consumer and a pro paired with collaboration the application of ideology such ducer this is also important in many cases of cc in which as sustainability and green consumption is mainly propelled the participants can be consumers providers or both by reputation and economic concern as a result we propose four possible and distinguishable categories in which the sharing online the term sharing has experienced a forthcoming hypotheses are developed namely sustainabil major change in meaning with the evolution of online ser ity enjoyment reputation and economic benefits these will vices especially in an sns setting kaplan haenlein be discussed in more detail in an sns context the concept of sharing commonly self determination theory sdt deci ryan refers to sharing information such as status updates links posits that motivations can be distinguished as intrinsic or or photos however increased reliance on it based extrinsic the former emerge from the intrinsic value or e commerce systems has also facilitated the sharing of enjoyment related to the given activity whereas extrinsic goods and services through information technology motivations are related to external pressures such as repu galbreth ghosh shor hennig thurau etal tation and monetary gain according to lindenberg such as cc platforms like couchsurfing zipcar there are two kinds of intrinsic motivations enjoyment neighbourgoods and sharetribe derived from the activity itself and value derived from acting the development of cc platforms have thus far primar appropriately that is conforming to norms related ily been investigated from a service design perspective e g studies have also classified these motivations by the degree hamari lamberton rose suhonen of association with other people lakhani wolf lampinen cheshire antin for example couch nov et al which is complementary to lindenberg surfing a community for sharing accommodation among conceptualization for example striving to enjoy an travelers and one of the most successful sharing services to activity or obtaining economic gains through the activity are date has received the most attention molz rosen not directly affected by others opinions on the other hand lafontaine hendrickson sharing has been studied reputation and conforming to norms depend directly on how in the context of digital goods e g music files see e g other people reflect upon the activity we operationalize shang chen chen and open source software for these motivational dimensions as follows for intrinsic moti example huang studies norms and motivations asso vations we consider a enjoyment b sustainability and for ciated with peer to peer music sharing whereas zentner extrinsic motivations c economic benefits and d reputa focuses on the effects of music sharing on record tion the following subsections discuss the variables and sales finally in the context of information sharing nov hypotheses in more detail examines motivations for wikipedia editors and nov et al address online photography sharing sustainability ideological considerations information technology is participation in cc is generally expected to be highly increasingly used as a means to further collective action in ecologically sustainable prothero et al sacks support of the advancement of an ideology or idea oh such motivations are generally linked to ideology and norms agrawal rao for instance the social media plat lindenberg which in our theoretical framework and form twitter was used as a reporting tool during the arab in related work lakhani wolf nov et al are spring metzgar maruggi and the u s presi conceptualized as intrinsic motivations recent develop dential candidates campaigned extensively through social ments suggest that cc platforms are used to foster a sus media wattal schuff mandviwalla williams tainable marketplace phipps et al that optimizes open source and in particular the free software movement the environmental social and economic consequences of have strong ideological underpinnings raymond consumption in order to meet the needs of both current and however the ideology and ideas that underlie the sharing future generations luchs et al p also open economy may go beyond collective action for political pur source software development and participation in peer proposes even if notions of anticonsumerism clearly are related duction e g wikipedia are driven by altruistic motives ozanne ballantine we argue that green consump such as openness and freedom of information as argued by tion see e g eckhardt et al and other sustainable nov as well as oreg and nov thus partici behavior are even more important drivers in the context of pation and collaboration in online platforms may be influcend enced by attitudes shaped by ideology and socio economic concerns such as anti establishment sentiments hennig research model and hypotheses thurau et al or a preference for greener consumption which we believe to be a particularly important factor as discussed online collaboration such as peer to peer in the context of cc therefore we operationalize the intrin activity is fuelled by enjoyment economic incentive sic motivation related to norms as ecological sustainability journal of journal the association of the association for information for information science and science technology september and technology 5 doi doi asi asi we hypothesize that sustainability is a major predictor for attitude formation and behavioral intentions towards cc intrinsic motivation sustainability perceived sus tainability of cc positively influences attitudes towards cc intrinsic motivation sustainability perceived sus tainability of cc positively influences behavioral intentions to participate in cc enjoyment a fundamental dimension of intrinsic motivation is the autotelic nature of the activity or the enjoyment derived from the activity itself deci ryan lindenberg in terms of intrinsic motivation software developers contribute to open source projects as a result of enjoyment and a feeling of competence lakhani wolf nov roberts et al wasko faraj see also ryan deci enjoyment has been regarded as an important factor also in other sharing related activities such as infor mation system use van der heijden 2004 and information sharing on the internet nov nov et al nev ertheless the initial motivation to collaborate does not explain nor predict sustained participation fang neufeld a study on the continued use of social networking services established that enjoyment is a primary factor fol lowed by the number of peers and usefulness lin lu social networking services and similar service design used elsewhere can be seen to especially promote related ness see hamari koivisto and e g deci ryan ryan deci on relatedness which is a major determinant for intrinsically motivated use such as enjoy ment therefore we include enjoyment as the second intrin sic motivation to our model to predict attitudes and behavioral intentions towards cc intrinsic motivation enjoyment perceived enjoy ment from participating in cc positively influences attitude towards cc intrinsic motivation enjoyment perceived enjoy ment from participating in cc positively influences behav ioral intentions to participate in cc reputation reputation has been shown to be an important external motivation factor in determining participation in communi ties and other online collaboration activities such as informa tion sharing davenport prusak wasko faraj and open source projects lakhani wolf nov et al in particular gaining reputation among like minded people has been shown to motivate sharing in online communities and open source projects parameswaran whinston raymond anthony smith and williamson reported that reputation and commitment to the community are important drivers for wikipedia editors when wasko and faraj explored why individuals share knowledge in electronic networks of practice they established that contribution is often underlined by the per ception that it enhances personal reputation donath also supported the conclusion that reputation can be a moti vator for active participation yang and lai p found that individuals are more likely to gain self based achievement rather than enjoyment in the process of sharing knowledge hars and ou also found that self marketing and building of reputation are the strongest indi cators of likelihood to collaborate online similarly an active participant in cc may expect intangible rewards in the form of higher status within the cc community extrinsic motivation reputation perceived reputa tion increase from participating in cc positively influences attitude towards cc extrinsic motivation reputation perceived reputa tion increase from participating in cc positively influences behavioral intentions to participate in cc economic benefits as the previous sections discuss cc and sharing goods and services in general is often regarded as not only eco logically sound but also economical see for example the works of belk as well as lamberton and rose therefore participating in sharing can also be ratio nal utility maximizing behavior wherein the consumer replaces exclusive ownership of goods with lower cost options from within a cc service furthermore there are signs of both positive and negative influences of economic incentives on sharing behavior bock zmud kim lee davenport prusak kankanhalli tan wei hars and ou study both the intrinsic and extrinsic motivations of participation in open source devel opment and find that a strong extrinsic motivation is the potential future rewards such as economic benefits addi tionally in the context of peer to peer networks sharing serves as an incentive for saving economic resources luchs etal therefore we hypothesize that extrinsic rewards in the form of saving money and time derived from cc positively influence attitudes toward cc and intentions to participate in it extrinsic motivation economic outcomes perceived extrinsic reward of participating in cc positively influences attitude towards cc extrinsic motivation economic outcomes perceived extrinsic reward of participating in cc positively influences behavioral intentions to participate in cc attitude attitude is regarded as a major determinant of behavior ajzen furthermore when studying a phenomenon with which there is reason to expect a possible discrepancy between attitudes and behavior it is essential to measure them separately with respect to motivation to participate or consume certain goods consumer behavior literature suggests that journal journal of the of the association association for for information information science science and and technology technology september doi doi asi asi questionnaire although consumers may be ideologically and ethically we defined cc as an economic model based minded their aspirations may not translate into sustainable on sharing swapping bartering trading or renting access behavior e g bray et al phipps et al vermeir to products within a community as opposed to personal verbeke a few issues might explain this attitude ownership behavior gap a actually pursuing sustainable behavior can the questionnaire employed psychometric measurement be costly both in terms of coordination and direct cost b nunnally we measured each construct with four or people lack the means of deriving benefits from signaling five items that were all on a 7 point likert scale all items such behavior and thus not able to gain recognition from the were adapted from existing prominent published sources behavior for instance studies show that people are moti except for the items for the sust construct see appendix vated to take on sustainable behavior especially when other the primary analytical technique was structural equation consumers have been able to signal that they are also par modeling sem see e g hair et al nunnally ticipating goldstein cialdini griskevicius c sem provides the possibility to run multivariate multilevel there is not enough information for the consumers about path analyses and thus permits more complex models than sustainable consumption we argue that technologically traditional regression analyses for instance path modeling mediated cc may alleviate these concerns they may enable provides a powerful tool to investigate both direct and medi a more efficient coordination of sharing activities which in ated effects furthermore sem analyses are the primary turn aids in the facilitation of active communities around a technique when using latent psychometric variables the cause nonetheless the question remains whether peoples descriptive demographic data were analyzed in spss attitudes towards cc are determined by for example green and all of the model testing was conducted through partial values and if so do they also reflect their actual behavior or least squares pls analysis with smartpls ringle does the attitude behavior gap exist also in this context in wende will order to address this issue among other predictions we investigate iors the relationship between the attitudes and behav validity and reliability attitude towards cc positively influences behavioral we tested convergent validity with three metrics average intention to participate in cc variance extracted ave composite reliability cr and cronbach alpha alpha all of these values were accept able see ave should be greater than 5 cr methods and data greater than 7 and cronbach alpha above fornell data larcker nunnally construct extr had a slightly smaller alpha than recommended however the the data consist of responses obtained from regis other validity metrics were good and the lower alpha is not tered users of the service sharetribe who were recruited via likely to point to a validity issue the construct passed all of an official sharetribe e mail newsletter sharetribe the validity and reliability tests no indicators were omitted http www sharetribe com is an international cc hub that discriminant validity was first assessed by a comparison offers its service package to various organizations of the square root of the ave of each construct to all sharetribe is used in communities all over the world and at correlations between it and other constructs fornell the time of writing there were local sharetribes larcker where all of the square roots of the aves worldwide the company sharetribe ltd is a social for should be greater than any of the correlations between the profit enterprise registered in finland its stated mission is to corresponding construct and another construct chin help people connect with their community and to help elimi second we assessed discriminant validity by confirming nate excessive waste by making it easier for everyone to use that all items corresponding to a specific construct had a assets more effectively by sharing them most of the higher loading with the appropriate construct than with any sharetribes are narrow local communities such as orga nizations or neighborhoods where the benefits of cc are emphasized in forms of trust and information access and also to decrease transaction costs table demographic information the responses were gathered in january partici pants were informed that they had the chance of winning a n n euro gift card for an internet store the demographics gender of the sample are shown in table 2 we also want to point out that although the respondents were all registered users of sharetribe most of them were not active users of the site age female 42 tenure months 30 18 male months 17 12 months 23 19 12 24 months 24 25 39 24 months 18 11 however as registered users of a cc service we expect the 30 17 months respondents to be more knowledgeable about cc than the population at large and therefore in a better position to give an informed response to our survey at the beginning of the 35 21 12 36 40 12 7 40 22 journal of journal the association of the association for information for information science and science technology september and technology 7 doi doi asi asi table convergent and discriminant validity ave cr alpha att bi enj extr rep sust att 899 801 bi 907 684 enj 919 706 833 extr 829 473 rep 883 391 605 809 sust 907 798 526 306 sustainability reputation 591 attitude behavioral intention fig results model 8 journal of the association for information science and technology doi asi 421 enjoyment 316 economic benefits 125 other construct third following pavlou liang and xue results we determined that no intercorrelation between con structs was more than 9 in the correlation matrix see the model could account for 75 of the variance in table all three tests indicate that the discriminant valid attitudes towards cc and 3 of the variance in behav ity and reliability are acceptable in addition to reduce the ioral intention to participate in cc the results are summa likelihood of common method bias we randomized the rized in figure and table order of the measurement items in the survey limiting in case of the intrinsic motivations perceived sustainabil respondents ability to detect patterns between measurement ity significantly predicted attitude to cc beta 591 items cook campbell day t however it did not have a direct association the sample size satisfies different criteria for the lower with behavioral intentions beta 066 t 859 bounds of sample size for pls sem a times the largest further investigation though showed that perceived sus number of structural paths directed at a particular construct tainability has a small beta 121 t 832 total effect in the inner path model therefore the sample size threshold through attitude to behavioral intention perceived enjoy for the model in this study would be cases chin ment had a significant positive effect on both attitude newsted and b according to anderson and gerbing towards cc beta 421 t 7 491 and behavioral a threshold for any type of sem is approximately intention to participate in cc services beta 451 respondents for models where constructs comprise of t 936 three or four indicators c the sample size also satisfies in case of the extrinsic motivations expected gains in stricter criteria relevant for variance based sem for reputation did not significantly affect either attitude towards example bentler and chou recommend a ratio of cc beta 047 t 913 or behavioral intention to five cases per observed variable therefore the sample size participate in cc services beta 108 t 1 581 threshold for the model in this study would be anticipated gain of economic benefits did not have a journal of the association for information science and technology september doi asi ________________ that table 4 direct and mediated effects affect attitudes and behavioral intentions perceived direct effects total effects direct effect mediated effect via attitude sustainability is an important factor in the formation of positive attitudes towards cc but economic benefits are a stronger motivator for intentions to participate in cc attitude behavioral intention behavioral intention eckhardt et al found three main reasons why people may not be willing to consume sustainably eco attitude n a 316 316 nomic rationalizations institutional dependencies and sustainability 591 066 121 developmental realism the same reasons might also apply enjoyment 421 451 reputation 047 108 economic benefits 583 093 to cc with regards to the motivations related to sustainabil ity for instance related to economic rationalizations cc might not in all cases turn out to be economical sporadic and unstandardized trades with a variety of unknown people can unexpectedly increase search and coordination costs although cc could be more economical in monetary terms it may not be so in other respects moreover as long as new imported products remain on the market with relatively low prices that do not necessarily reflect the ecological price or impact that the manufacturing and shipping necessitate people might not be interested in sharing along those same lines eckhardt et al suggest that people commonly justify their nonsustainable consumption with institutional reasons legislators have not curbed consumption manufac turing or imports of unsustainable products with regulations and taxes following from these institutional dependencies and as eckhardt et al suggest it is believed that sharing may curb economic growth although these notions have come up in general qualitative inquiries they deserve further research not only in the context of cc to investigate their quantitative impact on sustainable consumption behavior see also carrington neville whitwell and kollmuss agyeman 2002 cc has been regarded as a mode of consumption that engages especially environmentally and ecologically con scious consumers our results also support the notion that viewing cc as a sustainable activity can lead to an increase in participation but only if by taking this view we increase positive attitudes towards cc our results however also suggest that these aspirations might not translate strongly into action expectations as to the diffusion of cc might thus be deflated it may actually be people seeking economic benefits who in the end opportunistically adopt cc as one of the modes of consumption in a worst case scenario some users in a sharing economy might be altruistic and share their goods whereas other users may be mostly enjoying benefits from others sharing this situation might affect the sustainability of cc services in general further studies could investigate coordination mechanisms that would alle viate such problems in cc see for example ostrom on managing shared resources on the other hand our results also suggest that enjoy ment plays an essential role in attitude formation and use intentions some people might take part in cc simply because it is fun and provides a meaningful way to interact with other members of the community therefore even if the particular motivations of individual participants vary from mainly altruistic to strongly gain seeking the sharing economy as a whole remains functional provided that the 004 125 124 note p 1 p 01 significant effect on attitude towards cc beta 004 t 063 but did have significantly positive direct influ ence on intention to participate in cc beta 0 125 t 1 769 finally attitude had a significant positive effect on behavioral intentions beta 0 316 t 3 342 the effect of reported attitude on behavior is interesting in the contexts of sustainability as noted many studies have found that there is a gap between people attitudes and behavior in similarly motivated sharing activities although the path coefficient here is significant and positive the effect size from attitude is rather low when the path between attitude and behavior is deleted from a model the remaining models still explain 8 of behavioral intentions com pared to the original 3 therefore it appears that also in the context of cc an attitude behavior gap may exist more over the path coefficient between attitude and behavior can be regarded as relatively small when compared to studies on technology adoption in general discussion and directions for further research our results indicate that intrinsic motivations are a strong determinant of attitude and not rejected whereas extrinsic motivations did not reflect positively on attitude and rejected for continuous use intentions however extrinsic motivations were a more prominent pre dictor not rejected along with enjoyment from the activity not rejected attitude also as expected positively influences use intentions but to quite a small degree in comparison to the relationship typically observed between these constructs this could indicate a discrepancy between reported atti tudes and actual behavior in this context although per ceived sustainability positively influences attitudes towards cc it plays a lesser role when people consider actual par ticipation in cc however we could also observe that some of the perceived sustainability was translated into behavioral intentions through attitude on the other hand economic benefits saving money and time seem to have a significant effect on behavioral intentions but not on attitudes towards cc thus there seems to be a discrepancy between factors journal of journal the association of the association for information for information science and science technology september and technology 2016 2015 9 doi doi asi 1002 asi ________________ benefits for each participant outweigh possible costs incurred through the imbalance of contributions and of course economic gains as defined in this study also trans late into saving money which is an understandable motiva tor for many consumers such as those affected by the recent financial crisis norms diffuse in communities over time ajzen fishbein fishbein ajzen and according to lindenberg when obligation to those norms is a strong motivator for an individual personal gain seeking will be minimized our results might suggest that in rela tively new cc services see table 1 for how long users had been members of the sharetribe service perhaps not enough time has elapsed for diffusion and establishment of norms within the community also ties between people within the community may be too weak for norms to have a meaningful effect furthermore as initially discussed in the context of blood donation titmuss and further theoretically developed by frey and jegen the crowding out phenomenon might be at play within the sharing economy in this phenomenon extrinsic motivations start over shadowing the initial intrinsic motivations although people might have started participating in cc for intrinsic reasons e g because of perceived sustainability the motivations might have shifted toward extrinsic ones similar phenomena have been discussed in the contexts of recycling de young and information sharing nov two alternative approaches to preventing the crowding out effect and therefore preventing the economic benefits becoming the dominant motivator can be conceived we can either increase the intrinsic motivations or curb the extrinsic ones in the context of this study two main intrin sic motivations were considered the enjoyment of partici pating and internalized ideological reasons sustainability as seen in the results the enjoyment of participation was the strongest determinant therefore simply attempting to make participation more pleasurable more communal and supportive for the ideological cause by promoting a posi tive buzz should prove to hinder the crowding out effect via enforcing the intrinsic motivations the other approach would attempt to impede extrinsic motivations taking hold of those participating a softer form of this approach might include the employment of trust systems that enable par ticipants to formally signal to other users how equally they share or consume for instance gamification has been used for both increasing intrinsic motivations via attempt ing to make the interaction with the system more game like as well as for tracking participant behaviors hamari hamari huotari tolvanen 2015 a popular example of such a system would be achievements that monitor user behavior and award badges in user profiles of differing feats and predefined behaviors hamari eranti simple trust systems have been employed in several e commerce websites such as ebay in the form of seller feedback in sharing economy platforms different ideological and communal tendencies such as anti establishment sentiment freedom of information and in the case of ccs especially the greenness of the activity are considered important inter nalized drives for behavior if we observe for example the culture around file sharing we can immediately notice how strong and prevalent the ideology is within the communities that participate pirates have their own political parties they organize rallies and generally celebrate their idea of free information sharing there are many channels and mechanisms for participants to congregate and revel in the community binding ideological drive that potentially further boosts the internalized motivations to participate in the file sharing activities following this reasoning ccs also could benefit from employing affordances for participants to signal their norms and their compliance to those norms that are commonly held within a cc community a stricter method could employ systems that would allo cate resources evenly in contrast to merely monitoring the sharing activities with the aim of regulating free riding and preventing excessive economic exploitation of ccs this could be achieved for example by regulating the ratio of contributions and receivers of favors some file sharing systems employ such mechanisms although goods and services shared in ccs are not of equal value another method would be to monitor the inbound and outbound value rather than absolute amount of goods and services from the individuals participating in the cc however in the end all regulatory systems seem to partly defeat the original ideas of sharing economies freedom of exchange altruism and communal trust on the other hand even though the crowding out effect is commonly considered as a negative motivational phenomenon strong utilitarian motivations may also encourage people to liquidate their possessions and therefore stimulate the activity within the sharing economy therefore pure utilitarian or economic motivations do not necessarily have to be considered as solely negative aspects perhaps users with differing motivations for participating could coincide in cc platforms in mutually beneficial ways further studies could longitudinally follow the shifts in motivations for participating in the sharing economy the technological and economical developments around sharing economies can also lead to interesting legal reper cussions the maintaining organizations of some other online peer to peer coordination hubs have ended up in legal problems based on what individuals have exchanged through the hub e g pirate bay because in practice and principle all sharing economy services to varying degrees possess the trait of being autonomous and separated hubs from their users it is an interesting question to what extent their operators should be held responsible for the goods being exchanged through them legal troubles could loom over any distribution and coordination of sharing whether the goods being shared are digital or physical see e g manner siniketo polland radbod for craigslist and pirate bay related cases these aspects also pose interesting further research questions the data deluge has arrived much anticipated by the science community hey trefethen the popular press is now heralding the wide availability of data for use by anyone anywhere not only have nature and science the premier science journals published feature sections on big data so have wired magazine anderson and the economist universities are assessing their rights roles and responsibilities for managing and for exploiting data from their researchers association of research libraries lyon grand expectations for the data rich world include discoveries of new drugs a better understanding of the earth climate and improved ability to examine history and culture the growth of data in the big sciences such as astronomy and physics has led to new models of science collectively known as the fourth paradigm and to the emergence of new fields of study such as astroinformatics computational biology and digital humanities borgman hey tansley tolle the long tail of science also is becoming more data intensive as new methods and instrumentation enable individual investigators and small teams to gather unprecedented volumes of observations if the rewards of the data deluge are to be reaped then researchers who produce those data must share them and do so in such a way that the data are interpretable and reusable by others underlying this simple statement are thick layers of complexity about the nature of data research innovation and scholarship incentives and rewards economics and intellectual property and public policy sharing research data is thus an intricate and difficult problem in other words a conundrum the dirty little secret behind the promotion of data sharing is that not much sharing may be taking place despite pressure from funding agencies and findings that sharing research data may increase citation rates piwowar becich bilofsky crowley piwowar chapman piwowar day fridsma relatively few studies document consistent data release data sharing activities appear to be concentrated in a few fields and practices even within these fields are inconsistent british library received june revised december accepted december cragin palmer carlson witt palmer cragin heidorn smith wynholds fearon borgman asis t published online april in wiley online library wileyonlinelibrary com doi asi traweek in nine years of studying data practices in a national science foundation nsf science and technology journal of the american society for information science and technology data center we have found that little research data is circulated management plan that addresses the above mentioned beyond the research teams that produce them and few requirement and that the plan would be subject to peer review requests are made for these data mayernik wallis national science foundation the nsf mayernik borgman pepe the reasons for not requirement is thus more comprehensive than that of the nih sharing data are many researchers may lack the expertise which applies only to larger grants and is negotiated between resources or incentives to share their data data often do not investigators and program officers rather than being subject to exist in transferable forms some data are not sharable for peer review ethical or epistemological reasons in many cases it is not u k funding agencies began to formulate data release clear what are the data associated with a research project policies in the wellcome trust this article explores the complexity of data sharing economic and social research council lyon examining the roots of current discourse the problematic in response the digital curation centre dcc founded as notion of data per se current policy arguments in favor of part of the u k escience initiatives created a series of data sharing differing perspectives of stakeholders and templates for data management plans corresponding to the associated ethical professional and epistemological aspects requirements of individual u k funding agencies digital of research data it is a foray into a labyrinth worthy of curation centre the dcc is adding links to u s book length examination agency requirements and the dcc templates are the basis for many of the planning documents now being developed why is data sharing urgent by u s universities abrams cruse kunze witt sharing data is not a new topic of discussion in research and policy circles thoughtful reports on the reasons to improve data sharing and curation date at least from the fienberg martin straf and many more reports followed national research council national science board networking and infor mation technology research and development berman et al dalrymple esanu uhlir hanson et al data sharing has many meanings in these reports which are rarely made explicit for the purposes of this article data sharing is the release of research data for use by others release may take many forms from private exchange upon request to deposit in a public data collection posting datasets on a public website or providing them to a journal as supple mentary materials also qualifies as sharing the degree of usefulness trustworthiness and value of shared data varies widely however some may be richly structured and curated others may be raw files with minimal documentation simi larly the intended users may vary from researchers within a narrow specialty to the general public funding agencies have begun requiring data release to varying degrees and with varying degrees of enforcement the national institutes of health nih added a data man agement plan requirement in for grants over national science board the nsf has long had this statement requiring data sharing in its grant contracts but has not enforced the requirement consistently national science foundation carlson brandt cragin similarly selected journals long have required the deposit of data and other research documentation associated with published articles requirements to deposit genome sequences are best known wellcome trust genomecanada berman etal hilgartner but journals in economics and many other fields also require access to data the mechanisms of enforcement may be formal for example by requiring deposit in specific collections such as the protein data bank with the structure entry number included in the article protein data bank or less formal such as links to sources journal policies have also become more rigorous about data access as of late science in an editorial accompanying a special issue on data announced more extensive require ments such as sharing computer code involved in the creation or analysis of data and including a specific state ment regarding the availability and curation of data in the article acknowledgements hanson et al also recently announced are new data archiving policies by key journals in evolution and ecology including the american naturalist evolution journal of evolutionary biology molecular ecology and heredity dryad whitlock mcpeek rausher rieseberg moore p these journals are requiring or encouraging the deposit of data in public archives in the committee on data for science and tech nology of the international council for science codata established a new task group to study data citation and attribution data citation standards and practices investigators are expected to share with other researchers at no more than incremental cost and within a reasonable time the primary data samples physical collections and other support ing materials created or gathered in the course of work under nsf grants grantees are expected to encourage and facilitate among the chief concerns of the task group are best prac tices in citing data attributing data sources and giving credit to data creators curators and others who add value to research data national academies of science the codata task group is surveying scholarly disciplines and such sharing national science foundation n p stakeholders well beyond the sciences including universi ties publishers and funding agencies in the nsf made the long anticipated announce although none of these actions alone caused the sense ment that all future grant proposals would require a two page of urgency for data sharing the nsf requirement for data journal of the american society for information science and technology june doi asi the management plans appears to be the tipping point at least above mentioned notion of data transcends the in the united states the nsf has an encyclopedic scope sciences and other domains of scholarship acknowledging across the sciences and social sciences excluding only the the many forms that data can take data sources also vary arts humanities and medicine and even funds grants in widely in the physical and life sciences most data are these areas for projects that address scientific problems gathered or produced by researchers such as by observa the nsf requirement applies to all proposals of any size tions experiments or models in the social sciences in any directorate although these are data management researchers may gather or produce their own data or they plans and not data sharing plans they do strongly encour may obtain data from other sources such as public records of age sharing and they are subject to peer review thus an economic activity the notion of data is least well developed investigator ability to articulate what her or his data are in the humanities although the growth of digital humani how they will be managed how they will be shared and ties research has led to more common usage of the term if not shared then why will influence whether or not a humanities data most often are drawn from records of project is funded in making these plans part of the peer human culture whether archival materials published docu review process the nsf has accelerated the conversation ments or artifacts borgman about data sharing among stakeholders in publicly funded the term dataset is sometimes conflated with the notion research of data however definitions of dataset in the scientific what is often not explicit in the discussions of data literature have at least four common themes grouping management plans and data sharing requirements are the content relatedness and purpose each of which has competing interests and differing incentives of the many multiple categories renear sacchi wickett stakeholders involved these elements need to be brought to although dataset may be useful to refer to a collection of the foreground of the data sharing conversation data for the purposes of citation the term does little to clarify what is meant by data the difficulty of identifying what are data appropriate units of data to reference is a core problem in establishing practices for data citation and attribution a starting point to discuss the conundrum of sharing borgman research data is to examine the complex notion of data an artifact or observation may be at best alleged evidence to use michael buckland pithy phrase buckland communities and data data may exist only in the eye of the beholder the recog nition that an observation artifact or record constitutes data is itself a scholarly act data curators librarians archivists and others involved in data management may be offered a in the requirement for data management plans the nsf sidesteps the definition of data with the first of its frequently asked questions national science foundation collection that is deemed data by the collector but not per ceived as such by the recipients conversely an investigator may be holding collections of materials without realizing how valuable they may be as data the concept of data is difficult to define as data may take what constitutes data covered by a data management plan what constitutes such data will be determined by the community of interest through the process of peer review and program management this may include but is not limited to data publications samples physical collections software and many forms both physical and digital among the most models widely cited definitions is this one from a national aca demies of science report data are facts numbers letters the nsf choice of the term community of interest and symbols that describe an object idea condition situa echoes the practice of the digital archiving world where tion or other factors national research council policies are framed in terms of the designated community p a more current working definition from an internal consultative committee for space data systems it academy document is particularly useful for discussions of is left to the investigator or to the data archive to desig sharing nate the appropriate community of interest therein lies the rub an investigator may be part of mul the term data as used in this document is meant to be tiple overlapping communities of interest each of which broadly inclusive in addition to digital manifestations of lit may have different notions of what are data and different erature including text sound still images moving images models games or simulations it refers as well to forms of data and databases that generally require the assistance of computational machinery and software in order to be useful such as various types of laboratory data including spectrographic genomic sequencing and electron microscopy data observational data such as remote sensing geospatial and socioeconomic data and other forms of data either gen data practices the boundaries of communities of interest are neither clear nor stable in the case of data management plans an investigator is asked to identify the appropriate community for the purposes of a specific grant proposal and for the proposed duration of that award communities of interest as used by the nsf appear to be narrower than disciplines or research specialties com erated or compiled by humans or machines uhlir cohen munities of practice lave wenger wenger and epistemic cultures knorr cetina are groupings journal of the american society for information science and technology june doi asi also commonly used in social studies of science the concept of collect observations model social systems conduct communities of practice was originated by lave and wenger experiments and assemble records humanities scholars to describe how knowledge is learned and shared in groups do all of the above but are the least likely to perform a concept subsequently much studied and extended oster experiments lund carlile epistemic cultures in contrast are investigators collect data for many purposes using many neither disciplines nor communities they are more a set of methods research purposes methods and approaches all arrangements and mechanisms associated with the pro influence what investigators consider to be their data the cesses of constructing knowledge and include individuals degree to which those data might be sharable and the con groups artifacts and technologies knorr cetina van ditions under which researchers are willing to share those house common to both communities of practice and data with others the criteria for identifying data and for epistemic cultures is the idea that knowledge is situated and sharing them are not yet well understood understanding local nancy van house summarizes this perspective practices problems and policies for data is an expanding succinctly there is no view from nowhere knowledge area of research in the fields of information studies and is always situated in a place time conditions practices and social studies of science borgman bowker understandings there is no single knowledge but multiple edwards mayernik batcheller bowker borgman knowledges p karasti baker halkola mayernik it is the difficulty of bounding either community of inter mayernik batcheller borgman palmer est or data not to mention bounding the intersection of these renear palmer ribes baker millerand two concepts that makes data sharing requirements so chal bowker ribes finholt wynholds etal lenging to articulate thus the next sections are devoted to zimmerman explicating the many forms of research data that might be created and shared or at least made sharable purposes for collecting data categories of data a brief survey of the purposes for which research data are collected will illustrate some of the complexities that arise in some types of data have both immediate and enduring making them available to other potential users figure values some gain value over time some have transient presents three dimensions along which data collection may value and yet others are easier to recreate than to curate vary these dimensions are neither exhaustive nor mutually national research council many of exclusive for each dimension the first pole is the more these distinctions depend on the category of data as identi local and flexible type of purpose whereas the second pole fied in an influential nsb report national science board is more global and systematized four scenarios drawn from observational computational experimental and our research on data practices are used to illustrate these records dimensions along with a variety of other examples three of observational data include weather measurements and the scenarios are introduced in this section beach quality attitude surveys either of which may be associated with star dust and an online survey the fourth scenario archival specific places and times or may involve multiple places records is introduced in the section on approaches to and times e g cross sectional longitudinal studies com handling data putational data result from executing a computer model or simulation whether for physics or cultural virtual reality specificity of purpose the first dimension illustrated is replicating the model or simulation in the future may specificity of purpose ranging from exploratory research to require extensive documentation of the hardware software building observatories exploratory investigations pursue and input data in some cases only the output of the model specific questions often at a specific site usually about a might be preserved experimental data include results specific phenomenon and may take place in a laboratory a from laboratory studies such as measurements of chemical field setting or some combination thereof reactions or from field experiments such as controlled studies to identify sources of bacteria and other beach behavioral studies whether sufficient data and documen contaminants offer examples of exploratory research in the tation to reproduce the experiment are kept varies by beach quality scenario one or two students collect water the cost and reproducibility of the experiment records samples selected for time of day location weather condi of government business and public and private life also tions e g dry or rainy and other factors using a small yield valuable data for scientific social scientific and portable wet lab they dilute the samples to standard ph humanistic research levels the dilution varies by the expected concentration of the national science board four categories of data bacteria a judgment that requires scientific expertise spe are useful as a general framework but tend to obscure the cific to this type of research diversity of types of data that may be collected in any once samples are collected diluted and brought to given scholarly endeavor the physical and life sciences the campus lab one of two processing methods is applied which are the focus of the long lived data collections the simpler and least expensive method is to culture the report exemplify all four categories the social sciences samples for hours then to count the bacteria this journal of the american society for information science and technology june doi asi o b e r v a t o r y e x p l o r a t o r y describe phenomena model system sd scenarios bq beach quality sd star dust os online survey ar archival records specificity of purpose os ar bq t h e o re tic a l e m p iric a l goal of research scope of data collection fig purposes for collecting data method is slow and too insensitive to distinguish between whole entity or system such as the earth or sky global human and animal sources of bacteria the more sophisti climate modeling for example depends upon consistent cated method is quantitative polymerase chain reaction data collection of climate phenomena around the world at qpcr adapted from medical applications which requires agreed upon times locations and variables edwards greater expertise and is much more expensive this method is faster and more sensitive but results will vary between the value of observatories lies in systematically cap laboratories due to choices of local protocols filter material turing the same set of observations over long periods of machine type and model and handling methods protocols time astronomical observatories are massive invest and results are shared between partner laboratories seeking ments intended to serve a large community investigators to perfect the method but little other than the methods and others can mine the data to ask their own questions or to of data collection protocols and final curves might be identify bases for comparison with data from other sources reported in the journal articles biological samples are studies of the role of dust emission in star formation make fragile they degrade quickly or are destroyed in the analysis use of observatory data in this star dust scenario a team of process astrophysics researchers queries several data collections that at the other end of the specificity dimension are obser hold observations at different wavelengths extracting many vatories which are institutions for the observation and inter years of observations taken in a specific star forming region pretation of natural phenomena examples include neon of interest they apply several new methods of data analysis and lter in ecology national ecological observatory to model physical processes in star formation by combining network u s long term ecological research data from multiple observatories they produce empirical network porter geon in the earth sciences results that enable them to propose a new theory typically geon ribes bowker and synoptic sky the combined dataset is released when they publish the surveys in astronomy panoramic survey telescope journal article describing their results rapid response system large synoptic sky telescope sloan digital sky survey obser scope of data collection the second dimension of vatories attempt to provide a comprehensive view of some figure is the scope of data collection at one pole are journal of the american society for information science and technology june doi asi in studies that describe particular events or phenomena at the similar environments some of these observations could other are studies that model entire systems the beach be aggregated into models of phenomena while others quality scenario above is descriptive in nature while the star could not the beach quality scenario reflects the local dust scenario is closer to the model end of the dimension as characteristics of much data collection no matter how well the goal of their research is to model physical processes they document their field practices gathering an identical climate research spans this spectrum weather data in the set of water samples is impossible due to changing envi short term can be used to describe or predict rain snow ronmental conditions wind or other events systematic climate observations are used as inputs to models of physical processes to study the goal of research the third dimension in figure is the earth climate the same observations may be input to goal of the research ranging from empirical to theoretical multiple models each developed by different research most research in the sciences and social sciences relies on teams and each with its own set of parameters and theories empirical data of some sort scholars in the humanities edwards often collect data such as assembling records and notes survey research spans these dimensions as individual from archives theoreticians who may or may not be the studies may be more or less specific and may vary consid same people as those who collect empirical data draw erably in scope in the online survey scenario a single inves upon various forms of evidence to propose and to test tigator constructs a survey of student attitudes that theories is deployed to several hundred universities large scale in empirical investigations some variables are con surveys are suitable for hypothesis testing and description of trolled and others are tested to study beach quality for populations interviews are better for exploratory studies example researchers can control variables by diluting and also can be used to develop theories conversely samples to standard ph values and by following consistent because large online surveys are highly structured and protocols they can compare samples from different sites usually anonymized the resulting data are easier to share under different conditions their experiments can be repli interviews are more open ended personalized harder to cated by gathering new samples but they cannot reproduce anonymize and difficult to code in consistent formats results as any given sample can be analyzed only once among the great promises of data sharing is the ability those studying the atmosphere or the universe have to aggregate and compare multiple local studies however little control over their variables however they can con such integration of data is not always possible or desirable duct experiments on theoretical models in theoretical many research domains are concerned with rich descrip modeling whether climate astronomy or the economy tions of complex phenomena that are associated with spe data may be simulated rather than collected from the real cific times and places marine biologists for example world observations of the physical universe occur at a study local phenomena such as harmful algal blooms they unique place and time and can never be reconstructed may collect data for months or even years to capture con whereas experiments and models can be recreated ditions before during and after an event their goal is to national research council important differences understand the processes that trigger an event and how in terminology arise between experimentalists and theo those processes evolve borgman wallis mayernik rists however observations of the real world are data to pepe gobler boneillo debenham caron experimentalists while theoreticians often consider the small studies may cumulate into larger endeavors in that simulated observations that are output from models to be case the data from each individual study may become their data edwards wynholds et al more valuable as the data cumulate enabling comparisons across time periods and locations in other cases small studies may be one off investigations of individual phe approaches to handling data nomena at a particular time and place borgman wallis data collection processing management and interpreta enyedy bowker karasti et al karasti tion can be approached in many ways individuals and com baker millerand munities apply many combinations of techniques to identify it has proven difficult to aggregate studies of biological capture describe analyze derive and make sense of their events into comprehensive systems models of the type used data as with the dimensions of purposes outlined above in climate research due largely to differences in data char this selection of approaches is intended to be illustrative acteristics aronova baker oreskes the eco rather than exhaustive or mutually exclusive logical sciences community is promoting data sharing by they also range from the more local and flexible at the first creating standards for data documentation and by standard pole to the more global and systematized on the second pole izing collection practices as their research tends to focus of each dimension on local phenomena moore mcpeek rausher rieseberg whitlock whitlock data in most of the people involved the first dimension of approaches to data examples above would be considered observations per the collection is the number of people involved individuals nsb categorization some might be considered natural working alone have complete control over their methods and experiments such as comparisons of phenomena that occur their data teams who may be widely distributed have to journal of the american society for information science and technology june doi asi sd c o l l a b o r a t i v e t e a m bq i n d i v i d u a l i n v e t i g a t o r ar scenarios bq beach quality sd star dust os online survey ar archival records people involved os b y m a c h in e by hand by machine b y h a n d labor to process data labor to collect data fig approaches to handling data agree upon what data will be collected by what techniques months similarly the historian may spend months or years and instruments and who has the rights and responsibilities in historical archives taking notes on a laptop or only with to analyze publish and release those data borgman pencil on paper by the rules of some archives in this archi bowker finholt wallis david olson val records scenario the scholar may devote months or years zimmerman bos ribes finholt the to extracting useful data from those notes these labor historian works alone with archival records the sociologist intensive approaches have the advantage of flexibility and conducting the online survey of student attitudes may work local control by the investigators they have the disadvan alone or with a small team of students and statisticians the tages from a data sharing perspective of being difficult to beach quality research is conducted by to graduate and replicate and of producing data that are not consistent in undergraduate students and led by a single investigator in form or structure contrast dozens if not hundreds or even thousands of machine collected observations whether by telescopes people around the world may be involved in collecting and sensor networks online survey software or social network curating data in observatories those who draw upon the logs may be labor intensive to design and develop but once data from those observatories may be individuals or teams of deployed can produce massive amounts of data that can be any size used by many people major telescopes both on land and in space for example require long term collaborations among labor to collect data approaches to data collection also scientists and technologists data structures management differ in the amount of human labor required the second and curation plans are developed in parallel with the design dimension in figure investigators in beach quality marine of studies and instruments a process of a decade or more biology or other field research may spend days weeks or machine collected data tend to be consistent and structured months hand gathering physical samples of soil water or and to scale well but considerable expertise is required to plants which then must be processed in a laboratory to interpret them conversely these forms of data collection extract data a process that also may require days weeks or are less flexible and adaptable to an individual investigator journal of the american society for information science and technology june doi asi of research questions observation parameters may be hard great research value as well as spurious observations and wired or coded into the technologies thus determining the interpretations that can undermine research efforts citizen data that can be obtained science projects often devote considerable effort to data standards for data structures metadata and ontologies verification and validation cornell lab of ornithology depend on consistent data collection the european union galaxy zoo for example is developing project emodnet to assemble generally speaking the more handcrafted the data col fragmented and inaccessible marine data into interoperable lection and the more labor intensive the postprocessing for continuous and publicly available data streams for interpretation the less likely that researchers will share their maritime operations and research european marine obser data however data types and practices vary so widely vation and data network n p emodnet will pro across fields and research teams that any such generaliza mulgate data standards and infrastructure in multiple marine tions are difficult to make hilgartner brandt rauf and maritime observatory systems seadatanet pritchard carver anand another consideration is whose labor is involved in most cases investigators and their students and staff collect their own data in some forms of citizen science data are why share research data collected by volunteers with varying degrees of training or expertise whether in bird spotting ebird or identi as is evident from the above discussion of the purposes fying invasive species what invasive and approaches to handling data investigators and their collaborators students and staff devote massive amounts labor to process data a third dimension of approaches to of physical and intellectual labor to collecting managing data collection is the amount and type of processing required and analyzing their data and to publishing their results data for interpretation at one pole of this dimension in figure are the lifeblood of research in any field but just what are are hand processing techniques these techniques may be those data varies by purpose approach instrumentation simple such as measuring the dimensions of leaves assign community and many other local and global considerations ing geospatial and temporal parameters e g precise loca some of those data may be in sharable forms others not tion and time where gathered and experimental parameters some data are of recognized value to the community others e g amount of sunlight and shade proximity to the ground not some researchers wish to share all of their data all of the or water in other cases of collecting physical samples time some wish never to share any of their data and most the actual data may be instrument readings e g a type of are willing to share some of their data some of the time nitrate as indicated by a voltage measurement on a sensor or these competing perspectives the array of data types and concentration of a bacterium in parts per million of water origins and the variety of local circumstances all contribute whether the numbers are handwritten in a field notebook or to the intricacy and difficulty of sharing data machine generated they must be associated with a specific the pressure to share data comes from many quarters sample other information such as the type of machine its funding agencies both public and private policy bodies calibration the time date and place of data collection and such as national academies and research councils journal the method by which the sample was captured are necessary publishers educators the public at large and from research to interpret any given data point borgman wallis ers themselves these stakeholders each have their own enyedy similarly the historian in an archive is reasons for requiring or encouraging data sharing in exam documenting characteristics of the records examined so that ining the public statements of these entities some identify those notes can be interpreted later explicit benefits of data sharing to specific parties while in the most highly instrumented research such as astro others are vague about why data should be shared and whose nomical sky surveys instruments capture contextual infor interests will be served rationales arguments motivations mation about the data although minimal human labor may incentives and benefits often are conflated for the data be required for processing the data considerable expertise is sharing conundrum to be addressed effectively these oft required to assess the accuracy of data and metadata in these subtle distinctions need to be brought to the fore research environments as minute errors in calibration can the model presented below is framed in terms of ration influence analysis and interpretation significantly mayernik ales for sharing data a rationale is an explanation of the et al wynholds et al statistical analysis can controlling principles of opinion belief or practice an be applied both to online surveys and to the star dust data argument in contrast is intended to persuade it is the set of drawn from observatories human processing labor may be reasons given for an individual or an agency to take action minimal but the domain expertise required for analysis is underlying these rationales are motivations and incentives high of course the beach quality studies fall in the middle whether stated explicitly or left implicit a motivation is of the dimension as samples gathered by hand may be something that causes someone to act whereas an incentive processed by sophisticated technologies such as qpcr to is an external influence that incites someone to act ration yield numerical results ales for sharing data also include beneficiaries whether among the tradeoffs in citizen science is the amount of stated or implicit a beneficiary in this case is an individual expert postprocessing required the public can generate data agency community or other stakeholder who receives a journal of the american society for information science and technology june doi asi neither arguments for sharing dimension is absolute the poles represent relative positions of people or situations for example a n researcher or policy maker may make one argument on behalf of the producers of data and another on behalf of the users similarly an argument made in the name of scholar g n i ship may also serve the public good these arguments and beneficiaries are not mutually exclusive rather they provide a two dimensional space in which to place the various rationales in favor of sharing research data subtle distinctions in the rationales for data sharing may lead to markedly different policies economic models rationales b reproduce verify serve public interest n e e v v i i r r d d h h c c c r r a a e e ss e e r r r a h s f o e i r a i c i data producers data data users users f e n e research practices curation practices and degrees of compliance of particular concern is how those rationales ask new questions advance research align with the incentives of those whose work produces the data accordingly discussion of the four rationales focuses most heavily on the concerns of data producers and on their abilities motivations and incentives to share their data the model proposed here is intended to provoke discus sion among the many stakeholders in research data most of the examples are drawn from the sciences and social sciences as these are the areas most studied and are on the front lines of current policy debates this analysis can be extrapolated to the humanities where similar policies for data sharing are under discussion kansa kansa burton stankowski unsworth et al the ability to implement any data sharing policy will depend on many factors including local data practices differences in the intellectual property rights intrinsic to data sources and the need to maintain confidentiality of human subjects borgman to reproduce or to verify research reproducibility or replication of research is viewed as the gold standard for science jasny chin chong vignieri yet it is the most problematic rationale for sharing research data this rationale is fundamentally research driven but can also be viewed as serving the public good reproducing a study confirms the science and in doing so confirms that public monies were well spent however the argument can be applied only to certain kinds of data and types of research and rests upon several ques tionable assumptions pressure is mounting to share data for the purposes of reproducing research findings a recent special issue of science on replication and reproducibility examines the approaches benefits and challenges across multiple fields ioannidis khoury jasny et al peng ryan santer wigley taylor tomasello call the authors encourage data sharing to increase the likelihood of replication while acknowledging the very different methods and standards for reproducibility in each field discussed particularly challenging are the omics fields e g genomics transcriptomics proteom ics metabolomics in which clinically meaningful dis coveries are hidden within millions of analyses ioannidis khoury p fine distinctions are made between reproducibility validation utility replication and n e v i r d c i l b u p fig rationales for sharing research data benefit from the act of sharing data such as the use of those data for a particular purpose merriam webster collegiate dictionary four rationales are presented in figure positioned on two axes the sources for the model are the policy docu ments and studies of data sharing cited herein and the author participation in public discourse on these issues the four rationales are to a reproduce or verify research b make results of publicly funded research available to the public c enable others to ask new questions of extant data and d advance the state of research and innovation the dimensions on which these rationales are positioned are arguments for sharing and beneficiaries of sharing the model is not exhaustive either in terms of rationales or dimensions but is offered as a useful framework for exam ining the complex interactions of players policies and prac tices involved in sharing research data the arguments dimension vertical axis positions the rationales by their emphasis on the needs of the research community or the needs of the public at large researchers funding agencies and journals often make different argu ments for the value of sharing data motivations of the many stakeholders may be aligned but often they are in conflict the beneficiaries dimension horizontal axis positions the rationales by their emphasis on benefits to researchers who produce the data or benefits to those who might use research data here also motivations of stakeholders may be aligned but often they are in conflict funding agencies are responsible to their research communities and to the public journals must serve their readers their authors and their publishers researchers incentives to release their own data may or may not align with their motivations to gain access to the data of others similarly funding agencies and journals motivations for data release may conflict with the incentives of the researchers who create those data journal of the american society for information science and technology june doi asi other repeatability each of which has distinct meaning in indi researchers may be able to replicate the results if vidual omics fields identical circumstances can be achieved however experi the wall street journal in a front page article review enced researchers know that minute differences in proce ing this special issue of science oversimplified the concern dures machine calibrations temperature and humidity near by stating that reproducibility is the foundation of all the instruments and other factors can introduce undetected modern research the standard by which scientific claims are variation that influences replicability and interpretation evaluated naik naik article provides multiple even the research activities are difficult to replicate examples of biomedical companies spending tens or hun because processing and analysis tools such as statistical dreds of millions of dollars to reproduce research reported in mathematical and workflow software rarely maintain a journal articles often without success in this view lack of precise record of the systems or the data at each transaction reproducibility is a flaw in science often traceable to prob claerbout goble de roure questions of lems such as insufficient control groups or to the difficulty of the provenance in both the archival sense of chain of publishing negative results however by the research custody and the computing sense of transformations from in the sociology of science was describing the difficulties original state arise in interpreting data that may have of verifying scientific results harry collins studied the role evolved through multiple hands and processes buneman of replication in multiple scientific disputes providing khanna tan gil hunter cheung numerous examples of disagreements about validation other impediments to reproducibility include difficulties methods collins in extensive studies of gravita in gaining access to data and to tools used to create and tional waves in physics for example he found that some analyze those data lack of a licensing regime to provide scientists believed that only the experiments that detected access to proprietary software and to data stodden these waves were performed appropriately whereas other and conflicts in copyright law such as differences between scientists trusted only the experiments that failed to detect the united states and europe in the ability to make propri such waves collins etary claims on factual matters reichman uhlir peer review rests on expert judgment rather than on reproducibility is elusive because of the vagaries of the reproducibility reviewers or referees are expected to assess research process the varying notions of data disparate com the reliability and validity of a research report based on munity practices for documenting evidence the transitory the information provided in only a few fields do reviewers nature of observations and the combinations of expertise attempt to reanalyze or verify data or to reconstruct all the necessary for interpretation of evidence as a rationale for steps in a mathematical proof or other procedure even when sharing research data reproducibility is problematic not data are included with a journal article or conference paper only because it applies to so few types of research but rarely is enough information provided to reproduce the because it risks reducing the research process to a set of results instrument details and calibration may be omitted or mechanistic procedures even chemists will acknowledge lab specific practices may not be documented in sufficient that their work is as much art as it is science lagoze detail this is normal practice both because journal space velden true reproducibility requires deep constraints discourage elaborate methods sections and engagement with the epistemological questions of a given because research expertise relies upon tacit knowledge research specialty and the very different ways in which that is not easily documented bowker collins investigators obtain and value evidence reusers of data may kanfer et al latour latour woolgar not know or be able to know what prior actors did to the yet whenever published articles are withdrawn data each step in cleaning or processing data requires judg from major journals questions are raised about what the ments few of which may be fully documented later inter reviewers knew or should have known about the data pretations thus may depend upon multilevel inferences that and procedures brumfiel couzin unger are statistically problematic meng couzin frankel normile vogel couzin although reproducibility is a popular term in promoting reproducibility is a high bar and even it has several levels data release the concept incorporates a wide array of inter such as the precise duplication of observations or experi pretations including replication repeatability validation ments exact replication of a software workflow degree of and verification while a less precise notion verification effort necessary and whether proprietary tools are required may be easier to accomplish peer reviewers and readers can reproducible research stodden judge whether the research methods meet community stan vandewalle kovacevic vetterli observations of dards whether the evidence is appropriate for the claim and the real world such as water samples algae air and soil whether the arguments are reasonable even if they are temperature or comets are associated with times and unable to reconstruct all of the procedures potential reusers places and thus rarely can they be reproduced it is for this of data also can apply their own judgments about the verac reason that observations are worth curating experimental ity and usefulness of the data their criteria may be highly observations may be reproduced in laboratory conditions specific to their research topic faniel jacobsen given sufficient documentation and access to the same mate wynholds et al rials equipment software and technical expertise given the motivations to share research data for the purposes of same set of observations whether natural or experimental reproducibility or verification vary widely by type and journal of the american society for information science and technology june doi asi to condition of the data and expectations of the community serve the public good in this view data produced with where data deposit is required as a condition of publication public funds should be available for use and should not be as in the case of genomes and the protein data bank dis hoarded by researchers the public good argument is cussed earlier researchers will comply if reproducibility implicit in the oecd principles to which several of the requires materials that are very difficult to share such as funding agency policies refer namely that open access to specialized animals or cell lines then community practices research data is a means to leverage public investment in will dictate the conditions of data release considerable research organisation for economic co operation and human expertise labor and licensing of intellectual pro development the oecd document also builds upon perty may be involved researchers give these reasons an earlier u s study explicitly quoting this passage the and many others for refusing to release data campbell value of data lies in their use full and open access to et al hilgartner hilgartner scientific data should be adopted as the international norm brandt rauf if materials and documentation are for the exchange of scientific data derived from publicly highly automated if no licensing restrictions apply and if funded research national research council the researcher has completed his or her publication of the u s public policy tends toward openness of research results then data sharing is more likely to occur information federal law waives copyright protection on however researchers may never be done with their data and information directly produced by government data in cases where a research career is based on long term agencies putting those materials into the public domain in study of a specific species locale or set of artifacts data the united states reichman uhlir data and infor become more valuable as they cumulate researchers in mation resulting from research grants to universities and these situations may be particularly reluctant to release other agencies do not fall under the same law data and data associated with a specific publication as it might mean information produced by european governments generally releasing many years of data similarly reproducing the data do not fall into the public domain and some of their data associated with any given publication is problematic as the base laws are more restrictive than those of the united states set of observations reported may depend heavily on prior boyle boyle jenkins however initiatives studies and on interpretation of much earlier data such as emodnet and inspire in the environmental scholars may wish to verify findings either to build upon sciences are building infrastructure to combine and mine or to refute them the generalized rationale of sharing data for data throughout the european union inspire reproducibility or for verification of results lies near the european marine observation and data network research driven pole of the argument dimension in figure the rationale of making publicly funded research avail it spans the interests of data producers who can benefit by able to the public applies both to data and to publications having their findings verified by others to reinforce their but is playing out differently between the two the public veracity and the interests of researchers who would use monies for public goods rationale has succeeded in the others data the greater possibility of replication or verifi biomedical research community for the open deposit of pub cation occurs with data produced for the purposes of obser lications but not without resistance especially on the part of vatories modeling systems or for theory building the far end publishers biomedical information has a substantial audi of the dimensions in figure or those collected by large ence including biomedical researchers clinicians the phar teams by technologies and subject to machine processing maceutical industry and patients so it is not surprising that the far poles of figure these categories of data are more this was the first frontier for open access publications likely to be captured and described consistently than are data resulting from nih funding must be deposited into pubmed collected for exploratory investigations to describe phenom central within months of publication an embargo period ena for empirical studies by individual investigators or that protects the journals national institutes of health processed by hand innovation in research requires new however with regard to data nih requires the methods of research design analysis and technology release only for grants over a certain size and allows these from an epistemological perspective reproducibility and data to be embargoed for a certain period of time to enable verification are the most problematic of the four arguments the investigators to publish their findings the nih does not for sharing data often the research creativity lies in identi require that the data be deposited in any particular resource fying a new method required to approach an old problem only that they be released research outcomes often depend much more on interpreta the wellcome trust the largest funder of biomedical tion than on the data per se separating data from context is research in the united kingdom requires both publications a risky matter that must be balanced carefully against and data from their grants to be made available they demands for reproducibility support multiple types of open access publication but do not follow the nih model of requiring deposit in a specific to make results of publicly funded research available to the public repository wellcome trust fazackerley the nsf policy is ambiguous with respect to what must be released publications are defined as a type of data within public sentiment for sharing research data is based the scope of the data management plan but nsf does not largely on the rationale that tax monies should be leveraged specifically require that publications resulting from their journal of the american society for information science and technology june doi asi for grants be released openly national science foundation the misinterpretation and misuse of data are common in light of growing pressure for open access to reasons that researchers give for not sharing campbell et al publications the situation may change directory of open hilgartner hilgartner brandt rauf access journals open content alliance researchers are more willing to share their data with beaudouin lafon crow kaiser ware those in their immediate area of specialty than with young one result of the popularity of the nih the general public those within their community of publication deposit requirements is a bill introduced into interest to use the nsf term have the expertise to inter congress to require federal research granting agencies to pret the data and thus are most likely to benefit from access make resulting publications available to the public federal making data available to the users beyond one specialty research public access act of in late the u s requires much more documentation effort researchers are government solicited public input on policies for access to concerned about misuse such as selective extraction of data research data office of the federal register points or misinterpretation whether because of lack of the release of data and publications are coupled both as expertise lack of documentation or other factors data from an economic argument and as support for reproducibility observatories where resources for documentation and cura verification and reuse publications are the primary form of tion usually are included in the research design and funding documentation for most types of data they explain the are more readily released to the public observatory data are research problem addressed the methods by which the data only a small subset of extant data resources and they can were collected the analyses performed and the interpreta be sensitive global and comparative research on climate tion of the results publications add value to data and vice change depends on open access to data overpeck meehl versa borgman bourne pepe mayernik bony easterling santer et al yet the politi borgman van de sompel cization of climate data costello maslin montgomery the economic and social research council which is the johnson ekins gleick makes researchers in primary u k funding agency for these areas recently issued these and in other fields wary of releasing their data a data policy that goes well beyond the scope of the nsf nih and wellcome trust requirements the esrc requires not only the public release of data but also that investigators to enable others to ask new questions of extant data who wish to create new data must demonstrate that no suitable data are available for re use economic and social research council p for grants to create new data esrc requires a data management and sharing plan grantees also must prepare their data for re use and or archiving with an esrc data provider within three months of the end of the award if the grantees have not offered the data to an appropriate provider in that time period then the agency may withhold the final payment on the award the esrc funds repositories that can curate data from their grants notably the esrc also acknowledges that the repositories can enforce selection policies for the data they collect lest these repositories become a catch all for any one data regardless of quality or potential for future reuse the public monies for public good argument resonates with legislators taxpayers and the general public it also resonates with researchers whose data are readily reusable by those without substantial domain knowledge such as types of astronomical or earth observations that can support citizen science another public interest driven argument is that data release will minimize duplication of research effort which in turn results in fewer human subjects being required to estab a more focused rationale is that sharing data enables others to ask new questions whether from an individual dataset or by combining multiple sources this framing has two strands one for the benefit of researchers and one for the general public researchers have argued that open access to data encourages meta analysis the ability to combine data from multiple sources times and places to ask new ques tions whitlock in this view access to data is less about inspecting the findings of an individual project and more about the ability to combine data indeed the greatest advantages of data sharing may be in the combination of data from multiple sources compared or mashed up in innovative ways butler data are most reliably integrated when collected and processed systematically in ways that support the standards of large communities common data structures metadata formats and ontologies help support mining and integration of multiple data sources the public good strand of the ask new questions ratio nale was framed most visibly by chris anderson editor in chief of wired magazine in a special issue on data anderson lish findings fischer zigmond in most research projects data are collected by individuals or by small teams methods are local and are specific to the research questions at hand reusing these types of data the new availability of huge amounts of data along with the statistical tools to crunch these numbers offers a whole new way of understanding the world correlation supersedes causa tion and science can advance even without coherent models requires considerable knowledge of the procedures by which unified theories or really any mechanistic explanation at all they were collected which in turn requires considerable expertise in the research specialty the farther removed from anderson captures the public excitement about the the data collection activity the harder it is to make use of promise of big data to explore new questions and someone else data thus it is not surprising that concerns to combine data from multiple sources to identify new journal of the american society for information science and technology june doi asi from relationships although a popular phrase big data is a pro observatories and synoptic surveys such as astronomy blematic term that obscures the complexity quality and and social science survey research and fields in which com expertise required for analysis and interpretation ethical parisons across time and space are beneficial such as some questions also arise about the use of public data for purposes areas of biology and ecology when data are shared quickly other than those for which they were intended boyd and openly researchers can draw upon each other data crawford more readily for example some space based telescope those in the scientific and technical computing commu missions alert other astronomy projects when something of nities who argue for data sharing are well aware of the interest is spotted enabling other investigators to turn their difficulties involved in reusing others data assessing the instruments toward the specified coordinates thus one veracity and integrity of a given dataset requires domain instrument might identify an object or event and an unre expertise and that assessment depends upon the extent of the lated project might obtain follow up observations within documentation available faniel jacobsen the seconds drake et al farther the user is from the point of data origin the more fischer and zigmond writing in science and documentation that is required the more effort required on engineering ethics identify a number of ways in which data the part of the reuser and the greater the risk of misinterpre sharing advances the state of science these include maxi tation scientists compute upon large datasets both for explor mizing the use of data increasing the impact of findings atory investigations and to generate new theory which is progressing the state of research faster and farther laying a quite the opposite of anderson conclusion that big data broader foundation for knowledge expanding the scope of means the end of theory as edwards rogers research and diversifying perspectives and many others have explained data and theory are data from publicly funded research are more likely to be inseparable investigations are designed to test or to develop shared than are data resulting from privately funded theories and those theories are used to make sense of the data research especially in cases where the research is propri scholarship entails fitting data and theory etary academic researchers in fields where data have high this third rationale to enable others to ask new ques monetary value and much of the research is proprietary such tions of extant data benefits prospective users more than as chemistry and the biosciences are at a disadvantage in producers of data the data mashup strand of this argu terms of access to data data sharing does occur within ment falls in the upper right quadrant of figure as the academe and between academe and industry but to a lesser intended users are a peer community of researchers degree than in fields where most research relies on public whereas data mining by anyone for any reason falls in the funds haeussler lagoze velden lower right quadrant primarily benefiting the general establishing open data and metadata standards for chemistry public most researchers will share more readily with their has been highly contentious in comparison to other scientific peers given the concerns for labor interpretation and like fields murray rust rzepa open data structures lihood of reuse facilitate the massing of large amounts of data that can be mined to ask new questions in the biosciences the likeli to advance the state of research and innovation hood of sharing decreases with the competitive value of the requested information for both academic and industrial the rationale for sharing data that resonates with the researchers haeussler p the humanities and widest array of stakeholders is that research and innova social sciences have similar problems when their data tion can be advanced more effectively this is the claimed sources are copyrighted materials owned by others scholars fourth paradigm that computational science constitutes a may be able to quote small portions of texts but not repro new set of methods beyond empiricism theory and simula duce still or moving images or mine digital resources in tion bell hey szalay gray et al hey et al depth the fourth paradigm claim is appealing but tends the argument that data sharing can advance scholarship to overreach wilbanks strikes a middle ground goes beyond data release once made available data can be viewing data not as a method per se but as a rich resource curated in ways that add value for the research community for any of the empirical theoretical simulation or compu the notion that data curation is a means to advance science tational paradigms is the cornerstone for the data conservancy one of one distinction between the ask new questions and the consortia funded by the nsf datanet program national advance research rationales is that the latter is wholly science foundation n p the data conservancy motivated by research interests it also goes beyond asking dc embraces a shared vision scientific data curation is a new questions of extant data it addresses the need for means to collect organize validate and preserve data so that more data and for curation of existing data in ways that scientists can find new ways to address the grand research ensure their usefulness simply put science depends on challenges that face society good data whitlock et al p and data are the the oecd principles have a similar tone sharing and main asset of economic and social research economic and open access to publicly funded research data not only helps social research council p assertions such as to maximise the research potential of new digital technolo these are most common in data intensive fields that benefit gies and networks but provides greater returns from the journal of the american society for information science and technology june doi asi necessary public investment in research organisation for economic to reuse data similarly problematic is the co operation and development p requirement to curate data in such ways that they are inde advancing scholarship is a rationale that spans the inter pendently understandable consultative committee for ests of data producers and users and thus is positioned in space data systems although a laudable goal it is both upper quadrants of figure if data can be aggregated rarely feasible in any absolute sense any more than is into a critical mass and curated in ways that make them more reproducibility accessible and valuable then those who produce the data can disincentives to sharing research data include lack of exploit them better as can other data users researchers are reward or credit for sharing the substantial amount of labor more likely to share their data if they know the data will be required to document data in reusable forms concerns for well managed for future use data collected systematically misuse or misinterpretation of data control over intellectual to community standards for structure and content are most property and the need to restrict access or to de identify data easily curated and compared and researchers are most on human subjects or endangered species perhaps the most willing to share them many other types of valuable data do significant challenge to data sharing is the lack of demon not meet these criteria such as the beach quality and harmful strated demand for research data outside of genomics algal blooms examples presented earlier although sharing climate science astronomy social science surveys and a these data also may advance scholarship the researchers who few other areas most funding agencies and review panels produce those data are more likely to exchange them within evaluate grant proposals on the basis of the new data to be their immediate areas of specialty than to release them to the created in support of a research endeavor few promote general public innovation through reuse of data the esrc of the united kingdom is a notable exception hiring tenure promotion discussion and conclusions and research assessment panels rarely consider data citation in their evaluations of research productivity until these for the last years the need to share research data has been declared to be an urgent problem yet the discussion continues policies proliferate and evidence of data sharing is apparent in only a few research fields sharing research many disincentives are addressed data sharing is unlikely to increase substantially four rationales for sharing research data are presented and are positioned in the model in figure data is clearly a conundrum an intricate and difficult problem acknowledging that data sharing is difficult does not mean abandoning all hope that some data will be shared with some people some of the time the challenges are to understand which data might be shared by whom with whom under what conditions why and to what effects to reproduce or to verify research to make the results of publicly funded research available to the public to enable others to ask new questions of extant data to advance the state of research and innovation answers to these questions will inform data policy and practice the complexity of the simple statement in the intro duction to this article should by now be apparent the first rationale is the strongest from a research per spective and yet the most problematic questions of repro ducibility are deeply intertwined with the epistemology of the research specialty the second and third rationales are if the rewards of the data deluge are to be reaped then research ers who produce those data must share them and do so in such the most driven by public interests and are argued from the perspective of those who wish to use data produced by other a way that the data are interpretable and reusable by others parties the fourth which also serves the public is framed in terms of benefits to data producers and serves research neither the producers of data nor the agencies that innovation and scholarship require sharing can agree on what are the data data take the analysis presented here focuses most directly on the many forms both physical and digital they are much more concerns of the researchers who produce data in an effort to than numbers in a spreadsheet data can be samples soft identify types of research in which data sharing is most ware field notes code books instrument calibrations archi appropriate and to identify policies and practices that may val records or a myriad of other information objects none encourage data sharing motivations to release data depend of which may stand alone sharing can encompass acts as to a large degree on the labor required which varies both by varied as announcing the existence of data posting them on the purposes for which data were collected figure and a website or contributing them to a richly curated reposi the approaches to handling data figure data collected tory interpretable and reusable are the most problematic for observatories or for model building require structure and terms interpretable presumes sufficient expertise to documentation which makes them suitable for wider assess the integrity of the data and to grasp their meaning it release instrumented data may contain automated markup also presumes adequate documentation of the context of the that facilitates release hand collected observations by data creation processing and provenance reusable is a individual investigators whether ecological field studies or standard just short of reproducibility considerable exper ethnographies may require the most labor to document tise effort restructuring and proprietary software may be in sharable forms journal of the american society for information science and technology june doi asi a data are more likely to be shared when the policies given community however narrowly or broadly defined benefit those who produce the data this is a simple state requires close engagement and study the social study of ment of self interest researchers collaborate but they also science dates to the mid century latour woolgar compete for grants for jobs for publication venues and for merton and the interest in practices students they must choose carefully where to spend their associated with data has accelerated in the last decade time and resources time and money spent on documenting borgman bowker edwards social data for use by others are resources not spent in data col science and humanities research practices have received far lection analysis equipment publication fees conference less attention more studies of these also are needed travel writing papers and proposals or other research borgman necessities although it can be argued that good data prac initiatives such as the nsf datanet program national tices benefit the originating researcher far less documenta science foundation endeavor to bring researchers tion is required to maintain data for one own use than to librarians archivists data scientists and systems developers release those data to the public data release is costly even together to understand community driven design for data if data sharing is built into the cost of research funding the curation multiple parallel studies of individual research requirement may substantially increase the cost of doing groups and communities are under way to inform both research data release is more effective if those data are policy and design our research on astronomers as part of curated in ways that make them useful to others over some the datanet program data conservancy reveals that long period of time data curation likewise is very expen community driven design means selecting and organizing sive and unlikely to be justifiable for all forms of data data to reflect specific practices wynholds wynholds issues of selection and appraisal to determine which data et al at one extreme very fine details of instrument are worth curating and for how long are urgent matters that design and calibration must be associated with data multi require much more attention similarly more needs to be dimensional temporal and spatial coordinates also may be known about potential uses and users of research data one essential at the other extreme researchers would like to be reason that researchers do not release data is because they able to explore massive repositories of data without having cannot imagine who might use them mayernik to know those fine details to paraphrase one of the astrono they lack a recursive public to use kelty term kelty mers we interviewed only about of all our data has p eyes on we rely on analytical tools to see the rest of it however these concerns beg the question of what are several have expressed concern over the design of current the data in any given investigation releasing the spread data repositories which may be optimized for database per sheets or statistical files associated with tables in a journal formance rather than for scientific inquiry similar insights article is a much different requirement than is releasing likely exist for any field whose data may be curated they physical samples hand written field notebooks or raw await study and partnership observations from complex instruments numerical data the reasons that researchers do not share data readily are are of little value without the software associated with the becoming better understood however even less is known data collection analysis and processing technologies that about why researchers do share data and why they reuse software may be proprietary or it may be crafted locally as data thus much more research is needed about practices part of the research project in either case the software in fields that do share data consistently and practices in necessary to interpret the data may not be sharable even if fields where data are consistently reused only with this the data were produced with common tools those data knowledge in hand coupled with a richer understanding may be unreadable after a few years because of changes in of the array of physical and digital objects that might be hardware and software unless they have been curated well considered data can better policies practices services and and migrated to new technologies yet more problematic is systems be developed to support the sharing of research the fact that many new forms of research data are not data datasets that exist in bounded forms that can be curated rather they are streams of observations flowing from sensor networks telescopes social networks public increasing scrutiny of the thomson reuters journal impact factor casts some doubt on its claim of a systematic objective means to critically evaluate the world leading journals with quantifiable statistical information based on citation data thomson reuters the continued inappropriate use of this indicator despite serious flaws invites comparison with phrenology the outdated pseudo science that attempted to infer human behaviour from measurements of skull morphology davies although popular in the early nineteenth century most scientists now recognise that such measurements offered an inaccurate record of morphology and an unreliable indicator of human behaviour unlike j k vanclay school of environmental science and management southern cross university p o box lismore nsw australia e mail jvanclay scu edu au j k vanclay fig exponential increase in documents found with a scopus search for journal impact factor showing all documents dotted line circles editorial comment solid line squares and critical documents dashed line triangles with the words bias limitation problem manipulate misuse or flaw in the abstract phrenology the impact factor garfield has demonstrated utility in informing citation patterns and guiding library purchasing decisions althouse et al cameron however there are increasing concerns that the impact factor is being used inappropriately and in ways not originally envisaged garfield adler et al these concerns are becoming a crescendo as the number of papers has increased exponentially fig reflecting the contradiction that editors celebrate any increase in their index whilst more thoughtful analyses lament the inadequacies of the impact factor and its failure to fully utilize the potential of modern computing and bibliometric sciences although fit for purpose in the mid twentieth century the impact factor has outlived its usefulness has it become like phrenology a pseudo science from a former time this paper attempts to draw together key conclusions from many extant papers to illustrate limitations with explicit examples and to offer recommendations to improve the indicator it is appropriate to begin by examining exactly how the impact factor is computed and how there may be several variants depending on how this computation is made thus in this paper i adopt the term garfield index to denote the generic concept underpinning the impact factor and the abbreviation trif to denote the specific journal impact factor published by thomson reuters in their journal citation reports jcr garfield impact factor eugene garfield proposed an impact factor based on the mean number citations to articles published in the two preceding years to avoid confusion with particular commercial implementations let call this general concept the garfield index and define it as the ratio of citations received divided by the potential number of cited documents before exploring the variants of this index it is appropriate to recognise that technological challenges shaped some compromises made in the garfield and to note that nothing in the trif challenges the more advanced computational impact factor fig variants of the numerator forming the garfield index capability available today there may be several variants of garfield index reflecting how both the numerator and the denominator of this ratio are derived in principle the numerator of this ratio is simply the number of citations in year y to articles published in years y and y but there are a number of ways to deal with ambiguous citations fig the simplest form of the garfield index relies on the many to many numerator g mm fig its simplicity arises from the ease of extracting from a database a count of all references in year y to journal j in year y or y it is fault tolerant because it includes all citations matching journal title and year even if there are other errors in the citation figure deliberately shows a cloud to signify the uncertainty associated with the cited article there may or may not be an article corresponding to the citation as no cited side checking is done to ensure the integrity of the citation the thomson reuters impact factor trif garfield is a particular implementation of the g mm index drawn from the web of science wos database and published in the jcr a more discerning form of the numerator g am takes into account the nature of the citing document and restricts the count of citations to scientific articles i e excluding editorial comment thus eliminating some spurious content e g by google scholar in indexing indices to volume by the author area index as well as mischievous editorials promulgating self citations designed to inflate the index e g hernan the preferable form is the more rigorous g which checks for the existence of the cited document and ensures the validity of the one to one link defined by the citation database professionals would use different terminology but it is useful to retain the g notation to emphasise that each citing paper must link explicitly with each cited paper in a one to one link to clarify g mm involves minimal checking of citations g am involves citing side checking and g involves both citing side and cited side checking this quality control implicit in the g variant solves many of the problems of the common g mm form of the numerator but creates new challenges if an author mis quotes a reference even with a minor typographic error the database cannot create a link should the incorrect reference remain pointing nowhere or should an attempt be made to correct the reference and make a link however it sheds transparency on an insidious problem with the conventional fault tolerant g mm form which does not compare like with like a compelling advantage of the g approach is that it enables an analyst to select the relevant material for both the numerator and denominator figure illustrates the nature of denominator forming the garfield index most citations are made by articles including reviews to earlier articles solid black line in fig some editorial material may cite articles items by the editor and letters to the editor commenting on earlier articles solid grey line and this is a way to manipulate the trif there may also be a few references to earlier editorials and letters dashed black and dotted grey lines but in many journals these constitute a small proportion of the total citations j k vanclay fig variants of the denominator forming the garfield index received however the relative proportions may be surprising mcveigh and mann report several journals in which so called citeable articles i e articles and reviews comprise only of the total content the importance of these distinctions is that while logic might suggest an impact factor should rely on scientific citations by articles and reviews black lines the popular trif counts all the lines in calculating the numerator i e g mm and assumed only the solid lines in deriving the denominator i e the potential number of articles and reviews that might receive citations but excluding editorial material thus the trif uses a ratio of all citations received years n and n divided by the number of articles potentially available to be cited black lines only year n while this was computationally convenient in the past archambault and lariviere and is harmless in many cases with few editorial citations it is potentially problematic when editors choose to manipulate their trif with wanton self citations within their own journal e g rieseberg and smith rieseberg et al conversely editorial improvements to journal layout can also have unintended consequences such as when in the lancet divided its letters section into correspondence and research letters the latter being peer reviewed and hence citable for the denominator the increase in the denominator led to a fall in if from about scully and lodge if the trif is to be taken seriously it should be revised so that it deals only with g citations i e both citing side and cited side checking by articles and reviews to articles and reviews i e in fig solid black line only a third aspect of citation analysis is self citation citations to articles may originate from within a journal or from other journals in general most citations originate from other journals but the proportion of self citation varies with discipline and journal generally self citation rates for most journals remain below mcveigh but may be somewhat higher for specialist and national journals fassoulaki et al for instance in australian forestry had a self citation rate of about table australian forestry was chosen deliberately for this analysis because it is a small journal familiar to the author both necessary criteria because with the present versions of wos and scopus the reliable compilation of table required manual inspection of each citation table also reveals a fourth aspect about the garfield index the values obtained for any journal will depend on the database because each provider utilises a different collection of source material it is evident in table that scopus scans the new zealand journal of forestry science while the wos does not in addition wos noticed citations unseen by scopus to australian forestry in methods in ecology and evolution in forest systems and in geoderma conversely scopus noticed citations unseen by wos in papers and proceedings of the royal society of tasmania and in new zealand journal of forestry a third provider google scholar found citations including of those represented in scopus plus additional citations in cab reviews perspectives in agriculture impact factor table origins of citations in to items published in australian forestry during source web of science scopus article letter total article letter total australian forestry n z j forestry science int j appl earth obs and geoinf forest ecology and management australian journal of entomology other journals faulty references total a these data from a cited reference search in wos jcr reports self cites amongst a total of citations but it is not possible to reproduce this from the wos veterinary science nutrition and natural resources and amongst grey literature not scanned by the commercial providers conference proceedings technical reports etc the trif is displayed to three decimals by convention apparently to create a unique ranking and to minimize the number of tied places garfield but this is a misleading practice consider an informative and familiar analogy the body mass of a group of people while it is possible to obtain scales that display to the nearest gram e g kg such detail is irrelevant because the value displayed may depend on the time of day time since last meal or bathroom visit the surface supporting the scales sample the mechanism and the manufacturer thus in most cases people deal with human mass to the nearest kilogram except in carefully controlled studies stein et al so it is with garfield index which may vary from day to day depending on the last batch of error corrections and depend on the database used within wos science citation index expanded social sciences citation index etc or alternative databases such as scopus and google scholar and the methodology g mm or g as numerator articles or all items as denominator this issue of variability is particularly problematic for journals publishing few articles in any year for instance in mediterranean politics received citations indicating an impact of and rank in citations indicated an impact of and rank the convention for the trif to display three decimal places is illogical and deceitful logic indicates not more than one decimal with many journals tied in equal places this paper illustrates that these limitations with the generic garfield index apply equally to the trif and argues on the basis of literature and new analyses that journal impact should be assessed in ways other than with the current trif literature there is a vast and rapidly increasing literature surrounding the trif fig much of it critical but the respect afforded the annual update of the jcr e g press releases beal reller and editorials ho reflects continuing standing of the trif particularly amongst publishers amongst the academics who contribute articles and reviews there is much more scepticism e g simons including some editors and j k vanclay publishers who have called for reform e g campbell patterson many concerns have been dealt with repeatedly in earlier reviews e g braun and glanzel gla nzel and moed bensman braun so the current analysis seeks to offer a brief overview and synthesis to establish a pathway forward an overview of some aspects is summarised in fig which displays author supplied keywords derived from a web of science search for impact factor and displayed using citespaceii chen one of several software tools offering text analysis see e g cobo et al figure reveals that one of issues often canvassed in conjunction with the journal impact factor is that of quality figure displays a network of co occurring phrases that were detected automatically and citespaceii can apply similar techniques to articles journals and authors to compile article co citation networks journal co citation networks or author co citation networks all of these can be useful to identify important and pivotal material that may not be as conspicuous in a simple list of citations jahangiriana et al other text analysis packages may offer complementary insights for instance fig is a perceptual map compiled from a scopus search for journal impact factor using catpac woelfel jo rgensen figure makes it easy to note the juxtaposition of nursing and peer review of quality and isi predecessor to thomson reuters and of normalize and journal impact factor reflecting some of the issues canvassed in the literature catpac woelfel jo rgensen a text analysis system based on an artificial neural network and thoughtview woelfel and woelfel allow the use more control to weed the image of redundant detail to focus attention on salient features fig while such images assist discovery and the formulation of hypotheses they are ill suited for hypothesis testing as the user has ample scope to influence the terms included in or omitted from the display fig citespaceii display of author supplied keywords amongst articles retrieved with a web of science search for impact factor quality features prominently in this automated synthesis impact factor fig perceptual map compiled with catpac thoughtview from material derived using a scopus search for journal impact factor these images figs offer an overview of the topic and illustrate the range of viewpoints expressed and vocabulary used in the literature and thus help to clarify the trends illustrated in fig closer examination reveals that many of the articles found with a simple search merely used e g the factors related to publication in journals with an impact factor of more than were analysed bonillo perales or reported the trif e g our first impact factor timuralp amongst articles attempting some analysis of the strengths and weaknesses of the trif critical views clearly formed the majority table care is needed in interpreting table because status quo bias porter and mcintyre may mean that those who find the trif useful don t bother to publish while a minority of vocal critics appear as a majority however despite this possibility table leads to the conclusion that there are serious limitations inherent in the trif and that there are compelling reasons to overhaul or replace this indicator which is often assumed to be a valid proxy for journal quality in summary table reveals considerable concern that the trif is not a reliable indicator of journal quality that it lacks rigour and requires normalizing before comparisons are attempted that its year timeframe is too short for meaningful trends to be established that it lacks statistical validity suffers database problems and leads to problematic if unintended consequences to put it bluntly the trif plays a particularly significant role in choosing a journal and yet it is a controversial and some would say flawed metric soreide and winter an outmoded surrogate for quality plos medicine editors an ill defined and manifestly unscientific number rossner et al with bias that originates mainly from misuse as well as abuse falagas and alexiou others are more critical concluding that the trif is seriously debased inescapable conclusion is therefore that the impact factor is worthless should be killed j k vanclay table summary of recent literature on the journal impact factor aspect weakness strength indication of journal quality unreliable correlation between trif and independent measures of evidence bain and myles and quality seglen woolgar seglen bath et al frank walter et al bernstam et al maier ogden and bartley glynn et al goldstein and maier schumm citation rates and hence trif can be influenced by factors other than scientific merit knothe calver and bradley perneger weak correlation between trif and article rejection rates kurmis and kurmis not at core of principal component analysis bollen et al gla nzel year to year variation in rank is about altmann and gorman lacks robustness against single outliers rousseau metze incomplete and inadequate measure coleman dodgy evaluation criteria lawrence seriously debased williams correlates with expert opinion drew and karpf saha et al yue et al and with level of evidence lau and samman obremskey et al sonderstrup andersen and sonderstrup andersen minelli et al consistent with overall citation count hansen and henriksen callaham et al chapman et al haslam and koval hunt et al and with use of articles wulff and nixon overall a reasonable measure of quality if used correctly schoonbaert and roelants lomnicki cartwright and mcghee gluud et al racki ruiz et al abramo et al trif reflects likelihood of having a formal misconduct policy resnik et al rigour of trif open to manipulation rousseau and van hooydonk garfield jones jones kurmis dong et al monastersky cheek et al chew et al smith yu and wang campbell falagas and alexiou mullen ogden and bartley reedijk and moed archambault and lariviere brumback dempsey hernan ramsden kapeller krell statzner and resh foo mehrad and goltaji yu et al different editorial policies influence trif moed et al scully and lodge foo about of journals have self citation exceeding ha et al and self citation is significantly correlated with trif straub and anderson kurmis and kurmis mehrad and goltaji self citation contributes fluctuation in trif leutner and wirth moller et al campanario trif lacks transparency van driel et al no proof of widespread manipulation andrade et al editorial citations contribute little to trif campanario et al raising quality of journal is best way to improve trif wang et al normalization trif differs across disciplines and warrants normalization ugolini et al coleman pudovkin and garfield sombatsompop et al sombatsompop and markpin rezaei ghaleh and azizi althouse et al wagner leydesdorff and opthof owlia et al impact factor table continued aspect weakness strength timeframe years too short van leeuwen et al year impact factors may follow a mcgarty sombatsompop et al similar campanario or smith variable length may be needed complementary pattern jacso to reflect disciplinary norms vanclay to the year trif distribution and statistical assumptions non normal distribution weale et al journals cannot be ranked with great precision greenwood no statistics to inform significance leydesdorff and opthof possible to estimate standard errors schubert and glanzel database issues database problems may be a major source of bias ha et al language bias may arise from the journals scanned kotiaho et al winkmann and schweim winkmann et al taylor et al schopfel and prost xiao et al poomkottayil et al from insular citing patterns jacobs and ip stiftel and mukhopadhyay or from errors arising from surname conventions meneghini et al kumar et al journals with similar titles may be incorrectly dealt in a single trif lange some of all citations contain errors todd et al awrey et al information available in the science citation index is a rather unreliable indication neuhaus et al unintended consequences threatens viability of specialist journals zetterstrom johnstone and disciplines brown may distort publication patterns away from prime audience postma kapeller intrinsic and deliberate errors are transferred to other decisions monastersky starbuck todd and ladle may shift editorial focus to increase trif and away from other aspects of quality ketcham off and the sooner the better williams scientists should be outraged that the worth of science is being measured by a secretive proprietary metric should renounce the thomson reuters impact factor brumback moed and plume sum marised more constructively noting that the trif is a first generation bibliometric indicator whilst the state of the art is now third generation it is time to progress to something better apart from the limitations of the trif a further problem is the behavioural change that it influences along with unintended consequences for instance lehmkuhl et al opined that the goal is to increase the impact factor but in reality the goal should be to increase quality and the trif should be a fortunate consequence of the better quality of articles however seglen argued that that there was no evidence of a correlation between journal quality and the trif of concern is the increasing trend to the use of the 220 j k vanclay table comparison of variants of the garfield index and of different databases journal variants of garfield index using web of science trif from jcr g am from alternative databases g mm g am g g no self scopus scholar rev mod phys 610 402 b nature a a 079 b am j bioethics 630 932 707 world j gastroenterology a a 406 b forestry 218 149 725 aust forestry 818 400 aust meteor oceanogr j 593 390 949 a wos limits output of cited references to records b google scholar limits output to records trif for assessing the performance of individuals favaloro because the trif is also unsuited for this purpose epstein it is apparent that many authors even those involved with citation analysis do not have an adequate understanding of these limitations e g these issues are rarely understood holsapple thus it is appropriate to examine more closely some of the weaknesses of the trif an evaluation of the thomson reuters impact factor trif there are several aspects of the garfield index that warrant closer attention for convenience these are grouped broadly as data errors system faults sampling deficiencies and statistical shortcomings collectively these cast doubt on the ability to reproduce trifs as published in jcr with data from wos or from comparable third party data providers table attempts to reproduce the trif as published in the jcr with wos data may differ two fold e g american journal of bioethics table depending on the nature of editorial material the values obtained from different forms of the garfield index may vary four fold even when compiled from the same database american journal of bioethics table values for the garfield index estimated from different databases may vary six fold australian meteorological and oceanographic journal table such observations cast doubt on the relevance of publishing trif with three decimal points this study was triggered through curiosity surrounding the discovery of an error in the handling of citations to a paper in forestry and an exploration of how that error might affect the trif that exploration revealed more errors which in turn led to an analysis of several journals spanning extremes high and low trif many and few citations etc all of which revealed errors in the wos data undoubtedly sampling these extremes increased the likelihood of revealing errors so this study should be seen as indicative rather than representative of all data within the wos but it does raise several matters of concern and aligns with calls for an independent evaluation of jcr estimates of impact rossner et al impact factor gross errors at the time of writing august wos contained two records of a paper in forestry by skovsgaard and vanclay both incorrect fig correct pagination is curiously the second more incomplete record included the correct digital object identifier doi and recognised citing documents most of which contained the correct details so fig reflects data entry errors by wos not typographic errors by citing authors these errors were drawn to the attention of wos during the preparation of this manuscript and these errors were quickly amended a comparable example is also evident with an earlier article in the journal science fig and many other examples of such errors can be found figure illustrates errors due largely to wos encoding but author errors may also contribute substantially to incorrect references simkin and roychowdhury the error rate amongst journal articles is worrisome enough but error rate escalates for citations with books and conference proceedings figure illustrates a diverse range of errors introduced into the title of a single book modelling forest growth and yield some of these errors are faithful reproductions of errors made by citing authors but many appear to be data entry errors introduced by thomson reuters or its predecessors figures and feature problems with citations to the author work but the issues illustrated are common and similar issues can be reproduced for many other authors the errors illustrated in figs and are minor typographic errors but other errors exist in wos for instance amongst the citations contributing to the trif for australian forestry table is an incorrect citation from european journal of pharmaceutical sciences in which a paper by m thommes and others was apparently confused with a different australian forestry paper by d thomas fig it is unclear how this error may have arisen but it appears that the wrong author was selected from a drop down list during data entry yet another error arises from the large number of test records that remain littered throughout the wos database figure illustrates one such case where reference is clearly a partial reconstruction of reference used for testing purposes but which was not removed at the end of testing there is potential for these remnants of testing to inflate the trif for the journal test issn but the trif appears unaffected thus it is useful to identify four kinds of problematic citation citations found in a citing document may be correct and complete in this case wos creates a viewrecord link e g figs partially correct but incomplete so that an unambiguous link cannot be made without further research typically minor typographic errors e g fig faulty incorrect but complete in such a way that a reference has been erroneously linked to the wrong cited document potentially there are two forms the difficult to fig illustration of some errors in wos records of forestry articles j k vanclay fig selected wos errors in the title of a book modelling forest growth and yield published in fig reference list in wos left and original right of van gyseghem detect situation where an author incorrectly attributes a fact to the wrong document and the example in fig where a problem in wos has crossed wires and wrongly linked a valid reference with a different source document and ghostly in the sense that a reference is made to a document not seen by wos either because the cited document is not in a journal scanned by wos or because the document does not exist incorrect and incomplete these four possibilities embrace the four combinations of in complete and in correct links it is difficult to establish which of these four possibilities the trif relies upon for instance table illustrates a futile attempt to reconstruct the trif for the journal forestry the jcr reveals that during citations were made to papers published in indicating a trif of however a search of wos using the cited reference search reveals citations including correct citations and problematic citations table suggesting an impact factor of depending on assumptions it is instructive to examine the source of errors reported in table seven of the incomplete citations arose because of a single digit typographic error in the citing paper with wos faithfully reproducing this error forestry is not unique in this regard fig illustrates a corresponding example from nature one of the incorrect citations arose through an author error tal and gordon referred to a ghost paper the influence of early planting on the sapling success rate and the development in forestry which actually appeared as influence of autumn planting on forest tree seedlings survival and growth in forest journal of forests woodlands environment not seen by wos two of the incorrect citations arise because of miscoding by wos medarevic s ˇ umarstvo et al cited a paper as forestry in serbian issn and was encoded by that was in reality wos as forestry impact factor fig artifacts from testing remaining in the wos database top correct and test variants of citation to work by aguila bottom only one correct record amongst first records in a search for citations to the journal test which returned citations table citations during to forestry articles published category correct incorrect total in complete incomplete total fig illustration of some errors in wos records of nature articles j k vanclay despite a complete mismatch of year and volume number for the journal forestry issn and preti et al cited a paper correctly as iforest biogeosci forestry that was encoded by wos as forestry again despite the mismatch between year and volume number clearly wos suffers from some substantial coding errors because of these errors and because of updates to the wos database since the jcr was released it is impossible to independently reproduce the trif for forestry or for any journal reported in table or to empirically establish whether any erroneous citations are omitted from the trif calculation typographic errors such as those illustrated in fig are common and may be inevitable given that good researchers are not always good typists but the frequency of such errors appears to vary considerably between journals a strange phenomenon since the cited journal has little control over the attention to detail in the citing journal except in the special case of self citation one journal with a remarkably low incidence of such errors is molecular ecology previously mentioned because of its self citation practices wos reveals remarkably few instances of such errors that might affect their trif one instance occurred in when the journal of systematics and evolution erroneously referred to molecular ecology instead of molecular ecology another occurred in a author self citation when zoologica scripta referred to molecular ecology online instead of to molecular ecology 5219 other search engines reveal author errors in citations to molecular ecology but since these are not seen by wos these errors do not affect the trif for instance robert mistakenly used in conjunction with molecular ecology an understandable mistake since the article was first published online in two papers currently online first in biological invasions incorrectly cite invasion genetics of the round goby tracing eurasian source populations to the new world instead of the correct invasion genetics of the eurasian round goby in north america tracing sources and spread patterns and three articles including an author self citation in molecular ecology use the title landscape genetic structure of tailed frogs in protected versus managed forests instead of the correct landscape genetic structure of coastal tailed frogs ascaphus truei in protected versus managed forests but in the latter two illustrations since year volume and page numbers are correct they would not affect the trif of molecular ecology there is a slender possibility that authors interested in molecular ecology are more fastidious regarding the detail of citations than the general population of scientists but this would not explain the absence of wos introduced errors this low error rate for citations to molecular ecology suggests that its editors work closely with wos to identify and correct any errors a good strategy for trif conscious editors in an ideal world an impact factor would rely only on complete and correct citations reinforcing quality control through the whole journal publication chain however it appears that the jcr merely tallies up for each journal all the citations made to papers published in irrespective of whether these are complete correct or ghost this offers an entirely new possibility to manipulate the trif as well as citing many recent articles e g rieseberg et al editors could cite ghost articles that could usefully increase a journal trif without distorting the performance indicators for real contributors it also raises the prospect of another measure of journal quality control could the frequency of citation errors in the citing journal be useful as a proxy for journal quality such a proxy would not only reflect on the rigour of editorial procedures in checking errors but also in after sales service of the collaboration between a journal and wos or other provider to correct subsequent incoming citations impact factor fig selected mis matched citations from wos referring to plos one in the year before publication commenced given the lax error checking by wos it is tempting to include a series of ghost articles in a review of this kind to demonstrate weaknesses of the trif but this has not been done in the present case many journals receive relatively few citations e g australian forestry received about citations in table and mediterranean politics received fewer than citations in so it can be a relatively easy matter to double the trif with a single review or editorial with a series of references to ghost papers that would not fool a genuine researcher e g possibly concerning eucoliptus sokal a non existent and deliberately misspelled plant species commemorating a hoax by alan sokal a series of such ploys by sceptical editors could completely undermine the trif in its current form and force action on thomson reuter part possibly suspension of the citing journal but hopefully a revision of the trif to the g basis system faults in addition to the ad hoc data entry errors identified above the wos and jcr suffer several system errors perhaps the most conspicuous of these is the handling of dates when journals change in some way e g new addition change of name re commenced after suspension for instance one of the more conspicuous errors arises with plos one plos one was first published in but wos records citations to plos one articles published in that are presumably erroneous fig one of these citations includes a doi that indicates a article others have unique article numbers shown as page numbers that can resolve the anomalous year of publication table reveals a considerable difference in the various garfield indices for the australian meteorological and oceanographic journal these differences are due in part to the renaming of the journal from the australian meteorological magazine wos reports trif for both and respectively but is seems more appropriate to combine these as in all other respects it remains the same journal change of a journal name is a relatively common event with about such changes each year so the trif should accommodate them efficiently the world journal of gastroenterology was suspended from wos during apparently due to allegations of self citation and was reinstated in the jcr calculation of the year impact factor includes cites to articles published in but trif recognises no documents from so the denominator is underestimated and the j k vanclay trif is biased upwards the trif calculation should compare like with like either by including documents or omitting citations in any case the practice of gratuitous editorial self citation continues in other journals for instance molecular ecology recently published an editorial with self citations rieseberg et al science would be better served if the trif omitted self citations either to the offending journal or generally to all journals rather than suspending a few of the offenders sampling deficiencies the fact that wos is a sample of scientific literature is often overlooked and the trif is often treated as if it was based on a census in reality wos draws on a sample of the scientific literature selected following their own criteria anonymous as amended from time to time e g through suspensions for self citation other providers scopus google scholar and evaluation agencies e g the excellence for research in australia arc utilize different samples of the scientific literature so their interpretation of a corresponding impact factor would differ from the trif and wos policies to include or suspend a journal also affect the trif because world journal of gastroenterology was suspended in wos has no data but scopus indicates that this journal made over citations to articles from to with the result that the suspension of one journal could have deflated the trif for other gastroenterology journals by as much as these sources of variation lead one to question the practice of publishing the trif with three decimal points and to ask why there is no statement regarding variability the trif also represents a temporal sample evaluating the citations received in one calendar year by work published in the two previous calendar years for some disciplines this year window is appropriate but for others it is inadequate seglen vanclay bensman et al figure illustrates that patterns of citation accrual can vary greatly the trif sampled the mode of citations to articles in nature but the mode is not reached for ecology until a decade has elapsed fig the trif would have secured a reliable indication of nature impact but only the tip of the iceberg in ecology greatly underestimating its impact the release of a year impact factor only partly alleviates this problem furthermore the disconnect between the year window of fig aggregate citations to articles published in in nature left and ecology right illustrating different trends in citation accrual images from wos impact factor table selected parameters for citations accruing to articles published journal g mm mean median mode no of articles and reviews ca cancer plos pathogens 17 mol ecol plant phys 073 j hydro 923 nat prod comm the trif and the decade taken for some journals to peak e g ecology fig introduces yet another way to manipulate the trif by allowing contributions to appear informally online before releasing the official date stamped print version such a practice offers little advantage for a journal that peaks early e g nature fig but may substantially alter the trif of slow to peak journals such as ecology at the time of writing ecology displayed a month queue of articles displayed on line ahead of print a tactic probably designed to facilitate rapid science communication but which has the convenient benefit of inflating the trif statistical shortcomings the usual distribution of citations is highly skewed with a few articles often cited and many articles rarely cited the utility of the mean a conventional statistic with normal distributions does not apply to such distributions with a normal distribution such as would be expected with e g adult body mass the mode mean and median all have similar values however with citation data these common four parameters may differ dramatically table by way of analogy the pattern of citations is more like that of scratch lotto tickets than a normal distribution in this analogy each article is a ticket and the citations are the payout the mode should be of interest to a ticket buyer cf the reader or impact assessor because it reflects most likely prize but most players overlook the mode and focus on the major prizes even though the chance associated with these prizes is small the mean may be of interest to the auditor cf librarian because it reflects the total payout if all the tickets are sold the median is of little utility except to an advertiser who can claim that half the prizes exceed a certain value continuing with the analogy the g mm takes into account that some ticket sellers add extra incentives to try to increase their sales as do editors with strategic self citations the implication is that many users of impact statistics may be better advised to use the mode the most likely citation count rather than the garfield index which is a biased estimate ill suited to a distribution of this nature there is a long history of statistical misuse in science cohen but citation metrics should not perpetuate this failing way forward this review of literature and empirical evidence reveals broad recognition that the trif is not a panacea and leaves considerable room for improvement it is important that the trif is improved because it is influential in shaping science and publication patterns knothe j k vanclay larivie re and gingras the advent of several alternative metrics e g eigen factor article influence score h index and providers e g scopus scimago are both a welcome addition allowing users to choose the metric most suited to their needs and a force for change threatening the dominance of the single multi purpose factor provided by thomson reuters however there remains a need for many of the gate keeping services that thomson reuters provides in assessing timeliness of publication and the rigour of the review process this creates the opportunity for thomson reuters or new providers to reposition such services in a way that is more constructive and supportive of good science the garfield index had its origins in the desire to inform library subscription decisions garfield but it has gradually evolved into a status symbol for journals which at its best is used to attract good manuscripts and at worst is widely manipulated it often serves as a proxy for journal quality but is increasingly used more dubiously as a proxy for article quality postma it lacks transparency repeatability and rigour but despite all these failings there remains a general perception that the trif is useful probably because advocates have no better indicator of journal quality an ill defined notion about the value added offered by a journal that is not reflected in other citation metrics so what is the value adding that a journal can offer an author contribution in its journey from manuscript to published article there is no doubt that value is added most readers prefer a publisher reprint over an author preprint but the elements of this value adding are rarely defined elsevier defines this as peer reviewing and any other value added to it by a publisher such as formatting copy editing technical enhancement and the like but this definition lacks a clear explanation of the elements of peer review in common with laband value adding by editors appears to derive principally from efficient matching of papers with reviewers this neglects the editorial role of checking for duplication salami abraham plagiarism and fraud it is rarely made clear whether this checking is expected of the reviewers completed by the editorial office or adequately dealt with at all and therein lies an unfilled need for scientific publication and an opportunity for commercial providers science would be well served by an independent system to certify that editorial processes were prompt efficient and thorough roosendaal and geurts argued that science communication involves four competing elements registration certification awareness and archiving which together form a value chain van de sompel et al in many cases the weakest link of this value chain is the certification that establishes that a work is a valid scientific contribution there are several aspects involved table but few of these are an integral part of the review process weller hames whilst some of these aspects are assured by self interest and amenable to self regulation other important roles may be neglected and there are several accounts illustrating the reluctance of editors to take decisive action on table participant expectations regarding journal quality viewpoint author reader service providers rigour and ethics constructive feedback from editors and reviewers on language statistics and graphics assurance of rigorous science free of duplication salami plagiarism and fraud efficiency prompt editorial processes for efficient progression from submission to publication online access with click through links to sibling cited and citing articles online presence inclusion in bibliometric databases longevity impact factor problematic articles lock anonymous chalmers gollogly and momen and horner and minifie draw attention to post retraction citations that indicate deficiencies with the retraction process and other associated deficiencies many of the rigour and ethics responsibilities table are passed on to voluntary referees who often lack the time and inclination to rigorously check for fraud and duplicate or salami publication dost indeed bornmann et al point out that guidelines for referees rarely mention such aspects and wager et al observed that many science editors remain unconcerned about publication ethics and misconduct despite the efforts of the committee on publication ethics godlee worse is the observation by fox that when reviewers do detect suspicious findings they have been reluctant to alert editors some editors seek to push ethical responsibilities back to the author e g abraham tobin roberts despite the prevalence of duplicate and fraudulent publications indicating that self regulation by authors is insufficient gwilym et al johnson berquist there is little excuse for this complacency as tools exist to assist the detection of plagiarism and fraudulence e g mckeever meyer zu eissen and stein rossner and the consequences of some science fraud e g medical ghost writing e g eaton gøtzsche et al barbour may be serious and far reaching there is a potential role for google scholar in helping to reduce fraud and plagiarism in science google scholar already routinely displays n versions of this article in search results and it could usefully display other articles with similar text and other articles with similar images such an addition would be very useful for researchers when compiling reviews and meta analyses clearly good science requires a more proactive role from editorial offices and the pursuit of this role is not reflected in the trif in its present form the trif is not fit for purpose and does not serve the role of a journal quality indicator often assumed by users in broad terms there are three options for the future of the impact factor the trif could be retained in a similar form but amended to deal with its greatest limitations it should move from g mm to g verified citations only to reduce errors and to maintain pressure on the value chain for quality control and should rely only on citations from articles and reviews to articles and reviews it should cease with decimal detail and should round factors appropriately according to the error associated with the estimate the discontinuity introduced by these changes make it opportune to re examine the timeframe and to abandon the year window in favour of an alternative that reflects the varying patterns of citation accrual in different disciplines failing appropriate action by thomson reuters the scientific community could rely on a community based rating of journals in much the same was as plos one does for individual articles and as other on line service providers offer to clients e g tripadvisor jeacle and carter finally the preferable option is for thomson reuters to abandon the trif in its present form and to expand on other existing services that are currently implicit in the gatekeeping implicit in the inclusion in wos the scientific community would benefit from independent certification of journals assuring not only timeliness and rigorous review currently part of the gateway to be admitted to trif but also to certify more broadly the standard of quality control e g systematic checks for plagiarism duplicate publication and fraud j k vanclay this article is not the first to highlight the need for independent certification in their review saunders and savulescu called for independent monitoring and validation of research there have been several calls errami and garner errami et al butakov and scherbinin habibzadeh and winker foo for greater investment in and more systematic efforts directed at detecting plagiarism and duplication callaham and mcculloch concluded that the monitoring of reviewer quality is even more crucial to maintain the mission of scientific journals but despite these many calls for reform change has been imperceptible the trif remains essentially unchanged but supplemented with a year variant and the eigenfactor and article influence score and journals continue to tout their achievements when their trif increases despite the dubious nature of the indicator the time has come to abandon the trif and to replace it with a system that is better aligned with quality considerations in scientific publication such a system could be a rating system that allocated for instance one to five stars for editorial efficiency and value adding for the rigour and constructiveness of the review process and for procedures to detect and deal with plagiarism and other lapses of ethics such a system could be a powerful force for improving science stars should not be awarded for a high manuscript rejection rate but for the extent of value adding supported by a journal stars should not be jeopardised by an instance of fraud but should be won or lost on the procedures in place to detect and deal with fraud and other misdemeanours thomson reuters could show leadership with such a new certification system or failing them editors should collaborate to achieve the same end there are useful lessons from the years of experience with forest certification e g vlosky and ozanne cashore et al where independent scrutiny has been desirable to motivate progressive improvement in forest management forestry experience reveals the importance of a well resourced executive group overseen by a strong board representing stakeholder interests thomson reuters or other providers seeking to fill this space would do well to take inspiration from the forest certification experience concerns that the growing competition for funding and citations might distort science are frequently discussed but have not been verified directly of the hypothesized problems perhaps the most worrying is a worsening of positive outcome bias a system that disfavours negative results not only distorts the scientific literature directly but might also discourage high risk projects and pressure scientists to fabricate and falsify their data this study analysed over papers published in all disciplines between and measuring the frequency of papers that having declared to have tested a hypothesis reported a positive support for it the overall frequency of positive supports has grown by over between and with significant differences between disciplines and countries the increase was stronger in the social and some biomedical disciplines the united states had published over the years significantly fewer positive results than asian countries and particularly japan but more than european countries and in particular the united kingdom methodological artefacts cannot explain away these patterns which support the hypotheses that research is becoming less pioneering and or that the objectivity with which results are produced and published is decreasing keywords bias misconduct research evaluation publication publish or perish competition introduction competition in science is changing and concerns that this might distort scientific knowledge are openly and commonly discussed young et al statzner and resh the traditional race for priority of important discoveries is increasingly intertwined with a struggle for limited funding and jobs the winners of which are determined by measures of performance and impact young et al bonitz and scharnhorst statzner and d fanelli issti institute for the study of science technology and innovation the university of edinburgh old surgeons hall edinburgh scotland uk e mail dfanelli staffmail ed ac uk d fanelli resh individual scientists research institutions countries international organizations and scientific journals are increasingly evaluated based on the numbers of papers they publish and citations they receive shelton et al meho nicolini and nozza king from all these levels therefore come pressures on researchers to publish frequently and in high ranking journals lawrence this combination of competition and bibliometric evaluation has a longer history in the united states but is increasingly adopted across fields and countries as a way to improve productivity and the rational distribution of resources warner qiu de meis et al osuna et al how well bibliometric parameters reflect actual scientific quality however is controversial and the effects that this system might have on research practices need to be fully examined de rond and miller osuna et al young et al several possible problems have been hypothesised including undue proliferation of publications and atomization of results gad el hak statzner and resh impoverishment of research creativity favouring normal science and predictable outcomes at the expense of pioneering high risk studies de rond and miller growing journal rejection rates and bias against negative and non significant results because they attract fewer readers and citations statzner and resh lortie sensationalism inflation and over interpretation of results lortie atkin ioannidis increased prevalence of research bias and misconduct qiu indirect empirical evidence supports at least some of these concerns the per capita paper output of scientists has increased whilst their career duration has decreased over the last years in the physical sciences fronczak et al rejection rates of papers have increased in the high tier journals larsen and von ins lawrence negative sentences such as non significant difference have decreased in frequency in papers abstracts while catchy expressions such as paradigm shift have increased in the titles pautasso atkin no study however has yet verified directly whether the scientific literature is enduring actual changes in content one of the most worrying distortions that scientific knowledge might endure is the loss of negative data results that do not confirm expectations because they yield an effect that is either not statistically significant or just contradicts an hypothesis are crucial to scientific progress because this latter is only made possible by a collective self correcting process browman knight yet a lack of null and negative results has been noticed in innumerable fields song et al gerber and malhotra howard et al dwan et al jennions and moller their absence from the literature not only inflates effect size estimates in meta analyses thus exaggerating the importance of phenomena but can also cause a waste of resources replicating research that has already failed and might even create fields based on completely non existent phenomena ioannidis feigenbaum and levy song et al in meta analysis publication bias can in part be corrected by assuming that negative results are simply never written up and are left lying in scientists drawers formann however this assumption is obviously naıve a realistic scenario includes various forms of conscious and unconscious biases that affect all stages of research e g study design data collection and analysis interpretation and publication producing positive findings when there should be none thus creating distortions that are difficult to correct a posteriori ioannidis marsh and hanlon jeng the problem is bound to be particularly acute in fields where theories and methods are less clearly defined and true replication is rare or impossible palmer kelly evanschitzky et al this study verified whether the frequency of positive results has been increasing in the contemporary scientific literature papers that declared to have tested a hypothesis were negative results are disappearing from most disciplines and countries searched in over journals listed in the isi essential science indicators database excluding the highest impact multidisciplinary journals like science nature or pnas by reading the abstracts and eventually full text of papers sampled at random from all disciplines it was determined whether the authors of the study had concluded to have found a positive full or partial or negative null or negative support for the tested hypothesis analyses on a previous sample spanning the years n found that papers were more likely to report a positive result in disciplines and methodologies believed to be softer e g psychology vs space science behavioural vs chemical analyses and when the corresponding author worked in states of the usa where academics publish more papers per capita findings which suggest that this measure is a reliable proxy of bias fanelli b this study expanded the analysis to include papers published in the total n results the proportion of papers reporting a positive result in the sample was in peaked at in and was 9 in on average the odds or reporting a positive result have increased by around every year showing a statistically highly significant trend fig the size of this effect remained almost identical when controlling for differences between disciplines country of corresponding author and papers testing multiple versus single hypotheses table for simplicity only countries with more than papers plus all the others combined are shown in table but controlling for actual country of each paper countries in total yielded the same increase rate b 008 wald p or the effect was only very slightly reduced to around per year a non significant difference when controlling for all available potential confounding variables discipline domain method ology country broad geographical area i e us eu asia other and papers testing multiple versus single hypotheses b 009se wald p r 031 ci this latter maximal model explained over 8 of the variance in papers outcomes nagelkerke positive results differed significantly between disciplines both in the average frequency and in the rate of increase over the years the average frequency of positive results was significantly higher when moving from the physical to the biological to the social sciences and in applied versus pure disciplines tables all of which confirms previous findings fanelli space science had not only the lowest frequency of positive results overall table it was also the only discipline to show a slight decline in positive results over the years together with neuroscience behaviour fig a relatively stable no growth fig percentage of papers reporting a support for the tested hypothesis plotted against year of publication logistic regression estimates are uncorrected for any covariate a smoothed interpolation line was added to help visualize trends d fanelli table logistic regression slopes standard error wald test and significance odds ratio and confidence interval predicting the likelihood of a paper to report a positive result depending on the following characteristics year of publication discipline of journal national location of corresponding author only countries with n c paper testing one versus multiple hypotheses only the first of which was included in the analysis variable b se wald df sig or ci or year 008 042 discipline all agricultural sc 238 biology bioch 836 chemistry 268 040 clinical medicine 250 computer sc 367 economics bus 252 environment ec 226 engineering 296 535 geosciences 232 immunology 244 17 molecular biology 236 492 microbiology materials sc 344 6 neurosci beh 957 plant and an sc 220 021 physics 283 379 psyc psychiatry 19 pharm toxicol 244 576 social sc general 236 country papers 9 australia 199 852 canada 155 935 germany 477 france italy 291 japan 400 512 netherlands 261 695 united kingdom 147 686 other 121 867 multiple hypotheses 966 147 constant 452 310 categorical variables were tested for overall effect in the model then compared by indicator contrast to space science for domain and to united states for countries minimum statistical power to detect a small medium and large main effect for contrast between countries i e us vs italy 70 for contrasts between disciplines i e space sc vs computer sc smoothed interpolation lines were added to help visualize trends negative results are disappearing from most disciplines and countries trend was apparent in geosciences and plant and animal sciences but in most cases the frequency of positive results was increasing in eight disciplines the trend was significantly steeper than space science in descending order of effect size economics and business clinical medicine psychology psychiatry pharmacology and toxicology molecular biology physics agricultural sciences social sciences general fig in the last year of the series in five of these eight disciplines positive results were over of the total controlling for other factors the rate of growth of positive results was significantly higher in the social sciences compared to the physical sciences table fig 3 the overall frequency of positive results was significantly different between countries the united states published significantly fewer positive results than japan but more than the uk table if broader geographical regions were considered the us published significantly fewer positive results than asian countries but more than eu table although no statistically significant difference in growth over the years was detected between countries or broader geographical regions figs patterns appeared to vary showing a marked increase in the us and uk uniformly high frequencies in japan and a null or negative trend in canada australia and france fig discussion the proportion of papers that having declared to have tested a hypothesis reported a full or partial support has grown by more than between and underlying this overall increase were significant differences between disciplines and countries the trend was significantly stronger in the social sciences i e psychology psychiatry economics business and social sciences general and in applied disciplines whilst a few disciplines showed a null or even a slightly declining trend i e space science geosciences table logistic regression slopes standard error wald test statistic and significance odds ratio and confidence interval predicting the likelihood of a paper to report a positive result depending on the following characteristics year of publication scientific domain of journal geographical location of corresponding author journal pertaining to applied versus pure disciplines paper testing one versus multiple hypotheses only the first of which was included in the analysis variable b se wald df sig or ci or year 058 008 domain overall biological sc 175 3 059 social sc 426 127 226 region overall 3 asia 557 234 672 2 eu 3 other 006 multiple vs single hp 1 075 141 1 pure vs applied 188 087 687 1 1 1 1 intercept 215 1 001 001 categorical variables were tested for overall effect in the model then compared by indicator contrast to physical sciences for domain and to united states for geographical location minimum statistical power to detect a small effect for contrast between domains physical vs social sciences for contrast between geographical regions i e us vs other 95 d fanelli fig 2 percentage of papers reporting a support for the tested hypothesis plotted against year of publication and divided by discipline of journal sp space science ag agricultural sciences bb biology biochemistry ch chemistry cm clinical medicine cs computer science eb economics business ee environment ecology en engineering ge geosciences im immunology mb molecular biology genetics mi microbiology ms materials science nb neuroscience behaviour pa plant and animal sciences ph physics pp psychiatry psychology pt pharmacology toxicology so social sciences general regression values report the interaction effects estimated in a hierarchically well formulated logistic regression model controlling for year of publication scientific domain of journal paper testing single versus multiple hypotheses national location of corresponding author the main effects of this regression model with interaction components removed are reported in table 1 numbers in brackets are sample size smoothed interpolation lines were added to help visualize trends neuroscience behaviour plant and animal sciences most were undergoing a significantly positive growth e g clinical medicine pharmacology and toxicology molecular biology agricultural sciences corresponding authors based in asian countries and in particular japan reported more positive results than in the us who in turn reported more positives than in europe and particularly in the uk methodological artefacts cannot explain the main findings of this study although performed by only one author the coding was blind to year of publication and country of corresponding author the coding was not blind to discipline but the effects observed are independent of discipline or domain tables 1 2 the coding was not blind to decade having been performed first for and then for however if this had introduced a bias in the coding then we would expect a discontinuity between the years and such discontinuity was not observed fig 1 and there was no significant difference in the prevalence of positive results between decades when controlling for year of publication b 183 0 144 wald 1 607 df 1 p 0 205 power to detect a small effect 0 996 indeed positive results increased significantly within each decade negative results are disappearing from most disciplines and countries fig 3 percentage of papers reporting a support for the tested hypothesis plotted against year of publication and divided by scientific domain of the journal physical biological and social sciences logistic regression estimates are interaction effects in a hierarchically well formulated model the main effects of this model calculated with interaction components removed are reported in table 1 numbers in brackets are sample size fig 4 percentage of papers reporting a support for the tested hypothesis plotted against year of publication and divided by national location of corresponding author us united states uk united kingdom nl netherlands it italy jp japan au australia de germany ca canada the figure shows all countries more than papers in the study sample plus all other countries combined logistic regression estimates are interaction effects in a hierarchically well formulated model the main effects of this model calculated with interaction components removed are reported in table 2 b 0 0 wald p 0 001 b 0 052 0 024 wald 4 584 p 0 032 this trend had not been noticed in a previous study covering the years because year of publication had been treated as a purely confounding effect i e tested as a categorical variable changing the parameterization of year in these regression models did not affect the estimation of the other parameters in any meaningful way so previous conclusions remain valid fanelli to the best of the author knowledge this is the first direct evidence that papers reporting negative results have decreased in frequency across disciplines a recent study adopting a different approach reached similar conclusions by finding a decrease in the use of the term non significant difference in abstracts from various databases i e science and social sciences citation index medline cab over a period of up to years pautasso this latter study did not examine the actual outcome of each paper and d fanelli fig 5 percentage of papers reporting a support for the tested hypothesis plotted against year of publication and divided by geographical location of corresponding author us united states eu austria belgium denmark finland france germany greece ireland italy netherlands portugal spain sweden united kingdom as china hong kong india japan singapore south korea taiwan logistic regression estimates are interaction effects in a hierarchically well formulated model the main effects of this model calculated with interaction components removed are reported in table 2 numbers in brackets are sample size smoothed interpolation lines were added to help visualize trends only examined the frequency of a sentence this might have been an unreliable proxy of publication bias as suggested by the fact that it yielded very high rates of non significant results contradicting ample evidence that these are the minority in all fields pautasso the reliability of the present study approach which assessed the actual conclusions of each paper is supported by a close agreement with previous surveys that found statistically significant results to be around 95 in psychology in ecology and between and in biomedicine sterling et al csada et al kyzas et al an important limitation of the present study was the use of only one journal database a choice made to ensure coverage of all domains and unambiguous attribution of each paper to one discipline the esi database is a subset of the isi web of knowledge which is currently the main source of bibliometric and citation data for research evaluation around the world the isi system has been criticised in the past for over representing journals from the us shelton et al and for expanding more slowly than the actual growth of the scientific literature larsen and von ins such criticisms must be taken into account when evaluating the generality of this study but cannot undermine its conclusions a north american bias within the database might be supported by this study data in which over of all papers had the corresponding author based in the us but cannot explain away the various national patterns observed see discussion below the relatively negative results are disappearing from most disciplines and countries slow growth of the database would imply that it is covering a decreasing proportion of core journals amidst an expanding volume of publications larsen and von ins could negative results be increasingly published in journals not included in the esi database this possibility remains to be tested but it appears unlikely given that a similar study on abstracts in other databases see above reached identical conclusions pautasso in any case a growing positive outcome bias within esi indexed journals which supposedly cover the most important publications and most of the citations in each discipline would still reflect important changes occurring within the scientific system excluding methodological biases what caused the patterns observed the likelihood for a study to publish a positive result depends essentially on three factors fanelli which we will examine in turn 1 the hypotheses tested might be increasingly likely to be true obviously this would not happen because sciences are closer to the truth today than years ago but because researchers might be addressing hypotheses that are likely to be confirmed to make sure they will get publishable results 2 the average statistical power of studies might have increased for example if the average sample size of studies had increased boosting the discovery rate of true relationships ioannidis this would be good news suggesting an improvement of methods and quality of studies however it would be unlikely to explain alone all the patterns observed e g differences between disciplines moreover it is unsupported statistical power appears to be very low in all fields and there is no evidence that it has grown over the years delong and lang jennions and moller maddock and rossi 3 negative results could be submitted and accepted for publication less frequently or somehow turned into positive results through post hoc re interpretation re analysis selection or various forms of manipulation fabrication in the lightest scenario of hypothesis 3 changes would be occurring only in how results are written up tests would be increasingly mentioned in the paper only when the results are positive and negative results would be either embedded in positive papers or presented as positive by inverting the original hypothesis such scenario which would still be the symptom of growing pressures to present a positive outcome was not supported by the data in almost all papers examined the hypotheses were stated in the traditional form with the null hypothesis representing a no effect there was no evidence that negative results were increasingly embedded in papers reporting positive ones papers listing multiple hypotheses were more likely to report a negative support for the first one listed 2 but their frequency has not grown significantly over the years b 0 019 0 013 wald 2 058 p 0 151 power to detect a small and medium effect 0 543 and 0 999 there was also no evidence that negative results are communicated in other form such as conference proceedings a sample of these latter was initially included in the analysis by mistake n and they tended to report more positive results 3 df 1 p 0 076 power to detect a small effect 0 999 higher frequencies of positive results from non english speaking or non us countries have been observed in past meta analyses and were usually attributed to editorial and peer review biases which might tend to reject papers from certain countries unless they present particularly strong or appealing results song et al yousefi nooraie et al this could explain the higher rate of positive results from asian countries but cannot explain why the us have more positive results than the uk an equally developed and english speaking country an editorial bias favouring the us would allow them to publish as many or more negative results than any other country not fewer therefore the differences observed suggest that researchers in the us have a stronger bias against negative results than in europe this hypothesis remains to be fully tested but it would be d fanelli independently supported by at least two studies one showing that the us have a higher proportion of retractions due to data manipulation steen and the other suggesting a higher publication bias among union productivity studies from the us doucouliagos et al the causes of these differences remain to be understood one possible factor being higher pressures to publish imposed by the us research system a common argument against concerns for publication bias is that negative results are justifiably ignored per se but become interesting and are published when they contradict important predictions and or previous positive evidence ensuring self correction of the literature in the long run silvertown and mcconway this does indeed seem to be the case at least in some biomedical fields where the first paper to report a finding often shows extreme effects that subsequent replications reduce or contradict entirely ioannidis et al ioannidis and trikalinos however even if in the long run truth will prevail in the short term resources go wasted in pursuing exaggerated or completely false findings ioannidis moreover this self correcting principle will not work efficiently in fields where theoretical predictions are less accurate methodologies less codified and true replications rare such conditions increase the rate of both false positives and false negatives and a research system that suppresses the latter will suffer the most severe distortions this latter concern was supported by the finding that positive results were more frequent and had increased more rapidly in the social and many biological sciences where theories and methods tend to be less codified and replication is rare fanelli schmidt evanschitzky et al tsang and kwan kelly palmer jones et al hubbard and vetter in conclusion it must be emphasised that the strongest increase in positive results was observed in disciplines like clinical medicine pharmacology toxicology molecular biology where concerns for publication bias had a longer history and several initiatives to prevent and correct it have been attempted including registration of clinical trials enforcing guidelines for accurate reporting and creating journals of negative results bian and wu simera et al kundoor and ahmed knight this study suggests that such initiatives have not met their objectives so far and the problem might be worsening methods data collection the sentence test the hypothesis was used to search all journals available in the essential science indicators database in december which classifies journals univocally in 22 disciplines for esi classification methodology see http sciencewatch com about met the discipline of mathematics however yielded no usable paper while the multidisciplinary category including papers such as science or nature was excluded therefore papers from disciplines were included in the analysis the disciplines were grouped in the following domains physical sciences space science chemistry computer science engineering geosciences materials science physics biological sciences agricultural sciences biology biochemistry clinical medicine environment ecology immunology molecular biology genetics microbiology neuroscience behaviour plant and animal sciences pharmacology toxicology social sciences economics business psychiatry psychology social sciences general papers were sampled in two phases 1 papers published between and already used in previous studies 2 papers published between and in both negative results are disappearing from most disciplines and countries phases all retrieved titles were saved on bibliographic database software and then up to a maximum papers were sampled from each discipline when the number of titles retrieved from one discipline exceeded papers were selected using a random number generator in one discipline plant and animal sciences an additional papers from the period were analysed in order to increase the statistical power by examining the abstract and or full text the specific hypothesis tested in each paper was identified and it was determined whether the authors had concluded to have found a positive full or partial or negative null or negative support if more than one hypothesis was being tested only the first one listed in the text was considered meeting abstracts were excluded from sampling whilst sampled papers were excluded when they either did not test a hypothesis total n or when there was not sufficient information abstract unclear and full text not available to determine the outcome total n while the former have no role in the analysis the latter are technically missing values since access to full text was lower for older articles and some disciplines these missing values were unevenly distributed between disciplines p 0 001 and were negatively associated with year b 0 080 0 035 wald 5 352 p 0 021 however we can exclude that these missing values are an important confounding factor for three reasons 1 there is no reason to believe that these missing papers are more likely to report positive than negative results 2 they represent a very small fraction of the sample i e 0 8 3 their prevalence is higher until and then declines rapidly not matching the observed increase in positive results all data was extracted by the author an untrained assistant who was given basic written instructions scored papers the same way as the author in out of cases and picked up exactly the same sentences for hypothesis and conclusions in all but three cases the discrepancies were easily explained showing that the procedure is objective and replicable the country of location of each paper was attributed based on the address of the corresponding author geographical location was defined by the following groupings us united states eu austria belgium denmark finland france germany greece ireland italy netherlands portugal spain sweden united kingdom as china hong kong india japan singapore south korea taiwan information on year of publication and country was retrieved after all papers had been coded therefore the coding of papers as positive and negative was completely blind to year and country of origin statistical analyses the ability of independent variables to predict the outcome of a paper was tested with a general linear model assuming a binomial link function i e logistic regression in the form represents the various characteristics of the ith paper that were controlled for in each model as specified in the text e g dummy variables for discipline country etc statistical significance of the effect of each variable was calculated through wald test the relative fit of regression models was estimated with nagelkerke adjusted while it has been a belief for over a decade that wireless sensor networks wsn are application specific we argue that it can lead to resource underutilization and counter productivity we also identify two other main problems with wsn rigidity to policy changes and difficulty to manage in this paper we take a radical yet backward and peer compat ible approach to tackle these problems inherent to wsn we propose a software defined wsn architecture and address key technical challenges for its core component sensor openflow this work represents the first effort that synergizes software defined networking and wsn index terms software defined networking openflow wire less sensor networks e ver sensor since networks their i debut motivation over have a been decade conceived ago wireless to be application specific which has probably formed a belief thus far however we deem it worth an afterthought with the upsurge of sensor applications nowadays and the advancement of sensor technologies such as multi modal and high end mote platforms e g stargate and netbridge these new changes render application specific wsn prone to resource underutilization where multiple wsn for respective applications are deployed in the same or overlapping terrain while a single versatile wsn could achieve the same ob jectives and counter productivity where different vendors develop wsn in isolation without adequately reusing common functionalities which would considerably expedite prototyping and production another problem with wsn is that they are rigid to policy changes policies are rules related to network exogenous fac tors such as business operation and user access as opposed to network endogenous factors such as node and link properties which have been extensively studied over the years and han dled algorithmically policy changes engendered by the ever changing business needs are hard to cope with by algorithms and often dictate manual reconfiguration or reprogramming of wsn which is difficult because of vendors complicated and often proprietary implementations as a result it is usually inevitable to involve vendors and hence incur delay in policy enforcement and considerable financial and opportunity cost a third problem is that wsn are hard to manage this is because developing a network management system nms for distributed wsn is a demanding task in the first place furthermore in practice this task is often scheduled as manuscript received july the associate editor coordinating the review of this letter and approving it for publication was d qiao the authors are with the institute for infocomm research a star singapore t q s quek is also with singapore university of technol ogy and design singapore e mail luot hptan qsquek a star edu sg tonyquek sutd edu sg digital object identifier lcomm 1throughout this paper we use wsn regardless of its singular or plural form to avoid readers inconvenience in pronouncing wsns fig software defined wireless sensor networks phase ii in project planning and hence entails hacking existing code on sensor nodes which is a big headache to developers and is highly error prone the above problems are not superficial symptoms but are inherent to wsn and deep rooted in the architecture each node is fully fledged with all physical up to application layer functionalities collectively behaving like an autonomous system that performs various networking functions such as data forwarding and network control although this architecture works well most of the time due to many well designed algorithms it lacks good abstraction and carries too much complexity making wsn unwieldy inelastic to change and hard to manage ii sd wsn and sensor openflow in this paper we take the first move to tackle the above mentioned problems using a radical yet backward and peer compatible approach we propose software defined wsn sd wsn an architecture featuring a clear separation be tween a data and a control plane and sensor openflow sof the core component of sd wsn as a standard communication protocol between the two planes the data plane consists of sensors performing flow based packet forwarding and the control plane consists of one or possibly more controller that centralizes all the network intelligence performing network control such as routing and qos control this architecture is depicted in fig the whole idea is to make the underlying network i e data plane programmable by manipulating a user customizable flow table on each sensor via sof our proposal is underpinned by the recently emerged software defined networking sdn paradigm and open flow proposed for enterprise and carrier networks they have become a resounding success gaining wide support from a large number of industries including microsoft google facebook hp deutsche telekom verizon cisco ibm and samsung c ieee luo et al sensor openflow enabling software defined wireless sensor networks in the academia related research has recently heated up on a variety of topics these studies were all conducted in the context of enterprise and carrier networks alike such as data centers where sdn openflow originated from in stark contrast wsn represent a sheerly distinct environment and thereby confront concepts like sd wsn and sof with significant challenges however we deem it worth exploring to introduce the fundamental idea of sdn into wsn and tailor it into a viable approach to solve wsn inherent problems indeed we envisage sd wsn to transform traditional wsn into networks that are versatile supporting multiple applications in a plug and play manner sensors are no longer application dependent but application customizable this is achieved by i the programmable data plane which supports virtually all sorts of packet forwarding rules and ii the control plane which decouples upper applications from physical devices i e sen sors and provides a global unified view of the underlying network to the applications flexible easy to enforce policy changes throughout the entire network which used to be a tedious task and is prone to inconsistency this is achieved by the centralized and extremely granular network control in sd wsn where the granularity is supplied by the fully user customizable flow tables easy to manage building an nms is no different from adding another application on top of the control plane using open apis and without hassle of hacking existing code in addition the centralized network view provided by the control plane is a great facility as network administrators prefer centralized nms in practice iii technical challenges the fundamental assumption that openflow makes is that the underlying network is composed of high speed switches such as ethernet mpls switches and ip routers openflow was also designed as a wired protocol and its typical use cases are data centers cellular backhaul enterprise wifi backbone and wans compared to wsn these represent significant disparities and lead to major challenges in designing sof a data plane creating flows at the data plane packets are handled as per flows a flow is a programmable i e user customizable set of packets that share certain properties specified by a match in a flow table entry or flow entry for short the match is often wildcarded like ip source address is and packets that match it will be treated as in the same flow and be imposed an action e g send to port specified by the same flow entry cf fig or openflow implicitly assumes the presence of ip like ad dressing so as to create flows for example it defines match fields such as and to match packets with source address ethernet destination address and mpls label respectively however as opposed to address centric openflow networks wsn are typically data centric acquiring data of interest is more important than knowing who sent the data and employ different addressing such as attribute based naming e g nodes whose sensing temperature c sof must cope with this in the first place for flow creation b control plane sof channel an openflow channel is an end to end connection used to transmit control messages between a controller and a switch an sof channel is similarly defined however this channel must provide tcp ip connectivity for reliable end to end in order message delivery and the two end parties are identified using ip addresses these are generally unavailable in wsn and need to be addressed c overhead of control traffic sdn can and usually would host the openflow channel out of band i e using a separate dedicated network this is normally not practical for wsn and the sof channel has to be hosted in band i e by the wsn itself thus the resource constrained wsn will have to additionally carry control traffic between controllers and sensors this is further exacerbated by the fact that control traffic in wsn tends to be large due to the high network dynamics node link failures energy aware routing mobility etc hence without a proper mechanism to curb control traffic overhead the underlying wsn can be overloaded d traffic generation end users are considered peripheral to sdn and hence out of the scope of openflow on the contrary sensor nodes behave like end users by generating data packets in addition to merely forwarding data as openflow switches do e in network processing at times wsn need to process data in situ e g perform data aggregation or decision fusion in order to reduce data redundancy and conserve network resource such as bandwidth and energy this is another feature absent in sdn f backward and peer compatibility albeit radical sof should desirably provide backward compatibility with respect to traditional non openflow and non sof networks so as to protect legacy investments it is also desirable for sof to offer peer compatibility i e to be compatible with openflow networks which are concur rently being developed and standardized for interoperability purposes iv proposed solutions in this section we provide a suite of preliminary solutions to the issues identified above based on the latest openflow specification ieee communications letters vol no november fig a flow entry with match defined in the oxm format fig creating class flows by defining match using oxm a data plane creating flows we give two solutions for flow creation from which a network operator can choose based on his own preference the first solution is to redefine flow tables to cater for the special addressing schemes in wsn we classify wsn addressing schemes into class compact network unique addresses such as the zigbee bit network addresses e g as as signed by the cskip algorithm and class concatenated attribute value pairs cav such as temperature and zone id and x coordinate we handle class by exploiting the openflow extensible match oxm a type length value tlv format used to define flow matches as explained by fig in sof we introduce two new to represent class addresses source and destination and construct class flow matches under the same oxm frame work a self explanatory example is given in fig where the flow will be composed of packets with source network address of and destination address of zigbee coordinator we handle class by introducing our cav format which is a quadruple defined by fig then by adding a new we are able to form any class flows as illustrated by fig where temperature is assumed to be an stored at offset of each packet zone id at offset and x coordinate at offset the second solution is to augment wsn with ip for which we capitalize on and recommend two off the shelf ip stacks uip and uip is an implementation on contiki os an open source operating system for wsn and the internet of things is a fully compliant extension to uip and is ready phase certified has the right to use the ready silver logo both uip and are now part of contiki os contributed by cisco redwire sap and sics blip the berkeley ip implementation for low power net works which is an open source implementation for tinyos based on the standard it has been extensively verified on micaz telosb and epic platforms b control plane sof channel the design of control plane must be consistent with the data plane therefore corresponding to the two solutions above we also provide two solutions for the sof channel if the network fig the new cav format with one av pair multiple av pairs shall be concatenated fig creating class flows by defining match using cav operator chooses the non ip solution i e redefining flow tables then the sof channel can be supplied by overlaying a transport protocol directly over wsn designing such a protocol has been well studied in the past decade and hence not to reinvent the wheel we compile and make publicly accessible a comprehensive list of credible works on wsn transport protocol design implementation experimentation and verification if otherwise the network operator chooses to augment wsn with ip and use our recommended ip stacks sof channels will be self supplied because uip and blip are all shipped with ready to use tcp implementations c curbing control traffic as aforementioned the control traffic carried by the in band sof channel may overload the underlying wsn our solution is based on the following observations first the control traffic is mainly comprised of two types of control messages packet in and packet out or flow mod a packet in is a flow setup request sent by a sensor to a controller to seek instruction on how to handle an incoming packet upon table miss a packet does not match any flow entry a packet out or flow mod is the response from the controller giving instruction to the requesting sensor second the control traffic resulting from this can be sig nificant because wsn traffic is often bursty an event of interest can trigger a large swarm of packets from several sensors leading to many flow setup requests simultaneously this scenario repetitively appears because each flow entry is associated with an expiring timer in our solution a sensor only sends one packet in for the first table miss and suppresses subsequent packet in whose associated packets have the same destination address class or class as the first packet until the corresponding packet out or flow mod is received or a predefined timeout occurs the rationale is twofold firstly because the end to end sof channel is slow as it is in band of wsn the latency between sending packet in and receiving packet out or flow mod is much larger than in openflow therefore there can be a large number of incoming packets during this interval secondly since the major traffic in wsn is upstream from many sensors to one or a few sinks it is effective to use destination addresses to bundle packets into flows luo et al sensor openflow enabling software defined wireless sensor networks b openflow compatible sof flow entries a class entry upper and a class entry lower fig making sof peer compatible with openflow in fact this method can also be applied to general openflow networks we have developed such an algorithm called control message quenching cmq for openflow networks and verified its effectiveness in reducing flow setup latency and elevating network throughput d traffic generation we add a traf gen module on each sensor as shown in fig it is a simple module that can be implemented as an interrupt routine because sensory data generation consists of two very simple steps reading data from the sensing hardware and converting the data if needed an example is given below based on the arduino sensor platform int raw analogread pin read raw data from h w pin next convert to user friendly data in the case of ultrasound sensors raw is the measured distance in cm float distanceinmeter raw in the case of temperature sensors raw is degree celsius scaled by float temperatureincelsius raw in the case of potentiometers raw is voltage scaled from v to float voltage raw depending on implementation this traf gen module can run in a blocking synchronously awaiting sensory data to become available callback asynchronously triggered by a data available event or round robin periodically checking if data are available manner e in network processing an in net proc module is added as shown in fig if the processing is not needed by a packet this module simply passes the packet intact to the flow table in case of possible future changes to the processing algorithm we use over the air programming ota a commercially available technology that allows updating sensor firmware and software wirelessly and remotely libelium for instance offers a package that features secured and interference free ota programming over a large network in seconds or up to a few minutes we note that in network processing makes the design not purely clean slate which we deem as a compromise that wsn has to make due to its unique characteristics in fact data aggregation is the most common in network processing and can usually be accomplished using standard operations such as average median min max or removing redundant data therefore it could be absorbed into flow tables as a special way of packet handling which would mitigate the compromise and enhance network programmability we leave this as an open question to the research community f backward and peer compatibility sof inherits backward compatibility from openflow an sof hybrid sensor will have a normal logical port defined a prerequisite of openflow a tlv of must be preceded by a tlv of similarly as by openflow which directs packets to traditional sensor network forwarding the novelty lies in handling peer compatibility to be compatible with openflow sof should enable openflow to recognize sof flow tables so that openflow can rele gate class and class flow entries to sof we exploit a prerequisite structure of openflow which specifies that a protocol specific oxm tlv e g must be preceded by a particular value of for differentiation purposes for example an tlv must be preceded by a of fig and similarly by and arp by for the newly introduced class and class flows we designate two unused values of and respectively thus an sof flow entry dst or can be defined as exemplified by fig which achieves peer compatibility transmitter channel state information csit is cru cial for the multiplexing gains offered by advanced interference management techniques such as multiuser multiple input mul tiple output mimo and interference alignment such csit is usually obtained by feedback from the receivers but the feedback is subject to delays the usual approach is to use the fed back information to predict the current channel state and then apply a scheme designed assuming perfect csit when the feedback delay is large compared to the channel coherence time such a prediction approach completely fails to achieve any multiplexing gain in this paper we show that even in this case the completely stale csi is still very useful more concretely we show that in an mimo broadcast channel with transmit antennas and receivers each with receive antenna degrees of freedom is achievable even when the fed back channel state is completely independent of the current channel state moreover we establish that if all receivers have independent and identically distributed channels then this is the optimal number of degrees of freedom achievable in the optimal scheme the transmitter uses the fed back csi to learn the side information that the receivers receive from previous transmissions rather than to predict the cur rent channel state our result can be viewed as the first example of feedback providing a degree of freedom gain in memoryless channels index terms feedback delay interference alignment multiple antenna channels network coding output feedback side informa tion vector gaussian broadcast channels i n channel wireless state communication information i introduction transmitter knowledge of the csit can be very important while in point to point channels csit only provides power gains via waterfilling in multiuser channels it can also pro vide multiplexing gains for example in a multiple input mul tiple output mimo broadcast channel csit can be used to manuscript received september revised february accepted march date of publication april date of cur rent version june this work was supported in part by a gift from qualcomm inc and by the air force office of scientific research under grant 0317 an initial version of this paper has been reported as technical report no ucb eecs at the university of cali fornia berkeley september the material in this paper was presented in part at the annual allerton conference on communication control and computing monticello il september m a maddah ali was with wireless foundations department of electrical engineering and computer sciences university of california berkeley ca usa he is now with bell labs alcatel lucent holmdel nj usa e mail maddah ali ee gmail com d tse is with wireless foundations department of electrical engineering and computer sciences university of california berkeley ca usa e mail dtse eecs berkeley edu communicated by s tatikonda associate editor for communications color versions of one or more of the figures in this paper are available online at http ieeexplore ieee org digital object identifier tit send information along multiple beams to different receivers simultaneously in interference channels csit can be used to align the interference from multiple receivers to reduce the ag gregate interference footprint in practice it is not easy to achieve the theoretical gains of these techniques in the high regime where the multi plexing gain offered by these techniques is particularly signif icant the performance of these techniques is very sensitive to inaccuracies of the csit however it is hard to obtain accurate csit this is particularly so in frequency division duplex sys tems where the channel state has to be measured at the receiver and fed back to the transmitter this feedback process leads to two sources of inaccuracies quantization error the limited rate of the feedback channel restricts the accuracy of the csi at the transmitter delay there is a delay between the time the channel state is measured at the receiver and the time when the infor mation is used at the transmitter the delay comes from the fact that the receivers need some time to receive pi lots estimate csi and then feed it back to the transmitter in a relatively long coding block in time varying wire less channels when the channel information arrives at the transmitter the channel state has already changed much work in the literature has focused on the first issue the general conclusion is that the rate of the feedback channel needed to achieve the perfect csit multiplexing gain scales well with the for example for the mimo broadcast channel it was shown in that the rate of feedback should scale linearly with since the capacity of the mimo broadcast channel also scales linearly with this result says that the overhead from feedback will not overwhelm the capacity gains we now focus on the second issue the issue of feedback delay the standard approach of dealing with feedback delay is to exploit the time correlation of the channel to predict the current channel state from the delayed measurements the predicted channel state is then used in place of the true channel state in a scheme designed assuming perfect csit is available however as the coherence time of the channel becomes shorter compared to the feedback delay due to higher mobility for ex ample the delayed feedback information reveals no information about the current state and a prediction based scheme can offer no multiplexing gain in this paper we raise the question is this a fundamental lim itation imposed by feedback delay or is this just a limitation of the prediction based approach in other words is there an other way to use the delayed feedback information to achieve nontrivial multiplexing gains we answer the question in the affirmative ieee maddah ali and tse completely stale transmitter channel state information is still very useful for concreteness we focus on a channel which has received significant attention in recent years the mimo broadcast channel in particular we focus on a system where the trans mitter has antennas and there are receivers each with a single receive antenna the transmitter wants to send an independent data stream to each receiver to model completely outdated csi we allow the channel state to be independent from one symbol time to the next and the csi is available to both the transmitter and the receivers one symbol time later this means that by the time the feedback reaches the transmitter the current channel is already completely different we also assume that the overall channel matrix is full rank at each time our main result is that for one can achieve a total of degrees of freedom per second per hz in this channel in other words we can achieve a sum rate that scales like as the grows moreover we show that under the further assumption that all receivers have independent and identically distributed i i d channels this is the optimal number of de grees of freedom achievable it is instructive to compare this result with the case when there is no csit and the case when there is perfect csit while the capacity or even the number of degrees of freedom is unknown for general channel statistics when there is no csit in the case when all receivers have identically distributed channels it is easy to see that the total number of degrees of freedom is only since for any we see that at least in that case there is a multiplexing gain achieved by exploiting completely outdated csi however the multiplexing gain is not as good as the number of degrees of freedom achieved in the perfect csit case on the other hand when is large almost linear in why is outdated csit useful when there is perfect csit information intended for a receiver can be transmitted to that receiver without other receivers overhearing it say by using a zero forcing precoder so that there is no cross interference when the transmitter does not know the current channel state this cannot be done and information intended for a receiver will be overheard by other receivers this overheard side informa tion is in the form of a linear combination of data symbols the coefficients of which are the channel gains at the time of the transmission without csit at all this side information will be wasted since the transmitter does not know what the coef ficients are and hence does not know what side information was received in previous transmissions with outdated csit how ever the transmitter can exploit the side information already received at the various receivers to create future transmissions which are simultaneously useful for more than one receiver and can therefore be efficiently transmitted note that there is no such overheard side information in simpler scenarios such as point to point and multiple access channels where there is only a single receiver indeed it is shown in and that for such channels the only role of delayed csit is to predict the current state and when the delayed csit is independent of the current state the delayed csit provides no capacity gains the rest of this paper is structured as follows in section ii the problem is formulated and the main results are stated pre cisely sections iii vi and vii describe the proposed schemes and section iv describes the converse in section v the re gion for the case of is characterized the connection between our results and those for the packet erasure broadcast channel is explained in section viii some follow up results to the conference version of this paper are discussed in section ix we conclude with a discussion of our result in the broader con text of the role of feedback in communication in section x ii problem formulation and main results we consider a complex baseband broadcast channel with transmit antennas and receivers each equipped with a single antenna in a flat fading environment this channel can be mod eled as where denotes transpose conjugate operation and the sequences are i i d and mutually independent in addition we define as we assume that is available at the transmitter and all receivers with one unit let us define as we assume that for any subset of the receivers the transmitter has a message with rate for example message is a common message for receivers one and two similarly or simply is a message for receiver one we define as if then we call an order message or a message of order we define degrees of freedom order as where denotes the capacity region of the channel and denotes the vector of the message rates for each subset of receivers we note that is the well known notion of the degrees of freedom of the channel in this paper we establish the following results theorem as long as is full rank almost surely for each and is stationary and ergodic then for our achievable results hold regardless of what the delay is since they do not depend on the temporal statistics of the channel hence for convenience we will just normalize the delay to be symbol time ieee transactions on information theory vol no july more generally as long as then for example and which are greater than one note that this achievability result holds under very weak assumptions about the channel statistics hence even when is an i i d process over time delayed csit is still useful in achieving a degree of freedom gain the following theorem gives a tight converse under specific assumptions on the channel process theorem if the channel matrices is an i i d process over time and the channels are also i i d across the receivers then the equality between the expressions in and in the case of can be verified using the identity proved in appendix a thus yielding the following corollary corollary if the channel matrices is an i i d process over time and is also i i d across the receivers then the lower bounds in theorem are tight in addition the region of order for the case is characterized as follows theorem if the channel matrices is an i i d process over time and is also i i d across the receivers then the region for the case is characterized as all positive tuples satisfying for all permutations of the set the achievability result for theorem holds for we have the following achievability result for general and theorem assume that is full rank almost surely for each and is stationary and ergodic if is achievable for order symbols then is achievable for order symbols where and starting from which is simply achievable one can use iterative to derive an achiev able with the following closed form for the case of unlike the case of however the expression in does not match the upper bound in theorem in particular this means that theorem does not allow us to characterize the degrees of freedom when the number of users is greater than the number of transmit antennas on the other hand it is easy to verify that the achievable in theorem is increasing with even when therefore unlike the situation with full csit the degrees of freedom under delayed csit is not deter mined by the minimum of the number of transmit antennas and the number of receivers for the special case of and we obtain an exact characterization of the degrees of freedom theorem assume that is full rank almost surely for each and is stationary and ergodic then iii a chievable s cheme for t heorem in this section we explain the achievable scheme for the orem the key is to understand the square case when for simplicity we start with the cases and a achievable scheme for in this section we show that for the case of the of is achievable we explain the achievable scheme from three different perspectives exploiting side information generating higher order messages interference alignment using outdated csit for notational clarity in this section we will use and to denote the two receivers instead of and exploiting side information let and be symbols from two independently encoded gaussian codewords intended for receiver the proposed communication scheme is per formed in two phases which take three time slots in total phase one feeding the receivers this phase has two time slots the first time slot is dedicated to receiver the transmitter sends the two symbols and intended for receiver i e at the receivers we have both receivers and receive noisy versions of linear combi nations of and receiver saves the overheard equation for later usage although it only carries information intended for receiver the second time slot of phase one is dedicated to the second receiver in this time slot the transmitter sends symbols in tended for receiver i e maddah ali and tse completely stale transmitter channel state information is still very useful fig achievable scheme for at receivers we have receiver saves the overheard equation for future usage al though it only carries information intended for receiver let us define short hand notations the transmission scheme is summarized in fig in this figure for simplicity we drop the thermal noise from the received signals we note that assuming is full rank there is a one to one map between and if receiver has the equation overheard by receiver i e then it has enough equations to solve for its own symbols and similarly assuming is full rank there is a one to one map between and if receiver has the equation overheard by receiver i e then it has enough equations to solve for its own symbols and therefore the main mission of the second phase is to swap these two overheard equations through the transmitter phase two swapping overheard equations this phase takes only one time slot at at this time the transmitter sends a linear combination of the overheard equations i e and we note that at this time the transmitter is aware of the csi at and therefore it can form the overheard equations and for example can be formed as at receivers we have remember that receiver already has a noisy version of thus together with it can solve for its two symbols we have a similar situation for receiver remark in this scheme we assume that in the first time slot transmit antenna one sends and transmit antenna two sends however antenna one and two can send any random linear combination of and therefore for example we can have where is a randomly selected matrix similar statement is true for the second time slot at time slot we send however we can send any combination of and in other words where is a randomly selected matrix however we can limit the choice of to rank matrices remark we note that only the number of independent noisy equations that each receiver has is important as long as the variance of the noise of each equation is bounded the is not affected therefore in what follows we ignore noise and just focus on the number of independent equations available at each receiver remark note that if the transmitter has transmit an tennas and each of the receivers has antennas then we can follow the same scheme and achieve of generating higher order symbols we can observe the achievable scheme from another perspective remember in the second phase we send a linear combination of and e g to both receivers we can consider as an order common symbol required by both receivers let us define if we have an algorithm which achieves the degrees of freedom of for order common symbols then we need time slots to deliver the common symbol to both receivers therefore in total we need to deliver four symbols and to the designated receivers thus we have it is easy to see that we can achieve by simply sending to both receivers in one time slot therefore of is achievable in summary phase one takes as input two order symbols for each receiver it takes two time slots to deliver one desired equa tion to each of the receivers therefore each receiver needs one ieee transactions on information theory vol no july fig achievable scheme for phase one more equation to resolve the desired symbols if the transmitter ignores the overheard equations we need two more time slots to deliver one more equation to each receiver and yield the of however by exploiting the overheard equations we can form a common symbol of order delivering one common symbol of order to both receivers takes only one time slot but it simul taneously provides one useful equation to each of the receivers therefore using this scheme we save one time slot and achieve rather than interference alignment using outdated csit putting together the symbols received by receiver over the three time slots we have shown at the bottom of the page from it is easy to see that at receiver the two inter ference streams and arrived from the same directions and therefore and are aligned note that the alignment is done using outdated csit by making the interference data symbols aligned at receiver the two symbols and collapse into one symbol eliminating the variable from we have shown at the bottom of the page which is an equation set of the two desired symbols and it is easy to see that as long as and then the desired data symbols are not aligned at receiver and they can be solved for we note that at is the determinant of the channel matrix indeed in this scheme receiver borrows the antenna of the second receiver at time slot to be able to solve for the two symbols b achievable scheme for in this section we show how we achieve of for the channel with a three antenna transmitter and three single antenna receivers as explained in the previous section we can observe the achievable scheme from three different per spectives however we find the second perspective simpler to follow therefore in the rest of this paper we just explain the algorithm based on the second perspective the achievable scheme has three phases phase one takes order symbols and generates order common symbols phase two takes order common symbols and generates order common symbols the last phase takes order common symbols and deliver them to all three receivers phase one this phase is similar to phase one for the case it takes three independent symbols for each receiver and generates three symbols of order assume that and represent three symbols independently gaussian encoded for receiver therefore in total there are nine data symbols this phase has three time slots where each time slot is dedicated to one of the receivers in the time slot dedicated to receiver the transmitter sends random linear combinations of and over the three antennas similarly in the time slot dedicated to receiver the transmitter sends random linear combinations of and over the three antennas in the time slot dedicated to receiver the transmitter sends random linear combinations of and over the three antennas refer to fig for details so far the algorithm has taken three time slots and delivered three desired equations to the designated receivers therefore in terms of counting the desired equations the algorithm delivers one equation per time slot which is natural progress for a system without csit if we ignore the overheard equations then we need six more time slots to successfully deliver the nine data streams which yield the of however as described in the maddah ali and tse completely stale transmitter channel state information is still very useful fig achievable scheme for phase two case the overheard equations can help us to improve the degrees of freedom let us focus on the time slot dedicated to receiver then we have the following observations the three equations and form three linearly independent equations of and almost surely if we somehow deliver the overheard equations and to receiver then it has enough equations to solve for and the two overheard equations and plus the equation received by receiver i e fully represent the original data symbols therefore sufficient information to solve for the data symbols is already available at the receivers but not exactly at the desired receiver we have similar observations about the equations received in the time slots dedicated to receivers and remember that originally the objective was to deliver and to receiver after these three transmissions we can redefine the objective the new objective is to deliver the overheard equations and to receiver the overheard equations and to receiver and the overheard equations and to receiver let us define as a random linear combination of and to be specific let then we have the following observations if receiver has then it can use the saved overheard equation to obtain remember is a desired equation for receiver if receiver has then it can use the saved overheard equation to obtain remember is a desired equation for receiver therefore is desired by both receivers and simi larly we define which is desired by receivers and and define which is desired by re ceivers and we note that if receiver has and then it has enough equations to solve the original data symbols and similarly it is enough that receiver has and and receiver has and therefore again we can redefine the objective as delivering to receivers and to receivers and and to receivers and suppose now we have an algorithm that can achieve degrees of freedom for order common symbols then the total time to deliver the original nine data symbols is the initial three time slots of sending linear combinations of the nine symbols plus time slots to deliver the three order symbols generated therefore the overall dof to send the order symbols is given by it is trivially easy to achieve which yields of however aswewillelaborateinthefollowing we can do better phase two phase one of the algorithm takes order symbols and generates order symbols to be delivered phase two takes order symbols and generates order symbols phases two and three together can also be viewed as an algorithm which delivers order common symbols assume that and represent two symbols that are desired by both receivers and similarly and are required by both receivers and and and are required by both receivers and therefore in total there are six order symbols we notice that phase one generates only three order symbols to provide six order symbols we can simply repeat phase one twice with new input symbols phase two takes three time slots where each time slot is dedicated to one pair of the receivers in the time slot dedicated to receivers and the transmitter sends random linear combinations and from two of the transmit antennas we have analogous transmissions in the other two time slots for details see fig in fig we focus on the first time slot dedicated to both users and then we have the following important observations and form two linearly in dependent equations of and almost surely similarly and form two linearly independent equations of and almost surely if is somehow delivered to both receivers and then both receivers have enough equations to ieee transactions on information theory vol no july solve for and therefore which is overheard and saved by receiver is simultaneously useful for receivers and we have similar observations about the received equations in the other two time slots therefore after these three time slots we canredefinetheobjectiveoftherestofthealgorithmasdelivering to receivers and to receivers and and to receivers and let us define and as any two linearly indepen dent combinations of and and where the constants and have been shared with receivers if we somehow deliver and to receiver then together with its saved overheard equation receiver has three linearly independent equations to solve for and then it has enough equations to solve for and we have the similar situation for receivers and therefore it is enough to deliver and to all three receivers if we have an algorithm that can provide degrees of freedom to deliver order common symbols then the total time to deliver the original six order common symbols is taking into account the first three transmissions described in fig therefore we have phase three phase three transmits order common sym bols this phase is very simple assume that is required by all three receivers then the transmitter can use only one transmit antenna and send all three receivers will re ceive a noisy version of therefore we use one time slot to send one order symbol therefore then from and we conclude that and c general proof of achievability for theorem in this section we explain the achievable scheme for the gen eral case in theorem first we focus on the general square case the algorithm is based on a concatenation of phases phase takes symbols of order and generates symbols of order for the phase is simple and generates no more symbols for each we can also view phases together as an algorithm whose job is to deliver common symbols of order to the receivers the th phase takes common symbols of order and yields symbols of order this phase has time slots with each time slot dedicated to a subset of receivers we denote the time slot ded icated to the subset by in this time slot the transmitter sends random linear combinations of the symbols desired by all the receivers in the transmitter utilizes of the transmit antennas the linear combination of the transmitted symbols received by receiver is denoted by let us focus on the linear com binations of the transmitted symbols received by all receivers in time slot we have the following observations for every the equations consisting of one equation and the overheard equations are linearly independent equations of the symbols this relies on the fact that the transmitter uses transmit antennas for any if we somehow deliver the equa tions to receiver then receiver has linearly independent equations to solve for all symbols having the aforementioned two observations we can say that the overheard equation by receiver is simultaneously useful for all receivers in after repeating the aforementioned transmission for all where and we have another important ob servation consider any subset of receivers where then each receiver has an overheard equation which is simultaneously useful for all the receivers in we note that the transmitter is aware of these over heard equations for every the transmitter forms random linear combinations of de noted by we note that is simultaneously useful for all receivers in indeed each re ceiver in can subtract the contribution of from and form linearly independent combina tions of using the previous procedure the transmitter generates symbols of order the important observation is that if these symbols are deliv ered to the designated receivers then each receiver will have enough equations to solve for all of the original common sym bols of order delivering order symbols takes using an algorithm that provides degrees of freedom for order symbols since the phase starts with symbols of order and takes time slots and generates symbols with order we have or it is also easy to see that is achievable solving the recursive equation we have maddah ali and tse completely stale transmitter channel state information is still very useful in particular therefore the achievablity of theorem in the square case has been established now observe that in the aforementioned algorithm phase only requires the use of transmit antennas not all of the transmit antennas moreover common symbols of order are delivered using phases hence we conclude that the degree of freedom of order messages achieved above in the square system can actually be achieved in a system with less transmit antennas as long as this proves theorem in the rectangular case as well remark we note that if the transmitter has transmit antennas and each of the receivers has receive antennas then the of isachievable moregenerally inthischannel fororder sym bols the of is achievable d implementation issues for simplicity the proposed scheme has been presented in a symbol by symbol based format however this scheme can be implemented in a block by block fashion as well this would allow us to exploit the coherence of the channel over time and frequency to reduce channel training and feedback overhead to be specific let us again focus on the case of consider a block of time frequency resources consecutive in time and frequency let us assume that in the first phase of the scheme we dedicate half of these resources to receiver and the other half to receiver to start the second phase the trans mitter needs to know channel coefficients during the first phase for example if the lengths of the block in time and frequency are respectively less than coherent time and bandwidth of the channel then during the first phase the channel coefficients are almost constant therefore to start the second phase the trans mitter needs only to know the four channel coefficients let us denote the coherent time and bandwidth by and respec tively then for each time frequency resources the trans mitter needs to dedicate at least two time frequency resources to send orthogonal pilot signals and learn four coefficients through feedback then the transmitter uses the remaining resources to send order symbols remember that the transmitter is also required to report the channel coefficients of each re ceiver to the other receiver since each receiver knows its own csi the transmitter can exploit that and send to both receivers the two symbols of and as the symbols of order in the second phase therefore the second phase takes resource units for order messages following the previous argument the scheme can achieve of if as in most wireless channels then the degree of freedom is close to iv outer bound in this section we aim to prove theorem in this theorem we focus on the degrees of freedom of the channel for order messages therefore we assume for every subset with car dinality of receivers the transmitter has a message with rate and degrees of freedom remember in section ii we assume that the csi is available to all nodes with one time unit delay as an outer bound we consider the capacity of a channel in which the csi at time is available to all receivers instantaneously at time therefore attime receiver has forany on the other hand the transmitter has not only the csi but also received signals both with one unit delay therefore attime thetransmitterhas now we improve the resultant channel even further as follows consider a permutation of the set we form a receiver broadcast channel by giving the output of the receiver to the receivers for all therefore we have an upgraded broadcast channel referred to as improved channel with receivers as shown at the bottom of the page we denote the capacity of the resultant channel as denoting the capacity of the original channel with we obviously have moreover it is easy to see that the improved channel is physi cally degraded in the improved channel consider message which is re quired by all receivers listed in let be the smallest in teger where then due to the degradedness of the channel if is decoded by receiver then it can be de coded by all other receivers in therefore we can assume that is just required by receiver using this argument we can simplify the messages requirements from order common messages to pure private messages as follows receiver re quires all messages where and similarly receiver requires all messages where and we follow the same argument for all receivers according to feedback does not improve the capacity of the physically degraded broadcast channels consequently we focus on the capacity region of the improved channel without feedback and with the new private message set on the other ieee transactions on information theory vol no july hand for broadcast channels without feedback the capacity re gion is only a function of marginal distributions therefore we can ignore the coupling between the receivers in the improved channel thus we have a broadcast channel where receiver has antennas and the distributions of the channels between the transmitter and any of the receive antennas are identical moreover receiver is interested in all messages where and therefore according to huang et al extended by vaze and varanasi one can conclude that by applying the same procedure for any permutation of the set and then adding all of the resulting in equalities the theorem follows v region for in this section we prove theorem which characterizes the region of the channel for the case we note that the region of theorem is the polyhedron pro posed by the outer bound for order messages where here we show by induction on that the region is achiev able the hypothesis is clearly true for now assume that the hypothesis is true for consider the case when first we argue that any point in the polyhedron such that for all and for some cannot be a corner point of the polyhedron without loss of generality we can assume that the coordinates of such a point is ordered in a nondecreasing order since the polyhedron is invariant to permutation of coordinates let be such that either or and now a direct calculation shows that is a permutation of which maximizes among all permutations if and only if when ever this means that the only constraints if any of the polyhedron that satisfies with equality corre spond to permutations satisfying for all and for all all other constraints are satisfied with strict inequality we define vector as for for otherwise and the point continue to satisfy the tight inequalities with equality more over for sufficiently small the constraints that are not tight on remain not tight on these two points hence both these points lie in the polyhedron and hence which is the average between these points cannot be a corner point thus the only point in the strict positive quadrant that can be a corner point of the polyhedron is the point this point is achievable by theorem any other point in the polyhedron is a convex combination of this point and points for which some of the coordinates are zero each one of these latter points is in fact in the polyhedron for some smaller value of by the induction hypothesis each of these points is achievable hence by time sharing any point in the polyhedron for is achievable vi achievable scheme for theorem in section iii we explained an algorithm to achieve when more generally we character ized when in this section we extend the optimal achievable scheme of section iii and de velop a suboptimal algorithm for the case that for order messages we first focus on the case and a achievable scheme for from theorems and we have and however for order messages we only know from the outer bound that on the other hand in terms of achievability it is easy to see that which can be achieved by simply ignoring one of the receivers now the question is whether is indeed the same as or the extra receiver can be exploited to achieve beyond here we propose an algorithm to show that the achievable scheme is as follows let and be four symbols for receiver the first phase of the scheme has six time slots the first two time slots are dedicated to receiver in these two time slots the transmitter sends four random linear combinations of and through the two transmit antennas as a particular example in the first time slot the transmitter sends and and in the second time slot it sends and refer to fig for details similarly in time slots and the transmitter sends four random linear combinations of and in time slots and the transmitter sends four random linear com binations of and an explicit calculation shows that for any point both the referring to fig we have the following observations receiver already has two independent linear equations and of and therefore it needs two more equations maddah ali and tse completely stale transmitter channel state information is still very useful fig suboptimal scheme for and the first phase the four overheard equations in and are not linearly indepen dent from what receiver has already received i e and we can purify the four overheard equations and form two equations that are linearly independent with and for example receiver can form as a random linear combination of and similarly receiver can form as a random linear combination of and the coefficients of these linear combinations have been preselected and shared among all nodes it is easy to see that almost surely and are linearly independent of and if somehow we deliver and to receiver then it has enough equations to solve for and similarly as shown in fig we can purify the over heard equations in time slots dedicated to receivers and now the available side information and the require ments are the same as those we had after phase one for the case of see section iii b equations and are available at receivers and respectively and are needed by receiver equations and are available at receivers and respectively and are needed by receiver and equations and are available at receivers and respectively and are needed by receiver we define considering the available overheard equations at each re ceiver one can easily conclude that is needed by both re ceivers and is needed by both receivers and and is needed by both receivers and the transmitter needs time slots to deliver these three order sym bols where according to theorem in sum mary phase one starts with order messages takes six time slots and generates order symbols therefore we achieve which is strictly greater than therefore the proposed achievable scheme exploits the extra receiver to im prove however we notice that the achieved of is still less than which is suggested by the outer bound b general proof for theorem here we explain a general version of the proposed algorithm again the algorithm includes phases phase takes symbols of order meaning that it is needed by receivers simultaneously and generates symbols of order for the phase is simple and generates no more symbols let us define as in addition we define as the greatest common factor of and i e phase takes symbolsoforder andyields symbols with order this phase has sub phases where each subphase is dedicated to a subset of the receivers the subphase dedicated to subset is de noted by each subphase takes time slots in the transmitter sends random linear combinations of symbols desired by all receivers in the transmitter uses at least of the transmit antennas the linear equation of the transmitted sym bols received by receiver in the th time slot of is denoted by let us focus on the equations of the trans mitted symbols received by all receivers in we have the following observations for every and the equations are not necessarily linearly independent the reason is that while the number of transmit antennas is which can be less than indeed among the overheard equations ieee transactions on information theory vol no july fig alternative achievable scheme for we can only form overheard equations that are simul taneously useful to receiver for any in therefore among overheard equations in we can form only overheard equations that are useful for any receiver we purify the overheard linear combinations to this end receiver forms linear combinations of the resultant equations are de noted by the coeffi cients of the linear combinations have been preselected and shared among all nodes it is easy to see that for every the following equations are linearly indepen dent and and therefore if we somehow deliver and to receiver then it will have linearly independent equa tions to solve for all desired symbols having the previous two observations we note that the purified linear combinations by receiver are simultaneously useful for all receivers in after repeating the previous transmission for all where and we have another important property con sider a subset of the receivers where then each receiver has purified linear combination which are simultaneously useful for all receivers in we note that the transmitter is aware of these purified equations through delayed csit for every the transmitter forms random linear combinations of denoted by we note that is simultaneously useful for all receivers in the reason is that each receiver can subtract the contributions of from and form linearly independent combinations of therefore using the aforemen tioned procedure the transmitter forms symbols with order the important observation is that if these symbols are delivered to the designated receivers then each re ceiver will have enough equations to solve for all designated messages with order in summary this phase takes symbols of order takes time slots and yields symbols of order if we have a scheme which achieves for order symbols then we achieve or vii i mproved s cheme for recall that the scheme of section vi achieves of the achieved is greater that which shows that we could exploit the extra receiver with respect to the number of transmit antennas however it is still smaller than which is suggested by the outer bound now the question is whether the achievable scheme or the outer bound is loose in what follows we show that for and the outer bound is tight and the achievable scheme of section vi is loose before that we explain an alternative solution for a system with the idea of the alternative solution is the key to achieve the optimal for the systems with and a alternative scheme for phase one of the algorithm takes order messages let us as sume that the transmitter has and for receiver and and for receiver here phase one takes only one time slot which is dedicated to both receivers in this time slot the trans mitter sends random linear combinations of all four symbols and and refer to fig to see the details of partic ular examples for the linear combinations receiver receives a linear combination of all four symbols we denote this linear combination by where represents the contribution of and and rep resents the contribution of and similarly receiver receives a linear combination of all four symbols denoted by then we have the following observations maddah ali and tse completely stale transmitter channel state information is still very useful fig optimal scheme for a system with and the first phase if we somehow give to receiver then receiver can compute by subtracting from what it already has then if we also give to receiver then it has two equations to solve for and if we somehow give to receiver then receiver can compute by subtracting from what it already has then if we also give to receiver then it has two equations to solve for and in other words both receivers and want and therefore we can define two order symbols and as in summary this phase starts with four order symbols takes one time slot and provides two order symbols two order symbols take time slots to deliver therefore we achieve since this scheme achieves b optimal scheme for and here we explain an algorithm for the systems with and the first phase of this algorithm takes order messages takes three time slots and gives order symbols this subalgorithm leads to an optimal scheme for systems with and let and be four symbols for receiver in the first time slot which is dedicated to receivers and the transmitter sends random linear combinations of four symbols and and refer to fig to see the details of particular realizations for the linear combina tions receiver receives a linear combination of all four sym bols denoted by receivers and also receive linear combinations of all four symbols denoted by and respectively in the second time slot which is dedicated to re ceivers and the transmitter sends random linear combi nations of four symbols and and in the third time slot which is dedicated to receivers and the trans mitter sends random linear combinations of four symbols and by referring to fig it is easy to see that for each receiver to solve for all four desired symbols it is enough that receiver has and receiver has and receiver has and therefore the transmitter needs to deliver and to both receivers and and to both receivers and and to both receivers and therefore we have six order symbols as therefore the transmitter needs more time slots to de liver these six order symbols thus we have where we used theorem to set note the outer bound in theorem yields and there fore this algorithm meets the outer bound this result shows that the scheme of section vi is in general suboptimal viii connections with the packet erasure broadcast channel the schemes we proposed in this paper are inspired by schemes designed for the packet erasure broadcast channel where each receiver observes the same transmitted packet but with a probability of erasure and acknowledgement feedback is received by the transmitter from both receivers here the delayed csi that is fed back to the transmitter is the erasure states of the previous transmissions the goal of these packet erasure broadcast schemes is to ex ploit the fact that a packet intended for a receiver may be erased at that receiver but received at other receivers these overheard packets become side information that can be exploited later the ieee transactions on information theory vol no july basic scheme initially proposed by for unicast setting and then by for multicasting setting in the two receiver case works as follows the transmitter sends packets intended for each receiver separately if a packet is received by the intended receiver then no extra effort is needed for that packet but if a packet is received by the nonintended receiver and not re ceived by intended receiver that receiver keeps that packet for later coding opportunity let us say packet intended for re ceiver is received by receiver and packet intended for receiver is received by receiver in this case the transmitter sends then if receiver receives it it can re cover by subtracting and if receiver receives it it can recover by subtracting in the outer bound of is used to show that the scheme of is optimal in and this two receiver scheme is extended to more than two re ceivers when all receivers have identical erasure probability the scheme we proposed in this paper for the mimo broadcast channel can be viewed as the counterpart to this scheme for the packet erasure broadcast channel ix f ollow up results after the conference version of this paper has appeared in the problem of exploiting outdated csit in networks have been investigated in several pieces of work in it is shown that for three user interference channels and two user x chan nels outdated csit can be used to achieve more than one in for two user x channels the result of has been improved and for three user case an achievable has been proposed in an achievable for user single an tenna interference channels has been derived in the regions of two user and three user mimo broadcasts chan nels and two user mimo interference channels with delayed csit are studied in the load of feedback to implement the proposed scheme is evaluated it is shown that for a wide and practical range of channel parameters the scheme of this paper outperforms zero forcing precoding and also single user transmission x conclusion from the point of view of the role of feedback in information theory this work provides yet another example that feedback can be useful in increasing the capacity of multiuser channels even when the channels are memoryless this is in contrast to shannon pessimistic result that feedback does not increase the capacity of memoryless point to point channels in the spe cific context of broadcast channels ozarow has in fact al ready shown that feedback can increase the capacity of gaussian scalar nonfading broadcast channels however the nature of the gain is unclear as it was shown numerically moreover the gain is quite limited we argue that the mimo fading broadcast channel considered in this paper provides a much more inter esting example of the role of feedback the nature of the gain is very clear in contrast to the gaussian scalar nonfading broad cast channel the main uncertainty from the point of the view of the transmitter is the channel direction rather than the addi tive noise particularly in the high regime this means that although the mimo channel has intrinsically multiple degrees of freedom the transmitter cannot segregate it into multiple or thogonal channels one for each receiver hence when transmit ting information for one receiver significant part of that infor mation is overheard at other receivers this overheard informa tion becomes side information that can be exploited in future transmissions the role of feedback is to provide the channel directions to the transmitter after the transmission to allow the transmitter to determine the side information that was received at the receivers overall feedback leads to a much more ef ficient use of the intrinsic multiple degrees of freedom in the mimo channel yielding a multiplexing gain over the nonfeed back case energy harvesting is a promising solution to pro long the operation of energy constrained wireless networks in particular scavenging energy from ambient radio signals namely wireless energy harvesting weh has recently drawn significant attention in this paper we consider a point to point wireless link over the narrowband flat fading channel subject to time varying co channel interference it is assumed that the receiver has no fixed power supplies and thus needs to replenish energy oppor tunistically via weh from the unintended interference and or the intended signal sent by the transmitter we further assume a single antenna receiver that can only decode information or harvest energy at any time due to the practical circuit limitation therefore it is important to investigate when the receiver should switch between the two modes of information decoding id and energy harvesting eh based on the instantaneous channel and interference condition in this paper we derive the optimal mode switching rule at the receiver to achieve various trade offs between wireless information transfer and energy harvesting specifically we determine the minimum transmission outage probability for delay limited information transfer and the maximum ergodic capacity for no delay limited information transfer versus the maximum average energy harvested at the receiver which are characterized by the boundary of so called outage energy region and rate energy region respectively moreover for the case when the channel state information csi is known at the transmitter we investigate the joint optimization of transmit power control information and energy transfer scheduling and the receiver mode switching the effects of circuit energy consumption at the receiver on the achievable rate energy trade offs are also characterized our results provide useful guidelines for the efficient design of emerging wireless communication systems powered by opportunistic weh index terms energy harvesting wireless power transfer power control fading channel outage probability ergodic ca pacity i introduction i n as conventional sensor networks energy constrained the lifetime wireless of the network networks is such an important performance indicator since sensors are usually equipped with fixed energy supplies e g batteries which are of limited operation time recently energy harvesting has become an appealing solution to prolong the lifetime of manuscript received april revised august and october accepted october the associate editor coordinating the review of this paper and approving it for publication was w su this paper has been presented in part at the ieee international symposium on information theory isit cambridge ma usa july l liu r zhang and k c chua are with the department of electrical and computer engineering national university of singapore e mail liu wireless networks unlike battery powered networks energy harvesting wireless networks potentially have an unlimited energy supply from the environment consequently the re search of wireless networks powered by renewable energy has recently drawn a great deal of attention see e g and references therein in addition to other commonly used energy sources such as solar and wind ambient radio signals can be a viable new source for wireless energy harvesting weh since radio signals carry information as well as energy at the same time an interesting new research direction namely simultaneous wireless information and power transfer has recently been pursued the above prior works have studied the fundamental performance limits of wireless information and energy transfer systems under different channel setups where the receiver is assumed to be able to decode the information and harvest the energy from the same signal which may not be realizable yet due to practical circuit limitations consequently a so called time switching scheme where the receiver switches over time between decoding information and harvesting energy was proposed in and as a practical design in this paper we investigate further the time switching scheme for a point to point single antenna flat fading channel subject to time varying co channel interfer ence as shown in fig our motivations for investigating time switching are as follows firstly with time switching off the shelf commercially available circuits that are separately designed for information decoding and energy harvesting can be used thus reducing the receiver complexity as compared to other existing designs e g power splitting and integrated receiver secondly time switching judiciously exploits the facts that information and energy receivers in practice operate with very different power sensitivity e g for energy receivers versus for information receivers and wireless transmissions typically experience time varying channels e g due to shadowing and fading and or interferences e g in a spectrum sharing environment which fluctuate in very large power ranges e g tens of dbs therefore a time switching receiver can utilize both the energy information receiver power sensitivity difference and channel interference power dynamics to optimize its switching operation for example the receiver can be switched to harvest energy when the channel or interference is strong or decode information when the channel or interference is relatively liang elezhang eleckc nus edu sg r zhang is also with the institute for weaker in this paper we assume that the transmitter has a fixed infocomm research a star singapore the work was supported in part by the national university of singapore under research grant r 133 digital object identifier twc 113012 power supply e g battery whereas the receiver has no fixed power supplies and thus needs to replenish energy via weh from the received interference and or signal sent by the c ieee liu et al wireless information transfer with opportunistic energy harvesting transmitter we consider an opportunistic weh at the single antenna receiver i e the receiver can only decode information or harvest energy at any given time but not both as a result the receiver needs to decide when to switch between an information decoding id mode and an energy harvesting eh mode based on the instantaneous channel gain and interference power which are assumed to be perfectly known at the receiver in this paper we derive the optimal mode switching rule at the receiver to achieve various trade offs between the minimum transmission outage probability if the information transmission is delay limited or the maximum ergodic capacity if the information transmission is not delay limited in id mode versus the maximum average harvested energy in eh mode which are characterized by the boundary of the so called outage energy o e region and rate energy r e region respectively moreover for the case when the channel state information csi is known at both the transmitter and the receiver we examine the optimal design of transmit power control and scheduling for information and energy transfer jointly with the receiver mode switching to achieve different boundary pairs of the o e region or r e region one important property of the proposed optimal resource allocation scheme is that the received signals with large power should be switched to the eh mode rather than id mode which is consistent with the fact that the energy receiver in general has a poorer sensitivity larger received power than the information receiver it is worth noting that from a traditional viewpoint inter ference is an undesired phenomenon in wireless communica tion since it jeopardizes the wireless channel capacity if not being decoded and subtracted completely in the literature fundamental approaches have been applied to deal with the interference in wireless information transfer e g decoding the interference when it is strong or treating the interference as noise when it is weak recently another approach namely interference alignment was proposed where interference signals are properly aligned in a certain subspace of the received signal at each receiver to achieve the maximum degrees of freedom dof for the sum rate different from the above works this paper provides a new approach to deal with the interference by utilizing it as a new source for weh however the fundamental role of interference in emerging wireless networks with simultaneous information and power transfer still remains unknown and is thus worth further investigation it is also worth pointing out that recently another line of research on wireless communication with energy harvesting nodes has been pursued see e g and references therein these works have addressed energy management policies at the transmitter side subject to intermittent and random harvested energy which are thus different from our work that mainly addresses opportunistic wireless energy harvesting at the receiver side the rest of this paper is organized as follows section ii presents the system model and illustrates the encoding and decoding schemes for wireless information transfer with opportunistic energy harvesting section iii defines the o e and r e regions and formulates the problems to characterize their boundaries sections iv and v present the optimal point to point wireless link tx rx information decoder co channel interference energy harvester fig system model mode switching rules at the receiver and power control and scheduling polices for information and energy transfer at the transmitter if csi is known to achieve various o e and r e trade offs respectively section vi extends the optimal decision rule of the receiver to the case where the receiver energy consumption is taken into consideration section vii provides numerical results to evaluate the performance of the proposed schemes as compared against other heuristic schemes finally section viii concludes the paper ii system model as shown in fig this paper considers a wireless point to point link consisting of one pair of single antenna transmitter tx and receiver rx over the flat fading channel it is assumed that there is an aggregate interference at rx which is within the same bandwidth as the transmitted signal from tx and changes over time for convenience we assume that the channel from tx to rx follows a block fading model since the coherence time for the time varying interference is in general different from the channel coherence time we choose the block duration to be sufficiently small as compared to the minimum coherence time of the channel and interference such that they are both assumable to be constant during each block transmission it is worth noting that the above model is an example of the block interference channel introduced in the channel power gain and the interference power at rx for one particular fading state are denoted by h ν and i ν respectively where ν denotes the joint fading state it is assumed that h ν and i ν are two random variables rvs with a joint probability density function pdf denoted by f ν h i at any fading state ν h ν and i ν are assumed to be perfectly known at rx in addition the additive noise at rx is assumed to be a circularly symmetric complex gaussian cscg rv with zero mean and variance we consider block based transmissions at tx and the time switching scheme at rx for decoding information or harvesting energy at each fading state next we elaborate the encoding and decoding strategies for our system of interest in the following two cases case i h ν and i ν are unknown at tx for all the fading states of ν referred to as csi unknown at tx and case ii h ν and i ν are perfectly known at tx at each fading state ν referred to as csi known at tx csit first consider the case of csi unknown at tx as shown in fig a in this case tx transmits information continuously with constant power p for all the fading states due to the lack of csit at each fading state ν rx decides whether to decode the information or harvest the energy from the received signal based on h ν and i ν for example as shown in fig a time slots and are switched to eh mode at rx while time slot is switched to id mode for convenience we define an ieee transactions on wireless communications vol no january fig encoding and decoding strategies for wireless information transfer with opportunistic weh via receiver mode switching the height of the block shown in the figure denotes the signal power indicator function to denote the receiver mode switching at any given ν as follows ρ ν id mode is active eh mode is active next we consider the case of csi known at tx i e the channel gain h ν and interference power i ν are known at tx for each fading state ν in this case tx is able to schedule transmission for information and energy transfer to rx based on the instantaneous csi as shown in fig b tx allocates time slot for energy transfer time slot for information transfer and transmits no signals in time slot accordingly rx will be in eh mode i e ρ ν to harvest energy from the received signal including the interference in time slot or solely from the received interference in time slot but in id mode i e ρ ν to decode the information in time slot in addition to transmission scheduling tx can implement power control based on the csi to further improve the information energy transmission efficiency let p ν denote the transmit power of tx at fading state ν in this paper we consider two types of power constraints on p ν namely average power constraint apc and peak power constraint ppc the apc limits the average transmit power of tx over all the fading states i e e ν p ν p avg where e ν denotes the expectation over ν in contrast the ppc constrains the instantaneous transmit power of tx at each of the fading states i e p ν p peak ν without loss of generality we assume p avg p peak for convenience we define the set of feasible power allocation as p p ν e ν p ν p avg p ν p peak ν iii information transfer and energy harvesting trade offs in fading channels in this paper we consider three performance measures at rx which are the outage probability and the ergodic capacity for wireless information transfer and the average harvested energy for weh for delay limited information transmission outage probability is a relevant performance indicator assum ing that the interference is treated as additive gaussian noise at rx and the transmitted signal is gaussian distributed the h i id ρ tx rx eh ρ a csi unknown at tx information transmission only with constant power h i id ρ tx rx eh ρ b csi known at tx scheduled information and energy transmission with power control information signal energy singal interference signal instantaneous mutual information imi for the tx rx link at fading state ν is expressed as r ν ρ ν log note that r ν if rx switches to eh mode i e ρ ν thus considering a delay limited transmission with constant rate r h ν p ν following the outage probability at rx can be expressed as ε pr r ν r where pr denotes the probability for information transfer without csit the receiver aware outage probability is usually minimized with a constant transmit power i e p ν p avg p ν whereas in the case with csit the transmitter aware outage probability can be further minimized with the truncated channel inversion based power allocation next consider the case of no delay limited information transmission for which the ergodic capacity is a suitable performance measure expressed as r e ν r ν for information transfer if csit is not available the ergodic capacity can be achieved by a random gaussian codebook with constant transmit power over all different fading states however with csit the ergodic capacity can be further maximized by the water filling based power allocation on the other hand the amount of energy normalized to the transmission fading i ν state ν is expressed block duration as q ν that can α ρ ν be harvested h ν p ν at rx at where α is a constant that accounts for the loss in the energy transducer for converting the harvested energy to electrical energy to be stored for convenience it is assumed that α in this paper moreover since the background thermal noise has constant power for all the fading states and is typically a very small amount for energy harvesting we may ignore it in the expression of q ν thus in the rest of this paper we assume q ν ρ ν h ν p ν i ν the average energy that can be harvested at rx is then given by q avg e ν q ν it is easy to see that there exist non trivial trade offs in assigning the receiver mode ρ ν and or transmit power p ν in the case of csit to balance between minimizing the outage probability or maximizing the ergodic capacity for information transfer versus maximizing the average harvested energy for weh to characterize such trade offs for the case when information transmission is delay limited we introduce a so called outage energy o e region defined below that consists of all the achievable non outage probability defined as δ ε with outage probability ε given in and average harvested energy pairs for a given set of transmit power constraints while for the case when information transmission is not delay limited we use another rate energy r e region i ν liu et al wireless information transfer with opportunistic energy harvesting defined below that consists of all the achievable ergodic capacity and average harvested energy pairs more specifically in the case without w o csit the corresponding o e region is defined as c non outage probability o e region with csit o e region without csit δ min q max w o csit o e δ q avg ρ ν ν y g r e n δ pr r ν r δ min while in the case with csit the o e region is defined as cwith o e csit q avg e ν q ν e d e t e v r a h q max δ q avg δ max p ν p ρ ν ν q min δ max q min δ pr r ν r q avg e ν q ν a o e region on the other side in the case without csit the r e region is defined as c ergodic capacity r e region with csit r q avg r min r e region without csit w o csit e e q max ρ ν ν y r e ν while in the case with csit the r e region is defined as r min cwith r e csit r ν q avg e ν q ν g r e n e d e t e v r a h d e g a r e q max r q avg v a p ν p ρ ν ν r max r e ν r max q min fig a and fig b show examples of the o e region without or with csit see sections iv a and iv b for the b r e region details of computing the o e regions for these two cases and the r e region without or with csit see sections v a and fig examples of o e region and r e region with or without csit v b for the corresponding details respectively it is assumed that p avg for the o e region we introduce the following indicator function for the event of non outage transmission at fading state ν for the convenience of our subsequent analysis x ν q min r ν q avg e ν q ν p peak r h ν and i ν are independent exponentially distributed rvs with mean and respectively it is observed that csit helps improve both the achievable outage energy and rate energy trade offs it is observed from fig that in each region there are two boundary points that indicate the extreme performance limits namely δ max if r ν r otherwise q min and δ min q max for the o e region or r max q min and r min q max for the r e region for brevity characterizations of these vertex points are given in appendix it thus follows that the non outage reformulated as δ pr r ν r probability δ can be since the optimal trade offs between the non outage prob ability ergodic capacity and the average harvested energy are characterized by the boundary of the corresponding o e r e region it is important to characterize all the boundary δ q avg e ν x ν then we consider the following two optimization problems maximize ρ ν e ν x ν or r q avg pairs in each case with or without csit from fig it is easy to observe that if q avg q min subject to e ν q ν q ρ ν ν the non outage probability δ max or ergodic capacity r max can still be achieved for both cases with and without csit maximize p ν ρ ν thus the remaining boundary of the o e region yet to be characterized is over the intervals q min e ν x ν q avg q max and δ min δ δ max while that of the r e region is over the intervals q min q avg q max and r min r r max subject to e ν q ν q p ν p ν ρ ν ν ieee transactions on wireless communications vol no january where q is a target average harvested energy required to maintain for all the q receiver min q operation by solving q max we are problem or able to characterize where energy λ constraint is the q dual variable associated with the harvested then the lagrange dual function of problem is expressed as the entire boundary of the o e region for the case without csit defined in or with csit defined in similarly for the r e region we consider the following two g λ ρ ν ν max optimization problems maximize ρ ν l ρ ν λ the maximization problem can be decoupled into parallel subproblems all having the same structure and each for one e ν r ν fading state for a particular fading state ν the associated subject to e ν q subproblem is expressed as ρ ν ν ρ max maximize p ν ρ ν q ν lo e ν ρ e ν r ν subject to e ν q ν q where index ν lo e for ν ρ x λq note that we have dropped the the fading state for brevity to solve problem we need to compare the values of p ν p ν lo e ν ρ ν ν then by solving problem or for all q min ρ for ρ and ρ it follows from and that when ρ q lo e ν if h q max we can characterize the boundary of the r e region for ρ the case without csit defined in or with csit defined in it is observed that the objective function of problem and when ρ lo e ν is in general not concave in p ν even if ρ ν are given furthermore due to the integer constraint ρ ν ν problems are in general non convex optimization problems however it can be verified that all of them satisfy the time sharing condition given in to show this for problem let φ ρ λhp λi thus the optimal solution to problem is obtained as ρ if h and otherwise λhp λi given the harvested ρb ν energy q energy denote constraint the optimal q problem value and ρa ν and with a given λ problem can be efficiently solved by solving problem for different fading states problem denote constraints the qa optimal and qb solutions respectively given the harvested we need to prove that for any θ there one qc ν is then solved by iteratively solving problem with a fixed λ and updating λ via a simple bisection method until the harvested energy constraint is met with equality solution θ φ qb ρc ν ρc ν and such that e ν h ν p qc ν i ν e θ ν qa xc ν always exists θ θφ qb at qa where least and xc ν is defined accordingly as in due to the space limitation the above next we examine the optimal solution ρ to problem to gain more insights to the optimal receiver mode switching in the case without csit with a given harvested energy con straint proof is omitted here in fact the time sharing condition implies that φ q we define the region on the h i plane consisting of all the points h i for which the optimal solution to problem q is concave in q which then guarantees the zero duality gap for problem according to the convex analysis in similarly it can be shown that strong duality is ρ versus ρ as the optimal id region versus the optimal eh region furthermore let λ denote the optimal dual solution to problem corresponding to the given holds for problems therefore in the following two sections we apply the lagrange duality method to solve problems to obtain the optimal o e and r e trade offs respectively iv outage energy trade off in this section we study the optimal receiver mode switch ing without with transmit power control to achieve different trade offs between the minimum outage probability and the maximum average harvested energy for both cases without and with csit by solving problems and respectively a the case without csit optimal receiver mode switching we first study problem for the csit unknown case to derive the optimal rule at rx to switch between eh and id modes the lagrangian of problem is formulated as l ρ ν λ e ν q then from the optimal id region for problem is expressed as d id λ h i h the rest of the non negative h i plane is thus the optimal eh region i e d eh λ where λ d id denotes the two dimensional nonnegative real do main and a b denotes the set x x a and x b with an q q illustration of d id λ λ is shown in fig constraint and d eh q min we it is noted that to meet the harvested energy need to sacrifice increase the outage prob e ν ability for information transfer fading states in the region h x ν λ q ν q by h allocating i log some non outage hp i λ hp λ i h i er p p otherwise i i er e p r i r liu et al wireless information transfer with opportunistic energy harvesting h non outage region outage region h i exp r p i fig illustration of the optimal id and eh regions for characterizing o e trade offs in the case without csit to eh mode an interesting question here is to decide which portion of h should be allocated to eh mode it is observed from fig that the optimal way is to allocate all h i pairs satisfying λ hp λ i or hp i in h to eh mode i e the fading states with sufficiently large signal plus interference total power values at rx should be allocated to eh mode this is reasonable since if we have to allocate a certain number of fading states in h to eh mode i e increase the transmission outage probability by the same amount these fading states should be chosen to maximize the harvested energy at rx furthermore note that λ increases monotonically with q thus the boundary line λ hp λ i that separates the optimal id and eh regions in fig will be shifted down as λ increases that if λ and as a result d id λ shrinks it can be shown λ ø which corresponds to the point δ min q max of the o e region shown in fig a for the case without csit it is worth noting that if i ν ν then the optimal id region reduces to d id λ h e r σ and the rest of the h axis is thus the eh region in this case the outage fading states h e r σ are all allocated to eh mode since they cannot be used by id mode however the harvested energy in the outage states only accounts for a small portion of the total harvested energy due to the poor channel gains most of the energy is harvested in the interval h i e when the channel power is above a certain threshold b the case with csit joint information energy scheduling power control and receiver mode switching in this subsection we address the case of csi known at tx and jointly optimize the energy information scheduling and power control at tx as well as eh id mode switching at rx as formulated in problem let λ and β denote the nonnegative dual variables corresponding to the average harvested energy constraint and average transmit power con straint respectively similarly as for problem problem can be decoupled into parallel subproblems each for one particular fading state and expressed as by ignoring the fading λ p er id λ then d id λ ph λ i p p eh λ λ h λ p index ν p p peak max p ρ where lo e ν ρ p ρ x λq βp to solve problem we need and ρ to compare respectively the optimal as shown values next of lo e ν p ρ for ρ when ρ it follows that lo e ν βp if p p βp otherwise where p p ρ er i it can be verified that the optimal power allocation for the id mode to maximize subject to p p peak is the well known truncated channel inversion policy given by p id p if h where h max β er e r when ρ it follows that lo e ν p ρ λhp λi βp define h β then the optimal power allocation for the eh mode can be expressed as p eh p peak if h h otherwise to summarize we have lo e ν β p if h lo e ν p id ρ p eh ρ λh λi β p peak λi if h h otherwise then given any pair of λ and β the optimal solution to problem for fading state ν can be expressed as ρ if otherwise lo e ν p id ρ lo e ν p eh ρ p p id if ρ p eh if ρ next to find the optimal dual variables λ and β for problem sub gradient based methods such as the ellip soid method can be sub gradient for updating e ν p ν where q ν applied it can be λ β is and p ν e ν q ν denote shown q that p avg the the harvested energy and transmit power at fading state ν respectively after solving problem for a given pair of λ and β hence problem is solved next we investigate further the optimal information energy transfer scheduling and power control at tx as well as the optimal mode switching at rx for simplicity we only study the case of i ν ν from the above analysis it follows that there are three possible transmission modes at tx for the case with csit information transfer mode with channel inversion power control energy transfer mode with peak transmit power and silent mode with no transmission where the first transmission mode corresponds to id mode at rx and the second transmission mode corresponds to eh mode at rx λ h h otherwise h otherwise p peak i lo e ν i ieee transactions on wireless communications vol no january off h fig illustration of the optimal transmitter and receiver modes for characterizing o e trade offs in the case with csit it is assumed that i ν ν and h h we thus axis as the define regions bid on corresponding beh on on the non negative h to the above three modes respectively since the explicit expressions for characterizing these q and regions p avg and b off are complicated and depend on the values of b off in the following we will in the special case of h h to study shed bid some on beh on light and on the optimal design let λ and β denote the optimal dual solutions to problem with h it can be shown that bid on h b off h h h h h h where h h beh is on the largest h h h root of and the equation λ p peak β p peak h β er the proof is omitted here due to the space limitation an illustration ν and h of h bid on is shown beh on and in b off for the case of i ν fig similar to the case without csit cf fig the optimal design for the case with csit is still to allocate the best channels to the eh mode rather than the id mode however unlike the case without csit when the channel condition is poor the transmitter in the case with csit will shut down its transmission to save power v rate energy trade off in this section we investigate the optimal resource allo cation schemes to achieve different trade offs between the maximum ergodic capacity and maximum averaged harvested energy for the two cases without and with csit by solving problems and respectively a the case without csit optimal receiver mode switching first we study problem for the csit unknown case to derive the optimal switching rule at rx between eh and id modes for characterizing different r e trade offs similarly as in section iv a problem can be decoupled into parallel subproblems each for one particular fading state ν expressed as max ρ lr e ν ρ where variable lr e ν associated ρ r λq with λ with the harvested energy denoting constraint the dual q note that we have dropped the index ν of the fading state for brevity to solve problem we need to compare the values of lr e ν ρ for ρ and ρ when ρ it follows that lr e ν ρ log when ρ it follows that lr e ν hp ρ λhp λi id on eh i on fig illustration of the optimal id and eh regions for characterizing r e trade offs in the case without csit thus the optimal solution to problem is obtained as ρ if log hp to find the optimal dual variable λ to problem a simple bisection method can be applied until the harvested energy constraint is met with equality thus problem is efficiently solved similar to section iv a in the following we characterize the optimal id region and eh region to get more insights to the optimal receiver mode switching for characterizing different r e trade offs variable corresponding to a let given λ energy denote target the q optimal the optimal dual id region can then be expressed as d id h i log λ the rest of the non negative h i plane is thus the optimal eh region i e d eh hp λ λ define g h i log hp d id λ hp λ i fig gives for a particular an illustration value of of the q q optimal id region and eh region min next we discuss the optimal mode switching rule at rx for achieving various r e trade offs in the case without csit similar energy to constraint the case of q o e we trade off need for meeting the harvested to sacrifice decrease the ergodic capacity for information transfer by allocating some fading states to eh mode similar to the discussions in section iv the optimal rule is to allocate fading states with largest values of h for information transfer to eh mode the reason is that although fading states with good direct channel gains are most desirable for id mode from and it is observed that the lagrangian value of id mode increases logarithmically with h while that of eh mode increases linearly with h as a result when h is above a certain threshold the value of lr e other ν ρ will be larger than that of words when h is good enough we lr e can ν ρ in gain more by switching from id mode to eh mode it is also observed that as the value of λ increases the optimal id region shrinks in the following we derive the h λhp λi otherwise id λ i i i λ hp λ i eh λ i liu et al wireless information transfer with opportunistic energy harvesting value of λ corresponding to the point r min q max in fig b from fig it can be observed that g h i has two intersection points with the h axis one of which is it can be shown that g λ hp is a monotonically increasing function of h in the interval h i log hp and decreasing function of h in the interval the other intersection point of g h i with the h axis will coincide λ with the point and thus d id λ ø if b the case with csit joint information energy scheduling power control and receiver mode switching in this subsection we study problem to achieve dif ferent optimal r e trade offs for the case of csit by jointly optimizing energy information scheduling and power control at tx together with the eh id mode switching at rx for problem let λ and β denote the nonnegative dual vari ables corresponding to the average harvested energy constraint and average transmit power constraint respectively then problem can be decoupled into parallel subproblems each for one particular fading state and expressed as by ignoring the fading index ν max p p peak lr e ν p ρ where lr e ν ρ p ρ r λq βp to solve problem we ρ need and to ρ compare respectively the maximum as shown values next of lr e ν p ρ for when ρ it follows that lr e ν p ρ log hp βp it can be shown that the optimal power allocation for this case is the well known water filling policy let p i the optimal power allocation for information transfer can be expressed as p id p p peak where expression when x b ρ a as that max min x it given follows in that b a lr e and ν p ρ has the same consequently the optimal power to allocation summarize for for eh id mode mode if p eh is given by we have lr e ν ρ p id log hp peak p peak otherwise if λ h β we have lr e ν p id ρ log h p λ p peak p peak log h p σ consequently if β otherwise β i β i i βp peak β i σ h β i σ h λ h β h p β p peak h i i i e λ i i β β β off fig illustration of the optimal transmitter and receiver modes for characterizing r e i ν ν and trade offs in the case with csit it is assumed that for eh mode the expression same as that given in of lr e ν ρ is the then given a pair of λ and β the optimal solution to problem for fading state ν can be expressed as ρ if otherwise lr e ν p id ρ lr e ν p eh ρ p p id if ρ p eh if ρ next to find the optimal dual variables λ and β for problem similarly as in section iv b the ellipsoid method can be applied thus problem is efficiently solved next we investigate further the optimal information energy transfer scheduling and power control at tx as well as the optimal mode switching rule at rx for simplicity we only consider the case of i ν ν since there is no interfer ence it can be observed from and that there are three possible transmission modes at tx for the case with csit information transfer mode with water filling power control energy transfer mode with peak transmit power and silent mode with no transmission where the first transmission mode corresponds to id mode at rx and the second transmission mode corresponds to eh mode at rx similar to the analysis in section non negative iv b h axis we as can the define regions bid corresponding on beh on on the to the above three modes respectively let λ and β denote the optimal dual solutions to problem for brevity in the following we only present the expressions of the above regions in the case of and b off it can be shown that in this case bid b on off h β h h β h where h h beh on is the h h h and largest root of the equation log h which interval can β be obtained by the bisection method over the the proof is omitted here due to the space limitation an illustration interference and of β bid on beh on is and b off in the case without given in fig compared with the case without csit cf fig it can be similarly observed that the channels with largest power are allocated to eh mode however when the channel condition is very poor the transmitter will shut down its transmission to save power in the case with csit instead of transmitting constant power in the case without csit vi consideration of receiver energy consumption in the above analysis we have ignored energy consumptions at the receiver for the purpose of exposition in this section β λ p peak β β p peak p peak id on eh β h λ hp peak p eh on β p peak h ieee transactions on wireless communications vol no january we extend the result by considering the receiver energy consumption firstly we explain in more details the operations of the receiver in each block and their corresponding energy consumptions as follows at the beginning of each block the receiver estimates the channel and interference power gains to determine which of the eh id mode it will switch to where we assume a constant energy q being consumed after that suppose the receiver switches to eh mode since practical energy receivers are mostly passive we assume that the energy consumed by the energy receiver is negligibly small and thus can be ignored however if the receiver switches to id mode more substantial energy consumption is required for simplicity we assume that a constant power p i incurs due to the information receiver when it is switched on in the following we will study the effect of the above receiver power consumptions on the optimal operation of the time switching receiver due to the space limitation we will only study the o e trade off in the case without csit while similar results can be obtained for other cases let q i ν ρ ν p i tion due to id mode at fading denote state the receiver ν and q power consump denote the net harvested energy obtained by subtracting q and e ν q i ν from the harvested energy e ν q ν to study the o e trade off in the case without csit we modify problem as maximize ρ ν e ν x ν subject to e ν q ρ ν ν since q q ν e ν q i ν q generality is we a absorb constant this for term all into fading q and states assume without q loss of in the let rest λ ˆ of denote this paper the optimal for convenience dual variable corresponding to the net harvested energy constraint we then solve problem in a similar way as for problem the optimal solution of problem can be expressed as ρ if h λ p ˆ i otherwise as a result the optima id region when the receiver energy consumption is considered can be defined as d ˆ id λ ˆ h i hp er λ p ˆ i λ hp ˆ λ i ˆ h i and the rest of the plane is the optimal eh region an illustration of the optimal id region and eh region is given in fig by comparing it with fig for the case without considering the receiver energy consumption we observe that to harvest the same amount of net energy we need to allocate more fading states in to eh mode i e allocating all h i pairs satisfying to eh mode with p i fig shows an example of the o e region without csit but considering the receiver power consumption the setup is the same as that for fig it is observed that the receiver power consumption degrades the o e trade off however q max does not change the value because it is achieved when i e i ˆ λ r p i p and hp i ˆ λ hp ˆ λ i λ h h i exp r λ ph λ i fig illustration of the optimal id and eh regions for characterizing o e trade offs with versus without receiver energy consumption in the case without csit y g r e n e d e t e v r a h o e region with receiver energy consumption p i non outage probability o e region with receiver energy consumption p i o e region without receiver energy consumption fig o e region with versus without receiver energy consumption in the case without csit all the fading states are allocated to eh mode and thus p i has no effects moreover it is observed that when p i the same maximum non outage probability δ max as that of the case without receiver energy consumption i e p i is achieved while when p i a smaller δ max is achieved the reason is as follows if p i is not large enough the energy harvested in the outage fading states can offset the receiver power consumption in the non outage fading states as a result all the non outage fading states can still be allocated to id mode otherwise if p i is too large then we have to sacrifice some non outage fading states to eh mode to harvest more energy for id mode and thus the value of δ max is reduced vii numerical results in this section we evaluate the performance of the proposed optimal schemes as compared to three suboptimal schemes to be given later that are designed to reduce the complexity at assume to maintain rx and that thus its rx normal yields needs operation to suboptimal have an thus average o e with or harvested r e a given trade offs q energy we will we q compute and then compare the minimum outage probability or the maximum ergodic capacity achievable by the optimal and suboptimal schemes id λ λ ph λ i λ p i p eh λ i liu et al wireless information transfer with opportunistic energy harvesting p avg periodic switching interference based switching sinr based switching optimal switching y t i l i b a b o r p e g a t u o fig outage probability comparison for delay limited information transfer in the case without csit and db q first we introduce three suboptimal receiver mode switch ing rules namely periodic switching interference based switching and sinr based switching as follows periodic switching in this scheme rx switches between id mode and eh mode periodically regardless of the csi for convenience let θ with θ denote the portion of time switched to eh mode then θ denotes the portion of time for id mode θ is determined such that the given energy the constraint value of q is satisfied for example for the o e trade off without csit the maximum thus θ harvested energy can be obtained as θ q max q is given in for other trade off cases θ can be obtained similarly interference based switching in this scheme we as sume that rx mode switching is determined solely by the interference power i ν when i ν i thr where i thr denotes a preassigned threshold rx switches to eh mode otherwise it switches to id mode the value of i thr constraint is q determined so as to meet the given energy and the derivation of i thr for different trade off cases are omitted for brevity sinr based switching in this scheme the mode switching is based on the receiver signal to noise plus interference ratio sinr h ν γ thr where γ thr denotes a predesigned sinr threshold rx switches to id mode otherwise it switches to eh mode the energy value constraint of γ thr q is determined while the derivation so as to meet of γ the thr given for different trade off cases are omitted due to the space limitation moreover if csit is available tx can implement the optimal power control to minimize the outage probability or maximize the ergodic capacity for information transfer according to each of the above three suboptimal rx mode switching rules next we show the performance comparison of the three suboptimal schemes with the optimal scheme given in sec tion iv a for delay limited transmission without csit and i ν if i ν q max h ν periodic switching interference based switching sinr based switching optimal switching fig ergodic capacity transfer in the case with csit comparison and q for no delay limited information that given in section v b for no delay limited transmission with csit in figs and respectively the setup is as follows the and for ppc is p peak the noise power is the o e case the constant rate requirement is r nats sec hz we further assume that h ν and i ν are independent exponentially distributed rvs with mean set and to be respectively q in addition the energy target at rx is of fig shows the achievable different schemes with given minimum q for outage the delay limited probability information transmission without csit it is observed that in general the interference based switching works pretty well since its performance is similar to that of the optimal switching derived in section iv a for all values of p avg with only a small gap on the contrary the periodic switching rule does not perform well with an outage probability loss of about as compared to the optimal switching another interesting observation is on the performance of the sinr based switching it is observed from fig that when p avg the performance of sinr based switching is the same as that of the optimal switching however as p avg increases its performance degrades when p avg its achievable outage probability is even higher than that of periodic switching the above observations can be explained as follows it can be seen from in appendix that if we view q min as a function of p the following trade off arises if the value of p is larger less number of fading states are allocated to eh mode but more energy are harvested in each fading state allocated to eh mode to analyze the behavior of q min over p for the case with h ν exp λ and i ν exp λ we can derive an explicit expression of q min as follows q min f p λ e λ e r σ it can be shown that in our setup λ λ z h c e t a n y t i c a p a c c i d o g r e p avg db p λ λ p λ er er p p er λ λ p λ λ p er r p ieee transactions on wireless communications vol no january and f p is a monotonically decreasing function with respect to p when p moreover when p q outage min probability q f p in with other harvested words thus if if energy p p constraint it the follows minimum q that is achieved when rx switches to id mode in the fading states h h i log hp and switches to eh mode in consequently any subset of the h sinr based h switching to meet the is optimal energy constraint when p is small when p the minimum harvested energy q min cannot meet the energy constraint and as shown in section iv a the optimal switching is to allocate some fading states with the largest value of hp i in h to eh mode however the sinr based switching does the opposite way it tends to allocate the fading states with small value of h to eh mode thus when p is large and a certain number of fading states are allocated to eh mode the incremental harvested energy by the sinr based switching is far from that by the optimal switching to recover this energy loss more fading states need to be allocated to eh mode this is why the sinr based switching results in very high outage probability when p becomes large schemes fig with shows given the q achievable for the maximum no delay limited rate of informa different tion transmission with csit similar to fig it is observed from fig that the performance of the interference based switching is very close to that of the optimal switching derived in section v b while the performances of the other two suboptimal switching rules are notably worse under certain conditions e g when snr in fig the performance of the sinr based switching can be even worse than that of the periodic switching this is as expected since although high sinr is preferred by information decoding the optimal mode switching rule derived in section v b is determined by both the values of h and i but has no direct relationship to the ratio of them i e the sinr value thus the performance of the sinr based switching cannot be guaranteed viii concluding remarks this paper studied an emerging application in wireless communication where the receiver opportunistically harvests the energy from the unintended interference and or intended signal in addition to decoding the information under a point to point flat fading channel setup with time varying interfer ence we derived the optimal id eh mode switching rules at the receiver to optimize the outage probability ergodic capacity versus harvested energy trade offs when the csi is known at the transmitter joint optimization of transmit ter information energy scheduling and power control with the receiver id eh mode switching was also investigated somehow counter intuitively we showed that for wireless information transfer with opportunistic energy harvesting the best strategy to achieve the optimal o e and r e trade offs is to allocate the fading states with the best direct channel gains to power transfer rather than information transfer moreover three heuristic mode switching rules were proposed to reduce the complexity at rx and their performances were compared against the optimal performance i r there are important problems unaddressed yet in this paper and thus worth further investigation some of which are highlighted as follows in this paper we assumed that the interference is within the same band as the transmitted signal from tx as a re sult the algorithms proposed in this paper to achieve the optimal o e or r e trade offs cannot be directly applied to the case of wide band interference it is thus interesting to investigate how to manage the wide band interference in a wireless energy harvesting communication system in this paper we studied the optimal mode switching and or power control rules in a single user setup subject to an aggregate interference at the receiver however how to extend the results of this paper to the multi user setup is an unsolved problem for the multi user interference channel interference management is a key issue traditionally interference is either decoded and subtracted when it is strong or treated as noise when it is weak in this paper we provide a new approach to deal with the interference by utilizing it as a new source for energy harvesting thus how should the tx rx links in an interference channel cooperate with each other to manage the interference by optimally balancing between information and power transfer is an intricate problem requiring further investigation appendix in this appendix we characterize the vertex points on the boundary of the o e region and r e region cf fig for both the cases with and without csit o e region without csit as shown in fig a q max is given by caching is a technique to reduce peak traffic rates by prefetching popular content into memories at the end users conventionally these memories are used to deliver requested content in part from a locally cached copy rather than through the network the gain offered by this approach which we term local caching gain depends on the local cache size i e the memory available at each individual user in this paper we introduce and exploit a second global caching gain not utilized by conventional caching schemes this gain depends on the aggregate global cache size i e the cumulative memory available at all users even though there is no cooperation among the users to evaluate and isolate these two gains we introduce an information theoretic formulation of the caching problem focusing on its basic structure for this setting we propose a novel coded caching scheme that exploits both local and global caching gains leading to a multiplicative improvement in the peak rate compared with previously known schemes in particular the improvement can be on the order of the number of users in the network in addition we argue that the performance of the proposed scheme is within a constant factor of the information theoretic optimum for all values of the problem parameters index terms caching coded caching content distribution prefetching t he in communication high temporal i introduction systems variability that of are network congested traffic results during peak traffic times and underutilized during off peak times one approach to reduce peak traffic is to take advantage of memories distributed across the network at end users servers routers to duplicate content this duplication of content called content placement or caching is performed during off peak hours when network resources are abundant during peak hours when network resources are scarce user requests can then be served from these caches reducing network congestion in this manner caching effectively allows to shift traffic from peak to off peak hours thereby smoothing out traffic variability and reducing congestion from the above discussion we see that the caching problem consists of two distinct phases the first phase is the placement phase which is based solely on the statistics of the user demands in this phase the network is not congested and the main limitation is the size of the cache memories the second manuscript received september accepted november date of publication march date of current version april this paper was presented at the ieee international symposium on information theory the authors are with bell labs alcatel lucent holmdel nj usa e mail mohammadali maddah ali alcatel lucent com urs niesen alcatel lucent com communicated by y kim associate editor for shannon theory color versions of one or more of the figures in this paper are available online at http ieeexplore ieee org digital object identifier tit phase is the delivery phase which is performed once the actual demands of the users have been revealed in this phase the network is congested and the main limitation is the rate required to serve the requested content various versions of this problem have been studied with the focus being mainly on exploiting the history or statistics of the user demands in these papers the operation of the delivery phase is fixed to consist of simple orthogonal unicast or multicast transmissions assuming this method of delivery the content placement is then optimized the gain of caching in this approach results from making popular content available locally in another line of research the objective is to optimize the delivery phase for fixed known cache contents and for specific demands see also the discussion in section viii a as pointed out above the gain from traditional uncoded caching approaches derives from making content available locally if a user requests some content that is stored in its cache this request can be served from its local memory we hence call this the local caching gain this gain is relevant if the local cache memory is large enough such that a sizable fraction of the total popular content can be stored locally on the other hand if the size of the local caches is small compared to the total amount of content then this gain is insignificant in this paper we propose a novel coded caching approach that in addition to the local caching gain is able to achieve a global caching gain this gain derives from jointly optimizing both the placement and delivery phases ensuring that in the delivery phase several different demands can be satisfied with a single coded multicast transmission since the content place ment is performed without knowledge of the actual demands in order to achieve this gain the placement phase must be carefully designed such that these multicasting opportunities are created simultaneously for all possible requests in the delivery phase we show that this global caching gain is relevant if the aggregate global cache size is large enough compared to the total amount of content thus even though the caches cannot cooperate the sum of the cache sizes becomes an important system parameter to formally analyze the performance of the proposed coded caching approach and in order to evaluate and isolate these two gains we introduce a new information theoretic formu lation of the caching problem focusing on its basic structure in our setting depicted in fig k users are connected to a server through a shared error free link the server has a database of n files of equal size each of the users has access to a cache memory big enough to store m of the files during the placement phase the caches are filled as a function of the database during the delivery phase each user may ask for ieee personal use is permitted but republication redistribution requires ieee permission see http www ieee org publications rights index html for more information maddah ali and niesen fundamental limits of caching fig caching system considered in this paper a server containing n files of size f bits each is connected through a shared link to k users each with an isolated cache of size mf bits the goal is to design the placement phase and the delivery phase such that the peak rate i e the load normalized by the file size of the shared bottleneck link is minimized in the figure n k and m any one of the n possible files the objective is to design the placement and delivery phases such that the load of the shared link in the delivery phase is minimized for simplicity we restrict the discussion in the introduction section to the most relevant case in which the number of files n is larger than or equal to the number of users k the main results are presented later for the general case in this setting the rate i e the load of the shared link nor malized by the file size in the delivery phase of conventional uncoded caching schemes is k m n here k is the rate without caching and m n is the local caching gain in contrast the coded caching scheme proposed in this paper attains a rate of k m n thus in addition to the local caching gain of m n coded caching also achieves a global caching gain of both of these gains indicate the multiplicative reduction in rate of the shared link so that a smaller factor means a larger rate reduction observe that the local caching gain depends on the nor malized local cache size m n and is relevant only if the cache size m is on the order of the number of files n on the other hand the global caching gain depends on the normalized cumulative cache size km n and is relevant whenever the cumulative cache size km is on the order of or larger than the number of files n by deriving fundamental lower bounds on the rate required in the delivery phase we show that the rate of the proposed coded caching scheme is within a factor of the information theoretic optimum for all values of n k and m to obtain some intuition for the effect of these two gains let us compare the rates of the conventional uncoded scheme achieving only the local gain versus the proposed coded scheme achieving both the local and global gains for a system with n files and k users as shown in fig when each user has a cache memory large enough to km n km n fig rate r required in the delivery phase as a function of memory size m for n files and k users the figure compares the performance of the proposed coded caching scheme to that of conventional uncoded caching store m files the rate of the uncoded caching scheme corresponds to sending files over the shared link while the proposed coded caching scheme achieves a rate corresponding to sending only files a reduction by a factor in rate the remainder of this paper is organized as follows section ii formally introduces our information theoretic for mulation of the caching problem section iii presents main results which are illustrated with examples in section iv sections v vii contain proofs section viii discusses some follow up results and directions for future research ii problem setting before formally introducing the problem in section ii b we start with an informal description in section ii a a informal problem description we consider a system with one server connected through a shared error free link to k users as shown in fig the server has access to a database of n files w w n each of size f bits each user k has an isolated cache memory z k of size mf bits for some real number m n the system operates in two phases a placement phase and a delivery phase in the placement phase the users are given access to the entire database w w n of files each user k is then able to fill the content of its cache z k using the database in the delivery phase only the server has access to the database of files each user k requests one of the files w and d k proceeds in the database the server is informed of these requests bits over the by transmitting a shared link for some signal fixed x d real d number k of size rf r the quantities rf and r are referred to as the load and the rate of the shared link respectively using the content z k of its cache and the signal each user k aims to reconstruct x d d k received over the shared its requested file w d k link a memory rate pair m r is achievable for requests d d k if every user k is able to recover its desired ieee transactions on information theory vol no may file a memory rate w d k with high probability for f large enough pair m r is said to be achievable if this pair is achievable for every possible request d of the shared link responding to the requests d d k n k during the delivery phase finally the kn k decoding d k in functions the delivery phase finally we denote by r m the smallest rate r such that m r is achievable the function r m μ d describes the memory rate tradeoff for the caching problem the aim of this paper is to characterize this memory rate tradeoff in other words we aim to find the minimum rate of communication over the shared link at which all possible demand tuples can be satisfied we illustrate these definitions with the example of the caching strategy employed by conventional uncoded caching systems which will serve as a baseline scheme throughout the remainder of this paper example uncoded caching for a memory size of mf bits one possible strategy is for each user to cache the same m n fraction of each file in the placement phase in the delivery phase the server simply transmits the remaining m n fraction of any requested file over the shared link clearly each user can recover its requested file from the content of its local cache and the signal sent over the shared link in the worst case the users request different files the delivery rate for this caching scheme is thus r u d k k rf fm map the signal received the cache content z k over the to the estimate shared link x d d k and w ˆ d d k k μ d d k k x d d k z k of error the is requested defined as file w d k of user k k the probability of max d w ˆ d definition the pair m r is achievable if for every ε and every large enough file size f there exists a m r caching scheme with probability of error less than ε we define the memory rate tradeoff r m inf max p d k n k k k d k k w d k r m r is achievable iii main results the first theorem presents an achievable rate r c m yielding an upper bound on the memory rate tradeoff r m m k m n min n k theorem for n n files and k n users each with we refer to this caching strategy as uncoded caching since cache of size m n k k n both content placement and delivery are uncoded the first factor k in is the rate without caching the r m r c second factor in is m n we call this the local caching gain since it arises from having a fraction m n of each file available locally at the user if n k the system enjoys an additional gain reflected in the third factor in in this case some users will necessarily request the same file resulting in a natural multicasting gain of n k b formal problem statement we now provide the formal definition of the information theoretic caching problem let w n m k m n min is achievable for general m n the lower convex envelope of these points is achievable the rate r c m is achieved by a coded caching scheme that is described and analyzed in detail in section v for ease of exposition we first focus on the case n k in which r c m k m n random variables each uniformly distributed n n be over n independent the achievable rate r c m consists of three distinct factors the first factor in r c for some f n each w n m is k this is the worst case rate without caches at the users i e m the second factor in r c m is m n referring to in example we see that this term capturing the local caching represents a file of size f bits gain appears also in the rate expression of the uncoded a m r caching scheme consists of k caching functions caching scheme observe that this local gain is a function n k encoding functions and kn k decoding functions of the normalized local memory size m n and it is relevant the k caching functions whenever m is on the order of n φ k n fm finally the third factor in r c m is which we call the global caching gain this gain is a function of the map the files w w n into the cache content z k φ k w w n normalized global or cumulative memory size km n and it is relevant whenever km is on the order of or larger than n this global gain is to be interpreted as a multicasting for each user k k during the placement phase the n k encoding functions ψ d n fr gain available simultaneously for all possible demands note that since the number of users is smaller than the number of files in the worst case all users request different files hence there are no natural multicasting opportunities the scheme map the files w proposed in theorem carefully designs the content place ment in order to create coded multicasting opportunities in the delivery phase even among users that request different files d k w n to the input x d d k ψ d d k w w n km n km n km n k n maddah ali and niesen fundamental limits of caching since the placement phase is performed without knowledge of the actual demands care must be taken to ensure that the same multicasting opportunities are created simultaneously for every possible set of requests in the delivery phase we point out that the uncoded caching scheme introduced in example achieves only the local caching gain whereas the coded caching scheme proposed in theorem achieves both the local as well as the global caching gains the following two examples compare these two gains example θ k improvement in rate consider a system with the same number of users as files i e n k assume each user has enough cache memory for half of the files so that m n then the local caching gain is and the global caching gain is k by uncoded caching achieves a rate of k on the other hand by theorem coded caching achieves a rate of k k a reduction by more than a factor k in rate compared to the uncoded scheme we refer the reader to fig in section i for a visualization of the effect of this improvement example θ k improvement in slope in this example we compare the performance of the coded and uncoded caching schemes for small values of the cache size m we consider again the case n k from the rate of uncoded caching has a slope of around m on the other hand by theorem the rate of coded caching has a slope less than k around m therefore the coded caching scheme reduces the rate over the shared link at least k times faster as a function of cache size than the uncoded caching scheme comparing the rates of the uncoded and coded schemes in fig in section i for small values of m illustrates the effect of this improvement consider next the case n k in which the third factor in r c m is the minimum of and n k the first term in this minimum is the coded multicasting gain created by careful content placement as discussed for the case n k however for a scenario with fewer files than users there exists already a natural multicasting opportunity by multicasting all n files to the k users we can achieve a gain of n k this is the second term in the minimum above the scheme in theorem achieves the better of these two gains we point out that for m n k this minimum is achieved by the first of the two gains in other words the natural multicasting gain is relevant only when the memory size is very small having established an upper bound on the memory rate tradeoff r m we proceed with a lower bound on it theorem for n n files and k n users each with cache of size m n r m min n k max the proof of theorem presented in section vi is based on a cut set bound argument tighter lower bounds on r m can be derived using stronger arguments than the cut set bound see the discussion in example in section iv however the cut set bound alone is sufficient for a constant factor approximation of the memory rate tradeoff r m as the next follows by calculating the slope of the straight line connecting the two consecutive corner points of r c m at m and m km n n m fig memory rate tradeoff for n files k users the achievable rate solid r blue c m curve of the the coded lower caching bound scheme on r m from theorem from theorem is indicated is indicated by the by the dashed red curve for the n k case r m can be found exactly and is indicated by the dotted black curve theorem shows by comparing the achievable rate r c m in theorem with the lower bound in theorem theorem for n n files and k n users each with cache of size m n r c m with the achievable rate r c m of coded caching as defined in theorem the proof of theorem is presented in section vii the bound r c m r m on the approximation ratio of the proposed caching scheme is somewhat loose due to the analytical bounding techniques that were used numerical simulations suggest that r c m for all n k and m n theorem shows that the rate r c m of the pro posed coded caching scheme in theorem is close to the information theoretic optimum r m for all values of the system parameters more precisely it shows that no scheme can improve upon the rate r c m of the proposed scheme by more than a factor this also suggests that the local and global caching gains identified in this paper are fundamental there are no other significant caching gains i e scaling with the problem parameters beyond these two iv e xamples example consider the case n k so that there are two files say w a w b and two users each with cache memory of size m the upper and lower bounds in theorems and on the memory rate tradeoff r m are depicted in fig to illustrate the proof techniques we now show how these two bounds are derived for this simple setting r m r m ieee transactions on information theory vol no may fig caching strategy for n files and k users with cache size m with all four possible user requests each file is split into two subfiles of size i e a a signal is constructed using the same logic of exchanging the missing subfiles this proves the achievability of the m r pair it is worth pointing out that in each case the server sends a single coded multicast transmission to satisfy two possibly different user requests moreover these coded multicasting opportunities are available simultaneously for all four possible user requests this availability of simultaneous multicasting opportunities enabled by careful content placement is criti cal since the placement phase has to be performed without knowledge of the actual demands in the delivery phase so far we have shown that the m r pairs at corner points and are achievable on the other hand by dividing the cache memories and the transmitted signal proportionally it is easy to see that if any two points m r and m r are achievable then the line connecting them is also achievable inspired by the term time sharing in network information theory we refer to this as memory sharing memory sharing between the corner points and establishes the achievability of the solid blue curve in fig which coincides with the upper bound stated in theorem a and b b b the scheme achieves rate r observe that while the transmission from the server changes we continue by analyzing the lower bound on r m in theorem the proof relies on the cut set bound we consider as a function of the user requests the cache contents do not two cuts the first cut separates x we start with the upper bound in theorem focusing on the corner points of the achievable region first let us consider the two extreme cases m and m n if m the server can always transmit both files a and b over the shared link since this satisfies every possible request the m r pair is achievable if m each user can cache both files a and b in the placement phase therefore no communication is needed in the delivery phase and the m r pair is achievable consider then the more interesting corner point at m the caching scheme achieving the upper bound in theorem is as follows see fig we split both files a and b into two subfiles of equal size i e a a z z from the two users assume m r is an achievable memory rate pair then this cut has capacity at most rf since x is at most rf bits by definition of achievability and since z z contain each mf bits on the other hand since the first user can recover a from x z and the second user can recover b from x z the number of bits that need to be transmitted over this cut is at least hence rf so that r a and b b b in the placement phase we set z a b and z a b in words each user caches one exclusive part of as this holds for all achievable memory rate conclude that r m pairs m r we each file for the delivery phase assume for example that user one requests file a and user two requests file b given the second cut separates x that user one already has subfile a x z from the first obtain the missing subfile a of a it only needs to which is cached in the second user note that this user can recover a and b from x z and x z respectively hence this cut yields user memory z similarly user two only needs to obtain mf the missing subfile b which is cached in the first user memory z in other words each user has one part of the file for any achievable memory rate pair m r implying that that the other user needs the server can in this case simply transmit a b where denotes bitwise xor since user one already has b it can recover a from a b similarly since user two already has a it can recover b from a b thus the signal a b received over the shared link helps both users to effectively r m m together this yields the dashed red curve in fig which coincides with the lower bound stated in theorem for the case n k the memory rate tradeoff can in fact be found exactly and is indicated by the dotted black curve exchange the missing subfiles available in the cache of the in fig this is argued by showing that the pair m r other user is also achievable and by deriving the additional non the signals sent over the shared link for all other requests cut set bound are depicted in fig one can see that in all cases the r m m maddah ali and niesen fundamental limits of caching this shows that while the bounds in theorems and are sufficient to characterize the memory rate tradeoff r m to within a constant multiplicative gap neither the achievable region nor the lower bound are tight in general the details of this derivation are reported in the appendix example in this example we assume that n k so that there are three users and three files say w a w b and w c again it is trivial to see that the m r pairs and are achievable we focus on two nontrivial corner points at m and m consider first caches of size m we split each file into three subfiles of equal size i e a a a a b b b b and c c c c in the placement phase the cache content of user k is selected as z k a k b k c k a more formal way to describe this content placement which is a bit exaggerated for this simple setting but will be useful for the general setting below is as follows let t be a subset of one element of then subfiles a t b t c t are placed into the cache of user k if k t for example a is cached at user one since in this case t fig memory rate achievable rate r c tradeoff for n files and k users the for the delivery phase let us consider as an example that user one requests file a user two requests file b and user three requests file c then the missing subfiles are a m of the coded caching scheme from theorem is indicated by the solid blue curve the lower bound on r m from theorem is indicated by the dashed red curve for user one b and a and b for user two and c and c for user three given the cache contents users one and two aim to exchange a three and user three misses subfile c which is available at both users one and two in other words the three users would and c and b users one and three aim to exchange a and users two and three aim to exchange b and c the signal a b a c b c enables all of these three exchanges all other requests can be satisfied in a similar manner since each subfile has rate the proposed scheme achieves a rate of and therefore m r pair is achievable observe that through careful content placement we have again created coded multicasting opportunities for any two users even with different demands moreover these coded multicasting opportunities are available simultaneously for all possible triples of user requests consider next caches of size m we again like to exchange the subfiles a b c this exchange can be enabled by transmitting the signal a b c over the shared link given its cache content each user can then recover the missing subfile all other requests can be satisfied in a similar manner the rate of transmission in the delivery phase is and therefore m r pair is achievable this approach again creates simultaneous coded multicasting opportunities but this time for all three users together the arguments so far show that the solid blue curve in fig is achievable this coincides with the upper bound in theorem for the lower bound on r m we use two cut set bounds the first cut separates x split each file into three subfiles of equal size however it will be convenient to label these subfiles differently namely a a z z z from the three users note that the users can recover a b and c from x z x z and x z respectively a a b b b b and c c c c in the placement phase the caching strategy is for any achievable m r pair the capacity of this cut is at most rf and the number of bits that need to be transmitted over it is at least hence z a a b b c c z a a b b c c rf which implies that z a a b b c c r m this content placement can be understood as follows let t the second cut separates x be a subset of two elements of then subfiles a t x x z b t c t are placed into the cache of user k if k t for example a is cached at users one and three since in this case t for the delivery phase let us again assume as an example from the first user note that this user can recover a b and c from x z x z and x z respectively for any achievable m r pair the cut capacity is at most mf and the number of bits that need to be transmitted over it is at least hence that user one requests file a user two requests file b and mf user three requests file c in this case user one misses subfile a which is available at both users two and three user two misses subfile b which is available at both users one and which implies that r m m ieee transactions on information theory vol no may together these two cut set bounds result in the dashed red curve in fig this coincides with the lower bound in theorem v c oded c aching s cheme p roof of t heorem we now present the general achievable scheme we first describe the algorithm in words focusing on the corner points of r c m consider cache size m n k k n and set t mk n observe that t is an integer between and k if m then in the delivery phase the server can simply transmit the union of all requested files over the shared link resulting in f min n k bits being sent hence r min n k if m n then all files in the database can be cached at every user in the placement phase hence r n assume in the following that m is strictly larger than zero and strictly less in the placement than n phase so that t k each file is split into k t nonover lapping subfiles of equal size it will be convenient to label the subfiles of file w n as w n w n t t k t t where we recall the notation k k for each n n subfile w n t is placed in k if k t thus each user caches since each of these subfiles has size a total f of k t n the this k t cache requires of subfiles user n k t f fm bits of cache memory at each user satisfying the memory constraint example let n k and m then t and the content placement is z n z w n w n n z w n w n n as we have already seen in example in section iv we next describe the delivery phase consider the request vector d w n w n d k n k i e we focus on a subset s k of user k requests s t users file observe w d k that every t users in s share a subfile in their caches that is needed at the remaining user in s more precisely fix a user s and note that s t the subfile requested by user since it is a subfile of w d w at d the s same is time it is missing at user since s finally it is present in the cache of any user k s k t f nt k for each such subset s k of cardinality s t the server transmits s w d where denotes bitwise xor each of these sums results in f s of subsets k t bits s being is sent over the shared link since the number shared link is k t the total number of bits sent over the fr t k f where we have used that t mk n example let n k and m then t and the content placement d d d the is signal z k transmitted w n k n in for request the delivery phase is x w w w w w w as we have already seen in example in section iv here the three sums correspond to s s and s respectively we now argue that each user can successfully recover its requested message consider again a subset s k with s t since each user k s already has access to the subfiles w d k s k from w d s the for signal all s k it can solve for s w d s sent over the shared link since this is true for every such subset s each receiver k is able to recover all subfiles of the form w d k t t k k t t of the requested form already w d available k t file for t such w d k that the k remaining subfiles t but these are of the subfiles are in the cache of user k hence each user k can recover all subfiles of its requested this shows that for m file n k k w d k k n k r m k m n as was pointed out earlier see example in section iv the points on the line connecting any two achievable points are also achievable finally taking the minimum between the rate derived here and the rate k m n min n k achieved by the conventional uncoded scheme described in example in section ii a proves theorem for completeness in algorithm we formally describe the placement and the delivery procedures of the coded caching scheme for n files k users we focus on the corner points f k t t f k m n km n k t km n maddah ali and niesen fundamental limits of caching algorithm coded caching m of r c which occur at cache size m such that mk n is a positive integer less than k observe that the placement phase of the algorithm is designed such that in the delivery phase the caches enable coded multicasting between mk n users with different demands these coded multicasting opportunities are available simultaneously for all n k possible user demands this is again critical since the placement phase has to be designed without knowledge of the actual demands in the delivery phase we also point out that while the problem setting allows for vanishing probability of error as f the achievable scheme presented here actually has zero error probability for finite f vi lower bound on r m proof of theorem let min n k and consider the first caches z z there exists a user demand and a corresponding input to the shared link say x z determine the files w such that x and z w similarly there exists an input to the shared link say x z determine the files w such that x and z w continue in the same manner selecting appropriate x see fig we then have that x x n determine w x n and z z w n consider then the cut separating x and z x n z from the corresponding users as indicated in fig by the cut set bound theorem n r m sm n solving for r and optimizing over all possible choices of we obtain r m max min n k proving the theorem w procedure p lacement w n t mk n t t k t t for n n do split w n t t of equal size end for for k k do z k into w n t w n t n n t t k t end for end procedure procedure d elivery t mk n s s k s t x d w w n d d k end procedure d k k s w d k s k s s n m fig cut corresponding to parameter in the proof of the converse in the figure n k and x x x x fig for n achievable files and rate k r c m users solid the curve distinct regimes which are approximately as defined in theorem function m r c max m has n k three max n k m n and n m n in the first and third is nonlinear regimes as r indicated c m is by essentially the dotted linear curve in the second regime r c m vii approximation of r m proof of theorem we now compare the upper bound on r m in theorem to the rate r c m achieved by the proposed scheme given by the lower convex envelope of the points r c for m a multiple of n k as described in theorem the proof is based on the following observation the function r c m min k m n m has three distinct regimes as depicted in fig the first regime is for m between and approximately max n k in this regime r c m is close to lin ear the second regime is for m between approximately max n k and n in which r c m is nonlinear and behaves essentially like n m the third regime is for m between approximately n and n in which r c m is again close to linear we bound the ratio r m r c m separately km n n m 2864 ieee transactions on information theory vol no may in each of these regimes though to optimize the constants we will choose slightly different definitions of the boundaries for the three regions it will be convenient to consider the two cases min n k and min n k separately we have r c m min n k m n by theorem on the other hand setting in theorem yields r m m n hence r c m min n k for min n k assume in the following that min n k we consider the three cases m max n k max n k m and m n separately assume first that m max n k we have r c m r c min n k by theorem by theorem and using that n n r m setting min n k min n k we obtain for m max n k r m r max n k min n k min n k n min n k min n k min n k where we have used the assumption that min n k combining and yields r c m for m max n k and min n k m assume be the largest next that multiple max of n k n k less than m or equal to let m so that m n k m m min n k n r m min n k r m n min k n 275 max n k m n then by theorem r c m r c km n n m where we have used that m n k in the last inequality setting m min n k in theorem we obtain r m combining and yields r c m for finally max assume n k m m n let m be the largest multiple of n k less than or equal to so that n k m then using convexity of r c n r c and that r c m r c m m m n where we have used that m now by theorem r c m hence r c m m n m n setting in theorem we obtain r m m n combining and yields r c m for m n m m n n m n m m m k m n k m n k m n k k k r m r m n m n 092 k r c m n n m 32n m 32 maddah ali and niesen fundamental limits of caching combining and yields r c m for all n k and all m n viii discussion and directions for future work we now discuss the connection of the caching problem to the index and network coding problems and list some follow up work as well as directions for future research a connection to index and network coding the caching problem introduced in this paper is related to the index coding problem 8 9 or equivalently the network coding problem we now explain this connection the caching problem consists of a placement phase and a delivery phase the most important aspect of this problem is the design of the placement phase in order to facilitate the delivery phase for any possible user demands now for fixed content placement and for fixed demands the delivery phase of the caching problem induces a so called index coding problem however it is important to realize that the caching problem actually consists of exponentially many parallel such index coding problems one for each of the n k possible user demands furthermore the index coding problem itself is computationally hard to solve even only approximately the main contribution of this paper is to design the content placement such that each of the exponentially many parallel index coding problems has simultaneously an efficient and analytical solution this in turn allows to solve the caching problem within a constant factor of optimality the proposed coded caching scheme is based on prefetching uncoded raw bits and on delivering linearly encoded messages this con trasts with the index coding problem where linear codes are not sufficient to operate within a bounded factor of optimality i e nonlinear codes can offer unbounded gain b decentralized caching the coded caching scheme described in this paper has a placement phase that is orchestrated by a central server crucially for this scheme both the number and the identity of the users in the delivery phase are already known in the prior placement phase this is clearly not a realistic assumption since we usually do not know in the morning which users will request content in the following evening moreover if instead of the synchronized user requests here we have more realistic asynchronous requests then users join and leave the system over a period of several hours during the delivery phase resulting in a time varying number of users finally users may be in different networks during the two phases e g a mobile connected to a wireless local area network e g wifi during the placement phase and connected to a cellular network e g lte during the delivery phase to deal with these issues we need a placement phase that is decentralized we have developed such a decentralized coded caching scheme in follow up work in which each user r m caches a randomly selected subset of the bits we show that the rate of this scheme is within a constant factor of optimal universally for any number of users k using this universality property allows to address the problem of asynchronous user requests and of differing networks during the two phases c online caching the caching problem here has two distinct phases place ment and delivery the cache is updated only during the placement phase but not during the delivery phase many caching systems used in practice use online cache updates in which a decision to update the cache is made during file delivery one popular update rule is least recently used better known by its abbreviation lru in which the least recently requested file is evicted from the cache it is hence of interest to develop an online version of the coded caching algorithm proposed here we present prelim inary results in this direction in where we introduce a coded least recently sent delivery and update rule for a probabilistic model of user requests this update rule is shown to be approximately optimal in an open question is to find schemes that have stronger competitive optimality guarantees for individual sequences of user requests as in d nonuniform file popularities throughout this paper we have adopted a worst case defi nition of rate with respect to user requests however different pieces of content have usually different popularities i e probabilities of being requested by the users in order to capture this effect the definition of rate needs to be changed from worst case to expected value we report initial results extending the coded caching approach to this setting in the optimality results there are however weaker providing only an approximation to within a factor that is in most cases of interest logarithmic in the number of users finding better schemes for this important setting of nonuniform file popularities and more precisely quantifying the impact of the file distribution on the global gain arising from coding is hence of interest furthermore the results in hold only when all users have the same file popularity distribution analyzing the impact of different per user file popularities is an open problem e more general networks the discussion in this paper focuses on a basic caching network consisting of a single shared link to be relevant in practice the results here need to be extended to more general networks we report extensions to tree networks with caches at the leaves as well as initial results on caches shared among several users in an interesting extension of the coded caching approach proposed in this paper to device to device networks without a central server is reported in adapting coded caching to general networks and analyzing asymmetric scenarios arising for example from nonuniform cache sizes are open questions 2866 ieee transactions on information theory vol no may f sharper approximations the upper and lower bounds on the memory rate tradeoff derived in this paper are shown here to be within a factor from each other sharpening this approximation guarantee is of interest as mentioned numerical simulations suggest that a more careful analysis should be able to decrease the gap to within a factor of to go beyond that better lower and upper bounds are needed both of which we know from example can be improved see also the appendix some further questions in this context are as follows we split each file into two subfiles i e a a a and b b b in the placement phase we choose the cache contents as z a b and z a b assume that user one requests file a and user two requests file b the server can satisfy these requests by transmitting b a at rate r the other three possible requests can be satisfied in a similar manner this shows that m r pair is achievable this new point improves the boundary of the achievable region from the solid blue to the dotted black curve in fig in section iv linear versus nonlinear coding as pointed out above nonlinear coding schemes can offer unbounded gain in network and index coding whereas for the caching we now argue that the lower bound on can also be improved we have for any achievable memory rate pair m r problem linear coding schemes are sufficient to achieve optimality within a constant factor this raises the ques h x tions if we can improve the gap using nonlinear schemes or if there exist caching networks for which nonlinear schemes can offer unbounded gain z h x z h x z w h x z w i w x z i w x z h x z x z w larger field size all operations in this paper are over the binary field in contrast in network and index coding i w larger field sizes are useful the question is thus if larger field sizes can improve the performance of coded caching x z i w x z i w x z x z w i w x z i w x z coded content placement example presents a scenario with two users two files and cache size where now since w coded content placement can improve the achievable rate while this type of coded content placement is not needed for a constant factor approximation of the memory rate tradeoff it might lead to a smaller constant factor can be decoded from x z and also from x z and since w can be decoded from x z x z we obtain using fano inequality that i w x z f εf zero error versus vanishing error the problem set ting as described in section ii allows for a vanish i w x z f εf i w ing error probability however the proposed achievable scheme has zero error probability while still being approximately optimal schemes with vanishing error probability or lower bounds making explicit use of the zero error requirement might be used to find sharper approximations g implementation complexity compared to uncoded caching the coded caching approach suggested in this paper imposes additional computational burden on the server and the users especially for large values of k one approach to deal with this burden is to use coded caching only among smaller subgroups of users this results in lower computational load at the expense of higher rates over the shared link deriving the fundamental tradeoff between rate and memory subject to complexity constraints is of great interest appendix example in section iv derives upper and lower bounds on the memory rate tradeoff r m claimed in theorems and for k n while these two bounds are sufficient to characterize r m to within a constant multiplicative gap as we show in what follows neither of the two bounds are tight in general we start by arguing that the achievable scheme can be improved by focusing on the case m as before x z x 2 z 2 w f εf for any ε as long as f is large enough hence 3f since ε is arbitrary this shows that r 2 m since this is true for any achievable m r pair this implies that r m 2 m this bound together with the two cut set bounds derived earlier proves that the dotted black curve depicted in fig 3 in section iv is indeed equal to r m this letter deals with a three node cooperative network where the relay node harvests energy from radio frequency rf radiation the source node is the only available rf generator and introduces a fundamental switching between energy harvesting and data relaying a greedy switching gs policy where the relay node transmits when its residual energy ensures decoding at the destination is investigated the gs policy is modeled as a markov chain for a discretized battery the stationary distribution and the outage probability of the system are derived in closed form expressions in addition an optimal switching policy that incorporates a priori knowledge of the channel coefficients is proposed and solved by a mixed integer linear programming formulation index terms relay channel cooperative networks energy harvesting rf energy transfer markov chain optimization i introduction conventional networks equipped studies with batteries on energy constrained focus on energy wireless sav ings and node lifetime maximization adaptive power control is a vital tool towards this perspective 3 on the other hand energy harvesting is introduced as an attractive solution to prolong network lifetime and several techniques and models have been proposed in the literature e g a promising harvesting technology is the radio frequency rf energy transfer where the ambient rf radiation is captured by the receiver antennas and converted into a direct current dc voltage through appropriate circuits rectennas 8 9 due to practical constraints simultaneous harvesting and data deliver ing is not possible and this limitation motivated several works in the literature in the authors investigate rf transfer based beamforming schemes for a three node multiple input multiple output mimo setup and discuss practical solutions for simultaneous energy and information transfer this work is extended in for a two hop mimo relay system with amplify and forward af relaying in this letter we study the rf energy transfer concept for a three node cooperative af network with an energy harvesting relay node in contrast to previous studies where external rf energy generators maintain the operation of the network here we assume that the information signal is the only rf radiation manuscript received june the associate editor coordinating the review of this letter and approving it for publication was a conti i krikidis and s timotheou are with the dept of electrical and com puter engineering university of cyprus nicosia e mail krikidis timotheou stelios ucy ac cy i krikidis is also with the dept of computer engineering informatics university of patras rio patras greece s sasaki is with the dept of electrical and electronic engineering niigata university niigata japan e mail kojiro eng niigata u ac jp digital object identifier lcomm 091712 f r d ε ε fig the three node network topology the considered set up introduces a fundamental switching between data relaying and energy harvesting and motivates the investigation of a greedy switching gs policy where the relay node transmits when its residual energy can support decoding at the destination node the charging discharging behavior of the gs policy is modeled as a discrete markov chain mc for a discrete level battery and the achieved performance is evaluated in terms of outage probability in addition an optimal scheme with a global channel knowledge for the whole transmission period is investigated and analyzed by solving a mixed integer linear optimization problem ii system model consider a simple single antenna network with one source s one half duplex relay r and one destination d as shown in fig the source always has new data to transmit and a direct link between the source and the destination is not available e g coverage extension scenario physical obstacles time is slotted with a time slot duration equal to one time unit and communication is performed in two time slots the first slot is dedicated to the source transmission and the second slot is assigned to the relay orthogonal relaying the destination can decode the relaying signal when the signal to noise ratio snr at the destination is at least equal to a target value γ all wireless links exhibit fading and additive white gaus sian noise awgn the fading is assumed to be frequency non selective rayleigh block fading this means that the fad ing coefficients f i j f s r s r d for the i j link remain constant during one slot but change independently from one slot to according to a complex gaussian distribution with zero mean and path loss variance and shadowing i j 2 the variance captures also effects furthermore the awgn is assumed to be normalized with zero mean and unit variance in order to simplify the notation of the analysis we define h f s r e f i j 2 g f r d 2 λ h the instantaneous gain g is assumed σ2 s r perfectly known σ2 at r d the relay node via an error free feedback channel if the source transmits with a constant power p and λ g and the relay transmits the sake of presentation the time slot index is omitted by the instantaneous expressions unless needed for clarification 00 c ieee krikidis et al rf energy transfer for cooperative networks data relaying or energy harvesting with a power p r the achieved snr at the destination can be expressed as γ d p p r hg min p for intermediate high snrs 2 based on the above approximation the required transmitted power at the relay node that ensures decoding is simplified to p r h p r g γ 3 a energy harvesting model due to the normalized slot duration the measures of energy and power become identical and therefore are used equiva lently signal processing energy cost is not considered and thus relaying transmission is the only energy operating cost for the system the relay node is equipped with an energy battery of size p b αp units of energy with α which can be charged from the received rf waves the battery is discretized in l 2 energy levels ε i l with i l l corresponds to a continuous battery model in addition we define l 2 corresponding energy states i ip b with i l and thus the battery is in state i when its stored energy is equal to ε i and e p i j denotes the transition probability p i j m ε i i l denotes the residual energy of the battery at the beginning of the m th slot for the considered network topology the source signal is the only resource of rf radiation and therefore it can be used to charge the relay battery due to practical limitations the relay node cannot simultaneously charge its battery and retransmit the source signal and therefore is enforced to switch between two operational modes a harvesting mode μ h the relay node harvests energy from the received signal and b relaying mode μ t the relay node transmits the source data μ m μ t μ h denotes the operational mode of the relay node for the m th transmission in addition we define the binary variable x m 2 denotes the indicator function based on the considered discretized battery model we define the energy that can be harvested from the received signal to be equal to ε ε i h μ m μ t where 2 4 where ζ denotes the conversion ζ of the received energy can be stored as for the transmission process the relay node also uses the l 2 discrete energy levels considered and therefore the required power p r where i h arg i l max ε i ε i ζp h given in 2 corresponds to a transmitted energy level ε ε i t if p r p b otherwise where i t argmin i l an outage event occurs when the source signal cannot be decoded at the destination node or equivalently when the relay node operates in the harvesting mode linear battery model closely approximates the actual behavior of the battery and is widely used in the literature e g 14 9 a practical cellular energy harvesting system was designed operating at ζ 25 conversion efficiency ε i ε i p r p h p r g h γ does not exist otherwise g if p iii a greedy switching policy given that the main optimization target is to minimize the number of times that the relay node does not transmit the gs policy prioritizes the operation mode μ t the key idea of the gs policy is that the relay node transmits when its residual energy can support the required transmitted energy otherwise it harvests energy from the received signal for the m th source transmission the gs policy can be expressed as μ gs m μ t if e m ε μ h 6 e otherwise p b a markov chain for the gs policy the gs policy results in a specific charging discharging behavior of the relay battery that can be represented by a finite mc the states of this mc are the defined energy states i m min e m x m ε x m ε and each state is connected with all the other states the transition probabilities p i j are determined in the following discussion the battery remains empty in this case the battery is empty at the beginning of the transmission and the received harvested energy is lower than the first energy level ε therefore we have the transition probability x x denotes the cumulative distribution function cdf of an exponential random variable x 2 the battery remains full l in this case the battery is fully charged at the beginning of the transmission therefore it cannot harvest more energy and a the required transmitted energy is higher than the battery size given that p l h γ or b p h γ the corresponding transition probability is expressed as p 3 the non empty and non full battery remains un changeable 4 the empty battery is fully charged in this case the empty battery harvests energy from the received signal and becomes fully charged this transition probability is equal to 1774 ieee communications letters vol no november 5 the non empty and non full battery is fully charged i l i l in contrast to the previous cases here the non empty and non full battery harvests energy and becomes fully charged the transition probability can be written as 6 the empty battery is partially charged j l in this case the empty battery harvests energy from the received signal and becomes partially charged this transition probability is equal to 7 the non empty battery is partially charged i i j l 1 in this case the non empty battery harvests energy from the received signal and becomes partially charged not full the associated transition probability is expressed as ζ 14 8 the non empty battery is discharged data relaying i j i this case refers to the relaying transmission and thus the residual energy is sufficient for supporting the required transmitted energy the corresponding transition probability is given as b steady state distribution of the mc we are interested in finding the stationary distributionπ of the mc where π i denotes the probability that the energy level of the discretized battery is ε i and hence explore how the relay s battery is charged and discharged according to the gs policy proposition 1 the state transition matrixp p i j of the mc that models the battery s behavior is irreducible and row stochastic the transition probabilities for any state pair are not equal p i j p j i i j given the previous analysis and thus the matrixp is not symmetric as a result the transition proba bility matrix is row stochastic only and not doubly stochastic in addition it is irreducible as all states communicate given that the transition matrixp is irreducible and row stochastic the stationary distribution is given by 12 app a π p t i b where b i j 1 i j andb 1 1 1 t c outage performance given the considered adaptive af relaying policy an outage occurs when the relay node operates in the μ h mode which transits the battery state to a non decreased energy level hence the outage probability can be expressed as p out l 1 π i i l 1 p i j j i π for p the above asymptotic behavior indicates that as the trans mission energy increases the outage probability is dominated by the probability that the relay s battery is empty iv a genie aided policy lower bound in this section we present a genie aided ga policy which assumes that the channel coefficients and the related energy parameters ε m ε m m are a priori perfectly known for the whole transmission period the ga policy serves as a performance benchmark for the evaluation of practical policies and corresponds to a lower bound the goal of the ga policy is to derive optimal values for x m m such that the outage probability is minimized subject to the battery energy constraints the problem can be expressed as a mixed integer linear programming milp formulation max m x m s t e m e m 1 1 x m ε m x m ε m m e m e m 1 1 x m ε m m e m e m 1 x m ε m m e m p b m x m 1 m in formulation the objective is to maximize the number of transmissions which is equivalent to the minimization of the outage probability constraints ensure that the energy limitations are always satisfied for all possible cases first if the battery residual energy is not sufficient for a transmission e m 1 ε m constraints and ensure that this is not possible second if transmission is pos sible and desired e m 1 ε m x m 1 constraints and ensure that e m e m 1 ε m third if energy harvesting takes place at the relay during the m th transmission x m then constraints and proof for any mc the transition from state s i to s j krikidis et al rf energy transfer for cooperative networks data relaying or energy harvesting 0 that gs is an efficient solution for the problem considered finally fig 2 plots the performance of the gs policy eq 2 l simulation with ζ 0 3 λ h λ g 1 2 and demonstrates that the conversion efficiency as well as path loss effects 1 degrades the achieved performance but does not change the y t i l i b a b o r p e g a t observed behavior of the curves vi conclusion this letter has dealt with the rf energy transfer in a gs with ζ 1 λ h three node cooperative network which is characterized by a fundamental switching between energy harvesting and data relaying a gs policy that prioritizes relay transmission has been investigated and analyzed in terms of outage probabil ity for a discrete level battery we have shown that battery 0 5 15 25 35 discretization affects the performance of the gs policy and fig 2 outage probability performance snr db versus snr for the gs and ga policies wireless power transfer wpt is a promising new solution to provide convenient and perpetual energy supplies to wireless networks in practice wpt is implementable by vari ous technologies such as inductive coupling magnetic resonate coupling and electromagnetic em radiation for short mid long range applications respectively in this paper we consider the em or radio signal enabled wpt in particular since radio signals can carry energy as well as information at the same time a unified study on simultaneous wireless information and power transfer swipt is pursued specifically this paper studies a multiple input multiple output mimo wireless broadcast system consisting of three nodes where one receiver harvests energy and another receiver decodes information separately from the signals sent by a common transmitter and all the transmitter and receivers may be equipped with multiple antennas two scenarios are examined in which the information receiver and energy receiver are separated and see different mimo channels from the transmitter or co located and see the identical mimo channel from the transmitter for the case of separated receivers we derive the optimal transmission strategy to achieve different tradeoffs for maximal information rate versus energy transfer which are characterized by the boundary of a so called rate energy r e region for the case of co located receivers we show an outer bound for the achievable r e region due to the potential limitation that practical energy harvesting receivers are not yet able to decode information directly under this constraint we investigate two practical designs for the co located receiver case namely time switching and power splitting and characterize their achievable r e regions in comparison to the outer bound index terms mimo system broadcast channel precoding wireless power simultaneous wireless information and power transfer swipt rate energy tradeoff energy harvesting e nergy constrained sensor networks i are introduction typically wireless powered networks by batteries such that as have limited operation time although replacing or recharging the batteries can prolong the lifetime of the network to a certain extent it usually incurs high costs and is inconvenient hazardous say in toxic environments or even impossible e g for sensors embedded in building structures or inside human bodies a more convenient safer as well as greener manuscript received february revised june and january accepted february the associate editor coordinating the review of this paper and approving it for publication was m torlak this paper has been presented in part at the ieee global communications conference globecom december houston usa r zhang is with the department of electrical and computer engineering national university of singapore e mail elezhang nus edu sg he is also with the institute for infocomm research a star singapore c k ho is with the institute for infocomm research a star singapore e mail hock a star edu sg this work was supported in part by the national university of singapore under the research grant r digital object identifier twc alternative is thus to harvest energy from the environment which virtually provides perpetual energy supplies to wireless devices in addition to other commonly used energy sources such as solar and wind ambient radio frequency rf signals can be a viable new source for energy scavenging it is worth noting that rf based energy harvesting is typically suitable for low power applications e g sensor networks but also can be applied for scenarios with more substantial power consumptions if dedicated wireless power transmission is implemented on the other hand since rf signals that carry energy can at the same time be used as a vehicle for transporting information simultaneous wireless information and power transfer swipt becomes an interesting new area of research that attracts increasing attention although a unified study on this topic is still in the infancy stage there have been notable results reported in the literature in varsh ney first proposed a capacity energy function to characterize the fundamental tradeoffs in simultaneous information and energy transfer for the single antenna or siso single input single output awgn additive white gaussian noise channel with amplitude constrained inputs it was shown in that there exist nontrivial tradeoffs in maximizing information rate versus vs power transfer by optimizing the input dis tribution however if the average transmit power constraint is considered instead the above two goals can be shown to be aligned for the siso awgn channel with gaussian input signals and thus there is no nontrivial tradeoff in grover and sahai extended to frequency selective single antenna awgn channels with the average power constraint by showing that a non trivial tradeoff exists in frequency domain power allocation for maximal information vs energy transfer as a matter of fact wireless power transfer wpt or in short wireless power which generally refers to the trans missions of electrical energy from a power source to one or more electrical loads without any interconnecting wires has been investigated and implemented with a long history generally speaking wpt is carried out using either the near field electromagnetic em induction e g inductive coupling capacitive coupling for short distance say less than a meter applications such as passive radio frequency identification rfid or the far field em radiation in the form of microwaves or lasers for long range up to a few kilometers applications such as the transmissions of energy readers may visit the company website of powercast at http www powercastco com for more information on recent applications of dedicated rf based power transfer c ieee ieee transactions on wireless communications vol no may information flow access point user terminals fig a wireless network with dual information and energy transfer from orbiting solar power satellites to earth or spacecrafts however prior research on em radiation based wpt in particular over the rf band has been pursued independently from that on wireless information transfer wit or radio communication this is non surprising since these two lines of work in general have very different research goals wit is to maximize the information transmission capacity of wireless channels subject to channel impairments such as the fading and receiver noise while wpt is to maximize the energy transmission efficiency defined as the ratio of the energy harvested and stored at the receiver to that consumed by the transmitter over a wireless medium nevertheless it is worth noting that the design objectives for wpt and wit systems can be aligned since given a transmitter energy budget maximizing the signal power received for wpt is also beneficial in maximizing the channel capacity for wit against the receiver noise hence in this paper we attempt to pursue a unified study on wit and wpt for emerging wireless applications with such a dual usage an example of such wireless dual networks is en visaged in fig where a fixed access point ap coordinates the two way communications to from a set of distributed user terminals uts however unlike the conventional wireless network in which both the ap and uts draw energy from constant power supplies by e g connecting to the grid or a battery in our model only the ap is assumed to have a constant power source while all uts need to replenish energy from the received signals sent by the ap via the far field rf based wpt consequently the ap needs to coordinate the wireless information and energy transfer to uts in the downlink in addition to the information transfer from uts in the uplink wireless networks with such a dual information and power transfer feature have not yet been studied in the literature to our best knowledge although some of their interesting applications have already appeared in e g the body sensor networks with the out body local processing units lpus powered by battery communicating and at the same time sending wireless power to in body sensors that have no embedded power supplies however how to characterize the fundamental information energy transmission tradeoff in such dual networks is still an open problem in this paper we focus our study on the downlink case with simultaneous wit and wpt from the ap to uts in the energy flow generic system model depicted in fig each ut can in gen eral harvest energy and decode information at the same time by e g applying the power splitting scheme introduced later in this paper however from an implementation viewpoint one particular design whereby each ut operates as either an information receiver or an energy receiver at any given time may be desirable which is referred to as time switching this scheme is practically appealing since state of the art wireless information and energy receivers are typically designed to operate separately with very different power sensitivities e g for information receivers vs for energy receivers as a result if time switching is employed at each ut jointly with the near far based transmission scheduling at the ap i e uts that are close to the ap and thus receive high power from the ap are scheduled for wet whereas those that are more distant from the ap and thus receive lower power are scheduled for wit then swipt systems can be efficiently implemented with existing information and energy receivers and the additional time switching device at each receiver for an initial study on swipt this paper considers the simplified scenarios with only one or two active uts in the network at any given time for the case of two uts we assume time switching i e the two uts take turns to receive energy or independent information from the ap over different time blocks as a result when one ut receives information from the ap the other ut can opportunistically harvest energy from the same signal broadcast by the ap and vice versa hence at each block one ut operates as an information decoding id receiver and the other ut as an energy harvesting eh receiver we thus refer to this case as separated eh and id receivers on the other hand for the case with only one single ut to be active at one time while all other uts are assumed to be in the off sleep mode the active ut needs to harvest energy as well as decode information from the same signal sent by the ap i e the same set of receiving antennas are shared by both eh and id receivers residing in the same ut thus this case is referred to as co located eh and id receivers surprisingly as we will show later in this paper the optimal information energy tradeoff for the case of co located receivers is more challenging to characterize than that for the case of separated receivers due to a potential limitation that practical eh receiver circuits are not yet able to decode the information directly and vice versa note that similar to the case of separated receives time switching can also be applied in the case of co located receivers to orthogonalize the information and energy transmissions at each receiving antenna however this scheme is in general suboptimal for the achievable rate energy tradeoffs in the case of co located receivers as will be shown later in this paper some further assumptions are made in this paper for the pur pose of exposition firstly this paper considers a quasi static fading environment where the wireless channel between the ap and each ut is assumed to be constant over a sufficiently long period of time during which the number of transmitted symbols can be approximately regarded as being infinitely large under this assumption we further assume that it is feasible for each ut to estimate the downlink channel from the ap and then send it back to the ap via the uplink since the time overhead for such channel estimation and feedback is zhang and ho mimo broadcasting for simultaneous wireless information and power transfer energy receiver transmitter information receiver fig a mimo broadcast system for simultaneous wireless information and power transfer a negligible portion of the total transmission time due to quasi static fading we will address the more general case of fading channels with imperfect partial channel knowledge at the transmitter in our future work secondly we assume that the system under our study typically operates at the high signal to noise ratio snr regime for the id receiver in the case of co located receivers this is to be compatible with the high power operating requirement for the eh receiver of practical interest as previously mentioned thirdly without loss of generality we assume a multi antenna or mimo multiple input multiple output system in which the ap is equipped with multiple antennas and each ut is equipped with one or more antennas for enabling both the high performance wireless energy and information transmissions as it is well known that for wit only mimo systems can achieve folded array capacity gains over siso systems by spatial beamforming multiplexing under the above assumptions a three node mimo broad cast system is considered in this paper as shown in fig wherein the eh and id receivers harvest energy and decode information separately from the signal sent by a common transmitter note that this system model refers to the case of separated eh and id receivers in general but includes the co located receivers as a special case when the mimo channels from the transmitter to both receivers become iden tical assuming this model the main results of this paper are summarized as follows energy harvester energy harvester information decoder a time switching information decoder a time switching b b power power splitting splitting fig two practical designs for the co located energy and information receivers which are applied for each receiving antenna paper is new and has not yet been studied by any prior work for the case of co located eh and id receivers we show that the proposed solution for the case of separated receivers is also applicable with the identical mimo channel from the transmitter to both id and eh receivers furthermore we consider a potential practical constraint that eh receiver circuits cannot directly decode the information i e any information embedded in received signals sent to the eh receiver is lost during the eh process under this constraint we show that the r e region with the optimal transmit covariance obtained without such a constraint in general only serves as a performance outer bound for the co located receiver case hence we investigate two practical receiver designs namely time switching and power splitting for the case of co located receivers as shown in fig for time switching each receiving antenna periodically switches between the eh receiver and id receiver whereas for power splitting the received signal at each antenna is split into two separate signal streams with different power levels one sent to the eh receiver and the other to the id receiver note that time switching has also been proposed in for the siso awgn channel furthermore note that the antenna switching scheme whereby the receiving antennas are divided into two groups with one group switched to information decoding and the other group to energy harvesting can be regarded as a special case of power splitting with only binary splitting power ratios at each receiving antenna for these practical receiver for the case of separated eh and id receivers we design the optimal transmission strategy to achieve dif ferent tradeoffs between maximal information rate vs energy transfer which are characterized by the boundary of a so called rate energy r e region we derive a semi closed form expression for the optimal transmit covariance matrix for the joint precoding and power allocation to achieve different rate energy pairs on the designs we derive their achievable r e regions as com pared to the r e region outer bound and characterize the conditions under which their performance gaps can be closed for example we show that the power splitting scheme approaches the tradeoff upper bound asymptot ically when the rf band antenna noise at the receiver becomes more dominant over the baseband processing noise more details are given in section iv c boundary of the r e region note that the r e region is the rest of this paper is organized as follows section a multiuser extension of the single user capacity energy ii presents the system model characterizes the rate energy function in also note that the multi antenna broadcast region and formulates the problem for finding the optimal channel bc has been investigated in e g for transmit covariance matrix section iii presents the opti information transfer solely by unicasting or multicasting mal transmit covariance solution for the case of separated however mimo bc for swipt as considered in this receivers section iv extends the solution to the case of ieee transactions on wireless communications vol no may co located receivers to obtain a performance upper bound proposes practical receiver designs and analyzes their perfor mance limits as compared to the performance upper bound finally section v concludes the paper and provides some promising directions for future work notation for a square matrix s tr s s s and s denote its trace determinant inverse and square root respec tively while s and s mean that s is positive semi definite and positive definite respectively for an arbitrary size matrix m m h and m t denote the conjugate transpose and transpose of m respectively diag x denotes an m m diagonal matrix with x x m x m being the diagonal elements i and denote an identity matrix and an all zero vector respectively with appropriate dimensions e denotes the statistical expectation the distribution of a circularly symmetric complex gaussian cscg random vector with mean x and covariance matrix σ is denoted by cn x σ and stands for distributed as cx y denotes the space of x y matrices with complex entries z is the euclidean norm of a complex vector z and z is the absolute value of a complex scalar z max x y and min x y denote the maximum and minimum between two real numbers x and y respectively and x max x all the log functions have base by default ii system model and problem formulation as shown in fig this paper considers a wireless broadcast system consisting of one transmitter one eh receiver and one id receiver it is assumed that the transmitter is equipped with m transmitting antennas and the eh receiver and the id receiver are equipped with n eh receiving antennas respectively in addition it is assumed that the transmitter and both receivers operate over the same frequency band assuming a narrow band transmission over quasi static fading channels the baseband equivalent channels from the transmitter to the eh receiver and id receiver can be modeled by matrices g cn eh and n id m respectively it is assumed that at each fading state g and h are both known at the transmitter and separately known at the corresponding receiver note that for the case of co located eh and id receivers g is identical to h and thus n eh m and h cn id n id it is worth noting that the eh receiver does not need to convert the received signal from the rf band to the baseband in order to harvest the carried energy nevertheless thanks to the law of energy conservation it can be assumed that the total harvested rf band power energy normalized by the baseband symbol period denoted by q from all receiving antennas at the eh receiver is proportional to that of the received baseband signal i e q ζe gx n where ζ is a constant that accounts for the loss in the energy transducer for converting the harvested energy to electrical energy to be stored for the convenience of analysis it is assumed that ζ in this paper unless stated otherwise we use x n cm to denote the baseband signal broadcast by the transmitter at the nth symbol interval which is assumed to be random over n without loss of generality the expectation in is thus used to compute the average power harvested by the eh receiver at each fading state note that for simplicity we assumed in that the harvested energy due to the background noise at the eh receiver is negligible and thus can be ignored on the other hand the baseband transmission from the transmitter to the id receiver can be modeled by y n hx n z n where y n cn id denotes the received signal at the nth symbol interval and z n cn id denotes the receiver noise vector it is assumed that z n are independent over n and z n cn i under the assumption that x n is random over n we use s e x n xh n to denote the covariance matrix of x n in addition we assume that there is an average power constraint at the transmitter across all trans mitting antennas denoted by e x n tr s p in the following we examine the optimal transmit covariance s to maximize the transported energy efficiency and information rate to the eh and id receivers respectively consider first the mimo link from the transmitter to the eh receiver when the id receiver is not present in this case the design objective for s is to maximize the power q received at the eh receiver since from it follows that q tr gsgh with ζ the aforementioned design problem can be formulated as max s gsg t tr s p s let t q tr h min m n eh and the reduced singular value decomposition where u g svd of g be denoted cn eh t and v g by g u g cm t γ each g v h g of which consists of orthogonal columns with unit norm and γ g furthermore diag g g let v denote t with g the first g column of v g g then t we have the following proposition proposition pv vh proof the proof follows directly by examining the svd of s and is thus omitted here for brevity please refer to a longer version of this paper for the complete proof it follows that the maximum harvested power at the eh receiver is given by q max g p it is worth noting that since s eh is a rank one matrix the maximum harvested power is achieved by beamforming at the transmitter which aligns with the strongest eigenmode of the matrix x n g h g n i e the where transmitted n is an signal arbitrary can random be written signal as over n with zero mean and unit variance and v is the transmit beamforming vector for convenience we name the above transmit beamforming scheme to maximize the efficiency of wpt as energy beamforming next consider the mimo link from the transmitter to the id receiver without the presence of any eh receiver results of this paper are readily extendible to study the impacts of non negligible background noise and or co channel interference on the swipt system performance given s s eh pv the optimal solution to is s eh zhang and ho mimo broadcasting for simultaneous wireless information and power transfer assuming the optimal gaussian codebook at the transmitter i e x n cn s the transmit covariance s to maximize the transmission rate over this mimo channel can be obtained by solving the following problem max s r log i hshh t tr s p s the optimal solution to the above problem is known to have the following v h cm t form s id is obtained from the reduced v h λv svd h h where of h expressed by h u u h cn id t γ h h γ h v h h with t min m n id diagonal h elements t diag h and λ diag p h p t obtained from the standard h h t water filling with the wf power allocation solution p i ν with t ν being the so called constant water level that makes i is rate then is p i given achieved p by the r in max corresponding general t i by maximum transmission rate the maximum spatial multiplexing over up to t log h i p i spatially decoupled awgn channels together with the gaussian codebook i e the transmitted signal can be expressed as x n sian random vector v cn i h λ n v h where and n is denote a gaus the precoding matrix and the diagonal power allocation matrix respectively remark it is worth noting that in problem it is assumed that the transmitter sends to the eh receiver continuously now suppose that the transmitter only transmits a fraction of the total time denoted by α with α furthermore assume that the transmit power level can be adjusted flexibly provided that the consumed average power is bounded by p i e α tr s α p or tr s p α in this case it can be easily shown that the transmit covariance s p α v q max g p vh for also any achieves the maximum harvested power α which suggests that the maximum power delivered is independent of transmission time however unlike the case of maximum power transfer the maximum information rate reliably transmitted to the id receiver requires that the transmitter send signals continuously i e α as assumed in problem this can be easily verified by observing that for any α and s αlog i h s α h h log i hsh h where the equality holds only when α since r is a nonlinear concave function of s thus to maximize both power and rate transfer at the same time the transmitter should broadcast to the eh and id receivers all the time furthermore note that the assumed gaussian distribution for transmitted signals is necessary for achieving the maximum rate transfer but not necessary for the maximum power transfer in fact for any arbitrary ministic transmitted complex number signal c x n that satisfies c even a deter c n achieves the maximum transferred power q max in problem however to maximize simultaneous power and information transfer with the same transmitted signal the gaussian input distribution is sufficient as well as necessary h i i t pv r eh q max w m y g r e n e optimal transmit covariance time sharing r max q id rate mbps fig rate energy tradeoff for a mimo broadcast system with separated eh and id receivers and m n eh n id now consider the case where both the eh and id receivers are present from the above results it is seen that the optimal transmission strategies for maximal power transfer and information transfer are in general different which are energy beamforming and information spatial multiplexing respectively it thus motivates our investigation of the fol lowing question what is the optimal broadcasting strategy for simultaneous wireless power and information transfer to answer this question we propose to use the rate energy r e region defined below to characterize all the achievable rate in bits sec hz or bps for information transfer and energy in joule sec or watt for power transfer pairs under a given transmit power constraint without loss of generality assuming that the transmitter sends gaussian signals continuously cf remark the r e region is defined as c r e p r q r log i hsh h q tr gsg h tr s p s in fig an example of the above defined r e region see section iii for the algorithm to compute the boundary of this region is shown for a practical mimo broadcast system with separated eh and id receivers i e g h it is assumed that m n eh n id the transmitter power is assumed to be p watt w or the distances from the transmitter to the eh and id receivers are assumed to be meter and meters respectively thus we can exploit the near far based energy and information transmission scheduling which may correspond to e g a dedicated energy transfer system to near users with op portunistic information transmission to far users or vice versa assuming a carrier frequency of f c and the power pathloss exponent to be the distance dependent signal attenuation from the ap to eh id receiver can be estimated as and respectively accordingly the average signal power at the eh id receiver is thus and respectively it is ieee transactions on wireless communications vol no may further assumed that in addition to signal pathloss rayleigh fading is present as such each element of channel matrices g and h is independently drawn from the cscg distribution with zero mean and variance for eh receiver and for for id receiver respectively to be consistent with the signal pathloss previously assumed furthermore the bandwidth of the transmitted signal is assumed to be while the receiver noise is assumed to be white gaussian with power spectral density hz which is dominated by the receiver processing noise rather than the background thermal noise or average power over the bandwidth of as a result considering all of transmit power signal attenuation fading and receiver noise the per antenna average snr at the id receiver is equal to which corresponds to p in the equivalent signal model for the id receiver given in with unit norm noise in addition we assume that for the eh receiver the energy conversion efficiency is ζ considering this together with transmit power and signal attenuation the average per antenna signal power at the eh receiver is thus from fig it is observed that with energy beamforming the maximum harvested energy rate for the eh receiver is around q max than those by simply time sharing the optimal transmit covariance matrices s eh and s id for eh and id receivers separately see the dashed line in fig problem is a convex optimization problem since its objective function is concave over s and its constraints specify a convex set of s note that resembles a similar problem formulated in see also and references therein under the cognitive radio cr setup where the rate of a secondary mimo link is maximized subject to a set of so called interference power constraints to protect the co channel primary receivers however there is a key difference between and the problem in the harvested power constraint in has the reversed inequality of that of the interference power constraint in since in our case it is desirable for the eh receiver to harvest more power from the transmitter as opposed to that in the interference power at the primary receiver should be minimized as such it is not immediately clear whether the solution in can be directly applied for solving with the reversed power inequality in the following we will examine the solutions to problem for the two cases with arbitrary g and h the case of separated receivers and g h the case of co located receivers respectively while with spatial multiplexing the maximum information rate for the id receiver is around r max it is easy to identify two boundary points of this r e region denoted by r eh q max and r max q id respectively for the former boundary point the transmit co variance is s eh which corresponds to transmit beamforming and achieves the maximum transferred power q max to the eh receiver while the resulting information rate for the id receiver is given by r eh on the other hand for the latter boundary point the transmit covariance is s id iii separated receivers consider the case where the eh receiver and id receiver are spatially separated and thus in general have different channels from the transmitter in this section we first solve problem with arbitrary g and h and derive a semi closed form expression for the optimal transmit covariance log hv then we examine the optimal solution for the special case of miso channels from the transmitter to id and or eh receivers which corresponds to transmit spatial multiplexing and achieves the maximum information rate transferred to the id receiver eh since problem is convex and satisfies the slater condition it has a zero duality gap and thus can be solved r max while the resulting receiver is given by q id power transferred tr gs id g h to the using the lagrange duality method thus we introduce two non negative dual variables λ and μ associated with the since the optimal tradeoff between the maximum energy and information transfer rates is characterized by the boundary of the r e region it is important to characterize all the boundary rate power pairs of c r e p for any p from fig it is easy to observe that if r r eh the maximum harvested power q max is achievable with the same transmit harvested power constraint and transmit power constraint in respectively the optimal solution to problem is then given by the following theorem in terms of λ and μ which are the optimal dual solutions of problem see appendix any pair of a q for q details id q q note max that for problem given covariance that achieves the rate power pair r eh q max similarly the maximum information rate r max is achievable and p there exists one unique pair of λ and μ theorem provided that q q id thus the remaining boundary the optimal solution to problem has the following form of c r e p yet to be characterized is over the intervals r eh r r max q id q q max we thus consider the following optimization problem s a v where a μ i λ g λ v h a max s h g v cm t is obtained i hshh from the reduced svd of the matrix ha given by log t tr gsg q tr s p s ha u γ v h with γ diag h h t h time sharing we mean that the ap transmits simultaneously to both eh and id receivers with the energy maximizing transmit covariance s eh note that if h i e q takes values from q id q q max the corresponding optimal rate solutions of the above problems energy beamforming for β portion of each block time and the information rate maximizing transmit covariance s id are the boundary rate points of the r e region over r eh i e spatial multiplexing for the remaining β portion of each block time with β r r max notice that the transmit covariance solutions to the above problems in general yield larger rate power pairs is worth noting that problem is convex and thus can be solved efficiently by the interior point method in this paper we apply the lagrange duality method for this problem mainly to reveal the optimal precoder structure zhang and ho mimo broadcasting for simultaneous wireless information and power transfer h p i h i h i t and λ diag p t p t with proof see appendix a h note that this theorem requires that a μ i λ g g implying that μ eigenvalue of matrix ghg λ g which recall is not that present g is the largest for a similar result in under the cr setup with the reversed interference power constraint one algorithm that can be used to solve is provided in table i of appendix a from theorem the maximum transmission rate for problem can be shown to be r log i hs h h t i log h i p i for which the proof is omitted here for brevity next we examine the optimal solution to problem for the special case where the id receiver has one single antenna i e row vector n id h h with and thus h the mimo channel cm suppose that h reduces to a the eh receiver is still equipped with n eh antennas and thus the mimo channel g remains unchanged from theorem we obtain the following corollary see for the proof corollary in the case of miso channel from the transmitter to id receiver i e h h h the optimal solution to problem reduces to the following form s a h where a μ i λ g h g with λ and μ denoting the optimal dual solutions of problem correspondingly the optimal value of is r from it is observed that the optimal transmit covariance is a rank one matrix from which it follows that beamforming is the optimal transmission strategy in this case where the transmit beamforming vector should be aligned with the vector a log a h moreover consider the case where both channels from the transmitter to id eh receivers are miso i e h h h and g gh with g cm from corollary it follows immediately that the optimal covariance solution to problem is still beamforming in the following theorem we show a closed form solution of the optimal beamforming vector at the transmitter for this special case which differs from the semi closed form solution that was expressed in terms of dual variables theorem in the case of miso channels from transmit ter to both id and eh receivers i e h h h and g gh the optimal solution to problem can be expressed as s pvvh where the beamforming vector v has a unit norm and is given by v h ˆ q gh h ˆ where ˆ h ˆ g h g h h h h ˆg h h ˆg ˆg g g and α gh ˆg h h with h g h g with α gh denoting the phase of complex number α gh correspondingly the optimal value of is given in shown at the top of the next page proof the proof is similar to that of theorem in and is thus omitted for brevity ˆg p q g ej α gh p a q g ˆ h g gh a ˆ h q p g h h a it n u ρ ρ y g r e n e ρ rate bits channel use fig rate energy tradeoff for a miso broadcast system with correlated miso channels to the separated eh and id receivers it is worth noting that in if h ˆ the optimal transmit beamforming vector is based on the principle of maximal ratio combining mrc with respect to the miso channel h q gh h from the transmitter to the id receiver and in this indeed case not the active harvested however power when constraint in h ˆ problem the optimal is beamforming vector is a linear combination of the two vectors ˆg and q gh h ˆ g and the combining coefficients are designed such that the harvested power constraint is satisfied with equality in fig we show the achievable r e regions for the case of miso channels from the transmitter to both eh and id receivers we set p for the purpose of exposition it is assumed that h g and α gh ρ with ρ denoting the correlation between the two unit norm vectors h and g this channel setup may correspond to the practical scenario where the eh and id receivers are equipped at a single device but still physically separated and as a result their respective miso channels from the transmitter have the same power gain but are spatially correlated due to the insufficient spacing between two separate receiving antennas from theorem the r e regions for the three cases of ρ and are obtained as shown in fig interestingly it is observed that increasing ρ enlarges the achievable r e region which indicates that the antenna correlation between the eh and id receivers can be a bene ficial factor for simultaneous information and power transfer note that in this figure we express energy and rate in terms of energy unit and bits channel use respectively since their practical values can be obtained by appropriate scaling based on the realistic system parameters as for fig iv co located receivers in this section we address the case where the eh and id receivers are co located and thus possess the same channel from the transmitter i e g h and thus n eh n id n we first examine the optimal solution of problem for this case from which we obtain an outer bound for the achievable rate power pairs in the r e region then we propose two ieee transactions on wireless communications vol no may r log h q gh h ˆ log practical receiver designs namely time switching and power splitting derive their optimal transmission strategies to maxi mize the achievable rate power pairs and finally compare the results to the r e region outer bound a performance outer bound consider problem reduced svd of h is given with by g h h u h recall that the γ h t min m n diag h h from t theorem h h we obtain γ h h the t v following h h with and corollary see for the proof corollary in the case of co located eh and id re ceivers with g h the optimal solution to problem has the form of s v h with the diagonal elements h h obtained from the following mod ified wf power allocation ˆp i σv where σ diag ˆp ˆp t with λ and μ denoting the optimal dual solutions of problem rate the is μ r algorithm λ h t i in the table corresponding i maximum transmission for solving problem with arbitrary g and h can be simplified to solve the special case with g h corollary reveals that for problem in the case of g h the optimal transmission strategy is in general still spatial multiplexing over the eigenmodes of the mimo channel h as for problem while the optimal tradeoffs between information transfer and power transfer are achieved by varying the power levels allocated into different eigenmodes as shown in it is interesting to observe that the power allocation in reduces to the conventional wf solution in with a constant water level when λ i e the harvested power constraint in problem is inactive with the optimal power allocation however when λ and thus the harvested power constraint is active corresponding to the pareto optimal regime of our interest the power allocation in is observed to have a non decreasing water level as h i log h i ˆp i increase note that this modified wf policy has also been shown in for power allocation in frequency selective awgn channels using corollary we can characterize all the boundary points of the r e region c r e p defined in for the case of co located receivers with g h for example if the total transmit power is allocated to the channel with the largest gain h i e ˆp p and ˆp i i t the maximum harvested power q max is achieved by transmit beam forming on the other hand if transmit spatial multiplexing is applied with the conventional wf power allocation given in with λ the corresponding r becomes the maximum transmission rate r max ph however unlike the case of separated eh and id receivers in which the entire boundary of c r e p is achievable in the case of co located receivers except the μ λ h i h i i t g q α gh p g q two boundary rate power pairs r max all the other boundary pairs of c r e p may not be achievable in practice note that these boundary points are achievable if and only if iff the following premise is true the power of the received signal across all antennas is totally harvested and at the same time the carried information with a transmission rate up to the mimo channel capacity for a given transmit covariance is decodable however existing eh circuits are not yet able to directly decode the information carried in the rf band signal even for the siso channel case as a result how to achieve the remaining boundary rate power pairs of c r e p in the mimo case with the co located eh and id receiver remains an interesting open problem therefore in the case of co located receivers the boundary of c r e p given by corollary in general only serves as an outer bound for the achievable rate power pairs with practical receiver designs as will be investigated in the following subsections b time switching first as shown in fig a we consider the time switching ts scheme with which each transmission block is divided into two orthogonal time slots one for transferring power and the other for transmitting data the co located eh and id re ceiver switches its operations periodically between harvesting energy and decoding information between the two time slots it is assumed that time synchronization has been perfectly established between the transmitter and the receiver and thus the receiver can synchronize its function switching with the transmitter with orthogonal transmissions the transmitted signals for the eh receiver and id receiver can be designed separately but subject to a total transmit power constraint let α with α denote the percentage of transmission time allocated to the eh time slot we then consider the following two types of power constraints at the transmitter fixed power constraint the transmitted signals to the id and eh receivers have the same fixed power constraint given by tr s and s p and tr s p where s denote the transmit covariance matrices for the id and eh transmission time slots respectively flexible power constraint the transmitted signals to the id and eh receivers can have different power constraints provided that their average consumed power is below p i e α tr s αtr s p note that the ts scheme under the fixed power constraint has been considered in for the single antenna awgn channel the achievable r e regions for the ts scheme with the fixed referred to as ts power constraints are then given as follows cts r e vs flexible referred to as ts p r q r α log i hs hh α q αtr hs hh tr s p tr s p h α gh gh ˆ h q p g and q max zhang and ho mimo broadcasting for simultaneous wireless information and power transfer cts r e p r q r α log i hs hh α q αtr hs h h α tr s αtr s p it is worth noting since any pair of that s c r e ts and p s c r e ts that p must be true satisfy the fixed power constraint will satisfy the flexible power constraint but not vice versa the optimal transmit covariance and power s constraint to achieve are the given boundary in section of c r e ts ii matrices s p with the fixed assuming g h in fact connecting the boundary of the two points c r r e ts max p is and simply q a max straight line cf fig by sweeping α from to similarly for the case of flexible power constraint the transmit covariance solutions boundary set of eigenvectors point of c r e ts as for s and s to achieve any p can be shown to have those given in section ii the same assuming g h respectively however the corresponding time allocation for α and power allocation for s and s remain unknown we thus have the following proposition proposition in the case of flexible power constraint except the two points r max boundary points of the and region c ts q max all other α accordingly c r e ts r e p are achieved p can be simplified as as c r e ts p r q r log i hs h h tr s p q h s proof the proof follows easily from the fact that the harvested power is a linear function of s while the informa tion rate is a logarithm function of s cf remark thus to maximize the information rate with any given harvested power we should minimize the portion of time allocated for power transfer i e α for the complete proof of this proposition please refer to and s can be easily obtained given and are thus omitted for brevity proposition suggests that to achieve any boundary point the portion r q of of transmission c r e ts p with r r max and q q max time α allocated to power transfer in each block should asymptotically go to zero when n where n denotes the number of transmitted symbols in each block for example by allocating o logn symbols per block for power transfer and the remaining symbols for information transmission yields α logn n as n which satisfies the optimality condition given in proposition ible it is power worth constraint noting that case the is boundary achieved of under c r e ts p the in the flex assumption that the transmitter and receiver can both operate in the regime of infinite power in the eh time slot due to α which can not be implemented with practical power amplifiers hence a more feasible region for transmit power constraints c r e ts p is in obtained by adding as tr s p peak that the peak power constraint in this context is different from the signal amplitude constraint considered in the corresponding optimal power allocation for s ρ power power eh receiver splitter id receiver tn a ρ tn p a co located receivers with a power splitter tn a id receiver b id receiver without a power splitter fig receiver operations with without a power splitter the energy harvested due to the receiver noise is ignored for eh receiver and tr s tn p p peak with p peak p similar to proposition it can be shown that the boundary of the achievable r e region in this case denoted by α q h p peak note the achievable r e region without any peak power by that c we r e ts can p p equivalently peak is achieved c r e ts constraint p defined as c r e ts denote in or p c power splitting next we propose an alternative receiver design called power splitting ps whereby the power and information transfer to the co located eh and id receivers are simulta neously achieved via a set of power splitting devices one device for each receiving antenna as shown in fig b in order to gain more insight into the ps scheme we consider first the simple case of a single antenna awgn channel with co located id and eh receivers which is shown in fig a for the ease of comparison the case of solely information transfer with one single id receiver is also shown in fig b the receiver operations in fig a are explained as follows the received signal from the antenna is first corrupted by a gaussian noise denoted by n a t at the rf band which is assumed to have zero mean and equivalent baseband power which a the rf band signal is is assumed to be perfect then fed into a power splitter without any noise induced after the power splitter the portion of signal power split to the eh receiver is denoted by ρ and that to the id receiver by ρ the signal split to the id receiver then goes through a sequence of standard operations see e g to be converted from the rf band to baseband during this process the signal is additionally corrupted by another noise n p t which is independent of n a t and assumed to be gaussian and have zero mean and case with solely the id variance receiver it p is to be consistent reasonable to to the assume that the antenna noise n a t and processing noise n p t have the same distributions in both figs a and b it is further assumed model introduced that a in section p to ii be consistent with the system for this simple siso awgn channel we denote the transmit power constraint by p and the channel power gain ieee transactions on wireless communications vol no may by h it is then easy to compute the r e region outer bound c r e the noiseless antenna is assumed which leads to the smallest p for this channel with co located id and eh r e region for the siso awgn channel case the obtained receivers which is simply a box specified by three vertices r e region will thus provide the performance lower bound for q max r max and r max q max with q max ph the ps scheme with practical receiver circuits in this case and r max log ph interestingly we will show next that since there is no antenna noise and the processing noise is under certain conditions the ps scheme can in fact achieve all added after the power splitting it is equivalent to assume the rate energy pairs in this r e region outer bound without that the aggregated receiver noise power remains unchanged loss of generality it suffices to show that the vertex point with a power splitter at each receiving antenna let ρ i r max with q max is achievable ρ i with reference to fig a we discuss the ps scheme in the following three regimes with different values of antenna and processing noise power denote the portion of power split to the eh receiver at the ith receiving antenna i n the achievable r e region for the ps scheme in the worst case is thus given by receiving a antenna p case i in this ideal case with perfect the antenna noise can be ignored and cps r e thus we have a p r q q tr λ ρ hsh h to show that the snr and denoted p by accordingly τ at the id ρ i i it is easy receiver r log i in fig a is ρ ph the achievable r e region in this case is then given by c where λ ρ λ ρ hsh h λ ρ tr s p s r log ρ ph q r e ps i ρp p h this ρ region r can q be shown to coincide with the r e region for the ts scheme with the flexible power constraint given by for the siso case diag ρ ρ n and λ ρ i λ ρ note that the two points r max and q max on the boundary case since a case ii this is the most practically valid and ρ i is given by of i cps r e respectively p can be with simply the achieved corresponding with ρ i transmit i τ p ρ p a ρ p h a we can show that h ρ a τ in this case covariance matrices given in section ii with g h similar p to the ts case all the other boundary points be obtained as follows let h λ of cps r e p can accordingly the achievable in r this log case is τ q given by ρp c r e ps ii h p it is easy ρ to r e region r q show that and for a rps given r e p ρ set i denote the achievable ρ of ρ i h g r e region λ with ρ h ps c then we can obtain the boundary of to baseband r e a ps ii p p enlarges case iii strictly in this as ideal a increases case with from perfect to signal conversion the processing noise rps with r e h p ρ and i g replaced by solving by similar h the boundary of cps r e problems like problem rf and g respectively finally can be ignored and thus we have p p can be obtained by taking a union and in this case the snr for the id receiver is given a by operation over different rps p ρ i with all possible ρ i in particular we consider two special cases of the ps τ ph regardless of the value of ρ thus to maximize scheme i uniform power splitting ups with ρ i the power transfer ideally we should set ρ i e splitting infinitesimally small power to the id receiver since both the signal and antenna noise are scaled iden tically by the power splitter and there is no additional processing noise induced after the power splitting with ρ the achievable r e region in this case is given by c ρ i and ρ and ii on off power splitting with ρ i i i e ρ i taking the value of either or for the case of on off power splitting let ω receiving antennas with ρ i n then ω denote n one subset of ω denotes the clearly ω and other ω subset specify of receiving the antennas with ρ i sets of receiving antennas which r e ps iii becomes p r q r log ph q ph identical to the r e region outer bound switched to eh and id receivers respectively thus the on off power splitting is also termed antenna switching as c r e p which is a box as defined earlier let rups p ρ denote the achievable r e region for the therefore we know from the above discussions that only ups scheme with any fixed ρ and cups r e for the case of noise free rf band to baseband processing i e case iii the ps scheme achieves the r e region outer bound and is thus optimal however in practice such a condition can never be met perfectly and thus the r e region outer bound c r e p be the r e region by taking the union of all rups p ρ over ρ furthermore let ras p ω denote the achievable r e region for pair the of ω as and or ω on off it is not power difficult splitting to see scheme that for with any a p given p is in general still non achievable with practical ps receivers in the following we will study further the achievable cups while r e p cups r e cps r e p and ras p ω cps r e p ω r e region by the ps scheme for the more general case of mimo channels it is not difficult to show that if each receiving antenna satisfies the condition in case iii the r e region outer bound c r e channel of h p moreover cps r e the p following for the proposition case of miso siso shows that for is also the true case of simo channel of h cups r e p cps r e p p defined in with g h is achievable for the mimo case by the ps scheme with each receiving antenna to set ρ for a more practical proposition in the case of co located eh and id receivers with a simo channel h h cn for any p cups r e purpose we consider in the rest of this section the worst case performance of the ps scheme i e case i in the above when p log h q q cps r e p r q r h proof see appendix b zhang and ho mimo broadcasting for simultaneous wireless information and power transfer d performance comparison the following proposition summarizes the performance comparison between the ts and ups schemes proposition for the co located eh and id receivers with any p c ts ts cups r e p c r e ts r e p p iff p cups r e p c r e p h h while proof please refer to from the above proposition it follows that the ts scheme with the fixed power constraint performs worse than the ups scheme in terms of achievable rate energy pairs however the ups scheme in general performs worse than the ts scheme under the flexible power constraint without any peak power constraint while they perform identically iff the condition p h h is satisfied this may occur when e g p is sufficiently small unlikely in our model since high snr is of interest or h i e h is miso or simo note that the performance comparison between the ts scheme with the flexible power constraint and the ps scheme with arbitrary power splitting instead of ups remains unknown theoretically next for the purpose of exposition we compare the rate energy tradeoff for the case of co located eh and id receivers for a symmetric mimo channel g h θ θ with θ for fig and θ for fig respectively it is assumed that p the r e region outer bound is obtained as c r e p with g h according to corollary the two achievable r e regions for the ts scheme with fixed vs flexible power constraints are shown for comparison and it is observed that region for the ts cts scheme r e p with cts r e the flexible p the power achievable constraint r e p as well also shown as which the is peak observed power to constraint lie between p peak c ts is c the r e ts ups p moreover scheme is the achievable shown whose boundary r e region points cups r e r e constitute p p and for those of rups p ρ with different ρ from to it is observed accordance that with c r e ts proposition p cups r e p note c that r e ts for p this which channel is in the r e only region provides cps r e negligible p by the ups scheme and for the general ps scheme defined in is thus rate energy not shown gains here over in cups addition r e p the achievable r e region ras p ω for the as scheme is shown which is the same for ω or due to the symmetric channel setup furthermore by comparing figs and it is observed that the performance gap between cups to r e p this and is because c r e ts p is reduced when θ increases from for this channel setup h θ and h θ and as a result in θ proposition for any p for cups r e the condition finally p cts it r e is p h h p will hold when worth pointing out that in practical swipt systems with the co located eh id receiver under the high snr condition the receiver typically operates at the high energy regime in the achievable rate energy regions shown in figs and which corresponds to applying very large values of the time switching coefficient α or the power splitting coefficient ρ i e α and ρ t i n u y g r e n e outer bound time switching with fixed power constraint time switching with flexible power constraint p peak time switching with flexible power constraint p peak power splitting ρ antenna switching rate bits channel use fig rate energy tradeoff for a mimo broadcast system with co located eh and id receivers and h 250 t i n u y g r e n e outer bound time switching with fixed power constraint time switching with flexible power constraint p peak time switching withflexible power constraint p peak uniform power splitting ρ antenna switching rate bits channel use fig rate energy tradeoff for a mimo broadcast system with co located eh and id receivers and h v concluding remarks this paper investigated the performance limits of emerging wireless powered communication networks by means of opportunistic energy harvesting from ambient radio signals or dedicated wireless power transfer under a simplified three node setup our study revealed some fundamental tradeoffs in designing wireless mimo systems for maximizing the effi ciency of simultaneous information and energy transmission due to the space limitation there are serval important issues unaddressed in this paper and left for our future work some of which are highlighted as follows it will be interesting to extend the rate energy re gion characterization to more general mimo broadcast systems with more than two receivers depending on whether the energy and information receivers are sep arated or co located and the broadcast information is for unicasting or multicasting various new problems ieee transactions on wireless communications vol no may can be formulated for which the optimal solutions are challenging to obtain for the case of co located energy and information re ceivers this paper shows a performance bound that in general cannot be achieved by practical receivers although this paper has shed some light on practical hardware designs to approach this limit e g by the power splitting scheme further research endeavor is still required to further reduce or close this gap even for the siso awgn channel in this paper to simplify the analysis it is assumed that the energy conversion efficiency at the energy receiver is independent of the instantaneous amplitude of the received radio signal which is in general not true for practical rf energy harvesting circuits thus how to design the broadcast signal waveform namely energy modulation to maximize the efficiency of energy transfer to multiple receivers under practical energy conversion constraints is an open problem of high practical interests unlike the traditional view that the receiver noise and or co channel interference degrade the communication link reliability they are however beneficial from the viewpoint of rf energy harvesting thus there exist non trivial tradeoffs in allocating communication resources to optimize the network interference levels for achieving maximal information vs energy transfer more studies to reveal such tradeoffs are worth pursuing appendix a proof of theorem the lagrangian of can be written as l s λ μ log i hsh λ tr gsg μ tr s p then the lagrange dual function of is defined as g λ μ max s h h q l s λ μ and the dual problem of denoted as d is defined as min λ μ g λ μ since can be solved equivalently by solving d in the following we first maximize the lagrangian to obtain the dual function with fixed λ and μ and then find the optimal dual solutions λ and μ to minimize the dual function the transmit covariance s that maximizes the lagrangian to obtain g λ μ is thus the optimal primal solution of consider first the problem of maximizing the lagrangian over s with fixed λ and μ by discarding the constant terms associated with λ and μ in this problem can be equivalently rewritten as max s i hsh tr μi λg log h h g s recall that g is the largest eigenvalue of the matrix ghg we then have the following lemma lemma a for the problem in to have a bounded optimal value μ λg must hold proof we prove this lemma by contradiction suppose that μ λg then let s positive constant substituting s βv into vh with yields β being any log β hv μ since h and g are either inde pendent in the case of separated receivers or identical in the case of co located receivers it is valid to assume that hv and thus the value of the above function or the optimal value of problem becomes unbounded when β thus the presumption that μ λg cannot be true which completes the proof since problem should have a bounded optimal value it follows from the above lemma that the optimal primal and dual solutions μi λg h of are obtained when μ λg let a thus a g it then follows that a with μ λg and exists the problem in is then rewritten as max s log i hsh h tr as let the reduced svd of the matrix ha be given by ha γ diag h u γ v h where u cm t v cm t has been shown in h t under with h h the cr setup that h the t optimal it solution to problem with arbitrary a has the following form s a v h a where λ v λ diag p h i i t p t with p i next we address how to solve the dual problem d by minimizing the dual function g λ μ subject to λ μ and the new constraint μ λg this can be done by applying the subgradient based method e g the ellipsoid method for which it can be shown the proof is omitted for given brevity by tr gs that the g h subgradient q of g λ μ at point λ μ is p tr s where s is given in which is the optimal solution of problem for a given pair of λ and μ when the optimal dual solutions λ and μ are obtained by the ellipsoid method the corresponding optimal solution s for problem converges to the primal optimal solution to problem denoted by s the above procedures for solving are summarized in table i the proof of theorem is thus completed appendix b proof of proposition since h h h h n t for any set of ρ i with p n ρ i the harvested power is equal to q simo i channel ρ i h i for clearly decoding q information h then the becomes equivalent h t since for the simo channel the transmit covariance matrix degrades to a scalar equal to initialize λ μ μ λg repeat until λ and μ converge to the prescribed accuracy set s s ρ compute s using with the given λ and μ compute the subgradient of g λ μ update λ and μ using the ellipsoid method subject to μ λg β λg h table i a lgorithm for s olving ρ n h n p roblem zhang and ho mimo broadcasting for simultaneous wireless information and power transfer p the maximum achievable rate is given by via applying the mrc beamforming at id receiver r log m gastpar on capacity under receive and spatial spectrum sharing constraints ieee trans inf theory vol no pp feb h r zhang and y c liang exploiting multi antennas for opportunistic spectrum sharing in cognitive radio networks ieee j sel topics signal process vol no pp feb log r zhang y c liang and s cui dynamic resource allocation in cognitive radio networks ieee signal process mag vol no pp may s boyd and l vandenberghe convex optimization cambridge uni versity press j proakis digital communication mcgraw hill science press s boyd convex optimization ii stanford university available http www stanford edu class lectures html t le k mayaram and t fiez efficient far field radio frequency energy harvesting for passively powered sensor networks ieee j solid state circuits vol no pp may v r cadambe and s a jafar interference alignment and the degrees of freedom for the k user interference channel ieee trans inf theory vol no pp aug r zhang and c k ho mimo broadcasting for simultaneous wireless information and power transfer available arxiv log n i ρ i h i log n i h i n ρ i h i h q i we thus q have q cps r e h p r q r log h furthermore since the above proof is valid for any value of q pρ that cups r e p ρ r i n i and q h i changing r from ρ from to yields the to h log it thus follows h q q proposition h which is thus is completed the same as cps r e p the proof of what will be what it will not be is an incre mental advance on the previous four generations of cellu lar technology have each been a major paradigm shift that has broken backward compatibility indeed will need to be a paradigm shift that includes very high carrier frequencies with massive bandwidths extreme base station and device densities and unprecedented numbers of antennas however unlike the previous four generations it will also be highly integrative tying any new air interface and spectrum together with lte and wifi to provide universal high rate coverage and a seamless user experience to support this the core network will also have to reach unprecedented levels of flexibility and intelligence spectrum regulation will need to be rethought and improved and energy and cost efficiencies will become even more critical considerations this paper discusses all of these topics identifying key challenges for future research and preliminary standardization activities while providing a comprehensive overview of the current litera ture and in particular of the papers appearing in this special issue index terms cellular systems energy efficiency hetnets massive mimo millimeter wave small cells i i ntroduction a the road to i n about just a the possible past year preliminary standard interest and discussions have evolved into a full fledged conversation that has captured the attention and imagi nation of researchers and engineers around the world as the long term evolution lte system embodying has now been deployed and is reaching maturity where only incre mental improvements and small amounts of new spectrum can be expected it is natural for researchers to ponder what next however this is not a mere intellectual exercise thanks largely to the annual visual network index vni reports manuscript received december revised april accepted may date of publication june date of current version july j g andrews is with the university of texas at austin austin tx usa e mail jandrews ece utexas edu s buzzi is with the university of cassino and southern latium cassino fr italy and also with consorzio nazionale interuniversitario per le telecomunicazioni cassino frosione italy e mail buzzi unicas it w choi is with the department of electrical engineering korea advanced institute of science daejeon korea e mail wchoi kaist edu s v hanly is with macquarie university sydney n s w australia e mail stephen hanly mq edu au a lozano is with the universitat pompeu fabra barcelona spain e mail angel lozano upf edu a c k soong is with huawei technologies plano tx usa e mail anthony soong huawei com j c zhang is with samsung electronics richardson tx usa e mail jianzhong z samsung com color versions of one or more of the figures in this paper are available online at http ieeexplore ieee org digital object identifier jsac released by cisco we have quantitative evidence that the wire less data explosion is real and will continue driven largely by smartphones tablets and video streaming the most recent feb vni report and forecast makes plain that an incremental approach will not come close to meeting the demands that networks will face by in just a decade the amount of ip data handled by wireless networks will have increased by well over a factor of from under exabytes in to over exabytes by on pace to exceed exabytes by this deluge of data has been driven chiefly by video thus far but new unforeseen applications can reasonably be expected to materialize by in addition to the sheer volume of data the number of devices and the data rates will continue to grow exponentially the number of devices could reach the tens or even hundreds of billions by the time comes to fruition due to many new applications beyond personal communications it is our duty as engineers to meet these intense demands via innovative new technologies that are smart and efficient yet grounded in reality academia is engaging in collaborative projects such as metis and while industry is driving preliminary standardization activities cf section iv b to further strengthen these activities the public private partnership for infrastructure recently constituted in europe will funnel massive amounts of funds into related research this article is an attempt to summarize and overview many of these exciting developments including the papers in this special issue in addition to the highly visible demand for ever more network capacity there are a number of other factors that make interesting including the potentially disruptive move to millimeter wave mmwave spectrum new market driven ways of allocating and re allocating bandwidth a major ongoing virtualization in the core network that might pro gressively spread to the edges the possibility of an internet of things comprised of billions of miscellaneous devices and the increasing integration of past and current cellular and wifi standards to provide a ubiquitous high rate low latency experience for network users this editorial commences with our view of the big three technologies ultra densification mmwave and massive multiple input multiple output mimo then we consider im portant issues concerning the basic transmission waveform the increasing virtualization of the network infrastructure and the need for greatly increased energy efficiency finally we provide a comprehensive discussion of the equally important regulatory and standardization issues that will need to be addressed for with a particular focus on needed innovation in spectrum regulation ieee personal use is permitted but republication redistribution requires ieee permission see http www ieee org publications rights index html for more information ieee journal on selected areas in communications vol no june b engineering requirements for in order to more concretely understand the engineering chal lenges facing and to plan to meet them it is necessary to first identify the requirements for a system the following items are requirements in each key dimension but it should be stressed that not all of these need to be satisfied simultane ously different applications will place different requirements on the performance and peak requirements that will need to be satisfied in certain configurations are mentioned below for example very high rate applications such as streaming high definition video may have relaxed latency and reliability requirements compared to driverless cars or public safety appli cations where latency and reliability are paramount but lower data rates can be tolerated data rate the need to support the mobile data traffic explosion is unquestionably the main driver behind data rate can be measured in several different ways and there will be a goal target for each such metric a aggregate data rate or area capacity refers to the total amount of data the network can serve characterized in bits per unit area the general consensus is that this quantity will need to increase by roughly from to b edge rate or rate is the worst data rate that a user can reasonably expect to receive when in range of the network and so is an important metric and has a concrete engineering meaning goals for the edge rate range from mbps easily enough to support high definition streaming to as much as gbps meeting mbps for of users will be extraordinarily challenging even with major technological advances this requires about a advance since current systems have a typical rate of about mbps although the precise number varies quite widely depending on the load the cell size and other factors c peak rate is the best case data rate that a user can hope to achieve under any conceivable network configuration the peak rate is a marketing number devoid of much meaning to engineers and likely to be in the range of tens of gbps meeting the requirements in a b which are about and current technology respectively are the main focus of this paper latency current roundtrip latencies are on the order of about ms and are based on the ms subframe time with necessary overheads for resource allocation and access although this latency is sufficient for most current services anticipated applications include two way gaming novel cloud based technologies such as those that may be touch screen activated the tactile internet and virtual and enhanced reality e g google glass or other wearable comput ing devices as a result will need to be able to support a roundtrip latency of about ms an order of magnitude faster than in addition to shrinking down the subframe structure such severe latency constraints may have important implications on design choices at several layers of the protocol stack and the core network cf section iii energy and cost as we move to costs and energy consumption will ideally decrease but at least they should not increase on a per link basis since the per link data rates being offered will be increasing by about this means that the joules per bit and cost per bit will need to fall by at least in this article we do not address energy and cost in a quantitative fashion but we are intentionally advocating technological solutions that promise reasonable cost and power scaling for example mmwave spectrum should be cheaper per hz than the and spectrum below ghz similarly small cells should be cheaper and more power efficient than macrocells a major cost consideration for even more so than in due to the new bs densities and increased bandwidth is the backhaul from the network edges into the core we address backhaul and other economic considerations in section iv c as for energy efficiency we address this more substantially in section iii c c device types and quantities will need to be able to efficiently support a much larger and more diverse set of devices with the expected rise of machine to machine communication a single macrocell may need to support or more low rate devices along with its traditional high rate mobile users this will require wholesale changes to the control plane and network management relative to whose overhead channels and state machines are not designed for such a diverse and large subscriber base ii key technologies to get to data rate of the requirements outlined in section i b certainly the one that gets the most attention is the need for radically higher data rates across the board our view is that the required will for the most part be achieved through combined gains in three categories a extreme densification and offloading to improve the area spectral efficiency put differently more active nodes per unit area and hz b increased bandwidth primarily by moving toward and into mmwave spectrum but also by making better use of wifi unlicensed spectrum in the ghz band alto gether more hz c increased spectral efficiency primarily through advances in mimo to support more bits hz per node the combination of more nodes per unit area and hz more hz and more bits hz per node will compound into many more bits per unit area other ideas not in the above cate gories e g interference management through bs cooperation may also contribute improvements but the lion share of the surge in capacity should come from ideas in the above categories in the remainder of this section these are distilled in some detail a extreme densification and offloading a straightforward but extremely effective way to increase the network capacity is to make the cells smaller this approach has andrews et al what will be been demonstrated over several cellular generations the first such generation in the early had cell sizes on the order of hundreds of square kms since then those sizes have been progressively shrinking and by now they are often fractions of a square km in urban areas in japan for instance the spacing between bss can be as small as two hundred meters giving a coverage area well under a tenth of a square km networks are now rapidly evolving to include nested small cells such as picocells range under meters and femtocells wifi like range as well as distributed antenna systems that are functionally similar to picocells from a capacity and coverage standpoint but have all their baseband processing at a central site and share cell ids cell shrinking has numerous benefits the most important being the reuse of spectrum across a geographic area and the ensuing reduction in the number of users competing for resources at each bs contrary to widespread belief as long as power law pathloss models hold the signal to interference ratio sir is preserved as the network densifies thus in principle cells can shrink almost indefinitely without a sacrifice in sir until nearly every bs serves a single user or is idle this allows each bs to devote its resources as well as its backhaul connection to an ever smaller number of users as the densification becomes extreme some challenges arise preserving the expected cell splitting gains as each bs be comes more lightly loaded particularly low power nodes determining appropriate associations between users and bss across multiple radio access technologies rats which is crucial for optimizing the edge rate supporting mobility through such a highly heterogeneous network affording the rising costs of installation maintenance and backhaul we next briefly discuss these challenges particularly in view of the other technologies raised in this article base station densification gains we define the bs den sification gain ρ λ like csma that are inefficient in high density it is possible that ρ which is colloquially referred to as the tragedy of the commons but for cellular network with a centralized mac we can safely assume ρ in an interference limited network with full buffers the signal to interference plus noise ratio sinr is essentially equal to the sir and because the sir distribution remains approximately constant as the network densifies the best case scenario is ρ in reality buffers are not always full and small cells tend to become more lightly loaded than macrocells as the network densifies altogether the sinr usually increases with density in noise limited networks because of the increase in received signal power and in interference limited networks because the lightly loaded small cells generate less interference while still providing an option for connectivity nev ertheless at microwave frequencies the gain in sinr is not enough to keep up with the decrease in small cell utilization and thus ρ in an extreme case consider λ and r held fixed with λ in this asymptotic setting the small cells compete for a finite pool of users becoming ever more lightly loaded and thus ρ empirically and theoretically we observe that ρ improves and can approach with macro bs muting termed eicic in vs the macrocells transmitting all the time and thus interfering with the small cells all the time an intriguing aspect of mmwave frequencies is that densi fication gains ρ may be possible this is because as dis cussed in section ii b at these frequencies communication is largely noise limited and increasing the density not only splits the cell resources and lightens the load but it may increase the sinr dramatically as a striking example of this it was recently shown in that under a plausible urban grid based deployment increasing the bs count in a given area from to which decreased the inter bs distance from meters down to meters increased the cell edge rate from mbps up to mbps giving ρ while conceding that this massive densification gain corresponds to a particular setup and model it is nevertheless remarkable in general quantifying and optimizing the densification gains λ as the effective increase in data in a wide variety of deployment scenarios and network models rate relative to the increase in network density which is a proxy is a key area for continued small cell research here for cost specifically if we achieve a data rate r could multi rat association networks will continue to be be any measure thereof e g edge rate or aggregate when the bs density is λ bss and then we consider a higher bs come increasingly heterogeneous as we move toward a key feature therein will be increased integration between different density λ with corresponding rate r then the densification rats with a typical enabled device having radios capable gain is the slope of the rate increase over that destiny range of supporting not only a potentially new standard e g at ρ λ λ r λ mmwave frequencies but also numerous releases of lte including possibly lte unlicensed several types of wifi and perhaps direct device to device communica tion all across a great many spectral bands hence determining which standard and spectrum to utilize and which bs or users to associate with will be a truly complex task for the network see fig determining the optimal user association is for general utility functions a massive combinatorial optimization problem that depends on the sinr from every user to every bs the instantaneous load at each bs the choices of other users in the network and possibly other constraints such as the requirement r r for example if the network density is doubled and the edge data rate increases by for example since some of the added bss were lightly loaded then the densification gain is ρ in some applications with channel access protocols power law pathloss model ceases to apply in the near field very close to the transmitter λ λ ieee journal on selected areas in communications vol no june fig user association in a multi rat network over many frequency bands is complex in this simplified scenario a mobile user in turn associates with different bss based on a tradeoff between the gain to that bs and the traffic load congestion that it is experiencing to utilize the same bs and standard in both uplink and downlink to facilitate functioning control channels for resource alloca tion and feedback therefore simplified procedures must be adopted an example of which appears in this special issue the key such simplified procedures are biasing and macrocell blanking or muting biasing refers to associating with a small cell even if it provides a lower sinr than the macrocell and is useful for pushing users off of the heavily loaded macrocell and onto the lightly loaded small cell everyone wins the remaining macrocell users get more resources while the biased users have a lower sinr spectral efficiency but can utilize a large number of resource blocks on the small cell ultimately attaining a higher data rate blanking refers to shutting off the macrocell transmissions for some fraction of the time preferably while the biased small cell users are being served this raises all the small cell sinrs considerably enough to justify actually shutting down even congested macrocell bss while also providing a mechanism for the biased users to hear common control channels that would otherwise be swamped by the macrocells even a simple seemingly highly suboptimal association ap proach based on aggressive but static biasing about db depending on various factors toward small cells and blanking about half of the macrocell transmissions has been shown to increase edge rates by as much as the joint problem of user association and resource allocation in two tier heterogeneous networks hetnets with adaptive tuning of the biasing and blanking in each cell is considered in and an interesting model of hotspot traffic is considered in where it is shown that under various network utility metrics the optimal cell association is determined by rate ratio bias rather than power or sinr bias it will be interesting to extend these models to more gen eral scenarios a dynamic model of cell range expansion is considered in where traffic arrives as a poisson process in time and the feasible arrival rates for which a stabilizing scheduling policy exists are characterized user association and load balancing in a hetnet with massive mimo at the bss is considered in the problem of determining the optimal associations when there are multiple rats operating at different frequencies and using different protocols has not yet received much attention however an interesting game the fig calculated mmwave bs associations with real building locations the shaded regions correspond to association with the bs centered at that shade blocking los vs non los propagation and beam directionality render our usual notion of cell boundaries obsolete oretic approach is taken in to the rat selection problem where convergence to nash equilibria and the pareto efficiency of these equilibria are studied a related paper in this special issue explores the interaction between cellular operators and wifi network owners adding mmwave into the picture adds significant additional complexity since even the notion of a cell boundary is blurry at mmwave frequencies given the strong impact of blockages which often result in nearby bss being bypassed in favor of far ther ones that are unblocked cf fig on the positive side in terference is much less important in mmwave cf section ii b and thus the need for blanking is reduced in summary there is a great deal of scope for modeling analyzing and optimizing bs user associations in mobility support clearly the continued network densi fication and increased heterogeneity poses challenges for the support of mobility although a hefty share of data is served to stationary indoor users the support of mobility and always on connectivity is arguably the single most important feature of cellular networks relative to wifi because modeling and analyzing the effect of mobility on network performance is difficult we expect to see somewhat ad hoc solutions such as in lte rel where user specific virtual cells are defined to distinguish the physical cell from a broader area where the user andrews et al what will be can roam without the need for handoff communicating with any bs or subset of bss in that area or in mmwave restricting highly mobile users to macrocells and microwave frequen cies thereby forcing them to tolerate lower rates handoffs will be particularly challenging at mmwave frequencies since transmit and receive beams must be aligned to communicate indeed the entire paradigm of a handoff initiated and managed at layer by the core network will likely not exist in instead handoffs may be opportunistic based on mmwave beam alignments or indistinguishable from phy mac inter ference management techniques whereby users communicate with multiple coordinated bss as exemplified by in this special issue cost evolving to ever smaller cells requires ever smaller lower power and cheaper bss and there is no funda mental reason a bs needs to be more expensive than a user device or a wifi node nevertheless obtaining permits ensuring fast and reliable backhaul connections and paying large monthly site rental fees for operator controlled small cell placements have proven a major hindrance to the growth of picocell distributed antennas and other enterprise quality small cell deployments of these only the backhaul is pri marily a technical challenge regulatory reforms and infras tructure sharing cf section iv c may help address the other challenges turning to end user deployed femtocells and wifi access points these are certainly much more cost effective both from a capital and operating expense perspective however major concerns exist here too these include the coordination and management of the network to provide enterprise grade service which given the scale of the deployments requires automated self organization a further challenge is that these end user deployments utilize the end user backhaul connection and access point both of which the end user has a vested interest in not sharing and in some countries a legal requirement not to anecdotally all readers of this article are familiar with the scenario where a dozen wifi access points are within range but all are secured and inaccessible from an engineering perspective this closed access status quo is highly inefficient and the cost for would be greatly reduced in an open access paradigm for small cells one preliminary but successful example is fon which as of press time boasts over million shared wifi access points and all networks beyond it will be extremely dense and heterogeneous which introduces many new challenges for network modeling analysis design and optimization we further discuss some of the nonobvious intersections of extreme densification with mmwave and massive mimo respectively in the next two sections before proceeding however we briefly mention that besides cell shrinking a second approach to densi fication exists in the form of communication this allows users in close proximity to establish direct communication replacing two relatively long radio hops via the bs with a single and shorter direct hop provided there is sufficient spatial locality in the wireless traffic this can bring about reduced power consumption and or higher data rates and a diminished latency reference in this special issue pro poses a novel way of scheduling concurrent transmis sions so as to densify while offering interference protection guarantees b millimeter wave terrestrial wireless systems have largely restricted their op eration to the relatively slim range of microwave frequencies that extends from several hundred mhz to a few ghz and cor responds to wavelengths in the range of a few centimeters up to about a meter by now though this spectral band often called beachfront spectrum has become nearly fully occupied in particular at peak times and in peak markets regardless of the efficacy of densification and offloading much more bandwidth is needed although beachfront bandwidth allocations can be made significantly more efficient by modernizing regulatory and al location procedures as discussed in section iv a to put large amounts of new bandwidth into play there is only one way to go up in frequency fortunately vast amounts of relatively idle spectrum do exist in the mmwave range of ghz where wavelengths are mm there are also several ghz of plausible spectrum in the ghz range the main reason that mmwave spectrum lies idle is that until recently it had been deemed unsuitable for mobile com munications because of rather hostile propagation qualities including strong pathloss atmospheric and rain absorption low diffraction around obstacles and penetration through objects and further because of strong phase noise and exorbitant equipment costs the dominant perception had therefore been that such frequencies and in particular the large unlicensed band around ghz were suitable mainly for very short range transmission thus the focus had been on wifi with the wigig standard in the ghz band and on fixed wireless in the and ghz however semi conductors are maturing their costs and power consumption rapidly falling largely thanks to the progress of the aforemen tioned short range standard and the other obstacles related to propagation are now considered increasingly surmountable given time and focused effort propagation issues concerning mmwave propagation for the main issues under investigation are pathloss if the electrical size of the antennas i e their size measured by the wavelength λ c f c where f c is the carrier frequency is kept constant as the frequency increases the antennas shrink and their effective aperture scales with then the free space pathloss between a transmit and a receive order of antenna grows magnitude say with from f c to thus increasing f c by an ghz adds db of power loss regardless of the transmit receive distance how ever if the antenna aperture at one end of the link is kept constant as the frequency increases then the free space pathloss remains unchanged further if both the transmit and receive antenna apertures are held constant then the free space pathloss actually counter diminishes the higher noise with floor c a power associated gain that would help with broader signal bandwidths although preserving the electrical size of the antennas is desirable for a number of reasons maintaining at the same ieee journal on selected areas in communications vol no june time the aperture is possible utilizing arrays which aggregate the individual antenna apertures as the antennas shrink with frequency progressively more of them must be added within the original area the main challenge becomes cophasing these antennas so that they steer and or collect energy productively this challenge becomes more pronounced when the channel changes rapidly for instance due to mobility whose effect in terms of doppler shift increases linearly with frequency or due to rapid alterations in the physical orientation of the devices blocking mmwave signals exhibit reduced diffraction and a more specular propagation than their microwave counterparts and hence they are much more susceptible to blockages this results in a nearly bimodal channel depending on the presence or absence of line of sight los according to recent mea surements as the transmit receive distance grows the pathloss accrues close to the free space value of db decade under los propagation but drops to db decade plus an additional blocking loss of db otherwise because of the sensitivity to blockages a given link can rapidly transition from usable to unusable and unlike small scale fading large scale obstructions cannot be circumvented with standard small scale diversity countermeasures new channel models captur ing these effects are much needed and in fact currently being developed and applied to system level analysis and simulation studies such as and in this special issue atmospheric and rain absorption the absorption due to air and rain is noticeable especially the db km oxygen absorption within the ghz band which is in fact why this band is unlicensed but it is inconsequential for the urban cellular deployments currently envisioned where bs spacings might be on the order of m in fact such absorption is beneficial since it further attenuates interference from more distant bss effectively increasing the isolation of each cell the main conclusion is that the propagation losses for mmwave frequencies are surmountable but require large an tenna arrays to steer the beam energy and collect it coher ently while physically feasible the notion of narrow beam communication is new to cellular communications and poses difficulties which we next discuss large arrays narrow beams building a wireless system out of narrow and focused beams is highly nontrivial and changes many traditional aspects of wireless system design mmwave beams are highly directional almost like flashlights which completely changes the interference behavior as well as the sensitivity to misaligned beams the interference adopts an on off behavior where most beams do not interfere but strong interference does occur intermittently overall interference is de emphasized and mmwave links may often be noise limited which is a major reversal from indeed even the notion of a cell is likely to be very different in a mmwave system since rather than distance blocking is often the first order effect on the received signal power this is illustrated in fig link acquisition a key challenge for narrow beams is the difficulty in establishing associations between users and bss both for initial access and for handoff to find each other a user and a bs may need to scan lots of angular positions fig mmwave enabled network with phantom cells where a narrow beam could possibly be found or deploy extremely large coding spreading gains over a wider beam that is successively narrowed in a multistage acquisition procedure developing solutions to this problem particularly in the context of high mobility is an important research challenge leveraging the legacy network a concurrent utiliza tion of microwave and mmwave frequencies could go a long way toward overcoming some of the above hurdles an inter esting proposal in that respect is the notion of phantom cells relabeled soft cells within where mmwave frequencies would be employed for payload data transmission from small cell bss while the control plane would operate at microwave frequencies from macro bss cf fig this would ensure stable and reliable control connections based on which blazing fast data transmissions could be arranged over short range mmwave links sporadic interruptions of these mmwave links would then be far less consequential as control links would remain in place and lost data could be recovered through retransmissions novel transceiver architectures needed despite the progress made in wifi mmwave systems nontrivial hardware issues remain and in some cases will directly affect how the communication aspects are designed chief among these is the still exorbitant power consumption of particularly the analog to digital a d but also the digital to analog d a converters operating on enormous bandwidths a main consequence is that although large antenna arrays and high receiver sensitivities are needed to deal with the pathloss having customary fully digital beamformers for each antenna appears to be unfeasible more likely are structures based on old fashioned analog phase shifters or perhaps hybrid structures where groups of antennas share a single a d and d a on the flip side offering some relief from these difficulties the channels are sparser and thus the acquisition of channel state information is facilitated in particular channel estimation and beamforming techniques exploiting sparsity in the framework of compressed sensing are being explored c massive mimo stemming from research that blossomed in the late mimo communication was introduced into wifi systems around and into cellular shortly thereafter in essence mimo embodies the spatial dimension of the commu nication that arises once a multiplicity of antennas are available at bss and mobile devices if the entries of the channel matrix that ensues exhibit by virtue of spacing cross polarization andrews et al what will be and or angular disposition sufficient statistical independence multiple spatial dimensions become available for signaling and the spectral efficiency multiplies accordingly in single user mimo su mimo the dimensions are lim ited by the number of antennas that can be accommodated on a mobile device however by having each bs communicate with several users concurrently the multiuser version of mimo mu mimo can effectively pull together the antennas at those users and overcome this bottleneck then the signaling dimen sions are given by the smallest between the aggregate number of antennas at those users and the number of antennas at the bs furthermore in what is now known as coordinated multi point comp transmission reception multiple bss can coop erate and act as a single effective mimo transceiver thereby turning some of the interference in the system into useful signals this concept in fact underpins many of the approaches to interference and mobility management mentioned earlier in this section well established by the time lte was developed mimo was a native ingredient thereof with two to four antennas per mobile device and as many as eight per bs sector and it appeared that because of form factors and other apparent limitations such was the extent to which mimo could be leveraged marzetta was instrumental in articulating a vision in which the number of antennas increased by more than an order of magnitude first in a presentation with the details formalized in a landmark paper the proposal was to equip bss with a number of antennas much larger than the number of active users per time frequency signaling resource and given that under reasonable time frequency selectivities accurate channel estimation can be conducted for at most some tens of users per resource this condition puts the number of antennas per bs into the hundreds this bold idea initially termed large scale antenna systems but now more popularly known as massive mimo offers enticing benefits enormous enhancements in spectral efficiency with out the need for increased bs densification with the possibility as is always the case of trading some of those enhancements off for power efficiency improve ments smoothed out channel responses because of the vast spa tial diversity which brings about the favorable action of the law of large numbers in essence all small scale randomness abates as the number of channel observations grows simple transmit receive structures because of the quasi orthogonal nature of the channels between each bs and the set of active users sharing the same signaling resource for a given number of active users such orthogonality sharpens as the number of bs antennas grows and simple linear transceivers even plain single user beamforming perform close to optimally the promise of these benefits has elevated massive mimo to a central position in preliminary discussions with a foreseen role of providing a high capacity umbrella of ubiq uitous coverage in support of underlying tiers of small cells however for massive mimo to become a reality several challenges must first be overcome and the remainder of this section is devoted to their dissection for recent contributions on these and other aspects the reader is referred to a companion special issue on massive mimo the present special issue contains further new contributions mentioned throughout the discussion that follows plus reference dealing with the massification of mimo multicasting pilot contamination and overhead reduction pilot transmissions can be made orthogonal among same cell users to facilitate cleaner channel estimates but must be reused across cells for otherwise all available resources would end up consumed by pilots this inevitably causes interference among pilots in different cells and hence puts a floor on the quality of the channel estimates this interference so called pilot contamination does not vanish as the number of bs antennas grows large and so is the one impairment that remains asymptotically however pilot contamination is a relatively secondary factor for all but colossal numbers of antennas furthermore various methods to reduce and even eliminate pilot contamination via low intensity bs coordination have already been formulated still a careful design of the pilot structures is required to avoid an explosion in overhead the ideas being considered to reign in pilot over heads include exploiting spatial correlations so as to share pilot symbols among antennas and also segregating the pilots into classes e g channel strength gauging for link adaptation v data detection such that each class can be transmitted at the necessary rate and no faster architectural challenges a more serious challenge to the realization of the massive mimo vision has to do with its ar chitecture the vision requires radically different bs structures where in lieu of a few high power amplifiers feeding a handful of sector antennas we would have a myriad of tiny antennas fed by correspondingly low power amplifiers most likely each antenna would have to be integrated with its own amplifier scalability antenna correlations and mutual couplings and cost are some of the issues that must be sorted out at the same time opportunities arise for innovative topologies such as conformal arrays along rooftops or on building facades and we next dwell on a specific topological aspect in which innovation is taking place within this special issue explores alternative and highly innovative antenna designs based on the utilization of an electromagnetic lens focusing antenna full dimension mimo and elevation beamforming ex isting bss mostly feature linear horizontal arrays which in tower structures can only accommodate a limited number of antennas due to form factors and which only exploit the azimuth angle dimension by adopting planar arrays as in fig and further exploiting the elevation angle the so called full dimension mimo fd mimo can house many more an tennas with the same form factor as a side benefit tailored vertical beams increase the signal power and reduce interference to users in neighboring cells some preliminary cell average and edge data rates obtained from samsung network simulator are listed in table i where with numbers of antennas still modest for what massive mimo is envisioned to be multiple fold improvements are already observed ieee journal on selected areas in communications vol no june table i fd mimo system level downlink simulation results at gh z h alf w avelength a ntenna s pacings in b oth the horizontal and vertical dimensions at the bss per u ser o verhead t he b aseline i s su mimo w ith a ntennas per bs and the fd mimo r esults a verage and edge data rates are for mu mimo with and antennas r espectively c orresponding to and p lanar a rrays per bs s ector channel models parallel to the architectural issues run those related to channel models which to be sound require ex tensive field measurements antenna correlations and couplings for massive arrays with relevant topologies must be determined and a proper modeling of their impact must be established in particular the degree of actual channel orthogonalization in the face of such nonidealities must be verified and for fd mimo besides azimuth the modeling needs to incorporate elevation which is a dimension on which far less data exists concerning power spectra and angle spreads a channel modeling study currently under way within is expected to shed light on these various issues references and in this special issue also deal with this subject coexistence with small cells as mentioned earlier massive mimo bss would most likely have to coexist with tiers of small cells which would not be equipped with massive mimo due to their smaller form factor although the simplest alternative is to segregate the corresponding transmissions in frequency the large number of excess antennas at massive mimo bss may offer the opportunity of spatial nulling and in terference avoidance with relative simplicity and little penalty to confirm the feasibility of this idea put forth in and fur ther developed in within this special issue comprehensive channel models are again needed as networks become dense and more traffic is offloaded to small cells the number of active users per cell will diminish and the need for massive mimo may decrease aspects such as cost and backhaul will ultimately determine the balance between these complementary ideas coexistence with mmwave as discussed in section ii b mmwave communication requires many antennas for beam steering the antennas are much smaller at these frequencies and thus very large numbers thereof can conceivably fit into portable devices and these antennas can indeed provide beam forming power gain but also mimo opportunities as considered in within this special issue any application of massive mimo at mmwave frequencies would have to find the cor rect balance between power gain interference reduction and parallelization iii d esign i ssues for in addition to supporting higher data rates net works must decrease latencies lower energy consumption lower costs and support many low rate connections in this sec tion we discuss important ongoing research areas that support these requirements we begin with the most fundamental aspect of the physical layer the waveform and then consider the evolution of cloud based and virtualized network architectures latency and control signaling and energy efficiency a the waveform signaling and multiple access the signaling and multiple access formats i e the waveform design have changed significantly at each cellular generation and to a large extent they have been each generation defining technical feature they have also often been the subject of fierce intellectual and industrial disputes which have played out in the wider media the approach based on analog frequency modulation with fdma transformed into a digital format for and although it employed both fdma and tdma for multiple access was generally known as tdma due to the novelty of time multiplexing meanwhile a niche spread spectrum cdma standard that was developed by qual comm to compete for became the dominant approach to all global standards once the limitations of cdma for high speed data became inescapable there was a discreet but unmistakable retreat back toward tdma with minimal spectrum spreading retained and with the important addition of channel aware scheduling due to the increasing sig nal bandwidths needed to support data applications orthogo nal frequency division multiplexing ofdm was unanimously adopted for in conjunction with scheduled fdma tdma as the virtues of orthogonality were viewed with renewed appreciation in light of this history it is natural to ponder the possibility that the transition to could involve yet another major change in the signaling and multiple access formats ofdm and ofdma the default approach ofdm has become the dominant signaling format for high speed wireless communication forming the basis of all current wifi standards and of lte and further of wireline technologies such as digital subscriber lines digital tv and commercial radio its qualities include a natural way to cope with frequency selectivity computationally efficient implementation via fft ifft blocks and simple frequency domain equalizers an excellent pairing for mimo since ofdm allows for the spatial interference from multiantenna transmission to be dealt with at a subcarrier level without the added complication of intersymbol interference from a multiple access vantage point ofdm invites dy namic fine grained resource allocation schemes in the digital domain and the term ofdma is employed to denote orthog onal multiple access at a subcarrier level in combination with tdma this parcels the time frequency grid into small units known as resource blocks that can be easily discriminated through digital filtering being able to do frequency and time slot allocation digitally also enables more adaptive and sophisticated interference management techniques such as fractional frequency reuse or spectrum partitions between small cells and macrocells finally given its near universal adoption industry has by now a great deal of experience with its implementation and tricky aspects of ofdm such as frequency offset correction and synchronization have been essentially conquered andrews et al what will be drawbacks of ofdm given this impressive list of qual ities and the large amount of inertia in its favor ofdm is the unquestionable frontrunner for however some weak points do exist that could possibly become more pronounced in networks first the peak to average power ratio papr is higher in ofdm than in other formats since the envelope samples are nearly gaussian due to the summation of uncorrelated inputs in the ifft although a gaussian signal distribution is capacity achieving under an average power constraint in the face of an actual power amplifier a high papr sets up an unattractive tradeoff between the linearity of the transmitted signal and the cost of the amplifier this problem can be largely overcome by precoding the ofdm signals at the cost of a slightly more involved equalization process at the receiver and a slight power penalty indeed this is already being done in the lte uplink second ofdm spectral efficiency is satisfactory but could perhaps be further improved upon if the requirements of strict orthogonality were relaxed and if the cyclic prefixes cps that prevent interblock interference were smaller or discarded paper in this special issue proposes a novel ofdma based modulation scheme named frequency and quadrature amplitude modulation fqam which is shown to improve the downlink cell edge rate perhaps the main source of concerns or at least of open questions is the applicability of ofdm to mmwave spectrum given the enormous bandwidths therein and the difficulty of developing efficient power amplifiers at those frequencies for example a paper in this special issue proposes a single carrier signaling with null cyclic prefix as an alternative to ofdm at mmwave frequencies potential alternatives to ofdm to address ofdm weaknesses we now overview some alternative approaches being actively investigated most of these however can be considered incremental departures from ofdm rather than the step function changes that took place in previous cellular generations further tutorial treatment can be found in a recent tutorial time frequency packing time frequency packing and faster than nyquist signaling have been re cently proposed to circumvent the limitations of strict orthog onality and cp in contrast to ofdm where the product of the symbol interval and the subcarrier spacing equals in faster than nyquist signaling products smaller than can be accommodated and spectral efficiency improvements on the order of have been claimed nonorthogonal signals there is a growing interest in multicarrier formats such as filterbank multicarrier that are natively nonorthogonal and thus do not require prior syn chronization of distributed transmitters a new format termed universal filtered multicarrier ufmc has been proposed whereby starting with an ofdm signal filtering is performed on groups of adjacent subcarriers with the aim of reducing sidelobe levels and intercarrier interference resulting from poor time frequency synchronization see fig filterbank multicarrier to address the drawbacks of rectangular time windowing in ofdm namely the need for fig frequency domain magnitude responses of some adjacent waveforms for ofdm frequency packed ofdm and filtered ofdm the two signaling formats alternative to ofdm trade subcarrier orthogonality for either better spectral efficiency frequency packed ofdm or lower out of band emissions filtered ofdm large guard bands shows that the use of filterbank mul ticarrier permits a robust estimation of very large propagation delays and of arbitrarily high carrier frequency offsets whereas ofdm would have required a very long cp to attain the same performance levels generalized frequency division multiplexing gfdm is a multicarrier technique that adopts a shortened cp through the tail biting technique and is particularly well suited for noncontiguous frequency bands which makes it attractive for spectrum sharing where frequency domain holes may have to be adaptively filled single carrier single carrier transmission has also been attracting renewed interest chiefly due to the development of low complexity nonlinear equalizers implemented in the frequency domain tunable ofdm we conclude with our own opinion that ofdm could be well adapted to different requirements by allowing some of its parameters to be tunable rather than designed for essentially the worst case multipath delay spread in particular given the increasingly software defined nature of radios the fft block size the subcarrier spacing and the cp length could change with the channel conditions in scenarios with small delay spreads notably dense urban small cells and ieee journal on selected areas in communications vol no june mmwave channels the subcarrier spacing could grow and the fft size and the cp could be significantly shortened to lower the latency the papr the cp power and bandwidth penalty and the computational complexity in channels with longer delay spreads that could revert to narrower subcar riers longer fft blocks and a longer cp this is concep tually similar to the null cyclic prefix single carrier scheme proposed by nsn in this special issue which is essentially ofdm with dft precoding to reduce papr and a punctured variable length null prefix that is fixed with oversampling at the receiver b cloud based networking although this special issue is mainly focused on the air interface for the sake of completeness we briefly touch on the exciting changes taking place at the network level in that respect the most relevant event is the movement of data to the cloud so that it can be accessed from anywhere and via a variety of platforms this fundamentally redefines the endpoints and the time frame for which network services are provisioned it requires that the network be much more nimble flexible and scalable as such two technology trends will become paramount in the future network function virtualization nfv and software defined networking sdn together these trends represent the biggest advance in mobile communication net working in the last years bound to fundamentally change the way network services are provided although the move toward virtualization is thus far taking place only within the core network this trend might eventually expand toward the edges in fact the term cloud ran is already being utilized but for now largely to refer to schemes whereby multiple bss are allowed to cooperate if and when the bss themselves become virtualized down to the mac and phy this term will be thoroughly justified network function virtualization nfv enables network functions that were traditionally tied to hardware appliances to run on cloud computing infrastructure in a data center it should be noted that this does not imply that the nfv infrastructure will be equivalent to commercial cloud or enterprise cloud what is expected is that there will be a high degree of reuse of what commercial cloud offers it is natural to expect that some requirements of mobile networks such as the separation of the data plane control plane and management plane will not be feasible within the commercial cloud nevertheless the separation of the net work functions from the hardware infrastructure will be the cornerstone of future architectures the key benefit will be the ability to elastically support network functional demands furthermore this new architecture will allow for significant nimbleness through the creation of virtual networks and of new types of network services a detailed description of the nfv architecture is beyond the scope of this paper and interested readers can consult and the references therein as virtualization of the communication network gains trac tion in the industry an old concept dating back to the will emerge the provision of user controlled management in network elements advances in computing technology have reached a level where this vision can become a reality with the ensuring architecture having recently been termed software defined networking sdn software defined networking sdn is an architectural framework for creating intelligent programmable networks specifically it is defined as an architecture where the control and data planes are decoupled network intelligence and state are logically centralized and the underlying network infrastruc ture is abstracted from the application the key ingredients of sdn are an open interface be tween the entities in the control and data planes as well as programmability of the network entities by external applica tions the main benefits of this architecture are the logical decoupling of the network intelligence to separate software based controllers exposing the network capabilities through an application program interface and enabling the appli cation to request and manipulate services provided by the network from a wireless core network point of view nfv and sdn should be viewed as tools for provisioning the next generation of core networks with many issues still open in terms of scalability migration from current structures management and automation and security c energy efficiency as specified in our stated re quirements for the energy efficiency of the commu nication chain typically measured in either joules bit or bits joule will need to improve by about the same amount as the data rate just to maintain the power consumption and by more if such consumption is to be reduced this implies a several order of magnitude increase in energy efficiency which is extremely challenging unsurprisingly in recent years there has been a surge of interest in the topic of energy efficient communications as can be seen from the number of recent special issues conferences and research projects devoted to green communications in addition to laudable environmental concerns it is simply not viable from a logis tical cost or battery technology point of view to continually increase power consumption due to the rapidly increasing network density cf section ii a the access network consumes the largest share of the energy research has focused on the following areas resource allocation the literature is rich in contribu tions dealing with the design of resource allocation strategies aimed at the optimization of the system energy efficiency the common message of these papers is that by accepting a moderate reduction in the data rates that could otherwise be achieved large energy savings can be attained within this special issue introduces an energy efficient coordinated beamforming design for hetnets network planning energy efficient network planning strategies include techniques for minimizing the number of bss for a coverage target and the design of adaptive bs sleep wake algorithms for energy savings the underlying philosophy of these papers is that since networks have been designed to meet peak hour traffic energy can be saved by partially switching off bss when they have no active users or simply very low traffic of course there are different andrews et al what will be degrees of hibernation available for a and attention must be paid in order to avoid unpleasant coverage holes this is usually accomplished through an increase of the transmitted power from nearby bss renewable energy another intriguing possibility is that of bss powered by renewable energy sources such as solar power this is of urgent interest in developing countries lacking a reliable and ubiquitous power grid but it is also intriguing more broadly as it allows drop and play small cell deployment if wireless backhaul is available rather than plug and play a recent paper showed that in a dense hetnet plausible per bs traffic loads can actually be served solely by energy harvesting bss a more relaxed scenario is considered in where the resource allocation makes efficient use of both renewable and traditional energy sources hardware solutions finally much of the power con sumption issues will be dealt with by hardware engineers with recent work in low loss antennas antenna muting and adaptive sectorization according to traffic requirements see e g in summary energy efficiency will be a major research theme for spanning many of the other topics in this article true cloud ran could provide an additional opportunity for energy efficiency since the centralization of the base band processing might save energy especially if advances on green data centers are leveraged the tradeoff between having many small cells or fewer macrocells given their very different power consumptions is also of considerable interest a complete characterization of the energy consumed by the circuitry needed for massive mimo is currently lacking mmwave energy efficiency will be particularly crucial given the unprecedented bandwidths iv spectrum regulation and standardization for departing from strictly technical issues we now turn our attention to the crucial intersections that technologies will encounter with public policy industry standardization and economic considerations a spectrum policy and allocation as discussed in section ii b the beachfront microwave spec trum is already saturated in peak markets at peak times while large amounts of idle spectrum do exist in the mmwave realm due to the different propagation characteristics and recalling the concept of phantom cells future systems will need to integrate a broad range of frequencies low frequencies for wide coverage mobility support and control and high frequencies for small cells this will require new approaches to spectrum policy and allocation methods topics such as massive mimo and small cells which address the efficient use of spectrum must also be considered important issues in spectrum policy an example a bs serving few users may choose to operate on a reduced set of subcarriers or it may switch off some of its sectors needless to say spectrum allocation and policy is an essential topic for so this section considers the pros and cons of different approaches to spectrum regulation in that context exclusive licenses the traditional approach to spectrum policy is for the regulator to award an exclusive license to a particular band for a particular purpose subject to limitations e g power levels or geographic coverage exclusive access gives full interference management control to the licensee and provides an incentive for investments in infrastructure allowing for quality of service guarantees downsides include high entry barriers because of elevated sunk costs both in the spectrum itself and in infrastructure and that such allocations are inherently inefficient since they occur over very long time scales typically decades and thus the spectrum is rarely allocated to the party able to make the best economic use of it to address these inefficiencies market based approaches have been propounded attempting to implement this idea spectrum auctions have been conducted recently to refarm spectrum a process whereby long held commercial radio and tv allocations are moved to different smaller bands releas ing precious spectrum for wireless communications a prime example of this is the so called digital dividend auctions arising from the digitization of radio and tv however there are claims that spectrum markets have thus far not been successful in providing efficient allocations because such markets are not sufficiently fluid due to the high cost of the infrastructure according to these claims spectrum and infrastructure cannot be easily decoupled unlicensed spectrum at the other extreme regulators can designate a band to be open access meaning that there is no spectrum license and thus users can share the band provided their devices are certified by class licenses examples are the industrial scientific and medical ism bands which are utilized by many devices including microwave ovens medical devices sensor networks cordless phones and especially by wifi with open access barriers to entry are much lower and there is enhanced competition and innovation as the incredible success of wifi and other ism band applications makes plain the downside of open access is potentially unmanageable interference no quality of service guarantees and possibly the tragedy of the commons where no one achieves a desired outcome still it is useful to consider the possibility of open access for bands utilized in small cells as future networks may involve multiple players and lower entry barriers may be needed to secure the emergence of small cell infrastructures although interference is indeed a significant problem in current open access networks it is interesting to note that cellular operators nevertheless rely heavily on wifi offloading currently about half of all cellular data traffic is proactively offloaded through unlicensed spectrum wifi hotspots are nothing but small cells that spatially reuse ism frequencies at mmwave frequencies the main issue is signal strength rather than interference and it is therefore plausible that mmwave bands be unlicensed or at a minimum several licensees will share a given band under certain new regulations this question is of pressing interest for spectrum sharing options do exist halfway between exclusive licenses and open access such as the opportunistic ieee journal on selected areas in communications vol no june use of tv white space while the potential of reusing this spectrum is enticing it is not crystal clear that reliable com munication services can be delivered that way alternatively authorized shared access and licensed shared access are regulatory frameworks that allow spectrum sharing by a limited number of parties each having a license under carefully specified conditions users agree on how the spectrum is to be shared seeking interference protection from each other thereby increasing the predictability and reliability of their services market based approaches to spectrum allocation given the advantages of exclusive licenses for ensuring quality of service it is likely that most beachfront spectrum will con tinue to be allocated that way nevertheless better utilization could likely be obtained if spectrum markets could become more fluid to that end liberal licenses do not in prin ciple preclude trading and reallocation on a fast time scale rendering spectrum allocations much more dynamic close attention must be paid to the definition of spectrum assets which have a space as well as a time scale and the smaller the scales the more fluid the market in small cells traffic is much more volatile than in macro cells and operators may find it beneficial to enter into sharing arrangements for both spectrum and infrastructure dynamic spectrum markets may emerge managed by brokers allow ing licenses to spectrum assets to be bought and sold or leased on time scales of hours minutes or even ms along these lines an interesting possibility is for a decoupling of infrastructure spectrum and services in particular there may be a separation between spectrum owners and oper ators various entities may own and or share a network of bss and buy and sell spectrum assets from spectrum owners via brokers these network owners may offer capacity to operators which in turn would serve the end customers with performance guarantees all of this however would require very adaptable and frequency agile radios we conclude this discussion by noting that offloading onto unlicensed spectrum such as tv whitespace or mmwave bands could have unexpected results in particular adding an unli censed shared band to an environment where a set of operators have exclusive bands can lead to an overall decrease in the total welfare braess paradox this is because operators might have an incentive to offload traffic even when this runs counter to the overall social welfare defined as the total profit of the operators and the utilities of the users minus the costs an operator might have an incentive to increase prices so that some traffic is diverted to the unlicensed band where the cost of interference is shared with other operators and this price increase more than offsets the operator benefits further while unlicensed spectrum generally lowers barriers to entry and increases competition the opposite could occur and in some circumstances a single monopoly operator could emerge within the unlicensed bands b regulation and standardization standardization status several regional forums and projects have been established to shape the vision and to study its key enabling technologies for example the aforementioned eu project metis has already released documents on scenarios and requirements meanwhile has been increasingly referred to as imt in many industry forums and international telecommu nications union itu working groups with the goal as the name suggests of beginning commercial deployments around to explore user requirements and to elaborate a standards agenda to be driven by them the etsi held a future mobile summit in nov the summit concluded in line with the thesis of this paper that an evolution of lte may not be sufficient to meet the anticipated requirements that conclusion notwithstanding standardization has not yet formally started within which is currently finalizing lte rel the third release for the lte advanced family of standards the timing of standardization has not even been agreed upon although it is not expected to start until later rel or rel likely around however many ongoing and proposed study items for rel are already closely related to candidate technologies covered in this paper e g massive mimo and thus in that sense the seeds of are being planted in whether an entirely new standards body will emerge for as envisioned in this paper is unclear the ongoing success of relative to its erstwhile competitors and the wimax forum certainly gives it an advantage although a name change to would seem to be a minimal step spectrum standardization spectrum standardization and harmonization efforts for have begun within the itu studies are under way on the feasibility of bands above ghz including technical aspects such as channel mod eling semiconductor readiness coverage mobility support potential deployment scenarios and coexistence with existing networks to be available for mmwave spectrum has to be re purposed by national regulators for mobile applications and agreement must be reached in itu world radiocommunication conferences wrc on the global bands for mmwave com munications these processes tend to be tedious and lengthy and there are many hurdles to clear before the spectrum can indeed be available on the itu side wrc is shaping up as the time and venue to agree on mmwave spectrum allocations for in addition to the itu many national regulators have also started their own studies on mmwave spectrum for mobile com munications in the usa the technological advisory council of the federal communications committee fcc has carried out extensive investigations on mmwave technology in the last few years and it is possible that fcc will issue a notice of inquiry in which is always the first step in fcc rulemaking process for allocation of any new frequency bands as discussed above it is also unclear how such bands will be allocated or even how they should be allocated and the technical community should actively engage the fcc to make sure they are allocated in a manner conducive to meeting requirements historically other national regulators have tended to follow the fcc lead on spectrum policy andrews et al what will be c economic considerations the economic costs involved in moving to are substantial even if spectrum costs can be greatly reduced through the approaches discussed above it is still a major challenge for carriers to densify their networks to the extent needed to meet our stated requirements two major challenges are that bs sites are currently expensive to rent and so is the backhaul needed to connect them to the core network infrastructure sharing one possible new business model could be based on infrastructure sharing where the owners of infrastructure and the operators are different there are several ways in which infrastructure could be shared passive sharing the passive elements of a network include the sites physical space rooftops towers masts and pylons the backhaul connection power supplies and air conditioning operators could cover larger geographical areas at a lower cost and with less power consumption if they shared sites and this might be of particular importance in dense networks regulation could be required to force major operators to share their sites and improve competition active sharing active infrastructure sharing would involve antennas bss radio access networks and even core networks bs and or radio access network sharing may be particularly attractive when rolling out small cell networks this type of sharing could lead to collusion with anticompetitive agree ments on prices and services regulations are required to prevent such collusion but on the positive side are the economies of scale mobile virtual network operators a small cell may be operated by a mobile virtual network operator that does not own any spectrum but has entered into an agreement with another operator to gain access to its spectrum within the small cell the small cell may provide coverage to an enterprise or business such that when a user leaves the enterprise it roams onto the other operator network offloading roaming is traditionally used to increase cover age in scenarios when service providers geographical reaches are limited however in and as discussed above traffic may be offloaded for a different reason spatial and temporal demand fluctuations such fluctuations will be greater in small cell networks recent papers consider the incentive for invest ment under various revenue sharing contracts it is shown in that sharing increases investment and the incentive is greater if the owner of the infrastructure gets the larger fraction of the revenue when overflow traffic is carried a bargaining approach for data offloading from a cellular network onto a collection of wifi or femtocell networks is considered in in this special issue backhaul a major consideration that has been consid ered in several places throughout the paper is backhaul which will be more challenging to provide for hyper dense ultra fast networks however we find optimism in three directions fiber deployments worldwide continue to mature and reach farther and farther into urban corridors wireless backhaul solutions are improving by leaps and bounds with considerable startup activity driving innova tion and competition further mmwave frequencies could be utilized for much of the small cell backhauling due to their ambivalence to interference this may in fact be the first serious deployment of non los mmwave with massive beamforming gains given that the backhaul con nection is quite static and outdoors to outdoors and thus more amenable to precise beam alignment backhaul optimization is becoming a pressing concern given its new status as a performance limiting factor and this is addressed in in this special issue the problem of jointly optimizing resources in the radio network and across the backhaul is considered in compression techniques for uplink cloud ran are devel oped in another approach is the proactive caching of high bandwidth content like popular video v conclusion it is an exciting time in the wireless industry and for wire less research at large daunting new requirements for are already unleashing a flurry of creative thinking and a sense of urgency in bringing innovative new technologies into reality even just two years ago a mmwave cellular system was considered something of a fantasy now it is almost considered an inevitability as this article has highlighted it is a long road ahead to truly disruptive networks many technical challenges remain spanning all layers of the protocol stack and their implementation as well as many intersections with regulatory policy and business considerations we hope that this article and those in this special issue will help to move us forward along this road digital object identifier msp date of publication december ieee signal processing magazine january transmitter receiver is equipped with and the more degrees of freedom that the propagation channel can provide the better the performance in terms of data rate or link reliability more pre cisely on a quasistatic channel where a codeword spans across only one time and frequency coherence interval the reliability of a point to point mimo link scales according to prob link outage snr n t n r where n t and n r are the numbers of transmit and receive antennas respectively and signal to noise ratio is denoted by snr on a channel that varies rapidly as a dynamic graphics function of time and frequency and where circumstances permit coding across many channel coherence intervals the achievable rate scales as min n t n r log snr the gains in multiuser systems are even more impressive because such systems offer the possibility to transmit simultaneously to several users and the flex ibility to select what users to schedule for reception at any given point in time the price to pay for mimo is increased complexity of the hard ware number of radio frequency rf chains and the complexity and energy consumption of the signal processing at both ends for point to point links complexity at the receiver is usually a greater concern than complexity at the transmitter for exam ple the complexity of optimal signal detection alone grows exponentially with n t in multiuser systems complexity at the transmitter is also a concern since advanced coding schemes must often be used to transmit information simultane ously to more than one user while maintaining a controlled level of interuser interference of course another cost of mimo is that of the physical space needed to accommodate the anten nas including rents of real estate with very large mimo we think of systems that use antenna arrays with an order of magnitude more elements than in systems being built today say antennas or more very large mimo entails an unprecedented number of antennas simultane ously serving a much smaller number of terminals the dispar ity in number emerges as a desir able operating condition and a practical one as well the number of terminals that can be simulta neously served is limited not by the number of antennas but rather by our inability to acquire channel state information for an unlimited number of termi nals larger numbers of terminals can always be accommodated by combining very large mimo technology with conventional time and frequency division multiplexing via orthogonal fre quency division multiplexing ofdm very large mimo arrays is a new research field both in communication theory propaga tion and electronics and represents a paradigm shift in the way of thinking both with regards to theory systems and imple mentation the ultimate vision of very large mimo systems is that the antenna array would consist of small active antenna units plugged into an optical fieldbus we foresee that in very large mimo systems each antenna unit uses extremely low power in the order of milliwatts at the very minimum of course we want to keep total transmitted power constant as we increase n t i e the power per antenna should be n t but in addition we should also be able to back off on the total transmitted power for example if our antenna array were serving a single terminal then it can be shown that the total power can be made inversely proportional to n t in which case the power required per antenna would be n t of course several complications will undoubtedly prevent us from ieee signal processing magazine january fully realizing such optimistic power savings in practice the need for multiuser multiplexing gains errors in channel state information csi and interference even so the prospect of saving an order of magnitude in transmit power is important because one can achieve better system performance under the same regulatory power constraints also it is important because the energy consumption of cellular base stations is a growing concern as a bonus several expensive and bulky items such as large coaxial cables can be eliminated altogether the coaxial cables used for tower mounted base stations today are up to cm in diameter moreover very large mimo designs can be made extremely robust in that the failure of one or a few of the antenna units would not appreciably affect the system malfunc tioning individual antennas may be hotswapped the contrast to classical array designs which use few antennas fed from a high power amplifier is significant so far the large number of antennas regime when n t and n r grow without bound has mostly been of pure academic interest in that some asymptotic capacity scaling laws are known for ideal situations more recently however this view is changing and a number of practically important system aspects in the large n t n r regime have been discovered for example showed that asymptotically as n t and very large mimo arrays is a new research field in communication theory propagation and electronics and represents a paradigm shift in the way of thinking with regards to theory under realistic assumptions on the propagation channel with a band width of mhz a time division multiplexing cellular system may accommodate more than single antenna users that are offered a net average throughput of mb both in the reverse uplink and the systems and implementation forward downlink links and a throughput of mb with probability these rates are achievable without cooperation among the base stations and by relatively rudimentary techniques for csi acquisition based on uplink pilot measurements several things happen when mimo arrays are made large first the asymptotics of random matrix theory kick in this has several consequences things that were random before now start to look deterministic for example the distribution of the singular values of the channel matrix approaches a deterministic function another fact is that very tall or very wide matrices tend to be very well conditioned also when dimensions are large some matrix operations such as inversions can be done fast by using series expansion techniques see the sidebar in the limit of an infinite number of antennas at the base station but with a single antenna per user then linear processing in the form of maximum ratio combining for the uplink i e matched filtering with the channel vector say h and maximum ratio transmission beam forming with h h h on the downlink is optimal this result ing processing is reminiscent of time reversal tr a technique used for focusing electromagnetic or acoustic waves the second effect of scaling up the dimensions is that thermal noise can be averaged out so that the system is predominantly limited by interference from other transmitters this is intuitively clear for the uplink since coherent averaging offered by a receive antenna array eliminates quantities that are uncorrelated between the antenna elements that is thermal noise in particular this effect is less obvious on the downlink however under certain cir cumstances the performance of a very large array becomes lim ited by interference arising from reuse of pilots in neighboring cells in addition choosing pilots in a smart way does not substan tially help as long as the coherence time of the channel is finite in a time division duplex tdd setting this effect was quantified in under the assumption that the channel is reciprocal and that the base stations estimate the downlink channels by using uplink received pilots finally when the aperture of the array grows the resolution of the array increases this means that one can resolve individ ual scattering centers with unprecedented precision interest ingly as we will see later on the communication performance of the array in the large number of antennas regime depends less on the actual statistics of the propagation channel but only on the aggregated properties of the propagation such as asymp totic orthogonality between channel vectors associated with distinct terminals of course the number of antennas in a practical system can not be arbitrarily large owing to physical constraints eventu ally when letting n r or n t tend to infinity our mathematical models for the physical reality will break down for example the aggregated received power would at some point exceed the transmitted power which makes no physical sense but long before the mathematical models for the physics break down there will be substantial engineering difficulties so how large is infinity in this article the answer depends on the precise circumstances of course but in general the asymptotic results of random matrix theory are accurate even for relatively small dimensions even ten or so in general we think of systems with at least antennas at the base station but probably fewer than taken together the arguments presented motivate entirely new theoretical research on signal processing and coding and network design for very large mimo systems this article will survey some of these challenges in particular we will discuss ultimate information theoretic performance limits some prac tical algorithms influence of channel properties on the system and practical constraints on the antenna arrangements information theory for very large mimo arrays shannon information theory provides under very precisely specified conditions bounds on attainable performance of com munications systems according to the noisy channel coding theorem for any communication link there is a capacity or achievable rate such that for any transmission rate less than the capacity there exists a coding scheme that makes the error rate arbitrarily small the classical point to point mimo link begins our discus sion and it serves to highlight the limitations of systems in which the working antennas are compactly clustered at both ieee signal processing magazine january ends of the link this leads naturally into the topic of multiuser mimo mu mimo which is where we envision very large mimo will show its greatest utility the shannon theory simplifies greatly for large numbers of antennas and it suggests capacity approaching strategies point to point mimo channel model a point to point mimo link consists of a transmitter having an array of n t antennas a receiver having an array of n r antennas with both arrays connected by a channel such that every receive antenna is subject to the combined action of all transmit anten nas the simplest narrowband memoryless channel has the following mathematical description for each use of the channel we have x t gs w where is the n t component vector of transmitted signals x is the n r component vector of received signals g is the n r n t propagation matrix of complex valued channel coefficients and w is the n r component vector of receiver noise the scalar t is a measure of the snr of the link it is proportional to the transmit ted power divided by the noise variance and it also absorbs vari ous normalizing constants in what follows we assume a normalization such that the expected total transmit power is unity e where the components of the additive noise vector are inde pendent and identically distributed i i d zero mean and unit variance circulary symmetric complex gaussian ran dom variables cn hence if there were only one antenna at each end of the link then within the quanti ties g x and w would be scalars and the snr would be equal to t g in the case of a wide band frequency dependent delay spread channel the channel is described by a matrix valued impulse response or by the equivalent matrix valued frequency response one may conceptually decompose the channel into par allel independent narrow band channels each of which is described in the manner of indeed ofdm rigorously per forms this decomposition achievable rate with i i d complex gaussian inputs the instantaneous mutual information between the input and the output of the point to point mimo channel under the assumption that the receiver has perfect knowledge of the channel matrix g measured in bits per symbol or equivalently bits per channel use is c i x log det c i n r t n t gg h m where i x denotes the mutual information operator i n r denotes the n r n r identity matrix and the superscript h denotes the hermitian transpose the actual capacity of the channel results if the inputs are optimized according to the water filling principle in the case that ggh equals a scaled iden tity matrix c is in fact the capacity to approach the achievable rate c the transmitter does not have to know the channel however it must be informed of the numerical value of the achievable rate alternatively if the chan nel is governed by known statistics then the transmitter can set a rate that is consistent with an acceptable outage probability for the special case of one antenna at each end of the link the achiev able rate becomes that of the scalar additive complex gaussian noise channel c log t g h the implications of are most easily seen by expressing the achievable rate in terms of the singular values of the propagation matrix g u d o w h where u and w are unitary matrices of dimension n r n r and n t n t respectively and d o is a n r n t diagonal matrix whose diagonal elements are the singular values o o g o min n t n r the achievable rate expressed in terms of the singular values c min n t n r log e to n t o is equivalent to the combined achievable rate of parallel links for which the th link has an snr of to n t with respect to the achievable rate it is interesting to consider the best and the worst possible distribution of singular values subject to the constraint obtained directly from that min n t n r o tr gg where tr denotes trace the worst case is when all but one of the singular values are equal to zero and the best case is when all of the min n t n r h h singular values are equal this is a simple conse quence of the concavity of the logarithm the two cases bound the achievable rate as follows log c t tr gg n t h h m c min n t n r log e t tr gg h n t min n t n r o if we assume that a normalization has been performed such that the magnitude of a propagation coefficient is typically equal to one then tr gg h n t n r h h and the above bounds simplify as follows log t n r h c min n t n r log c t max n n t t n r m ieee signal processing magazine january the rank worst case occurs either for compact arrays under line of sight los propagation conditions such that the transmit array cannot resolve individual elements of the receive array and vice versa or under extreme keyhole propagation conditions the equal singular value best case is approached when the entries of the propagation matrix are i i d random variables under favor able propagation conditions and a high snr the achievable rate is proportional to the smaller of the number of transmit and receive antennas limiting cases low snrs can be experienced by terminals at the edge of a cell for low snrs only beamforming gains are important and the achievable rate becomes c t tr n t h r gg h t ln ln t n this expression is independent of n t and thus even under the most favorable propagation conditions the multiplexing gains are lost and from the perspective of achievable rate multiple transmit antennas are of no value next let the number of transmit antennas grow large while keeping the number of receive antennas constant we further more assume that the row vectors of the propagation matrix are asymptotically orthogonal as a consequence c gg n t h m n t n r i n r and the achievable rate becomes c n t n r log det i n r t i n r h n r log t which matches the upper bound then let the number of receive antennas grow large while keeping the number of transmit antennas constant we also assume that the column vectors of the propagation matrix are asymptotically orthogonal so c g n h r g m n r n t i n t the identity det i aa h det i a h a combined with and yields c n r n t log det c i n t t n t g h g m n t log c t n n t r m which again matches the upper bound so an excess number of transmit or receive antennas combined with asymptotic orthogonality of the propagation vectors constitutes a highly desirable scenario extra receive antennas continue to boost the effective snr and could in theory compensate for a low snr and restore multiplexing gains that would otherwise be lost as in furthermore orthogonality of the propagation vectors implies that i i d complex gaussian inputs are optimal so that the achievable rates and are in fact the true channel capacities mu mimo the attractive multiplexing gains promised by point to point mimo require a favorable propagation environment and a good snr disappointing performance can occur in los propagation or when the terminal is at the edge of the cell extra receive antennas can compensate for a low snr but for the forward link this adds to the complication and expense of the terminal very large mimo can fully address the shortcomings of point to point mimo if we split up the antenna array at one end of a point to point mimo link into autonomous antennas we obtain the qualitatively different mu mimo our context for discussing this is an array of m antennas for example a base station which simultaneously serves k autonomous terminals since we want to study both forward and reverse link transmission we now abandon the notation n t and n r in what follows we assume that each terminal has only one antenna mu mimo dif fers from point to point mimo in two respects first the termi nals are typically separated by many wavelengths and second the terminals cannot collaborate among themselves either to transmit or to receive data propagation we will assume tdd operation so the reverse link propagation matrix is merely the transpose of the forward link propagation matrix our emphasis on tdd rather than fdd is driven by the need to acquire channel state information between extreme num bers of service antennas and much smaller numbers of terminals the time required to transmit reverse link pilots is independent of the number of antennas while the time required to transmit for ward link pilots is proportional to the number of antennas the propagation matrix in the reverse link g dimensioned m k is the product of an m k matrix h which accounts for small scale fading i e which changes over intervals of a wavelength or less and a k k diagonal matrix d b whose diagonal elements constitute a k vector b of large scale fading coefficients g hd b the large scale fading accounts for path loss and shadow fading thus the kth column vector of h describes the small scale fad ing between the kth terminal and the m antennas while the k th diagonal element of d b is the large scale fading coefficient by assumption the antenna array is sufficiently compact that all of the propagation paths for a particular terminal are subject to the same large scale fading we normalize the large scale fading coefficients such that the small scale fading coefficients typically have magnitudes of one for mu mimo with large arrays the number of antennas greatly exceeds the number of terminals under the most favor ieee signal processing magazine january able propagation conditions the column vectors of the propaga tion matrix are asymptotically orthogonal c g m h g m m k d b c h m h h m m k d d b b reverse link on the reverse link for each channel use the k terminals col lectively transmit a k vector of quadrature amplitude modu lation qam symbols q r and the antenna array receives an m vector x r x r t r gq r w r where w r is the m vector of receiver noise whose compo nents are independent and distributed as cn the quan tity t r is proportional to the ratio of power divided by noise variance each terminal is constrained to have an expected power of one e q r k k f k we assume that the base station knows the channel remarkably the total throughput e g the achievable sum rate of reverse link mu mimo is no less than if the terminals could collaborate among themselves c log det i k t r g h g h if collaboration were possible it could definitely make channel coding and decoding easier but it would not alter the ultimate sum rate the sum rate is not generally shared equally by the terminals consider for example the case where the slow fading coefficient is near zero for some terminal under favorable propagation conditions if there is a large number of antennas compared with terminals then the asymptotic sum rate is c m k log det i k m t r d b h k log m t r b k h k this has a nice intuitive interpretation if we assume that the columns of the propagation matrix are nearly orthogonal i e g h g m d b under this assumption the base station could process its received signal by a matched filter mf g h x r t r g h g q r g h w r m t r d b q r g h w r this processing separates the signals transmitted by the differ ent terminals the decoding of the transmission from the kth terminal requires only the kth component of this has an snr of m t r b k which in turn yields an individual rate for that terminal corresponding to the kth term in the sum rate forward link for each use of the channel the base station transmits an m vector f through its m antennas and the k terminals collec tively receive a k vector x f x f t f g t f w f where the superscript t denotes transpose and w f is the k vector of receiver noise whose components are indepen dent and distributed as cn the quantity t f is propor tional to the ratio of power to noise variance the total transmit power is independent of the number of antennas e f the known capacity result for this channel see e g and assumes that the terminals as well as the base station know the channel let d c be a diagonal matrix whose diagonal elements constitute a k vector c to obtain the sum capacity requires performing a constrained optimization c max c k log det i m t f gd g h h subject to k k k k c k c c under favorable propagation conditions and a large excess of antennas the sum capacity has a simple asymptotic form c m k max c k log det i k t f d c g h gd c h max c k log det i m t d d h max k k k f c b k log m tc f k b k h c where c is constrained as in this result makes intuitive sense if the columns of the propagation matrix are nearly orthogonal which occurs asymptotically as the number of antennas grows then the transmitter could use a simple mf linear precoder f m g d d p q f where q f b is the vector of qam symbols intended for the termi nals that following such k k that p k e q f k the and p is a vector of powers such substitution of into yields the x f t f m d b d p q f w f which we k k identify log translates p c m into t f p k b k h an achievable sum rate of identical to the sum capacity if ieee signal processing magazine january antenna and propagation aspects of very large mimo the performance of all types of mimo systems strongly depends on properties of the antenna arrays and the propagation environ ment in which the system is operating the complexity of the propagation environment in combination with the capability of the antenna arrays to exploit this complexity limits the achiev able system performance when the number of antenna ele ments in the arrays increases we meet both opportunities and challenges the opportunities include increased capabilities of exploiting the propagation channel with better spatial resolu tion with well separated ideal antenna elements in a sufficiently complex propagation environment and without directivity and mutual coupling each additional antenna element in the array adds another degree of freedom that can be used by the system in reality though the antenna elements are never ideal they are not always well separated and the propagation environment may not be complex enough to offer the large number of degrees of freedom that a large antenna array could exploit in this section we illustrate and discuss some of these opportunities and challenges starting with an example of how more antennas in an ideal situation improves our capability to focus the field strength to a specific geographical point a cer tain user this is followed by an analysis of how realistic noni deal antenna arrays influence the system performance in an ideal propagation environment finally we use channel measurements to address properties of a real case with a element base station array serving six single antenna users spatial focus with more antennas precoding of an antenna array is often said to direct the signal from the antenna array toward one or more receivers in a pure los environment directing means that the antenna array forms a beam toward the intended receiver with an increased field strength in a certain direction from the transmitting array in propagation environments where non los components domi nate the concept of directing the antenna array toward a certain receiver becomes more complicated in fact the field strength is not necessarily focused in the direction of the intended receiver but rather to a geographical point where the incoming mul tipath components add up constructively different techniques for focusing transmitted energy to a specific location have been addressed in several contexts in particular it has drawn atten tion in the form of tr where the transmitted signal is a time reversed replica of the channel impulse response tr with single as well as multiple antennas has been demonstrated lately in e g and in the context of this article the most interest ing case is multiple input single output and here we speak of tr beamforming trbf while most communications applica tions of trbf address a relatively small number of antennas the same basic techniques have been studied for almost two decades in medical extracorporeal lithotripsy applications with a large number of antennas transducers to illustrate how large antenna arrays can focus the elec tromagnetic field to a certain geographic point even in a narrowband channel we use the simple geometrical channel model shown in figure the channel is composed of uniformly distributed scatterers in a square of dimension m m where m is the signal wavelength the scattering points shown in the figure are the actual ones used in the example below the broadside direction of the m element uni form linear array ula with adjacent element spacing of d m is pointing toward the center of the scatterer area each single scattering multipath component is subject to an inverse power law attenuation proportional to distance squared propagation exponent and a random reflection coefficient with i i d complex gaussian distribution giving a rayleigh distributed amplitude and a uniformly distributed phase this model creates a field strength that varies rapidly over the geographical area typical of small scale fading with a complex enough scattering environment and a sufficiently large element spacing in the transmit array the field strength resulting from different elements in the transmit array can be seen as independent scatterers m m geometry of the simulated dense scattering environment with uniformly distributed scatterers in an m area the transmit m element ula is placed at a distance of m from the edge of the scatterer area with its broadside pointing toward the center two single scattering paths from the first ula element to an intended receiver in the center of the scatterer area are shown m ula m ula a b m normalized field strength in a m area centered around the receiver to which the beamforming is done parts a and b show the field strength when an m and an m ula are used together with mf precoding to focus the signal to a receiver in the center of the area ieee signal processing magazine january in figure we show the resulting normalized field strength in a small m m environment around the receiver to which we focus the trans mitted signal using mf precoding for ulas with d m of size m and m elements the normal ized field strength shows how much weaker the field strength is in a cer tain position when the spatial signa ture to the center point is used rather than the correct spatial signature for that point hence the normalized field strength is db at the center of both figures and negative at all other points figure illustrates two impor tant properties of the spatial mf pre coding that the field strength can be focused to a point rather than in a certain direction and that more antennas improve the ability to focus energy to a certain point which leads to less interference between spatially separated users with m antenna elements the focusing of the field strength is quite poor with many peaks inside the studied area increasing m to antenna elements for the same propagation environment con siderably improves the field strength focusing and it is more than db down in most of the studied area while the example above only illustrates spatial mf precod ing in the narrowband case the trbf techniques exploit both the spatial and temporal domains to achieve an even stronger spatial focusing of the field strength with enough antennas and favorable propagation conditions trbf will not only focus power and yield a high spectral efficiency through spatial multi plexing to many terminals it will also reduce or in the ideal case completely eliminate intersymbol interference in other words one could dispense with ofdm and its redundant cyclic prefix each base station antenna would merely convolve the data sequence intended for the kth ter minal with the conjugated time reversed version of his estimate for the channel impulse response to the kth terminal sum the k convolutions and feed that sum into his antenna again under favorable propagation con ditions and a large number of antennas intersymbol interference will decrease significantly antenna aspects it is common within the signal process ing communications and information theory communities to assume that the transmit and receive antennas are iso tropic and unipolarized electromag netic wave radiators and sensors respectively in reality such isotropic db unipolar antennas do not exist according to fundamental laws of electromagnetics nonisotropic antenna patterns will influ ence the mimo performance by changing the spatial correla tion for example directive antennas pointing in distinct directions tend to experience a lower correlation than nondirec tive antennas since each of these directive antennas see signals arriving from a distinct angular sector in the context of an array of antennas it is also common in these communities to assume that there is negligible electro magnetic interaction or mutual coupling among the antenna elements neither in the transmit nor in the receive mode this assumption is only valid when the antennas are well separated from one another in the rest of this section we consider very large mimo arrays where the overall aperture of the array is constrained for example by the size of the supporting structure or by aesthetic considerations increasing the number of antenna elements implies that the antenna separation decreases this problem has been examined in recent papers although the focus is often on spatial correlation and the effect of coupling is often neglected as in in the effect of coupling on the capacity of fixed length ulas is studied in general it is found that mutual coupling has a substantial impact on capacity as the number of antennas is increased for a fixed array aperture it is conceivable that the capacity performance in can be improved by compensating for the effect of mutual cou pling indeed coupling compensation is a topic of current interest much driven by the desire of implementing mimo arrays in a compact volume such as mobile terminals see and references therein one interesting result is that coupling among copolarized antennas can be perfectly mitigated by the use of optimal multiport impedance matching rf circuits this technique has been experimentally demonstrated only for up to four antennas though in principle it can be applied to very large mimo arrays never theless the effective cancellation of coupling also brings about diminish ing bandwidth in one or more output ports as the antenna spacing decreases this can be understood intui tively in that in the limit of small antenna spacing the array effectively reduces to only one antenna thus one can only expect the array to offer the same characteristics as a single antenna furthermore implementing practical matching circuits will intro duce ohmic losses which reduces the gain that is achievable from coupling cancellation another issue to consider is that due to the constraint in array aperture very large mimo arrays are expected to be implemented in a two dimensional d or three dimensional d array ieee signal processing magazine january structure instead of as a linear array as in a linear array with antenna elements of identical gain patterns e g isotropic elements suffers from the problem of front back ambiguity and is also unable to resolve signal paths in both azimuth and elevation however one drawback of having a dense array implementation in d or d is the increase of coupling effects due to the increase in the number of adjacent antennas for the square array d case there are up to four adjacent antennas located at the same distance for each antenna element and in d there are up to six a further problem that is specific to d arrays is that only the antennas located on the surface of the d array contribute to the information capacity which in effect restricts the usefulness of dense d array implementa tions this is a consequence of the integral representation of maxwell equations by which the electromagnetic field inside the volume of the d array is fully described by the field on its surface assuming sufficiently dense sampling and therefore no additional information can be extracted from elements inside the d array moreover in outdoor cellular environments signals tend to arrive within a narrow range of elevation angles therefore it may not be feasible for the antenna system to take advantage of the resolution in elevation offered by dense d or d arrays to perform signaling in the vertical dimension the complete single user mimo su mimo signal model with antennas and matching circuit in figure reproduced from with permission is used to demonstrate the perfor mance degradation resulting from correlation and mutual cou pling in very large arrays with fixed apertures in the figure z t and z r are the impedance matrices of the transmit and receive arrays respectively i it and i ir are the excitation and received currents at the ith port of the transmit and receive systems respectively and v is and v ir z and z l are the source and load voltages impedances respectively and v it is the terminal v v v g mc i v v z l v transmitter channel receiver i rl i v i v i diagram of a mimo system with antenna impedance matrices and matching networks at both link ends freely reproduced from v z z t z r i voltage across the ith transmit antenna port g mc is the overall channel of the system including the effects of antenna coupling and matching circuits recall that the instantaneous capacity from this point on we shall for simplicity refer to the log det formula with i i d complex gaussian inputs as the capacity to avoid the more clumsy notation of achievable rate is given by and equals c mc log det c i n t n t g t mc g t h mc m where g t mc r l z l z r gr t is the overall mimo channel based on the complete su mimo signal model g represents the propagation channel as seen by the transmit and receive r t re z t note that g t mc antennas and r l re z l is the normalized version of g mc shown in figure where the nor malization is performed with respect to the average channel gain of a single input single out put siso system the source impedance matrix z in mu mimo systems the terminals are autonomous so that we can assume that the does not appear in the expression since transmit array is uncoupled and uncorrelated g t mc represents the transfer func tion between the transmit and receive power waves and z is implicit in t to give an intuitive feel for the effects of mutual cou pling we next provide two examples of the impedance matrix z r one for small adjacent antenna spacing and one for moderate spacing for a given antenna array z t z r by the principle of reciprocity the following numer ical values are obtained from the induced electromotive t n e m e l e a n n e t n a r e e u l e n n a h c p t i correlation and coupling ula y t i c a p b correlation but no coupling ula correlation and coupling usa a c correlation but no coupling usa iid rayleigh adjacent element spacing m impact of correlation and coupling on capacity per antenna over different adjacent antenna spacing for autonomous transmitters m k and the apertures of ula and usa are and m m respectively ieee signal processing magazine january force method for a ula consisting of three parallel dipole antennas j j j z r m j j j h j j j and j j j z r m j j j h j j j it can be observed that the severe mutual coupling in the case of d results in off diagonal elements whose values are closer to the diagonal elements than in the case of d where the diagonal elements are more dominant despite this the impact of coupling on capacity is not immediately obvious since the impedance matrix is embedded in and is condi tioned by the load matrix z l therefore we next provide numer ical simulations to give more insight into the impact of mutual coupling on mimo performance in mu mimo systems the ter minals are autonomous so that we can assume that the transmit array in the reverse link is uncou pled and uncorrelated we remind the reader that in mu mimo sys tems we replace n t and n r with k and m respectively if the kronecker model is assumed for the propagation channel g can be expressed as g w r g iid w t where w t and w r are the transmit and receive correlation matrices respectively and g iid is a matrix with i i d rayleigh entries in this case w t i k and z t is diagonal for the particular case of m k figure shows a plot of the uplink ergodic capacity or average rate per user c mc k versus the antenna separation for ulas with a fixed aperture of at the base station with up to m k elements the correla tion but no coupling case refers to the mimo channel g w r g iid w t whereas the correlation and coupling case refers to the effective channel matrix g t mc in the environment is assumed to be uniform d angular power spectrum aps and the snr is t db the total power is fixed and equally divided among all users one thousand independent realiza tions of the channel are used to obtain the average capacity for comparison the corresponding ergodic capacity per user is also calculated for users and an element receive uniform square array usa with m k and an aperture size of m m for up to m elements rather than advocating the practicality of users in a single cell this assumption is only intended to demonstrate the limitation of aperture constrained very large mimo arrays at the base station to support parallel mu mimo channels as can be seen in figure the capacity per user begins to fall when the element spacing is reduced to below for the usas as opposed to below for the ulas which shows that for a given antenna spacing packing more elements in more than one dimension results in significant degradation in capac ity performance another distinction between the ulas and usas is that coupling is in fact beneficial for the capacity perfor mance of ulas with moderate antenna spacing i e between and whereas for usas the capacity with coupling is consistently lower than that with only correlation the observed phenomenon for ulas is similar to the behavior of two dipoles with decreasing element spacing there coupling induces a larger difference between the antenna patterns i e angle diver sity over this range of antenna spacing which helps to reduce correlation at even smaller antenna spacings the angle diver sity diminishes and correlation increases together with loss of power due to coupling and impedance mismatch the increasing correlation results in the capacity of the correlation and cou pling case falling below that of the correlation only case with the crossover occurring at approximately on the other hand each element in the usas experiences more severe cou pling than that in the ulas for the same adjacent antenna spac ing which inherently limits angle diversity even though figure demonstrates that both coupling and correlation are detrimental to the capacity performance of very large mimo arrays relative to the i i d case it does not provide any specific information on the behavior of g t mc in particular it is important to examine the impact of correlation and coupling on the asymptotic orthogonality assumption made in for a very large array with a fixed aperture in an mu setting to this end we assume that the base station serves k single antenna terminals the channel is normalized so that each user termi nal has a reference snr t k db in the siso case with conjugate matched single antennas as before the coupling and correlation at the base station is the result of implement ing the antenna elements as a square array of fixed dimensions m m in a channel with uniform d aps the number of elements in the receive usa m varies from to to support one dedicated channel per user the average condition number of g t h mc g t mc k is given in figure a for channel realizations since the ieee signal processing magazine january propagation channel is assumed to be i i d in for simplicity d i k t t should ideally approach one which is observed for the i i d ray leigh case by way of contrast it can be seen that the channel is not asymptotically orthogonal as assumed in in the pres ence of coupling and correlation the corresponding maximum rate for the reverse link per user is given in figure b it can be seen that if coupling is ignored spatial correlation yields only a minor penalty relative to the i i d case this is so because the transmit array of dimensions m m is large enough to offer almost the same number of spatial degrees of freedom k as in the i i d case despite the channel not being asymptotically orthogonal on the other hand for the realistic case with coupling and correlation adding more receive elements into the usa will eventually result in a reduc tion of the achievable rate despite having a lower average con dition number than in the correlation but no coupling case this is attributed to the significant power loss through coupling and impedance mismatch which is not modeled in the correla tion only case real propagation measured channels when it comes to propagation aspects of mimo as well as very large mimo the correlation properties are of paramount inter est since those together with the number of antennas at the terminals and base station determines the orthogonality of the propagation channel matrix and the possibility to separate dif ferent users or data streams in conventional mu mimo sys tems the ratio of number of base station antennas and antennas a b this implies that the condition number of g h mc g mc k n o i t i d n r correlation and coupling o e b correlation but no coupling c e m n iid rayleigh g a r e v u m a r e u r e p e t a r a e u l e n n a h c correlation and mutual coupling e g a r t i b correlation but no coupling e v iid rayleigh 102 m b impact of correlation and coupling on a asymptotic orthogonality of the channel matrix and b max sum rate of the reverse link for k at the terminals is usually close to one at least it rarely exceeds two in very large mu mimo systems this ratio may very well exceed if we also consider the number of expected simulta neous users k the ratio at least usually exceeds ten this is important because it means that we have the potential to achieve a very large spatial diversity gain it also means that the distance between the null spaces of the different users is usu ally large and as mentioned before that the singular values of the tall propagation matrix tend to have stable and large values this is also true in the case where we consider multiple users where we can consider each user as a part of a larger distrib uted but uncoordinated mimo system in such a system each new user consumes a part of the available diversity under certain reasonable assumptions and favorable propagation con ditions it will however still be possible to create a full rank propagation channel matrix where all the eigenvalues have large magnitudes and show a stable behavior the question is now what we mean by the statement that the propagation con ditions should be favorable one thing is for sure as compared to a conventional mimo system the requirements on the chan nel matrix to get good performance in very large mimo are relaxed to a large extent due to the tall structure of the matrix it is well known in conventional mimo modeling that scat terers tend to appear in groups with similar delays angle of arrivals and angle of departures and they form so called clusters usually the number of active clusters and distinct scat terers are reported to be limited see e g also when the number of physical objects is large the contributions from individual multipath components belonging to the same cluster are often correlated which reduces the number of effective meas meas iid iid p a i c b a b o r ordered eigenvalues of ghg db cdfs of ordered eigenvalues for a measured large array system a measured mimo system and simulated i i d and mimo systems note that for the simulated i i d cases only the cdfs of the largest and smallest eigenvalues are shown for clarity ieee signal processing magazine january scatterers similarly it has been shown that a cluster seen by different users so called joint clusters introduces correlation between users also when they are widely separated it is still an open question whether the use of large arrays makes it possible to resolve clusters completely but the large spatial res olution will make it possible to split up clusters in many cases there are measurements showing that a cluster can be seen dif ferently from different parts of a large array which is bene ficial since the correlation between individual contributions from a cluster then is decreased to exemplify the channel properties in a real situation we consider a measured channel matrix where we have an indoor antenna base station consisting of four stacked double polarized element circular patch arrays and six single antenna users three of the users are indoors at various posi tions in an adjacent room and three users are outdoors but close to the base station the measurements were performed at ghz with a bandwidth of mhz in total we consider an ensemble of snapshots taken from a continuous movement of the user antenna along a m line and frequency points giving us in total narrow band realizations it should be noted though that they are not fully independent due to the nonzero coherence bandwidth and coherence dis tance the channels are normalized to remove large scale fad ing and to maintain the small scale fading the mean power over all frequency points and base station antenna elements is unity for all users in figure we plot the cumulative distribu tion functions cdfs of the ordered eigenvalues of g h g the left most solid curve corresponds to the cdf of the smallest eigenvalue etc for the propagation matrix meas together with the corresponding cdfs for a measured conventional mimo meas system where we have used a subset of six adjacent copolar ized antennas on the base station as a ref erence we also plot the distribution of the largest and smallest eigenvalues for a sim ulated and conventional mimo system i i d and i i d with i i d complex gaussian entries note that for clarity of the figure the eigenvalues are not normalized with the number of antennas at the base station and therefore there is an offset of log m this offset can be interpreted as a beamforming gain in any case the relative spread of the eigenvalues is of more interest than their absolute levels it can be clearly seen that the large array provides eigenvalues that all show a stable behavior low variances and have a relatively low spread small distances between the cdf curves the difference between the smallest and largest eigen value is only around db which could be compared with the conventional mimo system where this dif ference is around db this eigenvalue spread corresponds to that of a conventional mimo system with i i d complex gaussian channel matrix entries keeping in mind the circular structure of the base station antenna array and that half of the elements are cross polarized this number of effective chan nels is about what one could anticipate to get one important factor in realistic channels especially for the uplink is that the received power levels from different users are not equal power variations will increase both the eigenvalue spread and the vari ance and will result in a matrix that still is approximately orthogonal but where the diagonal elements of g gh have vary ing mean levels specifically the d b to do multiuser precoding in the forward link and detection in the reverse link the base station must acquire csi matrix in transceivers we next turn our attention to the design of practical transceiv ers a method to acquire csi at the base station begins the dis cussion then we discuss precoders and detection algorithms suitable for very large mimo arrays acquiring csi at the base station to do multiuser precoding in the forward link and detection in the reverse link the base station must acquire csi let us assume that the frequency response of the channel is constant over n coh con secutive subcarriers with small antenna arrays one possible sys tem design is to let the base station antennas transmit pilot symbols to the receiving units the receiving units perform chan nel estimation and feed back partial or complete csi via dedi cated feedback channels such a strategy does not rely on channel reciprocity i e the forward channel should be the transpose of the reverse channel however with a limited coherence time this strategy is not viable for large arrays the number of time slots devoted to pilot symbols must be at least as large as the number of antenna elements at the base station divided by n coh when m grows the time spent on transmitting pilots may surpass the coherence time of the channel consequently large antenna array technology must rely on channel reciprocity with channel reciprocity the receiving units table snr and sinr expressions for a collection of standard precoding techniques snr and sinr expressions as km mk a precoding technique perfect csi imperfect csi benchmark intererence free system t f a zero forcing t f a p t f a p t f matched filter t a p ta f f vector perturbation f f t t t f ar c a m a a m n a ieee signal processing magazine january send pilot symbols via tdd since the frequency response is assumed constant over n coh subcarriers n coh terminals can transmit pilot symbols simultaneously during one ofdm symbol interval in total this requires k n coh time slots we remind the reader that k is the number structs its of channel terminals estimate served g t the t kk subsequently base station in used the for kth precoding cell con in the forward link based on the pilot observations the power of each pilot symbol is denoted t p precoding in the forward link collection of results for single cell systems user k receives the kth component of the composite vector x f g t f w f the vector f is a precoded version of the data symbols q f each component of f has average power t f m further we assume that the channel matrix g has i i d cn entries in what follows we derive snr signal to interference plus noise ratio sinr expressions for a number of popular precoding techniques in the large system limit i e with m k but with a fixed ratio a m k the obtained expressions are tabulated in table let us first discuss the performance of an intererence free if system that will subsequently serve as a benchmark reference the best performance that can be imagined will result if all the channel energy to terminal k is delivered to terminal k without any inter user interference in that case terminal k receives the sample x f k f m q f k w f k since x k g k m g k j m m and e q f k q f h k t f k the snr per receiving unit for if systems converges to t f a as m we now move on to practical precoding methods the concep tually simplest approach is to invert the channel by means of the pseudoinverse this is referred to as zero forcing zf precoding a variant of zero forcing is block diagonalization which is not covered in this article intuitively when m grows g tends to have nearly orthogonal columns as the terminals are not corre lated due to their physical separation this assures that the perfor mance of zf precoding will be close to that of the if system however a disadvantage of zf is that processing cannot be done distributedly at each antenna separately with zf precoding all data must instead be collected at a central node that handles the processing formally the zf precoder sets f c g t q f c g g g q t f where the superscript denotes the pseudoinverse of a matrix i e g t g g t g and c normalizes the average power in t f to f a suitable choice for c is c tr g t g k which aver ages fluctuations in transmit power due to g but not to q f the received sample x kf with zf precoding becomes x f k q f k c w f k with that the instantaneous received snr per terminal equals snr t f k c tr t f g t g when both the number of terminals k and the number of base station antennas m grow large but with fixed ratio a m k tr g t g converges to a fixed deterministic value tr g t g a as k m m k substituting into gives the expression in table the conclusion is that zf precoding achieves an snr that tends to the optimal snr for an if system with m k transmit antennas when the array size grows note that when m k one gets snr a approximate matrix inversion much of the computational complexity of the zf precoder and the reverse link detectors lies in the inversion of a k k matrix z although base stations have high computational power it is of interest to find approximate solutions by simpler means than outright inversion in the following we review an intuitive method for approximate matrix inversion it is known that if a k k matrix z has the property n lim i z n k then its inverse can be expressed as a neumann series z i z n k k n ostensibly it appears that matrix inversion using is even more complex than direct inversion since both matrix inversion and multiplication are o k operations however in hardware matrix multiplication is strongly preferred over inversion since it does not require any divisions moreover if only the result of the inverse times a vector z is of interest then can be implemented as a series of cas caded matched filters the complexity of each matched filter operation is only o k let us first consider the case of a k m matrix g with independent and cn distributed entries we remind the reader that a m k the objective is now to approxi mate the inverse of the wishhart matrix z ggh as k and m grows the eigenvalues of z converges to a fixed deter ministic distribution known as the marchenko pastur ieee signal processing magazine january a problem with zf precoding is that the construction of the pseudoinverse g t g g t g requires the inversion of a k k matrix which is computationally expensive however as m grows g t g m tends to the identity matrix which has a trivial inverse consequently the zf precoder tends to g which is noth ing but an mf this suggests that matrix inversion may not be needed when the array is scaled up as the mf precoder approxi mates the zf precoder well for practical values of a the matrix can be simplified greatly see approximate matrix inver sion formally the mf sets f c g q f with c tr g t g k a few simple manipulations lead to an asymptotic expression of the sinr which is given in table from the mf precoding sinr expression it is seen that the sinr can be made as high as desired by scaling up the antenna array however the mf precoder exhibits an error floor since t f sinr a has we imperfect next turn csi the let attention gt t to scenarios where the base station denote the minimum mean square error mmse channel estimate of the forward link the estimate satisfies g t t p g t p e distribution the largest and the smallest eigenvalues of z converge to m max z c a m m min z c a m some minor manipulations show that m max a a z j a a m min a a z j a a hence the eigenvalues of i k a a z i k z m k lie approximately in the range a a a a note that a a whenever a therefore n lim c i k m k z m n k when m k is large say five to ten or so converges rapidly and only a few terms needs to be computed for finite dimen sions k and m the eigenvalues of a particular channel realization can lie outside the range a a a a there fore an attenuation factor d is introduced altogether the inverse matrix zzg h can be approximated as z d m k n l c i k d m k z m n replacing the weighting coefficent m k with c tr z c a constant provides a robust method for matrix approximation when the channel matrix has an unknown distribution other techniques e g based on the cayley hamilton theorem and random matrix theory have been extensively used for cdma receivers see and if the weighting coefficients are optimized the matrix inversion in cdma receivers can be approximated with only terms where p represents the reli ability of the estimate and e is a matrix with i i d cn distributed entries sinr expressions for mf and zf pre coding are given in table for any reliability p the sinr can be made as high as desired by scaling up the antenna array nonlinear precoding techniques such as dpc vector perturbation vp and lattice aided methods are important techniques when m is not much larger than k this is true since in the m k regime the performance gap of zf to the if benchmark is signif icant see table and there is room for improvement by nonlinear techniques however the gap of zf to an if system scales as a a when m is say two times k this gap is only db nonlinear techniques will oper ate closer to the if benchmark but cannot surpass it therefore the gain of nonlinear methods does not at all justify the complex ity increase the measured channels that we discussed earlier in the article behave as if a hence linear precoding is virtually optimal and one can dispense with dpc for completeness we give an approximate large limit snr expression for vp derived from the results of in table the expression is strictly speaking an upper bound to the snr but is reasonably tight so that it can be taken as an approximation for a n the sinr expression surpasses that of an if system which makes the expression meaningless however for larger val ues of a linear precoding performs well and there is not much gain in using vp anyway for vp no sinr expression is available in the literature with imperfect csi in figure we show ergodic sum rate capacities for mf pre coding zf precoding and dpc as benchmark performance we also show the ensuing sum rate capacity from an if system in all cases k users are served and we show results for m for m it can be seen that dpc decisively outperforms zf and is about db away from the if benchmark performance but as m grows the advantage of dpc quickly dimin ishes with m the gain of dpc is about db this confirms that the performance gain does not at all justify the complexity increase with base station antennas zf precoding performs almost as good as an if system at low snr mf precoding is better than zf precoding it is interesting to observe that this is true over a wide range of snrs for the case of m k sum rate capacity expressions of vp are currently not available in the literature since the optimal distribution of the inputs for vp is not known to date precoding in the forward link the ultimate limit of noncooperative multicell mimo with large arrays in this section we investigate the limit of noncooperative cellular mu mimo systems as m grows without limit the presentation if dpc zf sum rate capacities of single cell mu mimo precoding techniques the channel is i i d complex gaussian cn and there are k terminals circles show the performance of if systems dpc is denoted by an x solid lines refer to zf and the dotted lines refer to mf k k k cell cell cell k cell a b illustration of the pilot contamination concept a during the training phase the base station in cell overhears the pilot transmission from other cells b as a consequence the transmitted vector from base station will be partially beamformed to the terminals in cell ieee signal processing magazine january y t i c a e u l 150 p a c e t a e n n a h c r m u s t i b summarizes and extends the results of for single cell as well as for multicell mimo the end effect of letting m grow without limits is that thermal noise and small scale rayleigh fading van ishes however as we will discuss in detail with multiple cells the interference from other cells due to pilot contamination does not vanish the concept of pilot contamination is novel in a cellu lar mu mimo context and is illustrated in figure but was an issue in the context of cdma usually under the name pilot pol lution the channel estimate computed by the base station in cell gets contaminated from the pilot transmission of cell the base station in cell will in effect beamform its signal par tially along the channel to the terminals in cell due to the beamforming the interference to cell does not vanish asymp totically as m we consider a cellular mu mimo ofdm system with hexago nal cells and n fft subcarriers all cells serves k autonomous ter minals and has m antennas at the base station further a sparse scenario k m is assumed for simplicity hence terminal sched uling aspects are not considered the base stations are assumed noncooperative the m k composite channel matrix between the k terminals in cell k and the base station in cell j is denoted g kj relying on reciprocity the forward link channel matrix k tf db m m m mf m m m between the base station in cell j and the terminals in cell k becomes g t kj see figure the base station in the kth cell transmits the vector f k which is a precoded version of the data symbols q f k intended for the ter minals in cell k each terminal in the kth cell receives his respec tive component of the composite vector x f k t f g t kj f j w f k j as before each element of g kj comprises a small scale ray leigh fading factor as well as a large scale factor that accounts for geometric attenuation and shadow fading with that g kj factors as g kj h kj d b kj in h kj is a m k matrix which represents the small scale fading between the terminals in cell k to the base station in cell j and all entries are i i d cn distributed the k k matrix d kj is a diagonal matrix comprising the ele ments kj b b b kj b kj f b kjk along its main diagonal each value b kj represents the large scale fading between terminal in the kth cell and the base station in cell j the base station in the tions and obtains a channel nth estimate cell processes g t t nn its pilot observa of g t nn in the worst case the pilot signals in all other cells are perfectly synchro nized mate g t with t nn gets the contaminated pilot signals from in cell pilot n hence signals in the other channel cells esti g t t nn t p g t nn t p g t in v t n i n in it is implicitly assumed that all terminals transmits identical pilot signals adopting different pilot signals in t g kj cell k cell j the composite channel between the base station in cell j and the terminals in cell k is denoted g t kj ieee signal processing magazine january different cells does not improve the situation much since the pilot signals must at least be confined to the same signal space which is of finite dimensionality note that due to the geometry of the cells g nn is generally stronger than g in i n v n is a matrix of receiver noise during the training phase uncorrelated with all propagation matrices and comprises i i d cn distributed elements t p is a mea sure of the snr during of the pilot transmission phase motivated by the virtual optimality of simple linear precod ing from the section precoding in the forward link collection of results for single cell systems we let the base station in cell n use the mf g t t nn h g t nn as precoder we later investigate zero forcing precoding power normalization of the precoding matrix is unimportant when m as will become clear shortly the th terminal in the jth cell receives the th compo nent of the vector x f j x f j x f j f x f jk t inserting into gives x f j t f g t j n g t nn q f n w f j n t f j f f the composite received signal vector f h n g t n t p i g t in v t n e q n w j x j in contains terms of the form g t j n g in as m grows large only terms where j i remain significant we get x f j t m t f t p f n g j n g j n m q n further as m grows the effect of small scale rayleigh fading vanishes j j j as m g t n g n m d b n hence the processed received signal of the th receiving unit in the jth cell is x f j m t f t p b j j q f j b j n q f n n j the signal to interference ratio sir of terminal becomes sir b b jj jn n j which does not contain any thermal noise or small scale fad ing effects note that devoting more power to the training phase does not decrease the pilot contamination effect and leads to the same sir this is a consequence of the worst case scenario assumption that the pilot transmissions in all cells overlap if the pilot transmissions are staggered so that pilots in one cell collide with data in other cells devoting more power to the training phase is indeed beneficial how ever in a multicell system there will always be some pilot transmissions that collide although perhaps not in neighbor ing cells we now replace the mf precoder in with the pseu doinverse of the channel estimate g t t nn g t nn g t t nn g t nn inserting the expression for the channel estimate gives g t t nn t p i g in v n e c t p i g t i n v t n e t p i g i n v n e m again when m grows only products of correlated terms remain significant g t t nn m t p t p i g in v n e c i d b in t p i m the processed composite received vector in the jth cell becomes f k t p x f j d b f n t jn c i d b in t p i k m q n hence the th receiving unit in the jth cell receives t p t f x f j i f f b b jj ij t p q j n j i b b jn in t p the sir of terminal k becomes sir n j jn i in q n we point out that with zf precoding the ultimate limit is inde pendent of f b jj b i b ij t p j b t p j t but not of t p as t p the performance of the zf precoder converges to that of the mf precoder another popular technique is to first regularize the matrix g t t nn g t nn before inverting so that the precoder is given by g t nn g t t nn g t nn d i k where d is a parameter subject to optimization setting d results in the zf precoder while d gives the mf precoder for single cell systems d can be chosen according to for multicell mimo much less is known and we briefly elaborate on the impact of d with simulations that will be presented later we point out that the effect of t p can be removed by taking d m t p the ultimate limit can be further improved by adopting a power allocation strategy at the base stations observe that we only study noncooperative base stations in a distributed mimo system i e the processing for several base stations is carried out at a central processing unit zf could be applied across the base stations to reduce the effects of the pilot contamination this would imply an estimation of the factors kj which is feasible since they are slowly changing and are assumed to be constant over frequency b ieee signal processing magazine january numerical results we assume that each base station serves k terminals the cell diameter to a vertex is m and no terminal is allowed to get closer to the base station than m the large scale fading factor kj decomposes as b kj z kj r kj where z kj b represents the shadow fading and abides a log normal distribution i e log z kj is zero mean gaussian distributed with standard deviation v shadow with v shadow db and r kj is the distance between the base station in the jth cell and terminal in the kth cell further we assume a frequency reuse factor of figure shows cdfs of the sir as m grows without limit we plot the sir for mf precoder the zf precoder and a regularized zf precoder with d m from the figure we see that the distribution of the sir is more concentrated around its mean for zf precoding compared with mf precoding how ever the mean capacity e log sir is larger for the mf precoder than for the zf precoder around b channel use compared to b channel use with a regularized zf precoder the mean capacity and outage probability are traded against each other we next consider finite values of m in figure the sir for mf and zf precoding is plotted against m for infinite snrs t p and t f by infinite we mean that the snrs are large enough so that the performance is limited by pilot contamination the two uppermost curves show the mean sir as m as can be seen the limit is around db higher with mf precoding the two bottom curves show the mean sir for mf and zf pre coding for finite m the zf precoder decisively outperforms the mf precoder and achieves a hefty share of the asymptotic limit with around base station antenna elements per terminal to reach a given mean sir mf precoding requires at least two orders of magnitude more base station antenna elements than zf precoding does in the particular case t p t f db the sir of the mf precoder is about db worse compared with infinite t p and t f over the entire range of m showed in figure note that as m this loss will vanish zf precoder mf precoder n o i t u b i r t i d e iv t a l u m regularized zf d m u c sir db cumulative distributions on the sir for the mf precoder the zf precoder and a regularized zf precoder with d m the number of terminals served is k detection in the reverse link survey of algorithms for single cell systems similarly to in the case of mu mimo precoders simple linear detectors are close to optimal if m k under favorable propa gation conditions however operating points with m k are also important in practical systems with many users two more advanced categories of methods iterative filtering schemes and random step methods have recently been proposed for detec tion in the very large mimo regime we compare these methods with the linear methods and to tree search methods in the fol lowing the fundamentals of the schemes are explained for hard output detection experimental results are provided and soft detection is discussed at the end of the section rough com putational complexity estimates for the presented methods are given in table iterative linear filtering schemes these methods work by resolving the detection of the signaling vector q by iterative linear filtering and at each iteration by means of new propagated information from the previous esti mate of q the propagated information can be either hard i e s b d r i mf e sir m zf e sir m mf e sir zf e sir 102 104 m sirs for mf and zf precoders as a function of m the two uppermost curves are asymptotic mean values of the sir as m the bottom two curves show mean values of the sir for finite m the number of terminals served is k table rough complexity estimates for detectors in terms of floating point operations if a significant amount of the computations in question can be preprocessed for each g in slow fading the preprocessing complexity is given in the column on the right detection technique complexity for each realization of x complexity for each realization of g mmse mk mk k mmse sic m k m n iter bi gdfe mkn iter m k m n iter ts m n tabu n neigh mk n iter mk k fcsd m k r s r mk k map mk s k ieee signal processing magazine january consist of decisions on the signal vectors or soft i e contain some probabilistic measures of the transmitted symbols observe that here soft information is propagated between different iterations of the hard detector the methods typically employ matrix inversions repeatedly during the iterations which if the inversions occur frequently may be computationally heavy when m is large luckily the matrix inversion lemma can be used to remove some of the complexity stemming from matrix inversions as an example of a soft information based method we describe the conditional mmse with soft interference cancellation mmse sic scheme the algorithm is initialized with a linear mmse estimate qu of q then for each user k an interference canceled signal x i k where subscript i is the itera tion number is constructed by removing interuser interfer ence since the estimated symbols at each iteration are not perfect there will still be interference from other users in the signals x i k this interference is modeled as gaussian and the residual interference plus noise power is estimated using this estimate an mmse filter conditioned on filtered output from the previous iteration is computed for each user k the bias is removed and a soft mmse estimate of each symbol given the filtered output is propagated to the next iteration the algo rithm iterates these steps a predefined number n iter of times matrix inversions need to be computed for every realization x every user symbol q k and every iteration hence the number of matrix inversions per decoded vector is kn iter one can employ the matrix inversion lemma to reduce the number of matrix inversions to one per iteration the idea is to formulate the inversion for user k as a rank one update of a general inverse matrix at each iteration the block iterative generalized decision feedback equal izer bi gdfe algorithm is equation wise similar to mmse sic compared to mmse sic it has two differ ences the linear mmse filters of mmse sic depend on the received vector x while the bi gdfe filters which are func tions of a parameter that varies with iteration the so called input decision correlation idc do not this means that for a channel g that is fixed for many signaling vectors all filters which still vary for the different users and iterations can be precomputed further bi gdfe propagates hard instead of soft decisions random step methods the methods categorized in this section are matrix inversion free except possibly for the initialization stage where the mmse solution is usually used a basic random step method starts with the initial vector and evaluates the mse for vectors in its neighborhood with n neigh vectors the neighboring vector with smallest mse is chosen and the process restarts and con tinues like this for n iter iterations the likelihood ascent search las algorithm only permits transitions to states with lower mse and converges monotonically to a local minima in this way an upper bound of bit error rate ber and a lower bound on asymptotic multiuser efficiency for the las detector were presented in tabu search ts is superior to the las algorithm in that it permits transitions to states with larger mse values and it can in this way avoid local minima ts also keeps a list of recently traversed signaling vectors with maximum number of entries n tabu that are temporarily forbidden moves as a means for moving away to new areas of the search space this strategy gave rise to the algorithm name tree based algorithms the most prominent algorithm within this class is the sphere decoder sd the sd is in fact an ml decoder but which only considers points inside a sphere with certain radius if the sphere is too small for finding any signaling points it has to be increased many tree based low complexity algorithms try to reduce the search by only expanding the fraction of the tree nodes that appear the most promising one such method is the stack decoder where the nodes of the tree are expanded in the order of least euclidean distance to the received signal the average complexity of the sphere decoder is however expo nential in k and sd is thus not suitable in the large mimo regime where k is large the fixed complexity sphere decoder fcsd is a low complexity suboptimal version of the sd all combinations of the first say r scalar symbols in q are enumerated i e with a full search and for each such combination the remaining k r symbols are detected by means of zf df this implies that the fcsd is highly parallelizable since s r hardware chains can be used and further it has a constant complexity a sorting algorithm employing the matrix inversion lemma for finding which symbols should be processed with full complex ity and which ones should be detected with zf df can be found in the fcsd eliminates columns from the matrix g which implies that the matrix gets better conditioned which in turn boosts the performance of linear detectors for m k the channel matrix is however already well conditioned so the sit uation does not improve much by eliminating a few columns therefore the fcsd should mainly be used in the case of m k numerical comparisons of the algorithms we now compare the detection algorithms described above experimentally quadrature phase shift keying qpsk is used in all simulations and rayleigh fading is assumed i e the channel matrix is chosen to have independent components that are dis tributed as cn the transmit power is denoted t in all experiments simulations are run until symbol errors are counted we also add an if genie solution that enjoys the same ieee signal processing magazine january receive signaling power as the other methods without mul tiuser interference as mentioned earlier when there is a large excess of base station antennas simple linear detection performs well it is natural to ask for the number a m k when this effect kicks in to give a feel for this we show the uncoded ber perfor mance versus a for the particular case of k in figure for the measurements in figure we let t m mmse sic uses n iter bi gdfe uses n iter since further iterations gave no improvement and the idc parameter was chosen from preliminary simulations the ts neighborhood is defined as the closest modulation points and ts uses n iter n tabu for fcsd we choose r we observe that when the ratio a is above five or so the simple linear mmse method performs well while there is room for improvements by more advanced detec tors when a mmse mmse sic bi gdfe ts r e b fcsd if a comparisons of ber for k and varying values of a mmse fcsd b comparisons of ber of the studied detectors as functions of their complexities given in table we consider the case without possibility of preprocessing i e the column entries in table are summed for each scheme the number of antennas m k and transmit signaling power t db mmse sic bi gdfe ts r e 108 number of floating point operations r e b mmse mmse sic bi gdfe ts fcsd if t db comparisons of the of the studied detectors for different transmit signaling power t the scheme parameters are the maximum values in figure and the number of antennas is m k since we saw in figure that there is a wide range of a where mmse is largely suboptimal we now consider the case m k figure shows comparisons of uncoded ber of the studied detectors as functions of their complexities given in table we consider the case without possibility of preprocessing i e the column entries in table are summed for each scheme m k and we use t db we find that ts and mmse sic perform best for example at a ber of 002 the ts is very large mimo has the potential to bring radical changes to the field times less complex than the fcsd figure shows a plot of ber versus transmit signaling power t for m k when the scheme parameters are the maximum values in the experiment in figure it is seen that ts and mmse sic perform best across the entire snr range presented note that the ml detector with a search space of size cannot outperform the if genie aided benchmark hence remarkably we can conclude that ts and mmse sic are operating not more than db away from the ml detector for mimo soft input soft output detection the hard detection schemes above are easily evolved to soft detection methods one should not in general draw conclu sions about soft detection from hard detection literature investigating schemes similar to the ones above but operating in the coded large system limit are in agreement with figures in analytic code division multiple access cdma spectral efficiency expressions for both mf zf and linear mmse are given the results are the following in the limit of large ratios a all three methods perform likewise and as well as the optimum joint detector and cdma with orthogonal ieee signal processing magazine january spreading codes for a mf starts to perform much worse than the other methods at a zf performs drastically worse than mmse but the mmse method loses significantly in performance compared to joint processing with mmse sic a priori information is easily incorporated in the mmse filter derivation by conditioning this requires the computation of the filters for each user each symbol inter val and each decoder iteration another mmse filter is derived by unconditional incorporation of the a priori proba bilities which results in mmse filters varying for each user and iteration similar to the bi gdfe above density evolution analysis of conditional and unconditional mmse sic in a cdma setting and in the limit of infinite n and k shows that their coded ber waterfall region can occur within one db from that of the map detector in terms of spectral effi ciency the map detector and conditional and unconditional mmse sic perform likewise for random step and tree based methods the main prob lem is to obtain a good list of candidate q vectors for approxi mate log likelihood ratio llr evaluation where all bits should take the values zero and one at least once with the ts and fcsd methods we start from lists containing the hard detection results and the vectors searched to achieve this result for creating an approximate max log llr if a bit value for a bit position is missing or if higher accuracy is needed one can add vectors in the vicinity of the obtained set see a soft output version of the las algorithm has been shown to operate around db away from capacity in a coded v blast set ting with m k instead of using the max log approximations for approximat ing llr as in the pm algo rithm keeps a sum of terms markov chain monte carlo techniques may also be suitable for large scale soft output soft output mimo detection summary very large mimo offers the unique prospect within wireless communication of saving an order of magnitude or more in transmit power as an extra bonus the effect of small scale fad ing averages out so that only the much more slowly changing large scale fading remains hence very large mimo has the potential to bring radical changes to the field as the number of base station antennas grows the system gets almost entirely limited from the reuse of pilots in neigh boring cells the so called pilot contamination concept this effect appears to be a fundamental challenge of very large mimo system design which warrants future research on the topic we have also seen that the interaction between antenna ele ments can incur significant losses both to channel orthogonal ity and link capacity for large mimo systems this is especially problematic since with a fixed overall aperture the antenna spacing must be reduced moreover the severity of the coupling problem also depends on the chosen array geometry e g linear finding the sparsest solution to underdetermined systems of linear equations is np hard in general we show here that for systems with typical random a good ap proximation to the sparsest solution is obtained by applying a fixed number of standard operations from linear algebra our proposal stagewise orthogonal matching pursuit stomp successively transforms the signal into a negligible residual starting with ini tial residual at the th stage it forms the matched filter identifies all coordinates with amplitudes exceeding a specially chosen threshold solves a least squares problem using the selected coordinates and subtracts the least squares fit pro ducing a new residual after a fixed number of stages e g it stops in contrast to orthogonal matching pursuit omp many coefficients can enter the model at each stage in stomp while only one enters per stage in omp and stomp takes a fixed number of stages e g while omp can take many e g we give both theoretical and empirical support for the large system effectiveness of stomp we give numerical examples showing that stomp rapidly and reliably finds sparse solutions in compressed sensing decoding of error correcting codes and overcomplete representation index terms compressed sensing decoding error correcting codes false alarm rate false discovery rate iterative thresholding minimization mutual access interference phase transition sparse overcomplete representation stepwise regression t he is attracting possibility growing of i exploiting introduction attention sparsity over in signal the processing years sev eral applications have been found where signals of interest have sparse representations and exploiting this sparsity offers striking benefits see for example manuscript received april revised august accepted august date of current version february this work was supported by grants from nih onr muri a darpa baa and nsf dms dms frg and dms 05303 the material in this paper was presented in part at the allerton conference on communcation control and computing sept monticello il usa d l donoho is with the department of statistics stanford university stan ford ca usa e mail donoho stanford edu y tsaig was with the institute for computational mathematics in engineering stanford university stanford ca usa e mail ytsaig gmail com i drori was with the department of statistics stanford university stanford ca usa e mail iddo drori gmail com j l starck is with the service d astrophysique dapnia sedi sap centre europeen d astronomie saclay f gif sur yvette cedex france e mail jstarck cea fr communicated by a host madsen associate editor for detection and es timation color versions of one or more of the figures in this paper are available online at http ieeexplore ieee org digital object identifier tit 2173241 at the icassp conference a special session addressed the theme of exploiting sparsity and a recent international workshop was largely devoted to this topic very recently considerable attention has focused on the fol lowing sparse solutions problem ssp we are given an matrix which is in some sense random for example a matrix with i i d gaussian entries we are also given an vector and we know that where is an unknown sparse vector we wish to recover however crucially the system of equations is underdetermined and so of course this is not a properly stated problem in linear algebra nevertheless spar sity of is a powerful property that sometimes allows unique solutions applications areas for which this model is relevant include the following compressed sensing represents the coefficients of a signal or image in a known basis which happens to sparsely represent that signal or image encodes a mea surement operator i e an operator yielding linear combi nations of the underlying object here means that we collect fewer data than unknowns despite the indeter minacy sparsity of allows for accurate reconstruction of the object from what would naively seem to be too few samples error correction information is transmitted in a coded block in which a small fraction of the entries may be corrupted from the received data one constructs a system here represents the values of errors which must be identifed corrected represents generalized check sums and represents a generalized checksum operator it is assumed that the number of errors is smaller than a threshold and so is sparse this sparsity allows to perfectly correct the gross errors sparse overcomplete representation represents the synthesis coefficients of a signal which is assumed to be sparsely represented from terms in an overcomplete expansion those terms are the columns of the spar sity allows to recover a unique representation using only a few terms despite the fact that representation is in general nonunique in these applications several algorithms are available to pursue sparse solutions in some cases attractive theoretical results are known guaranteeing that the solutions found are the sparsest possible solutions for example consider the algorithm of minimization which finds the solution to having min imal norm also called basis pursuit bp this method enjoys some particularly striking theoretical properties such as rigorous proofs of exact reconstruction under seemingly quite general circumstances ieee donoho et al sparse solution of underdetermined systems of linear equations unfortunately some of the most powerful theoretical results are associated with fairly heavy computationally burdens the research reported here began when in applying the theory of compressed sensing to nmr spectroscopy we found that a straightforward application of the minimization ideas in required several days solution time per multidimensional spectrum this seemed prohibitive computational expense to us even though computer time is relatively less precious than spectrometer time in fact numerous researchers have claimed that general pur pose minimization is much too slow for large scale appli cations some have advocated a heuristic approach orthog onal matching pursuit omp also called greedy approxima tion and stepwise regression in other fields 52 which though often effective in empirical work does not offer the strong theoretical guarantees that attach to minimization for other heuristic approaches see and in this paper we describe stagewise orthogonal matching pursuit stomp a method for approximate sparse solution of underdetermined systems with the property either that is random or that the nonzeros in are randomly located or both stomp is significantly faster than the earlier methods bp and omp on large scale problems with sparse solutions moreover stomp permits a theoretical analysis showing that stomp is similarly succcessful to bp at finding sparse solutions our analysis uses the notion of comparison of phase transi tions as a performance metric we consider the phase diagram a d graphic with coordinates measuring the relative sparsity of number of nonzeros in number of rows in as well as the indeterminacy of the system number of rows in number of columns in stomp large performance ex hibits two phases success failure in this diagram as does the performance of bp the success phase the region in the phase diagram where stomp successfully finds sparse solutions is large and comparable in size to the success phase for mini mization in a sense stomp is more effective at finding sparse solutions to large extremely underdetermined problems than ei ther minimization or omp its phase transition boundary is even higher at extreme sparsity than the other methods more over stomp takes a few seconds to solve problems that may require days for solution as a result stomp is well suited to large scale applications indeed we give several explicitly worked out examples of realistic size illustrating the perfor mance benefits of this approach our analysis suggests the slogan noiseless underdetermined problems behave like noisy well determined problems i e coping with incompleteness of the measurement data is for random similar to coping with gaussian noise in complete measurements at each stomp stage the usual set of matched filter coefficients is a mixture of noise caused by cross talk non orthogonality and true signal setting an appropriate threshold we can subtract identified signal causing a reduction in cross talk at the next iteration this is more than a slogan we develop a theoretical framework for rigorous asymptotic analysis theorems below allow us to track explicitly the successful capture of signal and the reduction in cross talk stage by stage rigorously establishing asymptotic success below phase transition together with the failure that occurs above phase transition the theory agrees with empirical finite results our paper is organized as follows section ii presents background on the sparse solutions problem section iii in troduces the stomp algorithm and documents its favorable performance section iv develops a performance measurement approach based on the phase diagram and phase transition section v analyzes the computational complexity of the al gorithm section vi develops an analytic large system limit for predicting phase transitions which agree with empirical performance characteristics of stomp section vii develops the gaussian noise viewpoint which justifies our thresholding rules section viii describes the performance of stomp under variations adding noise of distribution of nonzero coefficients of matrix ensemble and documents the good performance of stomp under all these variations section ix presents computational examples in applications app3 that document the success of the method in simu lated model problems section x describes the available soft ware package which reproduces the results in this paper and section xi discusses the relationship of our results to related ideas in multiuser detection theory and to previous work in the sparse solutions problem ii sparse solution preliminaries recall the sparse solutions problem ssp mentioned in the introduction in the ssp an unknown vector is of interest it is assumed sparse which is to say that the number of nonzeros is much smaller than we have the linear mea surements where is a known by matrix and wish to recover of course if were a nonsingular square matrix with we could easily recover from but we are interested in the case where elementary linear algebra tells us that is then not uniquely recoverable from by linear algebraic means as the equation may have many solutions however we are seeking a sparse solution and for certain matrices sparsity will prove a powerful constraint some of the rapidly accumulating literature documenting this phenomenon includes for now we consider a specific collection of matrices where sparsity proves valuable until we say otherwise let be a random matrix taken from the uniform spherical ensemble use the columns of are i i d points on the unit sphere later several other ensembles will be introduced iii stagewise orthogonal matching pursuit stomp aims to achieve an approximate solution to where comes from the use and is sparse in this section we describe its basic ingredients in later sections we document and analyze its performance a procedure stomp operates in stages building up a sequence of ap proximations by removing detected structure from a ieee transactions on information theory vol no february fig schematic representation of the stomp algorithm sequence of residual vectors fig gives a diagram matic representation stomp starts with initial solution and initial residual the stage counter starts at the algorithm also maintains a sequence of estimates of the locations of the nonzeros in the th stage applies matched filtering to the current residual getting a vector of residual correlations which we think of as containing a small number of significant nonzeros in a vector disturbed by gaussian noise in each entry the procedure next performs hard thresholding to find the sig nificant nonzeros the thresholds are specially chosen based on the assumption of gaussianity see below thresholding yields a small set of large coordinates here is a formal noise level and is a threshold parameter we merge the subset of newly selected coordinates with the pre vious support estimate thereby updating the estimate we then project the vector on the columns of belonging to the enlarged support letting denote the matrix with columns chosen using index set we have the new approxima tion supported in with coefficients given by the updated residual is we check a stopping condition and if it is not yet time to stop we set and go to the next stage of the procedure if it is time to stop we set as the final output of the procedure remarks the procedure resembles orthogonal matching pursuit known to statisticians as forward stepwise regression in fact the two would give identical results if were equal to and if by coincidence the threshold were set in such a way that a single new term were obtained in at each stage the thresholding strategy used in stomp to be described below aims to have numerous terms enter at each stage and aims to have a fixed number of stages hence the re sults will be different from omp the formal noise level and typically the threshold parameter takes values in the range there are strong connections to stagewise stepwise re gression in statistical model building successive interfer ence cancellation multiuser detectors in digital communi cations and iterative decoders in error control coding see the discussion in section xi our recommended choice of and our recommended threshold setting procedures see section iii d below have been designed so that when is sufficiently sparse the fol lowing two conditions are likely to hold upon termination all nonzeros in are selected in has no more than entries the next lemma motivates this design criterion recall that is sampled from the use and so columns of are in general position the following is proved in appendix a lemma let the columns of be in general position let denote the support of suppose that the support of is a subset of suppose in addition that then we have perfect recovery iii b example we give a simple example showing that the procedure works in a special case we generated a coefficient vector with nonzeros having amplitudes uniformly distributed on we sampled a matrix at random from the use with and computed a linear measurement vector thus the problem of recovering given is underdetermined i e with underlying sparsity measure to this ssp we applied stomp coupled with the cfar threshold selection rule to be discussed below the results are illustrated in fig panels a i depict each matched filtering output its hard thresholding and the evolving approximation as can be seen after stages a result is obtained which is quite sparse and as the final panel shows matches well the object which truly generated the data in fact the error at the end of the third stage measures i e a mere three stages were required to achieve an accuracy of two decimal digits donoho et al sparse solution of underdetermined systems of linear equations fig progression of the stomp algorithm panels a d g successive matched filtering outputs panels b e h successive thresholding results panels c f i successive partial solutions in this example a matched filtering b hard thresholding c approximate solution d matched filtering e hard thresholding f approximate solution g matched filtering h hard thresholding i approximate solution c approximate gaussianity of residual mai at the heart of our procedure are two thresholding schemes often used in gaussian noise removal n b at this point we assume there is no noise in to explain the relevance of gaussian noise concepts note that at stage the algorithm is computing tthis is essentially the usual matchedfilter estimate of if and vanishes except in one coordinate the matchedfilter output equals perfectly in that one coordinate hence is a measure of the disturbance to exact reconstruc tion caused by multiple nonzeros in the same notion arises in digital communications where it is called multiple access in terference mai perhaps surprisingly because there is no noise in the problem the mai in our setting typically has a gaussian behavior more specifically if is a matrix from the use and if and are both large then the entries in the mai vector have a histogram which is nearly gaussian with stan dard deviation iii the heuristic justification is as follows the mai has the form the thing we regard as random in this expression is the ma trix the term measures the projection of a random point on the sphere onto another random point this random variable has approximately a gaussian distribution for from the use for a given fixed the different random variables are independently distributed hence the quantity is an i i d sum of approximately normal r v and so by standard arguments should be approximately normal with mean and variance setting this justifies iii computational experiments validate gaussian approximation for the mai in fig panels a d g display gaussian qq plots of the mai in the sparse case with and in the uniform spherical ensemble with and in each case the qq plot appears straight as the gaussian model would demand through the rest of this paper the phrase gaussian approx imation means that the mai has an approximately gaussian marginal distribution the reader interested in formal proofs of gaussian approximation can consult the literature of multiuser that in these experiments and later examples also we always have a significant number of nonzeros in but a small fraction this allows the central limit theorem to work a referee has pointed out that if the were of highly disparate amplitudes say each coefficient was about as big as the sum of all the smaller ones this phenomenon might be absent ieee transactions on information theory vol no february fig qq plots comparing mai with gaussian distribution left column middle column right column top row use middle row rse bottom row urp the rse and urp ensembles are discussed in section viii the plots all appear nearly linear indicating that the mai has a nearly gaussian distribution a use b rse c urp d use e rse f urp g use h rse i urp detection e g and such a proof is implicit in the proofs of theorems and below the connection between our work and mud theory will be amplified in section xi below properly speaking the term mai applies only at stage of stomp at later stages there is residual mai i e mai which has not yet been cancelled this can be defined as the coordinates are ignored at stage the residual in those coordinates is deterministically empirically residual mai has also a gaussian behavior fig shows quantile quantile plots for the first few stages of the cfar variant comparing the residual mai with a standard normal distribution the plots are effectively straight lines illustrating the gaussian approximation later we provide theoretical support for a perturbed gaussian approximation to residual mai d threshold selection our threshold selection proposal is inspired by the gaussian behavior of residual mai we view the vector of correlations at stage as consisting of a small number of truly nonzero entries combined with a large number of gaussian noise en tries the problem of separating signal from noise in such problems has generated a large literature including the papers which influenced our way of thinking we adopt language from statistical decision theory and the field of multiple comparisons 38 recall that the support of is being crudely estimated in the stomp algorithm if a coordinate belonging to does not appear in we call this a missed detection if a coordinate not in does appear in we call this a false alarm the coordinates in we call dis coveries and the coordinates in we call false discoveries note false alarms are also false discoveries the terminolog ical distinction is relevant when we normalize to form a rate thus the false alarm rate is the number of false alarms divided by the number of coordinates not in the false discovery rate is the fraction of false discoveries within we propose two strategies for setting the threshold ulti mately each strategy should land us in a position to apply lemma i e to arrive at a state where and there are no missed detections then lemma assures us we perfectly recover the two strategies are false alarm control we attempt to guarantee that the number of total false alarms across all stages does not exceed the natural codimension of the problem defined as subject to this we attempt to make the max imal number of discoveries possible to do so we choose a threshold so the false alarm rate at each stage does not exceed a per stage budget donoho et al sparse solution of underdetermined systems of linear equations fig qq plots comparing residual mai with gaussian distribution quantiles of residual mai at different stages of stomp are plotted against gaussian quan tiles near linearity indicates approximate gaussianity a stage no b stage no c stage no d stage no e stage no f stage no false discovery control we attempt to arrange that the number of false discoveries cannot exceed a fixed fraction of all discoveries and to make the maximum number of discoveries possible subject to that constraint this leads us to consider simes rule the false alarm control strategy requires knowledge of the number of nonzeros or some upper bound false discovery control does not require such knowledge which makes it more convenient for applications if slightly more complex to imple ment and substantially more complex to analyze the choice of strategy matters the basic stomp algorithm behaves differ ently depending on the threshold strategy as we will see below implementation details are available by downloading the software we have used to generate the results in this paper see section x below iv performance analysis by phase transition when does stomp work to discuss this we use the notions of phase diagram and phase transition a problem suites performance measures by problem suite we mean a collection of sparse solution problems defined by two ingredients a an ensemble of random matrices of size by b an ensemble of sparse vectors by standard problem suite we mean the suite with sampled from the uniform spherical ensemble with a random variable having nonzeros sam pled i i d from a standard distribution for a given problem suite a specific algorithm can be run numerous times on instances sampled from the problem suite its performance on each realization can then be measured according to some numerical or qualitative criterion if we are really ambitious and insist on perfect recovery we use the performance measure more quantitative is the norm the number of sites at which the two vectors disagree both these measures are inappropriate for use with floating point arithmetic which does not produce exact agreement we prefer to use instead the number of sites at which the reconstruction and the target disagree by more than we can also use the quantitative measure declaring success when the measure is smaller than a fixed threshold say for a qualitative performance indicator we simply report the fraction of realizations where the qualitative condition was true for a quantitative performance measure we present the mean value across instances at a given b phase diagram a phase diagram depicts performance of an algorithm at a sequence of problem suites the average value of some performance measure as displayed as a function of and both of these variables so the diagram occupies the unit square to illustrate such a phase diagram consider a well studied case where something interesting happens let solve the op timization problem as mentioned earlier if where has nonzeros we may find that exactly when is small enough fig displays a grid of values with ranging through equispacedpointsintheinterval and rangingthrough equispaced points in here each point on the grid shows the mean number of coordinates at which original and reconstruction differ by more than averaged over independent realizations of the standard problem suite the experimental setting just described i e the grid setup the values of and the number of realizations ieee transactions on information theory vol no february fig phase diagram for minimization shaded attribute is the number of coordinates of reconstruction which differ from optimally sparse solution by more than the diagram displays a rapid transition from perfect recon struction to perfect disagreement overlaid red curve is theoretical curve is used to generate phase diagrams later in this paper although the problem suite being used may change this diagram displays a phase transition for small it seems that high accuracy reconstruction is obtained while for large reconstruction fails the transition from success to failure occurs at different for different values of this empirical observation is explained by a theory that ac curately predicts the location of the observed phase transition and shows that asymptotically for large this transition is per fectly sharp suppose that problem is drawn at random from the standard problem suite and consider the event that i e that minimization exactly recovers the paper defines a function called there with the following property consider sequences of obeying and suppose that then as on the other hand suppose that then as the theoretical curve described there is overlaid on fig showing good agreement between asymptotic theory and experimental results c phase diagrams for stomp we now use phase diagrams to study the behavior of stomp fig displays performance of stomp with cfar thresholding with per iteration false alarm rate the problem suite and underlying problem size are the same as in fig the shaded attribute again portrays the number of entries where the reconstruction misses by more than once again for very sparse problems small the algorithm is successful at recovering a good approximation to while for less sparse problems large the algorithm fails superposed on this display is the graph of a heuristically fig phase diagram for cfar thresholding overlaid red curve is heuris tically derived analytical curve see appendix b shaded attribute number of coordinates wrong by more than relative error underlying sparse sequences have nonzeros of equal amplitude with see section vi fig phase diagram for cfdr thresholding overlaid red curve is heuristi cally derived curve see appendix b shaded attribute number of co ordinates wrong by more than relative error underlying sparse sequences have nonzeros of equal amplitude with see section vi derived function which we call the predicted phase transition for cfar thresholding again the agreement be tween the simulation results and the predicted transition is reasonably good appendix b explains the calculation of this predicted transition although it is best read only after first reading section vi fig shows the number of mismatches for the stomp algo rithm based on cfdr thresholding with false discovery rate here and the display shows that again for very sparse problems small the algorithm is successful at recovering a good approximation to while for less sparse problems large the algorithm fails superposed on this display is the graph of a heuristically derived function which we call the predicted phase transition for cfdr thresholding donoho et al sparse solution of underdetermined systems of linear equations table i c omparison of e xecution t imes in s econds for i nstances of the r andom p roblem s uite again the agreement between the simulation results and the pre dicted transition is reasonably good though visibly not quite as good as in the cfar case v computation since stomp seems to work reasonably well it makes sense to study how rapidly it runs a empirical results table i shows the running times for stomp equipped with cfar and cfdr thresholding solving an instance of the problem suite we compare these figures with the time needed to solve the same problem instance via minimization and omp here minimization is implemented using michael saunders pdco solver the simulations used to generate the figures in the table were all executed on a ghz xeon workstation comparable with current desktop cpus table i suggests that a tremendous saving in computation time is achieved when using the stomp scheme over traditional minimization the conclusion is that cfar and cfdr based methods have a large domain of applicability for sparsely solving random underdetermined systems while running much faster than other methods in problem sizes of current interest b complexity analysis the timing studies are supported by a formal analysis of the asymptotic complexity in this analysis we consider two scenarios dense matrices in this scenario the matrix defining an underdetermined linear system is an explicit dense matrix stored in memory thus applying to an vector involves flops fast operators here the linear operator is not ex plicitly represented in matrix form rather it is imple mented as an operator taking a vector and returning classical examples of this type include the fourier trans form hadamard transform and wavelet transform just to name a few all of these operators are usually implemented without matrix multiplication such fast operators are of key importance in large scale applications as a concrete example consider an imaging scenario where the data is a array and is an by partial fourier operator with proportional to direct application of would involve operations whereas applying a d fft followed by random sampling would require merely flops the computational gain is ev ident in our analysis below we let denote the cost of one application of a linear operator or its adjoint corre sponding to one matrix vector multiplication in fact as we will now see the structure of the stomp al gorithm makes it a prime choice when fast operators are avail able as nearly all its computational effort is invested in solving partial least squares systems involving and in detail as sume we are at the th stage of execution stomp starts by ap plying matched filtering to the current residual which amount to one application of at a cost of flops next it applies hard thresholding to the residual correlations and updates the active set accordingly using at most additional flops the core of the computation lies in calculating the projection of onto the subset of columns to get a new approximation this is implemented via a conjugate gradient cg solver each cg iteration involves application of and costing at most flops the number of cg iterations used is a small constant independent of and which we denote in our implementation we use finally we compute the new residual by applying to the new approximation requiring an additional flops summarizing the total operation count per stomp stage amounts to the total number of stomp stages is a prescribed constant indepen dent of the data in the simulations in this paper we set readers familiar with omp have by now doubtless recog nized the evident parallelism in the algorithmic structure of stomp and omp indeed much like stomp at each stage omp computes residual correlations and solves a least squares problem for the new solution estimate yet unlike stomp omp builds up the active set one element at a time hence an effi cient implementation would necessarily maintain a cholesky factorization of the active set matrix and update it at each stage thereby reducing the cost of solving the least squares system in total steps of omp would take at most flops without any sparsity assumptions on the data omp takes at most steps thus its worst case performance is bounded by operations a key difference between stomp and omp is that the latter needs to store the cholesky factor of the active set matrix in its explicit form taking up to memory elements when is large as is often the case in d and d image reconstruction scenarios this greatly hinders the applicability of omp in contrast stomp has very modest storage requirements at any given point of the algorithm execution one needs only store the current estimate the current residual vector and the current active set this makes stomp very attractive for use in large scale applications table ii summarizes our discussion so far offering a compar ison of the computational complexity of stomp omp and minimization via linear programming lp for the lp solver we use a primal dual barrier method for convex optimization d enotes the m aximum n umber of cg i terations e mployed per s tage of s t omp and s tands for the c ost of o ne m atrix v ector p roduct i mplemented as a f ast o perator table ii w orst c ase c omplexity b ounds for s t omp omp and m inimization pdco d enotes the m aximum n umber of s tages ieee transactions on information theory vol no february table iii c omparison of e xecution t imes in s econds in the r andom p artial f ourier s uite b ecause of the f ast o perator s t omp o utperforms omp pdco developed by michael saunders the estimates listed in the table all assume worst case behavior examining the bounds in the dense matrix case closely we notice that stomp is the only algorithm of the three admitting quadratic order com plexity estimates in contrast omp and pdco require cubic order estimates for their worst case performance bound there fore for large scale problems stomp can dominate due to its simple structure and efficiency in the case where fast operators are applicable stomp yet again prevails it is the only algo rithm of the three requiring a constant number of matrix vector multiplications to reach a solution to convey the scale of computational benefits in large prob lems we conduct a simple experiment in a setting where can be implemented as a fast operator we consider a system where is made from only rows of the fourier matrix can be implemented by application of a fast fourier transform followed by a coordinate selection table iii gives the results clearly the advantage of stomp is even more con vincing to make the comparison still more vivid we point ahead to an imaging example from section ix a below there an image of dimensions is viewed as a vector of length again the system where is made from only rows of the fourier matrix one matrix vector product costs how do the three algorithms compare in this setting plug ging in and as above we see that the leading term in the complexity bound for stomp is in contrast for omp the leading term in the worst case bound be comes and for minimization the leading term is the computational gains from stomp are in deed substantial moreover to run omp in this setting we may need up to memory elements to store the cholesky factor ization which renders it unusable for anything but the smallest in section ix a we present actual running times of the dif ferent algorithms vi large system limit figs and suggest phase transitions in the behavior of stomp which would imply a certain well defined asymptotic system capacity below which stomp successfully finds a sparse solution and above which it fails in this section we review the empirical evidence for a phase transition in the large system limit and develop theory that rigorously estab lishes it we consider the problem suite defined by random sampled from the use and with generated as where has nonzero coefficients in random positions having entries this ensemble generates a slightly lower transition than the ensemble used for figs and where the nonzeros in had i i d gaussian entries fig empirical transition behaviors varying a fraction of cases with termination before stage b fraction of missed detections averages of trials with and and varying sharpness of each transition seems to be increasing with a evidence for phase transition fig presents results of simulations at fixed ratios but increasing three different quantities are considered in panel a the probability of early termination i e termination before stage because the residual has been driven nearly to zero in panel b the missed detection rate i e the fraction of nonzeros in that are not supported in the reconstruction both quantities undergo transitions in behavior near significantly the transitions become more sharply defined with increasing as increases the early termination probability behaves increasingly like a raw discontinuity as while the fraction of missed detections properties behave increasingly like a discontinuity in derivative in statistical physics such limiting behaviors are called first order and second order phase transitions respectively donoho et al sparse solution of underdetermined systems of linear equations table iv d imension n ormalized r atios p roblems of d ifferent s izes w ith i dentical r atios of and n ote s imilarity of e ntries in a djacent r ows w ithin e ach h alf of the t able t op h alf of t able j ust b elow p hase t ransition b ottom h alf j ust a bove p hase t ransition b evidence for intensivity in statistical physics a system property is called intensive when it tends to a stable limit as the system size increases many properties of stomp when expressed as ratios to the total system size are intensive such properties include the number of detections at each stage the number of true detections the number of false alarms and the squared norm of the residual when sampling from the standard problem suite all these properties after normalization by the problem size behave as intensive quantities table iv illustrates this by considering six different com binations of all six at the same value of determinacy in two groups of three each group at one common value of within each group with common values of and we considered three different problem sizes stage of stomp considers an dimensional subspace using nonzeros out of possible terms where true discoveries prior to stage discoveries prior to stage discoveries prior to stage the table presents dimension normalized ratios if these quantities are intensive they will behave similarly at the same stage even at different problem sizes the evidence of the table suggests that they are indeed intensive also important in what follows are two threshold detector operating characteristics the stage specific false alarm rate and the stage specific correct detection rate table v d imension n ormalized d etector o perating c haracteristics p roblems of i dentical r atios of and n ote s imilarity of e ntries in a djacent r ows w ithin e ach h alf of the t able t op h alf of t able j ust b elow p hase t ransition b ottom h alf j ust a bove p hase t ransition there is also evidence of intensivity for these quantities see table v c limit quantities we have seen that the dimension normalized quantities and are empirically nearly constant for large we now present a theoretical result to explain this for our result we fix and consider the cfar algorithm designed for that specified we also fix let run stomp on an instance of the problem suite let denote the norm of the residual at stage recall the notation for limit in probability a sequence of random variables has the nonstochastic limit in probability written if for each as in the result below let denote the random quantity on a problem from the standard suite at size similarly for also if the iteration stops immediately and the monitoring variables at that and all later stages up to stage are assigned values in the obvious way etc theorem large system limit for there are constants depending on and on so that we also have large system limits in probability for the detector operating characteristics where the limits depend on and finally the normalized dimensions also have large system limits d ifferent s izes w ith ieee transactions on information theory vol no february table vi e mpirical and t heoretical p hase t ransitions c omparisons at s everal v alues of i ndeterminacy top half of t able e mpirical c alculation b ottom h alf t heoretical c alculation with limits depending on and on see appendix c for the proof it is best studied after first becoming familiar with section vii d the predicted phase transition fix a small we say that stomp is successful if at termination of the stage stomp algorithm theactiveset containsallbutafraction oftheelements of the active set contains no more than elements lemma motivates this definition in the case when this property holds it is typically the case that as experiments have shown the existence of large system limits allows us to derive phase transitions in the success property the corresponding curves and decorate figs and empirically these transitions happen at about the same place as apparent transi tions for other candidate definitions of success such as exact equality the key point is that the transitions in this property can be calculated analytically and are rigorously in force at large whereas empirical phase transitions are simply interpretations this analytic calculation works by tracking the large system limit variables as a function of thus we use dimension normalized units and this state vector is initialized to the heart of the calculation is an iteration over at stage we first calculate the model false alarm rate and the model true detect rate vi vi this part of the calculation requires theoretical developments from the next section specifically corollaries we then update the limit quantities in the obvious way the calculation announces success if at or before stage otherwise it announces failure this calculation evaluates a specific parameter combination for success or failure we are really interested in the boundary which separates the success region from the failure region by binary search we obtain a numer ical approximation to this boundary in this calculation there is no notion of problem size in principle the calculation is applicable to all large problem sizes the assumption being made is that certain variables such as the empirical false alarm rate are intensive and though random can be approximated by a limit quantity this has been estab lished for the relevant variables by theorem table vi compares the calculations made by this approach with the results of a stomp simulation the degree of match is apparent the difference between the empirical transition and the theoretical prediction is smaller than the width of the tran sition compare fig since the empirical transition point is not a sharply defined quantity the degree of match seems quite acceptable vii conditioned gaussian limit underlying theorem and the subsequent phase transition calculations is a particular model for the statistical behavior of coefficients we now introduce and derive that model a conditioned gaussian model our model considers the quantities driving the stomp algorithm there are two kind of behaviors one for the null case and one for the non null case null case define jointly gaussian random variables with means zero and variances defined by theorem the variances are decreasing the random variables have the covariance structure that is to say the process behaves as a time reversed martingale consider the coefficient obtained by matched fil tering of the th residual and suppose that is a truly null coordinate i e is not in the support of for a random variable let denote the probability law of consider the use problem suite with given values of and and large our conditioned gaussian model says that in the cfar case in words we model each null coefficient as a certain gaussian random variable conditioned on certain non exceedance events involving other correlated random variables in particular we do not model it simply as a gaussian random variable except if to enforce this distinction we let denote the random variable conditioned on non null case define jointly gaussian random variables with means and variances again deriving donoho et al sparse solution of underdetermined systems of linear equations from theorem there is again the covariance appropriate to a time reversed martingale consider now the coefficient obtained by matched filtering of the th residual where is a truly non null coordi nate i e is in the support of consider again the standard problemsuitewithgivenvaluesof and and large thecon ditioned gaussian model says that in words we model each non null coefficient at stage as a cer tain nonzero mean gaussian random variable conditioned on a certain sequence of non exceedances at earlier stages in the se quence in this case the conditioning event looks the same as in the null case however the random variables do not have mean zero we let denote the random variable condi tioned on the gaussian approximation the cg model which will later be seen to be highly accurate explains why the gaussian approximation sometimes works the model has the following consequence let denote the marginal probability den sity of the cg variable and let denote the probability density of a gaussian under a simple normal approx mation we would have under our model we have the identity where a parallel definition for the random variables sets in fig panel a we display exact results for under our model with a choice of obtained from analyzing the case as one can see each is effectively near zero and drops to zero in the tails in this case each underlying is small and each is effectively concentrated over the region where is nearly hence the gaussian approximation to the conditioned gaussian model is not bad for the parameters and underlying this situation panel b depicts with a choice of obtained from analyzing the case now we have only a vaguely gaussian shape b derivation of the model the first part of this section will prove theorem let be as defined in section vii ai then for and as we immediately gain a formula for computing the limiting threshold false alarm probability fig density correction factors a the factor multiplying the stan dard normal density to get the conditioned gaussian density null case b the factor multiplying the nonstandard normal density to get the conditioned gaussian density nonnull case here and corollary vii the comparable result in the non null case is theorem let be as defined in section vii ai then we obtain a formula for computing the limiting threshold cor rect detection rate corollary vii ieee transactions on information theory vol no february the formulas vii and vii explain how to perform in principle the calculations vi vi needed for calculating phase transitions in section vi d for complete documentation of the calculation procedure see section x null case suppose we are given a deterministic vector and a sequence of deterministic nested orthoprojectors where and we are given a random vector gaussian distributed with mean zero and diagonal covariance matrix define lemma define random variables then is a jointly gaussian se quence with mean zero variances and covari ances proof the sequence of vector valued random variables is a reversed martingale indeed is stochastically independent of because is isotropic gaussian and is a nested sequence of orthoprojectors but is then a gaussian reversed martingale and the variance computation is then inevitable now introduce the random variable vii of course is a random point on in fact uniformly distributed lemma define random variables for fixed the sequence is asymptotically well approximated by a joint gaussian se quence with variances and covariances more precisely for a sequence depending on only proof of course the gaussian sequence we have in mind is just introduced above linked to by vii then now has a distri bution so that converges in probability to as in fact using the analytic expression for the density in terms of beta functions one can show that more over hence as putting gives the claimed result it is useful to restate the last lemma let denote the joint distri bution function of conditional on let denote the joint distribu tion function of again conditional on then the last lemma implies however a certain degree of uniformity is present which will be important for us in the following result and is defined by lemma the family is uni formly continuous in and locally uniformly continuous in at each the family of functions is uniformly continuous in uni formly in locally uniformly at each proof the function is the multivariate cdf of a jointly gaussian family of random variables provided the un derlying covariance has a positive determinant we have an im mediate bound on all derivatives of the density in terms of the reciprocal of the determinant using qr factorization and the reversed martingale structure of the covariance the determinant can in turn be bounded below by hence for all obeying a given fixed value we have a uniform bound on the derivatives of independently of and any other details of the function is as we have seen the cdf of random variables which are perturbations of the rv underlying more specifically where we have the polar decomposition of the random variable where and are stochastically independent and is a scaled so that is a function of and the thus the random variables and are stochastically independent hence temporarily omitting subscripts on and we may write where denotes the density of the rv we can differentiate and obtain as we conclude that this explicitly allows us to control the modulus of continuity of in a manner uniform in and across values of such that hence the control is locally uniform in the variables away from the set where suppose that we have a sample from the standard problem suite and that the random variable introduced above is stochastically independent of suppose that stomp has been run through stages recall the values etc produced by the stomp algorithm and condition on those results defining etc as is a random point on but not a column in the matrix the donoho et al sparse solution of underdetermined systems of linear equations probability distribution of conditional on is exactly that of above with parameters etc now let denote the probability density of induced by stomp and let denote the corresponding probability measure let denote the joint cumulative distribution function of the random variables then we have the exact formula vii now by theorem there exist constants so that for vii combining this with the uniform equicontinuity of lemma and the scale mixture identity vii we conclude that as vii of course is of no interest to us per se consider now a column from and suppose that is both a null coor dinate and a surviving coordinate at stage it might seem that would have the same distribu tion as but this is only true for at later stages of the algorithm behaves as subjected to conditioning vii we now make the observation that probabilities of hyper rect angles can be computed simply from the joint cumulative dis tribution function we state without proof an elementary fact lemma let denote random variables with joint cumulative distribution function the rectangle probability can be expressed as a linear combination with coefficients the rectangle probability similarly has a representation it follows that if we have a sequence of such cdf converging uniformly to a limit cdf then we also have convergence of the corresponding rectangle probabilities just mentioned a conditional probability is a ratio of two such terms the probability law given on the right hand side of vii has cumulative distribution function invoking lemmas and as well as vii we get the middle step invoked the fact that in the sense of con vergence in probability in uni form norm locally uniformly in non null case the technical side of the argument paral lels the null case and we will not repeat it the only point we clarify here is the calculation of the means and standard de viations for this calculation we propose to model as a where the are arbitrary signs and are gaussian random vectors this model corresponds to gaussianizing the ssp instance a vector uniformly distributed on the unit sphere in is gaussianized by multiplying it by an independent scalar random variable where is chi distributed on degrees of freedom the resulting vector is distributed now apply such gaussianization independently to each of the columns of producing the columns of a matrix the vector has indeed the distribution of we will make some computations using this gaussianization the result exactly true in the gaussian case is asymptotically correct for the original pre gaussianization model the same approach was used less explicitly in the last subsection gaussianization has also been heavily used in the banach space literature see also for examples in the present spirit we start with a typical bayesian calculation lemma suppose that are gaussian vectors in distributed suppose that given has a gaussian conditional distribution we omit the proof of this well known fact consider now a deterministic vector and deterministic orthoprojectors yielding vectors because projections of gaussians are gaussian and linear combinations of gaussians are gaussian we immediately obtain lemma let be a random vector in with gaussian distribution define random variables their marginal distribution is precisely ieee transactions on information theory vol no february fig phase diagrams for cfar thresholding when the nonzeros have non gaussian distributions a uniform amplitude distribution b power law dis tribution compare to fig shaded attribute number of entries where reconstruction misses by we again omit the elementary proof consider now in parallel with lemma we have lemma define the family of random variables this family is well approximated by the gaussian random variables defined above in fact for a sequence depending only on clearly the above elements can be combined to give our re sult in much the same fashion that used in the null case can be carried out in the present case let then fig performance of cfdr thresholding noisy case relative error as a function of indeterminacy and sparsity performance at signal to noise ratio shaded attribute gives relative error of reconstruction signal to noise ratio is define gaussian random variables with mean and vari ance let denote the random variable conditional on the event by the same ap proach as in the null case we obtain here of course the presence of the factor does not affect the limit as will eventually be large with overwhelming probability this completes our proof of theorem viii variations a how many stages in the experiments reported here we chose stages our main consideration in choosing the number of stages is the speed of the resulting algorithm obviously choosing smaller or larger would modify the speed and modify the phase transi tion diagram and so give us a larger or smaller range of effec tiveness because we make available the code that performed our experiments see section x it is straightforward to study variations on the procedure described here b varying coefficient distributions the phase transitions displayed in section iv were computed assuming the nonzero coefficients have a gaussian distribution the phase transitions in section vi assumed the nonzero coef ficients in have a symmetric distribution on there are small differences with the gaussian coefficients leading to tran sitions at higher values of we have of course tried other distri butions as well experiments in fig panel a show the case of a uniform distribution on the coefficients while fig panel b illustrates the power law case we conjecture that among donoho et al sparse solution of underdetermined systems of linear equations fig reconstruction of bumps with hybrid cs panel a signal bumps with samples panel b reconstruction with bp panel c reconstruction with omp panel d reconstruction with cfdr panel e reconstruction with cfar a signal bumps b hybrid cs with bp b hybrid cs with omp c hybrid cs with cfdr thresholding d hybrid cs with cfar thresholding coefficient distributions the worst phase transition is approxi mately given by the sign case where we have worked to give a rigorous theory c noisy data the methods discussed above extend quite naturally to the case of data contaminated with white gaussian noise indeed suppose that our observations obey where denotes an i i d noise the matched filter will obey the same conditioned normal appproximation with dif ferent variances hence to the extent that our approach was ap plicable before it remains applicable we remark however that cfdr seems most appropriate in the noisy case fig displays the performance of cfdr thresholding in the noisy case the transition behavior is less clear cut than in the noiseless case it seems to indicate graceful smoothing out of the sharp transition seen in the noiseless case d other matrix ensembles our attention has focused on the case where is a random matrix generated from the uniform spherical ensemble similar results follow immediately for two closely related ensembles urpe uniform random projection ensemble contains the first rows of an by random orthogonal matrix ge gaussian ensemble the entries of are i i d in fact we have already used more than once the fact that ge and use are intimately related matrices in the two ensembles differ only by the normalization of the columns a member of urp can be obtained by sampling from the gaussian ensemble and normalizing the columns to unit length also the close rela tionship of urpe and ge is quite evident by viewing one as pro duced from the other by a gram schmidt process on the rows fig panels c f and i show that for the urpe the mai for matched filtering obeys the gaussian approximation exten sive experiments have shown that stomp has the same behavior at the urpe ge and use but we omit details for reasons of space scripts generating such experiments are included in the software publication see section x more interestingly we considered other random ensembles the most well known ones being random signs ensemble the entries of the matrix are the signs chosen randomly partial fourier ensemble rows are chosen at random from an by fourier matrix partial hadamard ensemble rows are chosen at random from an by hadamard matrix possible only for cer tain these are important for various applications of compressed sensing for each ensemble we found that the gaussian approx imation applies fig panels b e and h illustrate the mai for matched filtering at the rse thus we propose stomp for such ensembles as well ieee transactions on information theory vol no february ix s tylized a pplications we now illustrate the performance of stomp and the thresh olding strategies a compressed sensing recently there has been considerable interest both from the orists 33 and from practitioners 47 in the possibility of dramatically reducing the number of samples that have to be measured in various remote sensing problems in effect one views the problem as one of recon structing a high dimensional vector from a low di mensional data sample which is obtained from by linear sampling here although samples seem to be needed according to standard linear algebra everything we have shown in this paper as well as the cited prior work shows that samples can suffice to get either an exact or approximate recon struction of we now study the performance of stomp and the thresh olding strategies in concrete instances inspired by applications in spectroscopy and imaging bumps our first example uses the object bumps from the wavelab package rendered with samples this object shown in panel a of fig is a caricature of sig nals arising in nmr spectroscopy characterized by a few lo calized peaks such signals are known to have wavelet expan sions with relatively few significant coefficients we considered a compressed sensing cs scenario where sensed samples are taken reflecting random linear combinations of the wavelet coefficients of bumps the details are the same as for hybrid cs in in our simulations we compared the perfor mance of stomp equipped with cfdr and cfar thresholding to that of basis pursuit i e minimization and matching pur suit i e omp the results are summarized in fig evi dently the accuracy of reconstruction is comparable for all the algorithms used however the speed of the two stomp imple mentations is unrivaled by bp or omp compare the re quired by stomp with cfar to generate a solution with the needed by omp or nearly min of computation time en tailed by minimization as for the results appearing in table i all the simulations described in this section were obtained on a ghz xeon workstation we now consider a larger scale example in two dimensions fig a shows a synthesized image of d gaussian spectra created in the following manner forty gaussians were gen erated with standard deviations randomly varying amplitudes drawn from an exponential distribution and positions i i d uni form the image is discretized on a grid of pixels we applied multiscale cs as in we used a wavelet basis and formed a matrix which gathered random linear combinations of the coefficients in the three finest wavelet scales see for details the total number of sensed samples was here the number of pixels fig panels b d present reconstruction results for bp cfdr and cfar respectively we did not consider omp in our simu lations it seems impractical to apply in such large scale appli cations due to memory constraints all three algorithms pro duced faithful reconstructions however careful investigation fig reconstruction of d bumps with multiscale cs panel a two di mensional gaussian spectra on a 256 grid panel b reconstruction with bp panel c reconstruction with cfdr panel d re construction with cfar a original image b multiscale cs with bp c multiscale cs with cfdr d multiscale cs with cfar of the error and timing measurements reveals that cfdr and cfar outperformed bp in terms of both speed and accuracy mondrian we now consider a geometric example panel a of fig displays a photograph of a painting by piet mondrian the dutch neo plasticist this image has a relatively sparse expansion in a tensor wavelet basis and therefore is suitable for cs this test image while of a relatively simple geometric nature still presents a challenging trial as its wavelet expansion is not very sparse in fact out of wavelet coefficients there are only coefficients with magnitude smaller than more naturalistic images would be equally fitting candidates provided they admit sparse repre sentations in an appropriate basis frame such as the curvelets frame for instance much as with the d bumps image we used the mondrian image in a multiscale cs setting applied to the fourfinest scales of the wavelet expansion a total of sensed sam ples were used overall since this stands for roughly one quarter of the original dimension of the data fig panels b d have reconstruction results indeed all three algorithms performed well in terms of reconstruction ac curacy of the three minimization produced the most accu rate reconstruction as measured by distance to the original it did so however at an outstanding cost over hours of com putation were required by bp with the pdco solver to reach a solution in contrast stomp with cfar produced a re sult of comparable accuracy in just over a minute observant readers may notice that here data size is comparable to the d spectra example but the computation time required by direct minimization is significantly larger the cause is our specific implementation of bp the primal dual barrier method favors solution vectors which contain many exact zeros we find it instructive to take a closer look at the reconstruc tion results in the wavelet domain to that end we zoomed in on donoho et al sparse solution of underdetermined systems of linear equations fig reconstruction of d imagery with multiscale cs panel a mondrian painting pixels panel b reconstruction with bp panel c reconstruction with cfdr panel d reconstruction with cfar a original image b multiscale cs with bp c multiscale cs with cfdr d multiscale cs with cfar a horizontal slice of wavelet coefficients at one scale below the finest as displayed in panel a of fig comparing the reconstruction results of the iterative thresholding algorithms with the original slice of coefficients reveals a great deal about their performance when the underlying signal is not sufficiently sparse both cfdr and cfar successfully recovered the sig nificant coefficients while keeping the rest of the coefficients at zero in fact it makes sense to view the small coefficients as the result of digitization noise in which case the thresholding algo rithms are actually removing noise while remaining faithful to the original signal in contrast minimization tends to exacer bate the noise under insufficient sparsity conditions as was dis cussed in detail in in short stomp is a dependable choice even beyond its region of success b error correction virtually every digital communication system employs error control coding as an integral part of its operation there is el egant coding theory showing showing how to encode items in a block of transmitted numbers with the ability to correct up to arbitrary errors unfortunately for general linear coding schemes the task of identifying the most likely sites for the errors is known to be np hard lately there has been much interest in developing good fast decoding schemes the litera ture in the ieee transactions on information theory on mes sage passing decoding and turbo decoding is literally too volum nious to charcterize recently candès and tao pointed out that minimization sparsity ideas have a role to play in decoding linear error correcting codes over naturally it fol lows that stomp also has an opportunity to contribute here is the experimental mise en place assume is a dig ital signal of length with entries representing bits to be transmitted over a digital communication channel prior to transmission we encode with an ecc constructed in the fol lowing manner let be a random orthogonal matrix of size where is the redundancy factor of the code par tition where is the encoding matrix and is the decoding matrix then the en coding stage amounts to computing with the encoded signal of length at the decoder we receive where has nonzeros in random positions and the nonzero ieee transactions on information theory vol no february fig zoom in on a horizontal slice of wavelet coefficients panel a original horizontal slice of coefficients at scale panel b reconstruction with bp panel c reconstruction with cfdr panel d reconstruction with cfar a horizontal slice of wavelet coefficients at scale b reconstruction with bp c reconstruction with cfdr d reconstruction with cfar amplitudes are gaussian i i d in words the signal is sent over a sparse noisy channel there are no assumed bounds on noise power the minimum norm decoder solves call the solution at the output of the decoder we compute the key property being exploited is the mutual orthogonality of and specifically note that hence is essentially solving for the sparse error patten fig presents results of tests at redundancy and decoded data length we performed each experiment multiple times while varying the noise sparsity at each in stance we recorded the bitwise error rate ber and the de coding time for comparative purposes we repeated the experi ment using basis pursuit and omp the results are summarized in plots showing ber as a function of noise sparsity see panels a d of fig in terms of ber bp prevailed decoding suc cessfully even when gross errors contaminated more than half the received signal values yet stomp with cfar thresholding came remarkably close to the performance of true minimiza tion and it did so in a fraction of the time needed by bp com pare the average decoding time of required by stomp to needed by bp actually stomp can outperform decoding in terms of error resistance at very small consider fig it presents the results of a similar experiment in a slightly different setting here we set and i e we choose a long blocklength code with very little redundancy the phase diagram in fig shows that at and so stomp decoding is predicted to outperform decoding at such low redundancy in our experiment both cfar and cfdr thresholding performed better than minimization and omp in terms of ber again comparing timing measures we see that stomp runs at about th the time needed by bp to summarize stomp provides a rapid yet dependable alternative to costly minimization c component separation in overcomplete systems we consider now the problem of separating a signal into its harmonic and impulsive components see also and in detail assume we have a signal of length known to admit sparse synthesis by a few selected sinusoids and a few spikes with a total of such components formally we have where is an identity matrix is an fourier matrix and is a coefficient vector established bounds on the sparsity under which minimization success fully recovers the coefficient vector in this underdetermined setting here we investigate the performance of stomp as an alternative to direct minimization fig a shows a signal of length consisting of two harmonic terms perturbed by spikes with amplitudes drawn at random from a normal distribution for a total of nonzero synthesis coefficients in the time fre quency dictionary the spike and sinusoid components of are plotted individually in panels b and c we solved this underdetermined system using stomp with cfar and cfdr thresholding results are portrayed in the second and third rows of fig both thresholding strategies perfectly recovered the synthesis coefficients in four stages validating our claim that stomp is a fast alternative to minimization donoho et al sparse solution of underdetermined systems of linear equations fig performance of decoders with varying noise sparsity a cfar thresholding b cfdr thresholding c basis pursuit d omp vertical axes bitwise error rate horizontal axes number of errors coding redundancy a itsp with cfar avg time b itsp with cfdr avg time c bp avg time d omp avg time x reproducing our results the phrase reproducible research describes a dis cipline for publication of computational research together with a complete software environment reproducing that research in that spirit all the figures appearing in this paper can be repro duced using the sparselab library for matlab sparselab is a collection of matlab functions and scripts developed at stan ford university and which is available freely on the internet at www stat stanford edu sparselab it includes an array of tools to solve sparse approximation problems supplemented with de tailed examples and demos sparselab has been used by the au thors to create all the figures and tables used in this article and the toolbox contains scripts which will reproduce all the calcu lations of this paper xi related work we briefly discuss several significant precursors to this work a statistical modelling statisticians have since the advent of automatic computing developed a very large of model building strategies these in clude forward stepwise regression and screening regressions both closely related to the present work in our notation the statistical modelling problem goes as fol lows one is given data where is gaussian noise is the response the columns of are predictors and is a vector of coefficients it is believed that most potential predictors are irrelevant and that only a few predictors should be used but it is not known which ones these are likely to be equivalently most of the coefficients in are zero but the positions of the nonzeros are unknown forward stepwise regression simply selects the predictor having best correlation with the current residual at each stage and adds it to the current model which it then fits by least squares this procedure has been used extensively by persons of our acquaintance for more than years it is the same thing we have called omp in this paper a term that arose in signal processing about years ago the method of screening regressions selects all predictors having a significant correlation with the original signal this is the same as stage one of stomp no doubt over the last five decades many people have tried iterative screening regressions at least on an informal basis what is different in our proposal here our data may be noiseless the underlying matrix of predictors here must however be random e g generated with random independent columns this randomness of alone is somehow responsible for the phenomena described here our work shows that stagewise model building seemingly ad hocand hard to analyze can if the underlying matrix of pre dictors is random have impressive theoretical properties it sug gests that stagewise model building in gaussian linear modeling and perhaps elsewhere may have provably good properties ieee transactions on information theory vol no february fig performance of decoders in a setting with small coding redundancy a cfar thresholding b cfdr thresholding c basis pursuit d omp a cfar avg time b cfdr avg time c bp d omp avg time b digital communications also mentioned earlier is the connection between our calcu lations and several key notions in multiuser detection theory randomly spread cdma systems can be thought of as posing the problem of solving where is a transmitted bi nary vector is the received vector and is a rectangular array with random entries this is like our problem except a need not be sparse and b need not have more columns than rows in the mud literature the idea that looks like gaussian noise is clearly established in the work of poor verdú and others 61 also the idea that sophisticated multistage algorithms can be applied to suc cessively reduce mai e g onion peeling schemes or iterative decoders is completely consistent with our approach in this paper stagewise least squares projection when the nonzeros in have a power law distribution is something like onion peeling minimization is something like a bayesian maximum a posteriori iterative decoder finally the specific analysis technique we have developed the con ditioned gaussian limit bears some resemblance to density evolution schemes used in the mud cdma literature e g however there are important differences in the problem the primary one being that in the binary cdma case the vector is not sparse and takes known values which cause important differences in results also perhaps the study of very large and is less of interest in cdma at the moment c component separation the immediate inspiration for this paper was extensive work by jean luc starck and michael elad who attacked very large scale problems in image processing and component separation they found that by stagewise application of hard thresholding residualization and matched filtering they could often obtain results comparable to optimization but much more rapidly our paper arose from starck insistence that such an approach was essential to attacking very large scale problems with in the millions and demanded theoretical attention fo cusing on the special case of random we found his insis tence to be prophetic d mathematical analysis the mathematical developments in this paper and the form of the stomp algorithm itself arise from a lemma in the paper sec by one of the authors that lemma concerned behavior of random linear programs and was originally developed in order to show that minimzation can find sparse solutions when is drawn from use the authors noticed that it implicitly intro duces the conditioned gaussian model developed here e fine points two small but essential points the notion of phase transition considered here is weaker than notions often mentioned in connection with the study of optimization as the papers and may help clarify much of the literature discusses the notion of strong equivalence of and in this notion that for a given every sparse generates a problem for which the solution is the unique sparsest solution 55 in contrast in discussing minimization in section iv b above we used the notion of weak equivalence which says that equivalence holds for the typical sparse rather than for every sparse donoho et al sparse solution of underdetermined systems of linear equations fig time frequency separation with stomp top row original signal with its time and frequency components middle row corresponding output of stomp with cfar thresholding bottom row corresponding output of stomp with cfdr thresholding a original signal b time component c frequency component d cfar solution e time component of cfar solution e frequency component of cfar solution f cfdr solution g time component of cfdr solution h frequency component of cfdr solution in the setting of random matrices sampled from the uniform spherical ensemble independently of strong equivalence is not the relevant notion even though it has attracted the most theoretical attention if is random and is fixed in advance then is overwhelmingly likely to be typical for the realized the empirically observable phase transition is thus the weak transition whose theoret ical large system transition point for minimization was derived in and is given by the curve in fig for par allel discussion see the notion discussed here for stomp is still weaker because our probabilistic theory only studies approximate reconstruction of typical hence it might seem that the phase transition for stomp mapped out here is less useful in practice than the weak equivalence transition for minimization despite this empirically we have found that for below its phase transition curve stomp yields reconstructions which are numerically just as accurate as minimization yields below its phase transition curve iterative thresholding is an algorithm which like stomp successively strips structure out of a residual vector like stomp it computes the vector of current correlations and applies a threshold however instead of orthog onal projection to remove detected structure it uses a relax ation strategy subtracting structure associated to columns surviving thresholding the use of alternating thresholding to obtain sparse solu tion has been extensively deployed by starck et al and studied by daubechies et al an early ref erence applying a kind of alternating thresholding to seek sparse solutions to underdetermined systems was coifman and wickerhauser to our knowledge the alternating thresholding approach has yielded today most successful applications of large scale sparse solutions 51 stomp is subtly but crucially different our inspiration is the idea that for gaussian random matrices and their close relatives selection followed by projection affords certain definite probabilistic structure which we have ex posed and analyzed carefully here in theorems and the initially similar sounding proposals for alternating soft and hard thresholding do not seem to offer any com parably strong analytic framework ieee transactions on information theory vol no february xii s ummary we described a simple algorithm for obtaining sparse ap proximate solutions of certain underdetermined systems of linear equations the stomp algorithm iteratively thresholds selects and projects it selects nonzeros by thresholding the output of the matched filter applied to the current residual signal where the threshold is set above the mutual interference level it projects the residual on a lower dimensional subspace complementary to the span of the selected nonzeros stomp is effective when the matrix and or the object render the multiple access interference approximately gaussian relying on such gaussianity we proposed two natural strategies for threshold selection one based on false alarm rate control and one based on false discovery rate control we rated the success of this approach using the phase dia gram and showed that in the standard problem suite for either thresholding rule the phase diagram has two phases success and failure with a narrow transition zone between them the transition zone shrinks in size as the problem size grows for each method the success phase of stomp is comparable in size to the success phase of minimization computational experiments showed that within the intersection of the two suc cess phases stomp can deliver results comparable to mini mization with dramatically less computation also in numerous examples stomp runs much faster than standard greedy ap proximation matching pursuit supporting the practical advantages of stomp is a theoretical framework that accurately derives the boundary of the success phase this boundary can be regarded as a well defined asymp totic sampling theory for stomp to reconstruct a sparse vector by stomp an asymptotically precise number of samples will be required stomp extends naturally to noisy data and also extends to underdetermined systems outside the random ones where the method was derived important examples being the partial fourier ensemble and pairs of incoherent orthogonal bases such as dirac fourier stylized applications in compressed sensing suppression of impulsive noise and time frequency separation are given and software is available xiii note added in proof this work was performed in and submitted in the published version differs from the submitted ver sion because of referee requested edits two added proofs and some shortening of the abstract while the work was in sub mission it seems to have already had substantial citation im pact and practical impact on the analytical side the method of studying stagewise algorithms with gaussian i i d matrices appears to have been employed successfully in other contexts for example please see related ideas in bayati and montanari work on approximate message passing they may have been unaware of our unpublished work much work on sparse recovery and compressed sensing took place while the work was in submission but the associate editor and the authors felt that it would be difficult to rewrite the paper to integrate all the more recent work a ppendix a p roof of l emma consider the restriction i e throw away the entries in lying outside by hypothesis all the entries that are thrown out are zero and the residual from stomp is zero so similarly consider the restriction i e form the vector with entries outside of thrown away since by hy pothesis is supported inside we also have hence by hypothesis the columns of are in general position so there is no nontrivial relation where and is a column vector compatible with hence and also appendix b heuristic phase transition calculation for the standard ensemble we have found that the following heuristic model can be used to derive a predicted phase transition curve that qualitatively matches the behavior of the empirical phase transition in the standard problem suite we use notation paralleling the notation of section vi under this heuristic the normalized coefficients for act as random samples from a gaussian mixture distribution where and we therefore define and we start with and apply the ob vious updating formulas if at or before stage we achieve and we declare stomp to be successful at that given for cfar thresholding is constant set to the quantile of the standard normal distribution where for cfdr thresholding is variable set to the solution of donoho et al sparse solution of underdetermined systems of linear equations here is the assumed false discovery rate we used in all our work these heuristic formulas cannot be precisely correct for two reasons they omit the effect of conditioning caused by earlier stages as discussed in section vii the underlying coefficient distribution is a mixture but not of normal distributions and the pseudostandard deviation is merely a crude upper bound the true value reflects considerations from random matrix theory however if the variance drops very rapidly with in creasing often this heuristic is reasonably accurate appendix c proof of theorem by state vector we mean a four tuple note that this is a random variable for we have while later steps depend on the specific realization and the progress of the algorithm note that if at step so the algorithm must stop prior to planned termination we put by definition the distance between two state vectors is just the eu clidean distance between the tuples by history we mean the sequence of state vectors up to stage this is again a random variable by distance between two his tories we mean the maximal distance between corresponding states of the history we now observe that all the quantities defined in the statement of theorem are uniformly continuous func tions of the history therefore it is sufficient to prove that there exists a large system limit for the history i e that given there is so that for each c this can be done inductively as follows first for recall that we assume we have by elementary cal culations this proves lemma c large system limit let and as then c hence the inductive process is begun below we will show that there is a deterministic sequence obeying c for certain tuple valued functions of step histories for each this sequence obeys for c then c follows to establish c we consider in turn each individual compo nent the first three components share much in common owing to the similar relations c and consider the first component in the next subsection we prove the following lemma c suppose either that or that and c has been proven for let denote the conditioned gaussian random variable defined as in section vii by the sequences and where c then c now of course depends on the limit history so it makes sense to write exploiting the result which we regard as proved at a previous stage of the argument we rewrite c as we can argue very similarly for the second and third components of the state vector and put and define where is a conditioned gaussian random variable defined by and as in c and where is a conditioned we present an experiment based characterization of passive suppression and active self interference cancellation mechanisms in full duplex wireless communication systems in particular we consider passive suppression due to antenna separation at the same node and active cancellation in analog and or digital domain first we show that the average amount of cancellation increases for active cancellation techniques as the received self interference power increases our characterization of the average cancellation as a function of the self interference power allows us to show that for a constant signal to interference ratio at the receiver antenna before any active cancellation is applied the rate of a full duplex link increases as the self interference power increases second we show that applying digital cancellation after analog cancellation can sometimes increase the self interference and thus digital cancellation is more effective when applied selectively based on measured suppression values third we complete our study of the impact of self interference cancellation mechanisms by characterizing the probability distribution of the self interference channel before and after cancellation index terms full duplex radios i introduction current employ either deployed a time division wireless or communication frequency division systems ap proach to bidirectional communication this requires divid ing the temporal and or spectral resources into orthogonal resources and results in half duplex wireless communication systems the key deterrent in implementing a full duplex wireless communication system which consists on same band simultaneous bidirectional communication is the large self interference from a node own transmissions in comparison to the signal of interest from the distant transmitting antenna the large self interference spans most of the dynamic range of the analog to digital converter in the receiving chain which in turn dramatically increases the quantization noise for the signal of interest however recent experimental results for indoor scenarios have shown that it is possible to implement self interference cancellation mechanisms that can sufficiently manuscript received july 2011 revised may accepted septem ber the associate editor coordinating the review of this paper and approving it for publication was d michelson this work was partially supported by nsf grants cns cns and cns the first author was also supported by a xilinx fellowship and a roberto rocca fellowship m duarte is with the school of computer and communication sciences epfl lausanne switzerland e mail melissaduarte alumni rice edu a sabharwal is with the department of electrical and computer engineer ing rice university houston tx usa e mail ashu rice edu c dick is with xilinx inc san jose ca usa e mail chris dick xilinx com digital object identifier twc 102612 attenuate the self interference such that the resulting full duplex wireless system can achieve higher rates than a half duplex system hence recent results have demonstrated that full duplex systems are a feasible option for future indoor wireless communications in this paper we perform a data driven analysis of the full duplex architecture proposed in based on extensive experimental data our main contribution consists in charac terizing the impact of different self interference cancellation mechanisms on the performance of full duplex wireless com munication systems we consider three methods to reduce self interference which can be classified as either passive or active the passive suppression is simply attenuation caused by path loss made possible by antenna separation between the transmitting antenna and the receiving antenna on the same node to reduce the dynamic range of the self interference we use the active analog cancellation proposed in where an additional rf chain is used to cancel the self interference at the receiving antenna before the analog to digital converter in addition we also study active digital cancellation where the self interference is removed in baseband after analog to digital conversion our characterization of total and individual cancellation mechanisms based on extensive experimenta tions shows that a total average cancellation of db can be achieved our results show that the average amount of self interference cancelled by active cancellation increases as the power of the self interference signal increases this result is intuitive because the canceller relies on estimating the self interference channel and a higher self interference power implies lower channel estimation error and hence better can cellation our characterization of active cancellation as a function of the self interference power allows us to show that for a constant signal to interference ratio sir at the receiver antenna this is the sir before active cancellation the rate of a full duplex link increases as the self interference power increases this appears counter intuitive at first but follows from the fact that the average amount of self interference cancellation increases with increasing self interference power related work on implementation of self interference can cellation mechanisms has reported measured values of the amount of cancellation that can be achieved but a characterization of the effect of increasing self interference power on the amount of active cancellation and rate has not been reported before authors in have argued that the rate performance of full duplex is independent of the transmitter power this conclusion is valid for the particular model and implementation in however we demonstrate that this does c ieee duarte et al experiment driven characterization of full duplex wireless systems not generalize to all full duplex implementations specifically we demonstrate an implementation where the full duplex rate increases as the transmit power increases related work has shown that digital cancellation while by itself insufficient to deal with self interference can increase the total amount of cancellation when applied after analog cancellation however intuitively it is clear that in an ideal scenario where analog cancellation could achieve infinite db of cancellation then having digital cancellation after analog cancellation would be unnecessary this leads to the natural question regarding when is digital cancellation useful we present results that show that the self interference suppression achieved by digital cancellation when applied after analog cancellation decreases as the self interference suppression achieved by analog cancellation increases further our results show that when analog cancellation achieves large suppression then applying digital cancellation after analog cancellation can increase the noise in the system no previous work on full duplex system implementation has analyzed the performance of digital cancellation as a function of the performance of analog cancellation our results show that digital cancellation is an excellent safety net i e for the cases when analog cancellation delivers poor suppression digital cancellation is most useful when applied selectively frame by frame based on measured suppression performance we complete our study of the full duplex architecture in by characterizing the distribution of the self interference channel before and after active cancellation before applying active cancellation the self interference channel is the channel between two antennas that are close to each other conse quently there is a strong line of sight los component and the magnitude of the self interference channel can be modeled as a ricean distribution with large k factor after applying active cancellation the strong los component is reduced hence the magnitude of the self interference channel can be modeled as a ricean distribution with smaller k factor we present a characterization of the k factor values before and after active cancellation such characterization has not been reported before for any full duplex architecture full duplex wireless communications have been considered in the context of two way bidirectional communication and in the context of full duplex relays our analysis of the amount of self interference cancellation achieved by the full duplex architecture in is applicable to full duplex nodes either in a relay or a two way communication scenario our analysis of the rate performance of full duplex will focus only on a two way system the rest of the paper is organized as follows in section ii a we derive equations that model the full duplex architecture we have implemented and serve as a theoretical framework that we will use for explanation of our results a description of the experiment setup is presented in section iii in section iv we present a characterization of the amount self interference cancellation achieved and a characterization of the k factor of the self interference channel before and after active cancel lation in section v we present an analysis of the achievable rate performance of full duplex two way systems conclusions are presented in section vi ii channel model for full duplex fig shows the block diagram for the narrowband full duplex node architecture we consider part of the signal processing is implemented using the following major blocks upsampling and pulse shaping ups matched filter and downsampling mfd digital to analog converters dacs analog to digital converters adcs and transmitter and receiver radios the transmit radios tx radio upconvert the input signal from baseband bb to radio frequency rf and the receive radio rx radio downconverts from rf to bb in this section we first present self interference channel models with different stages of passive and active suppression at the end of the section we present the channel model for a two way full duplex system a channel model with antenna separation no active can cellation in fig signal x i n f denotes the n th symbol trans mitted from node i during frame f a frame consists of n sym consecutive transmitted symbols we define x i n f n f is the transmitted constellation symbol normalized to unit energy and e s i denotes the av erage symbol energy e x i n f e s i consequently e i n f and f to denote the wireless self interference channel at node i and we model this channel as a random variable that remains constant during the trans mission of a frame f and changes from one frame to the next we define ω i i we use h i i e h i i f antenna separation is the simplest passive self interference suppression mechanism and the amount of cancellation achieved by antenna separation depends on the propagation loss of the signal traveling through wireless channel h i i f at node i the received self interference signal after antenna separation is equal to h i i f x rf i t b with analog cancellation the analog self interference cancellation we consider de picted in fig consists in adding a cancelling signal to the received signal in the analog domain the hardware com ponents used for the implementation of analog cancellation consist of one dac one transmitter radio one rf attenuator and one rf adder the output of the rf attenuator is con nected to the rf adder via a wire at node i the canceling signal input to the rf adder is equal to h z i t h z i f c rf i f κ ac i f x rf i t where h z i f denotes the magnitude and phase change that affect signal κ ac i f x rf i t when passing through an attenuator and a wire to the rf adder analog self interference cancellation uses κ ac i f such that h i i f x rf i t h z i f κ ac i f x rf i t one can easily see that if κ ac i f h i i f h z i f then the self interference at the input of node i receiver radio will be completely cancelled however the estimate of channels h i i f and h z i f is not perfect due to additive noise and other distortions in the system we define the noiseless cancellation coefficient used for analog self interference cancellation during frame f at node i as κ ac i f h i i f h z i f and we use κ ac i f to e s i i n f where i 4298 ieee transactions on wireless communications vol no december denote the noisy estimate of κ ac i f the self interference at node i after applying analog cancellation is equal to yac i i amount of cancellation achieved by analog cancellation at node i α ac i is given by n f h i i f h z i f κ ac f x i n f if digital cancellation described below in section ii c is not α ac i h i i being used then κ dc i e f x i n f f and the received self interference y i i we n note f is that equal any to gains yac i i n applied f by the radios are included in the energy term e s i and the cancellation coefficient κ ac i f this reduces the amount of notation required to present the model we also note that the additional transmitter radio used and the average amount of cancellation achieved by combined analog and digital cancellation at node i α acdc i for analog cancellation does not require a power amplifier since the signal used for analog cancellation is being trans mitted over a wire for our specific implementation the radio used for analog cancellation had a power amplifier by default so we used an rf attenuator connected in series as shown in fig in order to cancel the effect of the power amplifier the attenuator used was a passive device with a fixed db of attenuation the rf adder used was also a passive device c with analog and digital cancellation digital cancellation at node i is simply adding κ dc i is given by α acdc i e h i i f x i n f ω i i further we define the self interference channel after can cellation as the coefficient that multiplies the self interference signal after applying self interference cancellation from we have that the self interference channel after analog can cellation at node i is equal to h i i f h z f κ ac f f x i n f in baseband to the received signal since digital cancellation without analog cancellation does not yield from we have that the self interference channel after analog and digital cancellation at node i is equal to h i i an interesting system else full duplex would not be a challenge we will not consider the case of digital cancellation without analog cancellation from we observe that perfect digital cancellation of the self interference signal at node i can be achieved by setting κ dc i f h z i f κ ac i f κ dc i f we define the normalized self interference channel after analog cancellation at node i as h ac i f h i i f h z i f κ ac i f f h i i f h z i f κ ac f however node i will not have a perfect estimate of h i i f h z i f κ ac f due to noise and other distortions and the normalized self interference channel after analog and present in the system we define the noiseless cancellation digital cancellation at node i as coefficient used for digital self interference cancellation at node i as κ dc i f h i i f h z i f κ ac i f and we use κ dc i h acdc i f f to denote the noisy estimate of κ dc i f the h i i f h z i f κ ac i f κ dc i f self interference after analog and digital cancellation is equal to yacdc i i n f h i i we rewrite and in terms of α ac i α acdc i h ac i f f h z i f κ ac i f κ dc i f x i n f and h acdc i f as follows for a full duplex system with active self interference cancellation mechanism φ the self hence cancellation y i i n are f applied yacdc i i n f when both analog and digital interference at node i after active cancellation is given by yφ i i d notation simplification and summary of self interference model parameters we now rewrite and in terms of the average amount of cancellation achieved by different self interference cancellation mechanisms and in terms of the normalized self interference channel after cancellation this will allow us to write the equations for the received self interference as function of a few parameters and will reduce the notation we define the average amount of cancellation achieved by a self interference cancellation mechanism as the ratio of the self interference energy before cancellation to the self interference energy after cancellation hence the average n f h φ i f n f where φ acdcl recall we use ac to denote analog cancellation and acdc to denote analog and digital cancel lation from we observe that the self interference at node i depends n f an experiment based characterization of h φ i f ω i i and α φ i for our full duplex implementation will be presented in section iv e channel model for two way full duplex our rate analysis will focus only on full duplex two way communication where two nodes node and node both implemented using the architecture in fig have data for each other and transmit and receive simultaneously in the duarte et al experiment driven characterization of full duplex wireless systems y i i mfd rf adder rx antenna tx rf d bb radio attenuator cκ ac i f fig block diagram of a full duplex node same frequency band in a two way full duplex system with active self interference cancellation φ the received signal at node after h s x n f at node after active yφ i active n f cancellation cancellation w n f is is and given given the by by received y y φ φ n n signal f f h the s x wireless n f channel yφ i n f between w n the f single we use h s to denote transmitter antenna at node and the single receiver antenna at node h s to denote the wireless channel between the single transmit ter antenna at node and the single receiver antenna at node and w to nodes and were connected via an ethernet switch to a host pc running matlab the digital baseband waveforms samples input to the dacs were constructed in matlab and downloaded from the matlab workspace to transmit buffers in the fpga of the warp nodes transmission and reception of over the air signals was done in real time using the warp hardware the samples at the output of the adcs were stored in receiver buffers and loaded to the matlab workspace on the host pc and processing of these samples was done in matlab we now explain in more detail the way in which κ fig time diagram for a full duplex frame time slots that are shaded in gray correspond to transmissions and processing that are used only for system characterization but are not required to enable full duplex communication the total duration of the frame is 6284 f h z i f κ ac f depends on errors in the estimate of were few people walking in the lab and our experiment setup h i i f and h z i f and these estimation errors vary randomly corresponds to a low mobility scenario the carrier frequency from one frame to the next the time diagram of a frame for was centered at ghz and the nodes shared the same our two way full duplex implementation is shown in fig carrier frequency reference clock the sampling frequency of the time between two consecutive frames is equal to adcs and dacs was equal to mhz the adcs had this duration is mainly due to the latency of the non real time bits of resolution the dacs had bits of resolution and processing and the reading and writing of samples between we implemented a single subcarrier narrowband system with the matlab workspace and the fpga buffers bandwidth of 625 mhz samples per symbol our self in our experiments both nodes were located at a height of interference cancellation can be extended to wideband e g meters above the floor with a line of sight los between by applying the proposed cancellation schemes per all antennas the distance between nodes was fixed to d subcarrier as shown in transmitted qpsk symbols were m for the separation between same node antennas we shaped using a squared root raised cosine pulse shaping filter used values of d cm cm cml the antennas used with roll off factor equal to one for our experiments the were ghz dbi desktop omni we used transmission number of symbols per frame was equal to n sym power values of p t dbm dbm dbm dbml and the number of frames transmitted in one experiment we ran the experiments in the laboratory of the center for was equal to n frames multimedia communication at rice university the laboratory is located in the second floor of duncan hall building we ran the experiments during school holiday recess hence there was limited to due to the constraint on the total number of samples that can be stored per warplab buffer setting n frames the value of n sym allowed us to approximate that during an h s f h s f duarte et al experiment driven characterization of full duplex wireless systems experiment the channels conditions remained approximately constant and variations were only due to small scale variations iv measurement based characterization of channel parameters a average cancellation by active cancellations in our experiments we used measurements of signal power before and after active cancellation in order to estimate α ac i and α acdc i α ac i p t α ac i p t for each node i as follows for a α ac i p t fixed inter antenna distance d at a node and transmit power p t we measured the power of the received self interference signal at node i during frame f which we label as p ri i α ac i p t α con ac i f α lin ac i this measured power is the power of the self interference simply due to path loss from antenna separation and before p ri i applying active cancellation we also measured the power of a results for active analog cancellation the constant fit is equal to the self interference signal after analog cancellation at node i α during frame f which we label as p ac i dbm where con ac i λ ac 55 21 db and db dbm the linear and β fit ac is lin f and the power of equal 42 to db α ac i λ ac p ri i β ac the self interference signal after combined analog and digital cancellation at node i during frame f which we label as p acdc i f per frame measurements of signal power were used to obtain estimates of average power by averaging over all frames interference the cancellation and average the specifically average power signal was power computed was of the the computed of average self interference the as self interference p as power ac i p ri i of signal the n f n f frames received frames after signal p p ac i ri i analog after self f f p ri i analog and digital cancellation was computed as p acdc i α acdc i p t α acdc i p t n f frames p acdc i f using these average powers we com α acdc i p α acdc i t puted the value of α φ p t for φ acdcl at node i as α φ i db p ri i dbm p φ i dbm the values of α ac i α con acdc i α lin acdc i and α acdc i from experimental data are shown in fig a and b respectively as a function of p ri i each marker in fig corresponds to an frame experiment for a fixed d b results for combined active analog and active digital cancellation p t the constant fit is equal to α dbm and i since we considered three values for d four values for p t equal to α lin and β ac con acdc i 31 56 db and the linear fit is and two values for i a total of different experiment acdc i λ acdc p ri i β acdc where λ acdc db dbm scenarios were considered for each scenario we ran two frame experiments resulting in data points from experiment results which are all shown in fig we do not specify the values of d and i in fig in order to avoid cluttering the figures varying either d p t db fig measurements of the average amount of active cancellation achieved and constant and linear fit for the measurements or i will result in a different reasons for result in order to implement the active value of p ri i and we analyze the dependence of α ac i and cancellation mechanisms we first need to estimate the wireless α acdc i on p ri i self interference channel the average power of the signal to understand the dominant trends of the experiment data in to fig the we compute the constant data the constant and linear fit fits αcon φ i both and computed linear fit using αlin φ i used to estimate the wireless self interference channel is p ri i as p ri i a least squares fit are shown in fig the key observation is that the linear fit captures the behavior of α as a function received interference power better than the constant fit for example for values of p ri i increases the error in the estimation of the wireless self interference channel decreases and thus the cancellation process is more exact leading to larger suppression of self interference result captures the total average performance of self lower than dbm the constant interference cancellation for both φ acdcl we dig fit of αcon p φ i ri i tends to overestimate deeper into the relative contributions of analog and digital larger than cancellation in acdc and discover the following result underestimate the value result as the average performance of analog cancella tion gets better the average effectiveness of digital cancella tion after analog cancellation reduces in fig we plot the average amount of cancellation achieved by digital cancellation after analog cancellation computed as α dc i the values of α φ i and for values dbm the of α φ i the constant better fit fit αcon between φ i tends to the experiment data and the linear fit leads to the following first result result as the average power of the received self interference p ri i increases the average amount of interfer ence suppression achieved by active cancellation both ac and acdc also increases db α acdc i db α ac i db as a function of the average amount of cancellation achieved by 4302 ieee transactions on wireless communications vol no december 22 26 α ac i α dc i p t α dc i p t α dc i p t 6 α dc i p t α con dc i α lin dc i α ac i fig measurements of the average amount of cancellation achieved by digital cancellation and cancellation values computed based on the constant and linear fit the constant fit is equal to α y t i l i b a b o r p db f db fig probability that digital cancellation after analog cancellation increases the db we compute the linear fit using the linear fit is equal to α lin con dc i equations db for α α con acdc i lin ac i and α α con ac i lin acdc i the total amount of cancellation during a frame as a function the cancellation achieved by analog cancellation λ acdc α lin ac i db β ac λ ac dc i β db acdc α lin ac i α lin acdc i db db α lin ac i db achieves more than db of cancellation applying digital cancellation after analog cancellation is not effective since the probability that digital cancellation results in an increase of analog cancellation results in fig show that in agreement the total amount of cancellation is less than hence it with result as α ac i increases α dc i decreases in fig we is most likely that digital cancellation will increase the self also show the value of α dc i computed based on the constant interference based on result we propose a design rule for fit and the linear fit results in fig show that the dominant full duplex systems which will be presented in section v c behavior is again better captured by the linear fit reasons for result and result intuitively it is clear dual to result is the following result that if analog cancellation can achieve perfect cancellation result the smaller the amount of suppression achieved infinite db of cancellation then digital cancellation is un by analog cancellation during a frame the larger the probabil necessary in fact if analog cancellation can achieve perfect ity that applying digital cancellation after analog cancellation cancellation then applying digital cancellation can result in an will result in an increase of the total suppression during that increase in the self interference this can be observed from frame notice that in case of perfect analog cancellation we in contrast to the average system performance in re have that h i i sults and result relates to frame by frame per formance in our experiments the suppression achieved by analog cancellation during frame f was computed as α ac i f h z i f κ ac i f but due to noise in the system the value of κ dc i f will not be equal to zero and will correspond to a measurement of noise hence adding κ dc i f x i n f to the signal after analog cancellation will f db p ri i f dbm p ac i f dbm and the result in an increase in the self interference we observe suppression achieved by analog and digital cancellation during that as the performance of analog cancellation improves the frame f was computed as α acdc i f db p ri i f dbm noise in the estimation of h i i f h z i f κ ac i f increases p acdc i f dbm the suppression achieved by digital cancel since h i i f h z i f κ ac i f becomes a smaller quantity and lation after analog cancellation during frame f was computed this reduces the effectiveness of applying digital cancellation as α dc i f db α acdc i f db α ac i f db digital after analog cancellation although our implementation of cancellation resulted in an increase in total suppression during analog cancellation does not achieve perfect cancellation we frame f if α dc i f db do observe from experiment results in fig that for values result is verified by experiment results in fig which of α ac i show the probability that digital cancellation results in an increase in total suppression during a frame as a function of the suppression achieved by analog cancellation during a frame for example for values α ac i f larger than db it is most likely that applying digital cancellation will increase the self interference this is consistent with average performance results in fig 4 which show that when the average cancellation achieved by analog f between db and cancellation is larger than db then digital cancellation after db the probability of having α dc i f db is equal to we observe from fig that digital cancellation analog cancellation can result in an increase in the average self interference which results in negative values of α dc i after analog cancellation becomes more effective as α ac i f the reasoning for results and was also noted in decreases fig shows that digital cancellation is effective simulation results in demonstrated that cancellation may for the frames where analog cancellation achieves less than actually degrade the isolation when the channel estimation db of suppression since in these frames the probability that error is large since cancellation adds a signal at the receiver digital cancellation increases the total suppression is greater side our experiment results now corroborate the observation than however for frames where analog cancellation made in which was based on simulation results duarte et al experiment driven characterization of full duplex wireless systems b passive suppression and total cancellation the amount of passive suppression at node i was computed as ω i i db p t dbm p ri i dbm averaging all the measurements of ω i i rayleigh distribution for h i i ricean distribution for h i i a rayleigh distribution for h ac i kl distance bits for a fixed d we obtain that the average ricean distribution for h ac i passive suppression for our implementation is equal to db for d cm db for d cm and db for d cm as expected larger d results in larger passive suppression the total cancellation cancellation achieved by combining passive suppression active analog cancellation and active digital cancellation at node i was computed as α tot i i c b a e c n a t i d l k y t i l i b a b rayleigh distribution for h acdc i ricean distribution for h acdc i p t db o r p dbm p acdc i dbm averaging all the measurements of α tot i for a fixed d we obtain that the average total cancellation for our implementation is equal to db for d cm db for d cm and db for d cm the total cancellation is observed to increase with increasing fig 6 cdf of kl distance d for the transmit powers we have used in our experiments between dbm and dbm a total cancellation of db will not bring the self interference down to the noise floor which is approximately dbm however as we will show independent estimates of k i i in section v b an average cancellation of db can result in full duplex rates larger than half duplex rates c characterizing the k factor we assume that the self interference channel has a ricean distribution and we model h i i 0 4 6 at a fixed d results in fig show that the value of k i i for d between cm and cm is between db and db these large values of k i i were expected due to the proximity of same node antennas it is expected that as the distance between antennas increases the value of k i i decreases hence the cdf of k i i for d cm should be more to the left than the cdf of k i i for d f and h φ i f as ricean cm and the cdf of k i i for with k factor k i i and k φ i respectively estimates of k i i and k φ i were computed based on experiment data and the moment based estimator presented in equation of each value of k i i k φ i that we computed was based on d cm should be more to the right however results in fig do not show a clear difference between the cdfs for d cm d cm and d cm our intuition is that an increase in separation from d cm to d cm results in a decrease in k i i that consecutive measurements of h i i f and h φ i f respec tively made at a fixed d and p t we note that the variations in each set of consecutive measurements corresponded to is smaller than the error in our estimate hence it is not captured by the cdfs shown in fig to support this intuition we also show in fig the cdf of k s i small scale variations hence each set of measurements can be used to obtain one estimate of the k factor our assumption that the self interference channel is ricean distributed makes sense intuitively because before applying active cancellation the self interference channel is the channel between two antennas that are close to each other hence there is a strong los component and the effect of active cancella tion would be a reduction of the los component 6 also we have computed the kullback leibler kl distance be tween the histogram of channel estimate magnitudes obtained from experiments and the probability density function of a ricean distribution with k factor computed from experiments the cdfs of the kl distances computed for our experiments are shown in fig 6 the low values of kl distances for the ricean distribution verify that modeling h i i which is the estimate of the k factor of the distribution of h s i f since h s i f is the channel between two antennas at los placed at distance d m the cdf of k s i should be noticeably to the left of the cdf of k i i and this is verified by results in fig next we characterize the k factor of the self interference channel after active cancellation and verify the following result result 4 the k factor for the self interference channel reduces due to active cancellation and the amount of reduc tion increases as the self interference cancellation increases hence the k factor for the self interference channel after active cancellation depends on the k factor value before active cancellation and the suppression achieved by active cancellation the cdf of k ac i and the cdf of k acdc i for our f and experiments are shown in fig results in fig show that h φ i f as ricean is a good fit to put these results in the k factor before active cancellation is larger than the k more perspective we also show results for the kl distance factor after active cancellation hence the k factor for the between the histogram of channel estimate magnitudes and a self interference channel reduces due to active cancellation rayleigh distribution which is a ricean distribution with k this reduction in k factor is a function of the suppression factor equal to zero we observe that ricean is a better fit achieved by active cancellation this is verified by experiment than rayleigh because the kl distances are lower for ricean results shown in fig where we plot the reduction in k we first characterize the k factor for the self interference factor due to active cancellation mechanism φ this reduction channel before active cancellation fig shows the cdf of is computed as k i i k i i db k φ i db as a function of the for our experiments for d equal to cm cm and average amount of suppression α φ i cm each of these three cdfs is the result of a total of for φ acdcl as the suppression increases the reduction in k factor also 4304 ieee transactions on wireless communications vol no december 100 90 k i i d cm k i i d node j to node i and node i computes an estimate of j n f cm which we label as j n f the aevms per frame transmitted a i c 70 k i i d cm k s i k ac i k acdc i from node j to node i is estimated as aevms i f n f the sinr for frame f b a r o t 60 received at node i is estimated as sinr i f aevms i f and the achievable rate for frame f received at node i is c a f k estimated as ar i rate for 0 k factor db fig cdf of k factor we aggregate k i i f transmission to log node i sinr i f the achievable y t i l i is computed by averaging b a b o 30 over all the achievable rates per frame and it is equal to ar i f the achievable r p sum rate of the full duplex two way system is computed by averaging over all the achievable sum rates per frame and it 0 is equal to asr f log in our experiments we used symbols sinr f and different values of p t to the cdf of k ac i values computed for i n f and s n f that were modulated using qpsk how obtain the cdf i for different and values k acdc i of d by and aggregating p t of k i i for a fixed d we obtained ever constellation s notice that the aevms value is independent of the the k factors computed for size and shape chosen for symbols s n f and n f this was also discussed in the numerator in the computation of sinr i 22 26 30 34 36 f is equal to one because we are using φ ac experiments φ acdc experiments linear fit a normalized constellation if the average energy per symbol was not equal to one then sinr i by a normalization factor α φ i fig reduction in k factor as a function of the average amount of cancellation increases reasons for result 4 active cancellation is based on estimation of the self interference channel before active cancellation the self interference channel has a strong los component hence an estimate of the self interference channel before active cancellation is virtually an estimate of the strong los component of this channel consequently most of the cancellation applied by active cancellation corresponds to attenuation of the strong los component that is present before active cancellation and as the suppression achieved by active cancellation increases the attenuation of the los component also increases v achievable rates a computation of achievable rates we compute the achievable rate based on the sinr per frame which is estimated based on the average error vector magnitude squared aevms per frame in our two node experiments the aevms per frame transmitted from node j to node i is estimated as follows symbol s j f would have to be scaled a natural question that arises is the following can the full duplex systems evaluated in our experiments achieve larger rates than a half duplex system in order to answer k 30 this question we also ran experiments for a two way half k i i duplex alamouti system this system uses two antennas two transmitter radios and one receiver radio per node hence it uses the same antenna and radio re sources per node as the full duplex systems we have im plemented for the two way half duplex system the nodes db time share the link with of the time dedicated for transmission from node to node and of the time dedicated for transmission from node to node the achievable sum rate for the half duplex system is com puted as asr b achievable rates with increasing power result if the signal to interference ratio before active cancellation at node i sir as i is maintained constant while the average received self interference power at node i p ri i is increased then the achievable rate for transmission to node i increases experiment results that verify result are shown in fig where each curve corresponds to an approximately constant value of sir as i experiments were performed with both nodes using the same transmission power p t the equation for sir as i dbm p ri i can be written as sir as i db p rs i dbm where p rs i is the average power of the received signal of interest at node i received powers p rs i dbm and p ri i dbm are both proportional to p t we were able to increase p ri i constant by increasing p t while keeping sir as i at both nodes by the same amount for each curve in fig the majority of the data points show that although the value of sir as i is approximately constant the achievable rate for n f is sent from transmission to node i is increasing as p ri i increases nsym f log nsym n sinr n frames s j n frames f n f s j f n frames n frames log n frames f n frames f sinr i log log sinr sinr duarte et al experiment driven characterization of full duplex wireless systems 0 0 15 fig 9 experiment results showing the achievable rate for transmission to node i as a function of the average received self interference power at node i for an approximately constant value of sir as i all the results correspond to a full duplex system with active analog cancellation active digital cancellation a distance d cm between self interfering antennas and a distance of m between node and node both nodes used the same transmission power p t and by increasing we were able to increase p t the exact values p ri i while keeping sir as i constant are shown in parenthesis as a pair of sir as i and p t for each data point to each data point each data point of values corresponds sir to as i an db frame p t dbm next experiment and for each node we show two curves because we did the same frame experiment twice the second experiment was done a few days after the first experiment hence the conditions surrounding the setup were not exactly the same for the two experiments due movement of people in the laboratory and this explains why the rates at a node are different between experiments reasons for result result can be explained using our derived equations and result as follows in result of section iv a we showed that α φ i increases as p ri i increases in section ii e we obtained that the sinr at node i when using active self interference cancellation mechanism φ is given by sinr φ i since sir following as i db p rs i dbm p ri i dbm if sir as i we observe the remains constant while p ri i increases then this means that p rs i is increasing and the rate of increase of p rs i is the same rate of increase as p ri i hence if sir as i remains constant while p ri i increases then the terms in the equation for sinr φ i that are changing are α φ i and snr i and they are both increasing consequently sinr i increases and this results in an increase in achievable rate notice that if the transmission power at both nodes in a two way full duplex system is increased by the same amount then sir as and sir as will not change and p ri and p ri will increase hence as can be concluded from result the achievable rate in both directions of the link will increase hence result leads to the following design rule for two way full duplex design rule rate power increase in a two way full duplex system increasing the transmission power at both nodes by the same amount results in an increase of the achievable rate in both directions of the link and this increases the achievable sum rate of the full duplex system node i at sir experiment node i at sir experiment node i at sir experiment node i at sir experiment 1 0 7 15 4 0 5 5 0 15 0 13 5 13 9 13 5 α φ i sir as i 1 snr i d cm experiments fd acdc d cm experiments fd ac d cm experiments hd d cm experiments fd acdc d cm experiments fd ac d cm experiments hd d cm experiments fd acdc d cm experiments fd ac d cm experiments hd fig experiment results showing an increase in the achievable sum rate as a function of the transmission power for a full duplex two way system using results the for same half duplex transmission experiments power where p t at both nodes the figure also shows the transmit power per antenna was set equal to p t hence for each p t value the systems compared have the same average transmitted power per node each data point corresponds to two frame experiments whose results were averaged to obtain one data point experiment results that verify our design rule 1 are shown in fig all results in fig correspond to a distance between nodes equal to 5 m and we show results for the three different values of d considered in our experiments for the full duplex systems we show results for the case where both nodes use active analog cancellation and active digital cancellation results labeled as fd acdc and for the case where both nodes use active analog cancellation and do not use digital cancellation results labeled as fd ac the majority of the data points for fd acdc and fd ac in fig show that the achievable sum rate of the full duplex system increases as the transmission power p t increases fig also shows results for half duplex experiments results labeled as hd as a function of the transmit power per antenna which was set equal to p t the half duplex results for different values of d look very similar because all the values of d are greater than half a wavelength for a fixed distance between nodes the antenna separation at a node will yield the same channel statistics between nodes independent channels for each transmit receive antenna pair for values of d larger than half a wavelength we observe that for d cm the half duplex system achieves larger rates than the fd ac and fd acdc systems for d 20 cm and d cm the full duplex systems achieve larger rates than the half duplex system we conclude that although a total self interference cancellation of db does not bring the self interference down to the noise floor a cancellation of db is enough to achieve full duplex gains over half duplex at a distance between nodes of 5 m for d cm the total cancellation was equal db and we observe that this total cancellation ________________ 4306 ieee transactions on wireless communications vol no december node i 1 node i node i 1 node i 2 node i 1 node i 2 node i 1 node i 2 fig experiment results showing the effect of different cancellation schemes on the achievable rate per node of a full duplex two way system using the same transmission power p t at both nodes results correspond to d 20 cm and a distance between node 1 and node 2 equal to 8 5 m each bar corresponds to results from two frame experiments is not enough to achieve full duplex gains over half duplex at 8 5 m between nodes c achievable rates with selective digital cancellation from results in fig 5 we observed that there are frames where applying digital cancellation after analog cancella tion reduces the self interference α dc i f db 0 but there are also frames where it increases the self interference α dc i f db 0 we would like to apply digital cancella tion only during frames where it reduces the self interference notice that training signals can be used to estimate α dc i f using α dc i f estimated based on training node i can decide if digital cancellation should be applied to frame f based on the following design rule design rule 2 selective digital cancellation if α dc i f db 0 then apply digital cancellation to the payload received at node i during frame f otherwise do not apply digital cancellation to the payload received at node i during frame f we performed experiments for a full duplex system with analog cancellation and the frames received were stored and post processed in the following three different ways 1 we did not apply digital cancellation to the received payload and this payload was used to compute the achievable rate of a full duplex system with analog cancellation fd ac 2 we ap plied digital cancellation to the received payload of all frames and the resulting payload was used to compute the achievable rate of a full duplex system with analog cancellation and digital cancellation fd acdc at each node we applied digital cancellation to the received payload selectively based on design rule 2 and the resulting payload was used to compute the achievable rate of a full duplex system with analog cancellation and selective digital cancellation fd acsdc fig shows achievable rate results for these three full duplex systems each thick bar in fig shows the achievable rate averaged over all frames the lower end of each thin bar in fig indicates the lowest achievable rate per p t 0 dbm p t 5 dbm p t dbm p t 15 dbm frame obtained for the corresponding full duplex system and the upper end of each thin bar indicates the largest achievable rate per frame obtained for the corresponding full duplex system hence thin bars show the range of achievable rate values observed for each full duplex system and thick bars show the average over all the achievable rate values per frame by comparing the thick bars in fig we conclude that the best full duplex system is the one that applies digital cancellation selectively based on design rule 2 by comparing the lower end of the thin bars in fig we observe that the lowest achievable rate per frame tends to be higher for the systems that use digital cancellation the reason is that for the frames where analog cancellation has poor performance leading to a low achievable rate of the fd ac system for those frames digital cancellation can increase the total self interference suppression and this leads to a larger achievable rate for the fd acdc and fd acsdc systems for those frames hence digital cancellation is an excellent safety net for the frames where analog cancellation delivers poor suppression a comparison of the upper end of the thin bars in fig shows that applying digital cancellation all the time can sometimes result in a reduction of the achievable rate but applying digital cancellation selectively based on design rule 2 avoids applying digital cancellation in frames where it would result in a decrease in achievable rate vi conclusions in order to advance the theory of full duplex systems it is important to have signal models based on actual measure ments we contribute to this area by providing a characteriza tion of the distribution of the self interference before and after active cancellation mechanisms we are the first ones to report a statistical characterization of the self interference based on extensive measurements we have characterized the effect of increasing self interference and transmission power on the rate and cancel lation performance of our full duplex implementation such a characterization while fundamental for the future deployment of full duplex systems is seldom provided in related work on full duplex implementation we are the first to characterize the performance of digital cancellation as a function of the performance of analog can cellation for a full duplex implementation the current belief in the literature has been that digital cancellation will always help improve the total cancellation further it has some times been conjectured that the total analog plus digital cancellation would be equal to the sum of the cancellations measured independently in isolation we demonstrate that when digital cancellation is preceded by analog cancellation the amount of digital cancellation varies as a function of the amount of analog cancellation thus only a full system implementation can reveal the true benefits of combined analog and digital cancellation in an actual system finally while our results are based on a single implementa tion of one full duplex system we believe our results analyze fundamental characteristics of full duplex systems that had not been analyzed before 