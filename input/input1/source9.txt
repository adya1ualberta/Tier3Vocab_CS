genomic mapping and mapping databases peter s white department of pediatrics university of pennsylvania philadelphia pennsylvania tara c matise department of genetics rutgers university new brunswick new jersey a few years ago only a handful of ready made maps of the human genome existed and these were low resolution maps of small areas biomedical researchers wishing to localize and clone a disease gene were forced by and large to map their region of interest a time consuming and painstaking process this situation has changed dramatically in recent years and there are now high quality genome wide maps of several different types containing tens of thousands of dna markers with the pend ing availability of a finished human sequence most efforts to construct genomic maps will come to a halt however integrated maps genome catalogues and com prehensive databases linking positional and functional genomic data will become even more valuable genome projects in other organisms are at various stages rang ing from having only a handful of available maps to having a complete sequence by taking advantage of the available maps and dna sequence a researcher can in many cases focus in on a candidate region by searching public mapping databases in a matter of hours rather than by performing laboratory experiments over a course of months subsequently the researcher burden has now shifted from mapping the genome to navigating a vast terra incognita of web sites ftp servers and databases there are large databases such as the national center for biotechnology information ncbi entrez genomes division genome database gdb and mouse genome database mgd smaller databases serving the primary maps published by genome centers sites sponsored by individual chromosome committees and sites used by smaller laboratories to publish highly detailed maps of specific regions each type of resource contains information that is valuable in its own right even when it overlaps with the information found at others finding one way around this infor mation space is not easy a recent search for the word genome using the altavista web search engine turned up potentially relevant documents this chapter is intended as a map of the maps a way to guide readers through the maze of publicly available genomic mapping resources the different types of markers and methods used for genomic mapping will be reviewed and the inherent complexities in the construction and utilization of genome maps will be discussed several large community databases and method specific mapping projects will be presented in detail finally practical examples of how these tools and resources can be used to aid in specific types of mapping studies such as localizing a new gene or refining a region of interest will be provided a complete description of the mapping resources available for all species would require an entire book therefore this chap ter focuses primarily on humans with some references to resources for other organisms interplay of mapping and sequencing the recent advent of whole genome sequencing projects for humans and select model organisms is dramatically impacting the use and utility of genomic map based in formation and methodologies genomic maps and dna sequence are often treated as separate entities but large uninterrupted dna sequence tracts can be thought of and used as an ultra high resolution mapping technique traditional genomic maps that rely on genomic markers and either clone based or statistical approaches for ordering are precursory to finished and completely annotated dna sequences of whole chromosomes or genomes however such completed genome sequences are predicted to be publicly available only in for humans for the mouse and even later for other mammalian species although complete sequences are now avail able for some individual human chromosomes and selected lower eukaryotes see chapter until these completed sequences are available mapping and sequencing approaches to genomic analysis serve as complementary approaches for chromosome analysis before determination of an entire chromosome sequence the types of se quences available can be roughly grouped into marker gene based tags e g ex pressed sequence tags ests and sequence tagged sites stss single gene se quences prefinished dna clone sequences and completed continuous genomic sequence tracts the first two categories provide rich sources of the genomic markers used for mapping but only the last two categories can reliably order genomic ele ments the human genome draft sequence is an example of a prefinished sequence in which of the entire sequence is available but most continuous sequence tracts are relatively short usually kb and often kb thus providing high local resolution but little long range ordering information genomic maps can help provide a context for this sequence information thus two or more sequences con taining unique genomic markers can be oriented if these markers are ordered on a map in this way existing maps serve as a scaffold for orienting directing and troubleshooting sequencing projects similarly users can first define a chromosomal region of interest using a traditional map approach and then can identify relevant dna sequences to analyze by finding long sequences containing markers mapping within the defined region ncbi tools such as blast and electronic pcr e pcr are valuable for finding marker sequence identities and several of the resources discussed below provide marker sequence integration as large sequence tracts emerge from the human and model organism projects sequence based ordering of genomic landmarks will eventually supplant map based ordering methods the evolution from a mapped chromosome to the determination of the chromosome complete sequence is marked by increasing incorporation of partial genomic sequence tracts into the underlying map once complete finished sequences can be used to confirm map determined marker orders given the error rates inherent in both map and sequence assembly methodology it is good practice to use both map and sequence information simultaneously for independent verifica tion of regional order genomic map elements dna markers a dna marker is simply a uniquely identifiable segment of dna there are several different types of markers usually ranging in size from one to nucleotide bases in size markers can be thought of as landmarks and a set of markers whose relative positions or order within a genome are known comprises a map markers can be categorized in several ways some markers are polymorphic and others are not monomorphic detection of markers may be either pcr based or hybridization based some markers lie in a sequence of dna that is expressed some do not or their expression status may be unknown pcr based markers are commonly referred to as sequence tagged sites stss an sts is defined as a segment of genomic dna that can be uniquely pcr amplified by its primer sequences stss are commonly used in the construction of physical maps sts markers may be developed from any genomic sequence of interest such as from characterized and sequenced genes or from expressed sequence tags ests chapter alternatively stss may be randomly identified from total genomic dna the est database dbest at ncbi stores information on most sts markers polymorphic markers polymorphic markers are those that show sequence variation among individuals polymorphic markers are used to construct genetic linkage maps the number of alleles observed in a population for a given polymorphism which can vary from two to determines the degree of polymorphism for many studies highly polymor phic markers alleles are most useful polymorphisms may arise from several types of sequence variations one of the earlier types of polymorphic markers used for genomic mapping is a restriction fragment length polymorphism rflp an rflp arises from changes in the se quence of a restriction enzyme recognition site which alters the digestion pat terns observed during hybridization based analysis another type of hybridization based marker arises from a variable number of tandem repeat units vntr a vntr locus usually has several alleles each containing a different number of copies of a common motif of at least nucleotides tandemly oriented along a chromosome a third type of polymorphism is due to tandem repeats of short sequences that can be detected by pcr based analysis these are known variously as microsatellites short tandem repeats strs str polymorphisms strps or short sequence length polymorphisms sslps these repeat sequences usually consist of two three or four nucleotides and are plentiful in most organisms all pcr converted str mark ers those for which a pair of oligonucleotides flanking the polymorphic site suitable for pcr amplification of the locus has been designed are considered to be stss the advent of pcr based analysis quickly made microsatellites the markers of choice for mapping another polymorphic type of pcr based marker is a single nucleotide poly morphism snp which results from a base variation at a single nucleotide position most snps have only two alleles biallelic because of their low heterozygosity maps of snps require a much higher marker density than maps of microsatellites snps occur frequently in most genomes with one snp occurring on average ap proximately once in every bases in humans snps lend themselves to highly automated fluidic or dna chip based analyses and have quickly become the focus of several large scale development and mapping projects in humans and other organisms further details about all of these types of markers can be found elsewhere chakravarti and lynn dietrich et al dna clones the possibility of physically mapping eukaryotic genomes was largely realized with the advent of cloning vehicles that could efficiently and reproducibly propagate large dna fragments the first generation of large insert cloning was made possible with yeast artificial chromosome yac libraries burke et al because yacs can contain fragments up to mb they are suitable for quickly making low resolution maps of large chromosomal regions and the first whole genome physical maps of several eukaryotes were constructed with yacs however although yac libraries work well for ordering stss and for joining small physical maps the high rate of chimerism and instability of these clones makes them unsuitable for dna sequencing the second and current generation of large insert clones consists of bacterial artificial chromosomes bacs and artificial chromosomes both of which act as episomes in bacterial cells rather than as eukaryotic artificial chromosomes bacterial propagation has several advantages including higher dna yields ease of use for sequencing and high integrity of the insert during propagation as such despite the relatively limited insert sizes usually kb bacs and pacs have largely replaced yacs as the clones of choice for large genome mapping and sequencing projects iaonnou et al shizuya et al dna fingerprinting has been applied to bacs and pacs to determine insert overlaps and to construct clone con tigs in this technique clones are digested with a restriction enzyme and the resulting fragment patterns are compared between clones to identify those sharing subsets of identically sized fragments in addition the ends of bac and pac inserts can be directly sequenced clones whose insert end sequences have been determined are referred to as sequence tagged clones stcs both dna fingerprinting and stc generation now play instrumental roles in physical mapping strategies as will be discussed below types of maps cytogenetic maps cytogenetic maps are those in which the markers are localized to chromosomes in a manner that can be directly imaged traditional cytogenetic mapping hybridizes a radioactively or fluorescently labeled dna probe to a chromosome preparation usu ally in parallel with a chromosomal stain such as giemsa which produces a banded karyotype of each chromosome pinkel et al this allows assignment of the probe to a specific chromosomal band or region assignment of cytogenetic positions in this manner is dependent on some subjective criteria variability in technology methodology interpretation reproducibility and definition of band boundaries thus inferred cytogenetic positions are often fairly large and occasionally overin terpreted and some independent verification of cytogenetic position determinations is warranted for crucial genes markers or regions probes used for cytogenetic map ping are usually large insert clones containing a gene or polymorphic marker of interest despite the subjective aspects of cytogenetic methodology karyotype anal ysis is an important and relatively simple clinical genetic tool thus cytogenetic positioning remains an important parameter for defining genes disease loci and chromosomal rearrangements newer cytogenetic techniques such as interphase fluorescence in situ hybridi zation fish lawrence et al and fiber fish parra and windle instead examine chromosomal preparations in which the dna is either naturally or mechanically extended studies of such extended chromatin have demonstrated a directly proportional relationship between the distances measured on the image and the actual physical distance for short stretches so that a physical distance between two closely linked probes can be determined with some precision van den engh et al however these techniques have a limited ordering range mb and are not well suited for high throughput mapping genetic linkage maps genetic linkage gl maps also called meiotic maps rely on the naturally occurring process of recombination for determination of the relative order of and map distances between polymorphic markers crossover and recombination events take place dur ing meiosis and allow rearrangement of genetic material between homologous chro mosomes the likelihood of recombination between markers is evaluated using gen otypes observed in multigenerational families markers between which only a few recombination occur are said to be linked and such markers are usually located close to each other on the same chromosome markers between which many recombina tions take place are unlinked and usually lie far apart either at opposite ends of the same chromosome or on different chromosomes because the recombination events cannot be easily quantified a statistical method of maximum likelihood is usually applied in which the likelihood of two markers being linked is compared with the likelihood of being unlinked this like lihood ratio is called a lod score for log of the odds and a lod score greater than corresponding to odds of or greater is usually taken as evidence that markers are linked the lod score is computed at a range of recombination fraction values between markers from to and the recombination fraction at which the lod score is maximized provides an estimate of the distance between markers a map function usually either haldane or kosambi is then used to convert the recombination fraction into an additive unit of distance measured in centimorgans cm with cm representing a probability that a recombination has occurred between two markers on a single chromosome because recombination events are not randomly distributed map distances on linkage maps are not directly proportional to physical distances the majority of linkage maps are constructed using multipoint linkage analysis although multiple pairwise linkage analysis and minimization of recombination are also valid approaches commonly used and publicly available computer programs for building linkage maps include linkage lathrop et al cri map lander and green multimap matise et al mapmaker lander et al and map collins et al the map o mat web server is available for estimation of map distances and for evaluation of statistical support for order matise and gitlin because linkage mapping is a based on statistical methods linkage maps are not guaranteed to show the correct order of markers therefore it is important to be critical of the various available maps and to be aware of the statistical criteria that were used in map construction typically only a subset of markers framework or index markers is mapped with high statistical support the remainder are either placed into well supported intervals or bins or placed into unique map positions but with low statistical support for order see additional discussion below to facilitate global coordination of human linkage mapping dnas from a set of reference pedigrees collected for map construction were prepared and distributed by the centre d etude du polymorphism humain ceph dausset et al nearly all human linkage maps are based on genotypes from the ceph reference pedigrees and genotypes for markers scored in the ceph pedigrees are deposited in a public database maintained at ceph most recent maps are composed almost entirely of highly polymorphic str markers these linkage maps have already ex ceeded the maximum map resolution possible given the subset of ceph pedigrees that are commonly used for map construction and no further large scale efforts to place str markers on human linkage maps are planned thousands of snps are currently being identified and characterized and a subset are being placed on linkage maps wang et al linkage mapping is also an important tool in experimental animals with many maps already produced at high resolution and others still under development see mapping projects and associated resources below radiation hybrid maps radiation hybrid rh mapping is very similar to linkage mapping both methods rely on the identification of chromosome breakage and reassortment the primary difference is the mechanism of chromosome breakage in the construction of radia tion hybrids breaks are induced by the application of lethal doses of radiation to a donor cell line which is then rescued by fusion with a recipient cell line typically mouse or hamster and grown in a selective medium such that only fused cells survive an rh panel is a library of fusion cells each of which has a separate collection of donor fragments the complete donor genome is represented multiple times across most rh panels each fusion cell or radiation hybrid is then scored by pcr to determine the presence or absence of each marker of interest markers that physically lie near each other will show similar patterns of retention or loss across a panel of rh cells and behave as if they are linked whereas markers that physically lie far apart will show completely dissimilar patterns and behave as if they are unlinked because the breaks are largely randomly distributed the break frequen cies are roughly directly proportional to physical distances the resulting data set is a series of positive and negative pcr scores for each marker across the hybrid panel these data can be used to statistically infer the position of chromosomal breaks and from that point on the procedures for map construction are similar to those used in linkage mapping a map function is used to convert estimates of breakage frequency to additive units of distance measured in centirays cr with cr rep resenting a probability that a chromosomal break has occurred between two markers in a single hybrid the resolution of a radiation hybrid map depends on the size of the chromosomal fragments contained in the hybrids which in turn is pro portional to the amount of irradiation to which the human cell line was exposed most rh maps are built using multipoint linkage analysis although multiple pairwise linkage analysis and minimization of recombination are also valid ap proaches three genome wide rh panels exist for humans and are commercially available and rh panels are available for many other species as well widely used computer programs for rh mapping are rhmap boehnke et al rhmap per slonim et al and multimap matise et al and on line servers that allow researchers to place their rh mapped markers on existing rh maps are available the radiation hybrid database rhdb is the central repository for rh data on panels available in all species the radiation hybrid information web site also contains multi species information about available rh panels maps ongoing projects and available computer programs transcript maps of particular interest to researchers chasing disease genes are maps of transcribed sequences although the transcript sequences are mapped using one of the methods described in this section and thus do not require a separate mapping technology they are often set apart as a separate type of map these maps consist of expressed sequences and sequences derived from known genes that have been converted into stss and usually placed on conventional physical maps recent projects for creating large numbers of ests adams et al houlgatte et al hillier et al have made tens of thousands of unique expressed sequences available to the mapping laboratories transcribed sequence maps can significantly speed the search for candidate genes once a disease locus has been identified the largest human transcript map to date is the genemap described below physical maps physical maps include maps that either are capable of directly measuring distances between genomic elements or that use cloned dna fragments to directly order el ements many techniques have been created to develop physical maps the most widely adopted methodology due largely to its relative simplicity is sts content mapping green and olson this technique can resolve regions much larger than mb and has the advantage of using convenient pcr based positional markers in sts content maps sts markers are assayed by pcr against a library of large insert clones if two or more stss are found to be contained in the same clone chances are high that those markers are located close together the fact that they are not close of the time is a reflection of various artifacts in the mapping procedure such as the presence of chimeric clones the sts content mapping tech nique builds a series of contigs i e overlapping clusters of clones joined together by shared stss the resolution and coverage of such a map are determined by a number of factors including the density of stss the size of the clones and the depth of the clone library maps that use cloning vectors with smaller insert sizes have a higher theoretical resolution but require more stss to achieve coverage of the same area of the genome although it is generally possible to deduce the relative order of markers on sts content maps the distances between adjacent markers cannot be measured with accuracy without further experimentation such as by re striction mapping however sts content maps have the advantage of being asso ciated with a clone resource that can be used for further studies including subcloning dna sequencing or transfection several other techniques in addition to sts content and radiation hybrid map ping have also been used to produce physical maps clone maps rely on techniques other than sts content to determine the adjacency of clones for example the ceph yac map see below used a combination of fingerprinting inter alu product hy bridization and sts content to create a map of overlapping yac clones finger printing is commonly used by sequencing centers to assemble and or verify bac and pac contigs before clones are chosen for sequencing to select new clones for sequencing that can extend existing contigs and to help order genomic sequence tracts generated in whole genome sequencing projects chumakov et al se quencing of large insert clone ends stc generation when applied to a whole genome clone library of adequate coverage is very effective for whole genome map ping when used in combination with fingerprinting of the same library deletion and somatic cell hybrid maps relying on large genomic reorganizations induced delib erately or naturally occurring to place markers into bins defined by chromosomal breakpoints have been generated for some human chromosomes jensen et al lewis et al roberts et al vollrath et al optical mapping visualizes and measures the length of single dna molecules extended and digested with restriction enzymes by high resolution microscopy this technique although still in its infancy has been successfully used to assemble whole chromosome maps of bacteria and lower eukaryotes and is now being applied to complex genomes aston et al jing et al schwartz et al comparative maps comparative mapping is the process of identifying conserved chromosome segments across different species because of the relatively small number of chromosomal breaks that have occurred during mammalian radiation the order of genes usually is preserved over large chromosomal segments between related species orthologous genes copies of the same genes from different species can be identified through dna sequence homology and sets of orthologous genes sharing an identical linear order within a chromosomal region in two or more species are used to identify conserved segments and ancient chromosomal breakpoints knowledge about which chromosomal segments are shared and how they have become rearranged over time greatly increases our understanding of the evolution of different plant and animal lineages one of the most valuable applications of com parative maps is to use an established gene map of one species to predict positions of orthologous genes in another species many animal models exist for diseases observed in humans in some cases it is easier to identify the responsible genes in an animal model than in humans and the availability of a good comparative map can simplify the process of identifying the responsible genes in humans in other cases more might be known about the gene responsible in humans and the same comparative map could be used to help identify the gene responsible in the model species there are several successful examples of comparative candidate gene map ping o brien et al as mapping and sequencing efforts progress in many species it is becoming possible to identify smaller homologous chromosome segments and detailed com parative maps are being developed between many different species fairly dense gene based comparative maps now exist between the human mouse and rat genomes and also between several agriculturally important mammalian species sequence and protein based comparative maps are also under development for several lower or ganisms for which complete sequence is available chapter a comparative map is typically presented either graphically or in tabular format with one species des ignated as the index species and one or more others as comparison species homol ogous regions are presented graphically with nonconsecutive segments from the com parison species shown aligned with their corresponding segments along the map of the index species integrated maps map integration provides interconnectivity between mapping data generated from two or more different experimental techniques however achieving accurate and useful integration is a difficult task most of the genomic maps and associated web sites discussed in this section provide some measure of integration ranging from the approximate cytogenetic coordinates provided in the ge ne thon gl map to the inter associated gl rh and physical data provided by the whitehead institute wicgr web site several integration projects have created truly integrated maps by placing genomic elements mapped by differing techniques relative to a single map scale the most advanced sources of genomic information provide some level of genomic cataloguing where considerable effort is made to collect organize and map all available positional information for a given genome complexities and pitfalls of mapping it is important to realize that the genomic mapping information currently available is a collection of a large number of individual data sets each of which has unique characteristics the experimental techniques methods of data collection annotation presentation and quality of the data differ considerably among these data sets al though most mapping projects include procedures to detect and eliminate and or correct errors there are invariably some errors that occur which often result in the incorrect ordering or labeling of individual markers although the error rate is usually very low or less a marker misplacement can obviously have a great impact on a study a few mapping web sites are beginning to flag and correct or at least warn users of potential errors but most errors cannot be easily detected successful strat egies for minimizing the effects of data error include simultaneously assessing as many different maps as possible to maximize redundancy note that ideally dif ferent maps use independently derived data sets or different techniques in creased emphasis on utilizing integrated maps and genomic catalogues that provide access to all available genomic information for the region of interest while closely monitoring the map resolution and marker placement confidence of the integrated map and if possible experimentally verifying the most critical marker positions or placements in addition to data errors several other more subtle complexities are notable foremost is the issue of nomenclature or the naming of genomic markers and ele ments many markers have multiple names and keeping track of all the names is a major bioinformatics challenge for example the polymorphic marker has several assigned names which is actually the name of the dna clone from which this polymorphism was identified shgc and two ex amples of genome centers renaming a marker to fit their own nomenclature schemes and both gdb and gdb which are database identifier numbers used to track the polymorphism and sts associated with this marker respectively in the genome database gdb genomic mapping groups working with a particular marker often assign an additional name to simplify their own data management but too often these alternate identifiers are subsequently used as a primary name fur thermore many genomic maps display only one or a few names making comparisons of maps problematic mapping groups and web sites are beginning to address these inherent problems but the difficulty of precisely defining markers genes and genomic elements adds to the confusion it is important to distinguish between groups of names defining different elements a gene can have several names and it can also be associated with one or more est clusters polymorphisms and stss genes spanning a large genomic stretch can even be represented by several markers that individually map to different positions web sites providing genomic catalogu ing such as locuslink unigene gdb genecards and egenome list most names associated with a given genomic element nevertheless collecting cross referencing and frequently updating one own sets of names for markers of interest is also a good practice see chapter for data management using sequin as even the ge nomic cataloguing sites do not always provide complete nomenclature collections each mapping technique yields its own resolution limits cytogenetic banding potentially orders markers separated by mb and genetic linkage gl and rh analyses yields long range resolutions of mb although localized order ing can achieve higher resolutions the confidence level with which markers are ordered on statistically based maps is often overlooked but this is crucial for as sessing map quality for genomes with abundant mapping data such as human or mouse the number of markers used for mapping often far exceeds the ability of the technique to order all markers with high confidence often confidence levels of or lod are used as a cutoff which usually means that a marker is times more likely to be in the given position than in any other mappers have taken two approaches to address this issue the first is to order all markers in the best possible linear order regardless of the confidence for map position of each marker examples include genemap and the genetic location database collins et al deloukas et al alternatively the high confidence linear order of a subset of markers is determined and the remaining markers are then placed in high confidence intervals or regional positions such as ge ne thon shgc and egenome dib et al stewart et al white et al the advantage of the first approach is that resolution is maximized but it is important to pay attention to the odds for placement of individual markers as alternative local orders are often almost equally likely thus beyond the effective resolving power of a mapping technique increased resolution often yields decreased accuracy and re searchers are cautioned to strike a healthy balance between the two each mapping technique also yields very different measures of distance cyto genetic approaches with the exception of high resolution fiber fish provide only rough distance estimates gl and sts content mapping provide marker orientation but only relative distances and rh mapping yields distances roughly proportional to true physical distance for gl analysis unit measurements are in centmiorgans with cm equivalent to a chance of recombination between two linked markers the conversion factor of cm mb is often cited for the human genome but is overstated as this is just the average ratio genome wide and many chromosomal regions have recombination hotspots and coldspots in which the cm to mb ratio varies as much as fold in general cytogenetic maps provide subband marker regionalization but limited localized ordering gl and sts content maps provide excellent ordering and limited to moderate distance information and rh maps pro vide the best combination of localized ordering and distance estimates finally there are various levels at which genomic information can be presented single resource maps such as the ge ne thon gl maps use a single experimental technique and analyze a homogeneous set of markers strictly comparative maps make comparisons between two or more different single dimension maps either within or between species but without combining data sets for integration gdb mapview program can display multiple maps in this fashion letovsky et al integrated maps recalculate or completely integrate multiple data sets to display the map position of all genomic elements relative to a single scale gdb comprehen sive maps are an example of such integration letovsky et al lastly genome cataloguing is a relatively new way to display genomic information in which many data sets and or web sites are integrated to provide a comprehensive listing and or display of all identified genomic elements for a given chromosome or genome com pletely sequenced genomes such as c elegans and s cerevisiae have advanced cataloguing efforts see chapter but catalogues for complex genome organisms are in the early stages examples include the interconnected ncbi databases mgd and egenome blake et al wheeler et al catalogues provide a one stop shopping solution to collecting and analyzing genomic data and are recom mended as a maximum impact means to begin a regional analysis however the individual data sets provide the highest quality positional information and are ulti mately the most useful for region definition and refinement data repositories there are several valuable and well developed data repositories that have greatly facilitated the dissemination of genome mapping resources for humans and other species this section covers three of the most comprehensive resources for mapping in humans the genome database gdb the national center for biotechnology information ncbi and the mouse genome database mgd more focused re sources are mentioned in the mapping projects and associated resources section of this chapter gdb the genome database gdb is the official central repository for genomic mapping data created by the human genome project pearson gdb central node is located at the hospital for sick children toronto ontario canada members of the scientific community as well as gdb staff curate data submitted to the gdb currently gdb comprises descriptions of three types of objects from humans ge nomic segments genes clones amplimers breakpoints cytogenetic markers fragile sites ests syndromic regions contigs and repeats maps including cytogenetic gl rh sts content and integrated and variations primarily relating to poly morphisms in addition contributing investigator contact information and citations are also provided the gdb holds a vast quantity of data submitted by hundreds of investigators therefore like other large public databases the data quality is variable a more detailed description of the gdb can be found in talbot and cuticchia gdb provides a full featured query interface to its database with extensive on line help several focused query interfaces and predefined reports such as the maps within a region search and lists of genes by chromosome report present a more intuitive entry into gdb in particular gdb mapview program provides a graph ical interface to the genetic and physical maps available at gdb a simple search is available on the home page of the gdb web site this query is used when searching for information on a specific genomic segment such as a gene or sts amplimer in gdb terminology and can be implemented by entering the segment name or gdb accession number depending on the type of segment queried and the available data many different types of segment specific information may be returned such as alternate names aliases primer sequences positions in various maps related segments polymorphism details contributor contact informa tion citations and relevant external links at the bottom of the gdb home page is a link to other search options from the other search options page there are links to three customized search forms markers and genes within a region maps within a region and genes by name or symbol sequence based searches specific search forms for subclasses of gdb elements and precompiled lists of data genetic diseases by chromosome lists of genes by chromosome and lists of genes by symbol name a particularly useful query is the maps within a region search this search allows retrieval of all maps stored in gdb that span a defined chromosomal region in a two step process the set of maps to be retrieved is first determined and from these the specific set to be displayed is then selected select the maps within a region link to display the search form to view an entire chromosome simply select it from the pop up menu however entire chro mosomes may take considerable time to download and display therefore it is usually best to choose a subchromosomal region to view a chromosomal region type the names of two cytogenetic bands or flanking genetic markers into the text fields labeled from and to an example query is shown in figure if the flanking markers used in the query are stored in gdb as more than one type of object the next form will request selection of the specific type of element for each marker for the example shown in figure it is appropriate to select amplimer the resulting form lists all maps stored in gdb that overlap the selected region given the flanking markers specified above there are a total of maps the user selects which maps to display by marking the respective checkboxes note that gdb comprehensive map is automatically selected if a graphical display is re quested the size of the region and the number of maps to be displayed can signifi cantly affect the time to fetch and display them the resulting display will appear in a separate window showing the selected maps in side by side fashion while the mapview display is loading a new page is shown in the browser window if your system is not configured to handle java properly a helpful message will be displayed in the browser window important do not close the browser window behind mapview because of an idiosyncrasy of java security specification the applet cannot interact properly with gdb unless the browser window remains open to safely exit the mapview display select exit from mapview file menu mapview has many useful options which are well described in the online help some maps have more than one tier each displaying different types of markers such as markers positioned with varying confidence thresholds on a linkage or radiation hybrid map it is possible to zoom in and out highlight markers across maps color code different tiers display markers using different aliases change the relative po sition of the displayed maps and search for specific markers to retrieve additional information on a marker from any of the maps double click on its name to perform a simple search as described above a separate browser window will then display the gdb entry for the selected marker two recently added gdb tools are gdb blast and e pcr these are available from the other search options page and enable users to employ gdb many data resources in their analysis of the emerging human genome sequence gdb blast returns gdb objects associated with blast hits against the public human sequence gdb e pcr finds which of its many amplimers are contained within queried dna sequences and is thereby a quick means to determine or refine gene or marker lo calization in addition the gdb has many useful genome resource web links on its resources page ncbi the ncbi has developed many useful resources and tools several of which are described throughout this book of particular relevance to genome mapping is the genomes division of entrez entrez provides integrated access to several different types of data for over organisms including nucleotide sequences protein struc tures and sequences pubmed medline and genomic mapping information the figure results of a maps within a region gdb query for the region with no limits applied to the types of maps to be retrieved twenty one maps were avail able for display only the genethon and marshfield linkage maps as well as the chromo some rh map were selected for graphical display markers that are shared across maps are connected by lines ncbi human genome map viewer is a new tool that presents a graphical view of the available human genome sequence data as well as cytogenetic genetic physical and radiation hybrid maps because the map viewer provides displays of the human genome sequence for the finished contigs the bac tiling path of finished and draft sequence and the location of genes stss and snps on finished and draft sequences it is an especially useful tool for integrating maps and sequence the only other organisms for which the map viewer is currently available is m musculus and d melanogaster the ncbi map viewer can simultaneously display up to seven maps that are selected from a set of including cytogenetic linkage rh physical and sequence based maps some of the maps have been previously published and others are being computed at ncbi an extensive set of help pages is available there are many different paths to the map vieweron the ncbi web site as described in the help pages the viewer supports genome wide or chromosome specific searches a good starting point is the homo sapiens genome view page this is reached from the ncbi home page by connecting to human genome resources listed on the right side followed by the link to the map viewer listed on the left side from the genome view page a genome wide search may be initiated using the search box at the top left or a chromosome specific search may be performed by entering a chromosome number in the top right search box or by clicking on a chromosome idiogram the searchable terms include gene symbol or name and marker name or alias the search results include a list of hits for the search term on the available maps clicking on any of the resulting items will bring up a graphical view of the region surrounding the item on the specific map that was selected for example a genome wide search for the term cmt returns hits representing the loci for forms of charcot marie tooth neuropathy on eight different chromosomes selecting the genes seq link for the gene the gene symbol for on chro mosome returns the view of the sequence map for the region surrounding this gene the display settings window can then be used to select simultaneous display of additional maps fig the second search box at the top right may be used to limit a genome wide search to a single chromosome or range of chromosomes alternatively to browse an entire chromosome click on the link below each idiogram doing so will return a graphical representation of the chromosome using the default display settings currently the default display settings select the sts map shows placement of stss using electronic pcr the genbank map shows the bac tiling path used for se quencing and the contig map shows the contig map assembled at ncbi from finished high throughput genomic sequence as additional maps to be displayed to select a smaller region of interest from the view of the whole chromosome either define the range using base pairs cytogenetic bands gene symbols or marker names in the main map viewer window or in the display settings or click on a region of interest from the thumbnail view graphic in the sidebar or the map view itself as with the gdb map views until all sequence is complete alignment of multiple maps and inference of position from one map to another must be judged cautiously and should not be overinterpreted see complexities and pitfalls of mapping section above there are many other tools and databases at ncbi that are useful for gene mapping projects including e pcr blast chapter the genemap see mapping projects and associated resources and the locuslink omim chapter dbsts dbsnp dbest chapter and unigene chapter databases e pcr and blast can be used to search dna sequences for the presence of markers and to confirm and refine map localizations in addition to est alignment infor mation and dna sequence unigene reports include cytogenetic and rh map lo cations the genemap is a good starting point for finding approximate map figure ncbi map view of the region surrounding the gene the ge ne thon sts and genes seq maps are displayed with lines connecting markers in common positions for est markers although additional fine mapping should be performed to confirm order in critical regions locuslink omim and unigene are good starting points for genome catalog information about genes and gene based markers locuslink pruitt et al presents information on official nomenclature aliases sequence accessions phenotypes ec numbers mim numbers unigene clusters homology map locations and related web sites the dbsts and dbest databases themselves play a lesser role in human and mouse gene mapping endeavors as their relevant information has already been captured by other more detailed resources such as locuslink genemap unigene mgd and egenome but are currently the primary source of genomic information for other organisms the dbsnp database stores population specific information on variation in humans primarily for single nucleotide repeats but also for other types of polymorphisms in addition the ncbi genomic biology page provides genomic resource home pages for many other or ganisms including mouse rat drosophila and zebrafish mgi mgd the mouse genome initiative database mgi is the primary public mouse genomic catalogue resource located at the jackson laboratory the mgi currently encom passes three cross linked topic specific databases the mouse genome database mgd the mouse gene expression database gxd and the mouse genome se quence project mgs the mgd has evolved from a mapping and genetics resource to include sequence and genome information and details on the functions and roles of genes and alleles blake et al mgd includes information on mouse ge netic markers and nomenclature molecular segments probes primers yacs and mit primers phenotypes comparative mapping data graphical displays of linkage cytogenetic and physical maps experimental mapping data and strain distribution patterns for recombinant inbred strains ris and cross haplotypes as of november there were over genetic markers and genes in mgd with and of these placed onto the mouse genetic map respectively over genes have been matched with their human ortholog and over matched with their rat ortholog genes are easily searched through the quick gene search box on the mgd home page markers and other map elements may also be accessed through several other search forms the resulting pages contain summary information such as ele ment type official symbol name chromosome map positions mgi accession id references and history additional element specific information may also be dis played including links to outside resources fig a thumbnail linkage map of the region is shown to the right which can be clicked on for an expanded view the mgd contains many different types of maps and mapping data including linkage data from different experimental cross panels and the wicgr mouse physical maps and cytogenetic band positions are available for some markers the mgd also computes a linkage map that integrates markers mapped on the various panels a very useful feature is the ability to build customized maps of specific regions using subsets of available data incorporating private data and showing ho mology information where available see comparative resources section below the mgd is storing radiation hybrid scores for mouse markers but to date no rh maps have been deposited at mgd mapping projects and associated resources in addition to the large scale mapping data repositories outlined in the previous section many invaluable and more focused resources also exist some of these are either not appropriate for storage at one of the larger scale repositories or have never been deposited in them these are often linked to specific mapping projects that primarily use only one or a few different types of markers or mapping approaches figure results of an mgd quick gene search for for most studies requiring the use of genome maps it remains necessary to obtain maps or raw data from one or more of these additional resources by visiting the resource specific sites outlined in this section it is usually possible to view maps in the form preferred by the originating laboratory download the raw data and review the laboratory protocols used for map construction cytogenetic resources cytogenetic based methodologies are instrumental in defining inherited and acquired chromosome abnormalities and especially gene based chromosomal mapping data is often expressed in cytogenetic terms however because cytogenetic markers are not sequence based and the technique is less straightforward and usually more sub jective than gl rh or physical mapping there is only a modicum of integration between chromosomal band assignments and map coordinates derived from other techniques in humans and very little or none in other species thus it is often difficult to determine the precise cytogenetic location of a gene or region useful human resources can be divided into displays of primary cytogenetic mapping data efficient methods of integrating cytogenetic and other mapping data and resources pertaining to specific chromosomal aberrations the central repository for human cytogenetic information is gdb which offers several ways to query for marker and map information using cytogenetic coordinates see above gdb is a useful resource for cross referencing cytogenetic positions with genes or regions of interest ncbi locuslink and unigene catalogues as well as their other integrated mapping resources are also valuable repositories of cytogenetic positions locuslink and ncbi online mendelian inheritance in man omim list cytogenetic positions for all characterized genes and genetic abnormal ities respectively mckusick pruitt et al the national cancer institute nci sponsored project to identify gl tagged bac clones at mb density through out the genome is nearing completion this important resource which is commer cially available both as clone sets and as individual clones provides the first complete integration of cytogenetic band information with other genome maps at this site bacs can be searched for individually by clone name band position or contained sts name and chromosome sets are also listed each clone contains one or more microsatellite markers and has gl and or rh mapping coordinates along with a fish determined cytogenetic band assignment this information can be used to quickly determine the cytogenetic position of a gene or localized region and to map a cytogenetic observation such as a tumor specific chromosomal rearrangement using the referenced gl and physical mapping reagents three earlier genome wide efforts to cytogenetically map large numbers of probes are complementary to the nci site the lawrence berkeley national labo ratory university of california san francisco resource for molecular cytogenetics has mapped large insert clones containing polymorphic and expressed markers using fish to specific bands and also with fractional length flpter coordinates in which the position of a marker is measured as a percentage of the length of the chromo some karyotype similarly the genetics institute at the university of bari italy and the max planck institute for molecular genetics have independently localized large numbers of clones mostly yacs containing gl mapped microsatellite markers onto chromosome bands by fish all three resources have also integrated the mapped probes relative to existing gl and or rh maps many data repositories and groups creating integrated genome maps list cyto genetic localizations for mapped genomic elements these include gdb ncbi the unified database udb the genetic location database ldb and egenome all of which infer approximate band assignments to many or all markers in their data bases these assignments rely on determination of the approximate boundaries of each band using subsets of their marker sets for which accurate cytogenetic mapping data are available the nci cancer chromosome aberration project ccap wheeler et al infobiogen wheeler et al the southeastern regional genetics group sergg and the coriell cell repositories all have web sites that display cytoge netic maps or descriptions of characterized chromosomal rearrangements these sites are useful resources for determining whether a specific genomic region is frequently disrupted in a particular disease or malignancy and for finding chromosomal cell lines and reagents for regional mapping however most of these rearrangements have only been mapped at the cytogenetic level nonhuman resources are primarily limited to displays or simple integrations of chromosome idiograms arkdb is an advanced resource for displaying chromosomes of many amniotes mgd incorporates mouse chromosome band assignments into queries of its database and the animal genome database has clickable chromosome idiograms for several mammalian genomes wada and yasue a recent work linking the mouse genetic and cytogenetic maps consists of bac clones dis tributed genome wide korenberg et al and an associated web site is available for this resource at the cedars sinai medical center genetic linkage map resources even with the sequence era approaching rapidly linkage maps remain one of the most valuable and widely used genome mapping resources linkage maps are the starting point for many disease gene mapping projects and have served as the back bone of many physical mapping efforts nearly all human linkage maps are based on genotypes from the standard ceph reference pedigrees there are three recent sets of genome wide gl maps currently in use all of which provide high resolution largely accurate and convenient mapping information these maps contain primarily the conveniently genotyped pcr based microsatellite markers use genotypes for only of the available ceph pedigrees and contain few if any gene based or cytogenetically mapped markers many chromosome specific linkage maps have also been constructed many of which use a larger set of ceph pedigrees and include hybridization and gene based markers over markers have been genotyped in the ceph pedigrees and these genotypes have been deposited into the ceph genotype database and are publicly available the first of the three genome wide maps was produced by the cooperative hu man linkage center chlc murray et al last updated in the chlc has identified genotyped and or mapped over microsatellite repeat markers the chlc web site currently holds many linkage maps including maps comprised solely of chlc derived markers and maps combining chlc markers with those from other sources including most markers in cephdb chlc markers can be recognized by unique identifiers that contain the nucleotide code for the tri or te tranucleotide repeat units for example chlc contains a repeat unit of gata whereas chlc contains an ata repeat there are over markers on the various linkage maps at chlc and most chlc markers were genotyped in ceph pedigrees the highest resolution chlc maps have an average map distance of cm between markers some of the maps contain markers in well supported unique positions along with other markers placed into intervals another set of genome wide linkage maps was produced in by the group at ge ne thon dib et al this group has identified and genotyped over dinucleotide repeat markers and has produced maps containing only ge ne thon mark ers these markers also have unique identifiers each marker name has the symbols afm at the beginning of the name the ge ne thon map contains genotyped in ceph pedigrees these markers have been placed into well supported map positions with an average map resolution of cm because of homogeneity of their marker and linkage data and the rh and yac based mapping efforts at ge ne thon that incorporate many of their polymorphic markers the ge ne thon map has become the most widely utilized human linkage map the third and most recent set of human maps was produced at the center for medical genetics at the marshfield medical research foundation broman et al this group has identified over dinucleotide repeats and has constructed high density maps using over markers like the chlc maps the marshfield maps include their own markers as well as others such as markers from chlc and ge ne thon these maps have an average resolution of cm per map interval mark ers developed at the marshfield foundation have an mfd identifier at the beginning of their names the authors caution on their web site that because only eight of the ceph families were used for the map construction the orders of some of the markers are not well determined the marshfield web site provides a useful utility for dis playing custom maps that contain user specified subsets of markers two additional linkage maps have been developed exclusively for use in per forming efficient large scale and or genome wide genotyping the abi prism link age mapping sets are composed of dinucleotide repeat markers derived from the ge ne thon linkage map the abi marker sets are available at three different map resolutions and cm containing and markers respectively the center for inherited disease research cidr a joint program sponsored by the johns hopkins university and the national institutes of health provides a gen otyping service that uses highly polymorphic tri and tetranucleotide repeat markers spaced at an average resolution of cm the cidr map is derived from the weber v marker set with improved reverse primers and some additional mark ers added to fill gaps although each of these maps is extremely valuable it can be very difficult to determine marker order and intermarker distance between markers that are not all represented on the same linkage map the map o mat web site at rutgers uni versity is a marker based linkage map server that provides several map specific que ries the server uses genotypes for over markers obtained from the ceph database and from the marshfield foundation and the cri map computer program to estimate map distances perform two point analyses and assess statistical support for order for user specified maps matise and gitlin thus rather than at tempting to integrate markers from multiple maps by rough interpolation likelihood analyses can be easily performed on any subset of markers from the ceph database high resolution linkage maps have also been constructed for many other species these maps are often the most well developed resource for animal species whose genome projects are in early stages the mouse and rat both have multiple genome wide linkage maps see mgd and the rat genome database other species with well developed linkage maps include zebrafish cat dog cow pig horse sheep goat and chicken o brien et al radiation hybrid map resources radiation hybrid maps provide an intermediate level of resolution between linkage and physical maps therefore they are helpful for sequence alignment and will aid in completion of the human genome sequencing project three human whole genome panels have been prepared with different levels of x irradiation and are available for purchase from research genetics three high resolution genome wide maps have been constructed using these panels each primarily utilizing est markers mapping servers for each of the three human rh panels are available on line to allow users to place their own markers on these maps rh score data are deposited to and publicly available from the radiation hybrid database rhdb although this sec tion covers rh mapping in humans many rh mapping efforts are also underway in other species more information regarding rh resources in all species are available at the radiation hybrid mapping information web site in general lower resolution panels are most useful for more widely spaced mark ers over longer chromosomal regions whereas higher resolution panels are best for localizing very densely spaced markers over small regions the lowest resolution human rh panel is the panel gyapay et al this panel contains hybrids that were exposed to rads of irradiation the maximum map resolution attainable by is kb an intermediate level panel was produced at the stanford human genome center stewart et al the stanford generation panel contains hybrids exposed to rads of irradiation this panel can localize markers as close as kb apart the highest resolution panel the next generation or tng was also developed at stanford beasley et al the tng panel has hybrids exposed to rads of irradiation and can localize markers as close as kb the whitehead institute mit center for genome research constructed a map with approximately markers using the panel hudson et al frame work markers on this map were localized with odds yielding a resolution of approximately mb between framework markers additional markers are lo calized to broader map intervals a mapping server is provided for placing markers scored in the panel relative to the mit maps the stanford group has constructed a genome wide map using the rh panel stewart et al this map contains markers with an average resolution of kb markers localized with odds are used to define high confidence bins and additional markers are placed into these bins with lower odds a mapping server is provided for placing markers scored in the panel onto the shgc maps a fourth rh map has been constructed using both the and panels this combined map the transcript map of the human genome genemap fig was produced by the rh consortium an international collaboration between several groups deloukas et al this map contains over ests localized against a common framework of approximately polymorphic ge ne thon markers the markers were localized to the framework using the rh panel the panel or both the map includes the majority of human genes with known function most markers on the map represent transcribed sequences with unknown function the order of the framework markers is well supported but most ests are mapped relative to the framework with odds the majority of markers on the genemap have a lod score and many are such markers are localized with relatively low support for local order and their map positions should be confirmed by other means if critical a mapping server for placing markers on genemap is available at the sanger centre the radiation hybrid database rhdb is the central repository for all rh data it is maintained at the european bioinformatics institute ebi in cambridge uk rodriguez tome and lijnzaad rhdb is a sophisticated web and ftp based figure genemap example segment of the human gene map showing the first map interval on human chromosome although the figure indicates that the map begins at a telomere on this acrocentric chromosome it actually begins near the centro mere the lower section of the figure contains columns describing the elements mapped to this interval column gives cm linkage map positions for the polymorphic markers none shown here column shows the computed cr position on either the or portion of the genemap column contains either an f for framework markers or p followed by a number this value represents the difference in statistical likelihood lod score for the given map position versus the next most likely position a lod score of is equivalent to odds of in favor of the reported marker position is equivalent to odds of and a lod score of represents odds of columns and provide marker and gene names if known searchable relational database that stores rh score data and rh maps data sub mission and retrieval are completely open to the public data are available in multiple formats or as flatfiles release september contained over rh entries for different stss scored on rh panels in different species as well as rh maps sts content maps and resources many physical mapping techniques have been used to order genomic segments for regional mammalian genome mapping projects however only rh and sts content large insert clone mapping methods have yielded the high throughput and automation necessary for whole genome analysis to date although advances in sequencing tech nology and capacity have recently made sequence based mapping feasible two land mark achievements by the ceph ge ne thon and wicgr groups have mapped the entire human genome in yacs the most comprehensive human physical mapping project is the collection of overlapping bac and pac clones being identified for the human dna sequencing project along with the now complete draft sequence of the human genome this information is being generated by many different labs and informatics tools to utilize the data are rapidly evolving the wicgr physical map is sts content based and contains more than markers for which yac clones have been identified thus providing an average resolution of approximately kb hudson et al this map has been inte grated with the ge ne thon gl and the wicgr rh maps together the integration provides sts coverage of kb and approximately half the markers are expressed sequences also placed on the map was generated primarily by screening the ceph megayac library with primers specific for each marker and then by assem bling the results by sts content analysis into sets of yac contigs contigs are separately divided into single linked and double linked depending on the min imum number of yacs one or two required to simultaneously link markers within a contig predictably the double linked contigs are shorter and much more reliable than the single linked ones largely because of the high chimeric rate of the megayac library thus some skill is required for proper interpretation of the yac based data the wicgr human physical mapping project home page provides links to downloadable but large gifs of the maps a number of ways to search the maps and access to the raw data maps can be searched by entering or selecting a marker name keyword yac or yac contig text based displays of markers list marker specific information yacs containing the marker and details of the associated con tig contig displays summarize the markers contained within them along with their coordinates on the gl and rh maps which is a very useful feature for assessing contig integrity details of which yacs contain which markers and the nature and source of each sts yac hit are also shown clickable sts content maps are also provided from the homepage and users have the option of viewing the content map alone or integrated with the gl and rh maps although there are numerous conflicts between the gl rh and sts content maps that often require clarification with other techniques this resource is very informative once its complexities and limi tations are understood especially where bac pac sequence coverage is not com plete and in linking together bac pac contigs the ceph ge ne thon yac project is a similar resource to the wicgr project also centered around screening of the ceph megayac library with a large set of stss chumakov et al much of the ceph yac screening results have been incorporated into the wicgr data those yac sts hits marked as c however the ceph data includes yac fingerprinting hybridization of yacs to inter alu pcr products and fish localizations as complementary methods to confirm contig hold ings as with wicgr these data suffer from the high yac chimerism rate long range contig builds should be interpreted with caution and the data are best used only as a supplement to other genomic data the ceph yac web site includes a rudimentary text search engine for stss and yacs that is integrated with the ge ne thon gl map and the entire data set can be downloaded and viewed using the associated quickmap application sun os only chumakov et al much of the human draft sequence was determined from bac libraries that have been whole scale dna fingerprinted and end sequenced to date over clones have been fingerprinted by washington university genome sequencing cen ter wugsc and the clone coverage is sufficient to assemble large contigs spanning almost the entire human euchromatin the fingerprinting data can be searched by clone name at the wugsc web site and provides a list of clones overlapping the input clone along with a probability score for the likelihood of each overlap alter natively users can download the clone database and analyze the raw data using the unix platform software tools image for fingerprint data and fpc for contig assembly which are available from the sanger centre in parallel with the bac fingerprinting a joint project by the institute for genome research tigr and the university of washington high throughput se quencing center uwhtsc has determined the insert end sequences stcs of the wugsc fingerprinted clones sequences these data can be searched by entering a dna sequence at the uwhtsc site or by entering a clone name at the tigr site together with the fingerprinting data this is a convenient way to build and analyze maps in silico the fingerprinting and stc data have been widely used for draft sequence ordering by the human sequencing centers and the bac pac contigs displayed by the ncbi map viewer are largely assembled from these data many human single chromosome or regional physical maps are also available because other complex genome mapping projects are less well developed the wicgr mouse yac mapping project is the only whole genome nonhuman physical map available this map is arranged almost identically to its human counterpart and consists of stss screened against a mouse yac library nusbaum et al however whole genome mouse fingerprinting and stc generation projects similar to their human counterparts are currently in production by tigr uwhtsc and the british columbia genome sequence centre bcgsc respectively dna sequence as mentioned above the existing human and forthcoming mouse draft genomic sequences are excellent sources for confirming mapping information positioning and orienting localized markers and bottom up mapping of interesting genomic regions ncbi tools like blast chapter can be very powerful in finding marker se quence links ncbi locuslink lists all homologous sequences including genomic sequences for each known human gene genomic sequences are type g on the locuslink web site maglott et al e pcr results showing all sequences containing a specific marker are available at the dbsts gdb and egenome web sites where each sequence and the exact base pair position of the marker in the sequence are listed large sequence contigs can also be viewed schematically by ncbi entrez contig viewer and the oakridge national laboratory genome chan nel web tool wheeler et al as the mammalian sequencing projects progress a sequence first approach to mapping becomes more feasible as an example a researcher can go to the ncbi human genome sequencing page and click on the idiogram of the chromosome of interest or on the chromosome number at the top of the page clicking on the idi ogram shows an expanded idiogram graphically depicting all sequence contigs rel ative to the chromosome clicking on the chromosome number instead displays a list of all sequence contigs listed in order by cytogenetic and rh extrapolated po sitions these contigs can then be further viewed for clone sequence and marker content and links to the relevant genbank and dbsts records are provided integrated maps and genomic cataloguing gdb comprehensive maps provide an estimated position of all genes markers and clones in gdb on a megabase scale this estimate is generated by sequential pairwise comparison of shared marker positions between all publicly available genome wide maps this results in a consensus linear order of markers at the gdb web site the web page for each genomic element lists one or more maps on which the element has been placed with the estimated mb position of the marker on each map element chromosome map coordinate units est mb genemap cr this example shows that marker has been placed cr from the telomere on and this calculates to mb from the telomere with the gdb mapping algorithm well mapped markers such as the ge ne thon microsatellites gen erally have more reliable calculated positions than those that are mapped only once and or by low resolution techniques such as standard karyotype based fish for chromosomes with complete dna sequence available the mb estimates are very precise ldb and udb are two additional sites that infer physical positions of a large heterogeneous set of markers from existing maps using algorithms analogous to gdb both web sites have query pages where a map region can be selected by mb coordinates cytogenetic band or specific marker names the query results show a text based list of all markers in the region ordered by their most likely positions along with an estimated physical distance in mb from the p telomere ldb also displays the type of mapping technique used to determine the comprehensive position the position of the marker in each underlying single dimension map and appropriate references an added feature of the udb site is its provision of marker specific links to other genomic databases at present there are no graphical depic tions for either map physical map positions derived from the computationally based algorithms used by gdb ldb and udb are reliant on the accuracy and integrity of the underlying maps used to determine the positions therefore these estimates serve better as initial localization guides and as supportive ordering information rather than as a primary ordering mechanism for instance a researcher defining a disease locus to a chro mosome band or between two flanking markers can utilize these databases to quickly collect virtually all mapped elements in the defined region and the inferred physical positions serve as an approximate order of the markers this information would then be supplanted by more precise ordering information present in single dimension maps and or from the researcher own experimental data the egenome project uses a slightly different approach for creating integrated maps of the human genome white et al all data from rhdb are used to generate an rh framework map of each chromosome by a process that maximizes the number of markers ordered with high confidence odds this extended high resolution rh framework is then used as the central map scale from which the high confidence intervals for additional rh and gl markers are positioned as with gdb the absolute base pair positions of all markers are calculated for chromosomes that have been fully sequenced egenome also integrates unigene est clusters large insert clones and dna sequences associated with mapped markers and it also infers cytogenetic positions for all markers the egenome search page allows que rying by marker name or genbank accession id or by defining a region with cy togenetic band or flanking marker coordinates the marker displays include the rh and gl if applicable positions large insert clones containing the marker cytoge netic position and representative dna sequences and unigene clusters advantages of egenome include the ability to view regions graphically using gdb mapview exhaustive cataloguing of marker names and an extensive collection of marker specific hypertext links to related database sites egenome maps are more conser vative than gdb ldb and udb as they show only the high confidence locations of markers often quite large intervals researchers determining a regional order de novo would be best advised to use a combination of these integrated resources for initial data collection and ordering because of the large number of primary data sources available for human genome mapping ensuring that the data collected for a specific region of interest are both current and all inclusive is a significant task genomic catalogues help in this regard both to provide a single initial source containing most of the publicly available ge nomic information for a region and to make the task of monitoring new information easier human genomic catalogues include the ncbi gdb and egenome web sites ncbi wide array of genomic data sets and analysis tools are extremely well inte grated allowing a researcher to easily transition between marker sequence gene and functional information gdb concentration on mapped genomic elements makes it the most extensive source of positional information and its inclusion of most genomic maps provides a useful mechanism to collect information about a defined region egenome also has powerful query by position tools to allow rapid collection of regional information no existing database is capable of effectively organizing and disseminating all available human genomic information however the egenome gdb and ncbi web sites faithfully serve as genomic web portals by providing hyperlinks to the majority of data available for a given genomic locus wicgr mouse mapping project and the university of wisconsin rat ge nome database rgd steen et al have aligned the gl and rh maps for the respective species in a comparative manner mgd function as a central repository for mouse genomic information makes it useful as a mouse genomic catalogue and increasingly rgd can be utilized as a rat catalogue unfortunately other complex species genome projects have not yet progressed to the point of offering true inte grated maps or catalogues comparative resources comparative maps provide extremely valuable tools for studying the evolution and relatedness of genes between species and finding disease genes through position based orthology there are several multispecies comparative mapping resources available that include various combinations of most animal species for which linkage maps are available in addition there are also many sequence based comparative analysis resources chapter each resource has different coverage and features presently it is necessary to search multiple resources as no single site contains all of the currently available homology information only the most notable resources will be described here a good starting point for homology information is ncbi locuslink database the locuslink reports include links to homologene a resource of curated and computed cross species gene homologies zhang et al currently homologene contains human mouse rat and zebrafish homology data for exam ple a locuslink search of all organisms for the gene peripheral myelin protein returns three entries one each for human mouse and rat at the top of the human page is a link to homol homologene homologene lists six homologous elements including the rat and mouse genes as well as addi tional mouse unigene cluster and a weakly similar zebrafish unigene cluster the availability of both curated and computed homology makes this a unique resource however the lack of integrated corresponding homology maps is a disadvantage the mgd does provide homology maps that simplify the task of studying con served chromosome segments homologies are taken from the reported literature for mouse human rat and other species homology information can be obtained in one of three manners searching for genes with homology information building a comparative linkage map or viewing an oxford grid the simple search returns detailed information about homologous genes in other species including map posi tions and codes for how the homology was identified links to the relevant references and links for viewing comparative maps of the surrounding regions in any two species for example a homology search for the gene returns a table listing homologous genes in cattle dog human mouse and rat figure shows the mouse human comparative map for the region surrounding in the mouse a comparative map can also be obtained by using the linkage map building tool to specify a region of the mouse as the index map and to select a second comparison species the resulting display is similar to that shown in figure an oxford grid can also be used to view a genome wide matrix in which the number of gene ho mologies between each pair of chromosomes between two species is shown this view is currently available for seven species further details on the gene homologies can be obtained via the links for each chromosome pair shown on the grid the map viewing feature of mgd is quite useful however the positions of homologous nonmouse genes are only cytogenetic so confirmation of relative marker order within figure mgd mouse human comparative map of the region surrounding the mouse gene is on mouse chromosome at the position cm on the mouse linkage map as shown by the human genes displayed on the right a segment of human chromosome is homologous to this mouse region small regions is not possible it is also possible to view mgd homology information using gdb gatewood and cottingham in silico mapping is proving to be a very valuable tool for comparative mapping the comparative mapping by annotation and sequence similarity compass ap proach ma et al has been used by researchers studying the cattle genome to construct cattle human comparative maps with identified human orthologs band et al automated comparison of cattle and human dna sequences along with the available human mapping information facilitated localization predic tions for tens of thousands of unmapped cattle ests the compass approach has been shown to have accuracy the bovine genome database displays the gene based comparative maps which also integrate mouse homologies a similar approach is being used at the bioinformatics research center at the medical college of wis consin here human rat and mouse radiation hybrid maps are coupled with theo retical gene assemblies based on est and cdna data such as the unigene set at ncbi for all three species and provide the fundamental resources allowing for the creation of iteratively built comparative maps tonellato et al homologies with uniformly mapped ests form the anchor points for the comparative maps this work has so far identified rat human rat mouse and mouse human unigene homologies most mapped on one or all of the organisms the creation of these comparative maps is an iterative exercise that is repeated as the radiation hybrid maps ests and unigene rebuilds are developed in addition the algorithm predicts the placement of unmapped assemblies relative to the anchor information providing a powerful environment for virtual mapping before radia tion hybrid or other wet lab methods are used to confirm the predictions another project utilizing electronic mapping has developed a high resolution human mouse comparative map for human chromosome recent efforts have greatly increased the number of identified gene homologies and have facilitated the construction of sequence ready bac based physical maps of the corresponding mouse regions thomas et al an additional notable resource details homology relationships between human mouse and rat derived from a high resolution rh maps homologies for over genes have been identified and are available in tabular format at a user friendly web site watanabe et al single chromosome and regional map resources although whole genome mapping resources are convenient for initial collection and characterization of a region of interest data generated for only a single chromosome or a subchromosomal region are often important for fine mapping in many cases these regional maps contain more detailed better integrated and higher resolution data than the whole genome maps can provide there are numerous such data sets databases and maps available for each human chromosome although little regional information is yet available on line for other complex genomes most published human chromosome maps are listed and can be viewed at gdb web site see above another excellent resource is the set of human chromosome specific web sites that have been created by various groups recently the human genome organization hugo has developed individual human chromosome web pages each of which is maintained by the corresponding hugo chromosome committees each page has links to chromosome specific information from a variety of mapping sources most of them being chromosome specific subsets of data derived from whole genome re sources such as the chromosome gl map from ge ne thon at the top of most hugo chromosome pages are links to other chromosome pages designed by groups mapping the specific chromosome these sites vary widely in their utility and con tent some of the most useful are briefly mentioned below the sites offer a range of resources including chromosome and or region specific gl rh cytogenetic and physical maps dna sequence data and sequencing progress single chromosome databases and catalogues of markers clones and genomic elements and links to related data and resources at other sites single chromosome workshop reports and chromosome e mail lists and discussion forums the major genome centers often include detailed mapping and sequence anno tation for particular chromosomes at their sites the sanger centre and the wugsc have two of the most advanced collections of chromosome specific genomic data informatics tools and resources sanger has collected and generated most available mapping data and reagents for human chromosomes and x these data are stored and displayed using acedb which can be utilized through a web interface webace at the sanger web site or alternatively downloaded onto a local machine unix os acedb is an object oriented database that provides a convenient relational organizational scheme for storing and managing genomic data as well as for viewing the information in both text based and graphical formats acedb is the database of choice for most researchers tackling large genomic map ping projects wugsc has recently implemented single chromosome acedb se quence and mapping databases for most human chromosomes each of which has a web interface the human chromosome web site is an example of a community based ap proach to genomic research this site includes a repository for chromosome data generated by several labs an extensive list of hyperlinks to chromosome data an e mail list and discussion forum a listing of chromosome researchers and their interests and several workshop reports the university of texas at san antonio chromosome site contains a database of large insert clones and markers along with gl rh cytogenetic and comparative maps the university of california irvine has an on line chromosome acedb database whereas the joint genome institute jgi maintains chromosome large insert clone maps and some external web links at their site the university of toronto chromosome web site includes a searchable comprehensive chromosome database containing markers clones and cytogenetic information this site also has a long list of chromosome links also the national human genome research institute chromosome web site contains a yac sts map a list of ests and integration with chromosome sequence files the uni versity college london maintains a good comprehensive resource of chromosome genomic links an e mail group workshop reports and a searchable chromosome database genome therapeutics corporation has developed an inclusive web site for chromosome this site has both gl physical and integrated sequence based maps links to related data and workshop reports imperial college maintains a searchable chromosome database at their chro mosome web site whereas the chromosome web site at jgi contains restric tion mapped bac and cosmid contigs and determined sequence along with a list of chromosome hyperlinks a similar jgi resource for chromosome includes a completely integrated physical map with sequence links and a list of external re sources the university of colorado the riken genomic sciences center and the max planck institute for molecular genetics mpimg have an interconnected set of resources that together integrate genomic clones markers and sequence for the completely sequenced chromosome the sanger centre and the ldb have com prehensive resources for the viewing and analysis of chromosome it is expected that additional resources for all completely sequenced chromosomes will be available soon the resources for the x chromosome are most impressive the mpimg has established a complete genomic catalogue of this chromosome that features integra tion of genomic mapping and sequence data derived from many sources and ex perimental techniques these data can be viewed graphically with the powerful on line java application derbrowser finally the sequenced and well characterized mitochondrial genome is well displayed at emory university where a highly ad vanced catalogue encompassing both genomic and functional information has been established practical uses of mapping resources potential applications of genomic data are numerous and to a certain extent depend on the creativity and imagination of the researcher however most researchers utilize genomic information in one of three ways to find out what genomic elements usually transcribed elements are contained within a genomic region to determine the order of defined elements within a region or to determine the chromosomal position of a particular element each of these goals can be accomplished by various means and the probability of efficient success is often enhanced by familiarity with many of the resources discussed in this chapter it is prudent to follow a logical course when using genomic data during the initial data acquisition step in which genomic data are either generated experimentally or retrieved from publicly available data sources simultaneous evaluation of multiple data sets will ensure both higher resolution and greater confidence while increasing the likelihood that the genomic elements of interest are represented second the interrelationships and limitations of the data sets must be sufficiently understood as it is easy to overinterpret or under represent the data finally it is important to verify critical assignments independently especially when using mapping data that are not ordered with high confidence be low we give some brief suggestions on how to approach specific map related tasks but many modifications or alternative approaches are also viable the section is organized in a manner similar to a positional cloning project starting with definition of the region boundaries determining the content and order of elements in the region and defining a precise map position of the targeted element defining a genomic region a genomic region of interest is best defined by two flanking markers that are com monly used for mapping purposes such as polymorphic ge ne thon markers in humans or mit microsatellites in mice starting with a cytogenetically defined region is more difficult due to the subjective nature of defining chromosomal band boundaries con version of cytogenetic boundaries to representative markers can be approximated by viewing the inferred cytogenetic positions of markers in comprehensive maps such as gdb universal map udb ldb or egenome because these cytogenetic po sitions are inferred and approximate a conservative approach is recommended when using cytogenetic positions for region definition the choice of flanking markers will impact how precisely a region size and exact boundary locations can be defined commonly used markers are often present on multiple independently derived maps so their position on the chromosome provides greater confidence for anchoring a regional endpoint in contrast the exact location of less commonly used markers is often locally ambiguous these markers can sometimes be physically tethered to other markers if a large sequence tract that contains multiple markers can be found this can be performed by blasting marker sequences against genbank or by scanning e pcr results in unigene or egenome for a particular marker determining and ordering the contents of a defined region once a region has been defined there are a number of resources available for de termining what lies within the region a good way to start is to identify a map that contains both flanking markers either from a chromosome wide or genome wide map from the sources listed above from a genomic catalogue or from a local map that has been generated by a laboratory interested in this particular region for hu mans gdb is the most inclusive map repository although many regional maps have not been deposited in gdb but can be found with a literature search of the corre sponding cytogenetic band or a gene known to map to the region many localized maps are physically based and are more accurate than their computationally derived whole chromosome counterparts for other species the number of maps to choose from is usually limited so it is useful to first define flanking markers known to be contained in the available maps the map or maps containing the flanking markers can then be used to create a consensus integrated map of the region this is often an inexact and tedious process to begin it is useful to identify from the available maps an index map that contains many markers high map resolution and good reliability integration of markers from additional maps relative to the index map proceeds by comparing the positions of markers placed on each map for example if an index map contains markers in the order a b c d and a second map has markers in the order b e d then marker e can be localized to the interval between markers b and d on the index map im portantly however the relative position of marker e with respect to marker c usually cannot be accurately determined by this method repeated iterations of this process should allow localization of all markers from multiple maps relative to the index map this process is of course significantly reinforced by experimental verification such as with sts content mapping of large insert clones identified for the region specific markers or ideally by sequence determined order each marker represents some type of genomic element a gene an est a pol ymorphism a large insert clone end or a random genomic stretch in humans iden tifying what a marker represents is relatively straightforward simply search for the marker name in gdb or egenome and in most cases the resulting web display will provide a summary of what the marker represents usually along with hyperlinks to relevant functional information for mice mgd provides a similar function to gdb for other organisms the best source is usually either dbsts or if present web sites or publications associated with the underlying maps genbank and dbsts are alternatives for finding markers but because these repositories are passive re quiring researchers to submit their markers rather than actively collecting markers many marker sets are not represented if a marker is known to be expressed unigene locuslink and dbest are excellent sources of additional information many genes and some polymorphisms have been independently discovered and de veloped as markers multiple times and creating a nonredundant set from a collection of markers is often challenging gdb egenome mgd and for genes unigene are good sources to use for finding whether two markers are considered equivalent but even more reliable is a dna sequence or sequence contig containing both marker primers blast and the related are efficient for quickly deter mining sequence relatedness chapter obviously the most reliable tool for marker ordering is a dna sequence or sequence contig for expressed human markers searching with the marker name in unigene or entrez genomes returns a page stating where or if the marker has been mapped in genemap and other maps a list of mrna genomic and est se quences and with entrez genomes a mapviewer based graphical depiction of the maps sequence ready contigs and available sequence of the region similarly gdb and egenome show which dna sequences contain each displayed marker for other markers the sequence from which the marker is derived or alternatively one of the primer sequences may be used to perform a blast search that can identify com pletely or nearly homologous sequences the nonredundant est gss and htgs divisions of genbank are all potentially relevant sources of matching sequence depending on the aim of the project only long sequences are likely to have worth while marker ordering capabilities finished genomic sequence tracts have at least some degree of annotation and scanning the genbank record for the large sequence will often yield an annotated list of what markers lie within the sequence and where they are keep in mind that such annotations vary considerably in their thoroughness and most are fixed in time that is they only recognize markers that were known at the time of the annotation blast or other sequence alignment programs are helpful in identification or confirmation of what might lie in a large sequence also the ncbi e pcr web interface can be used to identify all markers in dbsts contained within a given sequence and this program can be installed locally to query customized marker sets with dna sequences schuler for genomes for which dna sequencing is complete or is substantially under way it may be possible to construct local clone or sequence contigs among higher organisms this is currently possible only for the human and mouse genomes al though individual clone sequences can be found in genbank larger sequence contigs sequence tracts comprising more than one bac or pac are more accessible using the entrez genomes web site see above here by entering a marker or dna accession number into the contigs search box researchers can identify sequence contigs containing that marker or element this site also provides a graphical view of all other markers contained in that sequence the base pair position of the markers in the sequence and with the mapviewer utility graphical representations of clone contigs this process can also be performed using blast or e pcr although it is somewhat more laborious once a sequence has been identified for markers in a given region yac clone dna fingerprinting and stc data can be used to bridge gaps for humans and mice the wicgr yac data provide a mechanism for identifying yac clones linking adjacent markers however caution should be exercised to rely mainly on double linked contigs and or to experimentally confirm yac marker links also for human genome regions the uwhtsc and tigr web sites for identifying stcs from dna sequence or bac clones are very useful for example researchers with a sequence tract can go to the uwhtsc tsc search page enter their sequence and find stcs contained in the sequence any listed stc represents the end of a bac clone whose insert contains a portion of the input sequence venter et al the tigr search tool is complementary to the uwhtsc search as the tigr site requires input of a large insert clone name which yields stc sequences stcs represent large insert clones that potentially extend a contig or link two adjacent nonoverlapping contigs similarly human bac clones have been fingerprinted for rapid identifi cation of overlapping clones marra et al the fingerprinting data are avail able for searching at the washington university human genome sequencing center wugsc combined use of entrez blast the stc resources and the bac fin gerprinting data can often provide quick and reliable contig assembly by in silico sequence and clone walking defining a map position from a clone or dna sequence expressing the chromosomal position of a gene or genomic element in physical rh gl or cytogenetic terms is not always straightforward the first approach is to determine whether the element of interest has already been localized the great majority of human transcripts are precisely mapped and many genes have been well localized in other organisms as well for species with advanced dna sequencing projects it is helpful to identify a large dna sequence tract containing the genomic element of interest and then determine what markers it contains by looking at the sequence annotation record in genbank or by e pcr identified human and mouse genes are catalogued in gdb and locuslink or mgd respectively and searching unigene with a marker name mrna or est sequence accession number or gene name will often provide a localization if one is known here again nomenclature difficulties impede such searches making it necessary to search each database with one or more alternate names in some cases another alternative is to determine if the genomic element is contained in a genomic sequence by a simple blast search most large genomic sequences have been cytogenetically localized and this infor mation is contained in the sequence annotation record usually in the title if gene specific or closely linked markers have been used previously for map ping a position can usually be described in terms specific to the mapping method that was employed for example if an unknown gene is found to map very close to a ge ne thon marker then the gene position can be reported relative to the ge ne thon gl centimorgan coordinates most human markers and many maps have been placed in gdb so this is a good first step in determining whether a marker has been mapped simply search for the relevant marker and see where it has been placed on one or several maps listed under cytogenetic localizations and other localiza tions inferred cytogenetic positions of human genes and markers are usually listed in gdb unigene and egenome if the elements have been previously mapped if not band or band range assignments can usually be approximated by finding the cytogenetic positions of flanking or closely linked markers and genes many se quenced large insert clones have been assigned by fish to a cytogenetic position this information can usually be found in the sequence annotation or at the clone originator web site the process of determining whether a transcript or genomic element from another organism has been mapped varies somewhat due to the lack of extensive genomic catalogs making it usually necessary to cross reference a marker with the gl and or rh maps available for the species if no previous localization exists for a genomic element some experimental work must be undertaken for human and mouse markers an efficient and precise way to map a sequence based element is to develop and map an sts derived from the element by rh analysis a set of primers should be designed that uniquely amplify a product in the species of interest but not in the rh panel background genome an sts is usually unique if at least one primer is intronic primers designed from an mrna sequence will not amplify the correct sized product in genomic dna if they span an intron but a good approximation is to use primers from the untran slated region as these stretches only rarely contain introns and usually comprise sequences divergent enough from orthologous or paralogous sequences however beware of pseudogenes and repetitive sequences and genomic sequence stretches are superior for primer design suitable primers can then be used to type an appro priate rh panel currently human or tng mouse rat baboon zebrafish dog horse cow and pig panels are available commercially after the relevant panel is typed the resulting data can be submitted to a panel specific on line rh server see above for the human mouse rat and zebrafish panels for other species iso lation and fish of a large insert clone or gl mapping with an identified or known flanking polymorphism may be necessary internet resources for topics presented in chapter data repositories the genome database gdb http www gdb org national center for biotech nology information ncbi home page http www ncbi nlm nih gov entrez genomes division http www ncbi nlm nih gov entrez genome main genomes html locuslink http www ncbi nlm nih gov locuslink genemap http www ncbi nlm nih gov omim http www ncbi nlm nih gov omim homologene http www ncbi nlm nih gov homologene blast http www ncbi nlm nih gov blast epcr http www ncbi nlm nih gov sts entrez sequence viewer http www ncbi nlm nih gov genome seq genbank http www ncbi nlm nih gov genbank genomic biology page http www ncbi nlm nih gov genomes dbsts http www ncbi nlm nih gov dbsts mouse genome informatics mgd mgi resources and projects cytogenetic http www informatics jax org bac http bacpac med buffalo edu human overview html lbnl ucsf rmc http ioerror ucsf edu dfdavy rmc outside html u of bari http bioserver uniba it fish rocchi cytogenetic yac data http www mpimg berlin dahlem mpg de cytogen probedat htm nci http www ncbi nlm nih gov cgap ccap http www ncbi nlm nih gov ccap mitelsum cgi infobiogen http www infobiogen fr services chromcancer sergg http www ir miami edu genetics sergg chromosome html coriell http locus umdnj edu nigms ideograms html arkdb http www ri bbsrc ac uk bioinformatics ark overview html animal genome database http niai affrc go jp jgbase html cedars sinai http www csmc edu genetics korenberg korenberg html a genetic linkage ceph genotype database http www cephb fr cephdb chlc http lpg nci nih gov chlc ge ne thon http www genethon fr genethon en html marshfield http www marshmed org genetics map o mat http compgen rutgers edu mapomat rat genome database http www lgr mcw edu projects rgd html radiation hybrid rhdb http www ebi ac uk rhdb rh information page http compgen rutgers edu rhmap research genetics http www resgen com wicgr rh maps http www genome wi mit edu cgi bin contig phys map wicgr rh map server http www genome wi mit edu cgi bin contig rhmapper pl shgc rh maps http shgc www stanford edu mapping rh shgc map server http shgc www stanford edu rh sanger centre gm map server sts content wicgr human physical map http www sanger ac uk software rhserver http carbon wi mit edu cgi bin contig phys map ceph ge ne thon yac map http www cephb fr bio ceph genethon map html wugsc home http genome wustl edu gsc index shtml tigr stcs http www tigr org tdb humgen bac end search bac end intro html uwhtsc stcs http www htsc washington edu human info index cfm wugsc bac fingerprints http genome wustl edu gsc human human database shtml ubgsc mouse bac fingerprints http www bcgsc bc ca projects mouse mapping trask http fishfarm biotech washington edu bacresource random index html wicgr mouse physical genetic map dna sequence see ncbi links http carbon wi mit edu cgi bin mouse index ornl genome channel http compbio ornl gov tools channel integrated and catalogs udb http bioinformatics weizmann ac il udb ldb http cedar genetics soton ac uk public html ldb html ldb sequence based maps http cedar genetics soton ac uk public html html egenome http genome chop edu comparative mouse homology http www informatics jax org menus homology menu shtml otsuka oxford rat mouse human human chromosome mouse map http ratmap ims u tokyo ac jp http genome nhgri nih gov comparative bovine genome database http bos cvm tamu edu bovgbase html mcw rat mouse human http rgd mcw edu single chromosome regional rutgers http linkage rockefeller edu utsa http apollo uthscsa edu uci http hsis uci edu jgi http jgi doe gov data jgi mapping html hsc http www genet sickkids on ca nhgri http www nhgri nih gov dir gtb ucl http www gene ucl ac uk gtc http www cric com sequence center imperial college http bc ic ac uk jgi http jgi doe gov data jgi mapping html jgi http jgi doe gov data jgi mapping html colorado http www eri uchsc edu frames html riken http hgp gsc riken go jp index html mpimg http rz berlin mpg de x http www mpimg berlin dahlem mpg de xteam mito emory http infinity gen emory edu mitomap html hugo chromosome resources http www gdb org hugo sanger centre http www sanger ac uk hgp acedb http www acedb org problem set you have performed a large scale genome wide search for the gene for the inherited disorder bioinformatosis initial analyses have identified one region with significant results flanked by the markers there are many genes mapping within this region one of which is particularly interesting superoxide dismutase what is the cytogenetic location of this gene and hence at least part of the region of interest how large is this region in cm what polymorphic markers can be identified in this region that you might use to try to narrow the region choose six of these polymorphic markers based on the chosen markers can a map based on these markers be identified or constructed what sts markers have been developed for what are their map positions on the human transcript map genemap are these positions statistically well supported have any snp markers been identified within what other genes are in this region has the region including the gene been sequenced what contigs and or clones contain have orthologous regions been identified in any other species have mutations in been associated with any diseases other than bioinformatosis references adams m d kelley j m gocayne j d dubnick m polymeropoulos m h xiao h merril c r wu a olde b moreno r f et al complementary dna sequencing expressed sequence tags and human genome project science aston c mishra b and schwartz d c optical mapping and its potential for large scale sequencing projects trends biotechnol band m r larson j h rebeiz m green c a heyen d w donovan j windish r steining c mahyuddin p womack j e and lewin h a an ordered comparative map of the cattle and human genomes genome res beasley e stewart e mckusick k aggarwal a brady hebert s fang n lewis s lopez f norton j pabla h perkins s piercy m qin f reif t sun w vo n myers r and cox d the radiation hybrids improve the resolution of the panel am j hum genet suppl blake j a eppig j t richardson j e and davisson m t the mouse genome database mgd expanding genetic and genomic resources for the laboratory mouse nucleic acids res boehnke m lange k and cox d r statistical methods for multipoint radiation hybrid mapping am j hum genet broman k w murray j c sheffield v c white r l and weber j l com prehensive human genetic maps individual and sex specific variation in recombination am j hum genet burke d t carle g f and olson m v cloning of large segments of exogenous dna into yeast by means of artificial chromosome vectors science chakravarti a and lynn a meiotic mapping in humans in genome analysis a laboratory manual vol mapping genomes b birren e green p hieter s klapholz r myers h riethman and j roskams eds cold spring harbor cold spring harbor laboratory press chumakov i m rigault p le gall i bellanne chantelot c billault a guillou s soularue p guasconi g poullier e gros i belova m sambucy j l susini l gervy p glibert f beaufils s bul h massart c de tand m f dukasz f lecoulant s ougen p perrot v saumier m soravito c bahouayila r cohen akenine a barillot e bertrand s codani j j caterina d georges i lacroix b lucotte g sahbatou m schmit c sangouard m tubacher e dib c faure s fizames c cyapay g millasseau p nguyen s muselet d vignal a morissette j menninger j lieman j desai t banks a bray ward p ward d hudson t gerety s foote s stein l page d c lander e s weissenbach j le paslier d and cohen d a yac contig map of the human genome nature 297 collins a frezal j teague j and morton n e a metric map of humans loci in bands proc natl acad sci usa collins a teague j keats b and morton n linkage map integration genomics dausset j cann h cohen d lathrop m lalouel j m and white r centre d etude du polymorphisme humain ceph collaborative genetic mapping of the human genome genomics deloukas p schuler g d gyapay g beasley e m soderlund c rodriguez tome p hui l matise t c mckusick k b beckmann j s benolila s bihoreau m t birren b b browne j butler a castle a b chiannikulchai n clee c day p j r dehejia a dibling t drouot n duprat s fizames c fox s gelling s green l harison p hocking r holloway e hunt s keil s lijnzaad p louis dit sully c ma j mendis a miller j morissette j muselet d nusbaum h c peck a rozen s simon d slonim d k staples r stein l d stewart e a suchard m a thangarajah t vega czarny n webber c wu x auffray c no mura n sikela j m polymeropoulos m h james m r lander e s hudson t j myers r m cox d r weissenbach j boguski m s and bentley d r a physical map of human genes science dib c faure s fizames c samson d drouot n vignal a millasseau p marc s hazan j seboun e lathrop m gyapay g morissette j and weissenbach j a comprehensive genetic map of the human genome based on microsatellites nature dietrich w weber j nickerson d and kwok p y identification and analysis of dna polymorphisms in genome analysis a laboratory manual vol mapping genomes b birren e green p hieter s klapholz r myers h riethman and j roskams eds cold spring harbor cold spring harbor laboratory press gatewood b and cottingham r mouse human comparative map resources on the web briefings in bioinformatics green e d and olson m v chromosomal region of the cystic fibrosis gene in yeast artificial chromosomes a model for human genome mapping science gyapay g schmitt k fizames c jones h vega czarny n spillet d muselet d prud homme j dib c auffray c morissette j weissenbach j and goodfellow p n a radiation hybrid map of the human genome hum mol genet hillier l d lennon g becker m bonaldo m f chiapelli b chissoe s dietrich n dubuque t favello a gish w hawkins m hultman m kucaba t lacy m le m le n mardis e moore b morris m parsons j prange c rifkin l rohlfing t schellenberg k marra m and et al generation and analysis of human expressed sequence tags genome res houlgatte r mariage samson r duprat s tessier a bentolila s lamy b and auf fray c the genexpress index a resource for gene discovery and the genic map of the human genome genome res hudson t j stein l d gerety s s ma j castle a b silva j slonim d k baptista r kruglyak l xu s h hu x colbert a m e rosenberg c reeve daly m p rozen s hui l wu x vestergaard c wilson k m bae j s maitra s ganiatsas s evans c a deangelis m m kngalls k a nahf r w horton jr l t anderson m o collymore a j ye w kouyoumijan v zemsteva i s tam j devine r courtney d f renaud m t nguyen h o connor t j fizames c faure s gyapay g dib c morissette j orlin j b birren b w goodman n weissenbach j hawkins t l foote s page d c and lander e s an sts based map of the human genome science iaonnou p a amemiya c t garnes j kroisel p m shizuya h chen c batzer m a and de jong p j a new bacteriophage derived vector for the propagation of large human dna fragments nat genet jensen s j sulman e p maris j m matise t c vojta p j barrett j c brodeur g m and white p s an integrated transcript map of human chromosome genomics jing j lai z aston c lin j carucci d j gardner m j mishra b anantharaman t s tettelin h cummings l m hoffman s l venter j c and schwartz d c optical mapping of plasmodium falciparum chromosome genome res korenberg j r chen x n devon k l noya d oster granite m l and birren b w mouse molecular cytogenetic resource bacs link the chromosomal and genetic maps genome res lander e s and green p construction of multi locus genetic linkage maps in humans proc natl acad sci usa lander e s green p abrahamson j barlow a daly m j lincoln s e and newburg l mapmaker an interactive computer package for constructing primary ge netic linkage maps of experimental and natural populations genomics lathrop g m lalouel j m julier c and ott j strategies for multilocus linkage analysis in humans proc natl acad sci usa lawrence j b singer r h and ncneil j a interphase and metaphase resolution of different distances within the human dystrophin gene science letovsky s i cottingham r w porter c j and li p w d gdb the human genome database nucleic acids res lewis t b nelson l ward k and leach r j a radiation hybrid map of loci for the distal long arm of human chromosome genome res ma r z van eijk m j beever j e guerin g mummery c l and lewin h a comparative analysis of expressed sequence tags from a cattle ovary cdna library mamm genome maglott d r katz k s sicotte h and pruitt k d ncbi locuslink and refseq nucleic acids res marra m a kucaba t a dietrich n l green e d brownstein b wilson r k mcdonald k m hillier l w mcpherson j d and waterston r h high throughput fingerprint analysis of large insert clones genome res matise t and gitlin j map o mat marker based linkage mapping on the world wide web am j hum genet matise t c perlin m and chakravarti a automated construction of genetic linkage maps using an expert system multimap a human genome linkage map nat genet mckusick v a mendelian inheritance in man catalogs of human genes and genetic disorders edition baltimore johns hopkins university press murray j c buetow k h weber j l ludwigsen s scherpbier heddema t manion f quillen j sheffield v c sunden s duyk g m weissenbach j gyapay g dib c morrissette j lathrop g m vignal a white r matsunami n gerken s melis r albertsen h plaetke r odelberg o ward d dausset j cohen d and cann h a comprehensive human linkage map with centimorgan density science nusbaum c slonim d harris k birren b steen r stein l miller j dietrich w nahf r wang v merport o castle a husain z farino g gray d anderson m devine r horton l ye w kouyoumjian v zemsteva i wu y collymore a courtney d tam j cadman m haynes a heuston c marsland t southwell a trickett p strivens m ross m makalowski w wu y boguski m carter n denny p brown s hudson t and lander e a yac based physical map of the mouse genome nat genet o brien s j menotti raymond m murphy w j nash w g wienberg j stanyon r copeland n g jenkins n a womack j e and marshall graves j a the promise of comparative genomics in mammals science parra i and windle b high resolution visual mapping of stretched dna by fluo rescent hybridization nat genet pearson p l the genome data base gdb a human gene mapping repository nucleic acids res suppl pinkel d straume t and gray j w cytogenetic analysis using quantitative high sensitivity fluorescence hybridization proc natl acad sci usa pruitt k katz k sicotte h and maglott d introducing refseq and locuslink curated human genome resources at the ncbi trends genet roberts t auffray c and cowell j k regional localization of genic markers on human chromosome genomics rodriguez tome p and lijnzaad p rhdb the radiation hybrid database nucleic acids res schuler g d sequence mapping by electronic pcr genome res schwartz d c li x hernandez l i ramnarain s p huff e j and wang y k ordered restriction maps of saccharomyces cerevisiae chromosomes constructed by optical mapping science shizuya h birren b kim u j mancino v slepak t tachiiri y and simon m cloning and stable maintenance of kilobase pair fragments of human dna in escherichia coli using an f factor based vector proc natl acad sci usa slonim d kruglyak l stein l and lander e building human genome maps with radiation hybrids j comput biol steen r kwitek black a glenn c gullings handley j etten w atkinson s appel d twigger s muir m mull t granados m kissebah m russo k crane r popp m peden m matise t brown d lu j kingsmore s tonellato p rozen s slonim d young p knoblauch m provoost a ganten d colman s rothberg j lander e and jacob h a high density integrated genetic linkage and radiation hybrid map of the laboratory rat genome res stewart e a mckusick k b aggarwal a bajorek e brady s chu a fang n hadley d harris m hussain s lee r maratukulam a o connor k perkins s piercy m qin f reif t sanders c she x sun w tabar p voyticky s cowles s fan j mader c quackenbush j myers r m and cox d r an sts based radiation hybrid map of the human genome genome res talbot c a and cuticchia a j human mapping databases in current protocols in human genetics n dracopoli j haines b korf d moir c morton c seidman j seidman and d smith eds new york j wiley p thomas j w summers t j lee lin s q maduro v v idol j r mastrian s d ryan j f jamison d c and green e d comparative genome mapping in the sequence based era early experience with human chromosome genome res tonellato p j zho h chen d wang z stoll m kwitek black a and jacob h comparative mapping of the human and rat genomes with radiation hybrid maps recomb lyon france april van den engh g sachs r and trask b j estimating genomic distance from dna sequence location in cell nuclei by a random walk model science venter j c smith h o and hood l a new strategy for genome sequencing nature vollrath d foote s hilton a brown l g beer romero p bogan j s and page d c the human y chromosome a interval map based on naturally occurring deletions science wada y and yasue h development of an animal genome database and its search system comput appl biosci wang d g fan j b siao c j berno a young p sapolsky r ghandour g perkins n winchester e spencer j kruglyak l stein l hsie l topaloglou t hubbell e robinson e mittmann m morris m s shen n kilburn d rioux j nusbaum c rozen s hudson t j lander e s and et al large scale identification mapping and genotyping of single nucleotide polymorphisms in the human genome science watanabe t k bihoreau m t mccarthy l c kiguwa s l hishigaki h tsuji a browne j yamasaki y mizoguchi miyakita a oga k ono t okuno s kanemoto n takahashi e tomita k hayashi h adachi m webber c davis m kiel s knights c smith a critcher r miller j james m r and et al a radiation hybrid map of the rat genome containing markers nat genet wheeler d l chappey c lash a e leipe d d madden t l schuler g d tatusova t a and rapp b a database resources of the national center for biotechnology information nucleic acids res white p s sulman e p porter c j and matise t c a comprehensive view of human chromosome genome res zhang z schwartz s wagner l and miller w a greedy algorithm for aligning dna sequences j comput biol bioinformatics a practical guide to the analysis of genes and proteins second edition andreas d baxevanis b f francis ouellette copyright john wiley sons inc isbns hardback paper electronic information retrieval from biological databases andreas d baxevanis genome technology branch national human genome research institute national institutes of health bethesda maryland as discussed earlier in this book genbank was created in response to the explosion in sequence information resulting from a panoply of scientific efforts such as the human genome project to review genbank is an annotated collection of all pub licly available dna and protein sequences and is maintained by the national center for biotechnology information ncbi as of this writing genbank contains mil lion sequence records covering almost billion nucleotide bases sequences find their way into genbank in several ways most often by direct submission by indi vidual investigators through tools such as sequin or through direct deposit by large genome sequencing centers genbank or any other biological database for that matter serves little purpose unless the database can be easily searched and entries retrieved in a usable mean ingful format otherwise sequencing efforts have no useful end since the biological community as a whole cannot make use of the information hidden within these millions of bases and amino acids much effort has gone into making such data accessible to the average user and the programs and interfaces resulting from these efforts are the focus of this chapter the discussion centers on querying the ncbi databases because these more general repositories are far and away the ones most often accessed by biologists but attention is also given to a number of smaller specialized databases that provide information not necessarily found in genbank integrated information retrieval the entrez system the most widely used interface for the retrieval of information from biological da tabases is the ncbi entrez system entrez capitalizes on the fact that there are preexisting logical relationships between the individual entries found in numerous public databases for example a paper in medline or more properly pubmed may describe the sequencing of a gene whose sequence appears in genbank the nucleotide sequence in turn may code for a protein product whose sequence is stored in the protein databases the three dimensional structure of that protein may be known and the coordinates for that structure may appear in the structure database finally the gene may have been mapped to a specific region of a given chromosome with that information being stored in a mapping database the existence of such natural connections mostly biological in nature argued for the development of a method through which all information about a particular biological entity could be found without having to sequentially visit and query disparate databases entrez to be clear is not a database itself it is the interface through which all of its component databases can be accessed and traversed the entrez informa tion space includes pubmed records nucleotide and protein sequence data three dimensional structure information and mapping information the strength of entrez lies in the fact that all of this information can be accessed by issuing one and only one query entrez is able to offer integrated information retrieval through the use of two types of connection between database entries neighboring and hard links neighboring the concept of neighboring allows for entries within a given database to be con nected to one another if a user is looking at a particular pubmed entry the user can ask entrez to find all other papers in pubmed that are similar in subject matter to the original paper similary if a user is looking at a sequence entry entrez can return a list of all other sequences that bear similarity to the original sequence the estab lishment of neighboring relationships within a database is based on statistical mea sures of similarity as follows blast sequence data are compared with one another using the basic local alignment search tool or blast altschul et al this algorithm attempts to find high scoring segment pairs hsps which are pairs of sequences that can be aligned with one another and when aligned meet certain scoring and statistical criteria chapter discusses the family of blast algorithms and their application at length vast sets of coordinate data are compared using a vector based method known as vast for vector alignment search tool madej et al gibrat et al there are three major steps that take place in the course of a vast comparison first based on known three dimensional coordinate data all of the a helices and sheets that comprise the core of the protein are identified straight line vectors are then calculated based on the position of these secondary structure elements vast keeps track of how one vector is connected to the next that is how the c terminal end of one vector connects to the n terminal end of the next vector as well as whether a particular vector represents an a helix or a sheet subsequent steps use only these vectors in making comparisons to other proteins in effect most of the coordinate data is discarded at this step the reason for this apparent oversimplification is simply due to the scale of the problem at hand with over structures in pdb the time that it would take to do an in depth comparison of each and every structure with all of the other structures in the database would make the calculations both im practical and intractable the user should keep this simplification in mind when making biological inferences based on the results presented in a vast table next the algorithm attempts to optimally align these sets of vectors looking for pairs of structural elements that are of the same type and relative orien tation with consistent connectivity between the individual elements the ob ject is to identify highly similar core substructures pairs that represent a statistically significant match above that which would be obtained by com paring randomly chosen proteins with one another finally a refinement is done using monte carlo methods at each residue po sition in an attempt to optimize the structural alignment through this method it is possible to find structural and presumably functional relationships between proteins in cases that may lack overt sequence similarity the resultant alignment need not be global matches may be between individual domains of different proteins it is important to note here that vast is not the best method for determining structural similarities more robust methods such as homology model building pro vide much greater resolving power in determining such relationships since the raw information within the three dimensional coordinate file is used to perform more advanced calculations regarding the positions of side chains and the thermodynamic nature of the interactions between side chains reducing a structure to a series of vectors necessarily results in a loss of information however considering the mag nitude of the problem here again the number of pairwise comparisons that need to be made and both the computing power and time needed to employ any of the more advanced methods vast provides a simple and fast first answer to the question of structural similarity more information on other structure prediction methods based on x ray or nmr coordinate data can be found in chapter weighted key terms the problem of comparing sequence data somewhat pales next to that of comparing pubmed entries free text whose rules of syntax are not necessarily fixed given that no two people writing styles are exactly the same finding a way to compare seemingly disparate blocks of text poses a substantial problem entrez employs a method known as the relevance pairs model of retrieval to make such comparisons relying on what are known as weighted key terms wilbur and coffee wilbur and yang this concept is best described by ex ample consider two manuscripts with the following titles as a genetic marker for breast cancer genetic factors in the familial transmission of the breast cancer gene both titles contain the terms breast and cancer and the presence of these common terms may indicate that the manuscripts are similar in their subject matter the proximity between the words is also taken into account so that words common to two records that are closer together are scored higher than common words that are further apart in the current example the terms breast and cancer would score higher based on proximity than either of those words would against since the words are next to each other common words found in a title are scored higher than those found in an abstract since title words are presumed to be more important than those found in the body of an abstract overall weighting depends on the frequency of a given word among all the entries in pubmed with words that occur infrequently in the database as a whole carrying a higher weight regardless of the method by which the neighboring relationships are established the ability to actually code and maintain these relationships is rooted in the format underlying all of the constituent databases this format called abstract syntax no tation asn provides a format in which all similar fields e g those for a bib liographic citation are all structured identically regardless of whether the entry is in a protein database nucleotide database and so forth this ncbi data model is discussed in depth in chapter hard links the hard link concept is much easier conceptually than neighboring hard links are applied between entries in different databases and exist everywhere there is a logical connection between entries for instance if a pubmed entry talks about the sequenc ing of a cosmid a hard link is established between the pubmed entry and the cor responding nucleotide entry if an open reading frame in that cosmid codes for a known protein a hard link is established between the nucleotide entry and the protein entry if by sheer luck the protein entry has an experimentally deduced structure a hard link would be placed between the protein entry and the structural entry the hard link relationships between databases is illustrated in figure as suggested by the figure searches can in essence begin anywhere within entrez the user has no constraints with respect to where the foray into this infor mation space must begin however depending on which database is used as the jumping off point different fields are available for searching this stands to reason inasmuch as the entries in databases of different types are necessarily organized differently reflecting the biological nature of the entity they are trying to catalog implementations regardless of platform entrez searches can be performed using one of two inter faces the first is a client server implementation known as network entrez this is the fastest of the entrez programs in that it makes a direct connection to an ncbi dispatcher the graphical user interface features a series of windows and each time a new piece of information is requested a new window appears on the user screen because the client software resides on the user machine it is up to the user to obtain install and maintain the software downloading periodic updates as new features are introduced the installation process itself is fairly trivial network entrez also comes bundled with interactive graphical viewers for both genome sequences and three dimensional structures cf chapter figure overview of the relationships in the entrez integrated information retrieval system each square represents one of the elements that can be accessed through entrez and the lines represent how each element connects to the other elements entrez is under continuous evolution with both new components being added and the interrelationships between the elements changing dynamically the second and more widely used implementation is through the world wide web this option makes use of available web browsers such as internet explorer or netscape to deliver search results to the desktop the use of a web browser relieves the user of having to make sure that the most current version of entrez is installed as long as the browser is of relatively recent vintage results will always be displayed via the latest entrez release the web naturally lends itself to an ap plication such as this since all the neighboring and hard link relationships described above can easily be expressed as hypertext allowing the user to navigate by clicking on selected words in an entry the advantage of the web implementation over the network version is that the web allows for the ability to link to external data sources such as full text versions of papers maintained by a given journal or press or specialized databases that are not part of entrez proper the speed advantage that is gained by the network version causes its limitation in this respect the direct connection to the ncbi dispatcher means that the user once connected to ncbi cannot travel anywhere else the other main difference between the two methods lies simply in the presentation the net work version uses a series of windows in presenting search results whereas the web version is formatted as sequential pages following the standard web paradigm the final decision is one of personal preference for both methods will produce the same results within the entrez search space however given that web entrez can link to external data sources the remainder of this discussion will focus on the web implementation the entrez discovery pathway examples the best way to illustrate the integrated nature of the entrez system and to drive home the power of neighboring is by considering two biological examples using the web version of entrez as the interface the simplest way to query entrez is through the use of individual search terms coupled together by boolean operators such as and or or not consider the case in which one wants to retrieve all papers that discuss aspirin in the context of treating or preventing atherosclerosis beginning at the entrez home page one would select pubmed from the search pull down menu to indicate that the search is to take place in the bibliographic portion of the entrez search space within the text box to the right one would simply type atherosclerosis mh and aspirin nm the mh qualifying the first term indicates to entrez that this is a mesh term mesh stands for medical subject heading and is the qualifier that should be used when searching by subject the nm qualifying the second term indicates that this is the name of a substance or chemical in this case the query returned papers fig the query is echoed at the top of the new web page the user can further narrow down the query by adding additional terms if the user is interested in a more specific aspect of the pharmacology or if there are quite simply too many papers to read a list of all available qualifiers is given in table at this point to actually look at one of the papers resulting from the search the user can click on a hyperlinked author name by doing so the user is taken to the abstract view for the selected paper figure shows the abstract view for the first paper in the hit list by cayatte et al the abstract view presents the name of the paper the list of authors their institutional affiliation and the abstract itself in standard format a number of alternative formats are available for displaying this information and these various formats can be selected using the pull down menu next to the display button switching to citation format would produce a very similar looking entry the difference being that cataloguing information such as mesh terms and indexed substances relating to the entry are shown below the ab stract medline format produces the medline medlars layout with two letter codes corresponding to the contents of each field going down the left hand side of the entry e g the author field is denoted by the code au entries in this format can be saved and easily imported into third party bibliography management pro grams such as endnote and reference manager at the top of the entry are a number of links that are worth mentioning first on the right hand side is a hyperlink called related articles this is one of the entry points from which the user can take advantage of the neighboring and hard link relationships described earlier if the user clicks on related articles entrez will indicate that there are neighbors associated with the original cayatte reference that is references of similar subject matter and the first six of these papers are shown in figure the first reference in the list is the same cayatte paper because by definition it is most related to itself the parent the order in which the neighbored entries follows is from most statistically similar downward thus the entry closest to the parent is deemed to be the closest in subject matter to the parent by scanning the titles the user can easily find related information on other studies that look at the pharmacology of aspirin in atherosclerosis as well as quickly amass a bibliography of relevant references this is a particularly useful and time saving functionality when one is writing grants or papers because abstracts can be scanned and papers of real interest identified before one heads off for the library stacks the next link in the series is labeled books and clicking on that link will take the user to a heavily hyperlinked version of the original citation the highlighted words in this view correspond to keywords that can take the user to full text books that are available through ncbi the first of these books to be made available is figure a text based entrez query using boolean operators against pubmed the initial query is shown in the search box near the top of the window each entry gives the names of the authors the title of the paper and the citation information the actual record can be retrieved by clicking on the author list t a b l e entrez boolean search statements general syntax search term tag boolean operator search term tag where tag ad affiliation all all fields au author name o brien j au yields all of o brien ja o brien jb etc o brien j au yields only o brien j rn enzyme commission or chemical abstract service numbers edat entrez date yyyy mm dd yyyy mm or yyyy ip issue of journal ta journal title official abbreviation or issn number journal of biological chemistry j biol chem la language majr mesh major ropic one of the major topics discussed in the article mh mesh terms controlled vocabulary of biomedical terms subject ps personal name as subject use when name is subject of article e g varmus h ps dp publication date yyyy mm dd yyyy mm or yyyy pt publication type review clinical trial lectures letter technical publication sh subheading used to modify mesh terms hypertension mh and toxicity sh nm substance name name of chemical discussed in article tw text words all words and numbers in the title and abstract mesh terms subheadings chemical substance names personal name as subject and medline secondary sources uid unique identifiers pmid medline numbers vi volume of journal and boolean operator and or or not figure an example of a pubmed record in abstract format as returned through entrez this abstract view for the first reference shown in figure this view provides links to related articles books linkout and the actual full text journal paper see text for details figure neighbors to an entry found in pubmed the original entry cayette et al is at the top of the list indicating that this is the parent entry see text for details integrated information retrieval the entrez system molecular biology of the cell alberts et al following the cayette example if the user clicks on atherosclerosis at this point it will take them to the relevant part of the textbook a section devoted to how cells import cholesterol by receptor mediated endocytosis fig from this page the user can navigate through this particular unit gathering more general information on transport from the plasma membrane via endosomes and vesicular traffic in the secretory and en docytic pathways the final link in the series in the upper right is linkout this feature provides a list of third party web sites and resources relating to the entrez entry being viewed such as full text of articles that can be displayed directly through the web browser or the capability of ordering the document through services such as loansome doc a cubby service for linkout enables users to customize which links are displayed in a linkout view another way of getting to the full text of an article is by following a direct link to the publisher web site in the abstract view for the cayette example fig a button directly under the citation is marked atvb for arteriosclerosis thrombosis and vascular biology the journal in which the paper is published with the proper individual or institutional subscription privileges one would be able to immediately see the full text of the paper including all figures and tables there is another way to perform an entrez query involving some built in fea tures of the system consider an example in which one is attempting to find all genes coding for dna binding proteins in methanobacteria in this case the search would begin by setting the search pull down menu to nucleotide and then typing the term dna binding into the text box this search returns entries in which the search term appears fig at this point to narrow down the search the user would click on the limits hyperlink directly below the text box this brings the user to a new page that allows the search to be limited as implied by the name of the hyperlink here the search will be limited by organism so the limited to pull down is changed to organism and the word methanobacterium is typed into the search box fig clicking go will now return all of the entries in which methanobacterium is the organism the results from the first search can also be combined with those from the second by clinking on the history hyperlink below the text box resulting in a list of recent queries fig the list shows the individual queries whether those queries were field limited the time at which the query was performed and how many entries that individual query returned to com bine two separate queries into one the user simply combines the queries by number in this case because the queries are numbered and the syntax would be and clicking preview regenerates a table showing the new combined query as containing three entries alternatively clicking go shows the user the three entries in the now familiar nucleotide summary format fig as before there are a series of hyperlinks to the upper right of each entry three are shown for the first entry which is for the m thermoautotrophicum tfx gene the pubmed link takes the user back to the bibliographic entry or entries corresponding to this genbank entry clicking on protein brings into play one of the hard link relationships showing the genpept entry that corresponds to the tfx gene concep tual translation fig notice that within the entry itself the scientific name of the organism is represented as hypertext clicking on that link takes the user to the ncbi taxonomy database where information on that particular organism lineage is available one of the useful views at this level is the graphics view this view figure text related to the original cayette et al entry from molecular biology of the cell alberts et al see text for details figure formulating a search against the nucleotide portion of entrez the initial query is shown in the text box towards the top of the window and the nucleotide entries matching the query are shown below see text for details figure using the limits feature of entrez to limit a search to a particular organism see text for details figure combining individual queries using the history feature of entrez see text for details figure entries resulting from the combination of two individual entrez queries the command producing this entrez is shown in the text box at the top of the figure and information on the individual queries that were combined is given in figure figure the protein neighbor for the m thermoautotrophicum tfx gene clicking on the protein hyperlink next to the first entry in figure leads the user to this genpept entry see text for details attempts to show graphically all of the features described within the entry feature table providing a very useful overview particularly when the feature table is very long the related sequences link shows all sequences similar to that of the tfx gene at the nucleotide level in essence showing the results of a precomputed blast search the last part of entrez to be discussed deals with structures structure queries can be done directly by specifying structure in the search pull down menu for example suppose that one wishes to find out information about the structure of hmg box b from rat whose accession number is typing into the query box leads the user to the structure summary page for which has a decidedly different format than any of the pages seen so far fig this page shows details from the header of the source mmdb document which is derived from pdb links to pubmed and to the taxonomy of the source organism and links to both sequence and structure neighbors the sequence neighbors links show neigh bors to on the basis of sequence that is by blast search thus although this is a structure entry it is important to realize that sequence neighbors have nothing to do with the structural information at least not directly to get information about related structures one of the structure neighbor links can be followed pro ducing a table of neighbors as assessed by vast for a user interested in gleaning initial impressions about the shape of a protein the plug in invoked by click ing on view save structure provides a powerful interface giving far more information than anyone could deduce from simply examining a string of letters the sequence of the protein the protein may be rotated along its axes by means of the scroll bars on the bottom top and right hand side of the window or may be freely rotated by clicking and holding down the mouse key while the cursor is within the structure window and then dragging users are able to zoom in on particular parts of the structure or change the coloration of the figure to determine specific structural features about the protein in figure for instance spacefilling and hy drophobicity were chosen as the render and color options respectively more information on is presented in chapter as well as in the online doc umentation in addition users can save coordinate information to a file and view the data using third party applications such as kinemage richardson and richardson and rasmol sayle and milner white finally at any point along the way in using entrez if there are partial or com plete search results that the user wishes to retain while moving onto a new query the add to clipboard button can be pushed this stores the results of the current query which the user can return to by clicking the clipboard hyperlink directly under the text box the clipboard holds a maximum of items and the items are held in memory for h locuslink the entrez system revolves necessarily around the individual entries making up the various component databases that are part of the entrez search space another way to think about this search space is to organize it around discrete genetic loci ncbi locuslink does just this providing a single query interface to various types of information regarding a given genetic locus such as phenotypes map locations and figure the structure summary for resulting from a direct query of the structures accessible through the entrez system the entry shows header information from the corresponding mmdb entry links to pubmed and to the taxonomy of the source organism and links to sequence and structure neighbors figure the structure of rendered using version an interactive mo lecular viewer that acts as a plug in to web entrez is also bundled with and can be used with network entrez details are given in the text homologies to other genes the locuslink search space currently includes infor mation from humans mice rats fruit flies and zebrafish with the use of the gene for the high mobility group protein as an ex ample the locuslink query begins by the user simply typing the name of the gene into the query box appearing at the top of the locuslink home page alternatively the user could select the gene of interest from an alphabetical list the query on returns three locuslink entries from human mouse and rat fig in this view the user is given the locus id in the first column the locus id is intended to be a unique stable identifier that is associated with this gene locus clicking on the locus id for the human produces the locuslink report view as shown in figure the report view begins with links to external sources of information shown as colored buttons at the top of the page in this particular report the links figure results of a locuslink query using as the search term the report shows three entries corresponding to in human hs mouse mm and rat rn a brief description is given for each found locus as well as its chromosomal location a series of blocks is found to the right of each entry providing a jumping off point to numerous other sources of data these links are described in the text would lead the user to pubmed pub unigene ug cf chapter the dbsnp variation database var cf chapter homologene homol see below and the genome database gdb these offsite links will change from entry to entry depending on what information is available for the gene of interest a complete list of offsite data sources is given in the locuslink online documentation continuing down the report view in the section marked locus information the user is presented with the official gene symbol along with any alternate symbols that may have traditionally been used in the literature or in sequence records this section would also include information on the name of the gene product any aliases for the name of the gene product the enzyme commission number the name of any diseases that result from variants at this gene locus and links to omim and unigene only those pieces of information that are known or are applicable to this particular gene locus are shown in the section labeled map information the report shows what chromosome this locus is on the cytogenetic and genetic map positions when known and any sts markers that are found within the mrna corresponding to this locus there is a hyperlink that can take the user to the entrez map viewer showing the position of this locus and the relationship of this locus to surrounding loci fig the map viewer shows the chromosomal ideogram to the left with the highlighted region marked by a thick bar to the right of the ideogram the user can zoom in or out by clicking on the icon above the ideogram in the main window the user is presented figure the locuslink report view for human the report is divided into six sections providing gene symbol locus map refseq and genbank information as well as links to external data sources see text for details with both the cytogenetic and sequence map in this particular view genes are shown with the original locus of interest highlighted as with most graphical views of this type the majority of the elements in this view are clickable taking the user to more information about that particular part of either the cytogenetic or sequence map the next section deals with refseq information refseq is short for the ncbi reference sequence project which is an effort aimed at providing a stable reference sequence for all of the molecules corresponding to the central dogma of biology the intention is that these reference sequences will provide a framework for making figure the entrez map view for human the chromosomal position is indi cated by the ideogram at the left of the window the main window contains a depiction of both the cytogenetic and sequence map with the gene highlighted interestingly the gene shown at the very bottom of this view like is also a member of the high mobility group family of proteins baxevanis and landsman functional annotations as well as for information regarding mutations gene expres sion and polymorphisms the sequences listed in this section represent the sequences that were used to build the corresponding refseq record notice that refseq nucle otide records are preceded by nm and that protein records are preceded by np a blue button appears next to protein information if there is also structural information available about the protein the final portion of the locuslink report shows the genbank accession numbers that correspond to this locus the middle column in dicates the molecule type for these genbank entries with m standing for mrna g for genomic dna e for an est and u for undetermined in this particular case there is also a link to the genecard for clicking on that hyperlink takes the user to the genecards database at the weizmann institute providing a concise sum mary of the function of this gene another way to proceed through the information is to return to the query result screen shown in figure and use the linked alphabet blocks shown to the right of each of the entries in turn these links are p for pubmed bibliographic entries corresponding to the locus o for the online mendelian inheritance in man summary on this locus r for the refseq entries corresponding to the locus g for the individual genbank entries related to the locus these entries will correspond to the refseq entry as shown in the locuslink report view h for homologene homologene which is discussed in greater detail in chapter allows the user to find both orthologs and homologs for genes represented in locuslink and unigene the assignments being made based on sequence similarity u whether this locus is part of a unigene cluster and v for variation data on this locus contained within dbsnp when following either the pubmed or genbank links the user is in essence returned to the entrez search space enabling the user to take advantage of entrez navigational features once again sequence databases beyond ncbi although it may appear from this discussion that ncbi is the center of the sequence universe many specialized sequence databases throughout the world serve specific groups in the scientific community often these databases provide additional infor mation such as phenotypes experimental conditions strain crosses and map features the data are of great importance to these subsets of the scientific community inas much as they can influence rational experimental design but such types of data do not always fit neatly within the confines of the ncbi data model development of specialized databases necessarily ensued but they are intended to be used as an adjunct to genbank not in place of it it is impossible to discuss all of these kinds of databases here but to emphasize the sheer number of such databases that exist nucleic acids research devotes its first issue every year to papers describing these databases cf baxevanis an example of a specialized organismal database is the saccharomyces genome database sgd housed at the stanford human genome center the database pro vides a very simple search interface that allows text based searches by gene name gene information clone protein information sequence name author name or full text for example using gene name as the search topic and as the name of the gene to be searched for produces a sacchdb information window showing all known information on locus fig this window provides jumping off points to other databases such as genbank genpept mips and the yeast protein database ypd following the link to for this entry provides information on structural homologs of the protein product found in pdb links to sec ondary and tertiary structure prediction sites and precomputed blast reports against a number of query databases returning to the locus window and clicking on the map in the upper right hand corner the user finds a graphical view of the figure a sacchdb locus view resulting from an sgd query using as the gene name the information returned is extensive it includes the name of the gene product phenotypic data and the position of within the yeast genome there are numerous hyperlinks providing access to graphical views and to external database entries related to the current query area surrounding the locus in question available views include physical maps ge netic maps and chromosomal physical maps among others the chromosomal fea tures map view for is shown in figure note the thick bar at the top of the figure which gives the position of the current view with respect to the centro mere clicking on that bar allows the user to move along the chromosome and clicking on individual gene or orf name or as the authors cite in the figure legend any little colorful bar gives more detailed information about that particular region another example of an organism specific database is flybase whose goal is to maintain comprehensive information on the genetics and molecular biology of dro sophila the information found in flybase includes an extensive drosophila bibli ography addresses of over researchers involved in drosophila projects a com pilation of information on over alleles of more than genes information about the expression and properties of over transcripts and proteins and descriptions of over chromosomal aberrations also included is relevant map ping information functional information on gene products lists of stock centers and genomic clones and information from allied databases searches on any of these fields can be done through a simple search mechanism for example searching by gene symbol using capu as the search term brings up a record for a gene named cappuccino which is required for the proper polarity of the developing drosophila oocyte emmons et al calling up the cytogenetic map view generates a map showing the gene and cytologic location of cappuccino and other genes in that im mediate area and users can click on any of the gene bars to bring up detailed information on that particular gene fig the view can be changed by selecting figure a chromosomal features map resulting from the query used to generate the locus view shown in figure chromosome xvi is shown at the top of the figure with the exploded region highlighted by a box most items are clickable returning detailed information about that particular gene orf or construct figure genes view resulting from querying flybase for the cappucino gene capu in the figure between positions and on the cytologic map the graphical view can be changed by clicking on any of the class buttons that appear below the figure as de scribed in the text information on any of the genes shown can be obtained by clicking on the appropriate bar one of the class buttons at the bottom of the window so that a graphical view of cosmids deficiencies duplications inversions transpositions translocations or other aberrations can be examined instead medical databases although the focus of this chapter and the book in general is on sequences data bases cataloguing and organizing sequence information are not the only kinds of databases useful to the biologist an example of such a non sequence based infor mation resource that is tremendously useful in genomics is online mendelian in heritance in man omim the electronic version of the catalog of human genes and genetic disorders founded by victor mckusick at the johns hopkins university mckusick hamosh et al omim provides concise textual information from the published literature on most human conditions having a genetic basis as well as pictures illustrating the condition or disorder where appropriate and full citation information because the online version of omim is housed at ncbi links to entrez are provided from all references cited within each omim entry figure an example of a list of allelic variants that can be obtained through omim the figure shows the list of allelic variants for mckusick kaufman syndrome omim has a defined numbering system in which each entry is assigned a unique number similar to an accession number but certain positions within that number indicate information about the genetic disorder itself for example the first digit represents the mode of inheritance of the disorder stands for autosomal dominant for autosomal recessive for x linked locus or phenotype for y linked locus or phenotype for mitochondrial and for autosomal locus or phenotype the distinction between or and is that entries catalogued before may were assigned either a or whereas entries after that date were assigned a regardless of whether the mode of inheritance was dominant or recessive an asterisk preceding a number indicates that the phenotype caused by the gene at this locus is not influ enced by genes at other loci however the disorder itself may be caused by mutations at multiple loci disorders for which no mode of inheritance has been determined do not carry asterisks finally a pound sign indicates that the phenotype is caused by two or more genetic mutations omim searches are very easy to perform the search engine performs a simple query based on one or more words typed into a search window a list of documents containing the query words is returned and users can select one or more disorders from this list to look at the full text of the omim entry the entries include infor mation such as the gene symbol alternate names for the disease a description of the disease including clinical biochemical and cytogenetic features details on the mode of inheritance including mapping information a clinical synopsis and ref erences a particularly useful feature is lists of allelic variants a short description is given after each allelic variant of the clinical or biochemical outcome of that partic ular mutation there are currently over gene entries containing at least one allelic variant that either causes or is associated with a discrete phenotype in humans figure shows an example of an allelic variant list in this case for mutations observed in patients with mckusick kaufman syndrome mkks internet resources for topics presented in chapter blast http www ncbi nlm nih gov blast http www ncbi nlm nih gov structure shtml endnote http www niles com entrez http www ncbi nlm nih gov entrez flybase http flybase bio indiana edu gdb http www gdb org genecards http bioinfo weizmann ac il cards homologene http www ncbi nlm nih gov homologene kinemage http www umass edu microbio rasmol mage htm locuslink http www ncbi nlm nih gov locuslink mips http www mips biochem mpg de mmdb http www ncbi nlm nih gov structure mmdb mmdb shtml omim http www ncbi nlm nih gov omim pdb http www rcsb org pdb rasmol http www umass edu microbio rasmol reference manager http www risinc com http www genome stanford edu information retrieval from biological databases sgd http genome www stanford edu saccharomyces vast http www ncbi nlm nih gov structure vast vast shtml ypd http www proteome com databases index html problem set you have been watching the evening news and have just heard an interesting story regarding recent developments on the genetics of colorectal cancer you would like to get some more information on this research but the news story was short on details the only hard information you have is that the principal inves tigator was bert vogelstein at the johns hopkins school of medicine a how many of the papers that dr vogelstein has written on the subject of colorectal neoplasms are available through pubmed b a paper by hedrick and colleagues describes the role of the dcc gene product in cellular differentiation and colorectal tumorigenesis based on this study what is the chromosomal location of the dcc gene c dcc codes for a cell surface localized protein involved in tumor suppression from what cell line and tissue type was the human tumor suppressor protein not the precursor isolated d in the dcc human tumor suppressor protein precursor what range of amino acids comprise the signal sequence online mendelian inheritance in man omim indicates that the development of colorectal carcinomas involves a dominantly acting oncogene coupled with the loss of several genes such as dcc that normally suppress tumorigenesis a an allelic variant of dcc also involved in esophageal carcinoma has been cataloged in omim what was the mutation at the amino acid level and what biological effect did it have in patients b based on the mim gene map how many other genes have been mapped to the exact cytogenetic map location as dcc by pcr of somatic cell hybrid dna c the omim entry for dcc is coupled to the mouse genome database at the jackson laboratory showing that the corresponding mouse gene is located on mouse chromosome what is the resultant phenotype of a null mutation of dcc in the mouse a very active area of commercial research involves the identification and devel opment of new sweeteners for use by the food industry whereas traditional sweet eners such as table sugar sucrose are carbohydrates most current research is instead focusing on proteins which have an intrinsically sweet taste because these sweet tasting proteins are much sweeter than their carbohydrate counterparts they are in essence calorie free since so little is used to achieve a sweet taste in food the most successful example of such a protein is aspartame however aspartame is synthetic and does not occur in nature alternate natural protein sources are being investigated including a sweet tasting protein called monellin a according to ogata and colleagues how much sweeter than ordinary sugar is monellin on both a molar and weight bases references b based on the swiss prot entry for monellin chain b from serendipity berry how many a helices and strands does this protein possess c what residue amino acid and position when blocked abolishes monellin sweet taste d three dimensional structures are available for monellin what other structure is most closely related to monellin structure as assessed by vast p value does this structure have the highest sequence similarity to as well e the monellin structure is based on a single chain fusion product how do the stability and renaturation properties of the fusion product differ from that of the native protein references alberts b bray d lewis j raff m roberts k and watson j d molecular biology of the cell garland publishing new york altschul s gish w miller w myers e and lipman d basic local alignment search tool j mol biol baxevanis a d and landsman d the hmg box protein family classification and functional relationships nucleic acids res baxevanis a d the molecular biology database collection an updated compi lation of biological database resources nucleic acids res emmons s phan h calley j chen w james b and manseau l cappuccino a drosophila maternal effect gene required for polarity of the egg and embryo is related to the vertebrate limb deformity locus genes dev gibrat j f madej t and bryant s surprising similarities instructure comparison curr opin struct biol hamosh a scott a f amberger j valle d and mckusick v a online men delian inheritance in man omim human mutation madej t gibrat j f and bryant s threading a database ofprotein cores proteins mckusick v a mendelian inheritance in man catalogs of human genes and genetic disorders the johns hopkins university press baltimore richardson d and richardson j the kinemage a tool for scientific communication protein sci sayle r and milner white e rasmol biomolecular graphics for all trends biochem sci wilbur w and coffee l the effectiveness of document neighboring in search enhancement process manage wilbur w and yang y an analysis of statistical term strength and its use in the indexing and retrieval of molecular biology texts comput biol med bioinformatics a practical guide to the analysis of genes and proteins second edition andreas d baxevanis b f francis ouellette copyright john wiley sons inc isbns hardback paper electronic sequence alignment and database searching gregory d schuler national center for biotechnology information national library of medicine national institutes of health bethesda maryland introduction there is a long tradition in biology of comparative analysis leading to discovery for instance darwin comparison of morphological features of the galapagos finches and other species ultimately led him to postulate the theory of natural selec tion in essence we are performing the same type of analysis today when we compare the sequences of genes and proteins but in much greater detail in this activity the similarities and differences at the level of individual bases or amino acids are analyzed with the aim of inferring structural functional and evolutionary relation ships among the sequences under study the most common comparative method is sequence alignment which provides an explicit mapping between the residues of two or more sequences in this chapter only pairwise alignments in which only two sequences are compared will be discussed the process of constructing multiple alignments which involves more than two sequences is discussed in chapter the number of sequences available for comparison has grown explosively since the when development of rapid dna sequencing methodology sparked the big bang of sequence information expansion comparison of one sequence to the entire database of known sequences is an important discovery technique that should be at the disposal of all molecular biologists over the past years improvements in the speed and sophistication of sequence alignment algorithms not to mention perfor mance of computers have more than kept pace with the growth in the size of the sequence databases today with the complete genomes and large cdna sequence collections available for many organisms we are in the era of comparative gen omics in which the full gene complement of two organisms can be compared with one another the evolutionary basis of sequence alignment one goal of sequence alignment is to enable the researcher to determine whether two sequences display sufficient similarity such that an inference of homology is justified although these two terms are often interchanged in popular usage let us distinguish them to avoid confusion in the current discussion similarity is an ob servable quantity that might be expressed as say percent identity or some other suitable measure homology on the other hand refers to a conclusion drawn from these data that two genes share a common evolutionary history genes either are or are not homologous there are no degrees for homology as there are for similarity for example figure shows an alignment between the homologous trypsin proteins from mus musculus house mouse and astracus astracus broad fingered crayfish from which it can be calculated that these two sequences have identity bearing in mind the goal of inferring evolutionary relationships it is fitting that most alignment methods try at least to some extent to model the molecular mech anisms by which sequences evolve although it is presumed that homologous se quences have diverged from a common ancestral sequence through iterative molec ular changes it is actually known what the ancestral sequence was barring the possibility that dna could be recovered from a fossil all that can be observed are mouse ivggynceensvpyqvslns gyhfcggslineqwvvsaghcyk sriqv crayfish ivggtdavlgefpyqlsfqetflgfsfhfcgasiynenyaitaghcvygddyenpsglqi mouse rlgehnievlegneqfinaakiirhpqydrktlnndimliklssravinarvstislpta crayfish vageldmsvnegseqtitvskiilhenfdydlldndisllklsgsltfnnnvapialpaq mouse ppatgtkclisgwgntassgadypdelqcldapvlsqakceasypg kitsnmfcvgfle crayfish ghtatgnvivtgwg ttseggntpdvlqkvtvplvsdaecrddygadeifdsmicagvpe mouse ggkdscqgdsggpvvcng qlqgvvswgdgcaqknkpgvytkvynyvkwikntiaan crayfish ggkdscqgdsggplaasdtgstylagivswgygcarpgypgvytevsyhvdwikanav figure conserved positions are often of functional importance alignment of trypsin proteins of mouse swiss prot and crayfish swiss prot identical resi dues are underlined indicated above the alignments are three disulfide bonds s s with participating cysteine residues conserved amino acids side chains involved in the charge relay system asterisk and active side residue governing substrate specificity diamond the raw sequences from extant organisms the changes that occur during divergence from the common ancestor can be categorized as substitutions insertions and de letions in the ideal case in which a sequence alignment genuinely reflects the evo lutionary history of two genes or proteins residues that have been aligned but are not identical would represent substitutions regions where the residues of one se quence correspond to nothing in the other would be interpreted as either an insertion into one sequence or a deletion from the other these gaps are usually represented in the alignment as consecutive dashes or other punctuation character aligned with letters for example the alignment in figure contains five gaps in a residue by residue alignment it is often apparent that certain regions of a protein or perhaps specific amino acids are more highly conserved than others this information may be suggestive as to which residues are most crucial for maintaining a protein structure or function in the trypsin alignment of figure the active site residues that determine substrate specificity and provide the charge relay sys tem of serine proteases correspond to conserved positions as do the cysteines res idues that form several disulfide bonds important for maintaining the enzyme struc ture on the other hand there may be other positions that do not play a significant functional role yet happen to be identical for historical reasons particular caution should be taken when the sequences are taken from very closely related species because similarities may be more reflective of history than of function for example regions of high sequence similarity between mouse and rat homologs may simply be those that have not had sufficient time to diverge nevertheless sequence align ments provide a useful way to gain new insights by leveraging existing knowledge such as deducing structural and functional properties of a novel protein from com parisons to those that have been well studied it must be emphasized however that these inferences should always be tested experimentally and not assumed to be cor rect based on computational analysis alone by observing a surprisingly high degree of sequence similarity between two genes or proteins we might infer that they share a common evolutionary history and from this it might be anticipated that they should also have similar biological functions but again this should be treated as hypothetical until tested experimen tally zeta crystallin for instance is a component of the transparent lens matrix of the vertebrate eye however on the basis of extended sequence similarity it can be inferred that its homolog in e coli is the metabolic enzyme quinone oxidoreductase fig despite the common ancestry the function has changed during evolution gonzalez et al this is analogous to a railroad car that has been converted into a diner inspection of the exterior structure reveals the structure history but relying exclusively on this information may lead to an erroneous conclusion about its current function when a gene adapts to a new niche it might also be anticipated that the pattern of conserved positions would change for example active site resi dues should be conserved so long as the protein plays a role in catalysis but could drift once the protein takes on a different function the earliest sequence alignment methods were applicable to a simple type of relationship in which the sequences show easily detectable similarity along their entire lengths an alignment that essentially spans the full extents of the input se quences is called a global alignment the trypsin and quinone oxidoreductase zeta crystallin alignments discussed above are both examples of global alignments pro teins consisting of a single globular domain can often be aligned using a global strategy as can any homologous sequences that have not diverged substantially human zcr matgqklmravrvfefggpevlklrsdiavpipkdhqvlikvhacgvnpvetyirsgtys ecoli qor matriefhkhggpevlqa veftpadpaeneiqvenkaiginfidtyirsglyp human zcr rkpllpytpgsdvagvieavgdnasafkkgdrvftsstisggyaeyalaadhtvyklpek ecoli qor ppslpsglgteaagivskvgsgvkhikagdrvvyaqsalgayssvhniiadkaailpaa human zcr ldfkqgaaigipyftayralihsacvkagesvlvhgasggvglaacqiarayglkilgta ecoli qor isfeqaaasflkgltvyyllrktyeikpdeqflfhaaaggvgliacqwakalgakligtv human zcr gteegqkivlqngahevfnhrevnyidkikkyvgekgidiiiemlanvnlskdlsllshg ecoli qor gtaqkaqsalkagawqvinyreedlverlkeitggkkvrvvydsvgrdtwersldclqrr human zcr grvivvg srgtieinprdtmakes siigvtlfsstkeefqqyaaalqagmeigwl ecoli qor glmvsfgnssgavtgvnlgilnqkgslyvtrpslqgyittreelteasnelfsliasgvi human zcr kpvigsq yplekvaeaheniihgsgatgkmilll ecoli qor kvdvaeqqkyplkdaqrahe ilesratqgssllip figure optimal global sequence alignment alignment of the amino acid sequences of human zeta crystallin swiss prot and e coli quinone oxidoreductase swiss prot it is an optimal global alignment produced by the clustal w program higgins et al identical residues are marked by asterisks below the alignment and dots indicate conserved residues the modular nature of proteins many proteins do not display global patterns of similarity but instead appear to be mosaics of modular domains baron et al doolittle and bork patthy one example of this is illustrated in figure which shows the modular structure of two proteins involved in blood clotting coagulation factor xii and tissue type plasminogen activator plat besides the catalytic domain which pro vides the serine protease activity these proteins have different numbers of other structural modules two types of fibronectin repeats a domain with similarity to epidermal growth factor and a module that is called a kringle domain these modules can be repeated or appear in different orders patterns of modularity often arises by in frame exchange of whole exons patthy global alignment meth ods do not take this phenomenon into account which is understandable considering that they were developed before the exon intron structure of genes had been discov plat figure modular structure of two proteins involved in blood clotting schematic representation of the modular structure of human tissue plasminogen activator and co agulation factor xii a module labeled c is shared by several proteins involved in blood clotting and are frequently repeated units that were first seen in fibronectin e is a module resembling epidermal growth factor a module known as a kringle domain is denoted k ered in most cases it is advisable to instead use a sequence comparison method that can produce a local alignment such an alignment consists of paired subse quences that may be surrounded by residues that are completely unrelated conse quently users should bear in mind that some local similarities could be missed if a global alignment strategy is applied inappropriately another obvious case in which local alignments are desired is the alignment of the nucleotide sequence of a spliced mrna to its genomic sequence where each exon would be a distinct local alignment dot matrix representations have enjoyed a widespread popularity in part because of their ability to reveal complex relationships involving multiple regions of local similarity fitch gibbs and mcintyre an example of this approach is shown in figure in which the and plat protein sequences have been compared using dotter sonnhammer and durban the basic idea is to use the sequences as the coordinates of a two dimensional graph and then plot points of correspondence within its interior each dot usually indicates that within some small window the sequence similarity is above some cutoff or a range of cutoffs with the use of dotter each plotted using a different shade of gray when two sequences are coagulation factor xii e e k catalytic figure dot matrix sequence comparison dot matrix comparison of the amino acid sequences of human coagulation factor xii swiss prot and tissue plasmin ogen activator plat swiss prot the figure was generated using the dotter pro gram sonnhammer and durban consistently matching over an extended region the dots will merge to form a diag onal line segment it is instructive to compare the positions of the diagonals in dot matrix of figure with the known modular structure of the two proteins in par ticular note the way in which repeated domains appear starting with the kringle domain in the plat and scanning horizontally two diagonal segments may be seen corresponding to the two kringle domains present in the sequence although more sophisticated methods for finding local similarities are now available discussed below dot matrix representations have remained popular as illustrative tools in a dot matrix representation certain patterns of dots may appear to sketch out a path but it is up to the viewer to deduce the alignment from this information another graphical representation known as a path graph provides an explicit repre sentation of an alignment figure illustrates the relationship between the dot matrix path graph and alignment representations for the egf similarity domain present in both the tissue type plasminogen activator plat and the urokinase type plasminogen activator plau proteins to understand a path graph imagine a two dimensional lattice in which the vertices represent points between the sequence res idues as opposed to the residues themselves as in the case of the dot matrix an edge that connects two vertices along a diagonal corresponds to the pairing of one a b plau plau c plau epkkvkdhcskhspcqkggtcvnmp sgph clcpqhltgnhcqkek cfe plat elhqvpsncd clnggtcvsnkyfsnihwcncpkkfggqhceidksktcye figure dot matrix path graph and alignment all three views represent the align ment of the egf similarity domains in the human urokinase plasminogen activator plau swiss prot and tissue plasminogen activator plat swiss prot proteins a the entire proteins were compared with dotter and an enlargement of the small region corresponding to the egf domain is shown here b the path graph representation of the alignment found by blastp c the blastpgp alignment represented in the familiar text form residue from each sequence horizontal and vertical edges pair a residue from one sequence with nothing in the other in other words these edges constitute a gap in the alignment the entire graph corresponds to the search space which must be examined for potential alignments each possible path through this space corresponds to exactly one alignment optimal alignment methods for any but the most trivial problems the total number of distinct alignments is extraordinarily large so it is usually of interest to identify the best one among them or the several best ones this is where the concept of representing an align ment as a path pays off many problems in computer science can be reduced to the task of finding the optimal path through a graph for instance the problem of finding the most efficient way to route a telephone call from new york to san francisco and efficient algorithms have been developed for this purpose one requirement is a means of assigning a quality score to each possible path alignment normally this is accomplished by summing the incremental contributions of each step along its route more sophisticated scoring schemes are discussed below but for now let us assume that some positive incremental scores will be used for aligning identical residues with negative scores used for substitutions and gaps according to this definition of alignment quality finding the path whose total score is maximal will give us the best sequence alignment what is today known as the needleman wunsch algorithm is an application of a best path strategy called dynamic programming to the problem of finding optimal sequence alignments needleman and wunsch the basic idea behind dynamic programming comes from the observation that any partial subpath that ends at a point along the true optimal path must itself be the optimal path leading up to that point thus the optimal path can be found by incremental extension of optimal subpaths in the basic needleman wunsch formulation the optimal alignment must extend from beginning to end in both sequences that is from the top left corner in the search space to bottom right as it is typically drawn in other words it seeks global alignments a simple modification to the basic strategy allows the optimal local alignment to be found smith and waterman the path for this alignment does not need to reach the edges of the search graph but may begin and end inter nally such an alignment would be locally optimal if its score cannot be improved either by increasing or decreasing the extent of the alignment the smith waterman algorithm relies on a property of the scoring system in which the cumulative score for a path will decrease in regions of poorly matching sequences the scoring systems described below satisfy this criterion when the score drops to zero extension of path is terminated and a new one can begin there can be many individual paths bounded by regions of poorly matching sequence the one with the highest score is reported as the optimal local alignment it is important to bear in mind that optimal methods always report the best alignment that can be achieved even if it has no biological meaning on the other hand when searching for local alignments there may be several significant align ments so it is a mistake to look only at the optimal one refinements to the smith waterman algorithm were proposed for detecting the k best nonintersecting local alignments altschul and erickson sellers waterman and eggert these ideas were later extended in the development of the sim algorithm huang et al a program called lalign distributed with the fasta package provides a useful implementation of sim pearson looking for suboptimal alignments is especially important when comparing multimodule proteins this is illustrated in figure in which the lalign program was used to find the three best local align ments of the human coagulation factor ix and factor xii proteins the second and comparison of a human aa gi sp coagulation fa aa b hum aa gi sp coagulation aa using protein matrix identity in aa overlap score qsfndftrvvggedakpgqfpwqvvlngkvdafcggsivnekwivtaahcve tgvki kslssmtrvvgglvalrgahpyiaaly wghsfcagsliapcwvltaahclqdrpapedl tvvagehnieetehteqkrnviriiphhnynaainkynhdialleldepl vlnsy tvvlgqerrnhscepcqtlavrsyrlheafspv syqhdlallrlqedadgscallspy 370 vtpiciadkeytniflkfgsgyvsgwgrvfhkgrs alvlqylrvplvdratclrstkf vqpvclpsgaarpsettlcq vagwghqfegaeeyasflqeaqvpflslercsapdvhg 520 430 tiynnmfcagfheggrdscqgdsggphvtevegts fltgiiswgeecamkgkygiy ssilpgmlcagfleggtdacqgdsggplvcedqaaerrltlqgiiswgsgcgdrnkpgvy 570 tkvsryvnwikekt tdvayylawireht 610 identity in aa overlap score vdgdqcesnpclnggsckddinsyecwcpfgfegknceldvtcnikngr lasqacrtnpclhggrcleveghrlchcpvgytgpfcdvdtkascydgr identity in aa overlap score f9 dqcesn pclnggsckddinsyecwcpfgfegknce dhcskhspcqkggtcvnmpsgphclcpqhltgnhcq figure optimal and suboptimal local alignments the three best alignments found when using lalignto align the sequences of human coagulation factor ix f9 swiss prot and coagulation factor xii swiss prot third alignments represent functional modules that would have been missed by a standard smith waterman search which would have reported only the first optimal alignment substitution scores and gap penalties the scoring system described above made use of a simple match mismatch scheme but when comparing proteins we can increase sensitivity to weak alignments through the use of a substitution matrix it is well known that certain amino acids can substitute easily for one another in related proteins presumably because of their similar physicochemical properties examples of these conservative substitutions include isoleucine for valine both small and hydrophobic and serine for threonine both polar when calculating alignment scores identical amino acids should be given greater value than substitutions but conservative substitutions should also be greater than nonconservative changes in other words a range of values is desired furthermore different sets of values may be desired for comparing very similar sequences e g a mouse gene and its rat homolog as opposed to highly divergent sequences e g mouse and yeast genes these considerations can be dealt with in a flexible manner through the use of a substitution matrix in which the score for any pair of amino acids can be easily looked up the first substitution matrices to gain widespread usage were those based on the point accepted mutation pam model of evolution dayhoff et al one pam is a unit of evolutionary divergence in which of the amino acids have been changed this does not imply that after pams every amino acid will be different some positions may change several times perhaps even reverting to the original amino acid whereas others may not change at all if there were no selection for fitness the frequencies of each possible substitution would be primarily influenced by the overall frequencies of the different amino acids called the background fre quencies however in related proteins the observed substitution frequencies called the target frequencies are biased toward those that do not seriously disrupt the protein function in other words these are point mutations that have been ac cepted during evolution dayhoff and coworkers were the first to explicitly use a log odds approach in which the substitution scores in the matrix are proportional to the natural log of the ratio of target frequencies to background frequencies to es timate the target frequencies pairs of very closely related sequences which could be aligned unambiguously without the aid of a substitution matrix were used to collect mutation frequencies corresponding to pam and these data were then extrapolated to a distance of pams the resulting matrix is shown in figure although was the only matrix published by dayhoff et al the underlying mutation data can be extrapolated to other pam distances to produce a family of matrices when aligning sequences that are highly divergent best results are obtained at higher pam values such as or ma trices constructed from lower pam values can be used if the sequences have a greater degree of similarity altschul the blosum substitution matrices have been constructed in a similar fashion but make use of a different strategy for estimating the target frequencies henikoff and henikoff the underlying data are derived from the blocks database henikoff and henikoff which contains local multiple alignments blocks a r n d c q e g h i l k m f p s t w y v a r n d c q e g h i l k m f p s t w y v figure the scoring matrix involving distantly related sequences as opposed to the closely related sequences used for pam although there is no evolutionary model in this case it is advanta geous to have data generated by direct observation rather than extrapolation as with the pam model there is a numbered series of blosum matrices but the number in this case refers to the maximum level of identity that sequences may have and still contribute independently to the model for example with the ma trix sequences having at least identity are merged into a single sequence so that the substitution frequencies are more heavily influenced by sequences that are more divergent than this cutoff see fig substitution matrices have been con structed using higher cutoffs up to for comparing very similar se quences and lower cutoffs down to for highly divergent sequences it is desirable to allow some gaps to be introduced into an alignment to com pensate for insertions and deletions but not so many that the alignment asserts an a r n d c q e g h i l k m f p s t w y v a r n d c q e g h i l k m f p s t w y v figure the scoring matrix implausible series of molecular alterations this is accomplished by deducting some amount from the alignment score for each gap introduced although a number of strategies have been proposed for penalizing gaps the most common formulation known as affine gap penalties involves a fixed deduction for introducing a gap plus an additional deduction proportional to the length of the gap this is governed by two parameters g sometimes called the gap opening penalty and l the gap extension penalty for a gap of length n the total deduction would be g ln unfortunately the selection of gap parameters is highly empirical there is little theory to support the choice of any particular set of values however it is common to use a high value for g around in the context of and a low value for l around or the rationale for this is that insertion and mutation events are rare but when they do occur several adjacent residues may be involved statistical significance of alignments for any given alignment one can calculate a score representing the quality of the alignment but an important question is whether or not this score is high enough to provide evidence of homology in addressing this question it is helpful to have some notion of how high of a score can be expected due purely to chance alone unfor tunately there is no mathematical theory to describe the expected distribution of scores for global alignments one of the few methods available for assessing their significance is to compare the observed alignment score with those of many align ments made from random sequences of the same length and composition as those under study altschul and erickson fitch however for local alignments the situation is much better a statistical model advanced by karlin and altschul provides a mathematical theory to describe the expected distribution of random local alignment scores dembo et al karlin and altschul the form of the probability density function is known as the extreme value distribution this is worth noting because application of the more familiar normal distribution can result in greatly exaggerated claims of significance the extreme value distribution is characterized by two parameters k and a which should be tailored for the particular set of alignment scoring rules and residue back ground frequencies at hand although analytical calculation of these parameters can currently be done only for alignments that lack gaps methods have been developed to estimate appropriate values of k and a for gapped alignments altschul and gish waterman and vingron by relating an observed alignment score s to the expected distribution it is possible to calculate statistical significance in the form of an e value the simple interpretation of an e value is the number of alignments with scores at least equal to s that would be expected by chance alone the signif icance of an alignment also depends on the size of the search space that was used larger databases produce more chance alignments the search space has typically been calculated as the product of the sequence lengths but for correct statistics the lengths must be reduced by the expected length of a local alignment to avoid an edge effect altschul and gish this is due to the fact that an alignment that begins near the edge of the search space will run out of sequence before it can achieve a significant score database similarity searching the discussion so far has focused on the alignment of specific pairs of sequences but for a newly determined sequence one would generally have no way of knowing the appropriate sequence or sequences to use in such a comparison database sim ilarity searching allows us to determine which of the hundreds of thousands of se quences present in the database are potentially related to a particular sequence of interest this process sometimes leads to unexpected discoveries the first eureka moment with this strategy came when the viral oncogene v sis was found to be a modified form of the normal cellular gene that encodes platelet derived growth factor doolittle et al waterfield et al at the time of this discovery sequence databases were small enough that such a finding might have been considered sur prising today however it would be much more surprising to perform a database search and not get a hit large numbers of partial sequences representing novel human and mouse genes have been deposited in genbank as a result of a number of expressed sequence tag projects see chapter the genomes of s cerevisiae c elegans and d melanogaster have been completely sequenced as have many bacterial and viral genomes more recently an intermediate form of the human ge nome is now available in the form of a working draft given this explosion of data the new challenge has become how to focus searches in such a way as to reduce search times and limit the number of results that must be examined in database searching the basic operation is to sequentially align a query se quence to each subject sequence in the database the results are reported as a ranked hit list followed by a series of individual sequence alignments plus various scores and statistics e g fig as will be discussed in more detail below that choice of search program sequence database and various optional parameters can have an a b gi sp galactose phosphate ur aa initn opt z score e smith waterman score identity in aa overlap hit msfrfg qhlikpsvvflktelsfalvnrkpv x galt vwasnflpdiaqreersqqtyhnqhgkpllleyghqellrkerlvltseywivlvpfwav 220 hit vpghvlvcplrpverfhdlrpdevadlfqttqrvgtvvekhfhgtsltfsm qdgp x galt wpfqtlllprrhvqrlpeltpaerddlastmkklltkydnlfe tsfpysmgwhgapmgl 280 110 140 hit eagqtvkh vhvhvlprkagdfhrndsiyeelqkhdkedfpaswrseeemaaeaaalrv galt ktgatcdhwqlhahyyppllrsatvrkfmvgyemlaqaqrdltpeqaaerlrvlpevhyc 320 340 figure output of a fasta search a hit list from a fasta search with human histidine triad hit protein swiss prot as the query against the swissprot database the search was performed using ktup b optimal local alignment of the query to one of the database entries marked by arrow in hit list containing the sequence of rat galactose phosphate uridylyltransferase galt although the sequence similarity is weak these pro teins have been shown to share structural similarity impact on the effectiveness of a search furthermore there are various interfaces to these facilities such as console style commands web based forms and e mail fig ure shows an example of performing a database search using the blast web interface one advantage of this approach is that for any interesting alignment ob served complete annotation and literature citations can be obtained simply by fol lowing hypertext links to the original sequences entries and related on line literature current sequence databases are immense and have continued to increase at an exponential rate making straightforward application of dynamic programming meth ods impractical for database searching one solution is to use massively parallel computers and other specialized hardware but for the purposes of this discussion we will consider only what can be done using general purpose computers with optimal methods being impractical it is necessary to resort to heuristic methods which make use of approximations to significantly speed up sequence comparisons but with a small risk that true alignments can be missed one heuristic method is based on the strategy of breaking a sequence up into short runs of consecutive letters called words word based methods were introduced in the early and are used by virtually all popular search programs in use today wilbur and lipman the basic idea is that an alignment representing a true sequence relationship will contain at least one word that is common to both sequences these word hits can be identified extremely rapidly by preindexing all words from the query and then consulting the index as the database is scanned fasta the first widely used program for database similarity searching was fasta lipman and pearson pearson and lipman pearson to achieve a high degree of sensitivity this program performs optimized searches for local alignments using a substitution matrix however as noted above it would take a substantial amount of time to apply this strategy exhaustively to improve speed the program uses the observed pattern of word hits to identify potential matches before attempting the more time consuming optimized search the trade off between speed and sen sitivity is controlled by the ktup parameter which specifies the size of a word increasing the value of ktup decreases the number of background word hits i e those that do not mark the position of an optimal alignment this in turn decreases the amount of optimized searching required and improves overall search speed the default ktup value for comparing proteins is but for finding very distant relation ships it is recommended that it be reduced to the fasta program does not investigate every word hit encountered but instead looks initially for segments containing several nearby hits by using a heuristic method these segments are assigned scores and the score of the best segment found appears in the output as the score figure several segments may then be combined and a new initn score is calculated from the ensemble most potential matches are then further evaluated by performing a search for a gapped local align ment that is constrained to a diagonal band centered around the best initial segment the score of this optimized alignment is shown in the output as the opt score for those alignments finally reported a user specified number from the top of the hit list a full smith waterman alignment search without the constraining band is performed an example of such an alignment is shown in figure it should be noted that only the single optimal alignment is produced for each database sequence figure database similarity search on the world wide web the figure illustrates the use of the ncbi blast web front end the query sequence should be pasted from the clipboard into the large text field where the sequence of is shown in this figure other essential elements of the search are the name of the search program and the da tabase both of which may be selected from drop down lists additional optional param eters may be set if desired in addition to this advanced blast form there is also a basic blast form in which the advanced options are hidden in either case simply press the submit query button to begin the search as pointed out above meaningful alignments can be missed by this approach if the proteins contain multiple modules consequently it is recommended that matching sequences be further analyzed with the lalign program beginning with version fasta provides an estimate of the statistical sig nificance of each alignment found the program assumes an extreme value distri bution for random scores but with the use of a rewritten form of the probability density function in which the expected score is a linear function of the natural log of the length of the database sequence simple linear regression can then be used to calculate a normalized z score for each alignment finally an expectation e is cal culated which gives the expected number of random alignments with z scores greater than or equal to the value observed blast the blast programs introduced a number of refinements to database searching that improved overall search speed and put database searching on a firm statistical foun dation altschul et al one innovation introduced in blast is the idea of neighborhood words instead of requiring words to match exactly a word hit is achieved if the word taken from the subject sequence has a score of at least t when a comparison is made using a substitution matrix to the word from the query this strategy allows the word size w to be kept high for speed without sacrificing sensitivity thus t becomes the critical parameter determining speed and sensitivity and w is rarely varied if the value of t is increased the number of background word hits will go down and the program will run faster reducing t allows more distant relationships to be found the occurrence of a word hit is followed by an attempt to find a locally optimal alignment whose score is at least equal to a score cutoff s this is accomplished by iteratively extending the alignment both to the left and to the right with accumulation of incremental scores for matches mismatches and the introduction of gaps in practice it is more convenient to specify an expectation cutoff e which the program internally converts to an appropriate value of s which would depend on the search context in regions where matching residues are scarce the cumulative score will begin to drop as the mismatch and gap penalties mount it becomes less likely that the score will rebound and ultimately reach s this observation provides the basis for an additional heuristic whereby the extension of a hit is terminated when the reduction in score relative to the maximum value encountered exceeds the score dropoff threshold x using smaller values of x improves performance by reducing the time spent on unpromising hit extensions at the expense of occasionally missing some true alignments there are several variants of blast each distinguished by the type of sequence dna or protein of the query and database sequences see table the blastp program compares a protein query to a protein database the corresponding program for nucleotide sequences is blastn if the sequence types differ the dna sequence can be translated by the program in all six reading frames and compared to the protein sequence blastx compares a dna query sequence to the protein database which is useful for analyzing new sequence data and ests for a protein query against a nucleotide database use the tblastn program this is useful for finding unannotated coding regions in database sequences a final variant is used only in blast t a b l e blast programs program query database comments blastp protein protein uses substitution matrix for finding distant relationships seg filtering available blastn nucleotide nucleotide tuned for very high scoring matches not distant relationships blastx nucleotide translated protein useful for analysis of new dna sequences and ests tblastn protein nucleotide translated useful for finding unannotated coding regions in database sequences tblastx nucleotide translated nucleotide translated may be useful for est analysis but computationally intensive specialized situations but is mentioned here for the sake of completeness tblastx takes dna query and database sequences translates them both and compares them as protein sequences this program is mainly useful for comparisons of ests where it is suspected that sequences may have coding potential even though the exact coding region has not been determined all of these programs make use of sequence databases located on server ma chines which obviates the need for any local database maintenance some protein and nucleotide sequences databases currently available from the ncbi for blast searching are listed in tables and for routine searches the nr database provides comprehensive collections of both amino acid and nucleotide sequence data with redundancy reduced by merging sequences that are completely identical to examine all sequences submitted or updated within the last days a database called month is provided both nr and month are updated on a daily basis several other databases listed in tables and are useful in more specialized situations such as comparing against the complete genomes of model organisms ecoli or yeast searching specific classes of sequences est or sts or testing for the presence of contaminating or otherwise problematic sequences vector alu or mito t a b l e protein sequence databases for use with blast database description nr non redundant merge of swiss prot pir prf and proteins derived from genbank coding sequences and pdb atomic coordinates month subset of nr which is new or modified within the last days swissprot the swiss prot database pdb amino acid sequences parsed from atomic coordinates of three dimensional structures ecoli complete set of proteins encoded by the e coli genome yeast complete set of proteins encoded by the s cerevisiae genome drosoph complete set of proteins encoded by the d melanogaster genome t a b l e nucleotide sequence databases for use with blast database description nr nonredundant genbank excluding the est sts and gss divisions month subset of nr which is new or modified within the last days est genbank est division expressed sequence tags sts genbank sts division sequence tagged sites htgs genbank htg division high throughput genomic sequences gss genbank gss division genome survey sequences ecoli complete genomic sequence of e coli yeast complete genomic sequence of s cerevisiae drosoph complete genomic sequence of d melanogaster mito complete genomic sequences of vertebrate mitochondria alu collection of primate alu repeat sequences vector collection of popular cloning vectors an example of a blast search will serve to introduce various elements of a search output for the example in figure the amino acid sequence of one of the alzheimer disease susceptibility proteins conceptual translation of genbank was used as the query in a tblastx search of the est database one goal of such a search would be to identify cdna clones for potential homologs in model organisms thereby opening the door for experimental studies that would not be practical in humans the clones corresponding to est sequences are readily avail able each of the est sequences in the database is translated in all reading frames before they are compared against the alzheimer protein sequence figure shows the hit list produced by this search the first two columns give the identifiers and descriptions for each sequence having a significant match although the defini tions are truncated in this overview the figures shows that sequences from both mouse and drosophila are represented the next column gives the reading frame that produced the best alignment although there may be hits to translations from other frames as well the next three columns provide the score of the best alignment the sum p value and the number of hsps that were used in the p value calculation the alignment involving one of the drosophila ests marked by the arrow is shown in figure there are actually two alignments involved and scores are provided for each in each case the conceptual translation of the est is shown aligned with the query sequence identical amino acids are echoed to the text line in between the sequences and plus symbols are used to indicate nonidentical residues that have positive substitution scores i e conservative substitutions it is noteworthy that the two alignments arise from different reading frames and are adjacent to one another as can be seen from the sequence coordinates this pattern is indicative of a reading frame error in the est sequence when analyzing sequence single pass data it is extremely useful to have tools that are relatively error tolerant database searching artifacts a query sequence that contains repetitive elements is likely to produce many false and confounding database matches one clue that this may be a problem is the a reading high smallest sum probability sequences producing high scoring segment pairs frame score p n n gb s1 soares retina h gb infant brain bento soares gb r1 soares mouse nml mus m gb s1 homo sapiens cdna clon gb s1 homo sapiens cdna clon gb r1 soares mouse nml mus m 239 gb r1 soares mouse nml mus m 5e gb r1 homo sapiens cdna clon 5e gb r1 soares mouse mus gb s1 soares pregnant uterus gb r1 stratagene mouse skin gb r1 homo sapiens cdna clon gb r1 soares mouse nml mus m 234 gb r1 beddington mouse embry gb r1 homo sapiens cdna clon gb r1 soares nhhmpu homo gb r1 soares mouse gb s1 homo sapiens cdna clon gb s1 homo sapiens cdna clon 211 gb r1 soares pregnant uterus gb r1 soares mouse mus gb ld drosophila embr gb r1 beddington mouse embry 128 gb r1 homo sapiens cdna clon gb s1 soares nbhtgbc homo sa gb s1 homo sapiens cdna clon 0e gb rattus sp cdna end gb 5prime ld drosophila embr 5e gb 5prime ld drosophila embr 0e gb s1 homo sapiens cdna clon 151 gb 5prime ld drosophila embr gb r1 soares nbhtgbc homo sa gb seq f human fetal heart 0039 b gb 5prime ld drosophila embryo drosophila melanogaster cdna clone length score bits expect sum p identities positives frame query tiksvrfytekngqliyttftedtpsvgqrllnsvlntlimisvivvmtiflvvlykyrc i s fy l yt f e p li sv vvmt l vlyk rc sbjct sinsisfynstdvyllytpfheqspepsvkfwsalgsslilmsvvvvmtfllivlykkrc score bits expect sum p identities positives frame query leeeltlkygakhvimlfvpvtlcmivvva eee lkyga hvi lfvpv lcm vvva sbjct meeeqglkygaqhviklfvpvslcmlvvva figure output of a tblastn search the protein product of the alzheimer disease susceptibility gene genbank was used as the query in a tblastn search against the est database the goal was to identify cdna clones from other organisms that may represent homologs of the human gene a portion of the hit list showing the best hits each sequence is identified by genbank accession number and a portion of the definition line the reading frame and score of the best hsp are shown together with the sum probability of a chance occurrence the value in the last column gives the number of hsps that were used in the sum probability calculation at least sequences from mouse and one from drosophila may be seen on the hit list b match to the conceptual translation of the drosophila est sequence genbank two hsps were found each in a different reading frame identical residues are echoed to the central line and plus symbols indicate pairs of nonidentical amino acids with positive substitution scores finding of significant matches to repeat warning sequences that have been included in both genbank and swiss prot these entries are consensus sequences or trans lations thereof for different subfamilies of human alu repeats however with the large amount of human genomic sequence now present in the database it is common to have many hits to individual repeats with scores greater than those for any con sensus repeat consequently hits to alu warning entries are less striking than when the database was smaller other indications of likely artifacts would be finding hits to many proteins that seem to have no functional relationship to one another or hits to genomic sequences from many different chromosomes these patterns might also be seen if both query and database are contaminated with foreign sequences from the same source for instance cloning vectors although it is always good practice to critically evaluate database search results and be suspicious of artifacts when the data don t make sense a more proactive approach involves masking problematic sequences in the query before doing the search the problem of repetitive elements is ably handled by the popular program repeatmasker which identifies classifies and masks several types of repetitive el ements and simple repeats a f a smit and p green unpublished a masking strategy strategy which we will call hard masking is to replace subsequences with an ambiguity character n for nucleotide sequences or x for proteins alternatively a soft masking approach in which the resides are instead converted to lowercase letters may be used with certain search programs because ambiguous residues are treated as mismatches even when aligned to themselves hard masking effectively prohibits the identified repeats from making a positive contribution to the alignment score although hard masking is excellent for avoiding false hits the fact that even the true alignments may be altered can present problems particularly when alignment scores and lengths are used classify alignments the solution to this di lemma is to use soft masking recent versions of the blast programs have an option that ignores regions of the query sequence that are lowercase when construct ing the word dictionary however an alignment that is initiated in unique sequence may be extended through a repeat and would have the same alignment score as it would with unmasked sequence with repeatmasker the xsmall command line option may be used for soft masking both proteins and nucleic acids contain regions of biased composition which can lead to confusing database search results these low complexity regions lcrs range from the obvious homopolymeric runs and short period repeats to the more subtle cases where one or a few different residues may be overrepresented alignment of lcr containing sequences is problematic because they do not fit the model of residue by residue sequence conservation in some cases the functionally relevant attributes may be only the periodicity or composition and not any specific sequence furthermore methods for assessing the statistical significance of alignments are based on certain notions of randomness which lcrs do not obey consequently many false positives may be observed in the output of a database search with an lcr containing query sequence because the significance of matches can be over estimated altschul et al a program called seg has been developed to partition a protein sequence into segments of low and high compositional complexity wootton and federhen wootton and federhen using this program it has been shown that more than half of the proteins in the database contain at least one lcr wootton wootton and federhen the evolutionary functional and structural properties of lcrs are not well understood perhaps lcrs arise by such mechanisms as polymerase slippage biased nucleotide substitution or unequal crossing over in proteins lcrs are likely to exist structurally as nonglobular regions regions that have been defined physicochemically as nonglobular are usually identified correctly using seg wootton in dna there are many classes of satellite and microsatellite sequences that consist of many copies of a simple repeat unit the protein product of the human homolog of the drosophila achaete scute gene provides a good example of an lcr containing protein when analyzed with seg two regions of low compositional complexity were identified figure shows a gi sp achaete scute homolog messakmesgg agqqpqpqpqqpflppaacffataaaaaaa aaaaaaqsaqqqqqqqqqqqqqqapqlrpa a dgqpsggghksapkqvkrqrssspelmrck rrlnfsgfgyslpqqqp aavarrnerernrv 120 133 134 klvnlgfatlrehvpngaankkmskvetlr saveyiralqqlldehdavsaafqagvlsp tispnysndlnsmagspvssyssdegsydp lspeeqelldftnwf b gi sp achaete scute homolog messakmesggxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxdgqpsggghksapkqvkrqrssspelmrckrrlnfsgfgyslpqqqpx xxxxxxxxxxxxxklvnlgfatlrehvpngaankkmskvetlrsaveyiralqqlldehd avsaafqagvlsptispnysndlnsmagspvssyssdegsydplspeeqelldftnwf gi achaete scute homolog b danio rerio length score bits expect 7e identities positives 155 gaps 155 query kqvkrqrssspelmrckrrlnfsgfgyslpqqqpxxxxxxxxxxxxxxklvnlgfatlre k krqrssspel rckrrl f g gy pqqqp k vn gf tlr kvlkrqrssspellrckrrltfnglgytipqqqpmavarrnerernrvkqvnmgfqtlrq query hvpngaankkmskvetlrsaveyiralqqlldehdavsaafqagvlsptispnysndlns hvpngaankkmskvetlrsaveyiralqqlldehdavsa q gv sp s ys hvpngaankkmskvetlrsaveyiralqqlldehdavsavlqcgvpspsvsnays query mag spvssyssdegsydplspeeqelldftnwf ag sp s yssdegsy ls eeqelldft wf agpesphsayssdegsyehlsseeqelldfttwf figure identifying low complexity regions with seg analysis of the human achaete scute protein swiss prot using seg reveals two regions of low compositional complexity a program output in the default tree format shows the low complexity sequences in lower case letters on the left and high complexity in upper case on the right b using the x command line switch the seg program will generate a version of the sequence in which the low complexity sequences have been masked c for convenience the blast programs can be instructed to perform the masking automatically when a masked query sequence is used in a database search some of the alignments may contain masked segments as shown in this blastp output the default tree output in which the low complexity sequences are shown in low ercase letters on the left and the high complexity sequences in uppercase on the right the first region is a residue segment containing homopolymeric tracts of glutamine and alanine the second is a residue segment with a bias toward ar ginine without filtering many database sequences with biased regions involving these amino acids would be reported using a command line option seg can generate the masked version of the sequence for use as a search query figure alter natively filtering can be performed automatically by the blast programs through the use of optional parameters note that in some implementations of blast such as the web version filtering may be enabled by default position specific scoring matrices in a standard substitution matrix such as the substitution of one amino acid with another is associated with a single score an obvious simplification given that the same amino acid may have different conservation patterns in one context than another in accordance with differing roles in biological function database searches can be tailored to find specific proteins families or domains through the use of substitution scores that reflect the substitution frequencies of each individual amino acid position in a domain there is a large literature on the construction and application of these position specific scoring matrices pssms which may also be called hidden markov models hmms motifs or profiles bucher et al gribskov et al schneider et al staden tatusov et al in its simplest form a pssm consists of a set of substitution scores at each position along the motif one for each of the amino acids amino acids that are commonly found at a particular position receive higher scores whereas lower scores correspond to amino acids unlikely to appear at that position it is also possible to assign scores to insertions and deletions in a position specific manner a commonly used software package hmmer eddy et al contains a set of related programs for constructing and using pssms given a multiple align ment of several related proteins e g one made using clustal w the hmmbuild program may be used to calculate the position specific scores and save it to a file hmm file format using the hmmsearch program the hmm file may be used as a query against a sequence database conversely hmmpfam is used to compare a single query sequence against a database of pssms hmms a comprehensive da tabase of protein domains pfam bateman et al is often used for this purpose the power of pssms in database searches can be further enhanced by iterative approaches in which the highest scoring matches in one search are incorporated into a pssm used in successive searches position specific iterated blast psi blast provides an automated facility for constructing refining and searching pssms within the context of a single program starting with a query sequence provided by the user the process begins with a standard blastp search of a sequence database highly significant alignments found in this search are then used to construct a pssm on the fly comparisons of the pssm against the sequence database are performed using a variation of the word based blast algorithm used for standard sequence comparisons the process continues until no new matches are found or a specified limit on number of iterations is reached to demonstrate the improved sensitivity of the psi blast approach the se quence of histidine triad hit protein was used as a database search query simi sequences producing significant alignments high score e value pass sp fragile histidine triad protein 7e sp bis nucleosyl tetraphosphatase asymme sp hypothetical kd hit like protein 6e sp hypothetical kd hit like protein 07 sp protein orf u pass sp hypothetical kd hit like protein sp hypothetical kd hit like protein in p sp hypothetical kd hit like protein in h 09 sp hypothetical kd hit like protein in p 08 sp hypothetical hit like protein 9e 08 sp kd zinc binding protein protein kinase 07 sp kd zinc binding protein protein kinase sp hypothetical kd protein hit like prot sp protein kinase c inhibitor pkci sp protein kinase c inhibitor pkci 4e 06 sp hypothetical hit like protein 05 sp galactose phosphate uridylyltransferase 04 pass sp galactose phosphate uridylyltransferase sp galactose phosphate uridylyltransferase sp galactose phosphate uridylyltransferase 6e sp galactose phosphate uridylyltransferase 3e sp galactose phosphate uridylyltransferase 6e 09 sp galactose phosphate uridylyltransferase 4e 06 sp galactose phosphate uridylyltransferase pass sp galactose phosphate uridylyltransferase 8e sp galactose phosphate uridylyltransferase 1e 08 figure increased sensitivity using psi blast the human histidine triad hit protein swiss prot was used as the query in a blastp search with the psi blast func tionality enabled definition lines scores and e values are shown for all statistically signif icant matches newly identified in each iteration larity between hit and galactose phosphate uridylyltransferase galt has recently been described based on superimposition of their three dimensional structures holm and sander however sequence similarity between these two proteins is ex tremely weak with a standard single pass blastp search no significant hits to galt sequences are observed however with a multipass search new relationships are discovered at each iteration as shown in figure the rat galt protein is found in the second iteration and after information from this alignment is incorpo rated into the profile several additional homologs from other organisms are also identified spliced alignments the identification of genes within long stretches of dna sequence is a central prob lem for automatic annotation of complete genomes for the very compact genomes of viruses and bacteria this work amounts to little more than the enumeration of open reading frames however gene identification in eukaryotic genomes is signif icantly more challenging because of the larger amount of intergenic sequence and the fact that protein coding regions may be interrupted by introns the two funda mental strategies seek to identify genes using either the intrinsic signals in the dna sequence see chapter or alignments to mrna and protein sequences at first glance the problem of aligning mrna and genomic sequences seems trivial using a local alignment strategy each exon would come out as a separate locally optimal alignment separated by large deletions in the mrna sequence cor responding to the introns that have been spliced out if the goal is merely to obtain a crude sense of how a gene is organized a simple alignment is sufficient however for the purpose of genome annotation it is important that all exons be found with precise endpoints or a correct protein translation cannot be obtained the pro gram is designed to address genome annotation needs by performing mrna genomic alignments rapidly and accurately florea et al it begins with a blast like search for finding the obvious exons those with very high alignment scores and follows this with search at lower stringency to identify any missed usually short exons splice donor and acceptor signals in the genomic sequences are used to adjust the exon boundaries see fig to avoid problems caused by tandemly repeated genes an additional constraint is imposed to require that the order of the exons found in the genome match that implied by the mrna other programs available for per forming mrna genomic alignments are est genome mott and the program from the wise package birney et al it should be noted that an mrna sequence may align perfectly well to a pseu dogene and that such an alignment may be difficult to distinguish from a functional gene certain features may be indicative of retropseudogenes that is those resulting from integration of a reverse transcribed mrna for example a poly a tract found in the genomic sequence at the end of the gene is indicative of a pseudogene produced through an mrna intermediate such an mrna alignment will also lack the large gaps corresponding to introns although this alone cannot be used to con clude that it is a retropseudogene because there many authentic genes that consist of a single exon however once all mrna alignments have been generated for a genome one strategy for retropseudogene identification involves looking for pairs of highly similar alignments in which one lacks and the other contains introns other types of pseudogenes may arise by gene duplication events followed by inactivation of one of the copies such cases can be very difficult to diagnose programs that perform mrna genomic alignments usually do not use any knowledge of reading frame therefore determining that a potential gene contains a frameshift will require subsequent analysis of the protein translations in the case of genes predicted by alignment with ests only the protein coding sequence is not known so any inter ruptions of the reading frame will not be apparent the possibility that an apparent frame shift may actually be a sequencing error in the genomic sequence should also be considered particularly in the analysis of a working draft sequence in many cases determining whether an mrna predicted gene is a functional gene or a pseu dogene must await experimental validation conclusions sequence alignment and database searching are performed tens of thousands of times per day by scientists around the world and represent critical techniques that all mo mrna gi ref bp genomic gi emb 40662 bp complement 118 1247 ccccaggcgtgggaagatggaaccagaacaattcgaacgagcagagcaaa ccccaggcgtgggaagatggaaccagaacaattcgaacgagcagagcaaa acagatcggaattgcagacttcaggtcgtggcagagaaaaccagctgaga acagatcggaattgcagacttcaggtcgtggcagagaaaaccagctgaga cagggcgccacttactag ctctgaaagtctaggatattttg cagggcgccacttactaggtg cagctctgaaagtctaggatattttg ccactggaagaccagcagacaatgtcatgacaactcaagaggatacaaca ccactggaagaccagcagacaatgtcatgacaactcaagaggatacaaca 200 192 gggctgcatcaaaagacaagtctttggaccatgtcaagacctggagcgaa gggctgcatcaaaagacaagtctttggaccatgtcaagacctggagcgaa gaaggtaatgaactcctacttcatagcaggctgtgggccagcagtttgct gaaggtaatgaactcctacttcatagcaggctgtgggccagcagtttgct 292 actacgctgtctcttggttaaggcaag gtttcagtatcaac actacgctgtctcttggttaaggcaaggtc caggtttcagtatcaac ctgacttcttttggaaggatcccttggcctcacgctggagtgggcacctg ctgacttcttttggaaggatcccttggcctcacgctggagtgggcacctg 400 ccctagcccacagagctggatttctccctttcttcaatcacacagggagc ccctagcccacagagctggatttctccctttcttcaatcacacagggagc ouput truncated for brevity figure spliced alignment the program was used to align a novel human mrna refseq nm to the genomic sequence of a cosmid from chromosome embl three exons were identified on the complementary strand the third one has been truncated for brevity the symbols indicate splice sites found at the exon intron boundaries lecular biologists should be familiar with it can be expected that these methods will continue to evolve to meet the challenges of an ever increasing database size this chapter has described some of the fundamental concepts involved but it is useful to consult the documentation of the various programs for more detailed information researchers should have a basic understanding of how the programs work so that parameters can be intelligently selected in addition they should be aware of poten tial artifacts and know how to avoid them above all it is important to apply the same powers of observation and critical evaluation that are used with any experi mental method internet resources for topics presented in chapter blast http ncbi nlm nih gov blast clustal w ftp ftp ebi ac uk pub software dotter ftp ftp sanger ac uk pub dotter fasta lalign ftp ftp virginia edu pub fasta hmmer http hmmer wustl edu repeatmasker http ftp genome washington edu rm repeatmasker html seg ftp ncbi nlm nih gov pub seg http globin cse psu edu wise package http www sanger ac uk software problem set what is the difference between a global and a local alignment strategy calculate the score of the dna sequence alignment shown below using the fol lowing scoring rules for a match for a mismatch for opening a gap and for each position in the gap gactacgatccgtatacgcaca ggttcagac gactacgagccgtatacgcacacaggttcagac if a match from a database search is reported to have a e value of should it be considered highly insignificant or highly significant references altschul s f amino acid substitution matrices from an information theoretic per spective j mol biol altschul s f boguski m s gish w and wootton j c issues in searching molecular sequence databases nature genet 119 altschul s f and erickson b w locally optimal subalignments using nonlinear similarity functions bull math biol altschul s f and erickson b w significance of nucleotide sequence alignments a method for random sequence permutation that preserves dinucleotide and codon usage mol biol evol 526 altschul s f and gish w local alignment statistics methods enzymol 480 altschul s f gish w miller w myers e w and lipman d j basic local alignment search tool j mol biol 410 baron m norman d g and campbell i d protein modules trends biochem sci bateman a birney e durbin r eddy s r howe k l and sonnhammer e l the pfam protein families database nucleic acids res birney e thompson j d and gibson t j pairwise and searchwise finding the optimal alignment in a simultaneous comparison of a protein profile against all dna translation frames nucleic acids res bucher p karplus k moeri n and hofmann k a flexible motif search technique based on generalized profiles comput chem dayhoff m o schwartz r m and orcutt b c a model of evolutionary change in proteins in atlas of protein sequence and structure m o dayhoff ed washington national biomedical research foundation p dembo a karlin s and zeitouni o limit distribution of maximal non aligned two sequence segmental score ann prob doolittle r f hunkapiller m w hood l e devare s g robbins k c aaronson s a and antoniades h n simian sarcoma virus onc gene v sis is derived from the gene or genes encoding a platelet derived growth factor science doolittle r j and bork p evolutionarily mobile modules in proteins sci am eddy s r mitchison g and durbin r maximum discrimination hidden markov models of sequence consensus j comput biol fitch w m locating gaps in amino acid sequences to optimize the homology be tween two proteins biochem genet 108 fitch w m random sequences j mol biol florea l hartzell g zhang z rubin g m and miller w a computer program for aligning a cdna sequence with a genomic dna sequence genome res gibbs a j and mcintyre g a the diagram a method for comparing sequences its use with amino acid and nucleotide sequences eur j biochem gonzalez p hernandez calzadilla c rao p v rodriguez i r zigler j s jr and borras t comparative analysis of the zeta crystallin quinone reductase gene in guinea pig and mouse mol biol evol gribskov m mclachlan a d and eisenberg d profile analysis detection of distantly related proteins proc natl acad sci usa henikoff s and henikoff j g amino acid substitution matrices from protein blocks proc natl acad sci usa henikoff s and henikoff j g automated assembly of protein blocks for database searching nucleic acids res 6565 higgins d g thompson j d and gibson t j using clustal for multiple sequence alignments methods enzymol 383 holm l and sander c enzyme hit trends biochem sci 117 huang x hardison r c and miller w a space efficient algorithm for local similarities comput applic biosci karlin s and altschul s f methods for assessing the statistical significance of molecular sequence features by using general scoring schemes proc natl acad sci usa lipman d j and pearson w r rapid and sensitive protein similarity searches science mott r est genome a program to align spliced dna sequences to unspliced genomic dna comput appl biosci 477 needleman s b and wunsch c a general method applicable to the search for similarities in the amino acid sequence of two proteins j mol biol patthy l modular exchange principles in proteins curr opin struct biol 351 pearson w flexible sequence similarity searching with the program pack age methods mol biol pearson w r effective protein sequence comparison methods enzymol pearson w r and lipman d j improved tools for biological sequence comparison proc natl acad sci usa schneider t d stormo g d gold l and ehrenfeucht a information content of binding sites on nucleotide sequences j mol biol sellers p h pattern recognition in genetic sequences by mismatch density bull math biol 514 smith t f and waterman m s identification of common molecular subsequences j mol biol sonnhammer e l l and durban r a dot matrix program with dynamic threshold control suited for genomic dna and protein sequence analysis gene gc1 staden r methods to define and locate patterns of motifs in sequences comput appl biosci 53 tatusov r l altschul s f and koonin e v detection of conserved segments in proteins iterative scanning of sequence databases with alignment blocks proc natl acad sci usa waterfield m d scrace g t whittle n stroobant p johnsson a wasteson a wes termark b heldin c h huang j s and deuel t f platelet derived growth factor is structurally related to the putative transforming protein of simian sarcoma virus nature waterman m s and eggert m a new algorithm for best subsequence alignments with applications to trna rrna comparisons j mol biol waterman m s and vingron m rapid and accurate estimates of statistical signif icance for sequence data base searches proc natl acad sci usa wilbur w j and lipman d j rapid similarity searches of nucleic acid and protein data banks proc natl acad sci usa wootton j c non globular domains in protein sequences automated segmentation using complexity measures comput chem 269 wootton j c and federhen s analysis of compositionally biased regions in se quence databases methods enzymol wootton j c and federhen s statistics of local complexity in amino acid sequences and sequence databases comput chem 149 bioinformatics a practical guide to the analysis of genes and proteins second edition andreas d baxevanis b f francis ouellette copyright john wiley sons inc isbns hardback paper electronic creation and analysis of protein multiple sequence alignments geoffrey j barton european molecular biology laboratory european bioinformatics institute wellcome trust genome campus hinxton cambridge uk introduction when a protein sequence is newly determined an important goal is to assign possible functions to the protein the first computational step is to search for similarities with sequences that have previously been deposited in the dna and protein sequence databases if similar sequences are found they may match the complete length of the new sequence or only to subregions of the sequence if more than one similar sequence is found then the next important step in the analysis is to multiply align all of the sequences multiple alignments are a key starting point for the prediction of protein secondary structure residue accessibility function and the identification of residues important for specificity multiple alignments also provide the basis for the most sensitive sequence searching algorithms cf gribskov et al barton and sternberg attwood et al effective analysis of a well constructed multiple alignment can provide important clues about which residues in the protein are important for function and which are important for stabilizing the secondary and tertiary structures of the protein in addition it is often also possible to make pre dictions about which residues confer specificity of function to subsets of the sequences in this chapter some guidelines are provided toward the generation and analysis of protein multiple sequence alignments this is not a comprehensive review of techniques rather it is a guide based on the software that have proven to be most useful in building alignments and using them to predict protein structure and func tion a full summary of the software is available at the end of the chapter what is a multiple alignment and why do it a protein sequence is represented by a string a of letters coding for the different types of amino acid residues a protein sequence alignment is created when the residues in one sequence are lined up with those in at least one other sequence optimal alignment of the two sequences will usually require the insertion of gaps in one or both sequences in order to find the best alignment alignment of two residues implies that those residues are performing similar roles in the two different proteins this allows for information known about specific residues in one sequence to be potentially transferred to the residues aligned in the other for example if the active site residues of an enzyme have been characterized alignment of these residues with similar residues in another sequence may suggest that the second sequence possesses similar catalytic activity to the first the validity of such hypotheses depends on the overall similarity of the sequences which in turn dictate the confidence with which an alignment can be generated there are typically many millions of different pos sible alignments for any two sequences the task is to find an alignment that is most likely to represent the chemical and biological similarities between the two proteins a multiple sequence alignment is simply an alignment that contains more than two sequences even if one is interested in the similarities between only two of the sequences in a set it is always worth multiply aligning all available sequences the inclusion of these additional sequences in the multiple alignment will normally im prove the accuracy of the alignment between the sequence pairs as illustrated in figure as well as revealing patterns of conserved residues that would not have been obvious when only two sequences are directly studied although many pro grams exist that can generate a multiple alignment from unaligned sequences ex treme care must be taken when interpreting the results an alignment may show perfect matching of a known active site residue with an identical residue in a well characterized protein family but if the alignment is incorrect any inference about function will also be incorrect structural alignment or evolutionary alignment it is the precise arrangement of the amino acid side chains in the three dimensional structure of the protein that dictates its function comparison of two or more protein three dimensional structures will highlight which residues are in similar positions in space and hence likely to be performing similar functional roles such comparisons can be used to generate a sequence alignment from structure e g see russell and barton the structural alignment of two or more proteins is the gold standard against which sequence alignment algorithms are normally judged this is because it is the structural alignment that most reliably aligns residues that are of functional importance unfortunately structural alignments are only possible when the three figure histogram showing difference in accuracy between the same pairs of se quences aligned as a pair and as part of a larger multiple sequence alignment on average multiple alignments improve the overall alignment accuracy which in this example is judged as the alignment obtained by comparison of the three dimensional structures of the individual proteins rather than just their sequences russell and barton dimensional structures of all the proteins to be aligned are known this is not usually the case therefore the challenge for sequence alignment methods is to get as close as possible to the structural alignment without knowledge of structure although the structural alignment is the most important alignment for the prediction of function it does not necessarily correspond to the evolutionary alignment implied by diver gence from a common ancestor protein unfortunately it is rarely possible to deter mine the evolutionary alignment of two divergent proteins with confidence because this would require knowledge of the precise history of substitutions insertions and deletions that have led to the creation of present day proteins from their common ancestor how to multiply align sequences automatic alignment programs such as clustal w thompson et al will give good quality alignments for sequences that are more than j similar barton and sternberg however building good multiple alignments for sequences that are not trivially similar is a precise task even with the best available alignment tools this section gives an overview of some of the steps to go through to make alignments that are good for structure function predictions this is not a universal recipe in fact there are very few universal recipes in bioinformatics in general each set of sequences presents its own biologically based problems and only experience can guide the creation of high quality alignments some collections of expertly cre ated multiple alignments exist described later and these should always be consulted when studying sequences that are present there the key steps in building a multiple alignment are as follows find the sequences to align by database searching or by other means locate the region of each sequence to include in the alignment do not try to multiply align sequences that are substantially different in length most multiple alignment programs are designed to align sequences that are similar over their entire length therefore a necessary first step is to edit the sequences down to those regions that sequence database searches suggest are similar ideally assess the similarities within the set of sequences by comparing them pairwise with randomizations select a subset of the sequences to align first that cluster above j automatic alignment of such sequences are likely to be accurate barton and sternberg an alternative to doing randomization is to align only sequences that are similar to the query in a database search say with an e value of run the multiple alignment program manually inspect the alignment for problems pay particular attention to regions that appear to be speckled with gaps use an alignment visualization tool e g alscript jalview see below to identify positions in the align ment that show conserved physicochemical properties across the complete alignment if there are no such regions then look at subsets of the sequences remove sequences that appear to disrupt the alignment seriously and then realign the remaining subset after identifying key residues in the set of sequences that are straightforward to align attempt to add the remaining sequences to the alignment so as to preserve the key features of the family assessing quality of alignment multiple alignment programs will align any set of sequences however the fact that the program produces an alignment does not mean that the alignment has any bio logical meaning most programs will take unrelated protein sequences and align them just as easily as two genuinely related sequences even for related sequences there is no guarantee that the resulting alignment is in any way meaningful one way of assessing whether an alignment is meaningful is to perform a randomization or monte carlo test of significance to do this the two sequences are first aligned and the score s for the alignment is recorded the sequences are then shuffled so that they maintain their length and amino acid composition but have a randomized order the shuffled sequences are then compared again and the score is recorded the shuffling and realigning process is repeated a number of times typically and the mean and standard deviation j for the scores are calculated the z score provides an indication of the significance of the alignment if z then it is highly likely that the two sequences are alignable and the alignment correctly relates the key functional and structural residues in the individual proteins to one another bar ton and sternberg unfortunately this can only be a rough guide an align ment that gives a z may be poor and some alignments with low z scores are actually correct this is simply a reflection of the fact that during evolution sequence similarity has diverged faster than structural or functional similarity z scores are preferable to simple percent identities as a measure of similarity because it corrects for both compositional bias in the sequences as well as accounting for the varying lengths of sequences the z score therefore gives an indication of the overall sim ilarity between two sequences although it is a powerful measure it does not help to locate parts of the sequence alignment that are incorrect as a general rule if the alignment is between two or more sequences that do indeed share a similar three dimensional structure then the majority of errors will be concentrated around regions where there are gaps insertions deletions hierarchical methods the most accurate practical methods for automatic multiple alignment are hierar chical methods these work by first finding a guide tree and then following the guide tree to build the alignment the process is summarized in figure first all pairs of sequences in the set to be aligned are compared by a pairwise method of sequence comparison this provides a set of pairwise similarity scores for the sequences that can be fed into a cluster analysis or tree calculating program the tree is calculated to place more similar pairs of sequences closer together on the tree than sequences that are less similar the multiple alignment is then built by starting with the pair of sequences that is most similar and aligning them and then aligning the next most similar pair and so on pairs to be aligned need not be single sequences but can be alignments that have been generated earlier in the tree if an alignment is compared with a sequence or another alignment then gaps that exist in the alignment are preserved there are many different variations of this basic multiple alignment tech nique because errors in alignment that occur early in the process can get locked in and propagated some methods allow for realignment of the sequences after the initial alignment e g barton and sternberg gotoh other refinements include using different similarity scoring matrices at different stages in building up the align ment e g thompson et al gaps insertions deletions do not occur randomly in protein sequences since a stable properly folded protein must be maintained proteins with an insertion or deletion in the middle of a secondary structure a helix or strand are usually selected against during the course of evolution as a consequence present day proteins show a strong bias toward localizing insertions and deletions to loop regions that link the core secondary structures this observation can be used to improve the accuracy of multiple sequence alignments when the secondary structure is known for one or more of the proteins in practice by making the penalty for inserting a gap higher when in secondary structure regions than when in loops bar ton and sternberg jones a further refinement is to bias where gaps are most likely to be inserted in the alignment by examining the growing alignment for regions that are most likely to accommodate gaps pascarella and argos figure illustration of the stages in hierarchical multiple alignment of seven sequences the codes for these sequences are hahu hbhu haho hbho mywhp and lghb the table at the top left shows the pairwise z scores for comparison of each sequence pair higher numbers mean greater similarity see text hierarchical cluster analysis of the z score table generates the dendrogram or tree shown at the bottom left items joined toward the right of the tree are more similar than those linked toward the left based on the tree lghb is least similar to the other sequences in the set whereas hbhu and hbho are the most similar pair most similar to each other the first four steps in building the multiple alignment are shown on the right the first two steps are pairwise alignments the third step is a comparison of profiles from the two alignments generated in steps and the fourth step adds a single sequence mywhp to the alignment generated at step further sequences are added in a similar manner clustal w and other hierarchical alignment software clustal w combines a good hierarchical method for multiple sequence alignment with an easy to use interface the software is free although a contribution to de velopment costs is required when purchasing the program clustal w runs on most computer platforms and incorporates many of the techniques described in the previous section the program uses a series of different pair score matrices biases the location of gaps and allows you to realign a set of aligned sequences to refine the alignment clustal w can read a secondary structure mask and bias the positioning of gaps according to it the program can also read two preexisting align ments and align them to each other or align a set of sequences to an existing align ment clustal w also includes options to calculate neighbor joining trees for use in inferring phylogeny although clustal w does not provide general tools for viewing these trees the output is compatible with the phylip package felsenstein and the resultant trees can be viewed with that program clustal w can read a variety of different common sequence formats and produce a range of different output formats the manual for clustal w is clearly written and explains possible limitations of the alignment process although clustal w can be installed and run locally users can also access it through a faster web service via the ebi server by clicking the tools page with the exception of manual editing and visualization clustal w contains most of the tools that are needed to build and refine a multiple sequence alignment when combined with jalview as described below the process of building and refining a multiple alignment is greatly simplified although clus tal w is probably the most widely used multiple alignment program and for most purposes is adequate other software exists having functionality not found in clus tal w for example amps barton provides a pairwise sequence compar ison option with randomization allowing z scores to be calculated the program can also generate alignments without the need to calculate trees first for large numbers of sequences this can save a lot of time because it eliminates the need to perform all pairwise comparisons of the sequences amps also has software to visualize trees thus helping in the selection of sequences for alignment however the program has no simple menu interface therefore it is more difficult for the novice or occasional user to use more rigorous nonhierarchical methods hierarchical methods do not guarantee finding the one mathematically optimal mul tiple alignment for an entire set of sequences however in practice the mathematical optimum rarely makes any more biological sense than the alignment that is found by hierarchical methods this is probably because a great deal of effort has gone into tuning the parameters used by clustal w and other hierarchical methods to produce alignments that are consistent with those that a human expert or three dimensional structure comparison might produce the widespread use of these tech niques has also ensured that the parameters are appropriate for a wide range of alignment problems more rigorous alignment methods that attempt to find the math ematically optimal alignment over a set of sequences cf lipman et al may be capable of giving better alignments but as shown in recent benchmark studies they are on average no better than the hierarchical methods multiple alignment by psi blast multiple sequence alignments have long been used for more sensitive searches of protein sequence databases than is possible with a single sequence the program psi blast altschul et al has recently made these profile methods more easily available as part of its search psi blast generates a multiple alignment however this alignment is not like the alignments made by clustal w amps or other traditional multiple alignment tools in a conventional multiple alignment all sequences in the set have equal weight as a consequence a multiple alignment will normally be longer than any one of the individual sequences since gaps will be inserted to optimize the alignment in contrast a psi blast multiple alignment is always exactly the length of the query sequence used in the search if alignment of the query or query profile to a database sequence requires an insertion in the query then the inserted region from the database sequence is simply discarded the re sulting alignment thus highlights the amino acids that may be aligned to each position in the query perhaps for this reason psi blast multiple alignments and their associated frequency tables and profiles have proved very effective as input for pro grams that predict protein secondary structure jones cuff and barton multiple protein alignment from dna sequences although most dna sequences will have translations represented in the embl trembl or ncbi genpept databases this is not true of single pass est sequences because est data are accumulating at an exponential pace an automatic method of extracting useful protein information from ests has been developed in brief the protest server cuff et al searches est collections and protein sequence databases with a protein query sequence est hits are assembled into species specific contigs and an error tolerant alignment method is used to correct probable sequenc ing errors finally any protein sequences found in the search are multiply aligned with the translations of the est assemblies to produce a multiple protein sequence alignment the jpred server version will generate a multiple protein sequence alignment when presented with a single protein sequence by searching the swall protein sequence database and building a multiple alignment the jpred alignments are a good starting point for further analysis with more sensitive methods tools to assist the analysis of multiple alignments a multiple sequence alignment can potentially consist of several hundred sequences that are or more amino acids long with such a volume of data it can be difficult to find key features and present the alignments in a form that can be analyzed by eye in the past the only option was to print out the alignment on many sheets of paper stick these together and then pore over the massive poster with colored high lighter pens this sort of approach can still be useful but it is rather inconvenient visualization of the alignment is an important scientific tool either for analysis or for publication appropriate use of color can highlight positions that are either iden tical in all the aligned sequences or share common physicochemical properties alscript barton is a program to assist in this process alscript takes a multiple sequence alignment and a file of commands and produces a file in figure example output from the program alscript barton details can be found within the main text postscript format suitable for printing out or viewing with a utility such as ghostview figure illustrates a fragment of alscript output the full figure can be seen in color in roach et al in this example identities across all sequences are shown in white on red and boxed whereas positions with similar physicochemical properties are shown black on yellow and boxed residue numbering according to the bottom sequence is shown underneath the alignment green arrows illustrate the location of known strands whereas a helices are shown as black cylinders further symbols highlight specific positions in the alignment for easy cross referencing to the text alscript is extremely flexible and has commands that permit control of font size and type background coloring and boxing down to the individual residue the program will automatically split a large alignment over multiple pages thus permitting alignments of any size to be visualized however this flexibility comes at a price there is no point and click interface and the program requires the user to be familiar with editing files and running programs from the command line the alscript distribution includes a comprehensive manual and example files that make the process of making a useful figure for your own data a little easier subalignments amas alscript provides a few commands for calculating residue conservation across a family of sequences and coloring the alignment accordingly however it is really intended as a display tool for multiple alignments rather than an analysis tool in contrast amas analysis of multiply aligned sequences livingstone and barton is a program for studying the relationships between sequences in a multiple alignment to identify possible functional residues amas automatically runs al script to provide one output that is a boxed colored and annotated multiple alignment why might you want to run amas a common question one faces is which residues in a protein are important for its specificity amas can help identify these residues by highlighting similarities and differences between subgroups of sequences in a multiple alignment for example given a family of sequences that shows some variation positions in a multiple alignment that are conserved across the entire family of sequences are likely to be important to stabilize the common fold of the protein or common functions positions that are conserved within a subset of the sequences but different in the rest of the family are likely to be those important to the specific function or specificity of that subset and these positions can be easily identified using amas there are a number of subtle types of differences that amas will search for and these are summarized in figure to use amas one must first have an idea of what subgroups of sequences exist in a multiple alignment of interest one way to do this is to take a tree generated from the multiple alignment and identify clusters of sequences at some similarity threshold this is also illustrated in figure in which three groups have been selected on the basis of the tree shown at the top left alternatively if one knows in advance that finding common features and differences between for example sequences and in a multiple alignment is important one can specify these ranges explicitly the output of amas is a detailed text summary of the analysis as well as a colored and shaded multiple sequence alignment by default amas searches for general features of amino acid physicochemical properties however this can be narrowed down just to a single feature of amino acids such as charge an example of a charge analysis is shown in figure for repeats within the annexin supergene family of proteins barton et al the analysis highlights a charge swap within two subgroups of the sequences correctly predicting the presence of a salt bridge in the native folded protein huber et al the amas program may either be downloaded and run locally or a subset of its options can be accessed over the web at a server hosted by ebi secondary structure prediction and the prediction of buried residues from multiple sequence alignment when aligning sequences it is important to remember that the protein is a three dimensional molecule and not just a string of letters predicting secondary structure either for the whole collection of sequences or subsets of the sequences can be used to help discover how the protein might fold locally and guide the alignment of more distantly related sequences for example it is common for proteins with similar topologies to have quite different sequences and be unalignable by an automatic alignment method e g see russell and barton cf the scop database see murzin et al chapter in these circumstances the secondary structure may suggest which blocks of sequences should be equivalent the prediction of secondary structure a helix and strand is enhanced by around when performed from a multiple alignment compared with prediction from a single sequence cuff and figure stylized output from the program amas the sequence alignment has been shaded to illustrate similarities within each subgroup of sequences conservation numbers livingstone and barton zvelebil et al run from to and provide a nu merical measure of the similarity in physicochemical properties of each column in the align ment below the alignment the lines similar pairs show the conservation values obtained when each pair of subgroups is combined and the combined conservation number is not less than a threshold for example at position subgroups a and b combine with a con servation number of the lines different pairs illustrate positions at which a combina tion of subgroups lowers the conservation number below the threshold for example at position there is an identity in subgroup b and one in c but when the groups are combined the identity is lost and the conservation drops below the threshold of to a summary of the similarities and differences is given as a frequency histogram each upward bar represents the proportion of subgroup pairs that preserve conservation whereas each downward bar shows the percentage of differences for example at position pairs are conserved whereas at positions and pairs show differences with a large alignment the histogram can quickly draw the eye to regions that are highly con served or to regions where there are differences in conserved physicochemical properties figure illustration of an amas output used to find a charge pair in the annexins there are four groups of sequences in the alignment the highlighted positions highlight locations where the charge is conserved in each group of sequences yet different between groups a change from glutamine to arginine is shown at position barton the best current methods psipred jones and jnet cuff and barton give over accuracy for the prediction of three states a helix strand and random coil in rigorous testing this high accuracy is possible because the prediction algorithms are able to locate regions in the sequences that show patterns of conserved physicochemical properties across the aligned family these patterns are characteristic of particular secondary structure types and can often be seen by eye in a multiple sequence alignment as summarized below short runs of conserved hydrophobic residues suggest a buried strand i i and i patterns of conserved hydrophobic amino acids suggest a surface strand since the alternate residues in a strand point in the same direction if the alternate residues all conserve similar physicochemical prop erties then they are likely to form one face of a strand i i i and i and variations of that pattern e g i i i of conserved residues suggest an a helix with one surface facing the solvent insertions and deletions are normally only tolerated in regions not associated with the buried core of the protein thus in a good multiple alignment the location of indels suggests surface loops rather than a helices or strands although glycine and proline may be found in all secondary structure types a glycine or proline residue that is conserved across a family of sequences is a strong indicator of a loop secondary structure prediction programs such as jnet cuff and barton and phd rost and sander also exploit multiply aligned sequences to predict the likely exposure of each residue to solvent knowledge of solvent accessibility can help in the identification of residues key to stabilizing the fold of the protein as well as those that may be involved in binding both the jnet and phd programs may be run from the jpred prediction server whereas jnet may also be run from within jalview for further discussion of methods used to predict secondary structure the reader is referred to chapter jalview amas and alscript are not interactive they run a script or set of commands and produce a postscript file which can be viewed on screen using a postscript viewer or just printed out although this provides the maximum number of options and flexibility in its display it is comparatively slow and sometimes difficult to learn in addition the programs require a separate program to be run to generate the multiple alignment for analysis if the alignment requires modification or subsets of the align ment are needed a difficult cycle of editing and realigning is often required the program jalview overcomes most of these problems jalview encapsulates many of the most useful features of amas and alscript in an interactive mouse driven program that will run on most computers with a java interpreter the core of jalview is an interactive alignment editor this allows an existing alignment to be read into the program and individual residues or blocks of residues to be moved around a few mouse clicks permit the sequences to be subset into a separate copy of jalview jalview can call clustal w thompson et al either as a local copy on the same computer that is running jalview or the clustal w server at ebi thus one can also read in a set of unaligned sequences align them with clustal w edit the alignment and take subsets with great ease further functions of jalview will calculate a simple neighbor joining tree from a multiple alignment and allow an amas style analysis to be performed on the subgroups of sequences if the tertiary structure of one of the proteins in the set is available then the three dimensional structure may be viewed alongside the alignment in jalview in addi tion the jnet secondary structure prediction algorithm cuff and barton may be run on any subset of sequences in the alignment and the resulting prediction displayed along with the alignment the jalview application is available for free download and because it is written in java can also be run as an applet in a web browser such as netscape or internet explorer many alignment services such as the clustal w server at ebi and the pfam server include jalview as an option to view the resulting multiple alignments figure illustrates a typical jalview session with the alignment editing and tree windows open collections of multiple alignments this chapter has focused on methods and servers for building multiple protein se quence alignments although proteins that are clearly similar by the z score measure figure an example jalview alignment editing and analysis session the top panel contains a multiple alignment and the bottom left is the similarity tree resulting from that alignment a vertical line on the tree has separated the sequences into subgroups which have been colored to highlight conservation within each subgroup the panel at the bot tom right illustrates an alternative clustering method should be straightforward to align by the automatic methods discussed here getting good alignments for proteins with more remote similarities can be a very time consuming process a number of groups have built collections of alignments using a combination of automation and expert curation e g smart schultz et al pfam bateman et al and prints attwood et al and these together with the tools available at their web sites can provide an excellent starting point for further analyses internet resources for topics presented in chapter clustal w ftp ftp ebi ac uk pub software amas http barton ebi ac uk servers amas html jpred http barton ebi ac uk servers jpred html protest http barton ebi ac uk servers protest html jalview http barton ebi ac uk new software html amps http barton ebi ac uk new software html european bioinformatics institute http www ebi ac uk problem set the following problems are based on the annexin supergene family the same family used throughout the discussion in this chapter this family contains a amino acid residue unit that repeats either four eight or times within each protein the analysis required below will focus on the individual repeat units rather than the organization of the repeat units within the full length protein sequences the problems will require the use of clustal w and jalview which you may have to install or have installed on a unix or linux based system to which you have access the files referred to below are available on the book web site the file ann fa contains the sequence of a single annexin domain this sequence has been used as the query against the swall protein sequence database using the program scanps to make the pairwise sequence comparisons a partial listing of the results can be found in the file named ann frags fa generation of a multiple sequence alignment copy the file ann frags fa to a new directory run clustal w on ann frags fa accept all defaults and create an output file called ann frags aln pass this output file to jalview by typing jalview ann frags aln clustal select the fragment sequences by clicking on the id code select delete selected sequences from the edit menu save the modified alignment to a clustal formatted file called ann frags aln select average distance tree from the calculate menu a new window will now appear and after a few moments a tree dendrogram will be rendered within that window there should be outliers at the very top of that tree and these outliers will need to be eliminated click on the tree to the left of where the outliers join the tree a vertical line should now appear and the outliers will be highlighted in a different color return to the alignment window and delete the outliers from the alignment in the same way as was done in step save the resulting alignment to a file named ann frags aln this series of steps produces a clean alignment for inspection positions within the alignment can be colored in different ways to highlight certain features of the amino acids within the alignment for example selecting conservation from the calculate menu will shade each column on the basis of the relative amino acid conservation seen at that particular position in the alignment by doing so it im mediately becomes apparent which parts of the protein may lie within regions of secondary structure examine the area around positions to of the alignment the pattern observed should be two conserved two unconserved and two conserved residues a parttern that is characteristic of an alpha helix select jnet from the align menu this will return a secondary structure prediction based on the alignment alternatively the alignment file can be submitted to the server at ebi in order to submit the alignment to the server the alignment must first be saved in msf format ann frags msf either of these methods should corroborate that there is an alpha helical region in the area around residues by cleaning the alignment in this way information about sequences and sequences themselves has been discarded it is advisable to always save files at intermediate steps the clean alignment will be relatively easy to interpret but the results of the intermediate steps will have information about the parts of the align ment requiring more thought subfamily analysis the following steps will allow a subfamily analysis to be performed on the annexin family the input file is ideal annexins als start jalview and read in the alignment file by typing ideal annexins blc blc select average distance tree from the calculate menu the resultant tree will have four clear clusters with one outlier click on the tree at an appropriate position to draw a vertical line and highlight the four clusters return to the alignment window select conservation from the calculate menu the most highly conserved positions within each subgroup of sequences will be colored the brightest examine the alignment and identify the charge pair shown as an example in this chapter selecting either the taylor or zappo color schemes may help in identifying the desired region submit the file ideal annexins blc to the amas web server on the web page paste the contents of ideal annexins blc into the alignment win dow then paste the contents of the file ideal annexins grp into the sen sible groups window the server should return results quickly providing links to a number of output files the pretty output file contains the postscript alignment which should be identical to ideal annexins amas ps provided here references altschul s f madden t l schaffer a a zhang j zhang z miller w and lipman d j gapped blast and psi blast a new generation of protein database search programs nucl acids res attwood t k croning m d r flower d r lewis a p mabey j e scordis p selley j and wright w prints the database formerly known as prints nucl acids res 227 barton g j protein multiple sequence alignment and flexible pattern matching methods enz barton g j alscript a tool to format multiple sequence alignments prot eng references barton g j newman r h freemont p f and crumpton m j amino acid sequence analysis of the annexin super gene family of proteins european j biochem barton g j and sternberg m j e evaluation and improvements in the automatic alignment of protein sequences prot eng barton g j and sternberg m j e a strategy for the rapid multiple alignment of protein sequences confidence levels from tertiary structure comparisons j mol biol 337 barton g j and sternberg m j e flexible protein sequence patternsa sensitive method to detect weak structural similarities j mol biol bateman a birney e durbin r eddy s r finn r d and sonnhammer e l l pfam multiple alignments match the majority of proteins nucl acids res 260 cuff j a and barton g j evaluation and improvement of multiple sequence methods for protein secondary structure prediction proteins cuff j a and barton g j application of multiple sequence alignment profiles to improve protein secondary structure prediction proteins cuff j a birney e clamp m e and barton g j protest protein multiple sequence alignments from expressed sequence tags bioinformatics 111 felsenstein j phylip phylogeny inference package version cladistics 164 murzin a g brenner s e hubbard t and chothia c scop a structural clas sification of proteins database for the investigation of sequences and structures j mol biol gotoh o significant improvement in accuracy of multiple protein sequence align ments by iterative refinement as assessed by reference to structural alignments j mol biol 823 gribskov m mclachlan a d and eisenberg d profile analysis detection of distantly related proteins proc nat acad sci usa huber r romsich j and paques e p the crystal and molecular structure of human annexin v an anticoagulant protein that binds to calcium and membranes embo j 3867 jones d t protein secondary structure prediction based on position specific scoring matrices j mol biol lesk a m levitt m and chothia c alignment of the amino acid sequences of distantly related proteins using variable gap penalties prot eng 77 lipman d j altschul s f and kececioglu j d a tool for multiple sequence alignment proc nat acad sci usa livingstone c d and barton g j protein sequence alignments a strategy for the hierarchical analysis of residue conservation comp app biosci 745 pascarella s and argos p analysis of insertions deletions in protein structures j mol biol roach p l clifton i j fulop v harlos k barton g j hajdu j andersson i schofield c j and baldwin j e crystal structure of isopenicillin n synthase is the first from a new structural family of enzymes nature rost b and sander c prediction of protein secondary structure at better than accuracy j mol biol russell r b and barton g j multiple protein sequence alignment from tertiary structure comparison assignment of global and residue confidence levels proteins 309 russell r b and barton g j structural features can be unconserved in proteins with similar folds j mol biol 350 schultz j milpetz f bork p and ponting c p smart a simple modular archi tecture research tool identification of signalling domains proc nat acad sci usa thompson j d higgins d g and gibson t j clustal w improving the sensi tivity of progressive multiple sequence alignment through sequence weighting position specific gap penalties and weight matrix choice nucl acids res zvelebil m j j m barton g j taylor w r and sternberg m j e prediction of protein secondary structure and active sites using the alignment of homologous se quences j mol biol bioinformatics a practical guide to the analysis of genes and proteins second edition andreas d baxevanis b f francis ouellette copyright john wiley sons inc isbns hardback paper 471 electronic predictive methods using dna sequences andreas d baxevanis genome technology branch national human genome research institute national institutes of health bethesda maryland with the announcement of the completion of a working draft of the sequence of the human genome in june and the human genome project targeting the com pletion of sequencing in investigators will be faced with the challenge of developing a strategy by which they can deal with the oncoming flood of both unfinished and finished data whether the data are generated in their own laboratories or at one of the major sequencing centers these data undergo what can best be described as a maturation process starting as single reads off of a sequencing ma chine passing through a phase where the data become part of an assembled yet incomplete sequence contig and finally ending up as part of a finished completely assembled sequence with an error rate of less than one in bases even before such sequencing data reach this highly polished state investigators can begin to ask whether or not given stretches of sequence represent coding or noncoding regions the ability to make such determinations is of great relevance in the context of systematic sequencing efforts since all of the data being generated by these projects are in essence anonymous in nature nothing is known about the coding poten tial of these stretches of dna as they are being sequenced as such automated methods will become increasingly important in annotating the human and other ge nomes to increase the intrinsic value of these data as they are being deposited into the public databases in considering the problem of gene identification it is important to briefly go over the basic biology underlying what will become in essence a mathematical problem fig at the dna level upstream of a given gene there are promoters and other regulatory elements that control the transcription of that gene the gene itself is discontinuous comprising both introns and exons once this stretch of dna is transcribed into an rna molecule both ends of the rna are modified capping the end and placing a polya signal at the end the rna molecule reaches maturity when the introns are spliced out based on short consensus sequences found both at the intron exon boundaries and within the introns themselves once splicing has occurred and the start and stop codons have been established the mature mrna is transported through a nuclear pore into the cytoplasm at which point translation can take place although the process of moving from dna to protein is obviously more complex in eukaryotes than it is in prokaryotes the mere fact that it can be described in its entirety in eukaryotes would lead one to believe that predictions can confidently be made as to the exact positions of introns and exons unfortunately the signals that control the process of moving from the dna level to the protein level are not very well defined precluding their use as foolproof indicators of gene structure for ex ample upward of of the promoter regions contain a tata box but because the remainder do not the presence or absence of the tata box in and of itself cannot be used to assess whether a region is a promoter similarly during end mod ification the polya tail may be present or absent or may not contain the canonical dna transcription rna end modification gu ag gu ag g u ag mature mrna cap translation nucleus cytoplasm polya figure the central dogma proceeding from the dna through the rna to the pro tein level various sequence features and modifications can be identified that can be used in the computational deduction of gene structure these include the presence of promoter and regulatory regions intron exon boundaries and both start and stop signals unfortu nately these signals are not always present and when they are present they may not always be in the same form or context the reader is referred to the text for greater detail grail aataaa adding to these complications is the fact that an open reading frame is required but is not sufficient for judging a region as being an exon given these and other considerations there is at present no straightforward method that will allow for confidence in the prediction of an intron or an exon despite this a com binatorial approach can be used relying on a number of methods to increase the confidence with which gene structure is predicted briefly gene finding strategies can be grouped into three major categories con tent based methods rely on the overall bulk properties of a sequence in making a determination characteristics considered here include how often particular codons are used the periodicity of repeats and the compositional complexity of the se quence because different organisms use synonymous codons with different fre quency such clues can provide insight into determining regions that are more likely to be exons in site based methods the focus turns to the presence or absence of a specific sequence pattern or consensus these methods are used to detect features such as donor and acceptor splice sites binding sites for transcription factors polya tracts and start and stop codons finally comparative methods make determinations based on sequence homology here translated sequences are subjected to database searches against protein sequences cf chapter to determine whether a previously characterized coding region corresponds to a region in the query sequence although this is conceptually the most straightforward of the methods it is restrictive because most newly discovered genes do not have gene products that match anything in the protein databases also the modular nature of proteins and the fact that there are only a limited number of protein motifs chothia and lesk make predicting anything more than just exonic regions in this way difficult the reader is referred to a number of excellent reviews detailing the theoretical underpinnings of these various classes of methods claverie claverie guigo snyder and stormo claverie rogic et al although many of the gene prediction methods belong strictly to one of these three classes of methods most of the methods that will be discussed here use the strength of combining different classes of methods to optimize predictions with the complexity of the problem at hand and the various approaches described above for tackling the problem it becomes important for investigators to gain an appreciation for when and how each particular method should be applied a recurring theme in this chapter will be the fact that depending on the nature of the data each method will perform differently put another way although one method may be best for human finished sequences another may be better for unfinished sequences or for sequences from another organism in this chapter we will examine a number of the commonly used methods that are freely available in the public domain focusing on their application to human sequence data this will be followed by a general discus sion of gene finding strategy grail grail which stands for gene recognition and analysis internet link uberbacher and mural mural et al is the elder statesman of the gene prediction techniques because it is among the first of the techniques developed in this area and enjoys widespread usage as more and more has become known about gene structure in general and better internet tools have become more widespread grail has con tinuously evolved to keep in step with the current state of the field there are two basic grail versions that will be discussed in the context of this discussion grail makes use of a neural network method to recognize coding potential in fixed length base windows considering the sequence itself without looking for additional features such as splice junctions or start and stop codons an improved version of grail called grail expands on this method by con sidering regions immediately adjacent to regions deemed to have coding potential resulting in better performance in both finding true exons and eliminating false pos itives either grail or grail would be appropriate in the context of searching for single exons a further refinement led to a second version called grail in which variable length windows are used and contextual information e g splice junc tions start and stop codons polya signals is considered because grail makes its prediction by taking genomic context into account it is appropriate for determin ing model gene structures in this chapter the output of each of the methods discussed will be shown using the same set of input data as the query the sequence that will be considered is that of a human bac clone from a clone established as part of the systematic sequencing of chromosome genbank by using the same example throughout the strengths and weaknesses of each of the discussed methods can be highlighted for purposes of this example a client server application called xgrail will be used this software which runs on the unix platform allows for graphical output of grail 1a results as shown in figure because the dna sequence in question is rather large and is apt to contain at least one gene grail was selected as the method the large upper window presents an overview of the kb making up this clone and the user can selectively turn on or off particular markings that identify features within the sequence described in the figure legend of most importance in this view is the prediction of exons at the very top of the window with the histogram representing the probability that a given region represents an exon information on each one of the predicted exons is shown in the model exons window and the model exons can be assembled and shown as both model genes and as a protein translation only putative exons with acceptable probability values as defined in the grail algorithm are included in the gene models the protein translation can in turn be searched against the public databases to find sequence homologs using a program called genquest integrated into xgrail and these are shown in the db hits window in this case the exons in the first gene model from the forward strand are translated into a protein that shows significant sequence homology to a group of proteins putatively involved in anion transport everett et al most recently the authors of grail have released grail exp which is based on grail but uses additional information in making the predictions including a database search of known complete and partial gene messages the inclusion of this database search in deducing gene models has greatly improved the performance of the original grail algorithm fgeneh fgenes fgeneh developed by victor solovyev and colleagues is a method that predicts internal exons by looking for structural features such as donor and acceptor splice figure 2 xgrail output using the human bac clone from as the query the upper window shows the results of the prediction with the histogram representing the probability that a given stretch of dna is an exon the various bars in the center represent features of the dna e g arrows represent repetitive dna and vertical bars represent repeat sequences exon and gene models protein translations and the results of a genquest search using the protein translation are shown see color plate sites putative coding regions and intronic regions both and to the putative exon solovyev et al solovyev et al solovyev et al the method makes use of linear discriminant analysis a mathematical technique that allows data from multiple experiments to be combined once the data are combined a linear function is used to discriminate between two classes of events here whether a given stretch of dna is or is not an exon in fgeneh results of the linear discriminant approach are then passed to a dynamic programming algorithm that determines how to best combine these predicted exons into a coherent gene model an extension of fgeneh called fgenes can be used in cases when multiple genes are expected in a given stretch of dna the sanger centre web server provides a very simple front end for performing fgenes the query sequence again the bac clone from is pasted into the query box an identifier is entered and the search can then be performed the results are returned in a tabular format as shown in figure the total number of predicted genes and exons 2 and respectively is shown at the top of the output the information for each gene g then follows for each predicted exon the strand str is given with indicating the forward strand and indicating the reverse the feature list in this particular case includes initial exons cdsf internal exons cdsi terminal exons cdsl and polya regions pola the nucleotide region for the predicted feature is then given as a range in the current example the features of the second predicted gene are shown in reverse order since the prediction is based on the reverse strand on the basis of the information in the table predicted proteins are given at the bottom of the output in fasta format the definition line for each of the predicted proteins gives the range of nucleotide residues involved as well as the total length of the protein and the direction of the predicted gene mzef mzef stands for michael zhang exon finder after its author at the cold spring harbor laboratory the predictions rely on a technique called quadratic discriminant analysis zhang imagine a case in which the results of two types of predic tions are plotted against each other on a simple xy graph for instance splice site scores vs exon length if the relationship between these two sets of data is nonlinear or multivariate the resulting graph will look like a swarm of points points lying in only a small part of this swarm will represent a correct prediction to separate the correctly predicted points from the incorrectly predicted points in the swarm a quad ratic function is used hence the name of the technique in the case of mzef the measured variables include exon length intron exon and exon intron transitions branch sites and splice sites and exon strand and frame scores mzef is intended to predict internal coding exons and does not give any other information with respect to gene structure there are two implementations of mzef currently available the program can be downloaded from the cshl ftp site for unix command line use or the program can be accessed through a web front end the input is a single sequence read in only one direction either the forward or the reverse strand to perform mzef on both strands the program must be run twice returning to the bac clone from chromosome mzef predicts a total of exons in the forward strand fig focusing in on the first two columns of the table the region of the prediction is figure fgenes output using the human bac clone from as the query the columns going from left to right represent the gene number g strand str feature described in the main text start and end points for the predicted exon a scoring weight and start and end points for corresponding open reading frames orf start and orf end each predicted gene is shown as a separate block the tables are followed by protein translations of any predicted gene products given as a range followed by the probability that the prediction is correct p pre dictions with p are considered correct and are included in the table immedi ately one begins to see the difference in the predictions between methods mzef is again geared toward finding single exons therefore the exons are not shown in the context of a putative gene as they are in grail 2 or fgenes however the exons predicted by these methods are not the same a point that we will return to later in this discussion figure mzef output using the human bac clone from as the query the columns going from left to right give the location of the prediction as a range of included bases coordinates the probability value p frame preference scores an orf indicator showing which reading frames are open and scores for the splice site coding regions and splice site genscan genscan developed by chris burge and sam karlin burge and karlin burge and karlin is designed to predict complete gene structures as such genscan can identify introns exons promoter sites and polya signals as do a number of the other gene identification algorithms like fgenes genscan does not expect the input sequence to represent one and only one gene or one and only one exon it can accurately make predictions for sequences representing either partial genes or multiple genes separated by intergenic dna the ability to make these predictions accurately when a sequence is in a variety of contexts makes genscan a particularly useful method for gene identification genscan relies on what the author terms a probabilistic model of genomic sequence composition and gene structure by looking for gene structure descriptions that match or are consistent with the query sequence the algorithm can assign a probability as to the chance that a given stretch of sequence represents an exon promoter and so forth the optimal exons are the ones with the highest probability and represent the part of the query sequence having the best chance of actually being an exon the method will also predict suboptimal exons stretches of sequence having an acceptable probability value but one not as good as the optimal one the authors of the method encourage users to examine both sets of predictions so that alternatively spliced regions of genes or other nonstandard gene structures are not missed with the use of the human bac clone from again the query can be issued directly from the genscan web site using vertebrate as the organism the default suboptimal cutoff and predicted peptides only as the print option the results for this query are shown in figure the output indicates that there are three genes in this region with the first gene having exons the second gene having 13 exons and the third gene having exons the most important columns in the table are those labeled type and p the type column indicates whether the prediction is for an initial exon init an internal exon intr a terminal exon term a single exon gene sngl a promoter region prom or a polya signal plya the p column gives the probability that this prediction is actually correct genscan exons having a very high probability value p are accurate where the prediction matches a true annotated exon these high probability predictions can be used in the rational design of pcr primers for cdna amplification or for other purposes where extremely high confidence is necessary genscan exons that have probabilities in the range from to are deemed to be correct most of the time the best case accuracies for p values over is on the order of any predictions below should be discarded as unreliable and those data are not given in the table an alternative view of the data is shown in figure here both the optimal and suboptimal exons are shown with the initial and terminal exons showing the direction in which the prediction is being made or this view is particularly useful for large stretches of dna as the tables become harder to interpret when more and more exons are predicted by the time of this printing a new program named genomescan will be avail able from the burge laboratory at mit genomescan assigns a higher score to pu tative exons that overlap blastx hits than to comparable exons for which similarity evidence is lacking regions of higher similarity according to blastx e value for example are accorded more confidence than regions of lower similarity since weak similarities sometimes do not represent homology thus the predictions of genomescan tend to be consistent with all or almost all of the regions of high detected similarity but may sometimes ignore a region of weak similarity that either has weak intrinsic properties e g poor splice signals or is inconsistent with other extrinsic information the accuracy of genomescan tends to be significantly higher than that of genscan when a moderate or closely related protein sequence is available an example of the improved accuracy of genomescan over genscan using the human gene as the query is shown in figure procrustes greek mythology heralds the story of theseus the king of athens who underwent many trials and tribulations on his way to becoming a hero along with hercules as if amazons and the minotaur were not enough in the course of his travels theseus happened upon procrustes a bandit with a warped idea of hospitality procrustes which means he who stretches would invite passersby into his home for a meal and a night stay in his guest bed the problem lay quite literally in the bed in that procrustes would make sure that his guests fit in the bed by stretching them out on a rack if they were too short or by chopping off their legs if they were too long figure genscan output using the human bac clone from as the query the columns going from left to right represent the gene and exon number gn ex the type of prediction type the strand on which the prediction was made s with as the forward strand and as the reverse the beginning and endpoints for the prediction begin and end the length of the prediction len the reading frame of the prediction fr several scoring columns and the probability value p each predicted gene is shown as a separate block notice that the third gene has its exons listed in reverse order reflecting that the prediction is on the reverse strand the tables are followed by the protein trans lations for each of the three predicted genes genscan predicted genes in sequence human kb kb 33 57 kb 84 kb key initial exon internal exon terminal exon single exon gene optimal exon suboptimal exon figure genscan output in graphical form using the human bac clone from as the query optimal and suboptimal exons are indicated and the initial and terminal exons show the direction in which the prediction is being made or theseus made short order of procrustes by fitting him to his own bed thereby sparing any other traveler the same fate on the basis of this story the phrase bed of procrustes has come to convey the idea of forcing something to fit where it normally would not living up to its namesake procrustes takes genomic dna sequences and forces them to fit into a pattern as defined by a related target protein gelfand et al unlike the other gene prediction methods that have been discussed the algorithm does not use a dna sequence on its own to look for content or site based signals instead the algorithm requires that the user identify putative gene products before the prediction is made so that the prediction represents the best fit of the given dna sequence to its putative transcription product the method uses a spliced alignment algorithm to sequentially explore all possible exon assemblies looking for the best fit of predicted gene structure to candidate protein if the candidate protein is known to arise from the query dna sequence correct gene structures can be predicted with an accuracy of or better by making use of candidate proteins in the course of the prediction procrustes can take advantage of information known about this protein or related proteins in the public databases to better deter human gene genscan annotated genomescan blastx initial internal terminal exon exon exon hit exon exon exon kb 13 kb 5 figure comparison of genscan with genomescan using the human gene sequence as the query the genscan prediction top line is missing a number of the exons that appear in the annotation for the gene second line genbank and the genscan prediction is slightly longer than the actual gene at the 5 end the inclusion of blastx hit information vertical bars closest to the scale in genomescan produces a more complete and accurate prediction third line mine the location of the introns and the exons in this gene procrustes can handle cases where there are either partial or multiple genes in the query dna sequence the input to procrustes is through a web interface and is quite simple the user needs to supply the nucleotide sequence and as many protein sequences as are relevant to this region the supplied protein sequences will be treated as being sim ilar though not necessarily identical to that encoded by the dna sequence typical output from procrustes not shown here includes an aligned map of the pre dicted intron exon structure for all target proteins probability values a list of exons with their starting and ending nucleotide positions translations of the gene model which may not be the same as the sequence of the initially supplied protein and a spliced alignment showing any differences between the predicted protein and the target protein the nature of the results makes procrustes a valuable method for refining results obtained by other methods particularly in the context of positional candidate efforts geneid the current version of geneid finds exons based on measures of coding potential guigo et al the original version of this program was among the fastest in that it used a rule based system to examine the putative exons and assemble them into the most likely gene for that sequence geneid uses position weight matrices to assess whether or not a given stretch of sequence represents a splice site or a start or stop codon once this assessment is made models of putative exons are built on the basis of the sets of predicted exons that geneid develops a final refinement round is performed yielding the most probable gene structure based on the input sequence the interface to geneid is through a simple web front end in which the user pastes in the dna sequence and specifies whether the organism is either human or drosophila the user can specify whether predictions should be made only on the forward or reverse strand and available output options include lists of putative ac ceptor sites donor sites and start and stop codons users can also limit output to only first exons internal exons terminal exons or single genes for specialized anal yses it is recommended that the user simply select all exons to assure that all relevant information is returned geneparser geneparser snyder and stormo snyder and stormo uses a slightly different approach in identifying putative introns and exons instead of predetermin ing candidate regions of interest geneparser computes scores on all subintervals in a submitted sequence once each subinterval is scored a neural network approach is used to determine whether each subinterval contains a first exon internal exon final exon or intron the individual predictions are then analyzed for the combination that represents the most likely gene there is no web front end for this program but the program itself is freely available for use on sun dec and sgi based systems hmmgene hmmgene predicts whole genes in any given dna sequence using a hidden markov model hmm method geared toward maximizing the probability of an accurate prediction krogh the use of hmms in this method helps to assess the confidence in any one prediction enabling hmmgene to not only report the best prediction for the input sequence but alternative predictions on the same sequence as well one of the strengths of this method is that by returning multiple predictions on the same region the user may be able to gain insight onto possible alternative splicings that may occur in a region containing a single gene the front end for hmmgene requires an input sequence with the organismal options being either human or c elegans an interesting addition is that the user can include known annotations which could be from one of the public databases or based on experimental data that the investigator is privy to multiple sequences in fasta format can be submitted as a single job to the server examples of sequence input format and resulting output are given in the documentation file at the hmmgene web site how well do the methods work as we have already seen different methods produce different types of results in some cases lists of putative exons are returned but these exons are not in a genomic context in other cases complete gene structures are predicted but possibly at a cost of less reliable individual exon predictions looking at the absolute results for the bac clone anywhere between one and three genes are predicted for the region and those one to three genes have anywhere between and exons in cases of similar exons the boundaries of the exons are not always consistent which method is the winner in this particular case is not important what is important is the variance in the results returning to the cautionary note that different methods will perform better or worse depending on the system being examined it becomes important to be able to quantify the performance of each of these algorithms several studies have syste matically examined the rigor of these methods using a variety of test data sets burset and guigo claverie snyder and stormo rogic et al before discussing the results of these studies it is necessary to define some terms for any given prediction there are four possible outcomes the detection of a true positive true negative false positive or false negative fig two measures of accuracy can be calculated based on the ratios of these occurrences a sensitivity value reflecting the fraction of actual coding regions that are correctly predicted as truly being coding regions and a specificity value reflecting the overall fraction of the prediction that is correct in the best case scenario the methods will try to op timize the balance between sensitivity and specificity to be able to find all of the true exons without becoming so sensitive as to start picking up an inordinate amount of false positives an easier to understand measure that combines the sensitivity and specificity values is called the correlation coefficient like all correlation coefficients its value can range from meaning that the prediction is always wrong through zero to 1 meaning that the prediction is always right figure sensitivity vs specificity in the upper portion the four possible outcomes of a prediction are shown a true positive tp a true negative tn a false positive fp and a false negative fn the matrix at the bottom shows how both sensitivity and specificity are determined from these four possible outcomes giving a tangible measure of the ef fectiveness of any gene prediction method figure adapted from burset and guigo snyder and stormo as a result of a cold spring harbor laboratory meeting on gene prediction 1 a web site called the banbury cross was created the intent behind creating such a web site was twofold for groups actively involved in program development to post their methods for public use and for researchers actively deriving fully char acterized finished genomic sequence to submit such data for use as benchmark sequences in this way the meeting participants created an active forum for the dissemination of the most recent findings in the field of gene identification using these and other published studies jean michel claverie at cnrs in marseille com pared the sensitivity and specificity of different gene identification programs claverie and references therein procrustes was not one of the con sidered since the method varies substantially from that employed by other gene prediction programs in examining data from these disparate sources either the best performance found in an independent study or the worst performance reported by the authors of the method themselves was used in making the comparisons on the basis of these comparisons the best overall individual exon finder was deemed to be mzef and the best gene structure prediction program was deemed to be gen scan by back calculating as best as possible from the numbers reported in the claverie paper these two methods gave the highest correlation coefficients within their class with ccmzef 79 and ccgenscan 86 1 finding genes computational analysis of dna sequences cold spring harbor laboratory march because these gene finding programs are undergoing a constant evolution add ing new features and incorporating new biological information the idea of a com parative analysis of a number of representative algorithms was recently revisited rogic et al one of the encouraging outcomes of this study was that these newer methods as a whole did a substantially better job in accurately predicting gene structures than their predecessors did by using an independent data set con taining sequences from genbank in which intron exon boundaries have been annotated genscan and hmmgene appeared to perform the best both having a correlation coefficient of note the improvement of ccgenscan from the time of the burset and guigo study to the time of the rogic et al study strategies and considerations given these statistics it can be concluded that both mzef and genscan are particularly suited for differentiating introns from exons at different stages in the maturation of sequence data however this should not be interpreted as a blanket recommendation to only use these two programs in gene identification remember that these results represent a compilation of findings from different sources so keep in mind that the reported results may not have been derived from the same data set it has already been stated numerous times that any given program can behave better or worse depending on the input sequences it has also been demonstrated that the actual performance of these methods can be highly sensitive to g c content for example snyder and stormo reported that geneparser snyder and stormo and with assembly performed best on test sets having high g c content as assessed by their respective cc values whereas geneid guigo et al performed best on test sets having low g c content interestingly both genscan and hmmgene were seen to perform steadily regardless of g c content in the rogic study rogic et al there are several major drawbacks that most gene identification programs share that users need to be keenly aware of because most of these methods are trained on test data they will work best in finding genes most similar to those in the training sets that is they will work best on things similar to what they have seen before often methods have an absolute requirement to predict both a discrete beginning and an end to a gene meaning that these methods may miscall a region that consists of either a partial gene or multiple genes the importance given to each individual factor in deciding whether a stretch of sequence is an intron or an exon can also influence outcomes as the weighing of each criterion may be either biased or in correct finally there is the unusual case of genes that are transcribed but not trans lated so called noncoding rna genes one such gene ntt noncoding tran script in t cells shows no exons or significant open reading frames even though rt pcr shows that ntt is transcribed as a polyadenlyated kb mrna liu et al a similar protein ipw is involved in imprinting and its expression is correlated to the incidence of prader willi syndrome wevrick et al because hallmark features of gene structure are presumably absent from these genes they cannot be reliably detected by any known method to date it begins to become evident that no one program provides the foolproof key to computational gene identification the correct choice will depend on the nature of the data and where in the pathway of data maturation the data lie on the basis of the studies described above some starting points can be recommended in the case of incompletely assembled sequence contigs prefinished genome survey sequence mzef provides the best jumping off point since for sequences of this length one would expect no more than one exon in the case of nearly finished or finished data where much larger contigs provide a good deal of contextual information gen scan or hmmgene would be an appropriate choice in either case users should supplement these predictions with results from at least one other predictive method as consistency among methods can be used as a qualitative measure of the robustness of the results furthermore utilization of comparative search methods such as blast altschul et al or fasta pearson et al should be considered an absolute requirement with users targeting both dbest and the protein databases for homology based clues procrustes again should be used when some infor mation regarding the putative gene product is known particularly when the cloning efforts are part of a positional candidate strategy a good example of the combinatorial approach is illustrated in the case of the gene for cerebral cavernous malformation located at here a combination of mzef genscan xgrail and powerblast zhang and mad den was used in an integrated fashion in the prediction of gene structure kuehl et al another integrated approach to this approach lies in work benches such as genotator which allow users to simultaneously run a number of prediction methods and homology searches as well as providing the ability to an notate sequence features through a graphical user interface harris a combinatorial method developed at the national human genome research institute combines most of the methods described in this chapter into a single tool this tool named genemachine allows users to query multiple exon and gene pre diction programs in an automated fashion makalowska et al a suite of perl modules are used to run mzef genscan fgenes and blast repeatmasker and sputnik are used to find repeats within the query sequence once genemachine is run a file is written that can subsequently be opened using ncbi sequin in essence using sequin as a workbench and graphical viewer using sequin also has the advantage of presenting the results to the user in a familiar format basically the same format that is used in entrez for graphical views the main feature of genemachine is that the process is fully automated the user is only required to launch genemachine and then open the resulting file with ncbi sequin gene machine also does not require users to install local copies of the prediction programs enabling users to pass off to web interfaces instead although this reduces some of the overhead of maintaining the program it does result in slower performance an notations can then be made to these results before submission to genbank thereby increasing the intrinsic value of these data a sample of the output obtained using genemachine is shown in figure and more details on genemachine can be found on the nhgri web site the ultimate solution to the gene identification problem lies in the advancement of the human genome project and other sequencing projects as more and more gene structures are elucidated this biological information can in turn be used to develop better methods yielding more accurate predictions although the promise of such computational methods may not be completely fulfilled before the human genome project reaches completion the information learned from this effort will play a major role in facilitating similar efforts targeting other model genomes figure annotated output from genemachine showing the results of multiple gene prediction program runs ncbi sequin is used at the viewer the top of the output shows the results from various blast runs blastn vs dbest blastn vs nr and blastx vs swiss prot toward the bottom of the window are shown the results from the predictive meth ods fgenes genscan mzef and grail 2 annotations indicating the strength of the prediction are preserved and shown wherever possible within the viewer putative regions of high interest would be areas where hits from the blast runs line up with exon predic tions from the gene prediction programs see color plate internet resources for topics presented in chapter banbury cross http igs server cnrs mrs fr igs banbury fgeneh http genomic sanger ac uk gf gf shtml geneid http imim es geneid html genemachine http genome nhgri nih gov genemachine geneparser http beagle colorado edu eesnyder geneparser htl genscan http genes mit edu genscan html genotator http www fruitfly org nomi genotator grail http compbio ornl gov tools index shtml grail exp http compbio ornl gov grailexp hmmgene http www cbs dtu dk services hmmgene mzef http www cshl org genefinder references procrustes http www hto usc edu software procrustes repeatmasker http ftp genome washington edu rm repeatmasker html sputnik http rast abajian com sputnik problem set an anonymous sequence from requiring computational analysis is posted on the book web site http www wiley com bioinformatics to gain a better appreciation for the relative performance of the methods discussed in this chapter and how the results may vary between methods use fgenes genscan and hmmgene to answer each of the following questions 1 how many exons are in the unknown sequence 2 what are the start and stop points for each of these exons which strand forward or reverse are the putative exons found on are there any unique features present like polya tracts where are they located 5 can any protein translations be derived from the sequence what is the length in amino acids of these translations for hmmgene only can alternative translations be computed for this particular dna sequence if so give the number of exons and the length of the coding region cds for each possible alternative prediction note on which strand the alternative translations are found references altschul s f madden t l schaffer a a zhang j zhang z miller w and lipman d j gapped blast and psi blast a new generation of protein database search programs nucleic acids res burge c and karlin s prediction of complete gene structures in human genomic dna j mol biol burge c b and karlin s finding the genes in genomic dna curr opin struct biol 346 burset m and guigo r evaluation of gene structure prediction programs genomics chothia c and lesk a m the relation between the divergence of sequence and structure in proteins embo j 5 823 claverie j m computational methods for exon detection mol biotechnol 27 claverie j m computational methods for the identification of genes in vertebrate genomic sequences hum mol genet 1735 claverie j m exon detection by similarity searches methods mol biol everett l a glaser b beck j c idol j r buchs a heyman m adawi f hazani e nassir e baxevanis a d sheffield v c and green e d pendred syn drome is caused by mutations in a putative sulphate transporter gene pds nat genet 411 gelfand m s mironov a a and pevzner p a gene recognition via spliced sequence alignment proc natl acad sci usa guigo r computational gene identification j mol med guigo r knudsen s drake n and smith t prediction of gene structure j mol biol harris n l genotator a workbench for sequence annotation genome res 754 krogh a two methods for improving performance of an hmm and their application for gene finding in proceedings of the fifth international conference on intelligent sys tems for molecular biology gaasterland t karp p karplus k ouzounis c sander c and valencia a eds aaai press menlo park ca p kuehl p weisemann j touchman j green e and boguski m an effective approach for analyzing prefinished genomic sequence data genome res 189 liu a y torchia b s migeon b r and siliciano r f the human ntt gene identification of a novel kb noncoding nuclear rna expressed in activated t cells genomics makalowska i ryan j f and baxevanis a d genemachine a unified solution for performing content based site based and comparative gene prediction methods cold spring harbor meeting on genome mapping sequencing and biology cold spring harbor ny mural r j einstein j r guan x mann r c and uberbacher e c an artificial intelligence approach to dna sequence feature recognition trends biotech 67 pearson w r wood t zhang z and miller w comparison of dna sequences with protein sequences genomics 36 rogic s mackworth a and ouellette b f f evaluation of gene finding pro grams in press snyder e e and stormo g d identification of coding regions in genomic dna sequences an application of dynamic programming and neural networks nucleic acids res 607 snyder e e and stormo g d identifying genes in genomic dna sequences in dna and protein sequence analysis m j bishop and c j rawlings eds new york oxford university press p 224 solovyev v v salamov a a and lawrence c b identification of human gene structure using linear discriminant functions and dynamic programming ismb 367 solovyev v v salamov a a and lawrence c b predicting internal exons by oligonucleotide composition and discriminant analysis of spliceable open reading frames nucleic acids res solovyev v v salamov a a and lawrence c b the prediction of human exons by oligonucleotide composition and discriminant analysis of spliceable open reading frames ismb 2 354 uberbacher e c and mural r j locating protein coding regions in human dna sequences by a multiple sensor neural network approach proc natl acad sci usa wevrick r kerns j a and francke u the ipw gene is imprinted and is not expressed in the prader willi syndrome acta genet med gemellol zhang j and madden t l powerblast a new network blast application for interactive or automated sequence analysis and annotation genome res 649 bioinformatics a practical guide to the analysis of genes and proteins second edition andreas d baxevanis b f francis ouellette copyright john wiley sons inc isbns 471 2 hardback 471 paper 471 1 electronic predictive methods using protein sequences sharmila banerjee basu genome technology branch national human genome research institute national institutes of health bethesda maryland andreas d baxevanis genome technology branch national human genome research institute national institutes of health bethesda maryland the discussions of databases and information retrieval in earlier chapters of this book document the tremendous explosion in the amount of sequence information available in a variety of public databases as we have already seen with nucleotide sequences all protein sequences whether determined directly or through the translation of an open reading frame in a nucleotide sequence contain intrinsic information of value in determining their structure or function unfortunately experiments aimed at ex tracting such information cannot keep pace with the rate at which raw sequence data are being produced techniques such as circular dichroism spectroscopy optical ro tatory dispersion x ray crystallography and nuclear magnetic resonance are ex tremely powerful in determining structural features but their execution requires many hours of highly skilled technically demanding work the gap in information becomes obvious in comparisons of the size of the protein sequence and structure databases as of this writing there were protein entries release in swiss prot but only 624 structure entries july in pdb attempts to close the gap center around theoretical approaches for structure and function prediction these methods can provide insights as to the properties of a protein in the absence of biochemical data this chapter focuses on computational techniques that allow for biological dis covery based on the protein sequence itself or on their comparison to protein families unlike nucleotide sequences which are composed of four bases that are chemically rather similar yet distinct the alphabet of amino acids found in proteins allows for much greater diversity of structure and function primarily because the differences in the chemical makeup of these residues are more pronounced each residue can influence the overall physical properties of the protein because these amino acids are basic or acidic hydrophobic or hydrophilic and have straight chains branched chains or are aromatic thus each residue has certain propensities to form structures of different types in the context of a protein domain these properties of course are the basis for one of the central tenets of biochemistry that sequence specifies con formation anfinsen et al the major precaution with respect to these or any other predictive techniques is that regardless of the method the results are predictions different methods using different algorithms may or may not produce different results and it is important to understand how a particular predictive method works rather than just approaching the algorithm as a black box one method may be appropriate in a particular case but totally inappropriate in another even so the potential for a powerful synergy exists proper use of these techniques along with primary biochemical data can pro vide valuable insights into protein structure and function protein identity based on composition the physical and chemical properties of each of the amino acids are fairly well understood and a number of useful computational tools have been developed for making predictions regarding the identification of unknown proteins based on these properties and vice versa many of these tools are available through the expasy server at the swiss institute of bioinformatics appel et al the focus of the expasy tools is twofold to assist in the analysis and identification of unknown proteins isolated through two dimensional gel electrophoresis as well as to predict basic physical properties of a known protein these tools capitalize on the curated annotations in the swiss prot database in making their predictions although calculations such as these are useful in electrophoretic analysis they can be very valuable in any number of experimental areas particularly in chromatographic and sedimentation studies in this and the following section tools in the expasy suite are identified but the ensuing discussion also includes a number of useful programs made available by other groups internet resources related to these and other tools discussed in this chapter are listed at the end of the chapter aacompident and aacompsim expasy rather than using an amino acid sequence to search swiss prot aacompident uses the amino acid composition of an unknown protein to identify known proteins of the same composition wilkins et al as inputs the program requires the desired amino acid composition the isoelectric point pi and molecular weight of the protein if known the appropriate taxonomic class and any special keywords in addition the user must select from one of six amino acid constellations which influence how the analysis is performed for example certain constellations may combine residues like asp asn d n and gln glu q e into asx b and glx z or certain residues may be eliminated from the analysis altogether for each sequence in the database the algorithm computes a score based on the difference in compositions between the sequence and the query composition the results returned by e mail are organized as three ranked lists a list based on all proteins from the specified taxonomic class without taking pi or molecular weight into account a list based on all proteins regardless of taxonomic class without taking pi or molecular weight into account and a list based on the specified taxonomic class that does take pi and molecular weight into account because the computed scores are a difference measure a score of zero implies that there is exact correspondence between the query composition and that sequence entry aacompsim a variant of aacompident performs a similar type of analysis but rather than using an experimentally derived amino acid composition as the basis for searches the sequence of a swiss prot protein is used instead wilkins et al a theoretical pi and molecular weight are computed before computation of the difference scores using compute pi mw see below it has been documented that amino acid composition across species boundaries is well conserved cordwell et al and that by considering amino acid composition investigators can detect weak similarities between proteins whose sequence identity falls below ho bohm and sander thus the consideration of composition in addition to the ability to perform traditional database searches may provide additional insight into the relationships between proteins propsearch along the same lines as aacompsim propsearch uses the amino acid com position of a protein to detect weak relationships between proteins and the authors have demonstrated that this technique can be used to easily discern members of the same protein family hobohm and sander however this technique is more robust than aacompsim in that different physical properties are used in per forming the analysis among which are molecular weight the content of bulky res idues average hydrophobicity and average charge this collection of physical prop erties is called the query vector and it is compared against the same type of vector precomputed for every sequence in the target databases swiss prot and pir having this database of vectors calculated in advance vastly improves the proc essing time for a query the input to the propsearch web server is just the query sequence and an example of the program output is shown in figure 1 here the sequence of human autoantigen nor was used as the input query the results are ranked by a distance score and this score represents the likelihood that the query sequence and new figure 1 results of a propsearch database query based on amino acid composition the input sequence used was that of the human autoantigen nor explanatory material and a histogram of distance scores against the entire target database have been removed for brevity the columns in the table give the rank of the hit based on the distance score the swiss prot or pir identifier the distance score the length of the overlap between the query and subject the positions of the overlap from to the calculated pi and the definition line for the found sequence sequences found through propsearch belong to the same family thereby imply ing common function in most cases a distance score of or below indicates that there is a better than chance that there is similarity between the two proteins a score below increases the reliability to and a score below 5 increases the reliability to examination of the results showed nor to be similar to a number of nucleolar transcription factors protein kinases a retinoblastoma binding protein the actin binding protein radixin and a putative gtpase target none of these hits would necessarily be expected since the functions of these pro teins are dissimilar however a good number of these are dna binding proteins opening the possibility that a very similar domain is being used in alternative func tional contexts at the very least a blastp search would be necessary to both verify the results and identify critical residues mowse the molecular weight search mowse algorithm capitalizes on information ob tained through mass spectrometric ms techniques pappin et al with the use of both the molecular weights of intact proteins and those resulting from diges tion of the same proteins with specific proteases an unknown protein can be un ambiguously identified given the results of several experimental determinations this approach substantially cuts down on experimental time since the unknown protein does not have to be sequenced in whole or in part the mowse web front end requires the molecular weight of the starting se quence and the reagent used as well as the resultant masses and composition of the peptides generated by the reagent a tolerance value may be specified indicating the error allowed in the accuracy of the determined fragment masses calculations are based on information contained in the owl nonredundant protein sequence database akrigg et al scoring is based on how often a fragment molecular weight occurs in proteins within a given range of molecular weights and the output is returned as a ranked list of the top scores with the owl entry name matching peptide sequences and other statistical information simulation studies produced an accuracy rate of using five or fewer input peptide weights physical properties based on sequence compute pi mw and protparam expasy compute pi mw is a tool that calculates the isoelectric point and molecular weight of an input sequence determination of pi is based on pk values as described in an earlier study on protein migration in denaturing conditions at neutral to acidic ph bjellqvist et al because of this the authors caution that pi values determined for basic proteins may not be accurate molecular weights are calculated by the addition of the average isotopic mass of each amino acid in the sequence plus that of one water molecule the sequence can be furnished by the user in fasta format or a swiss prot identifier or accession number can be specified if a sequence is furnished the tool automatically computes the pi and molecular weight for the entire length of the sequence if a swiss prot identifier is given the definition and organism lines of the entry are shown and the user may specify a range of amino acids so that the computation is done on a fragment rather than on the entire protein protparam takes this process one step further based on the input sequence prot param calculates the molecular weight isoelectric point overall amino acid com position a theoretical extinction coefficient gill and von hippel aliphatic index ikai the protein grand average of hydrophobicity gravy value kyte and doolittle and other basic physicochemical parameters although this might seem to be a very simple program one can begin to speculate about the cellular localization of the protein for example a basic protein with a high propor tion of lysine and arginine residues may well be a dna binding protein peptidemass expasy designed for use in peptide mapping experiments peptidemass determines the cleav age products of a protein after exposure to a given protease or chemical reagent wilkins et al the enzymes and reagents available for cleavage through peptidemass are trypsin chymotrypsin lysc cyanogen bromide argc aspn and gluc bicarbonate or phosphate cysteines and methionines can be modified before the calculation of the molecular weight of the resultant peptides by furnishing a swiss prot identifier rather than pasting in a raw sequence peptidemass is able to use information within the swiss prot annotation to improve the calculations such as removing signal sequences or including known posttranslational modifica tions before cleavage the results are returned in tabular format giving a theoretical pi and molecular weight for the starting protein and then the mass position modified masses information on variants from swiss prot and the sequence of the peptide fragments tgrease tgrease calculates the hydrophobicity of a protein along its length kyte and doolittle inherent in each of the amino acids is its hydrophobicity the relative propensity of the acid to bury itself in the core of a protein and away from surrounding water molecules this tendency coupled with steric and other consid erations influences how a protein ultimately folds into its final three dimensional conformation as such tgrease finds application in the determination of putative transmembrane sequences as well as the prediction of buried regions of globular proteins tgrease is part of the fasta suite of programs available from the university of virginia and runs as a stand alone application that can be downloaded and run on either macintosh or dos based computers the method relies on a hydropathy scale in which each amino acid is assigned a score reflecting its relative hydrophobicity based on a number of physical char acteristics e g solubility the free energy of transfer through a water vapor phase transition etc amino acids with higher positive scores are more hydrophobic those with more negative scores are more hydrophilic a moving average or hydro pathic index is then calculated across the protein the window length is adjustable with a span of residues recommended to minimize noise and maximize infor mation content the results are then plotted as hydropathic index versus residue number the sequence for the human interleukin receptor b was used to generate a tgrease plot as shown in figure 2 correspondence between the peaks and the actual location of the transmembrane segments although not exact is fairly good figure 2 results of a kyte doolittle hydropathy determination using tgrease the input sequence was of the high affinity interleukin receptor b from human default win dow lengths were used the thick horizontal bars across the bottom of the figure were added manually and represent the positions of the seven transmembrane regions of il b as given in the swiss prot entry for this protein keep in mind that the method is predicting all hydrophobic regions not just those located in transmembrane regions the specific detection of transmembrane regions is discussed further below saps the statistical analysis of protein sequences saps algorithm provides extensive statistical information for any given query sequence brendel et al when a protein sequence is submitted via the saps web interface the server returns a large amount of physical and chemical information on the protein based solely on what can be inferred from its sequence the output begins with a composition analysis with counts of amino acids by type this is followed by a charge distribution analysis including the locations of positively or negatively charged clusters high scoring charged and uncharged segments and charge runs and patterns the final sections present information on high scoring hydrophobic and transmembrane segments re petitive structures and multiplets as well as a periodicity analysis motifs and patterns in chapter the idea of direct sequence comparison was presented where blast searches are performed to identify sequences in the public databases that are similar to a query sequence of interest often this direct comparison may not yield any interesting results or may not yield any results at all however there may be very weak sequence determinants that are present that will allow the query sequence to be associated with a family of sequences by the same token a family of sequences can be used to identify new distantly related members of the same protein family an example of this is psi blast discussed in chapter before discussing two of the methods that capitalize on such an approach several terms have to be defined the first is the concept of profiles profiles are quite simply a numerical representation of a multiple sequence alignment much like the multiple sequence alignments derived from the methods discussed in chapter imbedded within a multiple sequence alignment is intrinsic sequence information that represents the common characteristics of that particular collection of sequences frequently a protein family by using a profile one is able to use these imbedded common characteristics to find similarities between sequences with little or no absolute se quence identity allowing for the identification and analysis of distantly related pro teins profiles are constructed by taking a multiple sequence alignment representing a protein family and then asking a series of questions what residues are seen at each position of the alignment how often does a particular residue appear at each position of the alignment are there positions that show absolute conservation can gaps be introduced anywhere in the alignment once those questions are answered a position specific scoring table psst is con structed and the numbers in the table now represent the multiple sequence alignment the numbers within the psst reflect the probability of any given amino acid oc curring at each position it also reflects the effect of a conservative or nonconser vative substitution at each position in the alignment much like a pam or blosum matrix does this psst can now be used for comparison against single sequences the second term requiring definition is pattern or signature a signature also represents the common characteristics of a protein family or a multiple sequence alignment but does not contain any weighting information whatsoever it simply provides a shorthand notation for what residues can be present at any given position for example the signature iv g x g t livmf x 2 gs would be read as follows the first position could contain either an isoleucine or a valine the second position could contain only a glycine and so on an x means that any residue can appear at that position the x 2 simply means that two positions can be occupied by any amino acid the number just reflecting the length of the nonspecific run profilescan based on the classic gribskov method of profile analysis gribskov et al profilescan uses a method called pfscan to find similarities between a protein or nucleic acid query sequence and a profile library lu thy et al in this case three profile libraries are available for searching first is prosite an expasy database that catalogs biologically significant sites through the use of motif and sequence profiles and patterns hofmann second is pfam which is a collec tion of protein domain families that differ from most such collections in one impor tant aspect the initial alignment of the protein domains is done by hand rather than by depending on automated methods as such pfam contains slightly over en tries but the entries are potentially of higher quality the third profile set is referred to as the gribskov collection searches against any of these collections can be done through the profilescan web page which simply requires either an input sequence in plain text format or an identifier such as a swiss prot id the user can select the sensitivity of the search returning either significant matches only or all matches including borderline cases to illustrate the output format the sequence of a human heat shock induced protein was submitted to the server for searching against prosite profiles only normalized raw from to profile description pos 612 heat shock proteins although the actual prosite entry returned is no great surprise the output contains scores that are worth understanding the raw score is the actual score cal culated from the scoring matrix used during the search the more informative number is the normalized or n score the n score formally represents the number of matches one would expect in a database of given size basically the larger the n score the lower the probability that the hit occurred by chance in the example the n score of translates to 1 x 349 expected chance matches when normalized against swiss prot an extremely low probability of this being a false positive the from and to numbers simply show the positions of the overlap between the query and the matching profile blocks the blocks database utilizes the concept of blocks to identify a family of proteins rather than relying on the individual sequences themselves henikoff and henikoff the idea of a block is derived from the more familiar notion of a motif which usually refers to a conserved stretch of amino acids that confer a specific function or structure to a protein when these individual motifs from proteins in the same family are aligned without introducing gaps the result is a block with the term block referring to the alignment not the individual sequences themselves obvi ously an individual protein can contain one or more blocks corresponding to each of its functional or structural motifs the blocks database itself is derived from the entries in prosite when a blocks search is performed using a sequence of interest the query sequence is aligned against all the blocks in the database at all possible positions for each alignment a score is calculated using a position specific scoring matrix and results of the best matches are returned to the user searches can be performed optionally against the prints database which includes information on more than families that do not have corresponding entries in the blocks database to ensure complete coverage it is recommended that both databases be searched blocks searches can be performed using the blocks web site at the fred hutchinson cancer research center in seattle the web site is straightforward al lowing both sequence based and keyword based searches to be performed if a dna sequence is used as the input users can specify which genetic code to use and which strand to search regardless of whether the query is performed via a sequence or via keywords a successful search will return the relevant block an example is shown in figure in this entry for a nuclear hormone receptor called a steroid finger figure 3 structure of a typical blocks entry this is part of the entry for one block associated with steroid fingers the structure of the entry is discussed in the text the header lines marked id ac and de give in order a short description of the family represented by this block the blocks database accession number and a longer description of the family the bl line gives information regarding the original sequence motif that was used to construct this particular block the width and seqs parameters show how wide the block is in residues and how many sequences are in the block respectively some information then follows regarding the statistical validity and the strength of the construct finally a list of sequences is presented showing only the part of the sequence corresponding to this particular motif each line begins with the swiss prot accession number for the sequence the number of the first residue shown based on the entire sequence the sequence itself and a position based sequence weight these values are scaled with representing the sequence that is most distant from the group notice that there are blank lines be tween some of the sequences parts of the overall alignment are clustered and in each cluster of the sequence residues are identical cdd recently ncbi introduced a new search service aimed at identifying conserved domains within a protein sequence the source database for these searches is called the conserved domain database or cdd this is a secondary database with entries derived from both pfam described above and smart simple modular architec ture research tool smart can be used to identify genetically mobile domains and analyze domain architectures and is discussed in greater detail within the context of comparative genomics in chapter 15 the actual search is performed using reverse position specific blast rps blast which uses the query sequence to search a database of precalculated pssts the cdd interface is simple providing a box for the input sequence alterna tively an accession number can be specified and a pull down menu for selecting the target database if conserved domains are identified within the input sequence a graphic is returned showing the position of each conserved domain followed by the actual alignment of the query sequence to the target domain as generated by rps blast in these alignments the default view shows identical residues in red whereas conservative substitutions are shown in blue users can also select from a variety of representations including the traditional blast style alignment display hyperlinks are provided back to the source databases providing more information on that particular domain this cd summary page gives the underlying source database information references the taxonomy spanned by this entry and a sequence entry representative of the group in the lower part of the page the user can construct an alignment of sequences of interest from the group alternatively the user can allow the computer to select the top ranked sequences or a subset of sequences that are most diverse within the group if a three dimensional structure corresponding to the cd is available it can be viewed directly using see chapter 5 clicking on the cd link next to any of the entries on the cd summary page will in essence start the whole process over again using that sequence to perform a new rps blast search against cdd secondary structure and folding classes one of the first steps in the analysis of a newly discovered protein or gene product of unknown function is to perform a blast or other similar search against the public databases however such a search might not produce a match against a known protein if there is a statistically significant hit there may not be any information in the sequence record regarding the secondary structure of the protein information that is very important in the rational design of biochemical experiments in the absence of known information there are methods available for predicting the ability of a sequence to form a helices and strands these methods rely on ob servations made from groups of proteins whose three dimensional structure has been experimentally determined a brief review of secondary structure and folding classes is warranted before the techniques themselves are discussed as already alluded to a significant number of amino acids have hydrophobic side chains whereas the main chain or backbone is hydrophilic the required balance between these two seemingly opposing forces is accomplished through the formation of discrete secondary structural elements first described by linus pauling and colleagues in pauling and corey an a helix is a corkscrew type structure with the main chain forming the backbone and the side chains of the amino acids projecting outward from the helix the backbone is stabilized by the formation of hydrogen bonds between the co group of each amino acid and the nh group of the residue four positions c terminal n creating a tight rodlike structure some residues form a helices better than others alanine glutamine leucine and methionine are commonly found in a helices whereas proline glycine tyrosine and serine usually are not proline is commonly thought of as a helix breaker because its bulky ring structure disrupts the formation of n hydrogen bonds in contrast the strand is a much more extended structure rather than hydro gen bonds forming within the secondary structural unit itself stabilization occurs through bonding with one or more adjacent strands the overall structure formed through the interaction of these individual strands is known as a pleated sheet these sheets can be parallel or antiparallel depending on the orientation of the n and c terminal ends of each component strand a variant of the sheet is the turn in this structure the polypeptide chain makes a sharp hairpin bend producing an antiparallel sheet in the process in levitt and chothia proposed a classification system based on the order of secondary structural elements within a protein levitt and chothia quite simply an a structure is made up primarily from a helices and a structure is made up of primarily strands myoglobin is the classic example of a protein composed entirely of a helices falling into the a class of structures takano plasto cyanin is a good example of the class where the hydrogen bonding pattern be tween eight strands form a compact barrel like structure guss and freeman the combination class a is made up of primarily strands alternating with a helices flavodoxin is a good example of an a protein its strands form a central sheet which is surrounded by a helices burnett et al predictive methods aimed at extracting secondary structural information from the linear primary sequence make extensive use of neural networks traditionally used for analysis of patterns and trends basically a neural network provides com putational processes the ability to learn in an attempt to approximate human learn ing versus following instructions blindly in a sequential manner every neural net work has an input layer and an output layer in the case of secondary structure prediction the input layer would be information from the sequence itself and the output layer would be the probabilities of whether a particular residue could form a particular structure between the input and output layers would be one or more hidden layers where the actual learning would take place this is accomplished by providing a training data set for the network here an appropriate training set would be all sequences for which three dimensional structures have been deduced the network can process this information to look for what are possibly weak rela tionships between an amino acid sequence and the structures they can form in a particular context a more complete discussion of neural networks as applied to secondary structure prediction can be found in kneller et al nnpredict the nnpredict algorithm uses a two layer feed forward neural network to assign the predicted type for each residue kneller et al in making the predictions the server uses a fasta format file with the sequence in either one letter or three letter code as well as the folding class of the protein a or a residues are classified as being within an a helix h a strand e or neither if no prediction can be made for a given residue a question mark is returned to indicate that an assignment cannot be made with confidence if no information is available regarding the folding class the prediction can be made without a folding class being specified this is the default for the best case prediction the accuracy rate of nnpredict is reported as being over sequences are submitted to nnpredict by either sending an e mail message to nnpredict celeste ucsf edu or by using the web based submission form with the use of flavodoxin as an example the format of the e mail message would be as follows option a b flavodoxin anacystis nidulans akiglfygtqtgvtqtiaesiqqefggesivdlndianadasdlnaydyliigcptwnvgelqsdwegiy ddldsvnfqgkkvayfgagdqvgysdnfqdamgileekisslgsqtvgywpiegydfneskavrnnqfvg laidednqpdltknriktwvsqlksefgl the option line specifies the folding class of the protein n uses no folding class for the prediction a specifies a b specifies and a b specifies a only one sequence may be submitted per e mail message the results returned by the server are shown in modified form in figure 4 predictprotein predictprotein rost et al uses a slightly different approach in making its predictions first the protein sequence is used as a query against swiss prot to find similar sequences when similar sequences are found an algorithm called maxhom is used to generate a profile based multiple sequence alignment sander and schneider maxhom uses an iterative method to construct the alignment after the first search of swiss prot all found sequences are aligned against the query sequence and a profile is calculated for the alignment the profile is then used to search swiss prot again to locate new matching sequences the multiple align ment generated by maxhom is subsequently fed into a neural network for prediction by one of a suite of methods collectively known as phd rost phdsec the method in this suite used for secondary structure prediction not only assigns each residue to a secondary structure type it provides statistics indicating the confidence of the prediction at each position in the sequence the method produces an average accuracy of better than the best case residue predictions have an accuracy rate of over sequences are submitted to predictprotein either by sending an e mail message or by using a web front end several options are available for sequence submission the query sequences can be submitted as single letter amino acid code or by its swiss prot identifier in addition a multiple sequence alignment in fasta format or as a pir alignment can also be submitted for secondary structure prediction the input message sent to predictprotein embl heidelberg de takes the follow ing form figure 4 comparison of secondary structure predictions by various methods the sequence of flavodoxin was used as the query and is shown on the first line of the alignment for each prediction h denotes an a helix e a strand and t a turn all other positions are assumed to be random coil correctly assigned residues are shown in inverse type the methods used are listed along the left side of the alignment and are described in the text at the bottom of the figure is the secondary structure assignment given in the pdb file for flavodoxin smith et al joe buzzcut national human genome research institute nih buzzcut baldguys org do not align fasta list homeodomain proteins antp krgrqtytryqtlelekefhfnryltrrrrieiahalslterqikiwfqnrrmkwkk hdd mdekrprtafsseqlarlkrefnenrylterrrqqlsselglneaqikiwfqnkrakikk dlx kirkprtiysslqlqalnhrfqqtqylalperaelaaslgltqtqvkiwfqnkrskfkk ftt rkrrvlfsqaqvyelerrfkqqkylsaperehlasmihltptqvkiwfqnhrykmkr lqrnrtsftqeqiealekeferthypdvfarerlaakidlpeariqvwfsnrrakwrr above is an example of a fasta formatted multiple sequence alignment of homeodomain proteins submitted for secondary structure prediction after the name affiliation and address lines the sign signals to the server that a sequence in one letter code follows the sequence format is essentially fasta except that blanks are not allowed for this alignment the phrase do not align before the line start ing with assures that the alignment will not be realigned nothing is allowed to follow the sequence the output sent as an e mail message is quite copious but contains a large amount of pertinent information the results can also be retrieved from an ftp site by adding a qualifier return no mail in any line before the line starting with this might be a useful feature for those e mail services that have difficulty handling very large output files the format for the output file can be plain text or html files with or without phd graphics the results of the maxhom search are returned complete with a multiple align ment that may be of use in further study such as profile searches or phylogenetic studies if the submitted sequence has a known homolog in pdb the pdb identifiers are furnished information follows on the method itself and then the actual prediction will follow in a recent release the output can also be customized by specifying available options unlike nnpredict predictprotein returns a reliability index of prediction for each position ranging from to with being the maximum con fidence that a secondary structure assignment has been made correctly the results returned by the server for this particular sequence as compared with those obtained by other methods are shown in modified form in figure 4 predator the predator secondary structure prediction algorithm is based on recognition of potentially hydrogen bonded residues in the amino acid sequence frishman and argos it uses database derived statistics on residue type occurrences in dif ferent classes of local hydrogen bonded structures the novel feature of this method is its reliance on local pairwise alignment of the sequence to be predicted between each related sequence the input for this program can be a single sequence or a set of unaligned related sequences sequences can be submitted to the predator server either by sending an e mail message to predator embl heidelberg de or by using a web front end the input sequences can be either fasta msf or clus tal format the mean prediction accuracy of predator in three structural states is for a single sequence and for a set of related sequences psipred the psipred method developed at the university of warwick uk uses the knowl edge inferred from psi blast altschul et al cf chapter 8 searches of the input sequence to perform predictions psipred uses two feedforward neural net works to perform the analysis on the profile obtained from psi blast sequences can be submitted through a simple web front end in either single letter raw format or in fasta format the results from the psipred prediction are returned as a text file in an e mail message in addition a link is also provided in the e mail message to a graphical representation of the secondary structure prediction visualized using a java application called psipredview in this representation the positions of the helices and strands are schematically represented above the target sequence the average prediction accuracy for psipred in three structural states is 5 which is higher than any of the other methods described here sopma the protein sequence analysis server at the centre national de la recherche scien tifique cnrs in lyons france takes a unique approach in making secondary structure predictions rather than using a single method it uses five the predictions from which are subsequently used to come up with a consensus prediction the methods used are the garnier gibrat robson gor method garnier et al the levin homolog method levin et al the double prediction method de le age and roux the phd method described above as part of predictprotein and the method of cnrs itself called sopma geourjon and de leage briefly this self optimized prediction method builds subdatabases of protein se quences with known secondary structures each of the proteins in a subdatabase is then subjected to secondary structure prediction based on sequence similarity the information from the subdatabases is then used to generate a prediction on the query sequence the method can be run by submitting just the sequence itself in single letter format to deleage ibcp fr using sopma as the subject of the mail message or by using the sopma web interface the output from each of the component predictions as well as the consensus is shown in figure 4 comparison of methods on the basis of figure 4 it is immediately apparent that all the methods described above do a relatively good but not perfect job of predicting secondary structures where no other information is known the best approach is to perform predictions using all the available algorithms and then to judge the validity of the predictions in comparison to one another flavodoxin was selected as the input query because it has a relatively intricate structure falling into the a folding class with its six a helices and five sheets some assignments were consistently made by all methods for example all the methods detected and fairly well however some methods missed some elements altogether e g nnpredict with and and some predictions made no biological sense e g the double prediction method and where helices sheets and turns alternate residue by residue predictprotein and psipred which both correctly found all the secondary structure elements and in several places identified structures of the correct length appear to have made the best overall prediction this is not to say that the other methods are not useful or not as good undoubtedly in some cases another method would have emerged as having made a better prediction this approach does not provide a fail safe method of prediction but it does reinforce the level of confidence resulting from these predictions a new web based server jpred integrates six different structure prediction meth ods and returns a consensus prediction based on simple majority rule the usefulness of this server is that it automatically generates the input and output requirements for all six prediction algorithms which can be an important feature when handling large data sets the input sequence for jpred can be a single protein sequence in fasta or pir format a set of unaligned sequences in pir format or a multiple sequence alignment in msf or blc format in case of a single sequence the server first generates a set of related sequences by searching the owl database using the blastp algorithm the sequence set is filtered using scanps and then pairwise compared using amps finally the sequence set is clustered using a identity cutoff value to remove any bias in the sequence set and the remaining sequences are aligned using clustal w the jpred server runs phd rost and sander dsc king and sternberg nnssp salamov and solovyev pred ator frishman and argos zpred zvelebil et al and mulpred barton the results from the jpred server is returned as a text file in an e mail message a link is also provided to view the colored graphical representation in html or postscript file format the consensus prediction from the jpred server has an accuracy of 9 in the three structural states specialized structures or features just as the position of a helices and sheets can be predicted with a relatively high degree of confidence the presence of certain specialized structures or features such as coiled coils and transmembrane regions can be predicted there are not as many methods for making such predictions as there are for secondary structures primarily because the rules of folding that induce these structures are not completely under stood despite this when query sequences are searched against databases of known structures the accuracy of prediction can be quite high coiled coils the coils algorithm runs a query sequence against a database of proteins known to have a coiled coil structure lupas et al the program also compares query sequences to a pdb subset containing globular sequences and on the basis of the differences in scoring between the pdb subset and the coiled coils database deter mines the probability with which the input sequence can form a coiled coil coils can be downloaded for use with vax vms or may more easily be used through a simple web interface the program takes sequence data in gcg or fasta format one or more se quences can be submitted at once in addition to the sequences users may select one of two scoring matrices mtk based on the sequences of myosin tropomyosin and keratin or mtidk based on myosin tropomyosin intermediate filaments types i v desmosomal proteins and kinesins the authors cite a trade off between the scor ing matrices with mtk being better for detecting two stranded structures and mtidk being better for all other cases users may invoke an option that gives the same weight to the residues at the a and d positions of each coil normally hydro phobic as that given to the residues at the b c e f and g positions normally hydrophilic if the results of running coils both weighted and unweighted are substantially different it is likely that a false positive has been found the authors caution that coils is designed to detect solvent exposed left handed coiled coils and that buried or right handed coiled coils may not be detected when a query is submitted to the web server a prediction graph showing the propensity toward the formation of a coiled coil along the length of the sequence is generated a slightly easier to interpret output comes from macstripe a macintosh based application that uses the lupas coils method to make its predictions knight macstripe takes an input file in fasta pir and other common file formats and like coils produces a plot file containing a histogram of the probability of forming a coiled coil along with bars showing the continuity of the heptad repeat pattern the following portion of the statistics file generated by macstripe uses the complete sequence of as an example 89l5a 90d5b 91d5c 93 760448 000047 94 760448 000047 95e5g 760448 000047 760448 000047 97f5b 760448 000047 0 0 99 0 0 100 0 0 101 0 0 102t5g 0 812161 0 000101 the columns from left to right represent the residue number shown twice the amino acid the heptad frame the position of the residue within the heptad a b c d e f g the lupas score and the lupas probability in this case from the fifth column we can easily discern a heptad repeat pattern examination of the results for the entire sequence shows that the heptad pattern is fairly well maintained but falls apart in certain areas the statistics should not be ignored however the results are easier to interpret if the heptad pattern information is clearly presented it is possible to get a similar type of output from coils but not through the coils web server instead a c based program must be installed on an appropriate unix machine a step that may be untenable for many users transmembrane regions the kyte doolittle tgrease algorithm discussed above is very useful in detecting regions of high hydrophobicity but as such it does not exclusively predict trans membrane regions because buried domains in soluble globular proteins can also be primarily hydrophobic we consider first a predictive method specifically for the prediction of transmembrane regions this method tmpred relies on a database of transmembrane proteins called tmbase hofmann and stoffel tmbase which is derived from swiss prot contains additional information on each sequence regarding the number of transmembrane domains they possess the location of these domains and the nature of the flanking sequences tmpred uses this information in conjunction with several weight matrices in making its predictions the tmpred web interface is very simple the sequence in one letter code is pasted into the query sequence box and the user can specify the minimum and maximum lengths of the hydrophobic part of the transmembrane helix to be used in the analysis the output has four sections a list of possible transmembrane helices a table of correspondences suggested models for transmembrane topology and a graphic representation of the same results when the sequence of the g protein coupled receptor served as the query the following models were generated 2 possible models considered only significant tm segments used strongly preferred model n terminus outside strong transmembrane helices total score from to length score orientation 1 55 o i 2 83 i o 3 120 o i 4 166 19 i o 5 212 o i 6 255 i o 299 o i alternative model strong transmembrane helices total score from to length score orientation 1 i o 2 84 o i 3 123 19 i o 4 166 20 o i 5 219 18 i o 6 252 o i 7 303 i o each of the proposed models indicates the starting and ending position of each segment along with the relative orientation inside to outside or outside to inside of each segment the authors appropriately caution that the models are based on the assumption that all transmembrane regions were found during the prediction these models then should be considered in light of the raw data also generated by this method phdtopology one of the most useful methods for predicting transmembrane helices is phdtopol ogy which is related to the predictprotein secondary structure prediction method described above here programs within the phd suite are now used in an obviously different way to make a prediction on a membrane bound rather than on a soluble protein the method has reported accuracies that are nearly perfect the accuracy of predicting a transmembrane helix is and the accuracy for a loop is giving an overall two state accuracy of 94 7 one of the features of this program is that in addition to predicting the putative transmembrane regions it indicates the orien tation of the loop regions with respect to the membrane as before phdtopology predictions can be made using either an e mail server or a web front end if an e mail server is used the format is identical to that shown for predictprotein above except that the line predict htm topology must pre cede the line beginning with the pound sign regardless of submission method results are returned by e mail an example of the output returned by phdtopology is shown in figure 5 signal peptides the center for biological sequence analysis at the technical university of denmark has developed signalp a powerful tool for the detection of signal peptides and their joe buzzcut national human genome research institute nih buzzcut nhgri nih gov predict htm topology pendrin maapggrseppqlpeyscsymvsrpvyselafqqqherrlqerktlreslakccscsrkrafgvlktlvpilewlpkyrv kewllsdvisgvstglvatlqgmayallaavpvgyglysaffpiltyfifgtsrhisvgpfpvvslmvgsvvlsmap 40 aa yslkydypldgnqelialglgnivcgvfrgfagstalsrsavqestggktqiagligaii phd htm hhhhhhhhhhhhhh hhhhhhhhhh rel htm detail prh htm prl htm phdthtm iiiiiiiiiiiiiiiiiiittttttttttttttttttoooooooooooooootttttttt figure 5 partial output from a phdtopology prediction the input sequence is pendrin which is responsible for pendred syndrome everett et al the row labeled aa shows a portion of the input sequence and the row labeled rel htm gives the reliability index of prediction at each position of the protein values range from 0 to 9 with 9 representing the maximum possible confidence for the assignment at that position the last line labeled phdthm contains one of three letters a t represents a transmembrane region whereas an i or o represents the orientation of the loop with respect to the membrane inside or outside cleavage sites nielsen et al the algorithm is neural network based using separate sets of gram negative prokaryotic gram positive prokaryotic and eukar yotic sequences with known signal sequences as the training sets signalp predicts secretory signal peptides and not those that are involved in intracellular signal transduction using the web interface the sequence of the human insulin like growth factor ib precursor somatomedin c whose cleavage site is known was submitted to signalp for analysis the eukaryotic training set was used in the prediction and the results of the analysis are as follows signalp predictions using networks trained on euk data igf ib length pos aacsy a 0 365 0 823 0 495 47 t 0 450 0 654 0 577 a 0 176 0 564 0 369 g 0 0 0 50 p 0 185 0 163 0 376 is the sequence a signal peptide measure position value cutoff conclusion max c 0 925 0 yes max y 0 855 0 yes max s 0 0 yes 0 550 0 yes most likely cleavage site between pos 48 and ata gp in the first part of the output the column labeled c is a raw cleavage site score the value of c is highest at the position c terminal to the cleavage site the column labeled s contains the signal peptide scores which are high at all positions before the cleavage site and very low after the cleavage site s is also low in the n termini of nonsecretory proteins finally the y column gives the combined cleavage site score a geometric average indicating when the c score is high and the point at which the s score shifts from high to low the end of the output file asks the question is the sequence a signal peptide on the basis of the statistics the most likely cleavage site is deduced on the basis of the swiss prot entry for this protein the mature chain begins at position the same position predicted to be the most likely cleavage site by signalp nonglobular regions the use of the program seg in the masking of low complexity segments prior to database searches was discussed in chapter 8 the same algorithm can also be used figure 6 predicted nonglobular regions for the protein product of the neurofibro matosis type 2 gene as deduced by seg the nonglobular regions are shown in the left hand column in lowercase numbers denote residue positions for each block to detect putative nonglobular regions of protein sequences by altering the trigger window length w the trigger complexity and extension complexity when the command seg sequence txt 3 4 3 is received seg will use a longer window length than the default of 12 thereby detecting long nonglobular domains an example of using seg to detect nonglobular regions is shown in figure 6 tertiary structure by far the most complex and technically demanding predictive method based on protein sequence data has to do with structure prediction the importance of being able to adequately and accurately predict structure based on sequence is rooted in the knowledge that whereas sequence may specify conformation the same confor mation may be specified by multiple sequences the ideas that structure is conserved to a much greater extent than sequence and that there is a limited number of back bone motifs chothia and lesk chothia indicate that similarities be tween proteins may not necessarily be detected through traditional sequence based methods only deducing the relationship between sequence and structure is at the root of the protein folding problem and current research on the problem has been the focus of several reviews bryant and altschul eisenhaber et al lemer et al the most robust of the structure prediction techniques is homology model build ing or threading bryant and lawrence fetrow and bryant jones and thornton the threading methods search for structures that have a similar fold without apparent sequence similarity this method takes a query sequence whose structure is not known and threads it through the coordinates of a target protein whose structure has been solved either by x ray crystallography or nmr imaging the sequence is moved position by position through the structure subject to some predetermined physical constraints for example the lengths of secondary structure elements and loop regions may be either fixed or varying within a given range for each placement of sequence against structure pairwise and hydrophobic interactions between nonlocal residues are determined these thermodynamic calculations are used to determine the most energetically favorable and conformationally stable align ment of the query sequence against the target structure programs such as this are computationally intensive requiring at a minimum a powerful unix workstation they also require knowledge of specialized computer languages the threading meth ods are useful when the sequence based structure prediction methods fail to identify a suitable template structure although techniques such as threading are obviously very powerful their current requirements in terms of both hardware and expertise may prove to be obstacles to most biologists in an attempt to lower the height of the barrier easy to use programs have been developed to give the average biologist a good first approximation for comparative protein modeling numerous commercial protein structure analysis tools such as what if and look provide advanced capabilities but this discus sion is limited to web based freeware the use of swiss model a program that performs automated sequence struc ture comparisons peitsch is a two step process the first approach mode is used to determine whether a sequence can be modeled at all when a sequence is submitted swiss model compares it with the crystallographic database expdb and modeling is attempted only if there is a homolog in expdb to the query protein the template structures are selected if there is at least sequence identity in a region more than 20 residues long if the first approach finds one or more appropriate entries in expdb atomic models are built and energy minimization is performed to generate the best model the atomic coordinates for the model as well as the struc tural alignments are returned as an e mail message those results can be resubmitted to swiss model using its optimize mode which allows for alteration of the proposed structure based on other knowledge such as biochemical information an example of the output from swiss model is shown in figure 7 another automated protein fold recognition method developed at ucla in corporates predicted secondary structural information on the probe sequence in ad dition to sequence based matches to assign a probable protein fold to the query sequence in this method correct assignment of the fold depends on the ranked scores generated for the probe sequence based on its compatibility with each of the struc tures in a library of target three dimensional structures the inclusion of the predicted secondary structure in the analysis improves fold assignment by about the input for this method is a single protein sequence submitted through a web front end a web page containing the results is returned to the user and the results are physically stored on the ucla server for future reference the second approach compares structures with structures in the same light as the vector alignment search tool vast discussed in chapter 5 does the dali algorithm looks for similar contact patterns between two proteins performs an op timization and returns the best set of structure alignment solutions for those proteins holm and sander the method is flexible in that gaps may be of any length course syllabus cmpt bioinformatics and computational biology catalogue description provides an in depth algorithms based introduction to major concepts and techniques in bioinformatics topics include algorithms for structure prediction and similarity sequence similarity and alignment metabolic and regulatory pathways sequence assembly comparative genomics expression analysis database searching artificial life and biological computation prerequisite either a previous bioinformatics course or at least credit units of previous course work in each of computer science statistics and the life sciences should you not have these please ask the instructor class time and location tuesday thursday thorvaldson building spinks extension course objectives the course objectives include gaining comfort with interdisciplinary study computer science and mathematics applied to the life sciences learning to use online tools and databases learning small programming tasks as applied to bioinformatics and understanding bioinformatics algorithms the following topics indicate the tentative areas to be covered introduction to bioinformatics algorithms perl sequence alignment phylogenetic trees protein structure rna secondary structure microarrays mass spectrometry hidden markov models l systems information theory in natural computing student evaluation grading scheme there will be or assignments to be completed on an individual basis there will be a final take home exam whose date has yet to be scheduled there will be a project to be done individually the topic must be relevant to bioinformatics chosen by the student and approved by the instructor the project will consist of both an oral presentation and a written paper assignments class project final exam total textbook information there will be no required textbook for the class certain books which are good suggested reading would be understanding bioinformatics by zvelebil and baum published by garland science an introduction to bioinformatics algorithms by jones and pevzner published by mit press introduction to computational molecular biology by setubal and meidanis pws publishing call no algorithms on strings trees and sequences computer science and computational biology by gusfield cam bridge university press call no bioinformatics a practical guide to the analysis of genes and proteins by baxevanis and ouellette wiley interscience call no fundamental concepts of bioinformatics by d e krane and m l rayner benjamin cummings this book is also on reserve with call no policies missed examinations students who have missed an exam or assignment must contact their instructor as soon as possible arrangements to make up the exam may be arranged with the instructor missed exams throughout the year are left up to the discretion of the instructor if a student may make up the exam or write at a different time if a student knows prior to the exam that she he will not be able to attend they should let the instructor know before the exam final exams a student who is absent from a final examination through no fault of his or her own for medical or other valid reasons may apply to the college of arts and science dean office the appli cation must be made within three days of the missed examination along with supporting documentary evidence deferred exams are written during the february mid term break for term courses and in early june for term and full year courses incomplete course work and final grades when a student has not completed the required course work which includes any assignment or examination including the final examination by the time of submission of the final grades they may be granted an extension to permit completion of an assignment or granted a deferred examination in the case of absence from a final examination extensions for the completion of assignments must be approved by the department head or dean in non departmentalized colleges and may exceed thirty days only in unusual circumstances the student must apply to the instructor for such an extension and furnish satisfactory reasons for the deficiency deferred final examinations are granted as per college policy in the interim the instructor will submit a computed percentile grade for the course which factors in the incomplete course work as a zero along with a grade comment of inf incomplete failure if a failing grade in the case where the instructor has indicated in the cours e outline that failure to complete the required course work will result in failure in the course and the student has a computed passing percentile grade a final grade of will be submitted along with a grade comment of inf incomplete failure if an extension is granted and the required assignment is submitted within the allotted time or if a deferred examination is granted and written in the case of absence from the final examination the instructor will submit a revised computed final percentage grade the grade change will replace the previous grade and any grade comment of inf incomplete failure will be removed for provisions governing examinations and grading students are referred to the university council regulations on examinations subsection of the calendar university of saskatchewan calendar academic courses policy academic honesty the university of saskatchewan is committed to the highest standards of academic integrity and honesty students are expected to be familiar with these standards regarding academic honesty and to uphold the policies of the university in this respect students are particularly urged to familiarize themselves with the provisions of the student conduct appeals subsection of the university secretary website and avoid any behaviour that could potentially result in suspicions of cheating plagiarism misrepresentation of facts and or participation in an offence academic dishonesty is a serious offence and can result in suspension or expulsion from the university all students should read and be familiar with the regulations on academic student misconduct http www usask ca secretariat student conduct appeals studentacademicmisconduct pdf as well as the standard of student conduct in non academic matters and procedures for resolution of com plaints and appeals http www usask ca secretariat student conduct appeals studentnon academicmisconduct pdf academic honesty is also defined and described in the department of computer science statement on academic honesty http www cs usask ca undergrad honesty php for more information on what academic integrity means for students see the student conduct appeals subsection of the university secretary website at http www usask ca secretariat student conduct appeals forms integritydefined pdf examinations with disability services for students dss students who have disabilities learning medical physical or mental health are strongly encouraged to regis ter with disability services for students dss if they have not already done so students who suspect they may have disabilities should contact dss for advice and referrals in order to access dss programs and supports students must follow dss policy and procedures for more information check http www students usask ca disability or contact dss at or dss usask ca students registered with dss may request alternative arrangements for mid term and final examinations students must arrange such accommodations through dss by the stated deadlines instructors shall provide the examinations for students who are being accommodated by the deadlines established by dss cognitive processes and mechanisms in language comprehension the structure building framework morton ann gernsbacher i the structure building framework language can be viewed as a specialized skill involving language specific processes and language specific mechanisms another position views the processing of language be it comprehension or production as drawing on many general cognitive processes and mechanisms such processes and mechanisms might also underlie nonlinguistic tasks as well this commonality might arise because as bates lieberman and others have suggested language comprehension evolved from other nonlinguistic cognitive skill or the commonality might arise simply because the mind is best understood by reference to a common architecture e g a connectionist architecture in recent work i have adopted the view that many of the processes and mechanisms involved in language comprehension are general cognitive processes and mechanism this article describes a few of those cognitive processes and mechanisms using a simple framework the structure building framework as a guide according to the structure building framework the goal of comprehension is to build a coherent mental representation or structure of the information being comprehended several component processes are involved first comprehenders lay foundations for their mental structures next comprehenders develop their mental structure by the psychology of learning and motiv at on vol copyright by academic prcss lnc all nghts of in any form reserved morton ann gernsbacher mapping on information when that incoming information is coherent or related to previous informatio n however if the incoming information is less coherent or related comprehenders employ a different process they shift and initiate a new substructure thus most representations comprise several branching substructures the building blocks of these mental structures are memory nodes memory nodes are activated by incoming stimuli initial activation forms the foundation of mental structures once a foundation is laid subsequent information is often mapped onto a developing structure because the more coherent the incoming information is with the previous information the more likely it is to activate the same or connected memory nodes in contrast the less coherent the incoming information is the less likely it is to activate the same or connected memory nodes in this case the incoming information might activate a different set of nodes and the activation of this other set of nodes might form the foundation for a new substructure in addition once memory nodes are activated they transmit processing signals to either enhance other nodes activation they boost or increase those nodes activation or suppress dampen or decrease other nodes activation in other words once memory nodes are activated two mechanisms control their level of activation suppression and enhancement presumably memory nodes are enhanced because the information they represent is necessary for further structure building they are suppressed when the information they represent is no longer as necessary this article discusses the three subproc esses involved in the structure building process laying a foundation mapping coherent or relevant information onto that foundation and shifting to initiate a new substructure this article also discusses the two mechanisms that control the structure building processes enhancement which increases activation and suppression which dampens it when discussing these processes and mechanisms i begin by describing the empirical vidence to support them then i describe some exemplary phenomena for which these processes and mechanisms account let me stress that i assume that these processes and mechanisms are general i e the same processes and mechanisms should be involved in nonlinguistic phenomena this orientation suggests that some of the reasons that individuals differ in comprehension skill might not be specific to language toward the end of this article i describe research investigating this suggestion but first i describe the processes and mechanisms involved in structure building beginning with the process i refer to as laying a foundation processes and mechanisms in comprehension ii the process of laying a foundation according to the structure building framework the initial stage of comprehension involves laying a foundation for the mental representation or structure laying this foundation should require additional processing what manifestations might we see of this additional processing one possibility is increased comprehension time and indeed a large body of converging data suggest that comprehension slows down when comprehenders are laying their mental foundations for these mental structures for instance experiments measuring the reading time for each sentence in a paragraph show that initial sentences take longer to read than subsequent sentences see citations in gernsbacher in fact this is the case regardless of where the paragraph topic sentence occurs greeno noreen kieras in addition the first sentence of each miniepisode in a story takes longer to read than other sentences in that miniepisode haberlandt haberlandt berian sandson mandler goodman similarly experiments measuring the reading time for each word within a sentence show that initial words take longer to read than subsequent words aaronson ferres chang in fact the same word is read more slowly when it occurs at the beginning of a sentence or phrase than when it occurs later aaronson scarborough the same comprehension time effects are observed when comprehenders self pace their viewing of nonverbal picture stories comprehenders spend more time viewing the initial picture of each story and the initial picture of each subepisode gernsbacher when comprehending spoken language subjects are slower to identify a target phoneme or a target word when that target occurs during the beginning of its sentence or phrase than when it occurs later see citations in gernsbacher so both the comprehension time and the target identi some regression analyses of sentence by sentence reading times do not show a simple serial position effe ct e g graesser hoffman clark perhaps this is because the stimulus sentences vary in length and length is also a substantial predictor of reading time indeed when the same sentences are read word by word and the regression analyzes average word by word reading times per sentence and therefore equates sentence length these analyses also show that initial sentences take longer to read haberland graesser this effect is not manifested when subjects are required to memorize as opposed to comprehend the stimulus sentences neither is the effe ct manifested when subjects are required to perform a second task e g answer a question or press a key to signal an anomaly immediately after they finish reading ea h sentence in preparation of this second task subjects often delay their reading of the last words of the sentences morton ann gernsbacher fication data display the pattern one expects if comprehenders use initial words and sentences to lay foundations for their mental representations of larger units such as phrases sentences story episodes and paragraphs but rather importantly this pattern is not displayed when stimuli do not lend themselves to coherent mental representations e g when the sentences or paragraphs are self embedded or extensively right branching as in which is a self embedded version of foss lynch greeno noreen hakes foss kieras i grants manuscripts graduate students committees articles data experiments classes the professor taught conducted collected published served on trained reviewed and submitted the professor taught classes conducted experiments collected data published articles served on committees trained graduate students reviewed manuscripts and submitted grants memory data also support the proposal that a general cognitive process involved in comprehension is first laying a foundation for instance sentences are recalled better when they are cued by their first content words or by pictures of those first content words than when they are cued by later occurring words bock irwin prentice turner rommetveit similarly story episodes are recalled better when they are cued by their first sentences than when they are cued by later occurring sentences mandler goodman these data suggest that initial stimuli serve as a foundation onto which subsequent information is added indeed initial information plays such a fundamtneal role in organizing mental structures that when comprehenders are asked to recall the main idea of a paragraph they are most likely to select the initial sentenceeven when the actual theme is captured by a later occurring sentence kieras this phenomenon also suggests that the initial process of comprehension involves laying a foundation a the advantage of first mention another phenomenon that could be the result of the process of laying a foundation is what i refer to as the advantage of first mention the advantage is this after comprehending a sentence involving two participants it is easier to remember the participant mentioned first than the participant mentioned second for example after reading the sentence tina beat lisa in the state tennis match if subjects are asked whether the name tina occurred in the sentence they processes and mechanisms in comprehension respond considerably faster if tina was the first person mentioned in the sentence as she was in than if tina was the second person mentioned in the sentence as she is in lisa beat tina in the state tennis match the first mentioned participant is more accessible from comprehenders mental representations which is what i mean by the advantage of first mention the advantage of first mention has been observed numerous times by several researchers chang corbett chang gernsbacher stevenson von eckardt potter as a point of interest when corbett and chang observed this advantage they included filler trials in which they measured the accessibility of concepts that were words other than participants names so the advantage does not depend on some strategy that subjects might employ when they think that they only have to remember the names of sentence participants one explanation of the advantage of first mention draws on the proposal that comprehension involves laying a foundation for this reason firstmentioned participants are more accessible both because they form the foundations for their sentence level representations and because it is through them that subsequent information is mapped onto the developing representation however there are other explanations of the advantage of first mention and these other explanations draw on the linguistic structure of english for example first mentioned participants might be more accessible because in english declarative sentences they are virtually always the syntactic relation known as subject and they typically also fill the semantic role known as agent in a series of experiments gernsbacher hargreaves we tried to untangle these linguistic factors from the advantage of first mention in our first experiment we discovered that the advantage of first mention was not attributable to semantic agency that is the participant tina was just as accessible when she was the agent of the action as in as when tina was the recipient of the action or the semantic patient as she is in tina beat lisa in the state tennis match tina was beaten by lisa in the state tennis match the crucial factor affecting accessibility was whether the participants were mentioned first as tina is in and participants were less accessible when they were mentioned second as tina is in and lisa beat tina in the state tennis match morton ann gernsbacher lisa beaten by tina in the state tennis match these results are presented in the leftmost panel of fig before prematurely accepting the null hypothesis we conducted a replication experiment with an increased subject sample size of the results of the replication experiment were identical to those of the first experiment in our third experiment and its replication we investigated whether the advantage of first mention depended on the first mentioned participants being literally the initial words of our stimulus sentences if so our laboratory task might be somewhat to blame as the first word of each sentence was preceded by an attention getting warning signal which was itself preceded by a brief blank period to investigate this we manipulated whether an adverbial phrase like two weeks ago was preposed at the beginning of the sentence as in two weeks ago tina mailed lisa a box full of clothes or it was post posed at the end of the sentence as in tina mailed lisa a box full of clothes two weeks ago or it did not occur at all as in tina mailed lisa a box full of clothes we discovered that the advantage of first mention remained regardless of whether the first mentioned part icipants were literally the initial words of their stimulus sentences see the center panel of fig thus the advantage must depend on each participant position relative to the other participants in our fifth sixth and seventh experiments we investigated whether the advantage of first mention was due to the first mentioned participants being syntactic subjects this of course is the typical sequence of events in a language like english that is considered an svo subject verb object language greenberg however in our fifth experiment the advantage of first mention was not attenuated when the two participant were both subjects e g when both tina and lisa were the syntactic subjects as in as opposed to tina being the sole subject as in tina and lisa argued during the meeting tina argued with lisa during the meeting see the rightmost panel of fig in fact in our sixth and seventh experiments the advantage of first mention was not attenuated even when the first mentioned participant was no longer its sentence syntactic subject as tina is in g i t rf rt agents processes and mechanisms in comprehension rt patients m rl a padverbial phr m 223 conjoined single subjects subjects fig i results fr om gernsbacher and hargreaves experiments and the data dtsplayed are the subjects mean verification latencies to first vs second mentioned sentence parttctpants when the sentence participants were agents vs patients leftmost panel when the sentences had preposed adverbial phrases postposed adverbial phrases or no adverbml phrases center panel and when the sentence participants were conjoined subjects vs stngle subjects rightmost panel because of tina lisa was evicted from the apartment we conc lude d that the advantage of first mention does not arise from n of the hngmsttc factors that we investigated instead we suggested that tt ts a result of general cognitive processes that occur naturally during compreh enston these mvolve laying a foundation and mapping subsequent mformatton onto that foundation b the advantage of first mention vs the advantage of clause recency the advantage of first mention seems to contradict another well known advan tage what i shall call the advantage of clause recency the advantage of clause recency occurs immediately after subjects hear or read a two clause sentence words from the most recently read or heard clause are often m o e accessible than vords from an earlier clause for instance th word oil is more accesstble immediately after subjects hear than it ts tmmedtately after they hear caplan now that artists are working fewer hours oil prints are rare morton ann gernsbacher now that artists are working in oil prints are rare presumably this advantage arises because the word oil was in the most recent clause in so the advantage of clause recency is also an advantage for the order of mentioning concepts but the advantage is for the most recently or second mentioned concept see also chang kornfeld von eckardt potter in a series of experiments gernsbacher hargreaves beeman we resolved this discrepancy and discovered something about how comprehenders build mental representations of clauses in these experiments we measured the accessibility of sentence participants in two clause sentences e g tina gathered the kindling and lisa set up the tent the first mentioned participants were the syntactic subjects of the first clauses and the second mentioned participants were the syntactic subjects of the second clauses we began with the proposal that comprehenders represent each clause of a multiclause sentence in its own substructure so comprehending would require first building a substructure to represent the clause tina gathered the kindling and then building a substructure to represent the clause lisa set up the tent we also predicted that comprehenders have greatest access to the information represented in the substructure that they are currently developing we tested this prediction in our first expenment our goal was to measure accessibility of the sentence participants at the point where comprehenders were just finishing building their representation of the second clause we thought that if we could capture that point we would find an advantage of clause recency in other words we expected to observe an advantage of the second mentioned participant to capture that point we presented the test names coincident with the last words in the sentences but we presented those test names at a different place on the computer screen than where we presented the sentences we assumed that by the time our subjects shifted their eyes and their attention posner to the test names our coincident presentation was comparable to an extremely short delay and indeed at this point we observed a second as opposed to first mentioned participant advantage in other words we observed an advantage of clause recency similar in magnitude to those advantages observed by caplan and others our data are displayed in the two leftmost bars of fig these data suggest that comprehenders do have greatest access to information represented in the substructure that they are currently developing after comprehenders represent each clause we assume that they must e e c u a processes and mechanisms in comprehension first mentioned participant second mentioned participant coincident with ms after last word sentence ms after ms after sentence sentence fig results from gernsbacher hargreaves and beeman experiments i and the data displayed are the subjects mean verification latencies to first vs secondmentioned sentence participants when the first mentioned participants were the subjects of the first clauses of two clause sentences and the second mentioned participants were the subjects of the second clauses map their second clause representation onto their first clause representation in other words to fully represent a two clause sentence comprehenders must incorporate the two substructures into one our goal in our second experiment was to catch comprehenders after they had built their representations of each clause but before they had mapped their representation of the second clause onto their representation of the first clause we predicted that at that point information would be equally accessible from each clause and indeed the first mentioned and second mentioned participants were equally accessible see fig we observed the same effect in a replication experiment in our fourth experiment we predicted that if we measured accessibility a little bit later say a second later we would find that by this point the first mentioned participants would be more accessible this would suggest that comprehenders had successfully mapped the two clauses together and that the first clause was serving as a foundation for the second and indeed by this time the first mentioned participants were more accessible see fig in fact the advantage of first mention was identical in magnitude to the advantage we observed with simple sentences to review our results at our earliest test point we observed that the second mentioned participants were more accessible or in other words we observed an advantage of clause recency i suggest that at this point morton ann gernsbacher comprehenders were still developing their representations of the second clauses when we meassured accessibility msec later the two participants were equally accessible i suggest that at this point comprehenders had built their representations of both clauses but had not begun mapping those representations together when we measured accessibility after msec we observed an advantage of first mention i suggest that at this point comprehenders had finished mapping the second clause onto the first and the information from the first clause was more accessible because it served as the foundation for the whole sentence level representation each of these results is displayed in fig an alternative explanation is that the change in accessibility that we observed over time was due to catching subjects at different stages while they were cyclically rehearsing the two participants names e g tina l isa t ina l isa to rule out this explanation we conducted one final experiment in which we delayed the test point even longer for a total of msec at that point the first mentioned participants were still more accessible in fact at that point the first mentioned participants were even more accessible than they had been at the msec test point see two rightmost bars in fig this finding suggests that the advantage of first mention is a relatively long lived characteristic of the representation of a sentence i suggest that this advantage arises because first mentioned participants form the foundations for their sentence level representations and it is through them that subsequent information is mapped onto the developing representation i n contrast the advantage of clause recency appears to be relatively short lived i suggest that the advantage of clause recency arises because comprehenders build a substructure to represent each clause of a multiclause sentence and they have greatest access to information represented in the substructure that they are currently developing thus two seemingly contradictory phenomena are not mutually exclusive when comprehension is viewed as structure building in fact according to the structure building framework we should be able to observe both phenomena simultaneously that was the goal in our sixth experiment in this sixth experiment we measured the accessibility of each of four participants e g the four participants mentioned in dave and rick gathered the kindling and john and bill set up the tent as in two participants e g dave and rick were the conjoined subjects of the first clause and two participants e g john and bill were the conjoined subjects of the second clause in other words two participants were the first and second mentioned participants of the first clause processes and mechanisms in comprehension and two participants were the first and second mentioned participants of the second clause we predicted that in both clauses we would observe an advantage of first mention within each clause first mentioned participants would be more accessible than second mentioned participants in addition we predicted that if we could catch comprehenders at the point where they were completing their representations of the second clause we would observe an advantage of clause recency both participants from the second clause would be more accessible than both participants from the first clause and indeed that is what we found as shown in fig in both clauses the first mentioned participants were significantly more accessible than the second mentioned participants in other words we observed an advantage of first mention as also illustrated in fig participants from the second clause were significantly more accessible than participants from the first clause in other words when we tested accessibility msec after the end of each sentence we also observed an advantage of clause recency in a final experiment when we delayed the test point to msec after each sentence we no longer observed an advantage of clause recency only an advantage of first mention c ticis q a test name ms after sentence first clause first mentioned participant second mentioned second clause fig results from gernsbacher hargreaves and beeman experiment the data displayed are the subjects mean verification latencies to first vs second mentioned sentence participants in the first clause of two clause sentences and first vs secondmentioned sentence participants in the second clause of two clause sentences morton ann gernsbacher c fu nctional role of first mention given the privileged role that initial information plays in c omprehenders mental representations speakers and writers should senously confront what levelt dubbed the linearization problem what to say first what to say next and so on p indeed functional grammarians argue that different orders of mention code diff rent pragmatic dimensions therefore speakers and writers selection of a specific order serves a communicative functio n chafe firbas halliday however optmons dtffer over which dimension initial mention codes and which function speakers and writers intend to accomplish when they select among the grammatical forms that involve diffe rent orders of mention according to one perspective initial mention codes importance and functions to attract attention according to another perspective first mention co es givenness and functions to create a context for subsequent comprehension clark clark both perspectives are supported by experiments employing a range of laboratory tasks designed to simulate sentence production these tasks include elicited sentence formulation oral sentence recall sentence acceptability sentence ratings and sentence verification of pictures for example experiments that have manipu lated importance via perc ptual salience animacy definiteness or other markers have shown that tmport nt concepts are mentioned first similarly experiments that have m mpulated givenness via explicit prior mention verbatim or ptctonal cuemg or implicit presupposition have shown that given concepts are mentioned first see citations in gernsbacher however one cannot adopt the two perspectives simultaneously without entering into a paradox that i initial mention can only code importance and stmultaneously tf one assumes that new information is always less important or that important information is always old both assumptions seem unintuitive thus the two persp ectives conflict bock discusses a few resolutions to this conflict from the perspective of sentence production in gernsbacher and hargreaves in press we did not attempt to resolve this conflict for sentence comprehension but we did point out how the structure building account accomodates both functions if first mention is selected in order to signal importance then the function is accomplished because by virtue of being first mentionedinitial information gets represented at the core or fou ndation of the structure as mentioned above this privileged position leads to greater accessibility and presumably the goal of marking information as important is to gain this greater accessibility processes and mechanisms in comprehension on the other hand if first mention is selected in order to signal givenness then the function is also accomplished because by virtue of being first mention ed initial information organizes the representation of subsequent information that is subsequent information gets mapped onto the developing structure vis a vis the initial information presumably the mapping process proceeds more smoothly when new subsequent information is mapped onto given initial information rather than the other way around thus functional linguists suggest that speakers and writers exploit different gramm atical forms such as passivization or left dislocation to accomplish certain communicative functions such as attracting attention or signaling givenness i suggest that the cognitive processes involved in laying a foundation for mental structures accomplish these fu nctions iii processes of mapping and shifting according to the structure building framework once a foundation is laid incoming information that is coherent with the previous information is mapped onto the developing structure or substructure presumably the more coherent relevant related or similar the incoming information is the easier the mapping process should be how would ease in mapping be man ifested again one candidate is comprehension time and again data from reading time experiments support this assumption sentences that literally or conceptu ally repeat a previous word or a phrase and thereby signal coherence overtly are read faster than comparable sentences that are not literally or conceptu ally repetitive see citations in gernsbacher for example comprehenders more rapidly read the sentence the beer was warm after they read than after they read a we got some beer out of the trunk the beer was warm b we checked the picnic supplies the beer was warm the benefit does not derive solely from literally repeating the word beer as a sentence that simply mentions beer such as does not fa cilitate mapping too much more than the picnic supplies sentence haviland clark see also johnson laird p andrew was especially fond of beer the beer was warm in addition the assumption that coherent information is represented in the same mental substructure is supported by memory data sentences and phrases that are coreferenced by repetition are more likely to be morton ann gernsbacher bered when one phrase cues or primes the recall or recognition of the other such phrases are also more likely to be clu stered in comprehenders recall protocols hayes roth thorndyke kmtsch kozminsky streby mckoon keenan mck o ratcliff on the other hand according to the structure butldmg fram ework when incoming infor mation is less coherent compreh enders employ the r cess of shifting they shift from actively building one substructure a d imt ate another laying the foundation for this new substructure reqmres additional processing again this additional processing should be ma ifested in increased comprehension time and again numerous readmg time experiments support this assumption sentences and w rds that change the ongoing topic point of view or setting take substant ially longer to comprehend than those that continue it see citations in gernsbacher consider the following example this example draws on the narrative point of view which is the narrator location in relation to the action black turner bower for instance locates the narrator inside the lunchroom the door to henry lunchroom opened and two men came in in contrast l ocates the narrator outside the lunchroom the door to henry lunchroom opened and two men went in after reading comprehenders presumably adopt the narrator point of view inside the living room bill was sitting in the living room reading the evening paper then they have difficulty reading a sentence that sw tches this po nt of view as does compared with a sentence that mamtams th e pomt of view as does before bill had finished the paper john went into the room before bill had finished the paper john came into the room comprehenders also have more difficu lty retrieving infor mation presen ed before a change in topic point of view or setting than they do retnevmg infor mation presented after such a change a anderson garrod sanford clements mandler goodman presumably t is is because comprehenders shift when they encounter a change m topic point of view or setting if so then the information that occurred efore the change in topic point of view or setting will be represente m one substructure while the information that occurred after the change m topic point of view or setting will be represented in another substructure processes and mechanisms in comprehension a s h i fting as t h e cause of comprehenders rapid inacc essibility to information the process of shifting from building one structure or substructure to initiating another also accounts for a well known language comprehension phenomenon shortly after hearing or reading a passage comprehenders quickly lose access to recently comprehended information gernsbacher in particular information typically considered surface information becomes less accessible but see von eckardt potter this phenomenon is well known partly because we experience it everyday and partly because it has been repeatedly demonstrated in the laboratory see citations in gernsbacher in gernsbacher i too demonstrated this phenomenon but my demonstration was made with passages composed of professionally drawn pictures these stories were told completely without words an example sequence is shown in fig while subjects comprehended these nonverbal stories i measured how well they could remember each picture original left vs right orientation as illustrated in fig two goals directed this research first i wanted to demonstrate that this phenomenon was not unique to language based comprehension this goal was met by my first four experiments the first experiment demonstrated that comprehenders had more difficulty accessing recently comprehended information after they comprehended all four picture stories than after they comprehended each of the fou r picture stories the second experiment demonstrated that comprehenders had more difficulty accessing recently comprehended information after they comprehended an entire picture story than after they comprehended each half of that story so these first two experiments replicated the phenomenon in which comprehenders rapidly lose access to previously comprehended information but in these experiments the phenomenon was observed during the comprehension of nonverbal stimuli the data from these two experiments are summarized in table i the third and fourth experiments replicated a more intriguing aspect of the phenomenon several l anguage experiments have demonstrated that apart from the passage of time or the comprehension of more information the structure of the passage greatly affects the time course of accessibility more specifically information becomes markedly less accessible just after comprehension crosses a constituent boundary e g j ust after comprehenders finish a clause a phrase a sentence a paragraph or a miniepisode see citations in gernsbacher the third experiment demonstrated that comprehenders could segment the picture stories into their constituents or subepisodes the fourth ex u c e x i processes and mechanisms in comprehension periment demonstrated that recently comprehended information was less accessible after crossing these constituents boundaries than before even though the test interval in terms of the number of stimuli and the amount of time was the same in the after boundary vs the before boundary conditions see table i the second goal of my research was to investigate why this phenomenon occurs four explanations were considered the first was the linguistics hypothesis information becomes less accessible because sentence comprehension requires syntatic detransformation though detransformation provides syntactic tags that can be used to reconstruct the original sentence the tags are often lost mehler miller sachs one major problem with this explanation is that it requires a set of syntactic rules specifying the necessary transformations used during comprehension in other words it requires a psychologically real transformational grammar specifying such a grammar for english sentences has proved to be no easy task bresnan kaplan garnham and though there have been novel attempts to specify grammars for nonverbal media for example carroll attempted a grammar for cinematic films and bernstein attempted one for musical symphonies the possibility of specifying a grammar to describe my picture stories seemed remote another problem with the linguistic hypothesis was that over two decades of experiments using verbal stimuli alone this explanation has teadily lost support fodor bever garrett garnham gough diehl so i abandoned the linguistics hypothesis and table i s u bjects mean percentage correct a n d discri m i nation a scores i n gernsbacher experiment manipulation correct a after comprehending one vs several picture stories after comprehending half vs an entire picture story before a constituent boundary vs after a constituent boundary after comprehending a normal vs a scrambled picture story after comprehending a normal vs a scrambled written story morton ann gernsbacher searched for an explanation outside the language domain this approach is not unusual when other phenomena originally believed to be unique to language processing were demonstrated outside that domain e g categorical perception and selective adap tation amodal explanations were sought for them too diehl the second explanation i considered was the memory limitations hypothesis whereby recently comprehended information becomes less accessible becau se the limitations of a short term memory are exceeded these limitations might be quantitative short term memory can hold only a limited number of items or they might be temporal short term memory can hold information for only a limited period of time miller however my fo urth experiment and other constituent boundary experiments illustrate an aspect of the phenomenon that memory limitations cannot explain these experiments demonstrate that apart from the amount of information or the passage of time the structure of the information affects its acces sibility that is acces sing recently comprehended information does not depend completely on how much information has been held or how long that inform ation has been held in a hypothetical short term memory to account for such findings a corollary assumption is often made recently comprehended information is held in short term memory until a meaningful unit has been comprehended then it is lost j arvella sanford garrod however this assumption undermines the orig inal explanation all constituents are not the same size so they would not consume the same amount of space or be held for the same period of time if while waiting for a constituent to end short term memory can hold a variable amount of inform ation for a variable period of time then why is the information ever lost perhaps the system is so smart that when anticipating a time or space limitation it chooses to expunge at a structurally appropriate interval but this leaves us without an a priori specification of how long or how much information can be held and no causal link therefore i also considered the memory limitations hypothesis insufficient the third explanation was the recoding hypothesis whereby recently comprehended information becomes less accessible becau se during comprehension it is recoded into a more meaningful representation usually referred to as gist so even though initially all verbatim information is vital for successful comprehen sion the more successful the comprehension the more likely it is that verbatim information becomes recoded into gist bransford franks consider the analogy of baking a cake as the cake bakes several raw ingredients salt flour butter sugar become increasingly recoded in processes and mechanisms in comprehension act i the b king process is successful it is difficult to extract any of the mgredtents m their original raw forms now consider bransford and franks seminal experiment subjects comprehended a series of thematically cohesive sentences and on a later recognition test they were poor at remembering structural information about sentence boundaries less well nown is a later experiment by peterson and mcintyre in one condttjon they perfectly replicated bransford and franks in a second condition their input sentences were not thematically cohesive and for these sentences comprehenders were considerably better at remembenng sentence boundaries one explanation is that in bransford and fra ks paradigm the input sentences could easily be recoded into gist but m peterson and mcintyre unrelated second condition they could not so they had to remain in their relatively raw form other data converge on this explanation for instance comprehenders memory for the original active vs passive voice of a sentence is significantly worse when the input sentences form a cohesive story than when the sentences are semantically unrelated j r anderson bower p com r henders make more synonym substitutions when recalling sentences ongmally processed as a thematic story than when the sentences e unrelated de villiers luftig pompi lachman stmtlarly btltnguals memory for the language in which ditierent words were originally spoken is worse when the words compose a unified sentence rather than an unrelated list saegert hamayan ahmar see also rose rose king perez in each f these situations recoding the input into a more meaningful representation apparently caused some of its information to become jess ac however the situations that best support the recoding hypothesis least represent typical comprehension in these situations the to becomprehended stimuli were semantically unrelated and void of thematic integrity or at least it appeared that way to subjects it is difficult to draw conclusions about comprehension from situations where comprehenswn m the usual sense cannot actually occur for comparable arguments see moeser perfetti goldman a more valid test f the recoding hypothesis would involve two experimental condttwns tn both comprehension could occur but recoding woul be les likely in one than the other that was one purpose of the fifth expenment tn gernsbacher a second purpose was to test another explanation the shifting hypothesis this fourth explanat ion was derived from the structure building framework according to the shifting hypothesis recen tly comprehended information becomes less accessible because comprehenders shift from developing one substructure to develop another presumably information morton ann gernsbacher represented in one substructure is most available during the active processing of that substructure once a processing shift has occurred information represented in the previous substructure becomes less available in my fifth experiment gernsbacher half the stories were presented with their pictures in their normal chronological order and half were presented with their pictures in a scrambled order this scrambling manipulation served three purposes first it provided a more valid test of the recoding hypothesis because unlike lists of isolated or seemingly unrelated sentences stories composed of scrambled stimuli possess a theme with appropriate instructions subjects attempt to obtain the gist of scrambled stories and meet with some success though much less than with normal ones see citations in gernsbacher second the scrambling manipulation provided an empirical test of the shifting hypothesis because stimuli presented in a scrambled order are by definition relatively less coherent therefore building a mental structure of a scrambled story should induce more shifting third the scrambling manipulation pit the two hypotheses against one another because the predictions derived from each were in opposition according to the recoding hypothesis recently comprehended information becomes less available because it gets recoded into gist therefore the lower the probability of recoding the more accessible the information should be because comprehending scrambled stories leads to a lower probability of recoding the prediction derived from the recoding hypothesis was that recently comprehended information would be more accessible in the scrambled than the normal condition but according to the shifting hypothesis recently comprehended information becomes less accessible because of shifting from building one substructure to developing another the higher the probability of shifting the less accessible the information should be because comprehending scrambled stories leads to a higher probability of shifting the prediction derived from the shifting hypothesis was that recently comprehended information would be less accessible in the scrambled than the normal condition the results of this fifth experiment using picture stories were clearly those predicted by the shifting hypothesis i e information was less accessible in the scrambled than the normal condition see table i these results were replicated in a sixth experiment using the more traditional stimuli namely written stories see table i thus the cognitive process of shifting appears to be an adequate amodal explanation of why comprehenders rapidly lose access to recently comprehended information processes and mechanisms in comprehension b lingu istic cues for shifting how do comprehenders know when to shift and initiate a new substructure presumably speakers and writers and even picture story authorssignal their readers and listeners via various devices for instance when producing sentences speakers and writers use certain devices to signal that they are beginning a new clause or phrase bever clark clark fodor et al frazier fodor kaplan kimball wanner maratsos indeed one of kimball seven parsing principles was that the construction of a new node is signalled by the occurrence of a grammatical function word p thus comprehenders might as clark and clark suggested use signals such as determiners a an the and quantifiers some all six etc to initiate a new substructure representing a new noun phrase similarly they might use signals such as subordinating conjunctions because iv hen since etc to initiate a new substructure representing a new clause clark clark p at the level of passages speakers and writers use other devices to signal an upcoming change e g a change in topic point of view or setting carpenter just halliday one relatively subtle linguistic device is what i have referred to as an adverbial lead gernsbacher this involves simply placing an adverb like then or next at the beginning of a sentence in several experiments we have found that adverbial leads stimulated behavioral responses indicative of processing shifts in many of these experiments subjects read seven sentence passages that began like the lifeguard was watching the children swim he noticed one child was struggling he thought the child might be droh ning then either the fourth or fifth sentence began with an adverb like then or next as in or next he jumped into the water he began to administer cpr he jumped quickly into the water next he administered cpr in one experiment i measured sentence reading times and found that adverbial leads slowed comprehension gernsbacher experiment this suggests that adverbial leads trigger comprehenders to begin laying a foundation for a new substructure in a second experiment i measured question answering latencies and found that comprehenders had more difficulty accessing information presented before an adverbial lead than information presented afterward gernsbacher experi238 morton ann gernsbacher ment this suggests that the information occurring after an adverbial lead is represented in a different mental substructure in another experiment wisegarver used the priming in itemrecognition task pioneered by mckoon ratcliff 980b in this task ubjects first read a passage and then attempt to recognize whether each of a short list of words occurred in that passage wisegarver found that a word from one sentence of a passage was a worse prime for a word from another sentence when an adverbial lead intervened between the two finally using different passages from the one illustrated above beeman and i found that comprehenders ability to draw inferences between two facts was severely disrupted when one of those facts was presented prior to an adverbial lead and the other was presented after the adverbial lead in sum adverbial leads appear to stimulate behavior indicative of processing shifts perhaps speakers and writers use them to signal their readers or listeners of an upcoming change iv mechanisms of suppression and enhancement according to the structure building framework the building blocks of mental structures are memory nodes presumably memory nodes are activated by incoming stimuli once activated they transmit processing signals that either suppress decrease or dampen or enhance increase or boost the activation of other memory nodes in other words the activation of memory nodes is controlled by the mechanisms of suppression and enhancement suppression and enhancement might be responsible for many linguistic as well as nonlinguistic phenomena a role of suppression in fin e tuning the meanings of words the mechanism of suppression appears to control a phenomenon i refer to as fine tuning the activation of lexical concepts e g fine tuning the appropriate meaning of an ambiguous word the reason why such a finetuning process is needed is that contrary to intuition immediately after comprehenders hear or read an ambiguous word such as bug multiple meanings are activated in fact multiple meanings are activated even when a particular meaning is specified by the preceding semantic context as in spiders roaches and other bugs or the preceding syntactic context as in i like the watch vs i like to watch see citations in gernsbacher processes and mechanisms in comprehension cognitive psychologists usually attribute this multiple activation to some form of automatic or semiautomatic activation burgess simpson simpson simpson lorsbach computer models usually simulate the pattern by allowing all meanings of an ambiguous word to receive facilitation prior to getting any input from a semantic or syntactic processor charniak however behaviorally the phenomenon becomes more complex very shortly after the multiple meanings are simultaneously activated as intuition suggests only one meaning is available to consciousness after a period as brief as msec so the question arises what happens to the inappropriate meanings some have suggested that inappropriate meanings become less accessible through a mechanism that i have dubbed mutual inhibition their suggestion is that the appropriate meanings growth in activation causes the inappropriate meanings decline in activation as in a seesaw effect mcclelland kawamoto waltz pollack unfortunately the behavioral data do not demonstrate this compensatory pattern another explanation is that the inappropriate meanings simply decay r anderson however we tested this decay explanation and found that in its purest sense it cannot explain all the data gernsbacher faust this experiment examined the activation of multiple meanings of an ambiguous word like quack in one condition the ambiguous words were biased by a previous semantic context for example subjects read either or pam was diagnosed by a quack pam heard a sound like a quack in this condition the typical multiple activation phenomenon was observed immediately after the subjects read the ambiguous words both meanings were activated but within about msec the inappropriate meanings were no longer activated in a second condition the ambiguous words were left ambiguous as in pam was annoyed by the quack in this condition both meanings remained activated at msec in fact they were both activated at msec see also hudson tanenhaus if the decreased activation of an inappropriate meaning is due t decay then surely one or both of the meanings should have decayed in this a third explanation is that this phenomenon is attributable to backward priming glucksberg kreuz rho van petten kutas but see burgess tanenhaus seidenberg morton ann gernsbacher neutral condition instead i suggest that both meanings remained activated because neither was suppressed by the semantic context more recent pilot work suggests that the strength of the context affects the time course of the suppression mechanism i also suggest that the mechanism of suppression operates to finely tune the multiple associations of unambiguous words that is all concepts have multiple associations e g apple is associated with both pie and tree marshall cofer however in some contexts the association between apple and pie is more relevant as in in other contexts the association between apple and tree is more relevant as in james baked the apples james picked the apples at some point during comprehension associations must be finely tuned indeed a wealth of data demonstrate that more relevant associations provide better memory cues for instance pie would cue better whereas tree would cue better see citations in gernsbacher just like the m ultiple meanings of ambiguous words the multiple associations of unambiguous words are immediately activated gernsbacher faust however after a brief period only the more relevant association remains activated see also kintsch again i suggest that less relevant associations like the inappropriate meanings of ambiguous words are suppressed moreover a less efficient suppression mechanism while fine tuning the activation of lexical concepts appears to characterize less skilled comprehenders gernsbacher varner faust see also merrill sperber mccauley b role of suppression and enhancement in improving referential access another phenomenon that the mechanisms of suppression and enhancement appear to control is referential access via anaphora gernsbacher all languages have devices called anaphors that are used to refer to previously mentioned concepts called antecedents for example to refer to the antecedent john in one could use a variety of anaphors john went to the store one could use a repeated name such as john a synonymous noun phrase such as the guy a pronoun such as he or even a zero anaphor as in john went to the store and bought a quart of milk in the past few years understanding how language users negotiate processes and mechanisms in comprehension ana ph ora has been the focus of considerable psycholinguistic research see gernsbacher for a review why has ana ph ora captured so much attention for one reason anaphors are very common linguistic devices consider only pronoun anaphors in english they are some of the most frequently occurring lexical units for instance in kucera and francis samples of literary text pronouns accounted for nearly a third of the most common lexical types and over of their corpus of one million tokens one would assume that pronouns occur even more frequently in informal oral discourse but perhaps more important the process of understanding anaphors presents an extremely interesting case of lexical access maybe more than any other lexical unit the meaning of an ana ph or greatly depends on the context in which it occurs so how do comprehenders understand these ubiquitous but chameleon like lexical units in gernsbacher i suggested that the mechanisms of enhancement and suppression control referential access via anaphora recall that enhancement involves increasing or boosting activation and suppression involves dampening activation if anaphors enhanced or increased their antecedents activation that would surely improve those antecedents accessibility similarly if anaphors suppressed or dampened the activation of other concepts that would surely improve those antecedents accessibility by suppressing the activation of other concepts a rementioned concept would gain a privileged position in the queue of potential referents six experiments demonstrated that anaphors such as pronouns and repeated proper names do improve their antecedents accessibility by the mechanisms of suppression and enhancement gernsbacher in each of these experiments subjects read sentences that introduced two participants in their first clauses and referred to one of those two participants in their second clauses e g ann predicted that pam would lose the track race but she pam came infirst uery easily as in the second clause ana ph or was either a pronoun like she or a proper name like pam at different points while subjects were reading each sentence one participant name was presented e g ann or pam and the subjects task was to verify whether that participant had occurred in the sentence they were reading subjects verification latencies provided an index of how the anaphors affected both the antecedents like pam and what i shall refer to as the nonantecedents like ann the first experiment measured activation immediately before and morton ann gernsbacher diately after the pronoun vs name anaphors i e the participants names were tested at the two points marked with asterisks in ann predicted that pam would lose the track race but she pam came in first very easily this experiment demonstrated that proper name anaphors immediately use both suppression and enhancement to improve t eir anteced nts referential access the data from this experiment are dtsplayed m ftg what is displayed is the mean time it took subjects to verify that either the antecedents e g pam or the nonantecedents e g ann occurred in the experimental sentences as a fu nction of whether the anaphors were names vs pronouns as shown in fig when the anaphors were names responses to the antecedents were substantially fa ster after the anaphors than before thts effe ct supports the hypothesis that name anaphors immediately enhance their antecedents such that antecedents are more accessible after anaphors than before also when the anaphors were names responses to the nonantecedents were substantially slower after the anaphors than before so in addition to enhancing the activation of their antecede nts the name anaphors also appeared to suppress the activation of the nona t cedents lt was as if rementioning one participant made the other parttctpant less accessible however in this fi rst experiment the evidence of enhancement and suppression emerged with the name anaphors only as sho n in fig there was no i mmediate change in activation as a result of subjects having read the pronouns all of these results were replicated in a second experiment with the slight change that when activation was measured before the anap h rs it was measured at the end of the first clause instead of after the begmmng of the second in other words the participants names were tested at the two points marked with asterisks in ann predicted that pam would lose the track race but she pam came in first very easily again the name anaphors both immediately enhanced the activation of their antecedents and immediately suppressed the activation of other nonantecedents and again there was no immediate change in activation before vs after the pronouns this pattern for the pronouns also replicated a study by tyler and marslen wilson they too fo und that pronouns did not immediately affe ct the activation of their antecede nts however the third experiment of gernsbacher demonstrated that pronouns do suppress other nonantecedents they simply take more time to do so in this experiment activation was measured at two new test e o z processes and mechanisms in comprehension 950 850 immediately before anapbor nonantecedent name immediately afier anapbor fig results of gernsbacher experiment the data displayed are the subjects mean verification latencies to antecedents vs nonantccedents in sentences containing name vs pronoun anaphors points immediately after the pronoun or name anaphors and at the ends of the sentences as in ann predicted that pam would lose the track race but she pam came in first very easily and indeed by the ends of the sentences the pronouns nonantecedents had become considerably less activated thus sometime over the course of the second clauses the pronouns suppressed their nonantecedents one reason that these pronouns might have taken longer to suppress their nonantecedents is that it was not until the second clause that the pronouns were semantically disambiguated that is prior to the second clause the pronouns could have referred to either sentence participant however a fo urth experiment demonstrated very similar results even though the pronouns were disambiguated by a prior semantic context as in morton ann gernsbacher bill lost a tennis match to john accepting the defeat he walked quickly toward the showers or enjoying the victory he walked quickly toward the showers a fifth experiment demonstrated that pronouns still do not employ suppression immediately even when they match the gender of only one participant as in tim predicted that pam would lose the track race but she came in first very easily but once they do employ suppression it is more powerful than when the pronouns are not gender disambiguated the sixth and final experiment demonstrated that rementioned participants are not the only ones who improve their referential access by suppressing other participants newly introduced participants do so as well that is introducing a new participant as in has the same effect as rementioning an old participant as in ann predicted that pam would lose the track race but sue ann predicted that pam would lose the track race but pam both suppress the activation of other participants thus suppression seems to be a powerful mechanism controlling referential access c r o l e o f e n ha ncement a n d su ppression i n cataphoric access j ust as there are anaphoric devices that enable access to previously mentioned concepts i propose that there are also cataphoric devices that improve access to subsequently mentioned concepts recently we gernsbacher shroyer explored one device that might serve this cataphoric function the device we studied was the unstressed indefinite article this most of us are familiar with the indefinite this we use it to introduce concepts in jokes as in so this man walks into a bar or so a man walks into a bar with this p arrot on his shoulder we also use it to introduce concepts in narratives or conversations as illustrated by one of larson cartoon characters a cocktail waitress recounting the events o f a bar room brawl processes and mechanisms in comprehension so then this little sailor dude whips out a can of spinach this crazy music starts play in and well just look at this place emphasis mine actually only the first two occurrences of this in are examples of the indefinite this the third this as in well just look at this place is an example of the stressed this the indefinite this differs from both the stressed this and the deictic this as in this is a mess or look at this because both the stressed and deictic this are definite perlman according to linguists a classic test of indefiniteness is occurrence in the existential there construction as demonstrated in the indefinite article this and the indefinite article a pass this test but the definite article the fails making agrammatical as indicated by the asterisk there was this guy in my class who there was a guy in my class who there was the guy in my class who the indefinite this is interesting for a couple of reasons first it is a relative newcomer to english wald suggests that its use dates back only to the late second the indefinite this occurs considerably more often in informal spoken dialects than formal or written ones although some prescriptive grammarians dictate that it is unacceptable in any dialect because it is an indefinite article the indefinite this like the indefinite a or an is used to introduce new concepts into a discourse in fact of the occurrences of the indefinite this that prince observed in terkel book working introduced a distinctly new concept the only exception was arguably introducing the same lexical form but with a different referent but more interestingly in of those occurrences the noun introduced with the indefinite this was referred to again and as prince said within the next few clauses this observation was quantified more explicitly by wright and 987 they recorded and year olds telling one another jokes and informal stories when the children introduced nouns with the indefinite this they referred to those nouns an average of times in the subsequent clauses that they produced in contrast when they introduced nouns with the indefinite a they referred to those nouns an average of only 68 times in their next clauses these data suggest that speakers use the indefinite this to introduce concepts that are going to play a pivotal role in the subsequent narrative thus the indefinite this is a likely candidate for what i call a cataphoric device morton ann gernsbacher in gernsbacher and shroyer we asked the fol lowing question does introducing a concept with the indefinite this as opposed to the more typical a make that concept more accessible to answer this question we auditorily presented informal narratives to subjects telling them that at some point in each narrative the narrator would stop talking when that happened it was their job to continue telling the narrative we constructed our narratives so that the last clause introduced a new noun we man ipulated whether this critical noun was marked by the indefinite this or the more typical indefinite a e g i went to the coast last weekend with sally we d checked the tide schedule n we d planned to arrive at low tide cus i just love beachcombin right off i found three whole sanddollars so then i started lookin for agates but i couldn tfind any sally was pretty busy too she found this an egg from the transcriptions of our subjects continuations we measured three manifestations of accessibility fre quency of mention immediacy of mention and anaphoric explicitness we fou nd reliable effects of all three measures when the nouns were marked by this subjects mentioned the nouns more frequently often within the first clauses that they produced and typically via less explicit anaphors such as pronouns in contrast when the nouns were marked by a subjects mentioned the nouns less fre quently and typically via more explicit anaphors such as full noun phrases these results suggest that concepts initially marked with the indefinite this are subsequently more accessible therefore the indefinite this operates as a cataphoric device indeed prince has suggested that the indefinite this parallels a device in american sign language in which a signer establishes an absent third person on his or her right so that the signer might later refer to that individual an absent person who is not intended to be later referred to is not established this way clearly this american sign language device is also operating cataphorically how do cataphoric devices improve the accessibility of their concepts in gernsbacher and jescheniak we demonstrated that cataphoric devices like anaphoric devices improve referential access via the mechanisms of suppression and enhancement cataphoric devices improve the ir concepts accessibility by suppressing the activation of other concepts and by making their concepts more resistant to suppression by other concepts processes and mechanisms in comprehension d role of suppression and enhancement in the loss of access to surface information the mechanisms of suppression and enhancement might also explain why surface information as opposed to thematic information becomes less accessible more rapidly during comprehension sachs to understand how these mechanisms can account for this one must consider what surface information is typically su rface information is defined as information about a stimulus that does not contribute to its meaning e g the syntactic form of a sentence but another definition is that the surface properties of any stimulus are those that change the most rapidly for example consider a passage of text if the passage is well composed then each sentence conveys the same thematic idea but each sentence does not present the same syntactic form because the passage syntactic form changes more rapidly than its thematic contact its syntactic fo rm is considered surface information while its thematic content is not based on this de finition the mechanisms of suppression and enhancement explain why su rface info rmation becomes less accessible more rapidly than thematic information because surface information is constantly changing the newer surfac e information is constantly suppressing the old in contrast because thematic information is constantly being reintroduced it is repeatedly enhanced the net result is that thematic information is more activated than surfac e information therefore thematic information is more accessible this de finition accompanied by the mechanisms of suppression and enhancement can also explain why surface information is less accessible afte r comprehension of thematically organized than seemingly unrelated materials a ande rson et al de villiers peterson mcintyre with unrelated sentences surface information is no longer more rapidly changing than thematic information therefore it would be less likely to be suppressed or more likely to be enhanced for instance in j r anderson and bower experiment they presented sentences either grouped together as a related story or randomly arranged as an unrelated list in both conditions half the sentences were presented in the active voice and half in the passive voice because the sentences in the unrelated condition had no thematic continuity their greatest common denominator was their syntactic form on the other hand the greatest common denominator of the sentences in the related condition was their thematic content this definition of surface information and the mechanisms of suppression and enhancement can also explain another pattern of results surface morton ann gernsbacher information tested by synonym substitution is more accessible after comprehending abstract than concrete sentences in contrast thematic information tested by subject object reversal is more accessible after comprehension of concrete than abstract sentences begg paivo johnson bransford nyberg cleary moeser pezdek royer however in studies demonstrating this pattern the abstract sentences differed fundamentally from the concrete sentences the abstract sentences were less comprehensible according to several criteria holmes langford holyoak klee eysenck moeser schwanenfiugel shoben in other words the abstract sentences had less thematic content than the concrete ones so comprehending the words of abstract sentences might have been like comprehending the sentences of unrelated groups not thematically cohesive on the other hand comprehending the words of concrete sentences might have been like comprehending the sentences of related groups thematically cohesive thus performance with the abstract sentences could have resulted from less enhancement of their thematic information or less suppression of their surface information on the other hand performance with the concrete sentences could have resulted from greater enhancement of their thematic information or greater suppression of their suiface information evidence already exists to support this explanation when the abstract sentences were each embedded in their own contextual paragraph i e a thematic idea was supplied the pattern disappeared pezdek royer with the added thematic continuity comprehending abstract sentences mimicked comprehending concrete ones in sum the mechanisms of suppression and enhancement during structure building appear to play a fundamental role in many comprehension phenomena the fine tuning of lexical concepts the accessibility of concepts via anaphoric and cataphoric reference and rapid inaccessibility of surface as opposed to thematic information v individual differences in general comprehension skill according to the structure building framework many of the processes and mechanisms involved in language comprehension are general cognitive processes and mechanisms this orientation suggests that some of the reasons that individuals differ in comprehension skill might not be spec fic to language in this last section i describe how the structure building framework has provided a guide for understanding which cognitive processes and mechanisms underlie differential comprehension skill processes and mechanisms in comprehension experience informs us that individuals differ in comprehension skill laboratory research documents this as well see reviews by carr gibson levin perfetti smith spoehr unfortunately the focus of much of this research has been on comprehension of one modality i e the printed word and on individuals who differ at one stage of skill development i e beginning readers so it not too surprising that the processes and mechanisms previously suggested to underlie differences in comprehension skill are processes and mechanisms specific to reading but when studying adult comprehension skill one can go beyond those sources his is because at an adult level of proficiency skill at comprehending wntten language is highly correlated with skill at comprehending spoken language dane man carpenter jackson mcclelland palmer macleod hunt davidson peifetti lesgold sticht furthermore the high correlations between comprehending written and spoken language and the strong parallels between comprehending language and nonlinguistic media baggett gernsbacher jenkins wald pittenger suggest the hypothesis that differences in adult comprehension skill might not depend completely on facility with language in gernsbacher et al we tested this hypothesis by creating a multimedia comprehension battery gernsbacher varner the battery comprises six stimulus stories two are presented via written sentences two via auditory sentences and two via pictures the battery was administered to college aged subjects the correlation between reading and listening was between reading and picture viewing and between listening and picture viewing in addition a factor analysis revealed only one possible factor most likely a general comprehension skill to explain differences in this general comprehension skill one must look for general cognitive processes a starting point for investigation comes from a finding observed by perfetti and goldman and peifetti and lesgold they pinpointed a characteristic of less skilled comprehenders that appears during both reading and listening less skilled comprehenders have worse access to recently comprehended information that is although all comprehenders have difficulty remembering the wording of a recently comprehended sentence less skilled comprehenders have even more difficulty because i previously demonstrated that this phenomenon was not unique to language comprehension i hyp othesized that poorer access to recently comprehended information might be a good marker of less skilled comprehenders regardless of the modality they were comprehending we tested this hypothesis by selecting a set of less and a set of more morton ann gernsbacher skilled comprehenders from the extreme thirds of the distribution of subjects who had been tested on the multimedia battery gernsbacher et al experiment these subjects comprehended six new stimulus stories two in each of the three modalities at two points during each story the comprehenders access to recently comprehended information was tested the two points were after the subjects had comprehended half of a story and after they had comprehended an entire story the results of this experiment expressed in average percent correct are displayed in fig the more skilled comprehenders are indicated by the hashed lines and the less skilled by the unfilled bars as illustrated in the figure the less skilled comprehenders did indeed have poorer access to recently comprehended information and this was the case in all three modalities thus less skilled comprehenders poorer access to recently comprehended information is not limited to language based comprehension on the face of it these findings might suggest that less skilled comprehenders are plagued by smaller memory capacities b ut within the normal range of comprehension skil l which is the range of interest to us fig results of gernsbacher varner and faust experiment the data displayed are averages of subjects perce ntage correct recognition of a recently comprehended sentence or picture the more skilled comprehenders are indicated by the hashed bars and the less skilled comprehenders by the unfilled bars processes and mechanisms in comprehension here less skilled comprehenders cannot be distinguished from more skilled comprehenders by traditional immediate or short term memory measures see citations in gernsbacher in the spirit of perfetti and his colleagues i suggest that poorer access to recently comprehended information is not the cause of poorer comprehension skil l it is only a symptom to understand the underlying cause or causes one must understand why any comprehender loses access to recently comprehended information according to the structure building framework this results from shifting from actively building one structure or substructure to initiating another because information represented in one substructure is most available during the active processing of that substructure once the comprehender has shifted to a new substructure the information represented in the previous substructure becomes less available b ut meshing this explanation with the trademark of less skilled comprehenders namely poorer access to recently comprehended information yields the rather unusual hypothesis that l ess skilled comprehenders suffer from shifting too often that is instead of continuing to map incoming information onto the structure that they are developing less skilled comprehenders have a tendency to shift and initiate a new substructure we tested this hypothesis by selecting two more sets of more and less skilled comprehenders from the subjects tested with the comprehension battery gernsbacher et al experiment these subjects also comprehended six new stimulus stories two in each modality and information accessibility was again tested at two test points after half a story and after an entire story however unlike our second experiment our third experiment included a manipulation that was specifically designed to induce shifting the manipulation was scrambling the sentences or pictures within a story that is of the six stories that the subjects comprehended half were presented scrambled and half were presented in normal order one in each modality b y presenting half of the stories scrambled and half in their normal order we could compare a situation in which we know that all comprehenders have to shift more frequently during the scrambled stories with a situation in which we hypothesize that less skilled comprehenders might also be shifting too frequently during the normal stories the results of this experiment again expressed in average percent correct are shown in fig again the more skilled comprehenders are indicated by hashed lines and the less skilled by unfilled bars as illustrated in the top panel this third experiment replicated the second experiment by demonstrating that less skilled comprehenders have poorer access to f u w a a u f z w u a w o morton ann gernsbacher late fig results of gernsbacher varner and faust experiment the data displayed are subjects average latencies to reject an inappropriate meaning of an ambiguous hcnded sentence or picture all three modalities averaged top panel displays the difference between the two test points bottom panel displays the effect of the scrambling manipulation the more skilled comprehenders are indicated by the hashed bars and the less skilled comprehenders by the unfilled bars cently comprehended information again this difference was observed for all three modalities the novel finding of this experiment is illustrated in the bottom panel for the more skilled comprehenders scrambling the stories significantly reduced their access to recently comprehended information however for the less skilled comprehenders there was virtually no difference between the normal vs scrambled stories one interpretation of these data is that for less skilled comprehenders comprehending normal stories is like comprehending scrambled ones i e it involves almost as many processing shifts thus these data support the hypothesis that less skilled comprehenders shift too often during ordinary comprehension why might less skilled comprehenders shift too often consider the consequences of a less efficient suppression mechanism information that is less relevant or even inappropriate to the structure being developed processes and mechanisms in comprehension would remain activated b ecause this irrelevant information could not be mapped onto the developing structure its activation might lay the foundation for a new substructure thus one consequence of an inefficient suppression mechanism would be the development of too many substructures in other words a greater tendency toward shifting in our fourth experiment we tested the hypothesis that less skilled comprehenders are less able to selectively suppress irrelevant information we did this with a task that measures how well comprehenders can suppress irrelevant information we called this task context verification and the procedure was as follows subjects read a sentence and were then presented with a probe word their task was to vei ify whether the probe word matched the context of the sentence just read in half the trials the probe word did indeed match the context but we were more interested in trials in which the probe word did not match the context in half of those trials the last word of the sentence was an ambiguous word e g the man dug with the spade the probe word was the meaning of the ambiguous word that was inappropriate to the context e g ace we compared how rapidly subjects verified that a word like ace was not related to the sentence with how rapidly they verified that ace was not related to the same sentence but with the last word replaced by an unambiguous word e g the man dug h ith the shovel this comparison gave us a measure of how activated the inappropriate meaning of the ambiguous word was the slower subjects were to reject ace after the spade sentence the more activated the inappropriate meaning must have been i e the less they were able to suppress the inappropriate meaning we referred to this measure as the amount of interference the comprehenders experienced we measured interference at two test points i mmediately after subjects finished reading each sentence and three fourths of a second later we predicted that at the immediate test point both the less and more skilled comprehenders would show interference this prediction was based on the vast l iterature demonstrating that immediately after an ambiguous word is read multiple meanings are activated regardless of context our novel predictions concerned what would happen at the delayed test point if the decreased activation of the inappropriate meanings is due to suppression and if this suppression mechanism is less efficient in less skilled comprehenders then the less skilled comprehenders should still be morton ann gernsbacher encing a reliable amount of interference and indeed that is what we found the results of this experiment expressed in msec of interference are shown in fig immediately after the more skilled comprehenders read the ambiguous word they experienced a significant amount of interference suggesting that the inappropriate meaning was highly activated however three fourths of a second later they were no longer experiencing a reliable amount of interference suggesting that the inappropriate meaning had become considerably less activated perhaps via the mechanism of suppression in contrast for our l ess skilled comprehenders even as late as threefourths of a second after they read the ambiguous word the inappropriate meaning was still strongly activated that is the less skilled comprehenders were still experiencing a significant amount of interference in fact they were experiencing the same level of interference as they had experienced i mmediately after the ambiguous word this finding suggests that less skilled comprehenders are plagued with a less rapid and therefore less efficient suppression mechanism this in turn could l ead to their greater tendency toward shifting and their poorer access to recently comprehended information vi summary and conclusions in this chapter i have identified and described three general cognitive processes involved in language comprehension i e laying a foundation for a mental structure mapping coherent or relevant information onto that w zw a w u a w context verification immediate fig results of gernsbacher varner and faust experiment the data displayed are subjects average latencies to reject an inappropriate meaning of an ambiguous word minus their average latencies to reject the same words preceded by an unambiguous word see text for a fuller description the more skilled comprehenders are indicated by the hashed bars and the less skilled comprehenders by the unfilled bars processes and mechanisms in comprehension structure and shifting to develop a new structure when the incoming information is less coherent or relevant i also suggested that two general cognitive mechanisms underlie the processes of structure building they are suppression and enhancement these general cognitive processes and mechanisms account for many linguistic and nonlinguistic comprehension phenomena for example the process of laying a foundation accounts for the advantage of first mention after comprehending a sentence involving two participants it is easier to access the first mentioned participant than the second mentioned participant this advantage is not due to linguistic or structural factors such as the first mentioned participant greater tendency to be semantic agents or syntactic subjects rather i suggest that the advantage arises because comprehension involves laying a foundation and for this reason firstmentioned participants are more accessible both because they form the foundations for their sentence level representations and because it is through them that subsequent information is mapped onto the developing representation the process of laying a foundation also accounts for the change in accessibility of concepts from multiclause sentences when comprehenders are still developing their representations of a final clause concepts in that final clause are more accessible than concepts from an initial clause after comprehenders have built their representations of both clauses but before they begin mapping those representations together concepts from both clauses are equall y accessible a little bit later concepts from the first clause become more accessible i suggest that at that point comprehenders have finished mapping the second clause onto the first and the first c lause serves as the foundation for the whole sentence l evel representation the greater accessibility of concepts from the first clause strengthens over time demonstrating that order of mention is a relatively long lived characteristic of the mental representation of a sentence the process of laying a foundation also accomplishes what linguists suggest are the functional roles of order of mention according to some linguists initial mention codes importance and functions to attract attention according to others first mention codes givenness and functions to create a context for subsequent comprehension if first mention is selected in order to signal importance then the function is accomplished becauseby virtue of being first mentioned initial information gets represented at the core or foundation of the structure this causes the information to be more accessible which is most likely the goal of marking information as tmportant if first mention is selected in order to signal givenness then the function is also accomplished because by virtue of being first mentioned initial information organizes the representation of subsequent morton ann gernsbacher information that is subsequent information gets mapped onto the developing structure vis a vis the initial information other processes involved in structure building account for other comprehension phenomena for example the process of shifting accounts for why comprehenders rapidly lose access to recently comprehended information according to this explanation information becomes less accessible because comprehenders shift from developing one substructure in order to develop another presumably information represented in one substructure is most available during the active development of that substructure once a comprehender has shifted to initiate a new substructure information represented in the previous substructure becomes less available in gernsbacher i demonstrated that comprehenders rapid loss of access to recently comprehended information was not specific to language based comprehension and i tested the shifting explanation against a recoding explanation information becomes less accessible because it is recoded into gist the explanation based on the cognitive process of shifting clearly accounted for the phenomenon during both language and nonlanguage comprehension the process of shifting also predicts comprehenders responses to speakers and writers cues for a new phrase clause topic setting or point of view comprehenders slow their comprehension when they encounter these cues this suggests that these cues trigger comprehenders to begin laying a foundation for a new substructure comprehenders also have more difficulty accessing information presented before these cues than information presented after this suggests that the information presented after these cues is represented in a different mental substructure than that of information presented before the mechanisms of suppression and enhancement also account for many comprehension phenomena for example suppression helps finetune the meanings of ambiguous words and the associations of unambiguous words by suppressing the activation of ambiguous words inappropriate meanings and unambiguous words less relevant associations both suppression and enhancement play a role in referential access via anaphora in other words the act of referring to previously mentioned concepts antecedents via anaphors such as pronouns and repeated names some anaphors such as repeated names improve their antecedents accessibility by enhancing or increasing those antecedents activation other anaphors such as pronouns as well as repeated names improve their antecedents accessibility by suppressing or dampening the activation of other concepts suppression and enhancement also control referential access via cataphoric devices that is just as there are anaphoric devices that enable processes and mechanisms in comprehension access to previously mentioned concepts cataphoric devices improve access to subsequently mentioned concepts the unstressed indefinite this appears to operate in such a way cataphoric devices improve their concepts accessibility by enhancing the activation of those concepts by suppressing the activation of other concepts and by making their concepts more resistant to suppression suppression and enhancement also explain why surface information often becomes less accessible more rapidly than thematic information in a cohesive passage the surface information is the most rapidly changing characteristic whereas the thematic information is constantly being conveyed because surface information is constantly changing the newer surface information is constantly suppressing the old in contrast because thematic information is constantly being reintroduced it gets repeatedly enhanced the net result is that thematic information is more activated than surface information and thus more accessible finally the structure building framework provides a blueprint for investigating individual differences in general comprehension skill for example beginning with the finding that less skilled comprehenders have poorer access to recently comprehended information we have found that this phenomenon occurs during the comprehension of nonlinguistic stories as well we have also traced it to less skilled comprehenders tendency to shift too often and we have suggested that this tendency results from a less efficient suppression mechanism thus the structure building framework and its component processes and mechanisms account for many comprehension phenomena this framework should also be useful for understanding the comprehension of other media e g music lerdahl j ackendoff sloboda 985 this is because in many domains the goal of comprehension is to build a coherent representation of the entire stimulus language learning and development publication details including instructions for authors and subscription information http www tandfonline com loi universal reading processes are modulated by language and writing system charles a perfetti a lindsay n harris a a learning research development center university of pittsburgh please scroll down for article language learning and development 296 copyright taylor francis group llc issn print online doi universal reading processes are modulated by language and writing system charles a perfetti and lindsay n harris learning research development center university of pittsburgh the connections among language writing system and reading are part of what confronts a child in learning to read we examine these connections in addressing how reading processes adapt to the variety of written language and how writing adapts to language the first adaptation reading to writing as evidenced in behavioral and neuroscience data is achieved through a universal constraint that language places on writing and through the tuning of reading procedures imposed by specific features of writing systems children acquire skill in reading through increasing specialization of procedures tuned to their writing system while also acquiring more general universal procedures that serve language mapping and cognitive control for the second adaption writing to language we present examples from several languages to suggest that writing systems tend to fit their linguistic properties thus providing adaptive variation in writing to language mapping we suggest that this writing language fit facilitates the child learning how his or her writing system works the assumption that reading is based on language appears to be universally shared reading may also depend on the writing system that encodes the language the exact sense in which these two dependencies are true may not be so clear or at least not so universally ascribed to on one perspective reading is parasitic on language and only the relatively trivial achievement of learning to decode print into spoken language stands between the linguistically sophisticated year old and the achievement of fluent reading alternatively written language may have specific influences beyond decoding on cognitive processes that make it something more than language writ down how the language is written may matter for reading processes or it may matter only in details about how universal processes are implemented these issues are not new the complex relations between spoken and written language have been examined at multiple levels e g mattingly olson analyses of the influence of writing systems on reading processes have emphasized universal principles and specific vari ations e g perfetti ziegler goswami finally the idea that a language and its writing system are intertwined in some way halliday mattingly has been renewed in claims that this intertwining influences reading processes seidenberg frost in what follows we examine this complex of issues among language writing and reading with the hope of not adding too much additional confusion correspondence should be addressed to charles a perfetti lrdc o hara st university of pittsburgh pittsburgh pa e mail perfetti pitt edu the language reading divide there is a great divide between language and reading in how they are acquired as alvin liberman once put it in a state of nature everybody speaks nobody reads p thus when we study language development we are impressed by the biological head start of infants they come with a brain that depending on one point of view either has a universal grammar that needs only some modest amount of language specific input or has impressive statis tical learning procedures that either separately or in combination lead to very rapid development of language and so in language development we ask how development happens so fast we talk about reading differently referring to it correctly not as the development or even the acquisition of reading but as learning to read learning to read does not seem to be natural and biologically determined in the way that language development does furthermore in contrast to the normal course of spoken language development which of course is impaired for some children reading is sometimes hard to learn the thriving field of dyslexia research is aimed at trying to understand why learning to read fails for some children this contrast between rapid painless development and more effortful and failure prone learn ing is mirrored in the time lag of a hundred millennia of human history between language capacity and the development of writing the history of human writing has seemed relevant to understanding the differential cognitive demands of language and reading whereas meaning based graphs pictographs were common as initial attempts to communicate visually the graphs became more abstract less picture like in both middle eastern writing for example the devel opment of sumerian cuneiforms and in chinese writing defrancis most important the graphs over time perhaps suddenly de francis p started to be used to represent not meaning but language in its spoken form the cognitive advantage of coding language form rather than meaning is productivity a uni versal feature of human languages hockett that allows creation of an infinity of messages sentences from finite means syllables phonemes applied to written language productivity carries a cognitive advantage relieving cognitive resources that would otherwise have to store and remember an indefinitely large number of graph meaning associations instead one needs to learn and remember a small number of graph form syllable or phoneme associations by mov ing to productive form mapping writing inherited the productivity of language able to produce all the messages that a language allowed how does reading adapt to the variety of written language in principle one writing system could serve the vast variety of the world spoken languages indeed the international phonetic alphabet ipa or other forms of phonetic transcription do exactly that a universal set of graphs allows the sounds of all languages to be written down at least in principle actual written languages unlike ipa were not invented for the purpose of universal transcription but rather followed the more complex routes of cultural artifacts the result is wide ranging variety in the appearance of the graphs see figure across languages as well in how the graphs map onto units of language of course appearances are not everything and this variety is commonly considered to reflect only three basic writing types according to their basic unit of mapping morpho syllabic or logographic syllabic and alphabetic it is further figure variation in the appearance of writing argued by writing scholars that in the order given above these three types reflect the evolution of writing systems e g gelb the three basic writing systems can be exemplified by three asian languages the logography of chinese in which each grapheme i e character corresponds to a syllabic morpheme often a word in the spoken language the syllabary of japanese in which each grapheme corresponds to a syllable in the spoken language and the alphabet of korean in which each grapheme corresponds to a korean phoneme note that although the graphemes of chinese and japanese both represent spoken syllables japanese graphemes map to the phonetic building blocks of words whereas the morpheme or word to which a chinese character maps happens to be a single syllable there is no completely reliable cue to sounding out a character word note also that despite its visual resemblance to the other two asian scripts the korean writing system is alphabetic like the more familiar roman alphabet in short it is not how the graph looks or the size of the chunk of spoken language to which it corresponds but how it maps structurally to the spoken language that matters these asian examples also draw attention to anglocentrism share in reading research share argues that models of word reading were developed by exclusive attention to english and that at least some theoretical explanations of word reading and dyslexia would be different had there been attention to the variety of languages and orthographies from the begin ning although this may be true research on languages other than english has flourished in the last years bringing a broader perspective and many new results indeed the possibility of viewing universals in reading has been enabled by this broadening of language and writing samples universals in reading the major adaption of reading to variation in writing is through universal reading procedures put more carefully the claim is that writing itself developed its variations within constraints that honored the procedures by which human brains could get linguistic meaning from print the key is the language constraint that all true general purpose writing systems encode language not concepts from this constraint it follows that reading must engage the language of the reader a fact that builds into reading a highly general adaptation whatever the appearance of the writing reading processes engage the linguistic system beyond the logic of the language constraint the search for universals becomes an empiri cal question for example does chinese writing a system that does not represent phonemes nonetheless evoke phonology during reading chinese does not strictly require activating phonol ogy and it does not even encourage it as a matter of writing system design even at the syllable level figure shows why this is true because of a high level of homophony many different written characters map onto given syllable even when tones are taken into account in such a system a process that links characters only with their meaning seems to be encouraged thus chinese does not represent phonology at the phoneme level and its representation of phonology at the syllable level seems to discourage its use the formal separation of orthography from phonology which ranges from difficult english to nearly impossible shallow orthographies such as finnish or korean in alphabetic writing is built into chinese as figure illustrates accordingly it has been possible for experiments to vary orthography and phonology independently to study whether phonology is activated dur ing chinese reading these experiments by and large found that when characters are read for meaning their phonology is activated in making a decision about whether two characters are related in meaning decisions are slowed when the characters with different meanings have the same pronunciation perfetti zhang decisions about whether a single character belongs to a semantic category are also slowed when a non category character with the same pronunci ation as a category member is presented xu pollatsek potter such results led to an empirical generalization that is the universal phonological principle perfetti zhang berent that phonology is part of reading across all writing systems as with alphabetic exper iments there are task specific conditions that seem to influence the emergence of evidence for phonology relative to semantics but the generalization that phonology is part of written word identification seems to hold in chinese just as it does in alphabetic languages see perfetti liu tan for a review and a theoretical treatment of chinese reading figure homophony in chinese a single syllable phonological system is associated with many different meaning bearing morphemes meaning system many different characters orthographic system share a given syllable thus making phonological mediation of meaning difficult however a given character identifies the meaning bearing morpheme thus one way that reading adapts to variety in writing is through a universal attachment to language the phonological system of a language provides multiple units for this attachment syllables as well as phonemes and which unit is selected does matter for reading perfetti the larger syllable units are attached to orthographic units in chinese and japanese whereas the smaller phoneme units are attached in alphabetic writing ziegler and goswami generalize the importance of such variability in the size of the language units in a grain size theory of how reading differs across languages this emphasis on phonology reflects its importance as the surface form of language because it sits at the surface of language phonology allows a potentially simple mapping onto surface fea tures of the writing system nevertheless phonology is not the whole story and writing systems and orthographies may negotiate trade offs between phonology and morphology a language form to meaning mappings which include grammatical and lexical formation components to make reading efficient as well as effective for example in english the b in the word debt is not pronounced an example of english failure to map graphemes consistently to phonemes however the presence of the b reflects a meaning connection to debit where the b is pronounced in developing parallel distributed processing models of learning to read harm and seidenberg refer to the division of labor between phonology and morphology semantics writing can code mainly morphology mainly phonology and strike balance points in between reading procedures are tuned by writing systems and orthographies in addition to language based universals in reading there appear to be some writing specific pro cedures that are tuned to the features of the writing system the well known variation among alphabetic orthographies illustrates the potential need for tuning to a rather narrow range of dif ferences the continuum runs from shallow orthographies illustrated by almost pure alphabets in finnish and welsh where each letter corresponds to one phoneme and vice versa to the impure english where letters show a one to many mapping to phonemes the consequences of these variations have been shown in studies of reading for example by aro and wimmer who compared english reading by children in the united kingdom with reading more shallow european orthographies finnish swedish spanish dutch german and french they found a dramatic difference in the ability to read pseudowords which requires the use of grapheme phoneme connections as opposed to whole word strategies that can be applied to real words with english first graders way below first graders of all other comparison countries by fourth grade english language children were comparable to first graders who read the more shallow orthographies these kinds of differences have been seen in other comparative studies e g landerl wimmer that have ruled out alternative explanations for the differences the conclusion is that reading procedures in particular the relative use of grapheme phoneme mappings in decoding printed words have been tuned to features of the orthography an idea expressed by katz and frost as the orthographic depth hypothesis in addition to behav ioral studies brain imaging studies have shown differences between activation patterns for italian a shallow orthography reading compared with english reading paulesu et al differ ences that were consistent with more grapheme phoneme based reading in italian and more word retrieval reading in english with such adaptations within the relatively narrow range of alphabetic orthographies we also expect adaptations across the wider gulf that separates alphabetic and nonalphabetic writing indeed reading chinese again the high contrast case shows differences from reading alpha betic english one characterization of the basic difference keeping in mind that phonology is activated in chinese as well as english is that in chinese characters are recognized thresh old style while in alphabetic reading words are recognized cascade style perfetti et al the threshold cascade distinction which was introduced by coltheart curtis atkins and haller centers on the timing of phonological activation relative to orthographic in alphabetic cascaded processing phonemes are activated with grapheme activation based on links between them and this can happen prior to the moment of word identification in chinese this cascading is not possible and phonology instead is activated only when an orthographic recognition thresh old is reached thus in chinese as characters are recognized there is immediate activation of the corresponding syllable this threshold style processing also allows the activation of meaning directly by the character and reduces the likelihood that activation of phonology during reading mediates meaning in chinese except for characters with very few homophones tan perfetti as in research on reading in alphabetic systems these properties of chinese reading are mirrored in brain imaging research meta analyses of word reading studies bolger perfetti schneider tan laird li fox show both shared and nonshared activation patterns across writing systems a finding consistent with the existence of both universal i e the general need to map graphic forms to linguistic units and writing system specific i e the detailed implementation of phonological and semantic processes reading procedures word reading in all writing systems for instance activates the left hemisphere occipital temporal area around the fusiform gyrus this visual word form area shows enhanced responsivity to writ ten language with the development of expertise with a specific orthography mccandliss cohen dehaene recent evidence suggests also a universal premotor area that is involved in reading handwritten input in both chinese and alphabetic writing nakamura et al nonuniversal activation patterns include the bilateral activation of posterior visual orthographic areas in chinese compared with left lateralized areas in alphabetic reading in addition less acti vation of temporal parietal phonological areas occurs in chinese while more activation occurs in the left mid frontal gyrus compared with english tan et al the visual orthographic activation difference may reflect the specific demands of the layout of the chinese script while other differences in temporal parietal and frontal areas suggest differences in how phonology and meaning are associated with the orthography during recognition the system accommodation hypothesis perfetti liu based on such neuroimaging results and erp studies that show detailed temporal processing differences between chinese and english captures the generalization that reading processes and the neural structures that support them accommodate to specific visual and structural features of writing systems adding complexity to the picture is evidence that whereas chinese reading compared with alphabetic reading engages distinctive areas in normal readers chinese dyslexics show activation patterns that are highly overlapping with what is shown by alphabetic dyslexics hu et al children development of universal and specialized procedures despite the difficulty many children have in learning to read the typical course is one of learning print language mappings through generalized decoding procedures for alphabetic and syllabary writing and word specific learning all writing systems if the general nature of reading is a combination of universal language general procedures and language specific procedures a ques tion becomes the acquisition and refinement of these procedures with learning to read a starting point is the assumption that increased reading skill must bring increased specialization such that a chinese speaking child becomes very good at reading chinese and an english speaking child becomes very good at reading english with no corresponding increase in the ability to read some other language this is not to say that reading in a second language does not benefit from being a skilled reader in a first language it does wade woolley geva wang perfetti liu geva siegel durgunog lu et al based on developmental studies of reading in chinese cao et al and in english booth et al perfetti cao and booth hypothesized distinctive developmental courses of increased specialization in english the superior temporal gyrus stg which is associated with phonological phoneme level processing in alphabetic reading simos et al shows increased activation with experimenter designed phonological training shaywitz et al however this area shows decreased activation with age for chinese readers cao et al thus if phonological assembly is part of reading english but not chinese and if the stg supports this process then at least early in reading development adaptation to the specific demands of english increases the demands on this area while adaptation to the specific demands of chinese decreases them complementing this trend increased experience in chinese reading leads to more involve ment of right hemisphere visual orthographic areas including the right fusiform gyrus cao et al this area supports the visual spatial processes of word identification which are needed more for chinese characters than for linear alphabetic words experience related increases for chinese were also observed in the right superior parietal lobule these changes with experience suggest increased specialization to accommodate both the visual demands of chinese char acters seen in increased activation of bilateral visual orthographic areas and their syllabic phonological mapping demands seen in right superior parietal lobule activation with expe rience orthographic representations become higher in quality both as familiar and precisely detailed graphic forms and as tightly linked linguistically mapped units specialization serves this purpose however there are universal developments as well beginning with the importance of the left occipital temporal area that comes to support orthographic word identification and link it to lan guage areas despite the differences between the two writing systems in the level of mapping between orthography and language they share the basic need to map graphic units to spoken lan guage units it is possible that a universal function is also served by the left inferior parietal lobule ipl which shows age related increases in both writing systems cao et al booth et al and may be associated with a more writing independent mapping function that is word specific finally the left middle frontal gyrus lmfg has been consistently found to be involved in chinese reading with various interpretations offered for its function in reading e g perfetti et al tan et al because of the lmfg involvement in english read ing under some conditions it may serve an increasing control function in both writing systems perfetti et al thus with increased reading experience one sees both universal paths of skill development and more specialized paths that accommodate the demands of the writing system the universal paths may reflect the strengthening of the connections from posterior areas that handle graphic input with the left hemisphere language areas and the increased use of control areas to shift attention and processing resources to specific processing demands of phonology orthography and meaning which can vary with task demands and with the difficulty or complexity of the written input both of these developments serve reading skill no matter the writing system so far we have examined how reading accommodates to writing system variation however the overall story of adaption involves not just the writing system but also the language we turn to this question in the next section how does writing and thus reading adapt to language it is possible by the logic of experimental control that variations in reading that we have attributed to different writing systems are actually due to differences wholly or in part between languages chinese writing encodes the chinese language english writing encodes english and so forth although japanese offers intriguing complexities allowing a single language to be expressed by multiple writing systems the general situation is that writing systems and orthographies are not arbitrarily assigned to languages leaving from an analytic perspective a confound of variables one way through this confound is to consider how writing encodes language and whether it does so adaptively to the character of the language alphabetic writing in its purest form could approximate a notation system for speech letters denote phonemes and writing is written speech this characterization in practice would apply only narrowly to very shallow orthographies pure alphabetic writing a broader characteriza tion is that writing is encoded language not just encoded strings of phonemes on this view the adaptations that writing makes to language are based on the linguistic system not just its phonemic inventory the idea that writing adapts to linguistic systems is captured in the expression languages get the writing system they deserve this idea has had a number of expressions most recently by frost and seidenberg much earlier m a k halliday pointed out that the development of writing occurred over long periods of time allowing a language to get the sort of writing system it deserves halliday webster p reprinted from halliday behind this memorable expression lie the ideas of trade off and optimization seidenberg argued that what is traded off is the weight given to the language phonology compared with its meaning morphology the point of grapholinguistic equilibrium is the final resolu tion of the conflicting goals of having writing encode phonology as well as morphology at the equilibrium point spoken languages get the writing systems they deserve seidenberg p to demonstrate the feasibility of the hypothesis that writing adapts to language requires some particulars about the linguistic resources of a language we have sampled a range of languages to reflect a wide range of variety and hypothesized what features of writing would accommodate the language features table summarizes this effort the table provides estimates of the linguistic inventories of seven languages that represent the three major writing systems including two shallow alphabetic orthographies korean and finnish one deep alphabetic orthography english two vowelless alphabetic orthographies hebrew and arabic plus chinese and japanese the inventories are based on various sources but especially because counts of vowels consonants and syllables vary widely the encyclope dic work of comrie the analysis specifically targets the phonological structure and the morphological structure of each language if there is a trade off between phonology and morphology in the writing systems this should be visible in the pressures that the phonological and morphological structures put on writing which are suggested in the implications column of table english has a large phoneme inventory and a very large syllable inventory with little constraint on the formation of sylla bles the large phoneme inventory of english might work against pure alphabet coding of about phonemes but there does not seem to be some natural stop point for a grapheme set english gets by with letters but slovak expands the same latin alphabet used by english to let ters russian gets letters from the cyrillic alphabet but karbadian expands that to and modern hindi devana gar ı alphabet has letters these are all highly transparent shallow orthographies there would be little point to a large set of graphemes if they were not expanded by the use of diacritic marks attached to letters so one might suppose that when a one to one mapping of graphemes to phonemes would require some large enough number of graphs e g over alphabetic writing becomes burdensome however there are examples of orthographies for languages with large phoneme inventories that expand graphic resources to maintain a close mapping as in khmer cambodian whose phonology is captured by an alphabet with syllabary components so the implication for the phonology inventory is that a very large inventory might discourage alphabetic mapping but this by itself cannot be decisive the point of a trade off is that there is some other factor that also creates pressure and the result is a negotiation between these factors in the case of english the combination of phonemes into syllables with relatively few constraints e g phonotactic limits on strings of consonants allows for many syllables in table how writing systems may accommodate languages phonological structure of spoken language morphological structure of spoken language implications for writing system mapping approximate no maximal representative complexity of principle of language no vowels no syllables used in syllable morpho phonological inflectional how phonology and morphology writing language family consonants language structure processes morphology constrain writing system system english indo european arabic afro asiatic 40 total phonemes total phonemes cccvcccc e g strengths 000 cvvcc e g young shaabb heal health nation national a w a p in qa pilun speaker low phonological complexity leads to a large number of syllables making an alphabet efficient simple inflectional morphology and large number of morphophonemes combined with a mismatched letter to phoneme ratio lead to deep orthography high syllabary inappropriate because morpheme boundaries and syllable boundaries do not overlap abjad allows morpheme roots distributed consonants to be seen alphabet deep abjad conso nantal alpha bet mandarin chinese sino tibetan total phonemes disregarding tones cvc final consonant restricted to n or n of most frequent characters have context dependent pronunciations low small number of syllables paired with a predominance of monosyllabic words creates extensive homophony logographs differentiate between large number of homophones that are distinguished by tones in spoken language logography morpho syllabic continued downloaded by lindsay harris at august table continued phonological structure of spoken language morphological structure of spoken language implications for writing system mapping approximate no maximal representative complexity of principle of language no vowels no syllables used in syllable morpho phonological inflectional how phonology and morphology writing language family consonants language structure processes morphology constrain writing system system korean probably altaic total phonemes 000 cvcc got gochi see table high relative phonological complexity leads to larger number of syllables making an alphabet efficient near one to one grapheme phoneme correspondence and complex inflectional morphology result in shallow orthography alphabet shal low hebrew afro asiatic 22 total phonemes 000 mor pheme roots not syllables ccvc root morpheme ktb alternates to xtb in certain conjugations e g tixt ov future masculine of ktiv a writing medium syllabary inappropriate because morpheme boundaries and syllable boundaries do not overlap abjad allows morpheme roots distributed consonants to be seen abjad conso nantal alpha bet finnish uralic 23 total phonemes 000 cvvc or cvcc frequent morpho phonemic alteration at juncture of word stem e g sydän heart sydämeni my heart high extensive homophony avoided despite small number of phonemes through long words of several syllables complex inflectional morphology requires shallow orthography alphabet shal low japanese probably altaic 16 total phonemes cvc final consonant restricted to n person hito but people hitobito not hitohito medium logographs manage homophony resulting from extremely few syllables scarcity of syllable types almost all v and cv and small number of syllables lends language to syllabary prevalence of multisyllabic words makes tones unnecessary but makes it difficult to match words to logographs syllabic kana are paired with logographs to convey inflectional and derivational information logography syllabary sources include chen et al levelt et al graf ussishkin taylor taylor seidenberg frost halpern el imam and wiik estimates for number of vowels consonants and syllables in a language and even of maximal syllable structure vary widely we turned to comrie for this information whenever possible the table estimate so a syllabary that maps a graph to a syllable is not going to be privileged over an alphabet no matter how many phonemes the language contains because of this english is better adapted to an alphabet but its adaptation takes into account morphology english has a rather simple inflectional morphology lacking both case marking and gender on nouns however it makes use of morphophonemes that is it allows for a single morpheme to be expressed with varying phonemes the past tense morpheme ed attached to english verbs for example has variable but completely predictable expressions at the phoneme level t in jumped d in robbed and id in added written english spells all three variations with ed the past tense morpho phoneme so it jumped not jumpt preserving the morphology in the spelling at the expense of phonemic transparency to some extent the preservation of morphemes in english spelling may justify its opacity chomsky halle english spelling reflects derivational as well as inflectional morphology the d in judge jj preserves its link to judicial and use of the letter a in both nation and national keeps their morphemic link visible the argument that such spellings are also adaptive for english might be correct although examples are not proof the conventions of writing have complex origins and do not simply reflect an insightful balancing of the morphology phonology scales the irregularities of english spelling in particular have a number of sources venezky turning to a more transparent shallow orthography table shows korean and finnish as examples korean shows a manageable number of phonemes and about syllables making it a poor candidate for a syllabary and a relatively good candidate for an alphabet no historical reconstruction is needed because the korean alphabetic hangul was invented by king sejong in the century it served as a pure alphabetic system a graph for every phoneme however the system must have been slightly out of balance not at its equilibrium point because regres sive changes were eventually made the changes reflected a shift in the weighting from for phonology and for morphology to a weighting that sacrifices a small portion of the phonological transparency to preserve morphological relations in the spelling changes were made in korean that led to preserving graphs that carried morphological connections even though their pronunciation changed table adapted from perfetti illustrates this for the family of words related morphologically as case variations to the core sense of flower when the korean word for flower takes on agentive case the final consonant changes and in the classical korean orthography this led to a change in spelling thus the classical spelling reflected phonology table korean moved from pure alphabetic to a morphologically sensitive alphabet based on perfetti the pronunciation of the korean word for flower varies with grammatical context and in classical times the spelling changed with the pronunciation thus serving phonemic transparency modern written korean however privileges morphological transparency and retains a single spelling across contexts spoken form meaning got flower neutral got flower agent gotkwa flower with classical orthography modern orthography purely but at the cost of making invisible the morphemic relation of the word to its root flower morpheme later spelling changes brought morphemic spelling so that the graph for flower got was retained despite the pronunciation change in the agentive finnish did not have a king to impose its pure alphabet although it did have a bishop mikael agricola who some years after king sejong in korea wrote finnish as a not quite pure alphabet which was later moved farther toward the purity point uralic languages its relatively small number of phonemes and large number of syllables favor an alphabet over a syllabary its morphology can be expressed through agglutinative processes that join morphemes together in word strings and because changes to phonology are minimal in finnish with this process alphabetic writing continues to be favored in a sense finnish does not need to negotiate much it gets morphological transparency more or less free with its phonology within the alphabetic family but with distinctive deviations are hebrew and arabic west semitic languages with root morphemes represented by distributed noncontiguous consonants the idea of a morpheme taking the form of a consonant skeleton with vowels dropped into slots between and around the consonants can be perplexing for speakers of languages whose mor phemes come packaged in neat unbroken strings skeleton for instance shares the distributed s l n pattern with the english words salvation and subliminal but the triad of letters itself holds no meaning and the three words are not related in both arabic and hebrew by contrast the root k t b expresses the idea of writing kataba in arabic means he wrote and yaktubu is he writes with kita b signifying book maktaba signifying library and so on vowels are traditionally not expressed in either written hebrew or written arabic as context is generally sufficient to indicate how the empty slots should be filled although diacritics or points can be used to designate missing vowels for children who are just learning to read in contrast with these variations on the adaption of alphabetic writing is chinese its inventory of segmental phonemes is not particularly large but with the addition of tones the number of func tional i e morpheme distinguishing speech units is substantial the use of pin yin an alphabet that can be used to write chinese and is used for beginning reading instruction demonstrates the possibility of an alphabetic chinese however the large number of homophones works against the alphabetic solution a syllabary likewise would not eliminate the problem of homophony the character system provides a resolution of homophonic ambiguity as figure illustrates but it does so at a price thousands of graphic units need to be learned still one can argue that the chinese language found a system that is reasonably adaptive japanese has a very small syllable inventory that favors a syllabary solution it has even fewer phonemes which is again a hint that phoneme number is not a critical feature by itself because japanese is multisyllabic the syllabary presents a much neater more efficient representation for its longer words kana the graphemes of the japanese syllabary are extremely transparent phonologically as syllable units they are morphologically opaque however japanese also has kanji graphs based on the chinese logographic system japanese was not written down until about the century ad when chinese characters began to be used to encode japanese in most cases a character retained something like its chinese pronunciation at the time it was borrowed in addition to having a comparable japanese translation applied to it thus most japanese characters in use today have at least two pronunciations or readings and often many more this system presents obvious complications for instance one might argue that multisyllabic japanese is ill suited to characters designed for monosyllabic chinese in addition a reader of japanese is faced with multiple potential pronunciations of every character he encounters both of these concerns are addressed by supplementing kanji with kana take for example the character big its chinese reading is dai which is incidentally still its pronunciation in modern mandarin the native japanese word for big is ookii a four syllable word kanji are rarely asked to hold four syllables instead the first two syllables oo are applied to the character and the final two kii are appended as kana the kana help to clue in the reader to which reading should be used alternatively followed immediately by another kanji indicates that its chinese reading is required as in the character for university daigaku literally big study in theory japanese could long ago have dispensed with kanji and adopted kana as its sole writing system it might be the case that the prestige of kanji has prevented this from happening but it is also possible that reducing morphological transparency in favor of phonological trans parency is a step the japanese have been unwilling to make such reluctance to abandon kanji mirrors the american reluctance to reform spellings such as health which are phonologically misleading but morphologically illuminative we return to the discussion of spelling reform in a moment these examples suggest that writing systems have some degree of compatibility with the lan guages they encode however the range of languages with widely varying phoneme and syllable inventories that are served by an alphabet suggests the alignment of writing system and language is not perfect more generally the literal reading of the claim that languages get the writing sys tem they deserve has to go untested frost seems to make this stronger claim writing system development includes too many factors variations in technology and other cultural fac tors to allow one to say that over time the written language settles into an optimal alignment with the spoken language furthermore while languages continue to undergo change indefinitely written language once established is resistant to further change this is part of the reason that written english does not align perfectly as a pure alphabet with spoken english this resistance to change has many examples in the history of writing reform where gov ernments have been able to impose changes as in the periodically updated dutch het groene boekje the little green book there has been some success although never without organized resistance even in the dutch case english examples of reform abound although the extent to which the reform movements were serious endeavors might not be widely appreciated in the early century andrew carnegie funded an english spelling reform project convening a well paid commission in the united states the commission known as the simplified spelling board made serious efforts to bring about change recommending a broad set of more trans parent spellings the board recommendations actually gained some official acceptance when president theodore roosevelt ordered the government printing office to use the new spellings thus roosevelt message on the panama canal war declaration was written in the simplified english spellings recommended by the board spelling honor and thru instead of honour and through eventually the reform fell to political opposition within the congress and also the supreme court some of the spelling changes have survived in american english to this day anemia instead of anaemia and mold instead of mould it is informative to consider what distinguishes the recommendations from the spelling board that survived to modern american english from those that perished with the board in in addition to mold we have catalog as an alternative to catalogue and gage as alterna tive to gauge with spelling choices rather than simple no choice correctness being typical the losers the changes that failed to survive are well represented by addresst claspt dasht and wisht in each case the proposal to make spelling correspond to sound failed predictably from an equilibrium point of view the ability to see past tense the english morphophoneme trumps the need to see the voiceless alveolar t to be able to immediately decode wished as the past tense of wish is efficient for morpheme identification and thus sentence compre hension even if it is disrespectful to phonology american english has settled on a point of grapho linguistic equilibrium seidenberg that allows morphology to shine through its orthography notice however that equilibrium if that is what it is is not reached through purely natural forces whatever those might be in the case of writing but rather through the push and pull of cultural forces the idea of an optimization in which a language acquires a writing system that is the best possible fit for its linguistic features is flawed because of the multitude of influences at work instead we believe a more modest fit hypothesis is in order the character of a language contributes to the development of a writing system so that the language is efficiently encoded efficiency must refer to the ability to express the language in the writing and the ability to decode the writing into the language so a language tries on a writing system that fits ok and then keeps it even if the fit tightens or loosens with age bringing it in or letting it out but never throwing it away if writing fits language what are the implications for reading if features of a language contribute to shaping its choice of writing system and orthography there should be consequences for reading and there are frost provides a telling example con cerning the importance of orthographic precision in reading the transposed letter effect refers to the fact that in lexical decisions primes under conditions of masking that limit processing of the prime are effective even when they are disordered spellings of the target word for example slat salt produces nearly as large a priming effect as salt salt as if the word processing mech anism were indifferent to exact spelling this effect robustly observed in english forster davis schoknecht carter perea lupker andrews is also reported in spanish perea lipker french schoonbaert grainger and japanese kana perea perez however it is not found in hebrew velan frost the explanation for this lack of universality as frost explains is the critical role that consonant triads play in hebrew morphology to illustrate using roman letters in left to right order k t b is a root morpheme that depending on the context gets expressed as the verb write katabh or the agentive noun writer kotebh among other morphemically related words in hebrew and arabic it would not do to scramble the k t b order because only in that order is the morphemic identity preserved a change in order yields a different morpheme so out of order priming does not work this is a direct influence of the spoken language on the written language the conso nantal structure of spoken hebrew and arabic gets mapped directly onto the consonantal alphabet abjad thus reading is influenced through language structure by way of writing it is possible as we suggested earlier that some of the reading differences attributed to writing systems alone are partly due to properties of the languages that have tuned the writing system for example wang koda and perfetti found differences between korean and chinese learners of english as a second language in the degree to which they committed false alarms to phonological foils sale in an english semantic judgment task part of a boat sail korean learners made more such errors and chinese learners made them only when the foil shared most of its letters with the target wang et al also wang geva suggested that this was because korean readers transferred their alphabetic reading procedures readily to english yamada argued that instead such a difference could be attributed to a closer phonological relation between korean and english than between chinese and english the fact is that there is no clear basis for choosing among these alternatives and that is a general problem that is predicted by the fit hypothesis beyond the clear demonstration of reading differences across languages and writing systems is the question of whether writing system fit matters for learning to read we began with the observation that learning to read can be difficult compared with learning language do some writing systems make learning to read more difficult than other writing systems or if languages choose writing that fits then should learning to read face more or less similar challenges no matter what the writing system learning to read is often defined as learning to get meaning from print this is fine as far as it goes but it begs the question of what exactly is learned from our perspective a definition proposed by perfetti and zhang is more apt learning to read is learning how one writing system works that is how one writing system encodes one language when that learning is achieved procedurally the child is indeed getting meaning from print with this definition the role of language writing fit can be cast as a meaningful hypothesis learning to read learning how one writing system encodes one language is facilitated to the extent the system is mapped to the language so as to take advantage of its linguistic properties whether this hypothesis is true or even testable is another matter but it at least becomes a sensible question would english be easier to learn as a syllabary instead of an alphabet not according to our analysis in table on the other hand gleitman and rozin showed that children struggling to read standard alphabetic english found some success when english was written as picto syllabary with pictured objects for example a can contributing their pronunciation to a multisyllabic word such as candy however the idea was never that english orthography would become a syllabary for these children but rather that the syllabary could bootstrap their awareness that writing encoded speech sounds japanese provides a natural test of the fit hypothesis because whereas kanji is the traditionally favored system children learn to read using the kana syllabary this strategy implicitly acknowl edges the writing language fit between the japanese language and the syllabary chinese as taught in the mainland and taiwan but not hong kong may seem to do the same thing when it introduces children to reading through alphabetic writing the switch to character reading comes after weeks wu li anderson however this strategy for chinese is not a matter of writing language fit it is less like the japanese use of a syllabary for children and more like the gleitman and rozin use of a syllabary for english it helps introduce the children to the idea that the written word connects to a spoken word and when the alphabetic writing is presented below characters as they are introduced it supports the learning of the character as representing a chinese syllable morpheme the general state of affairs may be that as difficult as reading might be for some children it would be harder if the writing system did not fit the language to some degree beyond this gen eralization is the important fact that the writing language fit is really calibrated not with learning but with efficiency characters are not easier to learn than alphabetic writing even for chinese they are merely more efficiently read once they have been learned at a very high cost in learning greedy algorithms the algorithm uschange in chapter is an example of a greedy strategy at each step the cashier would only consider the largest denomination smaller than or equal to m since the goal was to minimize the number of coins re turned to the customer this seemed like a sensible strategy you would never use five nickels in place of one quarter a generalization of uschange bet terchange also used what seemed like the best option and did not consider any others which is what makes an algorithm greedy unfortunately bet terchange actually returned incorrect results in some cases because of its short sighted notion of good this is a common characteristic of greedy algorithms they often return suboptimal results but take very little time to do so however there are a lucky few greedy algorithms that find optimal rather than suboptimal solutions genome rearrangements waardenburg syndrome is a genetic disorder resulting in hearing loss and pigmentary abnormalities such as two differently colored eyes the disease was named after the dutch ophthalmologist who first noticed that people with two differently colored eyes frequently had hearing problems as well in the early biologists narrowed the search for the gene implicated in waardenburg syndrome to human chromosome but its exact location re mained unknown for some time there was another clue that shed light on the gene associated with waardenburg syndrome that drew attention to chromosome for a long time breeders scrutinized mice for mutants and one of these designated splotch had pigmentary abnormalities like patches of white spots similar to those in humans with waardenburg syndrome through breeding the splotch gene was mapped to one of the mouse chro mouse x chromosome human x chromosome figure transformation of the mouse gene order into the human gene order on the x chromosome only the five longest synteny blocks are shown here mosomes as gene mapping proceeded it became clear that there are groups of genes in mice that appear in the same order as they do in humans these genes are likely to be present in the same order in a common ancestor of humans and mice the ancient mammalian genome in some ways the human genome is just the mouse genome cut into about large genomic fragments called synteny blocks that have been pasted together in a different order both sequences are just two different shufflings of the ancient mam malian genome for example chromosome in humans is built from frag ments that are similar to mouse dna residing on chromosomes and it is no surprise then that finding a gene in mice often leads to clues about the location of the related gene in humans every genome rearrangement results in a change of gene ordering and a series of these rearrangements can alter the genomic architecture of a species analyzing the rearrangement history of mammalian genomes is a challeng ing problem even though a recent analysis of human and mouse genomes implies that fewer than genomic rearrangements have occurred since the divergence of humans and mice approximately million years ago every study of genome rearrangements involves solving the combinatorial puzzle of finding a series of rearrangements that transform one genome into an other figure presents a rearrangement scenario in which the mouse x chro mosome is transformed into the human x chromosome the elementary rearrangement event in this scenario is the flipping of a genomic segment called a reversal or an inversion one can consider other types of evolution ary events but in this book we only consider reversals the most common evolutionary events biologists are interested in the most parsimonious evolutionary scenario that is the scenario involving the smallest number of reversals while there is no guarantee that this scenario represents an actual evolutionary sequence it gives us a lower bound on the number of rearrangements that have occurred and indicates the similarity between two species even for the small number of synteny blocks shown it is not so easy to ver ify that the three evolutionary events in figure represent a shortest series of reversals transforming the mouse gene order into the human gene order on the x chromosome the exhaustive search technique that we presented in the previous chapter would hardly work for rearrangement studies since the number of variants that need to be explored becomes enormous for more than ten synteny blocks below we explore two greedy approaches that work to differing degrees of success sorting by reversals in their simplest form rearrangement events can be modeled by a series of reversals that transform one genome into another the order of genes rather of synteny blocks in a genome can be represented by a extreme conservation of genes on x chromosomes across mammalian species provides an opportunity to study the evolutionary history of x chromosomes independently of the rest of the genomes since the gene content of x chromosomes has barely changed throughout mammalian evolution however the order of genes on x chromosomes has been disrupted several times in other words genes that reside on the x chromosome stay on the x chromosome but their order may change all other chromosomes may exchange genes that is a gene can move from one chromosome to another in fact a sequence of reversals that transforms the x chromosome of mouse into the x chro mosome of man does not even represent an evolutionary sequence since humans are not de scended from the present day mouse however biologists believe that the architecture of the x chromosome in the human mouse ancestor is about the same as the architecture of the human x chromosome a permutation of a sequence of n numbers is just a reordering of that sequence we will always use permutations of consecutive integers for example is a permutation of π πn the order of synteny blocks on the x chromosome in hu mans is represented in figure by while the ordering in mice is a reversal ρ i j has the effect of reversing the order of synteny blocks πiπi πj in effect this transforms into π πi πj πn π ρ i j πi πi πn for example if π then π ρ with this rep resentation of a genome and a rigorous definition of an evolutionary event we are in a position to formulate the computational problem that mimics the biological rearrangement process reversal distance problem given two permutations find a shortest series of reversals that trans forms one permutation into another input permutations π and σ output a series of reversals ρt transforming π into σ i e π ρt σ such that t is minimum we call t the reversal distance between π and σ and write d π σ to denote the reversal distance for a given π and σ in practice one usually selects the second genome order as a gold standard and arbitrarily sets σ to be the identity permutation n the sorting by reversals problem is similar to the reversal distance problem except that it requires only one permutation as input in reality genes and synteny blocks have directionality reflecting whether they reside on the direct strand or the reverse complement strand of the dna in other words the synteny block order in an organism is really represented by a signed permutation however in this section we ignore the directionality of the synteny blocks for simplicity sorting by reversals problem given a permutation find a shortest series of reversals that transforms it into the identity permutation input permutation π output a series of reversals ρt transforming π into the identity permutation such that t is minimum in this case we call t the reversal distance of π and denote it as d π when sorting a permutation π it hardly makes sense to move the already sorted first three elements of π if we define prefix π to be the num ber of already sorted elements of π then a sensible strategy for sorting by reversals is to increase prefix π at every step this approach sorts π in steps generalizing this leads to an algo rithm that sorts a permutation by repeatedly moving its ith element to the ith position simplereversalsort π for i to n j position of element i in π i e πj i if j i i π π ρ i j output π if π is the identity permutation return simplereversalsort is an example of a greedy algorithm that chooses the best reversal at every step however the notion of best here is rather short sighted simply increasing prefix π does not guarantee the smallest number of reversals for example simplereversalsort takes five steps to sort however the same permutation can be sorted in just two steps note the superficial similarity of this algorithm to selectionsort in chapter therefore we can confidently say that simplereversalsort is not a correct algorithm in the strict sense of chapter in fact despite its commonsense appeal simplereversalsort is a terrible algorithm since it takes n steps to sort the permutation π n n even though d π even before biologists faced genome rearrangement problems computer scientists studied the related sorting by prefix reversals problem also known as the pancake flipping problem given an arbitrary permutation π find dpref π which is the minimum number of reversals of the form ρ i sort ing π the pancake flipping problem was inspired by the following real life situation described by the fictitious harry dweighter the chef in our place is sloppy and when he prepares a stack of pan cakes they come out all different sizes therefore when i deliver them to a customer on the way to a table i rearrange them so that the small est winds up on top and so on down to the largest at the bottom by grabbing several from the top and flipping them over repeating this varying the number i flip as many times as necessary if there are n pancakes what is the maximum number of flips that i will ever have to use to rearrange them an analog of simplereversalsort will sort every permutation by at most n prefix reversals for example one can sort by prefix reversals but it is not clear whether there exists an even shorter series of prefix reversals to sort this permutation william gates an undergraduate student at har vard in the mid and christos papadimitriou a professor at harvard in the mid now at berkeley made the first attempt to solve this problem and proved that any permutation can be sorted by at most n prefix reversals however the pancake flipping problem remains unsolved approximation algorithms in chapter we mentioned that for many problems efficient polynomial algorithms are still unknown and unlikely ever to be found for such prob lems computer scientists often find a compromise in approximation algorithms that produce an approximate solution rather than an optimal one the ap proximation ratio of algorithm a on input π is defined as a π where a π is the solution produced by the algorithm a and op t π is the correct op timal solution of the problem the approximation ratio or performance guar antee of algorithm a is defined as its maximum approximation ratio over all inputs of size n that is as max a π π n op t π we assume that a is a minimization algorithm i e an algorithm that attempts to minimize its objective function for maximization algorithms the approx imation ratio is min a π π n op t π in essence an approximation algorithm gives a worst case scenario of just how far off an algorithm output can be from some hypothetical perfect al gorithm the approximation ratio of simplereversalsort is at least n so a biologist has no guarantee that this algorithm comes anywhere close to the correct solution for example if n is this algorithm could return a series of reversals that is as large as times the optimal our goal is approximation algorithms are only relevant to problems that have a numerical objective function like minimizing the number of coins returned to the customer a problem that does not have such an objective function like the partial digest problem does not lend itself to ap proximation algorithms technically an approximation algorithm is not correct in the sense of chapter since there exists some input that returns a suboptimal incorrect output the approximation ratio gives one an idea of just how incorrect the algorithm can be figure breakpoints adjacencies and strips for permutation ex tended by and on the ends strips with more than one element are divided into decreasing strips and increasing strips the boundary between two non consecutive elements in this case and is a breakpoint breakpoints demarcate the boundaries of strips to design approximation algorithms with better performance guarantees for example an algorithm with an approximation ratio of or even better of course an algorithm with an approximation ratio of by definition a correct and optimal algorithm would be the acme of perfection but such al gorithms can be hard to find as of the writing of this book the best known algorithm for sorting by reversals has a performance guarantee of breakpoints a different face of greed we have described a greedy algorithm that attempts to maximize prefix π in every step but any chess player knows that greed often leads to wrong decisions for example the ability to take a queen in a single step is usually a good sign of a trap good chess players use a more sophisticated notion of greed that evaluates a position based on many subtle factors rather than simply on the face value of a piece they can take the problem with simplereversalsort is that prefix π is a naive mea sure of our progress toward the identity permutation and does not accu rately reflect how difficult it is to sort a permutation below we define break points that can be viewed as bottlenecks for sorting by reversals using the number of breakpoints rather than prefix π as the basis of greed leads to a better algorithm for sorting by reversals in the sense that it produces a solution that is closer to the optimal one it will be convenient for us to extend the permutation πn by and πn n on the ends to be clear we do not move or πn during the process of sorting we call a pair of neighboring elements πi and πi for i n an adjacency if πi and πi are consecutive numbers we call the pair a breakpoint if not the permutation in figure has five adjacen cies and and four breakpoints and a permutation on n elements may have as many as n breakpoints e g the permutation on seven elements has eight breakpoints and as few as the identity permutation every breakpoint corre sponds to a pair of elements πi and πi that are neighbors in π but not in the identity permutation in fact the identity permutation is the only per mutation with no breakpoints at all therefore the nonconsecutive elements πi and πi forming a breakpoint must be separated in the process of trans forming π to the identity and we can view sorting by reversals as the process of eliminating breakpoints the observation that every reversal can eliminate at most two breakpoints one on the left end and another on the right end of the reversal immediately implies that d π b π where b π is the number of breakpoints in π the algorithm breakpointreversalsort eliminates as many breakpoints as possible in every step in order to reach the identity permutation breakpointreversalsort π while b π among all reversals choose reversal ρ minimizing b π ρ π π ρ output π return one problem with this algorithm is that it is not clear why breakpointre versalsort is a better approximation algorithm than simplereversal sort moreover it is not even obvious yet that breakpointreversalsort terminates how can we be sure that removing some breakpoints does not introduce others leading to an endless cycle we define a strip in a permutation π as an interval between two consecutive breakpoints that is as any maximal segment without breakpoints see fig ure for example the permutation consists of five strips and strips can be further divided into increasing strips and decreasing strips and single element strips can be con sidered to be either increasing or decreasing but it will be convenient to we remind the reader that we extend permutations by and n on their ends thus intro ducing potential breakpoints in the beginning and in the end define them as decreasing except for elements and n which will always be classified as increasing strips we present the following theorems first to show that endless cycles of breakpoint removal cannot happen and then to show that the approxima tion ratio of the algorithm is while the notion of theorem and proof might seem overly formal for what is at heart a biological problem it is important to consider that we have modeled the biological process in math ematical terms we are proving analytically that the algorithm meets certain expectations this notion of proof without experimentation is very different from what a biologist would view as proof but it is just as important when working in bioinformatics theorem if a permutation π contains a decreasing strip then there is a reversal ρ that decreases the number of breakpoints in π that is b π ρ b π proof among all decreasing strips in π choose the strip containing the smallest element k k for permutation element k in π cannot belong to a decreasing strip since otherwise we would choose a strip ending at k rather than a strip ending at k therefore k belongs to an increasing strip moreover it is easy to see that k terminates this strip for permutation k and is at the right end of the increasing strip therefore elements k and k correspond to two breakpoints one at the end of the decreasing strip ending with k and the other at the end of the increasing strip ending in k reversing the segment between k and k brings them together as in thus reducing the number of breakpoints in π d for example breakpointreversalsort may perform the following four steps when run on the input in order to reduce the number of breakpoints b π b π b π b π b π in this case breakpointreversalsort steadily reduces the number of breakpoints in every step of the algorithm in other cases e g the permu tation without decreasing strips no reversal reduces the number of breakpoints in order to overcome this we can simply find any increasing strip excluding and πn of course and flip it this creates a decreasing strip and we can proceed improvedbreakpointreversalsort π while b π if π has a decreasing strip among all reversals choose reversal ρ minimizing b π ρ else choose a reversal ρ that flips an increasing strip in π π π ρ output π return the theorem below demonstrates that such no progress situations do not happen too often in the course of improvedbreakpointreversalsort in fact the theorem quanitifes exactly how often those situations could possibly occur and provides an approximation ratio guarantee theorem improvedbreakpointreversalsort is an approximation al gorithm with a performance guarantee of at most proof theorem implies that as long as π has a decreasing strip im provedbreakpointreversalsort reduces the number of breakpoints in π on the other hand it is easy to see that if all strips in π are increasing then there might not be a reversal that reduces the number of breakpoints in this case improvedbreakpointreversalsort finds a reversal ρ that reverses an increasing strip in π by reversing an increasing strip ρ cre ates a decreasing strip in π implying that improvedbreakpointreversal sort will be able to reduce the number of strips at the next step therefore for every no progress step improvedbreakpointreversalsort will make progress at the next step which means that improvedbreakpointre versalsort eliminates at least one breakpoint in every two steps in the worst case scenario the number of steps in improvedbreakpointrever salsort is at most π and its approximation ratio is at most π since d π b π improvedbreakpointreversalsort has a performance guar bounded above by π π d d π b π to be clear we are not claiming that improvedbreakpointreversalsort will take four times as long or use four times as much memory as an unknown optimal algorithm we a greedy approach to motif finding in chapter we saw a brute force algorithm to solve the motif finding prob lem with a disappointing running time of o l nt the practical limitation of that algorithm is that we simply cannot run it on biological samples we choose instead to rely on a faster greedy technique even though it is not correct in the sense of chapter and does not result in an algorithm with a good performance guarantee despite the fact that this algorithm is an approximation algorithm with an unknown approximation ratio a popular tool based on this approach developed by gary stormo and gerald hertz in consensus often produces results that are as good as or better than more complicated algorithms greedymotifsearch scans each dna sequence only once once we have scanned a particular sequence i we decide which of its l mer has the best contribution to the partial alignment score score i dn a for the first i sequences and immediately claim that this l mer is part of the alignment the pseudocode is shown below greedymotifsearch dn a t n l bestmotif for to n l for to n l if score dn a score bestmotif dn a bestm bestm bestm bestm for i to t for si to n l if score i dn a score bestmotif i dn a bestm otifi si si bestm otifi return bestmotif are saying that improvedbreakpointreversalsort will return an answer that contains no more than four times as many steps as an optimal answer unfortunately we cannot determine exactly how far from optimal we are for each particular input so we have to rely on this upper bound for the approximation ratio greedymotifsearch first finds the two closest l mers in the sense of hamming distance in sequences and and forms a l seed matrix this stage requires l n l operations at each of the remaining t itera tions greedymotifsearch extends the seed matrix into a matrix with one more row by scanning the ith sequence for i t each of the remaining t sequences and selecting the one l mer that has the maximum score i this amounts to roughly l n l operations in each iteration thus the running time of this algorithm is o lnt which is vastly better than the o lnt of simplemotifsearch or even the o of bruteforcemedi anstring when t is small compared to n greedymotifsearch really behaves as o and the bulk of the time is actually spent locating the l mers from the first two sequences that are the most similar as you can imagine because the sequences are scanned sequentially it is possible to construct input instances where greedymotifsearch will miss the optimal motif one important difference between the popular consen sus motif finding software tool and the algorithm presented here is that consensus can scan the sequences in a random order thereby making it more difficult to construct inputs that elicit worst case behavior another important difference is that consensus saves a large number usually at least of seed matrices at each iteration rather than only the one that greedymotifsearch saves making consensus less likely to miss the optimal solution however no embellishment of this greedy approach will be guaranteed to find an optimal motif notes the analysis of genome rearrangements in molecular biology was pioneered by theodosius dobzhansky and alfred sturtevant who in published a milestone paper presenting a rearrangement scenario for the species of fruit fly in nadeau and taylor estimated that surprisingly few genomic rearrangements about had taken place since the divergence of the human and mouse genomes this estimate made in the pregenomic era and based on a very limited data set comes close to the recent postgenomic estimates based on the comparison of the entire human and mouse dna sequences the computational studies of the reversal distance prob lem were pioneered by david sankoff in the early the greedy algorithm based on breakpoint elimination is from a paper by john ke cecioglu and david sankoff the best currently known algorithm for sorting by reversals has an approximation ratio of and was introduced by piotr berman sridhar hannenhalli and marek karpinski the first algorith mic analysis of the pancake flipping problem was the work of william gates and christos papadimitriou in the greedy consensus algorithm was introduced by gerald hertz and gary stormo and further improved upon in in a later paper by the same authors area go back to the early david sankoff currently holds the ca nada research chair in mathematical genomics at the university of ottawa he studied at mcgill university doing a phd in probability theory with don ald dawson and writing a thesis on sto chastic models for historical linguistics he joined the new centre de recherches mathématiques crm of the university of montreal in and was also a pro fessor in the mathematics and statistics department from he is one of the founding fathers of bioinformatics whose fundamental contributions to the sankoff was trained in mathematics and physics his undergraduate sum mers in the early however were spent in a microbiology lab at the university of toronto helping out with experiments in the field of virology and whiling away evenings and weekends in the library reading biological journals it was exciting and did not require too much background to keep up with the molecular biology literature the watson crick model was not even ten years old the deciphering of the genetic code was still incomplete and mrna was just being discovered with this experience sankoff had no problems communicating some years later with robert j cedergren a biochemist with a visionary interest in applying computers to problems in molecular biology in cedergren asked sankoff to find a way to align rna sequences sankoff knew little of algorithm design and nothing of discrete dynamic programming but as an undergraduate he had effectively used the latter in working out an economics problem matching buyers and sellers the same approach worked with alignment bob and david became hooked on the topic exploring statistical tests for alignment and other problems fortunately before they realized that needleman and wunsch had already published a dynamic programming technique for biological sequence com parison a new question that emerged early in the sankoff and cedergren work was that of multiple alignment and its pertinence to molecular evolution sankoff was already familiar with phylogeny problems from his work on lan guage families and participation in the early numerical taxonomy meetings before the schism between the parsimony promoting cladists led by steve farris and the more statistically oriented systematists combining phylo genetics with sequence comparison led to tree based dynamic programming for multiple alignment phylogenetic problems have cropped up often in sankoff research projects over the following decades sankoff and cedergren also studied rna folding applying several passes of dynamic programming to build energy optimal rna structures they did not find the loop matching reported by daniel kleitman group later integrated into a general widely used algorithm by michael zuker though they eventually made a number of contributions in the in particular to the problem of multiple loops and to simultaneous alignment and folding sankoff says my collaboration with cedergen also ran into its share of dead ends applying multidimensional scaling to ribosome structure did not lead very far efforts to trace the origin of the genetic code through the phy logenetic analyses of trna sequences eventually petered out and an attempt at dynamic programming for consensus folding of proteins was a flop the early and mid were nevertheless a highly productive time for sankoff he was also working on probabilistic analysis of grammatical vari ation in natural languages on game theory models for electoral processes and various applied mathematics projects in archaeology geography and physics he got peter sellers interested in sequence comparison sellers later attracted attention by converting the longest common subsequence lcs formulation to the edit distance version sankoff collaborated with promi nent mathematician vaclav chvatal on the expected length of the lcs of two random sequences for which they derived upper and lower bounds sev eral generations of probabilists have contributed to narrowing these bounds sankoff says evolutionary biologists walter fitch and steve farris spent sabbaticals with me at the crm as did computer scientist bill day generously adding my name to a series of papers establishing the hardness of var ious phylogeny problems most importantly the parsimony problem in sankoff became a fellow of the new evolutionary biology pro gram of the canadian institute for advanced research ciar at the very first meeting of the ciar program he was inspired by a talk by monique turmel on the comparison of chloroplast genomes from two species of al gae this led sankoff to the comparative genomics genome rearrangement track that has been his main research line ever since originally he took a probabilistic approach but within a year or two he was trying to develop algorithms and programs for reversal distance a phylogeny based on the re versal distances among sixteen mitochondrial genomes proved that a strong phylogenetic signal can be conserved in the gene order of even a miniscule genome across many hundreds of millions of years sankoff says the network of fellows and scholars of the ciar program including bob cedergren ford doolittle franz lang mike gray brian golding mike zuker claude lemieux and others across canada and a stellar group of international advisors such as russ doolittle michael smith marcus feldman wally gilbert and associates mike waterman joe felsenstein mike steel and many others became my virtual home department a source of intellectual support knowledge and expe rience across multiple disciplines and a sounding board for the latest ideas my comparative genomics research received two key boosts in the one was the sustained collaboration of a series of outstanding stu dents and postdocs guillaume leduc vincent ferretti john kece cioglu mathieu blanchette nadia el mabrouk and david bryant the second was my meeting joe nadeau i already knew his seminal pa per with taylor on estimating the number of conserved linkage seg ments and realized that our interests coincided perfectly while our backgrounds were complementary when nadeau showed up in montreal for a short lived appointment in the human genetics department at mcgill it took no more than an hour for him and sankoff to get started on a major collaborative project they refor mulated the nadeau taylor approach in terms of gene content data freeing it from physical or genetic distance measurements the resulting simpler model allowed them to thoroughly explore the mathematical properties of the nadeau taylor model and to experiment with the consequences of devi ating from it the synergy between the algorithmic and probabilistic aspects of com parative genomics has become basic to how sankoff understands evolution the algorithmic is an ambitious attempt at deep inference based on heavy assumptions and the sophisticated but inflexible mathematics they enable the probabilistic is more descriptive and less explicitly revelatory of histor ical process but the models based on statistics are easily generalized their hypotheses weakened or strengthened and their robustness ascertained in sankoff view it is the playing out of this dialectic that makes the field of whole genome comparison the most interesting topic of research today and for the near future my approach to research is not highly planned not that i don t have a vision about the general direction in which to go but i have no spe cific set of tools that i apply as a matter of course only an intuition about what type of method or model what database or display might be helpful when i am lucky i can proceed from one small epiphany to another working out some of the details each time until some clear story emerges whether this involves stochastic processes combina torial optimization or differential equations is secondary it is the bi ology of the problem that drives its mathematical formulation i am rarely motivated to research well studied problems instead i find my self confronting new problems in relatively unstudied areas alignment was not a burning preoccupation with biologists or computer scientists when i started working on it neither was genome rearrangement fif teen years later i am quite pleased though sometimes bemused by the veritable tidal wave of computational biologists and bioinformaticians who have inundated the field where there were only a few isolated researchers thirty or even twenty years ago problems problem suppose you have a maximization algorithm that has an approximation ratio of when run on some input π π what can you say about the true correct answer op t op t π op t op t op t op t op t op t problem what is the approximation ratio of the betterchange algorithm problem design an approximation algorithm for the pancake flipping problem what is its approximation ratio problem perform the breakpointreversalsort algorithm with π and show all intermediate permutations break ties arbitrarily since breakpointreversal sort is an approximation algorithm there may be a sequence of reversals that is shorter than the one found by breakpointreversalsort could you find such a sequence of reversals do you know if it is the shortest possible sequence of rever sals problem find a permutation with no decreasing strips for which there exists a reversal that reduces the number of breakpoints problem can you find a permutation for which breakpointreversalsort produces four times as many reversals than the optimal solution of the reversal sorting problem a dna molecule is not always shaped like a line segment some simple organisms have a circular dna molecule as a genome where the molecule has no beginning and no end these circular genomes can be visualized as a sequence of integers written along the perimeter of a circle two circular sequences would be considered equivalent if you could rotate one of the circles and get the same sequence written on the other problem devise an approximation algorithm to sort a circular genome by reversals i e trans form it to the identity circular permutation evaluate the algorithm performance guarantee problem devise a better algorithm i e one with a better approximation ratio for the sorting by reversals problem the swap sorting of permutation π is a transformation of π into the identity permutation by exchanges of adjacent elements for example 1234 is a three step swap sorting of permutation problem design an algorithm for swap sorting that uses the minimum number of swaps to sort a permutation problem design an algorithm for swap sorting that uses the minimum number of swaps to sort a circular permutation problem how many permutations on n elements have a single breakpoint how many per mutations have exactly two breakpoints how many permutations have exactly three breakpoints given permutations π and σ a breakpoint between π and σ is defined as a pair of adjacent elements πi and πi in π that are separated in σ for example if π and σ then and in π form a breakpoint between π and σ since and are separated in σ the number of breakpoints between π and σ is three and while the number of breakpoints between σ and π is also three and problem prove that the number of breakpoints between π and σ equals the number of break points between σ and π problem given permutations 143256 and compute the num ber of breakpoints between and and and and problem given the three permutations and from the previous problem find an an i cestral permutation σ which minimizes the total breakpoint distance i br π σ between all three genomes and σ br πi σ is the number of breakpoints between πi and σ problem given three permutations and from the previous problem find an ancestral i permutation σ which minimizes the total reversal distance i d π σ between all three genomes and σ analysis of genome rearrangements in multiple genomes corresponds to the following multiple breakpoint distance problem given a set of permutations πk find an ancestral permu i i tation σ such that i k br π σ is minimal where br π σ is the number of breakpoints between πi and σ problem design a greedy algorithm for the multiple breakpoint distance problem and evalu ate its approximation ratio problem alice and bob have been assigned the task of implementing the breakpointrever salsort approximation algorithm bob wants to get home early so he decides to naively implement the algorithm without putting any thought into performance improvements what is the run ning time of his program alice makes some changes to the algorithm and claims her algorithm achieves the same approximation ratio as bob and runs in time o give the pseu docode for alice algorithm not to be outdone bob gets a copy of alice algorithm and makes an improve ment of his own he claims that in the case where every strip is increasing he can guarantee that there will be a decreasing strip in each of the next two steps rather than one as in breakpointreversalsort bob believes that this will give his new algorithm a better approximation ratio than the previous algorithms what is bob improvement and what approximation ratio does it achieve problem design an input for the greedymotifsearch algorithm that causes the algorithm to output an incorrect result that is create a sample that has a strong pattern that is missed because of the greedy nature of the algorithm if optimalscore is the score of the strongest motif in the sample and greedyscore is the score returned by greedy motifsearch how large can optimalscore greedyscore be dynamic programming algorithms we introduced dynamic programming in chapter with the rocks prob lem while the rocks problem does not appear to be related to bioinfor matics the algorithm that we described is a computational twin of a popu lar alignment algorithm for sequence comparison dynamic programming provides a framework for understanding dna sequence comparison algo rithms many of which have been used by biologists to make important in ferences about gene function and evolutionary history we will also apply dynamic programming to gene finding and other bioinformatics problems the power of dna sequence comparison after a new gene is found biologists usually have no idea about its func tion a common approach to inferring a newly sequenced gene function is to find similarities with genes of known function a striking example of such a biological discovery made through a similarity search happened in when scientists used a simple computational technique to compare the newly discovered cancer causing ν sis oncogene with all at the time known genes to their astonishment the cancer causing gene matched a normal gene involved in growth and development called platelet derived growth factor pdgf after discovering this similarity scientists became suspicious that cancer might be caused by a normal growth gene being switched on at the wrong time in essence a good gene doing the right thing at the wrong time oncogenes are genes in viruses that cause a cancer like transformation of infected cells onco gene ν sis in the simian sarcoma virus causes uncontrolled cell growth and leads to cancer in monkeys the seemingly unrelated growth factor pdgf is a protein that stimulates cell growth another example of a successful similarity search was the discovery of the cystic fibrosis gene cystic fibrosis is a fatal disease associated with abnormal secretions and is diagnosed in children at a rate of in a defective gene causes the body to produce abnormally thick mucus that clogs the lungs and leads to lifethreatening lung infections more than million americans are unknowing and symptomless carriers of the defective cystic fibrosis gene each time two carriers have a child there is a chance that the child will have cystic fibrosis in the search for the cystic fibrosis gene was narrowed to a region of million nucleotides on the chromosome but the exact location of the gene remained unknown when the area around the cystic fibrosis gene was sequenced biologists compared the region against a database of all known genes and discovered similarities between some segment within this region and a gene that had already been discovered and was known to code for adenosine triphosphate atp binding proteins these proteins span the cell membrane multiple times as part of the ion transport channel this seemed a plausible function for a cystic fibrosis gene given the fact that the disease involves sweat secretions with abnormally high sodium content as a result the similarity analysis shed light on a damaged mechanism in faulty cystic fibrosis genes establishing a link between cancer causing genes and normal growth genes and elucidating the nature of cystic fibrosis were only the first success stories in sequence comparison many applications of sequence comparison algo rithms quickly followed and today bioinformatics approaches are among the dominant techniques for the discovery of gene function this chapter describes algorithms that allow biologists to reveal the simi larity between different dna sequences however we will first show how dynamic programming can yield a faster algorithm to solve the change prob lem the change problem revisited we introduced the change problem in chapter as the problem of changing an amount of money m into the smallest number of coins from denomina tions c cd we showed that the naive greedy solution used by cashiers everywhere is not actually a correct solution to this problem and ended with a correct though slow brute force algorithm we will con atp binding proteins provide energy for many reactions in the cell sider a slightly modified version of the change problem in which we do not concern ourselves with the actual combination of coins that make up the optimal change solution instead we only calculate the smallest number of coins needed it is easy to modify this algorithm to also return the coin combination that achieves that number suppose you need to make change for cents and the only coin denomi nations available are and cents the best combination for cents will be one of the following the best combination for cents plus a cent coin the best combination for cents plus a cent coin the best combination for cents plus a cent coin for cents the best combination would be the smallest of the above three choices the same logic applies to cents best of or cents and so on fig if bestn umcoinsm is the smallest number of coins needed to change m cents then the following recurrence relation holds bestn umcoinsm min bestn umcoinsm bestn umcoinsm bestn umcoins m in the more general case of d denominations c cd bestn umcoinsm min bestn umcoinsm bestn umcoinsm bestn umcoinsm cd this recurrence motivates the following algorithm figure the relationships between optimal solutions in the change problem the smallest number of coins for cents depends on the smallest number of coins for and cents the smallest number of coins for cents depends on the smallest number of coins for and cents and so on recursivechange m c d if m return bestn umcoins for i to d if m ci numcoins recursivechange m ci c d if numcoins bestn umcoins bestn umcoins numcoins return bestn umcoins the sequence of calls that recursivechange makes has a feature in com mon with the sequence of calls made by recursivefibonacci namely that recursivechange recalculates the optimal coin combination for a given amount of money repeatedly for example the optimal coin combination for cents is recomputed repeatedly nine times over and over as and the optimal coin combination for cents will be recomputed billions of times rendering recursivechange impractical to improve recursivechange we can use the same strategy as we did for the fibonacci problem all we really need to do is use the fact that the solution for m relies on solutions for m m and so on and then reverse the order in which we solve the problem this allows us to lever age previously computed solutions to form solutions to larger problems and avoid all this recomputation instead of trying to find the minimum number of coins to change m cents we attempt the superficially harder task of doing this for each amount of money m from to m this appears to require more work but in fact it simplifies matters the following algorithm with running time o m d cal culates bestn umcoinsm for increasing values of m this works because the best number of coins for some value m depends only on values less than m dpchange m c d bestn for m to m bestn umcoinsm for i to d if m ci if bestn umcoinsm ci bestn umcoinsm bestn umcoinsm bestn umcoinsm ci return bestn umcoinsm the key difference between recursivechange and dpchange is that the first makes d recursive calls to compute the best change for m and each of these calls requires a lot of work while the second analyzes the d already precomputed values to almost instantly compute the new one as surprising as it may sound simply reversing the order of computations in figure makes a dramatic difference in efficiency fig we stress again the difference between the complexity of a problem and the complexity of an algorithm in particular we initially showed an o m d algorithm to solve the change problem and there did not appear to be any easy way to remedy this situation yet the dpchange algorithm provides a simple o m d solution conversely a minor modification of the change problem renders the problem very difficult suppose you had a limited num ber of each denomination and needed to change m cents using no more than the provided supply of each coin since you have fewer possible choices in figure the solution for cents bestn depends on cents cents and cent but the smallest number of coins can be obtained by computing bestn umcoinsm for m this new problem it would seem to require even less time than the original change problem and that a minor modification to dpchange would work however this is not the case and this problem turns out to be very difficult the manhattan tourist problem we will further illustrate dynamic programming with a surprisingly useful toy problem called the manhattan tourist problem and then build on this intuition to describe dna sequence alignment imagine a sightseeing tour in the borough of manhattan in new york city where a group of tourists are determined to walk from the corner of street and avenue to the chrysler building at street and lexing ton avenue there are many attractions along the way but assume for the moment that the tourists want to see as many attractions as possible the tourists are allowed to move either to the south or to the east but even so they can choose from many different paths exactly how many is left as a problem at the end of the chapter the upper path in figure will take the tourists to the museum of modern art but they will have to miss times square the bottom path will allow the tourists to see times square but they will have to miss the museum of modern art the map above can also be represented as a gridlike structure figure with the numbers next to each line called weights showing the number of attractions on every block the tourists must decide among the many possi ble paths between the northwesternmost point called the source vertex and the southeasternmost point called the sink vertex the weight of a path from the source to the sink is simply the sum of weights of its edges or the overall number of attractions we will refer to this kind of construct as a graph the intersections of streets we will call vertices and the streets themselves will be edges and have a weight associated with them we assume that horizontal edges in the graph are oriented to the east like while vertical edges are ori ented to the south like a path is a continuous sequence of edges and the length of a path is the sum of the edge weights in the path a more detailed discussion of graphs can be found in chapter although the upper path in figure is better than the bottom one in the sense that the tourists will see more attractions it is not immediately clear if there is an even better path in the grid the manhattan tourist problem is to find the path with the maximum number of attractions that is a longest path we emphasize that the length of paths in the graph represent the overall number of attractions on this path and has nothing to do with the real length of the path in miles that is the distance the tourists travel there are many interesting museums and architectural landmarks in manhattan however it is impossible to please everyone so one can change the relative importance of the types of attractions by modulating the weights on the edges in the graph this flexibility in assigning weights will become important when we discuss scoring matrices for sequence comparison a path of maximum overall weight in the grid manhattan tourist problem find a longest path in a weighted grid input a weighted grid g with two distinguished vertices a source and a sink output a longest path in g from source to sink note that since the tourists only move south and east any grid positions west or north of the source are unusable similarly any grid positions south or east of the sink are unusable so we can simply say that the source vertex is at and that the sink vertex at n m defines the southeasternmost corner of the grid in figure n m but n does not always have to equal m we will use the grid shown in figure rather than the one corresponding to the map of manhattan in figure so that you can see a nontrivial example of this problem the brute force approach to the manhattan tourist problem is to search among all paths in the grid for the longest path but this is not an option for even a moderately large grid inspired by the previous chapter you may be tempted to use a greedy strategy for example a sensible greedy strat egy would be to choose between two possible directions south or east by comparing how many attractions tourists would see if they moved one block south instead of moving one block east this greedy strategy may provide re warding sightseeing experience in the beginning but a few blocks later may bring you to an area of manhattan you really do not want to be in in fact no known greedy strategy for the manhattan tourist problem provides an optimal solution to the problem had we followed the obvious greedy al gorithm we would have chosen the following path corresponding to twenty three attractions we will show that the optimal number is in fact thirty four figure a city somewhat like manhattan laid out on a grid with one way streets you may travel only to the east or to the south and you are currently at the north westernmost point source and need to travel to the southeasternmost point sink your goal is to visit as many attractions as possible figure manhattan represented as a graph with weighted edges instead of solving the manhattan tourist problem directly that is finding the longest path from source to sink n m we solve a more general problem find the longest path from source to an arbitrary vertex i j with i n j m we will denote the length of such a best path as si j noticing that sn m is the weight of the path that represents the solution to the manhattan tourist problem if we only care about the longest path between and n m the manhattan tourist problem then we have to answer one question namely what is the best way to get from source to sink if we solve the general problem then we have to answer n m questions what is the best way to get from source to anywhere at first glance it looks like we have just created n m different problems computing i j with i n and j m instead of a single one computing sn m but the fact that solving the more general problem is as easy as solving the manhattan tourist problem is the basis of dynamic programming note that dpchange also generalized the problems that it solves by finding the optimal number of coins for all values less than or equal to m finding j for j m is not hard since in this case the tourists do not have any flexibility in their choice of path by moving strictly to the east the weight of the path j is the sum of weights of the first j city blocks similarly si is also easy to compute for i n since the tourists move only to the south now that we have figured out how to compute and we can com pute the tourists can arrive at in only two ways either by trav eling south from or east from the weight of each of these paths is weight of the edge block between and weight of the edge block between and since the goal is to find the longest path to in this case we choose the larger of the above two quantities and note that since there are no other ways to get to grid position we have found the longest path from to we have just found similar logic applies to and then to and so on once we have calculated si for all i we can calculate si for all i once we have calculated si for all i we can use the same idea to calculate si for all i and so on for example we can calculate as follows max weight of the edge between and weight of the edge between and in general having the entire column j allows us to compute the next whole column j the observation that the only way to get to the intersection at i j is either by moving south from intersection i j or by moving east from the intersection i j leads to the following recurrence max si j weight of the edge between i j and i j si j weight of the edge between i j and i j this recurrence allows us to compute every score si j in a single sweep of the grid the algorithm manhattantourist implements this procedure here w is a two dimensional array representing the weights of the grid edges that run north to south and w is a two dimensional array representing the weights of the grid edges that run west to east that is wi j is the weight of the edge between i j and i j and wi j is the weight of the edge between i j and i j manhattantourist w w n m for i to n si si wi for j to m j j j for i to n for j to m si j max return sn m si j wi j si j wi j lines through set up the initial conditions on the matrix and line cor responds to the recurrence that allows us to fill in later table entries based on earlier ones most of the dynamic programming algorithms we will develop in the context of dna sequence comparison will look just like manhat tantourist with only minor changes we will generally just arrive at a recurrence like line and call it an algorithm with the understanding that the actual implementation will be similar to manhattantourist many problems in bioinformatics can be solved efficiently by the applica tion of the dynamic programming technique once they are cast as traveling in a manhattan like grid for example development of new sequence com parison algorithms often amounts to building an appropriate manhattan that adequately models the specifics of a particular biological problem and by defining the block weights that reflect the costs of mutations from one dna sequence to another manhattantourist computes the length of the longest path in the grid but does not give the path itself in section we will describe a minor modification to the algorithm that returns not only the optimal length but also the optimal path figure a city somewhat more like manhattan than figure with the compli cating issue of a street that runs diagonally across the grid broadway cuts across several blocks in the case of the manhattan tourist problem it changes the optimal path the optimal path in this new city has six attractions instead of five unfortunately manhattan is not a perfectly regular grid broadway cuts across the borough figure we would like to solve a generalization of the manhattan tourist problem for the case in which the street map is not a regular rectangular grid in this case one can model any city map as a graph with vertices corresponding to the intersections of streets and edges corresponding to the intervals of streets between the intersections for the sake of simplicity we assume that the city blocks correspond to directed edges so that the tourist can move only in the direction of the edge and that the resulting graph has no directed cycles such graphs are called directed acyclic graphs or dags we assume that every edge has an associated weight e g the number of attractions and represent a graph g as a pair of two sets v for vertices and e for edges g v e we number vertices from to v with a single integer rather than a row column pair as in the manhattan problem this does not change the generic dynamic programming algorithm other than in notation but it allows us to represent imperfect grids an edge from e can be specified in terms of its origin vertex u and its destination vertex v as u v the following problem is simply a generalization of the manhattan tourist problem that is able to deal with arbitrary dags rather than with perfect grids longest path in a dag problem find a longest path between two vertices in a weighted dag input a weighted dag g with source and sink vertices output a longest path in g from source to sink not surprisingly the longest path in a dag problem can also be solved by dynamic programming at every vertex there may be multiple edges that flow in and multiple edges that flow out in the city analogy any intersection may have multiple one way streets leading in and some other number of one way streets exiting we will call the number of edges entering a vertex i e the number of inbound streets the indegree of the vertex i e intersection and the number of edges leaving a vertex i e the number of outbound streets the outdegree of the vertex in the nicely regular case of the manhattan problem most vertices had a directed cycle is a path from a vertex back to itself that respects the directions of edges if the resulting graph contained a cycle a tourist could start walking along this cycle revisiting the same attractions many times in this case there is no best solution since a tourist may increase the number of visited attractions indefinitely figure a graph with six vertices the vertex v has indegree and outdegree the vertices and are all predecessors of v and and are successors of v indegree and outdegree except for the vertices along the boundaries of the grid in the more general dag problem a vertex can have an arbitrary indegree and outdegree we will call u a predecessor to vertex v if u v e in other words a predecessor of a vertex is any vertex that can be reached by traveling backwards along an inbound edge clearly if v has indegree k it has k predecessors suppose a vertex v has indegree and the set of predecessors of v is figure the longest path to v can be computed as follows weight of edge from to v weight of edge from to v in general one can imagine a rather hectic city plan but the recurrence relation remains simple with the score sv of the vertex v defined as follows sv max u p redecessors v su weight of edge from u to v here p redecessors v is the set of all vertices u such that u is a predecessor of v since every edge participates in only a single recurrence the running figure the dressing in the morning problem represented by a dag some of us have more trouble than others time of the algorithm is defined by the number of edges in the graph the one hitch to this plan for solving the longest path problem in a dag is that one must decide on the order in which to visit the vertices while computing this ordering is important since by the time vertex v is analyzed the values su for all its predecessors must have been computed three popular strategies for exploring the perfect grid are displayed in figure column by column row by row and diagonal by diagonal these exploration strategies correspond to different topological orderings of the dag corresponding to the perfect grid an ordering of vertices vn of a dag is called topological if every edge vi vj of the dag connects a vertex with a smaller index to a vertex with a larger index that is i j figure represents a dag that corresponds to a problem that we each face every morning every dag has a topological ordering fig a problem at the end of this chapter asks you to prove this fact a graph with vertex set v can have at most v edges but graphs arising in sequence com parison are usually sparse with many fewer edges figure two different ways of getting dressed in the morning corresponding to two different topological orderings of the graph in figure figure three different strategies for filling in a dynamic programming array the first fills in the array column by column earlier columns are filled in before later ones the second fills in the array row by row the third method fills array entries along the diagonals and is useful in parallel computation edit distance and alignments so far we have been vague about what we mean by sequence similarity or distance between dna sequences hamming distance introduced in chapter while important in computer science is not typically used to com pare dna or protein sequences the hamming distance calculation rigidly assumes that the ith symbol of one sequence is already aligned against the ith symbol of the other however it is often the case that the ith symbol in one sequence corresponds to a symbol at a different and unknown position in the other for example mutation in dna is an evolutionary process dna replication errors cause substitutions insertions and deletions of nu cleotides leading to edited dna texts since dna sequences are subject to insertions and deletions biologists rarely have the luxury of knowing in advance whether the ith symbol in one dna sequence corresponds to the ith symbol in the other as figure a shows while strings atatatat and tatatata are very different from the perspective of hamming distance they become very simi lar if one simply moves the second string over one place to align the i st letter in atatatat against the ith letter in tatatata for i strings atatatat and tataat present another example with more subtle similarities figure b reveals these similarities by aligning position in atatatat against position in tataat other pairs of aligned positions are against against against against and against positions and in atatatat remain unaligned in vladimir levenshtein introduced the notion of the edit distance between two strings as the minimum number of editing operations needed to transform one string into another where the edit operations are insertion of a symbol deletion of a symbol and substitution of one symbol for another for example tgcatat can be transformed into atccgat with five editing operations shown in figure this implies that the edit distance between tgcatat and atccgat is at most actually the edit distance between them is because you can transform one to the other with one move fewer as in figure unlike hamming distance edit distance allows one to compare strings of different lengths oddly levenshtein introduced the definition of edit dis tance but never described an algorithm for actually finding the edit distance between two strings this algorithm has been discovered and rediscovered many times in applications ranging from automated speech recognition to obviously molecular biology although the details of the algorithms are a t a t a t a t t a t a t a t a a alignment of atatatat against tatatata a t a t a t a t t a t a a t b alignment of atatatat against tataat figure alignment of atatatat against tatatata and of atatatat against tataat tgcatat tgcata tgcat atgcat atccat atccgat delete last t delete last a insert a at the front substitute c for g in the third position insert a g before the last a figure five edit operations can take tgcatat into atccgat slightly different across the various applications they are all based on dy namic programming the alignment of the strings v of n characters and w of m characters with m not necessarily the same as n is a two row matrix such that the first row contains the characters of v in order while the second row contains the characters of w in order where spaces may be interspersed throughout the strings in different places as a result the characters in each string appear in order though not necessarily adjacently we also assume that no column tgcatat atgcatat atgcaat atgcgat atccgat insert a at the front delete t in the sixth position substitute g for a in the fifth position substitute c for g in the third position figure four edit operations can also take tgcatat into atccgat of the alignment matrix contains spaces in both rows so that the alignment may have at most n m columns columns that contain the same letter in both rows are called matches while columns containing different letters are called mismatches the columns of the alignment containing one space are called indels with the columns con taining a space in the top row called insertions and the columns with a space in the bottom row deletions the alignment shown in figure top has five matches zero mismatches and four indels the number of matches plus the number of mismatches plus the number of indels is equal to the length of the alignment matrix and must be smaller than n m each of the two rows in the alignment matrix is represented as a string interspersed by space symbols for example at gttat is a represen tation of the row corresponding to v atgttat while atcgt a c is a representation of the row corresponding to w atcgtac another way to represent the row at gttat is which shows the number of symbols of v present up to a given position similarly atcgt a c is rep resented as when both rows of an alignment are represented in this way fig top the resulting matrix is each column in this matrix is a coordinate in a two dimensional n m grid the entire alignment is simply a path from to n m in that grid again see figure this grid is similar to the manhattan grid that we introduced earlier where each entry in the grid looks like a city block the main difference is that here we can move along the diagonal we can construct a graph this time called the edit graph by introducing a vertex for every intersection of streets in the grid shown in figure the edit graph will aid us in calculating the edit distance every alignment corresponds to a path in the edit graph and every path in the edit graph corresponds to an alignment where every edge in the path corresponds to one column in the alignment fig diagonal edges in the path that end at vertex i j in the graph correspond to the column vi wj horizontal edges correspond to wj and vertical edges correspond to vi the alignment above can be drawn as follows analyzing the merit of an alignment is equivalent to analyzing the merit of the corresponding path in the edit graph given any two strings there are a large number of different alignment matrices and corresponding paths in the edit graph some of these have a surplus of mismatches and indels and a small number of matches while others have many matches and few indels and mismatches to determine the relative merits of one alignment over another we rely on the notion of a scoring function which takes as input an alignment matrix or equivalently a path in the edit graph and produces a score that determines the goodness of the alignment there are a variety of scoring functions that we could use but we want one that gives higher scores to alignments with more matches the simplest functions score a column as a positive number if both letters are the same and as a negative number if the two letters are different the score for the whole alignment is the sum of the individual column scores this scoring scheme amounts to v a t g t t a t w a t c g t a c w a t c g t a c v a t g t t a t a t g t t a t a t c g t a c figure an alignment grid for v atgttat and w atcgtac every align ment corresponds to a path in the alignment grid from to n m and every path from to n m in the alignment grid corresponds to an alignment assigning weights to the edges in the edit graph by choosing different scoring functions we can solve different string com parison problems if we choose the very simple scoring function of for a match otherwise then the problem becomes that of finding the longest common subsequence between two strings which is discussed below be fore describing how to calculate levenshtein edit distance we develop the longest common subsequence problem as a warm up longest common subsequences the simplest form of a sequence similarity analysis is the longest common subsequence lcs problem where we eliminate the operation of substitu tion and allow only insertions and deletions a subsequence of a string v is simply an ordered sequence of characters not necessarily consecutive from v for example if v attgcta then agca and atta are subse quences of v whereas tgtt and tcg are not a common subsequence of two strings is a subsequence of both of them formally we define the com mon subsequence of strings v vn and w wm as a sequence of positions in v ik n and a sequence of positions in w jk m such that the symbols at the corresponding positions in v and w coincide vit wjt for t k for example tcta is a common to both atctgat and tgcata although there are typically many common subsequences between two strings v and w some of which are longer than others it is not immedi ately obvious how to find the longest one if we let v w be the length of the longest common subsequence of v and w then the edit distance be tween v and w under the assumption that only insertions and deletions are allowed is d v w n m v w and corresponds to the mini the difference between a subsequence and a substring is that a substring consists only of con secutive characters from v while a subsequence may pick and choose characters from v as long as their ordering is preserved g a t a t c t g a t c t a computing similarity v w computing distance d v w v and w have a subsequence tcta in common v can be transformed into w by deleting a g t and inserting g a alignment a t c t g a t t g c a t a figure dynamic programming algorithm for computing the longest common subsequence mum number of insertions and deletions needed to transform v into w fig ure bottom presents an lcs of length for the strings v atctgat and w tgcata and a shortest sequence of two insertions and three dele tions transforming v into w shown by in the figure the lcs problem follows longest common subsequence problem find the longest subsequence common to two strings input two strings v and w output the longest common subsequence of v and w what do the lcs problem and the manhattan tourist problem have in common every common subsequence corresponds to an alignment with no e t g c a t a e a t c t g a t figure an lcs edit graph mismatches this can be obtained simply by removing all diagonal edges from the edit graph whose characters do not match thus transforming it into a graph like that shown in figure we further illustrate the relationship between the manhattan tourist problem and the lcs problem by showing that these two problems lead to very similar recurrences define si j to be the length of an lcs between vi the i prefix of v and wj the j prefix of w clearly si j for all i n and j m one can see that si j satisfies the following recurrence si j max si j si j if vi wj the first term corresponds to the case when vi is not present in the lcs of the i prefix of v and j prefix of w this is a deletion of vi the second term corresponds to the case when wj is not present in this lcs this is an insertion of wj and the third term corresponds to the case when both vi and wj are present in the lcs vi matches wj note that one can rewrite these recurrences by adding some zeros here and there as si j if v w i j i j this recurrence for the lcs computation is like the recurrence given at the end of the section if we were to build a particularly gnarly version of manhattan and gave horizontal and vertical edges weights of and set the weights of diagonal matching edges equal to as in figure in the following we use to represent our dynamic programming table the data structure that we use to fill in the dynamic programming recur rence the length of an lcs between v and w can be read from the element n m of the dynamic programming table but to reconstruct the lcs from the dynamic programming table one must keep some additional informa tion about which of the three quantities si j si j or si j corre sponds to the maximum in the recurrence for si j the following algorithm achieves this goal by introducing backtracking pointers that take one of the three values or these specify which of the above three cases holds and are stored in a two dimensional array b see figure lcs v w for i to n si for j to m s0 j for i to n for j to m si j max ii si j si j si j if vi wj if si j si j return sn m b ii if i j si j the following recursive program prints out the longest common subse quence using the information stored in b the initial invocation that prints the solution to the problem is printlcs b v n m printlcs b v i j if i or j return if bi j ii printlcs b v i j print vi else if bi j ii printlcs b v i j else printlcs b v i j the dynamic programming table in figure left presents the compu tation of the similarity score v w between v and w while the table on the right presents the computation of the edit distance between v and w under the assumption that insertions and deletions are the only allowed op erations the edit distance d v w is computed according to the initial con ditions di i j j for all i n and j m and the following recurrence di j min di j di j i j i j global sequence alignment the lcs problem corresponds to a rather restrictive scoring that awards for matches and does not penalize indels to generalize scoring we extend the k letter alphabet a to include the gap character and consider an arbitrary k k scoring matrix δ where k is typically or depending on the type of sequences dna or protein one is analyzing the score of the column y in the alignment is δ x y and the alignment score is defined as the sum of the scores of the columns in this way we can take into account scoring of mismatches and indels in the alignment rather than choosing a particular scoring matrix and then resolving a restated alignment problem we will pose a general global alignment problem that takes the scoring matrix as input global alignment problem find the best alignment between two strings under a given scoring matrix input strings v w and a scoring matrix δ output an alignment of v and w whose score as defined by the matrix δ is maximal among all possible alignments of v and w the corresponding recurrence for the score si j of an optimal alignment between the i prefix of v and j prefix of w is as follows si j max si j δ vi si j δ wj δ v w i j i j when mismatches are penalized by some constant µ indels are penal ized by some other constant σ and matches are rewarded with the resulting score is matches µ mismatches σ indels the corresponding recurrence can be rewritten as si j σ max si j σ si j µ if vi i wj si j if vi wj we can again store similar backtracking pointer information while cal culating the dynamic programming table and from this reconstruct the align ment we remark that the lcs problem is the global alignment problem with the parameters µ σ or equivalently µ σ scoring alignments while the scoring matrices for dna sequence comparison are usually de fined only by the parameters µ mismatch penalty and σ indel penalty scoring matrices for sequences in the amino acid alphabet of proteins are quite involved the common matrices for protein sequence comparison point accepted mutations pam and block substitution blosum reflect the frequency with which amino acid x replaces amino acid y in evolutionarily related sequences random mutations of the nucleotide sequence within a gene may change the amino acid sequence of the corresponding protein some of these muta tions do not drastically alter the protein structure but others do and impair the protein ability to function while the former mutations usually do not affect the fitness of the organism the latter often do therefore some amino acid substitutions are commonly found throughout the process of molecu lar evolution and others are rare asn asp glu and ser are the most mutable amino acids while cys and trp are the least mutable for exam ple the probability that ser mutates into phe is roughly three times greater than the probability that trp mutates into phe knowledge of the types of changes that are most and least common in molecular evolution allows biologists to construct the amino acid scoring matrices and to produce bio logically adequate sequence alignments as a result in contrast to nucleotide sequence comparison the optimal alignments of amino acid sequences may have very few matches if any but still represent biologically adequate align ments the entry of amino acid scoring matrix δ i j usually reflects how often the amino acid i substitutes the amino acid j in the alignments of re lated protein sequences if one is provided with a large set of alignments of related sequences then computing δ i j simply amounts to counting how many times the amino acid i is aligned with amino acid j a minor compli cation is that to build this set of biologically adequate alignments one needs to know the scoring matrix fortunately in many cases the alignment of very similar sequences is so obvious that it can be constructed even without a scoring matrix thus resolving this predicament for example if proteins are identical even a naive scoring matrix e g a matrix that gives pre mium for matches and penalties for mismatches and indels would do the job after these obvious alignments are constructed they can be used to compute a scoring matrix δ that can be used iteratively to construct less obvious alignments this simplified description hides subtle details that are important in the construction of scoring matrices the probability of ser mutating into phe in proteins that diverged million years ago e g related proteins in mouse and rat is smaller than the probability of the ser phe mutation in pro teins that diverged million years ago e g related proteins in mouse and human this observation implies that the best scoring matrices to compare two proteins depends on how similar these organisms are biologists get around this problem by first analyzing extremely similar proteins for example proteins that have on average only one mutation per amino acids many proteins in human and chimpanzee fulfill this re quirement such sequences are defined as being one pam unit diverged and to a first approximation one can think of a pam unit as the amount of time in which an average protein mutates of its amino acids the pam scor ing matrix is defined from many alignments of extremely similar proteins as follows given a set of base alignments define f i j as the total number of times amino acids i and j are aligned against each other divided by the total num ber of aligned positions we also define g i j as f i j where f i is the frequency of amino acid i in all proteins from the data set g i j defines the probability that an amino acid i mutates into amino acid j within pam unit the i j entry of the pam matrix is defined as δ i j log f i j log g i j f i f j stands for the frequency of aligning amino acid i against amino acid j that one expects simply by chance the pam n matrix can be defined as the result of applying the pam matrix n times if g is the matrix of frequencies g i j then gn multiplying this matrix by it self n times gives the probability that amino acid i mutates into amino acid j during n pam units the i j entry of the pam n matrix is defined as log n i j f j for large n the resulting pam matrices often allow one to find related proteins even when there are practically no matches in the alignment in this case the underlying nucleotide sequences are so diverged that their compar ison usually fails to find any statistically significant similarities for example the similarity between the cancer causing ν sis oncogene and the growth fac tor pdgf would probably have remained undetected had russell doolittle and colleagues not transformed the nucleotide sequences into amino acid sequences prior to performing the comparison local sequence alignment the global alignment problem seeks similarities between two entire strings this is useful when the similarity between the strings extends over their en tire length for example in protein sequences from the same protein family these protein sequences are often very conserved and have almost the same length in organisms ranging from fruit flies to humans however in many biological applications the score of an alignment between two substrings of v and w might actually be larger than the score of an alignment between the entireties of v and w for example homeobox genes which regulate embryonic development are present in a large variety of species although homeobox genes are very dif ferent in different species one region in each gene called the homeodomain is highly conserved the question arises how to find this conserved area and ignore the areas that show little similarity in temple smith and michael waterman proposed a clever modification of the global sequence alignment dynamic programming algorithm that solves the local alignment problem figure presents the comparison of two hypothetical genes v and w of the same length with a conserved domain present at the beginning of v and at the end of w for simplicity we will assume that the conserved domains in these two genes are identical and cover one third of the entire length n of these genes in this case the path from source to sink capturing the similarity between the homeodomains will include approximately n horizontal edges n diagonal match edges corresponding to homeodomains and n vertical edges therefore the score of this path is nσ n nσ n σ however this path contains so many indels that it is unlikely to be the high est scoring alignment in fact biologically irrelevant diagonal paths from the source to the sink will likely have a higher score than the biologically relevant alignment since mismatches are usually penalized less than indels the expected score of such a diagonal path is n µ since every diagonal edge corresponds to a match with probability and mismatch with proba bility since σ µ for many settings of indel and mismatch penalties the global alignment algorithm will miss the correct solution of the real biological problem and is likely to output a biologically irrelevant near diagonal path indeed figure bears exactly this observation when biologically significant similarities are present in certain parts of dna fragments and are not present in others biologists attempt to maxi mize the alignment score vi wj over all substrings vi of v and wj of w this is called the local alignment problem since the alignment does not necessarily extend over the entire string length as it does in the global alignment problem local alignment problem find the best local alignment between two strings input strings v and w and a scoring matrix δ output substrings of v and w whose global alignment as defined by δ is maximal among all global alignments of all substrings of v and w the solution to this seemingly harder problem lies in the realization that the global alignment problem corresponds to finding the longest local path between vertices and n m in the edit graph while the local align ment problem corresponds to finding the longest path among paths between arbitrary vertices i j and ii ji in the edit graph a straightforward and in efficient approach to this problem is to find the longest path between every pair of vertices i j and ii ji and then to select the longest of these com puted paths instead of finding the longest path from every vertex i j to every other vertex ii ji the local alignment problem can be reduced to finding the longest paths from the source to every other vertex by this will result in a very slow algorithm with o running time there are roughly pairs of vertices i j and computing local alignments starting at each of them typically takes o time t cc c agt tatgt caggggacacg a gcatgcaga gac aattgccgcc gtcgt t ttcag ca gttatg t cagat c tcccagttatgtcaggggacacgagcatgcagagac aattgccgccgtcgttttcagcagttatgtcagatc a a t t g c c g c c g t c g t t t t c a g c a g t t a t g t c a g a t c t c c c a g t t global t g t c a g g g g a c a c g a g c a t g c a g a g a c figure a global and b local alignments of two hypothetical genes that each have a conserved domain the local alignment has a much worse score according to the global scoring scheme but it correctly locates the conserved domain figure the smith waterman local alignment algorithm introduces edges of weight here shown with dashed lines from the source vertex to every other vertex in the edit graph adding edges of weight in the edit graph these edges make the source vertex a predecessor of every vertex in the graph and provide a free ride from the source to any other vertex i j a small difference in the following recurrence reflects this transformation of the edit graph shown in figure max si j δ vi si j δ wj si j δ vi wj the largest value of si j over the whole edit graph represents the score of the best local alignment of v and w recall that in the global alignment problem we simply looked at sn m the difference between local and global alignment is illustrated in figure top optimal local alignment reports only the longest path in the edit graph at the same time several local alignments may have biological significance and methods have been developed to find the k best nonoverlapping local align ments these methods are particularly important for comparison of multido main proteins that share similar blocks that have been shuffled in one protein compared to another in this case a single local alignment representing all significant similarities may not exist alignment with gap penalties mutations are usually caused by errors in dna replication nature fre quently deletes or inserts entire substrings as a unit as opposed to deleting or inserting individual nucleotides a gap in an alignment is defined as a con tiguous sequence of spaces in one of the rows since insertions and deletions of substrings are common evolutionary events penalizing a gap of length x as σx is cruel and unusual punishment many practical alignment algo rithms use a softer approach to gap penalties and penalize a gap of x spaces by a function that grows slower than the sum of penalties for x indels to this end we define affine gap penalties to be a linearly weighted score for large gaps we can set the score for a gap of length x to be ρ σx where ρ is the penalty for the introduction of the gap and σ is the penalty for each symbol in the gap ρ is typically large while σ is typically small though this may seem to be complicating our alignment approach it turns out that the edit graph representation of the problem is robust enough to accommodate it affine gap penalties can be accommodated by adding long vertical and horizontal edges in the edit graph e g an edge from i j to i x j of length ρ σx and an edge from i j to i j x of the same length from each vertex to every other vertex that is either east or south of it we can then apply the same algorithm as before to compute the longest path in this graph since the number of edges in the edit graph for affine gap penalties increases at first glance it looks as though the running time for the alignment algorithm also increases from o to o where n is the longer of the two string lengths however the following three recurrences keep the running time down si j max si j σ si j ρ σ i j max i j σ si j ρ σ the complexity of the corresponding longest path in a dag problem is defined by the number of edges in the graph adding long horizontal and vertical edges imposed by affine gap penalties increases the number of edges by a factor of n si j max si j δ vi wj si j i j the variable si j computes the score for alignment between the i prefix of v and the j prefix of w ending with a deletion i e a gap in w while the variable i j computes the score for alignment ending with an insertion i e a gap in v the first term in the recurrences for si j and i j corresponds to extending the gap while the second term corresponds to initiating the gap essentially si j and i j are the scores of optimal paths that arrive at vertex i j via vertical and horizontal edges correspondingly figure further explains how alignment with affine gap penalties can be reduced to the manhattan tourist problem in the appropriate city grid in this case the city is built on three levels the bottom level built solely with vertical edges with weight σ the middle level built with diagonal edges of weight δ vi wj and the upper level which is built from horizontal edges with weight σ the lower level corresponds to gaps in sequence w the middle level corresponds to matches and mismatches and the upper level corresponds to gaps in sequence v also in this graph there are two edges from each vertex i j middle at the middle level that connect this vertex with vertex i j lower at the lower level and with vertex i j upper at the upper level these edges model a start of the gap and have weight ρ σ finally one has to introduce zero weight edges connecting vertices i j lower and i j upper with vertex i j middle at the middle level these edges model the end of the gap in effect we have created a rather complicated graph but the same algorithm works with it we have now introduced a number of pairwise sequence comparison prob lems and shown that they can all be solved by what is essentially the same dynamic programming algorithm applied to a suitably built manhattan style city we will now consider other applications of dynamic programming in bioinformatics multiple alignment the goal of protein sequence comparison is to discover structural or func tional similarities among proteins biologically similar proteins may not ex hibit a strong sequence similarity but we would still like to recognize resem σ σ σ σ σ σ figure a three level edit graph for alignment with affine gap penalties every vertex i j in the middle level has one outgoing edge to the upper level one outgo ing edge to the lower level and one incoming edge each from the upper and lower levels t cc c agt tatgt caggggacacg a gcatgcaga gac aattgccgcc gtcgt t ttcag ca gttatg t cagat c x xxx attgc g attcgtat gggaca tggatgcatgcag tgac figure multiple alignment of three sequences blance even when the sequences share only weak similarities if sequence similarity is weak pairwise alignment can fail to identify biologically related sequences because weak pairwise similarities may fail statistical tests for significance however simultaneous comparison of many sequences often allows one to find similarities that are invisible in pairwise sequence com parison let vk be k strings of length nk over an alphabet a let ai denote the extended alphabet a where denotes the space char acter reserved for insertions and deletions a multiple alignment of strings vk is specified by a k n matrix a where n i k ni each element of the matrix is a member of ai and each row i contains the char acters of vi in order interspersed with n ni spaces figure we also assume that every column of the multiple alignment matrix contains at least one symbol from a that is no column in a multiple alignment contains only spaces the multiple alignment matrix we have constructed is a generaliza tion of the pairwise alignment matrix to k sequences the score of a multiple alignment is defined to be the sum of scores of the columns with the optimal alignment being the one that maximizes the score just as it was in section the consensus of an alignment is a string of the most common characters in each column of the multiple alignment at this point we will use a very general scoring function that is defined by a k dimensional matrix δ of size ai ai that describes the scores of all possible combinations of k symbols from ai a straightforward dynamic programming algorithm in the k dimensional edit graph formed from k strings solves the multiple alignment problem sequences that code for proteins that perform the same function are likely to be somehow related but it may be difficult to decide whether this similarity is significant or happens just by chance this is a k dimensional scoring matrix rather than the two dimensional al al matrix for pairwise alignment which is a multiple alignment with k for example suppose that we have three sequences u v and w and that we want to find the best alignment of all three every multiple alignment of three sequences corresponds to a path in the three dimensional manhattan like edit graph in this case one can apply the same logic as we did for two dimensions to arrive at a dynamic programming recurrence this time with more terms to consider to get to vertex i j k in a three dimensional edit graph you could come from any of the following predecessors note that δ x y z denotes the score of a column with letters x y and z as in figure i j k for score δ ui i j k for score δ vj i j k for score δ wk i j k for score δ ui vj i j k for score δ ui wk i j k for score δ vj wk i j k for score δ ui vj wk we create a three dimensional dynamic programming array and it is easy to see that the recurrence for si j k in the three dimensional case is similar to the recurrence in the two dimensional case fig namely si j k δ vi si j k δ wj si j k δ uk i j k i k si j k δ wj uk si j k δ vi wj uk unfortunately in the case of k sequences the running time of this ap proach is o k so some improvements of the exact algorithm and many heuristics for suboptimal multiple alignments have been proposed a good heuristic would be to compute all optimal pairwise alignments between every pair of strings and then combine them together in such a way that pair wise alignments induced by the multiple alignment are close to the optimal a t g c c g t a a t g c figure the scoring matrix δ used in a three sequence alignment i j k i j k i j k i j k i j k i j k i j k i j k figure a cell in the alignment graph between three sequences ones unfortunately it is not always possible to combine optimal pairwise alignments into a multiple alignment since some pairwise alignments may be incompatible for example figure a shows three sequences whose opti mal pairwise alignment can be combined into a multiple alignment whereas b shows three sequences that cannot be combined as a result some mul tiple alignment algorithms attempt to combine some compatible subset of optimal pairwise alignments into a multiple alignment another approach to do this uses one particularly strong pairwise align ment as a building block for the multiple k way alignment and iteratively adds one string to the growing multiple alignment this greedy progressive multiple alignment heuristic selects the pair of strings with greatest similarity and merges them together into a new string following the principle once a gap always a gap as a result the multiple alignment of k sequences is reduced to the multiple alignment of k sequences the motivation for the choice of the closest strings at the early steps of the algorithm is that close strings often provide the most reliable information about a real alignment many popular iterative multiple alignment algorithms including the tool clustal use similar strategies although progressive multiple alignment algorithms work well for very close sequences there are no performance guarantees for this approach the problem with progressive multiple alignment algorithms like clustal is that they may be misled by some spuriously strong pairwise alignment in effect a bad seed if the very first two sequences picked for building multiple alignment are aligned in a way that is incompatible with the optimal multiple alignment the error in this initial pairwise alignment will propagate all the way through to the whole multiple alignment many multiple alignment algorithms have been proposed and even with systematic deficiencies such as the above they remain quite useful in computational biology we have described multiple alignment for k sequences as a generalization of the pairwise alignment problem which assumed the existence of a k dimensional scoring matrix δ since such k dimensional scoring matrices are not very practical we briefly describe two other scoring approaches that are more biologically relevant the choice of the scoring function can drastically affect the quality of the resulting alignment and no single scoring approach is perfect in all circumstances the columns of a multiple alignment of k sequences describe a path of essentially this principle states that once a gap has been introduced into the alignment it will never close even if that would lead to a better overall score aaaatttt ttttgggg ttttgggg aaaatttt aaaa gggg aaaatttt aaaa gggg aaaa gggg ttttgggg a compatible pairwise alignments aaaatttt ttttgggg aaaatttt ggggaaaa ggggaaaa ttttgggg b incompatible pairwise alignments figure given three sequences it might be possible to combine their pairwise alignment into a multiple alignment a but it might not be b edges in a k dimensional version of the manhattan gridlike edit graph the weights of these edges are determined by the scoring function δ intuitively we want to assign higher scores to the columns with a low variation in let ters such that high scores correspond to highly conserved sequences for example in the multiple longest common subsequence problem the score of a column is set to if all the characters in the column are the same and if even one character disagrees in the more statistically motivated entropy approach the score of a multiple alignment is defined as the sum of the entropies of the columns which are defined to px log px x where px is the frequency of letter x ai in a given column in this case the more conserved the column the larger the entropy score for example a column that has each of the nucleotides present k times will have an entropy score of log while a completely conserved column as in the multiple lcs problem would have entropy finding the longest path in the k dimensional edit graph corresponds to finding the multiple alignment with the largest entropy score while entropy captures some statistical notion of a good alignment it can be hard to design efficient algorithms that optimize this scoring function another popular scoring approach is the sum of pairs score sp score any multiple alignment a of k sequences vk forces a pairwise alignment between any two sequences vi and vj of score sa vi vj the sp score for a multiple alignment a is given by i j k sa vi vj in this definition the score of an alignment a is built from the scores of all pairs of strings in the alignment gene prediction in sydney brenner and francis crick demonstrated that every triplet of nucleotides codon in a gene codes for one amino acid in the corresponding protein they were able to introduce deletions in dna and observed that deletion of a single nucleotide or two consecutive nucleotides in a gene dra matically alters its protein product paradoxically deleting three consecutive the correct way to define entropy is to take the negative of this expression but the definition above allows us to deal with a maximization rather than a minimization problem we remark that the resulting forced alignment is not necessarily optimal nucleotides results in minor changes in the protein for example the phrase the sly fox and the shy dog written in triplets turns into gibber ish after deleting one letter the syf oxa ndt hes hyd og or two let ters the sfo xan dth esh ydo g but makes some sense after delet ing three nucleotides the sox and the shy dog inspired by this ex periment charles yanofsky proved that a gene and its protein product are collinear that is the first codon in the gene codes for the first amino acid in the protein the second codon codes for the second amino acid rather than say the seventeenth and so on yanofsky ingenious experiment was so influential that nobody even questioned whether codons are represented by continuous stretches in dna and for the subsequent fifteen years biologists believed that a protein was encoded by a long string of contiguous triplets however the discovery of split human genes in proved that genes are often represented by a collection of substrings and raised the computational problem of predicting the locations of genes in a genome given only the ge nomic dna sequence the human genome is larger and more complex than bacterial genomes this is not particularly surprising since one would expect to find more genes in humans than in bacteria however the genome size of many eukaryotes does not appear to be related to an organism genetic complexity for exam ple the salamander genome is ten times larger than the human genome this apparent paradox was resolved by the discovery that many organisms con tain not only genes but also large amounts of so called junk dna that does not code for proteins at all in particular most human genes are broken into pieces called exons that are separated by this junk dna the difference in the sizes of the salamander and human genomes thus presumably reflects larger amounts of junk dna and repeats in the salamander genome split genes are analogous to a magazine article that begins on page con tinues on page then takes up again on pages and with pages of advertising appearing in between we do not understand why these jumps occur and a significant portion of the human genome is this junk ad vertising that separates exons more confusing is that the jumps between different parts of split genes are inconsistent from species to species a gene in an insect edition of the genome will be organized differently than the related gene in a worm genome the number of parts exons may be different the information that appears in one part in the human edition may be broken up into two in the mouse version or vice versa while the genes themselves are related they may be quite different in terms of the parts structure split genes were first discovered in in the laboratories of phillip sharp and richard roberts during studies of the adenovirus the discovery was such a surprise that the paper by roberts group had an unusually catchy ti tle for the journal cell an amazing sequence arrangement at the end of adenovirus messenger rna sharp group focused their experiments on an that encodes a viral protein known as hexon to map the hexon mrna in the viral genome mrna was hybridized to adenovirus dna and the hybrid molecules were analyzed by electron microscopy strikingly the mrna dna hybrids formed in this experiment displayed three loop struc tures rather than the continuous duplex segment suggested by the classic continuous gene model figure further hybridization experiments re vealed that the hexon mrna is built from four separate fragments of the adenovirus genome these four continuous segments called exons in the adenovirus genome are separated by three junk fragments called introns gene prediction is the problem of locating genes in a genomic sequence human genes constitute only of the human genome and no existing in silico gene recognition algorithm provides completely reliable gene recogni tion the intron exon model of a gene seems to prevail in eukaryotic organ isms prokaryotic organisms like bacteria do not have broken genes as a result gene prediction algorithms for prokaryotes tend to be somewhat simpler than those for eukaryotes there are roughly two categories of approaches that researchers have used for predicting gene location the statistical approach to gene prediction is to look for features that appear frequently in genes and infrequently elsewhere many researchers have attempted to recognize the locations of splicing signals at exon intron junctions for example the dinucleotides ag and gt on the left and right hand sides of an exon are highly conserved figure in addition there are other less conserved positions on both sides of the exons the simplest way to represent such binding sites is by a profile describing the propensities of different nucleotides to occur at different positions unfortu at that time messenger rna mrna was viewed as a copy of a gene translated into the rna alphabet it is used to transfer information from the nuclear genome to the ribosomes to direct protein synthesis this is not to say that bacterial gene prediction is a trivial task but rather to indicate that eukaryotic gene finding is very difficult if genes are separated into exons interspersed with introns then the rna that is transcribed from dna i e the complementary copy of a gene should be longer than the mrna that is used as a template for protein synthesis therefore some biological process needs to remove the introns in the pre mrna and concatenate the exons into a single mrna string this process is known as splicing and the resulting mrna is used as a template for protein synthesis in cytoplasm figure an electron microscopy experiment led to the discovery of split genes when mrna below is hybridized against the dna that generated it three dis tinct loops can be seen above because the loops are present in the dna and are not present in mrna certain parts introns must be removed during the process of mrna formation called splicing intron intron gt ag gt ag figure exons typically are flanked by the dinucleotides ag and gt nately using profiles to detect splice sites has met with limited success since these profiles are quite weak and tend to match frequently in the genome at nonsplice sites attempts to improve the accuracy of gene prediction led to the second category of approaches for gene finding those based on similar ity the similarity based approach to gene prediction relies on the observation that a newly sequenced gene has a good chance of being related to one that is already known for example of mouse genes have human analogs however one cannot simply look for a similar sequence in one organism genome based on the genes known in another for the reasons outlined above both the exon sequence and the exon structure of the related gene in differ ent species are different the commonality between the related genes in both organisms is that they produce similar proteins accordingly instead of em ploying a statistical analysis of exons similarity based methods attempt to solve a combinatorial puzzle find a set of substrings putative exons in a genomic sequence say mouse whose concatenation fits a known human protein in this scenario we suppose we know a human protein and we want to discover the exon structure of the related gene in the mouse genome the more sequence data we collect the more accurate and reliable similarity based methods become consequently the trend in gene prediction has re cently shifted from statistically motivated approaches to similarity based al gorithms statistical approaches to gene prediction as mentioned above statistical approaches to finding genes rely on detecting subtle statistical variations between coding exons and non coding regions the simplest way to detect potential coding regions is to look at open reading frames or orfs one can represent a genome of length n as a sequence of n codons the three stop codons taa tag and tga break this sequence into segments one between every two consecutive stop codons the subseg ments of these that start from a start codon atg are orfs orfs within a single genomic sequence may overlap since there are six possible reading frames three on one strand starting at positions and and three on the reverse strand as shown in figure one would expect to find frequent stop codons in noncoding dna since the average number of codons between two consecutive stop codons in ran dom dna should be this is much smaller than the number of codons in an average protein which is roughly therefore orfs longer than some threshold length indicate potential genes however gene predic tion algorithms based on selecting significantly long orfs may fail to detect short genes or genes with short exons in fact there are three such representations for each dna strand one starting at position another at ignoring the first base and the third one at ignoring the first two bases there are codons and three of them are stop codons gly thr val gly glu stop taccgtggcagccactcattgcgtaac auggcaccgucggugaguaacgcauug stop gln thr val figure the six reading frames for the sequence atgcttagtctg the string may be read forward or backward and there are three frame shifts in each direction many statistical gene prediction algorithms rely on statistical features in protein coding regions such as biases in codon usage we can enter the fre quency of occurrence of each codon within a given sequence into a element codon usage array as in table the codon usage arrays for coding regions are different than the codon usage arrays for non coding regions enabling one to use them for gene prediction for example in human genes codons cgc and agg code for the same amino acid arg but have very different frequencies cgc is times more likely to be used in genes than agg ta ble therefore an orf that prefers cgc over agg while coding for arg is a likely candidate gene one can use a likelihood ratio to compute the conditional probabilities of the dna sequence in a window un der the hypothesis that the window contains a coding sequence and under the hypothesis that the window contains a noncoding sequence if we slide this window along the genomic dna sequence and calculate the likelihood the likelihood ratio technique allows one to test the applicability of two distinct hypotheses when the likelihood ratio is large the first hypothesis is more likely to be true than the second one table the genetic code and codon usage in homo sapiens the codon for methio nine or aug also acts as a start codon all proteins begin with met the numbers next to each codon reflects the frequency of that codon occurrence while coding for an amino acid for example among all lysine lys residues in all the proteins in a genome the codon aag generates of them while the codon aag generates these frequencies differ across species u c a g uuu phe ucu ser uau tyr ugu cys uuc phe ucc ser uac tyr ugc cys uua leu uca ser uaa stp uga stp uug leu ucg ser uag stp ugg trp cuu leu ccu pro cau his cgu arg cuc leu ccc pro cac his cgc arg cua leu cca pro caa gln cga arg cug leu ccg pro cag gln cgg arg auu ile acu thr aau asn agu ser auc ile acc thr aac asn agc ser aua ile aca thr aaa lys aga arg aug met acg thr aag lys agg arg guu val gcu ala gau asp ggu gly guc val gcc ala gac asp ggc gly gua val gca ala gaa glu 68 gga gly gug val gcg ala gag glu ggg gly ratio at each point genes are often revealed as peaks in the likelihood ratio plots an even better coding sensor is the in frame hexamer proposed by mark borodovsky and colleagues gene prediction in bacterial genomes also takes advantage of several conserved sequence motifs often found in the re gions around the start of transcription unfortunately such sequence motifs are more elusive in eukaryotes while the described approaches are successful in prokaryotes their appli cation to eukaryotes is complicated by the exon intron structure the average length of exons in vertebrates is nucleotides and exons of this length are too short to produce reliable peaks in the likelihood ratio plot while analyz ing orfs because they do not differ enough from random fluctuations to be detectable moreover codon usage and other statistical parameters proba the in frame hexamer count reflects frequencies of pairs of consecutive codons bly have nothing in common with the way the splicing machinery actually recognizes exons many researchers have used a more biologically oriented approach and have attempted to recognize the locations of splicing signals at exon intron junctions there exists a weakly conserved sequence of eight nucleotides at the boundary of an exon and an intron donor splice site and a sequence of four nucleotides at the boundary of an intron and exon acceptor splice site since profiles for splice sites are weak these approaches have had limited success and have been supplanted by hidden markov model hmm that capture statistical dependencies between sites a popular example of this latter approach is genscan which was developed in by chris burge and samuel karlin genscan combines coding region and splicing signal predictions into a single framework for example a splice site prediction is more believable if signs of a coding region appear on one side of the site but not on the other many such statistics are used in the hmm framework of genscan that merges splicing site statistics coding region statistics and motifs near the start of the gene among others however the accuracy of genscan decreases for genes with many short exons or with unusual codon usage similarity based approaches to gene prediction a similarity based approach to gene prediction uses previously sequenced genes and their protein products as a template for the recognition of un known genes in newly sequenced dna fragments instead of employing statistical properties of exons this method attempts to solve the following combinatorial puzzle given a known target protein and a genomic sequence find a set of substrings candidate exons of the genomic sequence whose concatenation splicing best fits the target a naive brute force approach to the spliced alignment problem is to find all local similarities between the genomic sequence and the target protein sequence each substring from the genomic sequence that exhibits sufficient similarity to the target protein could be considered a putative exon the putative exons so chosen may lack the canonical exon flanking dinucleotides ag and gt but we can extend or shorten them slightly to make sure that they are flanked by ag and gt the resulting set may contain overlapping hidden markov models are described in chapter putative here means that the sequence might be an exon even though we have no proof of this substrings and the problem is to choose the best subset of nonoverlapping substrings as a putative exon structure we will model a putative exon with a weighted interval in the genomic se quence which is described by three parameters l r w as in figure here l is the left hand position r is the right hand position and w is the weight of the putative exon the weight w may reflect the local alignment score for the genomic interval against the target protein sequence or the strength of flanking acceptor and donor sites or any combination of these and other measures it reflects the likelihood that this interval is an exon a chain is any set of nonoverlapping weighted intervals the total weight of a chain is the sum of the weights of the intervals in the chain a maximum chain is a chain with maximum total weight among all possible chains below we assume that the weights of all intervals are positive w exon chaining problem given a set of putative exons find a maximum set of nonoverlapping putative exons input a set of weighted intervals putative exons output a maximum chain of intervals from this set the exon chaining problem for n intervals can be solved by dynamic pro gramming in a graph g on vertices n of which represent starting left positions of intervals and n of which represent ending right positions of in tervals as in figure we assume that the set of left and right interval ends is sorted into increasing order and that all positions are distinct forming an ordered array of vertices in graph g there are edges in this graph there is an edge between each li and ri of weight wi for i from to n and additional edges of weight which simply connect adjacent vertices vi vi forming a path in the graph from to in the algo rithm below si represents the length of the longest path in the graph ending at vertex vi thus is the solution to the exon chaining problem we choose nonoverlapping substrings because exons in real genes do not overlap in particular we are assuming that no interval starts exactly where another ends figure a short genomic sequence a set of nine weighted intervals and the graph used for the dynamic programming solution to the exon chaining problem five weighted intervals and shown by bold edges form an optimal solution to the exon chaining problem the array at the bottom shows the values generated by the exonchaning algo rithm exonchaining g n for i to si for i to if vertex vi in g corresponds to the right end of an interval i j index of vertex for left end of the interval i w weight of the interval i si max sj w si else si si return one shortcoming of this approach is that the endpoints of putative exons are not very well defined and this assembly method does not allow for any flexibility at these points more importantly the optimal chain of intervals may not correspond to any valid alignment for example the first interval in the optimal chain may be similar to a suffix of the protein while the second interval in the optimal chain may be similar to a prefix in this case the putative exons corresponding to the valid chain of these two intervals cannot d exon exon figure an infeasible chain that might have a maximal score the first exon corresponds to a region at the end of the target protein while the second exon cor responds to a region at the beginning of the target protein these exons cannot be combined into a valid global dna protein alignment be combined into a valid alignment figure spliced alignment in mikhail gelfand and colleagues proposed the spliced alignment ap proach to find genes in eukaryotes use a related protein within one genome to reconstruct the exon intron structure of a gene in another genome the spliced alignment begins by selecting either all putative exons between po tential acceptor and donor sites e g between ag and gt dinucleotides or by finding all substrings similar to the target protein as in the exon chaining problem by filtering this set in a way that attempts not to lose true exons one is left with a set of candidate exons that may contains many false exons but definitely contains all the true ones while it is difficult to distinguish the good true exons from the bad false exons by a statistical procedure alone we can use the alignment with the target protein to aid us in our search in theory only the true exons will form a coherent representation of a protein given the set of candidate exons and a target protein sequence we explore all possible chains assemblies of the candidate exon set to find the assembly with the highest similarity score to the target protein the number of differ ent assemblies may be huge but the spliced alignment algorithm is able to find the best assembly among all of them in polynomial time for simplicity we will assume that the protein sequence is expressed in the same alphabet as the geneome of course this is not the case in nature and a problem at the end of this chapter asks you to modify the recurrence relations accordingly let g gn be the genomic sequence t tm be the target sequence and b be the set of candidate exons blocks as above a chain γ is any sequence of nonoverlapping blocks and the string formed by a chain is just the concatenation of all the blocks in the chain we will use γ to denote the string formed by the chain γ the chain that we are searching for is the one whose concatenation forms the string with the highest similarity to the target sequence spliced alignment problem find a chain of candidate exons in a genomic sequence that best fits a target sequence input genomic sequence g target sequence t and a set of candidate exons blocks b output a chain of candidate exons γ such that the global alignment score γ t is maximum among all chains of candidate exons from b as an example consider the genomic sequence it was brilliant thrilling morning and the slimy hellish lithe doves gyrated and gambled nimbly in the waves with the set of blocks shown in figure top by overlapping rectangles if our target is the famous lewis carroll line twas brillig and the slithy toves did gyre and gimble in the wabe then figure illustrates the spliced alignment problem of choosing the best exons or blocks in this case that can be assembled into the target the spliced alignment problem can be cast as finding a path in a directed acyclic graph fig middle vertices in this graph shown as rectangles correspond to blocks candidate exons and directed edges connect nonover lapping blocks a vertex corresponding to a block b is labeled by a string represented by this block therefore every path in the spliced alignment graph spells out the string obtained by concatenation of labels of its vertices the weight of a path in this graph is defined as the score of the optimal align ment between the concatenated blocks of this path and the target sequence note that we have defined the weight of an entire path in the graph but we have not defined weights for individual edges this makes the spliced alignment problem different from the standard longest path problem nev ertheless we can leverage dynamic programming to solve the problem we emphasize the difference between the scoring functions for the exon chaining prob lem and the spliced alignment problem in contrast to the spliced alignment problem the set of nonoverlapping substrings representing the solution of the exon chaining problem does not necessarily correspond to a valid alignment between the genomic sequence and the target protein sequence t w a s b r i l l i g a n d t h e s l i t h y t ov e s d i d gy r e a n d gi m b l e i n t h e w a b e t w a s b r i l l i g a n d t h e s l t h e d ov e s gy r a t e d a n d ga m b l e d i n t h e w a v e t w a s b r i l l i g a n d t h e s l t h e d ov e s gy r a t e d n i m b l y i n t h e w a v e t hr i l l i n g a n d a n d h e l l i s h d ov e s gy r a t e d a n d ga m b l e d i n t h e w a v e t hr i l l i n g h e l l i s h d ov e s gy r a t e d n i m b l y i n t h e w a v e i mornin l i figure the spliced alignment problem four different block assemblies with the best fit to lewis carroll line top the corresponding spliced alignment graph middle and the transformation of the spliced alignment graph that helps reduce the running time bottom to describe the dynamic programming recurrence for the spliced align ment problem we first need to define the similarity score between the i prefix of the spliced alignment graph in figure and the j prefix of the target sequence t the difficulty is that there are typically many different i prefixes of the graph since there are multiple blocks containing position i let b gleft gi gright be a candidate exon containing position i in ge nomic sequence g define the i prefix of b as b i gleft gi and end b for example there are two i prefixes ending with e within hellish and four i prefixes ending in r within gyrated in figure right the words left and right are used here as indices if the chain γ b ends at block b define γ i to be the concatenation of all candidate exons in the chain up to and excluding b plus all the characters in b up to i that is γ i b i finally let s i j b max all chains γ ending in b γ i t j that is given i j and a candidate exon b that covers position i s i j b is the score of the optimal spliced alignment between the i prefix of g and the j prefix of t under the assumption that this alignment ends in block b the following recurrence allows us to efficiently compute s i j b for the sake of simplicity we consider sequence alignment with linear gap penal ties for insertion or deletion equal to σ and use the scoring matrix δ for matches and mismatches the dynamic programming recurrence for the spliced alignment problem is broken into two cases depending on whether i is the starting vertex of block b or not in the latter case the recurrence is similar to the canonical sequence alignment s i j b max s i j b σ s i j b σ s i j b δ gi tj on the other hand if i is the starting position of block b then s i j b max s i j b σ maxall blocks preceding b s end bl j bl δ gi tj maxall blocks preceding b s end bl j bl σ after computing this three dimensional table s i j b the score of the optimal spliced alignment is max s end b m b b where the maximum is taken over all possible blocks one can further reduce the number of edges in the spliced alignment graph by making a transfor the notation x y denotes concatenation of strings x and y mation of the graph in figure middle into a graph shown in figure bottom the details of the corresponding recurrences are left as a problem at the end of this chapter the above description hides some important details of block generation the simplest approach to the construction of the candidate exon set is to gen erate all fragments between potential acceptor sites represented by ag and potential donor sites represented by gt removing possible exons with stop codons in all three frames however this approach does not work well since it generates many short blocks experiments with the spliced alignment al gorithm have shown that incorrect predictions are frequently associated with the mosaic effect caused by very short potential exons the difficulty is that these short exons can be easily combined to fit any target protein simply be cause it is easier to construct a given sentence from a thousand random short strings than from the same number of random long strings for example with high probability the phrase filtration of candidate exons can be made up from a sample of a thousand random two letter strings fi lt ra etc are likely to be present in this sample the probability that the same phrase can be made up from a sample of the same number of random five letter strings is close to zero even finding the string filtr in this sample is unlikely this observation explains the mosaic effect if the number of short blocks is high chains of these blocks can replace actual exons in spliced align ments thus leading to predictions with an unusually large number of short exons to avoid the mosaic effect the candidate exons should be subjected to some filtering procedure notes although the first dynamic programming algorithm for dna sequence com parison was published as early as by saul needleman and christian wunsch russell doolittle and colleagues used heuristic algorithms to establish the similarity between cancer causing genes and the pdgf gene in when needleman and wunsch published their paper in they did not know that a very similar algorithm had been published two years earlier in a pioneering paper on automatic speech recognition though the details of the algorithms are slightly different they are both variations of dynamic programming earlier still vladimir levenshtein introduced the notion of edit distance in albeit without an algorithm for com puting it the local alignment algorithm introduced by temple smith and michael waterman in quickly became the most popular alignment tool in computational biology later michael waterman and mark eggert developed an algorithm for finding the k best nonoverlapping local align ments the algorithm for alignment with affine gap penalties was the work of osamu gotoh the progressive multiple alignment approach initially explored by da fei feng and russell doolittle feng and doolittle resulted in many practical algorithms with clustal one of the most popular the cellular process of splicing was discovered in the laboratories of phillip sharp and richard roberts applications of both markov models and in frame hexamer count statistics for gene prediction were proposed by borodovsky and mcinnich chris burge and samuel karlin developed an hmm approach to gene prediction that resulted in the popular gen scan algorithm in in snyder and stormo developed a similarity based gene prediction algorithm that amounts to the solution of a problem that is similar to the exon chaining problem the spliced alignment algorithm was developed by mikhail gelfand and colleagues in i went to college to escape what i considered to be a dull and dreary existence of raising livestock on pasture land in western oregon where my family has lived since my goal was to find an occupation with a steady income where i could look forward to going to work this eliminated ranching and logging which was how i spent my col lege summers research and teaching didn t seem possible or even desirable but i went on for a phd because such a job did not appear in graduate school at michigan state i found a wonderful advisor in john kinney from whom i learned ergodic and information theory john aimed me at a branch of number theory for a thesis we were doing statistical properties of the iteration of deterministic functions long before that became a fad i began using computers to explore it eration something which puzzled certain of my professors who felt i was wasting time i could be spending proving theorems after gradu ation and taking a nonresearch job at a small school in idaho my work in iteration led to my first summer visit to los alamos national labs later i met temple smith there in and was drawn into problems from biology later i wrote in my book of new mexico essays skiing the sun i was an innocent mathematician until the summer of it was then than i met temple ferris smith and for two months was cooped up with him in an office at los alamos national laboratories that experience transformed my research my life and perhaps my sanity soon after we met he pulled out a little blackboard and started lecturing me about biology what it was what was important what was going on somewhere in there by implication was what we should work on but the truth be told he didn t know what that was either i was totally confused amino acids nucleosides beta sheets what were these things where was the mathematics i knew no modern biology but studying alignment and evolution was quite attractive to me the most fun was formulating problems and in my opinion that remains the most important aspect of our subject temple and i spent days and weeks trying to puzzle out what we should be working on charles delisi a biophysicist who went on to play a key role in jump starting the human genome project was in t theoretical biology at the lab when he saw the progress we had made on alignment problems he came to me and said there was another problem which should interest me this was the rna folding problem which was almost untouched tinoco had published the idea of making a base pair matrix for a sequence and that was it by the fall of i had seen the neat connection between alignment and folding and the following summer i wrote a long manuscript that defined the objects of study established some of their properties explicitly stated the basic problem of folding which included free energies for all struc tural components and finally gave algorithms for its solution i had previously wondered what such a discovery might feel like and it was wonderfully satisfying however it felt entirely like exploration and not a grand triumph of creation as i had expected in fact i had always wanted to be an explorer and regretted the end of the american fron tier wandering about this new rna landscape was a great joy just as i had thought when i was a child trying to transport myself by day dreams out of my family fields into some new and unsettled country problems in lewis carroll proposed the following puzzle to the readers of vanity fair transform one english word into another by going through a series of intermediate english words where each word in the sequence differs from the next by only one substitution to transform head into tail one can use four intermediates head heal teal tell tall tail we say that two words v and w are equivalent if v can be transformed into w by substituting individual letters in such a way that all intermediate words are english words present in an english dictionary problem find an algorithm to solve the following equivalent words problem equivalent words problem given two words and a dictionary find out whether the words are equivalent input the dictionary d a set of words and two words v and w from the dictionary output a transformation of v into w by substitutions such that all intermediate words belong to d if no transformation is possible output v and w are not equivalent given a dictionary d the lewis carroll distance dlc v w between words v and w is defined as the smallest number of substitutions needed to transform v into w in such a way that all intermediate words in the transformation are in the dictionary d we define dlc v w if v and w are not equivalent problem find an algorithm to solve the following lewis carroll problem lewis carroll problem given two words and a dictionary find the lewis carroll distance between these words input the dictionary d and two words v and w from the diction ary output dlc v w problem find an algorithm to solve a generalization of the lewis carroll problem when inser tions deletions and substitutions are allowed rather than only substitutions problem modify dpchange to return not only the smallest number of coins but also the cor rect combination of coins problem let v w be the length of a longest common subsequence of the strings v and w and d v w be the edit distance between v and w under the assumption that insertions and deletions are the only allowed operations prove that d v w n m v w where n is the length of v and m is the length of w problem find the number of different paths from source to sink n m in an n m rectangular grid problem can you find an approximation ratio of the greedy algorithm for the manhattan tourist problem problem let v vn be a string and let p be a m profile generalize the sequence alignment algorithm for aligning a sequence against a profile write the correspond ing recurrence in lieu of pseudocode and estimate the amount of time that your algorithm will take with respect to n and m problem there are only two buttons inside an elevator in a building with floors the ele vator goes floors up if the first button is pressed and floors down if the second button is pressed is it possible to get from floor to floor what is the minimum number of buttons one has to press to do so what is the shortest time one needs to get from floor to floor time is proportional to the number of floors that are passed on the way problem a rook stands on the upper left square of a chessboard two players make turns moving the rook either horizontally to the right or vertically downward as many squares as they want the player who can place the rook on the lower right square of the chessboard wins who will win describe the winning strategy problem a queen stands on the third square of the uppermost row of a chessboard two play ers take turns moving the queen either horizontally to the right or vertically down ward or diagonally in the southeast direction as many squares as they want the player who can place the queen on the lower right square of the chessboard wins who will win describe the winning strategy problem two players play the following game with two chromosomes of length n and m nucleotides at every turn a player can destroy one of the chromosomes and break another one into two nonempty parts for example the first player can destroy a chromosome of length n and break another chromosome into two chromosomes of length m and m m the player left with two single nucleotide chromosomes loses who will win describe the winning strategy for each n and m problem two players play the following game with two sequences of length n and m nu cleotides at every turn a player can either delete an arbitrary number of nucleotides from one sequence or an equal but still arbitrary number of nucleotides from both sequences the player who deletes the last nucleotide wins who will win describe the winning strategy for each n and m problem two players play the following game with two sequences of length n and m nu cleotides at every turn a player must delete two nucleotides from one sequence either the first or the second and one nucleotide from the other the player who cannot move loses who will win describe the winning strategy for each n and m problem two players play the following game with a nucleotide sequence of length n at every turn a player may delete either one or two nucleotides from the sequence the player who deletes the last letter wins who will win describe the winning strategy for each n problem two players play the following game with a nucleotide sequence of length n na nt nc ng where na nt nc and ng are the number of a t c and g in the sequence at every turn a player may delete either one or two nucleotides from the sequence the player who is left with a uni nucleotide sequence of an arbitrary length i e the sequence containing only one of possible nucleotides loses who will win describe the winning strategy for each na nt nc and ng problem what is the optimal global alignment for apple and happe show all optimal align ments and the corresponding paths under the match premium mismatch penalty and indel penalty problem what is the optimal global alignment for moat and boast show all optimal align ments and the corresponding paths under the scoring matrix below and indel penalty a b m o s t a b m o s t problem fill the global alignment dynamic programming matrix for strings at and aagt with affine scoring function defined by match premium mismatch penalty gap open ing penalty and gap extension penalty find all optimal global alignments problem consider the sequences v tacgggtat and w ggacgtacg assume that the match premium is and that the mismatch and indel penalties are fill out the dynamic programming table for a global alignment between v and w draw arrows in the cells to store the backtrack information what is the score of the optimal global alignment and what alignment does this score correspond to fill out the dynamic programming table for a local alignment between v and w draw arrows in the cells to store the backtrack information what is the score of the optimal local alignment in this case and what alignment achieves this score suppose we use an affine gap penalty where it costs to open a gap and to extend it scores of matches and mismatches are unchanged what is the optimal global alignment in this case and what score does it achieve problem for a pair of strings v vn and w wm define m v w to be the matrix whose i j th entry is the score of the optimal global alignment which aligns the character vi with the character wj give an o nm algorithm which computes m v w define an overlap alignment between two sequences v vn and w wm to be an alignment between a suffix of v and a prefix of w for example if v tatata and w aaattt then a not necessarily optimal overlap alignment between v and w is ata aaa optimal overlap alignment is an alignment that maximizes the global alignment score between vi vn and wj where the maximum is taken over all suffixes vi vn of v and all prefixes wj of w problem give an algorithm which computes the optimal overlap alignment and runs in time o nm suppose that we have sequences v vn and w wm where v is longer than w we wish to find a substring of v which best matches all of w global alignment won t work because it would try to align all of v local alignment won t work because it may not align all of w therefore this is a distinct problem which we call the fitting problem fitting a sequence w into a sequence v is a problem of finding a substring vl of v that maximizes the score of alignment vl w among all substrings of v for example if v gtaggcttaaggtta and w tagata the best alignments might be global local fitting v gtaggcttaaggtta tag taggctta w tag a t a tag taga ta score the scores are computed as for match for mismatch or indel note that the optimal local alignment is not a valid fitting alignment on the other hand the optimal global alignment con tains a valid fitting alignment but it achieves a suboptimal score among all fitting alignments problem give an algorithm which computes the optimal fitting alignment explain how to fill in the first row and column of the dynamic programming table and give a recurrence to fill in the rest of the table give a method to find the best alignment once the table is filled in the algorithm should run in time o nm we have studied two approaches to sequence alignment global and local alignment there is a middle ground an approach known as semiglobal alignment in semiglobal alignment the entire sequences are aligned as in global alignment what makes it semiglobal is that the internal gaps of the alignment are counted but the gaps on the end are not for example consider the following two alternative alignments sequence cagca cttggattctcgg sequence cagcgtgg sequence cagcacttggattctcgg sequence cagc g t gg the first alignment has matches mismatch and gaps the second alignment has matches no mismatches and gaps using the simplest scoring scheme match mis match gap the score for the first alignment is and the score for the second alignment is so we would prefer the second alignment however the first alignment is more biologically realistic to get an algorithm which prefers the first alignment to the second we can not count the gaps on the ends under this new semiglobal approach the first alignment would have matches mismatch and gap while the second alignment would still have matches no mismatches and gaps now the first alignment would have a score of and the second alignment would have a score of so the first alignment would have a better score note the similarities and the differences between the fitting problem and the semiglobal align ment problem as illustrated by the semiglobal but not fitting alignment of acgtcat against tcatgca sequence acgtcat sequence tcatgca problem devise an efficient algorithm for the semiglobal alignment problem and illustrate its work on the sequences acagata and agt for scoring use the match premium mismatch penalty and indel penalty define a nodeletion global alignment to be an alignment between two sequences v vn and w wm where only matches mismatches and insertions are allowed that is there can be no deletions from v to w i e all letters of w occur in the alignment with no spaces clearly we must have m n and let k m n problem give an o nk algorithm to find the optimal nodeletion global alignment note the improvement over the o nm algorithm when k is small problem substrings vi vi k and k of the string vn form a substring pair if il i k m ingap where m ingap is a parameter define the substring pair score as the global alignment score of vi vi k and vi1 k design an algorithm that finds a substring pair with maximum score problem for a parameter k compute the global alignment between two strings subject to the constraint that the alignment contains at most k gaps blocks of consecutive indels nucleotide sequences are sometimes written in an alphabet with five characters a t g c and n where n stands for an unspecified nucleotide in essence a wild card biologists may use n when sequencing does not allow one to unambiguously infer the identity of a nucleotide at a specific position a sequence with an n is referred to as a degenerate string for example attng may correspond to four different interpretations attag atttg attgg and attcg in general a sequence with k unspecified nucleotides n will have different interpretations problem given a non degenerate string v and a degenerate string w that contains k ns devise a method to find the best interpretation of w according to v that is out of all possible interpretations of w find wl with the minimum alignment score wl v problem given a non degenerate string v and a degenerate string w that contains k ns devise a method to find the worst interpretation of w according to v that is out of all possible interpretations of w find wl with the minimum alignment score wl v problem given two strings and explain how to construct a string w minimizing d w d w such that d w d w d d is the edit distance between two strings problem given two strings and and a text w find whether there is an occurrence of and interwoven without spaces in w for example the strings abac and bbc occur interwoven in cabbabccdw give an efficient algorithm for this problem a string x is called a supersequence of a string v if v is a subsequence of x for example ablue is a supersequence for blue and able problem given strings v and w devise an algorithm to find the shortest supersequence for both v and w a tandem repeat p k of a pattern p pn is a pattern of length n k formed by concatenation of k copies of p let p be a pattern and t be a text of length m the tandem repeat problem is to find a best local alignment of t with some tandem repeat of p this amounts to aligning p k against t and the standard local alignment algorithm solves this problem in o time problem devise a faster algorithm for solving the tandem repeat problem an alignment of circular strings is defined as an alignment of linear strings formed by cutting linearizing these circular strings at arbitrary positions the following problem asks to find the cut points of two circular strings that maximize the alignment of the resulting linear strings problem devise an efficient algorithm to find an optimal alignment local and global of circu lar strings the early graphical method for comparing nucleotide sequences dot matrices still yields one of the best visual representations of sequence similarities the axes in a dot matrix correspond to the two sequences v vn and w wm a dot is placed at coordinates i j if the substrings si si k and tj tj k are sufficiently similar two such substrings are considered to be sufficiently similar if the hamming distance between them is at most d when the sequences are very long it is not necessary to show exact coordinates figure is based on the sequences corresponding to the β globin gene in human and mouse in these plots each axis is on the order of base pairs long k and d problem use figure to answer the following questions how many exons are in the human β globulin gene the dot matrix in figure top is between the mouse and human genes i e all introns and exons are present do you think the number of exons in the β globulin gene is different in the human genome as compared to the mouse genome label segments of the axes of the human and mouse genes in figure to show where the introns and exons would be located a local alignment between two different strings v and w finds a pair of substrings one in v and the other in w with maximum similarity suppose that we want to find a pair of nonover lapping substrings within string v with maximum similarity optimal inexact repeat problem computing an optimal local alignment between v and v does not solve the problem since the resulting alignment may correspond to overlapping substrings problem 36 devise an algorithm for the optimal inexact repeat problem figure human β globulin cdna vs the gene sequence in two organisms in the chimeric alignment problem a string v and a set of strings wn are given and the problem is to find i j n v wi wj where wi wj is the concatenation of wi and wj stand for the score of optimal global alignment problem 37 devise an efficient algorithm for the chimeric alignment problem a virus infects a bacterium and modifies a replication process in the bacterium by inserting at every a a polya of length to at every c a polyc of length to at every g a polyg of arbitrary length at every t a polyt of arbitrary length no gaps or other insertions are allowed in the virally modified dna for example the sequence aaataaaggggccccctttttttcc is an infected version of atagctc problem given sequences v and w describe an efficient algorithm that will determine if v could be an infected version of w problem 39 now assume that for each nucleotide a c g t the virus will either delete a letter or insert a run of the letter of arbitrary length give an efficient algorithm to detect if v could be an infected version of w under these circumstances problem define homodeletion as an operation of deleting a run of the same nucleotide and homoinsertion as an operation of inserting a run of the same nucleotide for exam ple acaaaaaagctttta is obtained from acgctttta by a homoinsertions of a run of six a while acgcta is obtained from acgctttta by homodeletion of a run of three t the homo edit distance between two sequences is defined as the mini mum number of homodeletions and homoinsertions to transform one sequence into another give an efficient algorithm to compute the homoedit distance between two arbitrary strings problem 41 suppose we wish to find an optimal global alignment using a scoring scheme with an affine mismatch penalty that is the premium for a match is the penalty for an indel is ρ and the penalty for x consecutive mismatches is ρ σx give an o nm algorithm to align two sequences of length n and m with an affine mismatch penalty explain how to construct an appropriate manhattan graph and estimate the running time of your algorithm problem 42 define a nodiagonal global alignment to be an alignment where we disallow matches and mismatches that is only indels are allowed give a θ nm algorithm to de termine the number of nodiagonal alignments between a sequence of length n and a sequence of length m give a closed form formula for the number of nodiagonal global alignments e g something of the form f n m n πnm problem estimate the number of different not necessarily optimal global alignments between two n letter sequences problem devise an algorithm to compute the number of distinct optimal global alignments optimal paths in edit graph between a pair of strings problem 45 estimate the number of different not necessarily optimal local alignments between two n letter sequences problem 46 devise an algorithm to compute the number of distinct optimal local alignments op timal paths in local alignment edit graph between a pair of strings problem 47 let si j be a dynamic programming matrix computed for the lcs problem prove that for any i and j the difference between si j and si j is at most let in be a sequence of numbers a subsequence of in is called an increasing subsequence if elements of this subsequence go in increasing order decreasing subsequences are defined similarly for example elements of the sequence form an increasing subsequence while elements form a decreasing subsequence problem devise an efficient algorithm for finding longest increasing and decreasing subse quences in a permutation of integers problem show that in any permutati on of n distinct integers there is either an increasi ng sub sequence of length at least n or a decreasing subsequence of length at least n a subsequence σ of permutation π is increasing if as a set it can be written as σ where and are increasing subsequences of π for example and are increasing subsequences of π forming a increasing subsequence consisting of six elements problem devise an algorithm to find a longest increasing subsequence rnas adopt complex three dimensional structures that are important for many biological func tions pairs of positions in rna with complementary nucleotides can form bonds bonds i j and il jl are interleaving if i il j jl and noninterleaving otherwise fig every set of noninterleaving bonds corresponds to a potential rna structure in a very naive formu lation of the rna folding problem one tries to find a maximum set of noninterleaving bonds the more adequate model attempting to find a fold with the minimum energy is much more difficult problem develop a dynamic programming algorithm for finding the largest set of noninter leaving bonds given an rna sequences the human genome can be viewed as a string of n billion nucleotides partitioned into substrings representing chromosomes however for many decades biologists used a different band representation of the genome that is obtained via traditional light microscopy figure shows bands as seen on chromosme out of observable bands for the entire human genome although several factors e g local g c frequency have been postulated to govern the formation of these banding patterns the mechanism behind their formation remains poorly understood a mapping between the human genomic sequence which itself only became avail able in and the banding pattern representation would be useful to leverage sequence level a interleaving bonds g u g u a c a c g g u g u c a c a g g u g u a c a c g g u g u a c g a c b non interleaving bonds figure interleaving and noninterleaving bonds in rna folding figure band patterns on human chromosome gene information against diseases that have been associated with certain band positions how ever until recently no mapping between these two representations of the genome has been known the band positioning problem is to find the starting and ending nucleotide positions for each band in the genome for simplicity we assume that all chromosomes are concatenated to form a single coordinate system in other words the band positioning problem is to find an increasing array start b that contains the starting nucleotide position for each band b in the genome each band b begins at the nucleotide given by start b and ends at start b a naive approach to this problem would be to use observed band width data to compute the nucleotide positions however this solution is inaccurate because it assumes that band width is perfectly correlated with its length in nucleotides in reality this correlation is often quite poor and a different approach is needed in the last decade biologists have performed a large number of fish fluorescent in situ hybridiza tion experiments that can help to solve the band positioning problem fish data consist of pairs x b where x is a position in the genome and b is the index of the band that contains x fish data are often subject to experimental error so some fish data points may contradict each other given a solution start b b of the band positioning problem we define its fish quality as the number of fish experiments that it supports that is the number of fish experi ments x b such that start b x start b problem find a solution to the band positioning problem that maximizes its fish quality the fish quality parameter ignores the width of the bands a more adequate formulation is to find an optimal solution of the band positioning problem that is consistent with band width data that is the solution that minimizes width i start b start b b where width i is the estimated width of the ith band problem 53 find an optimal solution of the band positioning problem that minimizes x width i start b start b b problem 54 describe recurrence relations for multiple alignment of sequences under the sp sum of pairs scoring rule for simplicity we assume that start n thus implying that the last band starts at the nucleotide start and ends at n problem develop a likelihood ratio approach and design an algorithm that utilizes codon us age arrays for gene prediction problem consider the exon chaining problem in the case when all intervals have the same weight design a greedy algorithm that finds an optimal solution for this limited case of the problem problem 57 estimate the running time of the spliced alignment algorithm improve the running time by transforming the spliced alignment graph into a graph with a smaller number of edges this transformation is hinted at in figure introns are spliced out of pre mrna during mrna processing and biologists can perform cdna sequencing that provides the nucleotide sequence complementary to the mrna the cdna therefore represents the concatenation of exons of a gene consequently the exon intron structure can be determined by aligning the cdna against the genomic dna with the aligned regions representing the exons and the large gaps representing the introns this alignment can be aided by the knowledge of the conserved donor and acceptor splice site sequences gt at the splice site and ag at the splice site while a spliced alignment can be used to solve this cdna alignment problem there exists a faster algorithm to align cdna against genomic sequence one approach is to introduce gap penalties that would adequately account for gaps in the cdna alignment problem when aligning cdna against genomic sequences we want to allow long internal gaps in the cdna sequence in addition long gaps that respect the consensus sequences at the intron exon junctions are favored over gaps that do not satisfy this property such gaps that exceed a given length threshold and respect the donor and acceptor sites should be assigned a constant penalty this penalty is lower than the affine penalty for long gaps that do not respect the splice site consensus the input to the cdna alignment problem is genomic sequence v cdna sequence w match mismatch gap opening and gap extension parameters as well as l minimum intron length and δl fixed penalty for gaps longer than l that respect the consensus sequences the output is an alignment of v and w where aligned regions represent putative exons and gaps in v represent putative introns problem devise an efficient algorithm for the cdna alignment problem the spliced alignment algorithm finds exons in genomic dna by using a related protein as a template what if a template is not a protein but another uninterpreted genomic dna se quence or in other words can unannotated mouse genomic dna be used to predict human genes problem 59 generalize the spliced alignment algorithm for alignment of one genomic sequence against another problem for simplicity the spliced alignment problem assumes that the genomic sequence and the target protein sequence are both written in the same alphabet modify the recurrence relations to handle the case when they are written in different alphabets specifically proteins are written in a twenty letter alphabet and dna is written in a four letter alphabet divide and conquer algorithms as the name implies a divide and conquer algorithm proceeds in two dis tinct phases a divide phase in which the algorithm splits a problem instance into smaller problem instances and solves them and a conquer phase in which it stitches the solutions to the smaller problems into a solution to the bigger one this strategy often works when a solution to a large problem can be built from the solutions of smaller problem instances divide and conquer algorithms are often used to improve the efficiency of a polynomial algorithm for example by solving a problem in o n log n time that would otherwise require quadratic time divide and conquer approach to sorting in chapter we introduced the sorting problem and developed an algorithm that required o time to sort a list of integers the divide and conquer approach gives us a faster sorting algorithm suppose that instead of a single list of n integers in an arbitrary order we have two lists a of length and b of length each with approximately n elements but these two lists are both sorted how could we make a sorted list of n elements from these a reasonable approach is to traverse each list simultaneously as if each were a sorted stack of cards picking the smaller element on the top of either pile the merge algorithm below combines two sorted lists into a single sorted list in o time merge a b size of a size of b an1 i j for k to if ai bj ck ai i i else ck bj j j return c in order to use merge to sort an arbitrary list we made an inductive leap somehow we were presented with two half size lists that were already sorted it would seem to be impossible to get this input without actually solving the sorting problem to begin with however the merge algorithm is easily applied if we have a list c with only two elements break c into two lists each list with one element since those sublists are sorted a list of one element is always sorted then we can merge them into a sorted element list if c has four elements we can still break it into two lists each with two elements sort each of the two element lists and merge the resulting sorted lists afterward in fact the same general idea applies to an arbitrary list and gives rise to the mergesort algorithm mergesort c n size of c if n return c left list of first n elements of c right list of last n n elements of c sortedleft mergesort left sortedright mergesort right sortedlist merge sortedleft sortedright return sortedlist mergesort comprises two distinct phases a divide phase in lines where its input is split into parts and sorted and a conquer phase in line where the sorted sublists are then combined into a sorted version of the input list in order to calculate the efficiency of this algorithm we need to account for the time spent by mergesort in each phase we will use t n to represent the amount of time spent in a call to merge sort for a list with n elements this involves two calls to mergesort on lists of size n as well as a single call to merge if merge is called on two lists each of size n it will require o n n o n time to merge them this leads to the following recurrence relation where c is used to denote a positive constant t n n cn t the solution to this recurrence relation is t n o n log n a fact that can be verified through mathematical induction another way to establish the o n log n running time of mergesort is to construct the recursion tree fig and to notice that it consists of log n levels at the top level you have to merge two lists each with n elements requiring o n n o n time at the second level there are four lists each with n elements requiring o n n n n o n time at the ith level there are lists each with n elements again requiring o n time therefore merging requires overall o n log n time since there are log n levels in the recursion tree space efficient sequence alignment as another illustration of divide and conquer algorithms we revisit the se quence alignment problem from chapter when comparing long dna fragments the limiting resource is usually not the running time of the algorithm but the space required to store the dynamic programming table in daniel hirschberg proposed a divide and conquer approach that performs alignment in linear space at the ex pense of doubling the computational time the time complexity of the dynamic programming algorithm for aligning sequences of lengths n and m respectively is proportional to the number of edges in the edit graph or o nm on the other hand the space complex ity is proportional to the number of vertices in the edit graph which is also o nm however if we only want to compute the score of the alignment rather than the alignment itself then the space can be reduced to just twice the number of vertices in a single column of the edit graph that is o n this reduction comes from the observation that the only values needed to compute the alignment scores in column j are the alignment scores in col umn j fig therefore the alignment scores in the columns before j can be discarded while computing the alignment scores for columns j j m unfortunately to find the longest path in the edit graph re how many times do you need to divide an array in half before you get to single element sets divide conquer figure the recursion tree for mergesort the divide upper part consists of log levels not counting the root where the input is split into pieces the conquer lower part consists of the same number of levels where the split pieces are merged back together figure calculating an alignment score requires no more than space for an n n alignment problem computing the alignment scores in each column requires only the scores in the preceding column we show here the dynamic programming array the data structure that holds the score at each vertex instead of the graph quires backtracking pointers for the entire edit graph therefore the entire backtracking matrix b bi j needs to be stored causing the o nm space requirement however we can finesse this to require only o n space the longest path in the edit graph connects the source vertex with the sink vertex n m and passes through some unknown middle vertex mid m that is the vertex somewhere on the middle column m of the graph fig the key observation is that we can find this middle ver tex without actually knowing the longest path in the edit graph we define length i as the length of the longest path from to n m that passes through the vertex i m in other words out of all paths from to n m length i is the length of the longest of the ones that pass through i m since the middle vertex mid m lies on the longest path from the source to the sink length mid i n length i below we show that length i can be efficiently computed without knowing the longest path we will assume for simplicity that m is even and concentrate on finding only the middle vertex rather than the entire longest path vertex i m splits the length i long path into two subpaths which we will call prefix and suffix the prefix subpath runs from the source to i m and has length prefix i the suffix subpath runs from i m to the sink and has length suffix i it can be seen that length i prefix i suffix i and an important observation is that prefix i and suffix i are actually very easy to compute in linear space indeed pref ix i is simply the length of the longest path from to i m and is given by si m also suffix i is the length of the longest path from i m to n m or equivalently the length of the longest path from the sink n m to i m in the graph with all edges reversed therefore suffix i can be computed as a longest path in this re versed edit graph computing length i for i n can be done in linear space by com puting the scores si m lengths of the prefix paths from to i m for i n and the scores of the paths from i m to n m which can be computed as the score sreverse of the path from n m to i m in the reversed i m edit graph the value length i prefix i suffix i si m sreverse is the i m length of the longest path from to n m passing through the vertex i m therefore i n length i computes the length of the longest path and determines mid computing all length i values requires time equal to the area of the left rectangle from column to m plus the area of the right rectangle from col umn m to m and the space o n as shown in figure after the middle vertex mid m is found the problem of finding the longest path from to n m can be partitioned into two subproblems finding the longest path from to the middle vertex mid m and finding the longest path from the middle vertex mid m to n m instead of trying to find these paths we first try to find the middle vertices in the corresponding smaller rectangles fig this can be done in the time equal to the area of these rectangles which is half as large as the area of the original rectangle proceeding in this way we will find the middle vertices of all rectangles in time proportional to area area area area and therefore compute the longest path in time o nm and space o n path source sink if source and sink are in consecutive columns output longest path from source to sink else mid middle vertex i m with largest score length i path source mid path mid sink block alignment and the four russians speedup we began our analysis of sorting with the quadratic selectionsort algo rithm and later developed the mergesort algorithm with o n log n run ning time a natural question to ask is whether one could design an even faster sorting algorithm perhaps a linear one alas for the sorting prob lem there exists a lower bound for the complexity of any sorting algorithm essentially stating that it will require at least ω n log n operations there fore it makes no sense to improve upon mergesort with the expectation of improving the worst case running time though improving the practical running time is worth the effort similarly we began our analysis of the global alignment problem from the dynamic programming algorithm that requires o time to align two n nucleotide sequences but never asked whether an even faster alignment algorithm existed could it be possible to reduce the running time of the alignment algorithm from o to o n log n nobody has an answer to this question because nontrivial lower bounds for the global alignment problem remain unknown an o n log n alignment algorithm would revolutionize bioinformatics and would likely be the demise of the popular blast algo rithm although nobody knows how to design an o n log n algorithm for global alignment there exists a subquadratic o algorithm for a similar longest common subsequence lcs problem this result relies on certain assumptions about the nature of computation which are not really germane to this book as an example if you had an unlimited supply of computers sorting a list in parallel you could perhaps sort faster one cannot simply argue that the problem requires o time since one has to traverse the entire dynamic programming table because the problem might be solved by some ingenious technique that does not rely on a dynamic programming recurrency linear space sequence alignment m m m m i n n n m n m n m n m n m n m n m n m n m n m figure space efficient sequence alignment the computational time i e the area of the solid rectangles decreases by a factor of at every iteration figure two paths in a grid partitioned into subgrids of size the black path a is a block path while the gray path b is not let u un and v vn be two dna sequences partitioned into blocks of length t that is u ut ut un t un and v vt vt vn t vn for simplicity we assume that u and v have the same length and that it is divisible by t for example if t were one could view u and v as dna sequences of genes partitioned into codons the block alignment of u and v is an alignment in which every block in one sequence is either aligned against an entire block in the other sequence or is inserted or deleted as a whole to be sure the alignment path within a block can be completely arbitrary it simply needs to enter and leave the block through vertices on the corners figure shows an n n grid partitioned into t t subgrids a path in this edit graph is called a block path if it traverses every t t square through its corners i e enters and leaves every block at bold vertices an equivalent statement of this definition is that a block path contains at least two bold vertices from every square that it passes through it cannot for example lop off a corner of a square block alignments correspond to block paths in the edit graph and the block alignment problem is to find the highest scoring or longest block path through this graph we will see below that when t is on the order of the logarithm of the overall sequence length neither too small nor too large we can solve this problem in less than quadratic time block alignment problem find the longest block path through an edit graph input two sequences u and v partitioned into blocks of size t output the block alignment of u and v with the maximum score i e the longest block path through the edit graph one can consider n n pairs of blocks each pair defines a square in the t t edit graph and compute the alignment score βi j for each pair of blocks u i t ui t and v j t vj t this amounts to solving n n mini t t alignment problems of size t t each and takes o n n t t o time t t if si j denotes the optimal block alignment score between the first i blocks of u and the first j blocks of v then si j max si j σblock si j σblock β i j i j where σblock is the penalty for inserting or deleting the entire block the indices i and j in this recurrence vary from to n and therefore the running time of this algorithm is o if we do not count time to precompute β i j for i j n this approach allows one to solve the block alignment problem for any value of t but as we saw before precomputing all βi j takes the same o time that the dynamic programming algorithm takes the speed reduction we promised is achieved by the four russians tech nique when t is roughly log n instead of constructing n n minialignments t t for all pairs of blocks from u and v we will construct minialignments in the simplest case σblock σt where σ is the penalty for the insertion or deletion of a nucleotide since the block alignment problem takes a partitioned grid as input the algorithm does not get to make a choice for the value of t for all pairs of t nucleotide strings and store their alignment scores in a large lookup table at first glance this looks counterproductive but if t log n then n which is much smaller than n n n n t t the resulting lookup table which we will call score has only n entries computing each of the entries takes o log n log n time so the over all running time to compute all entries of this table is only o n log n we emphasize that the resulting two dimensional lookup table score is in dexed by a pair of t nucleotide strings thus leading to a slightly different recurrence si j max si j σblock si j σblock i j score ith block of v jth block of u since the time to precompute the lookup table score in this case is rel atively small the overall running time is dominated by the dynamic pro gramming step for example by the n n accesses it makes to the lookup t t table since each access takes o log n time the overall running time of this algorithm is o constructing alignments in subquadratic time so now we have an algorithm for the block alignment problem that is sub quadratic for convenient values of one of its input parameters but it is not clear whether similar ideas could be used to solve any of the problems from chapter in this section we show how to design a o algorithm for finding the longest common subsequence of two strings again using the four russians speedup unlike the block path in a partitioned edit graph the path corresponding to a longest common subsequence can traverse the edit graph arbitrarily and does not have to pass through the bold vertices of figure therefore pre computing the length of paths between the upper left corner and lower right corner of every t t subsequence is not going to help instead we will select all vertices at the borders of the squares shown by bold vertices in figure rather than just the vertices at the corners as in figure this results in a significantly larger number of bold vertices than in the case of block alignments but we can keep the number subquadratic taken together these vertices form n whole rows and n whole columns in t t the edit graph the total number of bold vertices is o we will perform figure 5 the partitioned edit graph for the lcs problem dynamic programming on only the o bold vertices effectively ignoring the internal vertices in the edit graph in essence we are interested in the following problem given the alignment scores si in the first row and the alignment scores j in the first column of a t t minisquare compute the alignment scores in the last row and column of the minisquare the values of in the last row and last column depend entirely on four variables the values si in the first row of the square the values j in the first column of the square and the two t long substrings corresponding to the rows and columns of the square of course we could use this information to fill in the entire dynamic programming matrix for a t t square but we cannot afford doing this timewise if we want to have a subquadratic algorithm to use the four russians technique we again rely on the brute force men tality and build a lookup table on all possible values of the four variables all pairs of t nucleotide sequences and all pairs of possible scores for the first row si and the first column j for each such quadruple we store the precomputed scores for the last row and last column however this will be an enormous table since there may be a large number of possible scores for the first row and first column therefore we perform some trickery a care ful analysis of the lcs problem shows that the possible alignment scores in the first row or first column are not entirely arbitrary is a possible sequence of scores but 5 is not not only does the progression of scores have to be monotonically increasing but adjacent ele ments cannot differ by more than see problem 47 we can encode this as a binary vector of differences the above example would be en coded as thus since there are possible scores and possible strings the entire lookup table will require 4t space again we set t log n to make the size of the table collapse down to log n 5 alas this allows the precomputation step to be subquadratic and the run ning time of the algorithm is dominated by the process of filling in the scores for the bold vertices in figure 5 which takes o time 5 notes mergesort was invented in by the legendary john von neumann while he was designing edvac the world first stored program electronic computer the idea of using a divide and conquer approach for sequence comparison was proposed first by daniel hirschberg in for the lcs problem and then in by eugene myers and webb miller for the local alignment problem the four russians speedup was proposed by vladimir arlazarov efim dinic mikhail kronrod and igor faradzev in and first applied to sequence comparison by william masek and michael paterson is 4 ics as his new field he says webb miller born in washing ton state is professor in the depart ments of biology and of computer sci ence and engineering at pennsylvania state university he holds a phd in mathematics from the university of wash ington he is a pioneer and a leader in the area of dna and protein sequence comparison and in comparing whole genomes in particular for a number of years miller worked on computational techniques for under standing the behavior of computer pro grams that use floating point arithmetic in he completely changed his re search focus after picking bioinformat my reason for wanting a complete change was simply to bring more adventure and excitement into my life bioinformatics was attractive because i had no idea what the field was all about and because neither did anyone else at that time the catalyst was his friendship with gene myers who was already work ing in the new area it wasn t even called bioinformatics then miller was switching to a field without a name he loved the frontier spirit of the emerg ing discipline and the possibility of doing something useful for mankind the change was difficult for me because i was completely ignorant of biology and statistics it took a number of years before i really started to understand biology i m now on the faculty of a biology department so in some sense i successfully made the transition unfortunately i m still basically ignorant of statistics in another respect the change was easy because there was so little already known about the field i read a few papers by mike waterman and david sankoff and was off and running miller came to the new field armed with two skills that proved very use ful and with a couple of ideas that helped focus his research initially the skills were his mathematical training and his experience writing computer programs the first idea that he brought to the field was that an optimal alignment between two sequences can be computed in space proportional to the length of the longer sequence it is straightforward to compute the score of an optimal alignment in that amount of space but it is much less obvious how to produce an alignment with that score a very clever linear space alignment algorithm had been discovered by dan hirschberg around the other idea was that when two sequences are very similar and when alignments are scored rather simply an optimal alignment can be computed much more quickly than by dynamic programming using a greedy algo rithm that idea was discovered independently by gene myers with some prodding from miller and esko ukkonen in the mid miller hoped that these two ideas or variants of them would get him started in the new field he had solutions in search of biological problems rather than bio logical problems in search of solutions indeed this is a common mode of entry into bioinformatics for scientists trained in a quantitative field during his first decade in bioinformatics miller coauthored a few papers about linear space alignment methods finding a niche for greedy algo rithms took longer but for comparing very similar dna sequences partic ularly when the difference between them is due to sequencing errors rather than evolutionary mutations they are quite useful they deserve wider recog nition in the bioinformatics community than they now have the most successful of miller bioinformatics projects have involved ideas other than the ones he brought with him to the field his most widely known project was the collaboration to develop the blast program where it was david lipman insights that drove the project in the right direction how ever it is miller work on comparison methods for long dna sequences that brought him closer to biology and made miller algorithms a household name among teams of scientists analyzing mammalian and other whole genome sequences miller picked this theme as his holy grail around and he has stuck with it ever since when he started there were only two people in the world brave or foolish enough to publicly advocate sequenc ing the mouse genome and comparing it with the human genome miller and his long term collaborator the biologist ross hardison they occasion ally went so far as to tout the sequencing of several additional mammals nowadays it looks to everyone like the genome sequencing of mouse rat chimpanzee dog and so on was inevitable but perhaps miller many years of working on programs to compare genome sequences made the inevitable happen sooner what worked best for miller was to envision an advance in bioinformat ics that would foster new biological discoveries namely that development of methods to compare complete mammalian genome sequences would lead to a better understanding of evolution and of gene regulation and to do everything he could think of to make it happen this included developing algorithms that would easily align the longest sequences he could find and helping ross hardison to verify experimentally that these alignments are useful for studying gene regulation when miller and hardison decided to show how alignments and data from biological experiments could be linked through a database they learned about databases when they wanted to set up a network server to align dna sequences they learned about network servers when nobody in his lab was available to write software that they needed miller wrote it himself when inventing and analyzing a new al gorithm seemed important he worked on it the methods changed but the biological motivation remained constant miller has been more successful pursuing a biological problem in search of solutions than the other way around his colleague david haussler has had somewhat the same experience his considerable achievements bringing hidden markov models and other machine learning techniques to bioinfor matics have recently been eclipsed by his monumental success with the hu man genome browser which has directly helped a far wider community of scientists the most exciting point so far in my career is today with a new verte brate genome sequence coming my way every year some day i hope to look back with pride at my best achievement in bioinformatics but perhaps it hasn t happened yet problems problem construct the recursion tree for mergesort on the input 5 4 problem how much memory does mergesort need overall modify the algorithm to use as little as possible problem suppose that you are given an array a of n words sorted in lexicographic order and want to search this list for some arbitrary word perhaps w we write the number of characters in w as w design three algorithms to determine if w is in the list one should have o n w running time another should have o w log n running time but use no space except for a and w and the third should have o w running time but can use as much additional space as needed problem 4 we normally consider multiplication to be a very fast operation on a computer how ever if the numbers that we are multiplying are very large say digits then multiplication by the naive grade school algorithm will take a long time how long does it take write a faster divide and conquer algorithm for multiplication problem 5 develop a linear space version of the local alignment algorithm problem develop a linear space version of global sequence alignment with affine gap penal ties in the space efficient approach to sequence alignment the original problem of size n n is reduced to two subproblems of sizes i n and n i n for the sake of simplicity we assume that both sequences have the same length in a fast parallel implementation of sequence alignment it is desirable to have a balanced partitioning that breaks the original problem into subproblems of equal sizes problem design a space efficient alignment algorithm with balanced partitioning problem design a divide and conquer algorithm for the motif finding problem and estimate its running time have you improved the running time of the exhaustive search algo rithm problem explore the possibilities of using a divide and conquer approach for the median string problem can you split the problem into subproblems can you combine the solu tions of the subproblem into a solution to the main problem problem devise a space efficient dynamic programming algorithm for multiple alignment of three sequences write the corresponding recurrence relations for three n nucleotide sequences your algorithm should use at most quadratic o memory write the recursive algorithm that outputs the resulting alignment problem design a linear space algorithm for the block alignment problem problem write a pseudocode for constructing the lcs in subquadratic time graph algorithms many bioinformatics algorithms may be formulated in the language of graph theory the use of the word graph here is different than in many physical science contexts we do not mean a chart of data in a cartesian coordinate system in order to work with graphs we will need to define a few concepts that may not appear at first to be particularly well motivated by biological examples but after introducing some of the mathematical theory we will show how powerful they can be in such bioinformatics applications as dna sequencing and protein identification graphs figure a shows two white and two black knights on a chessboard can they move using the usual chess knight moves to occupy the posi tions shown in figure b needless to say two knights cannot occupy the same square while they are moving figure represents the chessboard as a set of nine points two points are connected by a line if moving from one point to another is a valid knight move figure shows an equivalent representation of the resulting dia gram that reveals that knights move around a cycle formed by points 4 and every knight move on the chessboard corresponds to moving to a neighboring point in the diagram in either a clockwise or counterclockwise direction therefore the white white black black knight arrangement cannot be transformed into the alternating white black white black arrangement in the game of chess knights the horses can move two steps in any of four directions left right up and down followed by one step in a perpendicular direction as shown in figure c a b c figure two configurations of four knights on a chessboard can you use valid knight moves to turn the configuration in a into the configuration in b valid knight moves are shown in c diagrams with collections of points connected by lines are examples of graphs the points are called vertices and lines are called edges a simple graph shown in figure consists of five vertices and six edges we de note a graph by g g v e and describe it by its set of vertices v and set of edges e every edge can be written as a pair of vertices the graph in figure is described by the vertex set v a b c d e and the edge set e a b a c b c b d c d c e the way the graph is actually drawn is irrelevant two graphs with the same vertex and edge sets are equivalent even if the particular pictures that represent the graph appear different see figure the only important feature of a graph is which vertices are connected and which are not a 4 5 4 5 b c figure a graph representation of a chessboard a knight sitting on some square can reach any of the squares attached to that square by an edge figure two equivalent representations of a simple graph with five vertices and six edges figure 4 represents another chessboard obtained from a 4 4 chessboard by removing the four corner squares can a knight travel around this board pass through each square exactly once and return to the same square it started on figure 4 b shows a rather complex graph with twelve vertices and sixteen edges revealing all possible knight moves however rearrang ing the vertices fig reveals the cycle that describes the correct sequence of moves the number of edges incident to a given vertex v is called the degree of the vertex and is denoted d v for example vertex in figure 4 c has degree while vertex 4 has degree the sum of degrees of all vertices is in this case vertices of degree and 4 vertices of degree twice the number of edges in the graph this is not a coincidence for every graph g with vertex set v and edge set e v v d v e indeed an edge connecting vertices v and w is counted in the sum v v d v twice first in the term d v and again in the term d w the equality v v d v e explains why you cannot connect fifteen phones such that each is connected to exactly seven others and why a country with exactly three roads out of every city cannot have precisely roads many bioinformatics problems make use of directed graphs in which ev ery edge is directed from one vertex to another as shown by the arrows in figure 5 every vertex v in a directed graph is characterized by indegree v the number of incoming edges and outdegree v the number of outgoing a b c figure 4 the knight tour through the twelve squares in part a can be seen by constructing a graph b and rearranging its vertices in a clever way c figure 5 a directed graph edges for every directed graph g v e indegree v outdegree v since every edge is counted once on the right hand side of the equation and once on the left hand side a graph is called connected if all pairs of vertices can be connected by a path which is a continuous sequence of edges where each successive edge begins where the previous one left off paths that start and end at the same vertex are referred to as cycles for example the paths 2 and 2 6 5 in figure 4 c are cycles graphs that are not connected are disconnected fig 6 disconnected graphs can be partitioned into connected components one can think of a graph as a map showing cities vertices and the freeways edges that con nect them not all cities are connected by freeways for example you cannot drive from miami to honolulu these two cities belong to two different con nected components of the graph a graph is called complete if there is an edge between every two vertices graph theory was born in the eighteenth century when leonhard euler solved the famous königsberg bridge problem königsberg is located on the banks of the pregel river with a small island in the middle the various parts of the city are connected by bridges fig and euler was interested in whether he could arrange a tour of the city in such a way that the tour vis its each bridge exactly once for königsberg this turned out to be impossible but euler basically invented an algorithm to solve this problem for any city a b figure 6 a connected a and a disconnected b graph figure bridges of königsberg bridge obsession problem find a tour through a city located on n islands connected by m bridges that starts on one of the islands visits every bridge exactly once and returns to the originating island input a map of the city with n islands and m bridges output a tour through the city that visits every bridge ex actly once and returns to the starting island figure shows a city map with ten islands and sixteen bridges as well as the transformation of the map into a graph with ten vertices and sixteen edges every island corresponds to a vertex and every bridge corresponds to an edge after this transformation the bridge obsession problem turns into the eulerian cycle problem that was solved by euler and later found thousands of applications in different areas of science and engineering eulerian cycle problem find a cycle in a graph that visits every edge exactly once input a graph g output a cycle in g that visits every edge exactly once after the königsberg bridge problem was solved graph theory was for gotten for a century before it was rediscovered by arthur cayley who stud ied the chemical structures of noncyclic saturated hydrocarbons 2 fig 9 structures of this type of hydrocarbon are examples of trees which are simply connected graphs with no cycles it is not hard to show that every tree has at least one vertex with degree or leaf 2 this observation immedi ately implies that every tree on n vertices has n edges regardless of the structure of the tree indeed since every tree has a leaf we can remove it and its attached edge resulting in another tree so far we have removed one edge and one vertex in this smaller tree there exists a leaf that we again remove so far we have removed two vertices and two edges we keep this up until we are left with a graph with a single vertex and no edges since we have removed n vertices and n edges the number of edges in every tree is n fig 2 actually every tree has at least two leaves except for the trivial single vertex tree a b figure a more complicated version of königsberg a to solve the bridge ob session problem euler transformed the map of königsberg into a graph b and found an eulerian cycle the path that runs through vertices 2 4 5 6 2 9 9 is an eulerian cycle h c h h h methane h h h c h h h ethane h h h h c c h h h h propane h h h h h c c c c h h h h h butane h h h c c h h c c h h h h h isobutane figure 9 hydrocarbons the saturated nonaromatic variety as chemists see them left and their graph representation right two different molecules with the same number of the same types of atoms are called structural isomers figure proving that a tree with n vertices has n edges figure can you travel from any one of the vertices in this graph visit every other vertex exactly once and end up at the original vertex shortly after cayley work on tree enumeration sir william hamilton in vented a game corresponding to a graph whose twenty vertices were labeled with the names of twenty famous cities fig the goal is to visit all twenty cities in such a way that every city is visited exactly once before re turning back to the city where the tour started as the story goes hamilton sold the game for pounds to a london game dealer and it failed miserably despite the commercial failure of a great idea the more general problem of finding hamiltonian cycles in arbitrary graphs is of critical importance to many scientific and engineering disciplines the problem of finding hamil tonian cycles looks deceivingly simple and somehow similar to the eulerian cycle problem however it turns out to be n p complete while the eulerian cycle problem can be solved in linear time hamiltonian cycle problem find a cycle in a graph that visits every vertex exactly once input a graph g output a cycle in g that visits every vertex exactly once if such a cycle exists graphs like many freeway maps often give some sort of weight to every edge as in the manhattan tourist problem in chapter 6 the weight of an edge may reflect depending on the context different attributes for exam ple the length of a freeway segment connecting two cities the number of tourist attractions along a city block and the alignment score between two amino acids are all natural weighting schemes weighted graphs are often formally represented as an ordered triple g v e w where v is the set of vertices in the graph e is the set of edges and w is a weight function de fined for every edge e in e i e w e is a number reflecting the weight of edge e given a weighted graph one may be interested in finding some shortest path between two vertices e g a shortest path between san diego and new york though this problem may sound difficult if you were given a com plicated road map it turns out that there exist fast algorithms to answer this question shortest path problem given a weighted graph and two vertices find the shortest distance between them input a weighted graph g v e w and two distin guished vertices and t output the shortest path between and t in graph g 3 the hamiltonian cycle problem is equivalent in complexity to the traveling salesman prob lem mentioned in chapter 2 and is therefore n p complete 2 3 4 5 6 a 2 3 4 5 6 b figure two different hypothetical structures of a gene a linear structures ex hibit very different interval graphs than the graphs exhibited by b branched struc tures it is impossible to construct a linear sequence of overlapping intervals that gives rise to the graph in b we emphasize that this problem is different and somewhat more comp licated than the longest path in a directed acyclic graph dag problem that we considered in chapter 6 graphs are powerful tools in many applied studies and their role goes well beyond analysis of maps of freeways for example at first glance the following basketball problem has nothing to do with graph theory fifteen teams play a basketball tournament in which each team plays with each other team prove that the teams can always be numbered from to in such a way that team defeated team 2 team 2 defeated team 3 team defeated team a careful analysis reduces this problem to finding a hamiltonian path in a directed graph on fifteen vertices 2 graphs and genetics conceived by euler cayley and hamilton graph theory flourished in the twentieth century to become a critical component of discrete mathematics in the seymour benzer applied graph theory to show that genes are linear at that time it was known that genes behaved as functional units of dna much like pearls on a necklace but the chemical structure and organization of the genes was not clear were genes broken into still smaller components if so how were they organized prior to watson and crick elucidation of the dna double helix it seemed a reasonable hypothesis that the dna content of genes was branched as in figure or even looped rather than linear these two organizations have very different topological implications which benzer exploited in an ingenious experiment benzer studied a large number of mutants which hap pened to have a continuous interval deleted from an important gene in their normal state nonmutant phages will kill a bacterium the mutant phages that were missing a segment of their genome could not kill the bac terium different mutants had different intervals deleted from the gene and benzer had to determine which interval had been deleted in each mutant though benzer did not know exactly where in the gene the mutant dele tion was he had a way to test whether two deletions i e their intervals overlapped relying on how two phages with two different deletions behave 4 a bacteriophage is a virus that attacks bacteria a b c d e a b figure an interval graph a and a graph that is not an interval graph b inside a bacterial host cell when two mutant phages with two different dele tions infect a bacterium two outcomes are possible depending on whether the deleted intervals overlap or not if they do not overlap then two phages combined have all the genetic material of one normal phage however if two deleted intervals overlap some genetic material is absent in both phages the benzer experiment was based on the observation that in the former case all genetic material of a normal phage is present two mutants are able to kill the bacterium and in the latter case where some genetic material is re moved in both phages the bacterium survives benzer infected bacteria with each pair of mutant strains from his phage and simply noted which pairs killed the bacterial host pairs which were lethal to the host were mutants whose deleted intervals did not overlap ben zer constructed a where each strain was a vertex and two vertices were connected when a double infection of a bacterial host was nonlethal he reasoned that if genes were linear fig a he would probably see one type of graph but if the genes were branched fig b he would see another given a set of n intervals on a line segment the interval graph is defined as a graph with n vertices that correspond to the intervals there is an edge between vertices v and w if and only if the intervals v and w over lap interval graphs have several important properties that make them easy to recognize for example the graph in figure a is an interval graph whereas the house graph in figure b is not benzer problem was equivalent to deciding whether the graph obtained from his bacteriophage experiment represented an interval graph had the experiment resulted in a graph like the house graph then the genes could not have been organized as linear structures as it turned out the graph was indeed an interval graph indicating that genes were composed of linearly organized functional units 3 dna sequencing imagine several copies of a magazine cut into millions of pieces each copy is cut in a different way so a piece from one copy may overlap pieces from another assuming that some large number of pieces are just sort of lost and the remaining pieces are splashed with ink can you recover the original text this essentially is the problem of fragment assembly in dna sequenc ing classic dna sequencing technology allows one to read short to nucleotide sequences per experiment each fragment corresponding to one of the many magazine pieces assembling the entire genome from these short fragments is like reassembling the magazine from the millions of tiny slips of paper 6 both problems are complicated by unavoidable experimental errors ink splashes on the magazine and mistakes in reading nucleotides furthermore the data are frequently incomplete some magazine pieces get lost while some dna fragments never make it into the sequencing ma chine nevertheless efforts to determine the dna sequence of organisms have been remarkably successful even in the face of these difficulties 5 it is not clear that he actually knew anything about graph theory at the time but graph theo rists eventually noticed his work 6 we emphasize that biologists reading these to nucleotide sequences have no idea where they are located within the entire dna string two dna sequencing methods were invented independently and simul taneously in cambridge england by fred sanger and in cambridge mas sachusetts by walter gilbert sanger method takes advantage of how cells make copies of dna cells copy a strand of dna nucleotide by nucleotide in a reaction that adds one base at a time sanger realized that he could make copies of dna fragments of different lengths if he starved the reaction of one of the four bases a cell can only copy its dna while it has all of the bases in supply for a sequence acgtaagcta starving at t would produce a mixture of the fragments acg and acgtaagc by running one starvation experiment for each of a t g and c and then separating the resulting dna fragments by length one can read the dna sequence each of four star vation experiments produces a ladder of fragments of varying lengths called the sanger ladder this approach culminated in the sequencing of a nucleotide virus in and a nobel prize shortly thereafter since then the amount of dna sequence data has been increasing exponentially particu larly after the launch of the human genome project in by it had produced the roughly 3 billion nucleotide sequence of the human genome within the past twenty years dna sequencing technology has been de veloped to the point where modern sequencing machines can sequence to nucleotide dna fragments called sequencing reads these reads then have to be assembled into a continuous genome which turns out to be a very hard problem even though the dna reading process has become quite automated these machines are not microscope like devices that simply scan nucleotides as if they were a sentence in a book the dna sequencing machines measure the lengths of dna fragments in the sanger ladder but even this task is difficult we cannot measure a single dna fragment but must measure billions of identical fragments shotgun sequencing starts with a large sample of genomic dna the sam ple is sonicated a process which randomly partitions each piece of dna in the sample into inserts the inserts that are smaller than nucleotides are removed from further consideration before the inserts can be read each one must be multiplied billions of times so that it is possible to read the ladders produced by sanger technique to amplify the inserts a sample is cloned into a vector and this vector used to infect a bacterial host as the bacterium reproduces it creates a colony that contains billions of copies of the vector later sanger found chemicals so called dideoxynucleotides that could be inserted in place of a t g or c and cause a growing dna chain to end the sanger ladder for t shows the lengths of all sub fragments ending at t and therefore reveals the set of positions where t occurs and its associated insert as a result the cloning process results in the pro duction of a large sample of one particular insert that can then be sequenced by the sanger method usually only the first to nucleotides of the in sert can be interpreted from this experiment dna sequencing is therefore a two stage process including both experimental reading nucleotide sequences form different inserts and computational assembling these reads into a single long sequence components 4 shortest superstring problem since every string or read that we sequence came from the much longer ge nomic string we are interested in a superstring of the reads that is we want a long string that explains all the reads we generated however there are many possible superstrings to choose from for example we could concate nate all the reads together to get a not very helpful superstring we choose to be most interested in the shortest one which turns out to be a reasonable first approximation to the unknown genomic dna sequence with this in mind the simplest approximation of dna sequencing corresponds to the following problem shortest superstring problem given a set of strings find a shortest string that contains all of them input strings sn output a string that contains all strings sn as substrings such that the length of is as small as possible figure presents two superstrings for the set of all eight three letter strings in a alphabet the first trivial superstring is obtained by the concatenation of all eight strings while the second one is a shortest super string define overlap si sj to be the length of the longest prefix of sj that matches a suffix of si the shortest superstring problem can be cast as a traveling salesman problem in a complete directed graph with n vertices correspond ing to strings sn and edges of length overlap si sj fig this reduction of course does not lead to an efficient algorithm since the tsp is n p complete moreover it is known that the shortest superstring problem is itself n p complete so that a polynomial algorithm for this problem is un the shortest superstring problem set of strings concatenation superstring shortest superstring figure superstrings for the set of eight three letter strings in a alphabet concatenating all eight strings results in a letter superstring while the shortest superstring contains only letters the shortest superstring in this case represents a solution of the clever thief problem it is the minimum string of tests a thief has to conduct to try all possible k letter passwords for a combination lock likely the early dna sequencing algorithms used a simple greedy strategy repeatedly merge a pair of strings with maximum overlap until only one string remains it has been conjectured but not yet proved that this greedy algorithm has performance guarantee 2 5 dna arrays as an alternative sequencing technique when the human genome project started dna sequencing was a routine but time consuming and hard to automate procedure in four groups of biologists independently and simultaneously suggested a different sequenc ing technique called sequencing by hybridization sbh involves building a miniature dna array also known as a dna chip that contains thousands of short dna fragments called probes each of these short fragments reveals figure the overlap graph for the eight strings in figure whether or not a known but short sequence occurs in the unknown dna sequence all these pieces of information together should reveal the identity of the target dna sequence given a short probe an to nucleotide single stranded synthetic dna fragment and a single stranded target dna fragment the target will hy bridize with the probe if the probe is a substring of the target watson crick complement when the probe and the target are mixed together they form a weak chemical bond and stick together for example a probe accgtgga will hybridize to a target ccctggcaccta since it is complementary to the substring tggcacct of the target in almost nobody believed that the idea of using dna probes to se quence long genomes would work because both the biochemical problem of synthesizing thousands of short dna fragments and the combinatorial prob lem of sequence reconstruction appeared too complicated shortly after the first paper describing dna arrays was published the journal science wrote that given the amount of work required to synthesize a dna array using dna arrays for sequencing would simply be substituting one horrendous task for another a major breakthrough in dna array technology was made by steve fodor and colleagues in their approach to array manufac turing relies on light directed polymer synthesis 9 which has many similarities to computer chip manufacturing fig using this technique building an array with all probes of length l requires just 4 l separate reactions rather than the presumed reactions with this method a california based biotechnology company affymetrix built the first kb dna array in today building mb or larger arrays is routine and the use of dna arrays has become one of the most widespread new biotechnologies sbh relies on the hybridization of the target dna fragment against a very large array of short probes in this manner probes can be used to test the unknown target dna to determine its l mer composition the universal dna array contains all probes of length l and is applied as follows fig attach all possible probes of length l l in the first sbh papers to a flat surface each probe at a distinct and known location this set of probes is called the dna array apply a solution containing fluorescently labeled dna fragment to the array the dna fragment hybridizes with those probes that are complementary to substrings of length l of the fragment using a spectroscopic detector determine which probes hybridize to the dna fragment to obtain the l mer composition of the target dna frag ment apply the combinatorial algorithm described below to reconstruct the se quence of the target dna fragment from the l mer composition 9 light as in photons not light as in not heavy the l mer composition of a string is simply the set of all l mers present in the string for example the mer composition of ccctggcaccta is ccctggca cctggcac ctggcacc tggcacct ggcaccta figure a genechip produced by affymetrix picture courtesy of affymetrix inc 6 sequencing by hybridization given an unknown dna sequence an array provides information about all strings of length l that the sequence contains but does not provide in formation about their positions in the sequence for a string of length n the l mer composition or spectrum of is the multiset of n l l mers in and is written spectrum l if l 3 and tatggtgc then spectrum l tat atg tgg ggt gtg tgc we can now formu late the problem of sequencing a target dna fragment from its dna array data the l mers in this spectrum are listed in the order of their appearance in creating the impression that we know which nucleotide occur at each position we emphasize that the order of these l mers in is unknown and it is probably more appropriate to list them in lexicographic order like atg ggt gtg tat tgc tgg universal dna array aa at ag ac ta tt tg tc ga gt gg gc ca ct cg cc aa at ag ac ta tt tg tc ga gt gg gc ca ct cg cc dna target tatccgttt complement of ataggcaaa hybridizes to the array of all 4 mers a t a g g c a a a a t a g t a g g a g g c g g c a g c a a c a a a figure hybridization of tatccgttt with the universal dna array consisting of all 4 mers sequencing by hybridization sbh problem reconstruct a string from its l mer composition input a set s representing all l mers from an unknown string output string such that spectrum l s although conventional dna sequencing and sbh are very different ex perimental approaches you can see that the corresponding computational problems are quite similar in fact sbh is a particular case of the shortest superstring problem when the strings sn represent the set of all sub strings of of fixed size however in contrast to the shortest superstring problem there exists a simple linear time algorithm for the sbh problem notice that it is not a contradiction that the shortest superstring problem is n p complete yet we claim to have a linear time algorithm for the sbh problem since the shortest superstring problem is more general than the sbh problem although dna arrays were originally proposed as an alternative to con ventional dna sequencing de novo sequencing with dna arrays remains an unsolved problem in practice the primary obstacle to applying dna arrays for sequencing is the inaccuracy in interpreting hybridization data to distinguish between perfect matches i e l mers present in spectrum l and highly stable mismatches i e l mers not present in spectrum l but with sufficient chemical bonding potential to generate a strong hybridiza tion signal this is a particularly difficult problem for the short probes used in universal arrays as a result dna arrays have become more popular in gene expression and studies of genetic variations both of which are done with longer probes than in de novo sequencing in contrast to sbh where the target dna sequence is unknown these approaches assume that the dna sequence is either known or almost known i e known up to a small number of mutations for example to detect genetic variations one can design twenty to thirty nucleotide probes to reliably detect mutations bypassing the still unsolved problem of distinguishing perfect matches from highly stable mismatches in the case of short probes to detect mutations in in gene expression analysis a solution containing mrna rather than dna is applied to the array with the goal of figuring out whether a given gene is switched on or switched off in this case absence of a hybridization signal indicates that a gene is not being transcribed into an mrna and is therefore switched off sbh as a hamiltonian path problem sequence reconstruction hamiltonian path approach s atg h agg tgc tcc gtc ggt gca cag vertices l tuples from the spectrum s edges overlapping l tuples path visiting all vertices corresponds to sequence reconstruction atgcaggtcc figure sbh and the hamiltonian path problem the known sequence s an array should contain all mers from s as well as selected mutated versions of these mers sbh as a hamiltonian path problem two l mers p and q overlap if overlap p q l that is the last l letters of p coincide with the first l letters of q given the measured spectrum spectrum l of a dna fragment construct a directed graph h by introducing a vertex for every l mer in spectrum l and connect every two vertices p and q by the directed edge p q if p and q overlap there is a one to one correspondence between paths that visit each vertex of h exactly once and dna fragments with the spectrum spectrum l the spectrum presented in figure corresponds to the sequence reconstruction atgcaggtcc which is the only path visiting all vertices of h atg tgc gca cag agg ggt gtc tcc the spectrum shown in figure yields a more complicated graph with two hamiltonian paths each path corresponding to two possible reconstruc tions atgcgtggca and atggcgtgca as the overlap graph becomes in practice the mutated versions of an l mer are often limited to the 3 l mers with mutations at the middle position arrays constructed in this manner are called tiling arrays multiple sequence reconstructions hamiltonian path approach s atg tgg tgc gtg ggc gca gcg cgt h atgcgtggca atggcgtgca figure spectrum s yields two possible reconstructions corresponding to dis tinct hamiltonian paths larger this approach ceases to be practically useful since the hamiltonian path problem is n p complete sbh as an eulerian path problem as we have seen reducing the sbh problem to a hamiltonian path problem does not lead to an efficient algorithm fortunately reducing sbh to the eulerian path problem in a directed graph which leads to the simple linear time algorithm for sequence reconstruction mentioned earlier a directed path is a path vn from vertex to vertex vn in which every edge vi vi is directed from vi to vi the reduction of the sbh problem to an eulerian path problem is to con struct a graph whose edges rather than vertices correspond to l mers from spectrum l and then to find a path in this graph visiting every edge ex actly once in this approach we build a graph g on the set of all l mers rather than on the set of all l mers as in the previous section an l mer v is joined by a directed edge with an l mer w if the spectrum contains an l mer for which the first l nucleotides coincide with v and the last l nucleotides coincide with w fig each l mer from the spectrum corresponds to a directed edge in g rather than to a vertex as it does in h compare figures and therefore finding a dna fragment contain ing all l mers from the spectrum corresponds to finding a path visiting all edges of g which is the problem of finding an eulerian path superficially finding an eulerian path looks just as hard as finding a hamiltonian path but as we show below finding eulerian paths turns out to be simple we will first consider eulerian cycles that is eulerian paths in which the first and the last vertices are the same a directed graph g is eulerian if it contains an eulerian cycle a vertex v in a graph is balanced if the num ber of edges entering v equals the number of edges leaving v that is if indegree v outdegree v for any given vertex v in an eulerian graph the number of times the eulerian cycle enters v is exactly the same as the number of times it leaves v thus indegree v outdegree v for every ver tex v in an eulerian graph motivating the following theorem characterizing eulerian graphs theorem a connected graph is eulerian if and only if each of its vertices is balanced proof first it is easy to see that if a graph is eulerian then each vertex must be balanced we show that if each vertex in a connected graph is balanced then the graph is eulerian to construct an eulerian cycle we start from an arbitrary vertex v and form any arbitrary path by traversing edges that have not already been used we stop the path when we encounter a vertex with no way out that is a ver tex whose outgoing edges have already been used in the path in a balanced graph the only vertex where this can happen is the starting vertex v since for any other vertex the balance condition ensures that for every incoming edge there is an outgoing edge that has not yet been used therefore the resulting path will end at the same vertex where it started and with some luck will be eulerian however if the path is not eulerian it must contain a vertex w that still has some number of untraversed edges the cycle we just constructed multiple sequence reconstructions the eulerian path approach s atg tgg tgc gtg ggc gca gcg cgt vertices correspond to l tuples edges correspond to l tuples from the spectrum at ca at ca atggcgtgca atgcgtggca paths visiting all edges correspond to sequence reconstructions figure sbh and the eulerian path problem forms a balanced subgraph since the original graph was balanced then the edges that were not traversed in the first cycle also form a balanced sub graph since all vertices in the graph with untraversed edges are balanced there must exist some other path starting and ending at w containing only untraversed edges this process is shown in figures and one can now combine the two paths into a single one as follows traverse the first path from v to w then traverse the second path from w back to itself and then traverse the remainder of the first path from w back to v repeating this until there are no more vertices with unused edges will eventually yield an eulerian cycle this algorithm can be implemented in time linear in the number of edges in the graph d a subgraph is a graph obtained by removing some edges from the original graph v v v a b c figure constructing an eulerian cycle in an eulerian graph notice that we have described eulerian graphs as containing an eulerian cycle rather than an eulerian path but we have said that the sbh problem reduces to that of finding an eulerian path a vertex v in a graph is called semibalanced if indegree v outdegree v if a graph has an eulerian path starting at vertex and ending at vertex t then all its vertices are bal anced with the possible exception of and t which may be semibalanced the eulerian path problem can be reduced to the eulerian cycle problem by adding an edge between two semibalanced vertices this transformation balances all vertices in the graph and therefore guarantees the existence of an eulerian cycle in the graph with the added edge removing the added edge from the eulerian cycle transforms it into an eulerian path the following theorem characterizes all graphs that contain eulerian paths theorem 2 a connected graph has an eulerian path if and only if it contains at most two semibalanced vertices and all other vertices are balanced 9 fragment assembly in dna sequencing as we mentioned previously after the short to bp dna reads are se quenced biologists need to assemble them together to reconstruct the entire genomic dna sequence this is known as fragment assembly the shortest superstring problem described above is an overly simplified abstraction that does not adequately capture the essence of the fragment assembly problem find a cycle find a cycle combine cycles 5 2 2 6 9 figure constructing an eulerian cycle untraversed edges are shown in gray since it assumes error free reads the error rate in dna reads produced by modern sequencing machines varies from to 3 a further complication in fragment assembly is the fact that one does not know a priori which of two dna strands a read came from dna is double stranded and which of the two strands was sequenced by a read depends on how the insert was oriented in the vector since this is essentially arbitrary one never knows whether a read came from a target strand dna sequence or from its watson crick complement however sequencing errors and assignments of reads to one of two strands are just minor annoyances compared to the major problem in fragment as sembly repeats in dna the human genome contains many sequences that repeat themselves throughout the genome a surprisingly large number of times for example the roughly nucleotide alu sequence is repeated more than a million times throughout the genome with only 5 to se quence variation even more troublesome for fragment assembly algorithms is the fact that repeats occur at several scales the human t cell receptor lo cus contains five closely located repeats of the trypsinogen gene which is 4 kb long and varies only by 3 to 5 between copies these long repeats are particularly difficult to assemble since there are no reads with unique por tions flanking the repeat region the human genome contains more than million alu repeats bp and lin e repeats bp not to mention that an estimated of genes in the human genome have du plicated copies a little arithmetic shows that these repeats and duplicated genes represent about half the human genome if one models the genome as a 3 billion letter sequence produced by a ran dom number generator then assembling it from letter reads is actually relatively simple however because of the large number of repeats it is the oretically impossible to uniquely assemble real reads as long as some repeats are longer than the typical read length increasing the length of the reads to make them longer than most repeats would solve the problem but the sequencing technology has not significantly improved the read length yet figure upper presents a puzzle that looks deceivingly simple and has only sixteen triangular pieces people usually assemble puzzles by con necting matching pieces in this case for every triangle in the puzzle there is a variety of potentially matching triangles every frog in the puzzle is re peated several times as a result you cannot know which of the potentially matching triangles is the correct one to use at any step if you proceed with out some sort of guidance you are likely to end up in the situation shown in figure lower fourteen of the pieces have been placed completely consistently but the two remaining pieces are impossible to place it is dif ficult to design a strategy that can avoid such dead ends for this particular puzzle and it is even more difficult to design strategies for the linear puzzle presented by repeats in a genome since repeats present such a challenge in assembling long genomes the original strategy for sequencing the human genome was first to clone it into bacs each bac carrying an approximately bp long insert af ter constructing a library of overlapping bacs that covers the entire human genome which requires approximately bacs each one can be se quenced as if it were a separate minigenome this bac by bac sequenc ing strategy significantly simplifies the computational assembly problem by virtue of the fact that the number of repeats present within a bac is times smaller than the number of repeats in the entire genome but makes the sequencing project substantially more cumbersome although the hu man genome project demonstrated that this bac by bac strategy can be successful recent large scale sequencing projects including the mouse fortunately as different copies of these repeats have evolved differently over time they are not exact repeats bacterial artificial chromosomes figure repeats are not just a problem in dna sequence assembly this puzzle has deceptively few pieces but is harder than many jigsaw puzzles that have thou sands of pieces with permission of dan gilbert art group inc genome assembly mainly follow the whole genome assembly paradigm ad vocated by james weber and gene myers in myers led the fragment assembly efforts at celera genomics the company that announced the com pletion of a draft human genomic sequence in weber and myers suggested a virtual increase in the length of a read by pairing reads that were separated by a fixed size gap this suggestion resulted in the so called mate pair reads sequencing technique in this method inserts of length ap proximately l where l is much longer than the length of a read are se lected and both ends of the insert are sequenced this produces a pair of reads called mates at a known approximate distance l from each other the insert length l is chosen in such a way that it is larger than the length of most repeats in the human genome the advantage of mate pair reads is that it is unlikely that both reads of the mate pair will lie in a large scale dna re peat thus the read that lies in a unique portion of dna determines which copy of a repeat its mate is in most fragment assembly algorithms consist of the following three steps overlap finding potentially overlapping reads layout finding the order of reads along dna consensus deriving the dna sequence from the layout the overlap problem is to find the best match between the suffix of one read and the prefix of another in the absence of sequencing errors we could simply find the longest suffix of one string that exactly matches the prefix of another string however sequencing errors force us to use a variation of the dynamic programming algorithm for sequence alignment since errors are small to 3 the common practice is to filter out pairs of fragments that do not share a significantly long common substring an idea we will return to in chapter 9 when we discuss combinatorial pattern matching constructing the layout is the hardest step in fragment assembly the dif ficulty is in deciding whether two fragments really overlap i e their differ ences are caused by sequencing errors or actually come from two different copies of a repeat repeats represent a major challenge for whole genome shotgun sequencing and make the layout problem very difficult the final consensus step of fragment assembly amounts to correcting er rors in sequence reads the simplest way to build the consensus is to report 18 the human genome sequence was sequenced in by both the publicly funded human genome consortium and the privately financed celera genomics as a result there exist two slightly different versions of the human genome the most frequent character in the layout constructed in the layout step this assumes that each position in the genome was represented by a sufficiently large number of reads to ensure that experimental errors are reduced to mi nor noise protein sequencing and identification few people remember that before dna sequencing had even been seriously suggested scientists routinely sequenced proteins frederick sanger was awarded his first of two nobel prize for determining the amino acid se quence of insulin the protein needed by people suffering from diabetes se quencing the amino acid bovine insulin in the late seemed more challenging than sequencing an entire genome seems today the compu tational problem facing protein sequencing at that time was similar to that facing modern dna sequencing the main difference was in the length of the sequenced fragments in the late biologists discovered how to ap ply the edman degradation reaction to chop off one terminal amino acid at a time from the end of a protein and read it unfortunately this only works for a few terminal amino acids before the results become impossible to in terpret to get around this problem sanger digested insulin with proteases enzymes that cleave proteins into peptides short protein fragments and se quenced each of the resulting fragments independently he then used these overlapping fragments to reconstruct the entire sequence exactly like the dna sequencing break read the fragments assemble method today as shown in figure the edman degradation reaction became the predominant protein sequenc ing method for the next twenty years and by the late protein sequenc ing machines were on the market despite these advances protein sequenc ing ceased to be of central interest in the field as dna sequencing technol ogy underwent rapid improvements in the late in dna sequencing obtaining reads is relatively easy it is the assembly that is difficult in pro tein sequencing obtaining reads is the primary problem while assembly is easy having dna sequence data for a cell is critical to understanding the molec ular processes that the cell goes through however it is not the only impor modern protein sequencing machines are capable of reading more than fifty residues from a peptide fragment however these machines work best when the protein is perfectly purified which is hard to achieve in biological experiments give giveecca giveeccasv giveeccasvc giveeccasvcsl giveeccasvcsly svc sly slyeledyc ye yel yele eledy eledycd le ledycd edycd dycd cd fvdehlcg fvdehlcgshl hlcgshl shlvea veal vealy al aly ylvcg lvcgergf lvcgergff gerg gf gffytpk ytpka tpka figure the peptide fragments that frederick sanger obtained from insulin through a variety of methods the protein is split into two parts the a chain shown on the left and the b chain shown on the right as a result of an enzymatic digestion process sanger further elucidation of the disulfide bridges linking the various cys tein residues was the result of years of painstaking laboratory work the sequence was published in three parts the a chain the b chain and then the disulfide link ages insulin is not a particularly large protein so better techniques would be useful tant component one also needs to know what proteins the cell produces and what they do on the one hand we do not yet know the full set of proteins that cells produce so we need a way to discover the sequence of previously unknown proteins on the other hand it is important to identify which spe cific proteins interact in a biological system e g those proteins involved in dna replication lastly different cells in an organism have different reper toires of expressed proteins brain cells need different proteins to function than liver cells do and an important problem is to identify proteins that are present or absent in each biological tissue under different conditions there are two types of computational problems motivated by protein se quencing de novo protein sequencing is the elucidation of a protein sequence in the case when a biological sample contains a protein that is either not present in a database or differs from a canonical version present in a database e g mutated proteins or proteins with biochemical modifications the other problem is the identification of a protein that is present in a database this is usually referred to as protein identification the main difference be tween protein sequencing algorithms and protein identification algorithms is the difficulty of the underlying computational problems perhaps the easiest way to illustrate the distinction between protein identi fication and sequencing is with a gedanken experiment suppose a biologist wants to determine which proteins form the dna polymerase complex in rats having the complete rat genome sequence and knowing the location of all the rat genes does not yet allow a biologist to determine what chemical reactions occur during the dna replication process however isolating a rat dna polymerase complex breaking it apart and sequencing the pro teins that form parts of the complex will yield a fairly direct answer to the researcher question of course if we presume that the biologist has the complete rat genome sequence and all of its gene products he may not actu ally have to sequence every amino acid in every protein in the dna poly merase complex just enough to figure out which proteins are present this is protein identification on the other hand if the researcher decides to study an organism for which complete genome data are not available perhaps an obscure species of ant then the researcher will need to perform de novo protein sequencing as usual with gedanken experiments reality is more complicated even if the complete genomic sequence of a species is known and annotated the repertoire of all possible proteins usually is not due to the myriad alternative splicings different ways of constructing mrna from the gene transcript and post translational modifications that occur in a living cell for many problems protein sequencing and identification remain the only ways to probe a biological process for example gene splicing see chap ter 6 is a complex process performed by the large molecular complex called the spliceosome which consists of over different proteins complexed with some functional rna biologists want to determine the parts list of the spliceosome that is the identity of proteins that form the complex dna sequencing is not capable of solving this problem directly even if all the proteins in the genome were known it is not clear which of them are parts of the spliceosome protein sequencing and identification on the other hand are very helpful in discovering this parts list recently matthias mann and colleagues purified the spliceosome complex and used protein sequencing and protein identification techniques to find a detailed parts list for it another application of these technologies is the study of proteins involved in programmed cell death or apoptosis in the development of many organisms cells must die at specific times a cell dies if it fails to acquire certain survival factors and the death process can be initiated by the expression of certain genes in a developing nematode for example the death of individual cells in the nervous system may be prevented by mutations in several genes that are the subject of active investigation dna sequence data alone are not sufficient to find the genes involved in programmed cell death and until recently nobody knew the identity of these proteins protein analysis by mass spectrometry allowed the sequencing of proteins involved in programmed cell death and the discovery of some proteins involved in the death inducing signaling complex the exceptional sensitivity of mass spectrometry has opened up new ex perimental and computational possibilities for protein studies a protein can be digested into peptides by proteases like trypsin in a matter of seconds a tandem mass spectrometer breaks a peptide into even smaller fragments and measures the mass of each the mass spectrum of a peptide is a collection of masses of these fragments the protein sequencing problem is to derive the sequence of a peptide given its mass spectrum for an ideal fragmentation process where every fragment of a peptide is generated and in an ideal mass spectrometer the peptide sequencing problem is simple however the frag mentation process is not ideal and mass spectrometers measure mass with some imprecision these details make peptide sequencing difficult a mass spectrometer works like a charged sieve a large molecule pep tide gets broken into smaller fragments that have an electrical charge these fragments are then spun around and accelerated in a magnetic field until they hit a detector because large fragments are harder to spin than small ones one can distinguish between fragments with different masses based on the amount of energy required to fling the different fragments around it happens that most molecules can be broken in several places generating sev eral different ion types the problem is to reconstruct the amino acid sequence of the peptide from the masses of these broken pieces the peptide sequencing problem let a be the set of amino acids each with molecular masses m ai a peptide p pn is a sequence of amino acids with par ent mass m p i m pi we will denote the partial n terminal peptide pi of mass mi j m pj as pi and the partial c terminal peptide pi pn of mass m p mi as p for i n mass spectra obtained by tandem mass spectrometry ms ms consist predominantly of partial n terminal peptides and c terminal peptides a mass spectrometer typically breaks a peptide pn at different pep tide bonds and detects the masses of the resulting partial n terminal and c terminal peptides for example the peptide gpfna may be broken into the n terminal peptides g gp gpf gpfn and c terminal peptides pfna fna na a moreover while breaking gpfna into gp and fna it may lose some small parts of gp and fna resulting in fragments of a lower mass for example the peptide gp might lose a water and the peptide fna might lose an ammonia the resulting masses detected by the spec trometer will be equal to the mass of gp minus the mass of water water happens to weigh 18 daltons and the mass of fna minus the mass of ammonia daltons peptides missing water and ammonia are two different ion types that can occur in fragmenting a peptide in a mass spectrometer every protein is a linear chain of amino acids connected by a peptide bond the peptide bond starts with a nitrogen n and ends with a carbon c therefore every protein begins with an unstarted peptide bond that begins with n and another unfinished peptide bond that ends with c an n terminal peptide is a fragment of a protein that includes the leftmost end i e the n terminus a c terminal peptide is a fragment of a protein that includes the rightmost end i e the c terminus biologists typically work with billions of identical peptides in a solution a mass spectrome try machine breaks different peptide molecules at different peptide bonds some peptide bonds are more prone to breakage than others as a result many n terminal and c terminal peptide may be detected by a mass spectrometer this is a simplified description of the complex and messy fragmentation process in this section we intentionally hide many of the technical details and focus only on the computational challenges peptide fragmentation in a tandem mass spectrometer can be character ized by a set of numbers δk representing the different types of ions that correspond to the removal of a certain chemical group from a pep tide fragment we will call the set of ion types a δ ion of an n terminal partial peptide pi is a modification of pi that has mass mi δ correspond ing to the loss of a typically small chemical group of mass δ when p was fragmented into pi the δ ion of c terminal peptides is defined similarly the most frequent n terminal ions are called b ions ion bi corresponds to pi with δ and the most frequent c terminal ions are called y ions ion yi corresponds to p with δ shown in figure a examples of other frequent n terminal ions are represented by b a b fragment that loses a water or y and some others like b for tandem mass spectrometry the theoretical spectrum t p of peptide p can be calculated by subtracting all possible ion types δk from the masses of all partial peptides of p such that every partial peptide generates k masses in the theoretical spectrum as in figure b an experimental spectrum s sq is a set of numbers obtained in a mass spectrometry experiment that includes masses of some fragment ions as well as chemical noise note that the distinction between the theoretical spectrum t p and the experimental spectrum s is that you mathematically generate t p given the peptide sequence p but you experimentally gener ate s without knowing what the peptide sequence is that generated it the match between the experimentally measured spectrum s and peptide p is the number of masses in s that are equal to masses in t p this is often referred to as the shared peaks count in reality peptide sequencing algorithms use more sophisticated objective functions than a simple shared peaks count incorporating different weighting functions for the matching masses we formulate the peptide sequencing problem as follows a theoretical spectrum of a peptide may contain as many as masses but it sometimes contains less since some of these masses are not unique in reality a mass spectrometer detects charged ions and measures mass to charge ratios as a result an experimental spectrum contains the values m where m is the mass and z is an integer typically or 2 equal to the charge of a fragment ion for simplicity we assume that z through the remainder of this chapter g p f n a oh o a the fragmentation pattern of the peptide gpfna sequence mass less less less both gpfna 463 g 41 pfna 405 gp 114 fna 316 gpf 261 na 169 gpfn yl a 72 73 b a theoretical mass spectrum of gpfna intensity mass charge c an experimental mass spectrum of gpfna figure tandem ms of the peptide gp f n a two different types of fragment ions b ions and y ions are created a when the carbon nitrogen bond breaks in the spectrometer each of these ion types can also lose or or both resulting in the masses presented in b many other ion types are seen in typical experiments if we were to measure the mass spectrum of this peptide we would see a result similar to c where some peaks are missing and other noise peaks are present peptide sequencing problem find a peptide whose theoretical spectrum has a maximum match to a measured experimental spectrum input experimental spectrum s the set of possible ion types and the parent mass m output a peptide p of mass m whose theoretical spectrum matches s better than any other peptide of mass m in reality mass spectrometers measure both mass and intensity which re flects the number of fragment ions of a given mass are detected in the mass spectrometer as a result mass spectrometrists often represent spectra in two dimensions as in figure c and refer to the masses in the spectrum as peaks spectrum graphs there are two main approaches to solving the peptide sequencing prob lem that researchers have tried either through exhaustive search among all amino acid sequences of a certain length or by analyzing the spectrum graph which we define below the former approach involves the generation of all amino acid sequences of length l and their corresponding theoretical spectra with the goal of finding a sequence with the best match between the experimental spectrum and the sequence theoretical spectrum since the number of sequences grows exponentially with the length of the pep tide different branch and bound techniques have been designed to limit the combinatorial explosion in these methods prefix pruning restricts the compu tational space to sequences whose prefixes match the experimental spectrum well the difficulty with the prefix pruning is that it frequently discards the correct sequence if its prefixes are poorly represented in the spectrum the spectrum graph approach on the other hand does not involve gen erating all amino acid sequences and leads to a fast algorithm for peptide sequencing in this approach we construct a graph from the experimen tal spectrum assume for simplicity that an experimental spectrum s a match of a theoretical spectrum against an experimental spectrum with varying intensity then needs to reflect the intensity of the fragment ions while accounting for intensities is im portant for statistical analysis it does not seriously affect the algorithmic details and we ignore intensities in the remainder of this chapter sq consists of n terminal ions and we will ignore the c terminal ions for a while every mass s may have been created from a partial peptide by one of the k different ion types since we do not know which ion type from δk created the mass in the experimental spectrum we generate k different guesses for each of masses in the experimental spectrum every guess corresponds to the hypothesis that x δj where x is the mass of some partial peptide and j k therefore for every mass in the experimental spectrum there are k guesses for the mass x of some partial peptide δk as a result each mass in the experimental spectrum is transformed into a set of k vertices in the spectrum graph one for each possible ion type the vertex for δi for the mass is la beled with mass δi we connect any two vertices u and v in the graph by the directed edge u v if the mass of v is larger than that of u by the mass of a single amino acid if we add a vertex at and a vertex at the parent mass m connecting them to other vertices as before then the peptide sequencing problem can be cast as finding a path from to m in the resulting dag in summary the vertex set of the resulting spectrum graph is a set of num bers si δj representing potential masses of n terminal peptides adjusted by the ion type δj every mass si of spectrum s generates k distinct ver tices vi si si δk though the sets vi and vj may overlap if si and sj are close the set of vertices in a spectrum graph is therefore sinitial vq sfinal where sinitial and sfinal m the spectrum graph may have at most qk 2 vertices we label the edges of the spectrum graph by the amino acid whose mass is equal to difference be tween vertex masses if we look at vertices as putative n terminal peptides the edge from u to v implies that the n terminal sequence corresponding to v may be obtained by extending the sequence at u by the amino acid that labels u v a spectrum s of a peptide p pn is called complete if s contains at least one ion type corresponding to every n terminal partial peptide pi for every i n the use of a spectrum graph is based on the observation that for a complete spectrum there exists a path of length n from sinitial to sfinal in the spectrum graph that is labeled by p this observation casts the peptide sequencing problem as one of finding the correct path in the set of all paths between two vertices in a directed acyclic graph if the spectrum in addition to the experimental spectrum every mass spectrometry experiment always pro duces the parent mass m of a peptide 28 although we ignored c terminal ions in this simplified construction of the spectrum graph these ions can be taken into account by combining the spectrum s with its reversed version is complete then the correct path that we are looking for is often the path with the maximum number of edges the familiar longest path in a dag problem unfortunately experimental spectra are frequently incomplete moreover even if the experimental spectrum is complete there are often many paths in the spectrum graph to choose from that have the same or even larger length preventing one from unambiguously reconstructing the peptide the problem with choosing a path with a maximum number of edges is that it does not adequately reflect the importance of different vertices for example a vertex in the spectrum graph obtained by a shift of as si corresponding to the most frequent b ions should be scored higher than a vertex obtained by a shift of the rare b ion si 18 17 si further whenever there are two peaks si and such that si δj the vertex corresponding to that mass should also get a higher score than a vertex obtained by a single shift in the probabilistic approach to peptide sequencing each ion type δi has some probability of occurring which we write as p δi under the simplest assumption the probability that δi occurs for some partial peptide is inde pendent of whether δj also occurs for the same partial peptide under this assumption any given partial peptide may contribute as many as k masses in the spectrum this happens with probability i p δi and as few as this happens with probability i p δi the probabilistic model below scores the vertices of the spectrum graph based on these simple assumptions suppose that an n terminal partial peptide pi with mass mi produces ions δl present ions of mass mi mi mi δl but fails to produce ions δl δk missing ions in the experimental spectrum all l present ions will result in a vertex in the spectrum graph at mass mi corre sponding to pi how should we score this vertex a naive approach would be to reward pi for every ion type that explains it suggesting a score of i p δi however this approach has the disadvantage of not consider ing the missing ions so we combine those in by defining the score for the partial peptide to be l k tt p δi tt p δi however there is some inherent probability of chemical noise that is it can produce any mass that has nothing to do with a peptide of interest with certain probability pr therefore we adjust the probabilistic score as l k tt p δi tt p δi pr i i l 13 protein identification via database search de novo protein sequencing algorithms are invaluable for identification of both known and unknown proteins but they are most useful when work ing with complete or nearly complete high quality spectra many spectra are far from complete and de novo peptide sequencing algorithms often pro duce ambiguous solutions for such spectra if we had access to a database of all proteins from a genome then we would no longer need to consider all peptide sequences to interpret an ms ms spectrum but could instead limit our search to peptides present in this database currently most proteins are identified by database search effectively looking the answer up in the back of a book indeed an experimental spectrum can be compared with the theoretical spectrum for each peptide in such a database and the entry in the database that best matches the ob served spectrum usually provides the sequence of the experimental peptide this forms the basis of the popular sequest algorithm developed by john yates and colleagues we formulate the protein identification problem as follows protein identification problem find a protein from a database that best matches the experimental spectrum input a database of proteins an experimental spectrum s a set of ion types and a parent mass m output a protein of mass m from the database with the best match to spectrum s though the logic that sequest uses to determine whether a database entry matches an experimental spectrum is somewhat involved the basic approach of the algorithm is just a linear search through the database one drawback to ms ms database search algorithms like sequest is that pep tides in a cell are often slightly different from the canonical peptides present in databases the synthesis of proteins on ribosomes is not the final step in a protein life many proteins are subject to further modifications that reg ulate protein activities and these modifications may be either permanent or reversible for example the enzymatic activity of some proteins is regulated by the addition or removal of a phosphate group at a specific residue 29 phos phorylation is a reversible process protein kinases add phosphate groups while phosphatases remove them proteins form complex systems necessary for cellular signaling and meta bolic regulation and are therefore often subject to a large number of bio chemical modifications e g phosphorylation and glycosylation in fact almost all protein sequences are modified after they have been constructed from their mrna template and as many as distinct types of modifica tions of amino acid residues are known since we are unable to predict these post translational modifications from dna sequences finding naturally occur ring modifications remains an important open problem computationally a chemical modification of the protein pi pn at position i results in increased mass of the n terminal peptides pi pi pn and increased mass of the c terminal peptides p p p 2 i the computational analysis of modified peptides was also pioneered by john yates who suggested an exhaustive search approach that implicitly generates a virtual database of all modified peptides from a small set of po tential modifications and matches the experimental spectrum against this virtual database it leads to a large combinatorial problem even for a small set of modification types modified protein identification problem find a peptide from the database that best matches the experimental spectrum with up to k modifications input a database of proteins an experimental spectrum s a set of ion types a parent mass m and a parameter k capping the number of modifications output a protein of mass m with the best match to spec trum s that is at most k modifications away from an entry in the database the major difficulty with the modified protein identification problem is 29 phosphorylation uses serine threonine or tyrosine residues to add a phosphate group that very similar peptides and may have very different spectra and our goal is to define a notion of spectral similarity that correlates well with sequence similarity in other words if and are a few modifica tions apart the spectral similarity between and should be high the shared peaks count is of course an intuitive measure of spectral similarity however this measure diminishes very quickly as the number of mutations increases thus leading to limitations in detecting similarities by database search moreover there are many correlations between the spectra of related peptides and only a small proportion of these correlations is captured by the shared peaks count the spectral convolution algorithm below reveals potential peptide mod ifications without an exhaustive search and therefore does not require gener ating a virtual database of modified peptides spectral convolution let and be two spectra define the spectral convolution to be the multiset and let x be the multiplicity of element x in this multiset in other words x is the number of pairs such that s2 s1 x fig the shared peak count that we introduced earlier in this chapter is the number of masses common to both and and is simply ms ms database search algorithms that maximize the shared peak count find a peptide in the database that maximizes where is an experimental spectrum and is the theoretical spectrum of a peptide in the database however if and correspond to peptides that differ by k mutations or modifications the value of may be too small to determine that and really were generated by similar peptides as a result the power of the shared peak count to discern that two peptides are similar diminishes rapidly as the number of modifications increases it is bad at k and nearly useless for k the peaks in the spectral convolution allow us to detect mutations and modifications even if the shared peak count is small if peptides and corresponding to spectra and differ by only one mutation k with amino acid difference δ m m then is expected to have two approximately equal peaks at x and x δ if the mutation corresponds to a peptide from the database while corresponds to the modified ver sion of whose experimental spectrum is being used to search the database occurs at position t in the peptide then the peak at corresponds to n terminal peptides pi for i t and c terminal peptides p for i t the peak at δ corresponds to n terminal peptides pi for i t and c terminal peptides p for i t now assume that and are two substitutions apart one with mass difference δi and another with mass difference δ δi where δ denotes the difference between the parent masses of and these modifications generate two new peaks in the spectral convolution at δi and at δ δi it is therefore reasonable to define the similarity between spectra and as the overall height of the k highest peaks in although spectral convolution helps to identify modified peptides it does have a limitation let s 20 be a spectrum of peptide p and assume for simplicity that p produces only b ions let and si 20 sii be two theoretical spectra corresponding to peptides p i and p ii from the database which of the two peptides fits s better the shared peaks count does not allow one to answer this question since both si and sii have five peaks in common with s moreover the spectral convolution also does not answer this question since both s si and s sii reveal strong peaks of the same height at and 5 this suggests that both p i and p ii can be ob tained from p by a single mutation with mass difference 5 however a more careful analysis shows that although this mutation can be realized for p i by introducing a shift 5 after mass it cannot be realized for p ii the major difference between si and sii is that the matching positions in si come in clumps while the matching positions in sii do not below we describe the spectral alignment approach which addresses this problem spectral alignment let a an be an ordered set of integers an a shift i transforms a into ai ai i an i that is i alters all elements in the sequence except for the first i elements we only spectrum s prtein spectrum s prtein 375 a b spectrum s prtein 526 682 98 133 254 355 375 484 98 296 425 526 712 98 133 254 355 375 476 484 632 c d figure 26 detecting modifications of the peptide p rt ein a elements of the spectral convolution represented as elements of a difference matrix and are the theoretical spectra of the peptides p rt ein and p rt ey n respectively elements in the spectral convolution that have multiplicity larger than 2 are shaded while the elements with multiplicity exactly 2 are shown circled the high multiplic ity element corresponds to all of the shared masses between the two spectra while another high multiplicity element corresponds to the shift of masses by δ due to the mutation of i to y in p rt ein the difference in mass between y and i is in b two mutations have occurred in p rt ein r w with δl and i y with δll spectral alignments for a and b are shown in c and d respectively the main diagonals represent the paths for k the lines parallel to the main diagonals represent the paths for k every jump between diagonals corresponds to an increase in k mutations and modifications to a peptide can be detected as jumps between the diagonals consider shifts that do not change the order of elements that is the shifts with i ai ai the k similarity d k between sets a and b is defined as the maximum number of elements in common between these sets after k shifts for example a shift transforms s 20 40 60 into si 20 40 55 95 therefore d for these sets the set sii 55 95 has five elements in common with s the same as si but there is no single shift transforming s into sii d 6 below we analyze and solve the following spectral alignment problem spectral alignment problem find the k similarity between two sets input sets a and b which represent the two spectra and a number k number of shifts output the k similarity d k between sets a and b one can represent sets a an and b bm as ar rays a and b of length an and bm correspondingly the array a will contain n ones at positions an and an n zeros while the array b will contain m ones at positions bm and bm m zeros in such a model a shift i is simply a deletion of i zeros from a while a shift i is simply an insertion of i zeros in a with this model in mind the spectral align ment problem is simply to find the edit distance between a and b when the elementary operations are deletions and insertions of blocks of zeros as we saw in chapter 6 these operations can be modeled by long horizontal and vertical edges in a manhattan like graph the only differences between the traditional edit distance problem and the spectral alignment problem are a somewhat unusual alphabet and the scoring of paths in the resulting graph the analogy between the edit distance problem and the spectral alignment we remark that this is not a particularly dense encoding of the spectrum problem leads us to frame spectral alignment as a type of longest path prob lem define a spectral product a b to be the an bm two dimensional matrix with nm ones corresponding to all pairs of indices ai bj and all remaining elements zero the number of ones on the main diagonal of this matrix de scribes the shared peaks count between spectra a and b or in other words similarity between a and b figure 27 shows the spectral products s si and s sii for the example from the previous section in both cases the num ber of ones on the main diagonal is the same and d 5 the δ shifted peaks count is the number of ones on the diagonal that is δ away from the main diagonal the limitation of the spectral convolution is that it considers diagonals separately without combining them into feasible mutation scenar ios the k similarity between spectra is defined as the maximum number of ones on a path through the spectral matrix that uses at most k di agonals and the k optimal spectral alignment is defined as the path that uses these k diagonals for example similarity is defined by the maximum number of ones on a path through this matrix that uses at most two diago nals figure 27 demonstrates the notion that similarity shows that s is closer to si than to sii in the first case the optimal two diagonal path covers ten left matrix versus six in the second case right matrix figure 28 illustrates that the spectral alignment detects more and more subtle similar ities between spectra simply by increasing k compare figures 26 c and d below we describe a dynamic programming algorithm for spectral alignment let ai and bj be the i prefix of a and j prefix of b respectively define dij k as the k similarity between ai and bj such that the last elements of ai and bj are matched in other words dij k is the maximum number of ones on a path to ai bj that uses at most k different diagonals we say that ii ji and i j are codiagonal if ai bj and that ii ji i j if ii i and ji j to take care of the initial conditions we introduce a fictitious element with k and assume that is codiagonal with any other i j the dynamic programming recurrence for dij k is then i i dij k max i j k if i j and i j are codiagonal k otherwise the k similarity between a and b is given by d k maxij dij k to a limit of course when k is too large the spectral alignment is not very useful 20 40 60 20 40 55 95 20 40 60 15 55 75 95 figure 27 spectral products s sl left and s sll right where s 20 40 60 80 sl 10 20 40 55 75 95 and sll 10 15 35 55 70 75 95 the matrices have dimensions 95 with ones shown by circles zeros are too numerous to show the spectrum s can be transformed into sl by a single shift and d 10 however the spectrum s cannot be transformed into sll by a single shift and d 6 the above dynamic programming algorithm for spectral alignment is rather slow with a running time of o for two n element spectra and below we describe an o algorithm for solving this problem define diag i j as the maximal codiagonal pair of i j such that diag i j i j in other words diag i j is the position of the previous on the same diagonal as ai bj or if such a position does not exist define mij k max i j j1 k then the recurrence for dij k can be re written as d k max ddiag i j k mi j k 11 15 18 24 38 7 11 13 38 figure 28 aligning spectra the shared peaks count reveals only d 3 match ing peaks on the main diagonal while spectral alignment reveals more hidden sim ilarities between spectra d 5 and d 2 and detects the corresponding mutations the recurrence for mij k is given by dij k i j k the transformation of the dynamic programming graph can be achieved by introducing horizontal and vertical edges that provide the ability to switch between diagonals fig 29 the score of a path is the number of ones on this path while k corresponds to the number of switches number of used diagonals minus the simple dynamic programming algorithm outlined above hides many details that make the spectral alignment problem difficult a spectrum can be thought of as a combination of two series of numbers one increasing the n terminal ions and the other decreasing the c terminal ions these two 7 11 15 18 21 24 38 7 11 13 19 22 31 38 figure 29 modification of a dynamic programming graph leads to a fast spectral alignment algorithm series form diagonals in the spectral product s s the main diagonal and the perpendicular diagonal these correspond respectively to pairings of n terminal and c terminal ions the algorithm we have described deals with the main diagonal only finding post translationally modified proteins via mass spectrometry remains a difficult problem that nobody has yet solved and significant efforts are underway to extend the spectral alignment algo rithm to handle these complications and to develop new algorithmic ideas for protein identification notes the earliest paper on graph theory seems to be that of leonhard euler who in discussed whether or not it was possible to stroll around königs berg crossing each of its bridges across the pregel river exactly once euler remains one of the most prolific writers in mathematics aside from graph theory we owe him the notation f x for a function i for the square root of and π for pi he worked hard throughout his entire life only to become blind he commented now i will have fewer distractions and proceeded to write hundreds of papers more graph theory was forgotten for a century but was revived in the second half of the nineteenth century by prominent scientists such as sir william hamilton who among many other things invented quaternions and gus tav kirchhoff who is responsible for kirchhoff laws dna arrays were proposed simultaneously and independently in by radoje drmanac and colleagues in yugoslavia 29 andrey mirzabenov and colleagues in russia and ed southern in the united kingdom the inventors of dna arrays suggested using them for dna sequencing and the original name for this technology was sequencing by hybridization a major breakthrough in dna array technology was made by steve fodor and colleagues in 38 when they adapted photolithography a process similar to computer chip manufacturing to dna synthesis the eulerian path approach to sbh was described in sanger approach to protein sequencing influenced work on rna se quencing before biologists figured out how to sequence dna they rou tinely sequenced rna the first rna sequencing project resulted in seventy seven ribonucleotides and took seven years to complete though in rna sequencing used the same break read the fragments assemble ap proach that is used for dna sequencing today for many years dna se quencing was done by first transcribing dna to rna and then sequencing the rna dna sequencing methods were invented independently and simultane ously in by frederick sanger and colleagues and walter gilbert and colleagues 74 the overlap layout consensus approach to dna sequenc ing was first outlined in and further developed by john kececioglu and eugene myers in 55 dna sequencing progressed to handle the entire kb h influenzae bacterium genome in the mid in in spired by this breakthrough james weber and eugene myers proposed the whole genome shotgun approach first outlined by jared roach and col leagues in to sequence the entire human genome the human genome was sequenced in by j craig venter and his team at celera ge nomics with the whole genome shotgun approach and independently by eric lander and his colleagues at the human genome consortium using the bac by bac approach early approaches to protein sequencing by mass spectrometry were based on manual peptide reconstruction and the assembly of those peptides into protein sequences 51 the description of the spectrum graph approach pre sented in this chapter is from vlado dancik and colleagues 25 searching a database for the purposes of protein identification in mass spectrometry was pioneered by matthias mann and john yates in 34 in yates extended his original sequest algorithm to search for modified peptides based on a virtual database of all modified peptides the spectral alignment algorithm was introduced five years later 17 problems problem can phones be connected by wires in such a way that each phone is connected with exactly 11 others problem 2 can a kingdom in which 7 roads lead out of each city have exactly roads problem 3 can a knight travel around a chessboard pass through every square exactly once and end on the same square it started on problem 4 can a knight travel around a chessboard start at the upper left corner pass through every square exactly once and end on the lower right corner problem 5 can one use a 12 inch long wire to form a cube each of the 12 cube edges is inch long if not what is the smallest number of cuts one must make to form this cube problem 6 find the shortest common superstring for eight 3 mers agt aaa act aac ctt gta ttt taa and solve the following two problems construct the graph with vertices corresponding to these 3 mers hamiltonian path approach and find a hamiltonian path 7 edges which visits each vertex exactly once does this path visit every edge of the graph write the superstring corresponding to this hamiltonian path construct the graph with edges corresponding to these 3 mers eulerian path approach and find an eulerian path edges which visits each edge exactly once does this path visit every vertex of the graph exactly once write the superstring corresponding to this eulerian path problem 7 find the shortest common superstring for all 2 digit numbers from to problem find the shortest common superstring for all 3 digit binary numbers in alpha bet problem 9 use the eulerian path approach to solve the sbh problem for the following spectrum s atg ggg ggt gta gtg tat tgg label edges and vertices of the graph and give all possible sequences such that spectrum 3 s problem 10 the sbh problem is to reconstruct a dna sequence from its l mer composition suppose that instead of a single target dna fragment we have two target dna fragments and we simultaneously analyze both of them with a universal dna array give a precise formulation of the resulting problem something like the formulation of the sbh problem give an approach to the above problem which resembles the hamiltonian path approach to sbh give an approach to the above problem which resembles the eulerian path ap proach to sbh problem 11 suppose we have k target dna fragments and that we are able to measure the overall multiplicity of each l mer in these strings give an algorithm to reconstruct these k strings from the overall l mer composition of these strings problem 12 prove that if n random reads of length l are chosen from a genome of length g then the expected fraction of the genome represented in these reads is approximately e where c is the average coverage of the genome by reads the simplest heuristic for the shortest superstring problem is an obvious greedy algorithm repeatedly merge a pair of strings with maximum overlap until only one string remains the compression of an approximation algorithm for the shortest superstring problem is defined as the number of symbols saved by this algorithm compared to plainly concatenating all the strings problem 13 prove that the greedy algorithm achieves at least the compression of an optimal superstring that is greedy compression optimal compression 2 figure 30 a mask used in the synthesis of a dna array let p s1 sm be a set of positive strings and n tk be a set of negative strings we assume that no negative string ti is a substring of any positive string sj a consistent superstring is a string such that each si is a substring of and no ti is a substring of problem 14 design an approximation algorithm for the shortest consistent superstring problem problem 15 dna sequencing reads contain errors that lead to complications in fragment assem bly fragment assembly with sequencing errors motivates the shortest k approximate superstring problem given a set of strings find a shortest string such that each string in matches some substring of s with at most k errors design an approxima tion algorithm for this problem dna arrays can be manufactured with the use of a photolithographic process that grows probes one nucleotide at a time through a series of chemical steps every nucleotide carries a photo labile protection group protecting the probe from further growth this group can be removed by illuminating the probe in each step a predefined region of the array is illuminated thus removing a photolabile protecting group from that region and activating it for further nu cleotide growth the entire array is then exposed to a particular nucleotide which bears its own photolabile protecting group but reactions only occur in the activated region each time the process is repeated a new region is activated and a single nucleotide is appended to each probe in that region by appending nucleotides to the proper regions in the appropriate sequence it is possible to grow a complete set of l mer probes in as few as 4 l steps the light directed synthe sis allows random access to all positions of the array and can be used to make arrays with any probes at any site fig 30 the proper regions are activated by illuminating the array through a series of masks like those in figure 31 white areas of a mask correspond to the region of the array to be illuminated border length border length 16 figure 31 two masks with different border lengths only 3 mers starting with a are shown and black areas correspond to the region to be shadowed unfortunately because of diffrac tion and light scattering points that are close to the border between an illuminated region and a shadowed region are often subject to unintended illumination in such a region it is uncer tain whether a nucleotide will be appended or not this uncertainty gives rise to probes with unknown sequences and unknown lengths that may hybridize to a target dna strand thus complicating interpretation of the experimental data methods are being sought to minimize the lengths of these borders so that the level of uncertainty is reduced figure 31 presents two universal arrays with different arrangements of 3 mers and masks for synthesizing the first nucleotide a only probes with first nucleotide a are shown the border length of the mask at the bottom of figure 31 is significantly smaller than the border length of the mask at the top of figure 31 companies producing dna arrays try to arrange the probes on the universal array in such a way that the overall border length of all 4 l masks is minimal problem 16 find a lower bound on the overall border length of the universal array with l mers for two l mers x and y let dh x y be the hamming distance between x and y that is the number of positions in which x and y differ the overall border length of all masks equals 2 dh x y where the sum is taken over all pairs of neighboring probes on the array this observation establishes the connection between minimization of border length and gray codes an l bit gray code is defined as a permutation of the binary numbers from to each containing l binary digits such that neighboring numbers have exactly one differing bit as do the first and last numbers for example the arrangement of sixteen 4 mers in a 4 bit gray code is shown below 2 3 4 5 6 7 9 10 11 12 13 14 15 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 this gray code can be generated recursively starting with the 1 bit gray code 0 1 as follows for an l bit gray code gl 1 define an l 1 bit gray code as follows gl 1 0g1 1 1g2l 1 1g2 the elements of gl are simply copied with added to the front then reversed with added to the front clearly all elements in gl 1 are distinct and consecutive elements in gl 1 differ by exactly one bit for example the 2 bit gray code is 11 10 and the 3 bit gray code is 001 011 010 111 101 100 problem 17 design a gray code for all 4 digit decimal numbers from to we are interested in a two dimensional gray code composed of strings of length l over a four letter alphabet in other words we would like to generate a by matrix in which each of the l mers in a universal array is present at some position and each pair of adjacent l mers horizontally or vertically differs in exactly one position constructing such a two dimensional gray code is equivalent to minimizing the border length of the universal array problem 18 find the arrangement of probes on a universal array minimizing the border length accurate determination of the peptide parent mass is very important in de novo peptide se quencing an error in parent mass leads to systematic errors in the construction of the spectrum graph when both n terminal and c terminal fragment ions are considered wrong pairing of n terminal and c terminal fragment ions problem 19 given an ms ms spectrum without parent mass devise an algorithm that estimates the parent mass problem 20 develop an algorithm for combining n terminal and c terminal series together in the spectral alignment algorithm problem 21 develop a version of the spectral alignment algorithm that is geared to mutations rather than modifications in this case the jumps between diagonals are not arbitrary and one has to limit the possible shifts between diagonals to mass differences between amino acids participating in the mutation the i th prefix mass of protein p pn is mi j 1 m pj where m pj is the mass of amino acid pj the prefix spectrum of protein p is the increasing sequence mn of its prefix masses for example the prefix spectrum of protein cse is a peptide is any substring pi pj of p for 1 i j n the prefix spectrum of a peptide pi pj contains j i 1 masses for example the prefix spectrum of cs is while the prefix spectrum of se is for simplicity we assume that every amino acid has an unique mass problem 22 spectral assembly problem given a set of prefix spectra from a set of overlap ping peptides extracted from an unknown protein p reconstruct the amino acid sequence of p the spectral assembly problem is equivalent to the classic shortest common superstring problem assume that the protein p and the set of its peptides p satisfy the following conditions pipi 1 i pjpj 1 for 1 i j n 1 that is every amino acid 2 mer dipeptide occurs at most once in protein p for every two consecutive amino acids pipi 1 in protein p there exists a peptide in p con taining dipeptide pipi 1 for example a protein strand and the set of peptides str tr tran ra and and satisfy these conditions problem 23 design an algorithm to solve the spectral assembly problem under the above condi tions does the problem have a unique solution if your answer is no then also provide an algorithm to find all possible protein reconstructions problem 24 extend this algorithm to solve the spectral assembly problem under the following conditions pipi 1 pi l 1 pjpj 1 pj l 1 for 1 i j n l 1 that is every amino acid l mer l peptide occurs at most once in protein p for every l consecutive amino acids pipi 1 pi l 1 in protein p there exists a peptide in p containing l peptide pipi 1 pi l 1 problem 8 25 consider two proteins and the combined prefix spectrum of proteins and is defined as the union of their prefix spectra describe an algorithm for recon structing and from their combined prefix spectrum give an example when such a reconstruction is non unique generalize this algorithm for three and more proteins when analyzing a protein pn a mass spectrometer measures the masses of both the pre fix peptides pi and of the suffix peptides pi pn for 1 i n the prefix suffix mass spectrum includes the masses of all prefix and suffix peptides for example cse pro duces the following prefix suffix spectrum 87 129 103 129 216 and it remains unknown which masses in the prefix suffix spectrum are derived from the prefix peptides and which are derived from the suffix peptides the prefix suffix spectrum may contain as few as n masses for palindromic peptides with every suffix mass matched by a prefix mass and as many as 1 masses if the overall peptide mass is the only match between suffix and prefix masses problem 8 26 reconstruct a peptide given its prefix suffix spectrum devise an efficient algorithm for this problem under the assumption that the prefix suffix spectrum of a peptide of length n contains 1 masses in david schwartz and colleagues developed the optical mapping technique for construction of restriction maps in optical mapping single copies of dna molecules are stretched and attached to a glass under a microscope when restriction enzymes are activated they cleave the dna molecules at their restriction sites the molecules remain attached to the surface but the elasticity of the stretched dna pulls back the molecule ends at the cleaved sites these can be identified under the microscope as tiny gaps in the fluorescent line of the molecule thus a photograph of the dna molecule with gaps at the positions of cleavage sites gives a snapshot of the restriction map optical mapping bypasses the problem of reconstructing the order of restriction fragments but raises new computational challenges the problem is that not all sites are cleaved in each molecule and that some may incorrectly appear to be cut in addition inaccuracies in measur ing the length of fragments and the unknown orientation of each molecule left to right or vice versa make the reconstruction difficult in practice data from many molecules are gathered to build a consensus restriction map the input to the optical mapping problem is a 0 1 n m matrix s sij where each row corresponds to a dna molecule straight or reversed each column corresponds to a position in that molecule and sij 1 if and only if there is a cut in position j of molecule i the goal is to reverse the orientation of a subset of molecules subset of rows in s and to declare a subset of the t columns real cut sites so that the number of ones in cut site columns is maximized a naive approach to this problem is to find t columns with a large proportion of ones and declare them potential cut sites however in this approach every real site will have a reversed twin since each photograph corresponds to either straight or reversed dna molecules with equal probabilities let w i j be the number of molecules with both cut sites i and j present in either direct or reverse orientation in a different approach a graph on vertices correspondence to dr george hripcsak department of biomedical informatics columbia university medical center west street new york ny usa hripcsak columbia edu received june accepted august published online first september abstract the national adoption of electronic health records ehr promises to make an unprecedented amount of data available for clinical research but the data are complex inaccurate and frequently missing and the record reflects complex processes aside from the patient physiological state we believe that the path forward requires studying the ehr as an object of interest in itself and that new models learning from data and collaboration will lead to efficient use of the valuable information currently locked in health records introduction the national push for electronic health records ehr will make an unprecedented amount of clinical information available for research approxi mately one billion patient visits may be documen ted per year in the usa these data may lead to discoveries that improve understanding of biology aid the diagnosis and treatment of disease and permit the inclusion of more diverse populations and rare diseases ehr can be used in much the same way that paper records have been used with manual extraction and interpretation of clinical information the big promise however lies in large scale use automatically feeding clinical research quality improvement public health etc such uses require high quality data which are often lacking in ehr in this paper we investigate a path forward for exploiting ehr data challenges unfortunately the ehr carries many challenges completeness the data are largely missing in several ways data are occasionally missing by mistake in the sense that data that would normally be expected to be recorded are lacking data are often missing in the sense that patients move among institutions for their care so that individual institutional databases contain only part of their care and health infor mation exchange is insufficiently pervasive to address the issue the result is data fragmentation for research and discontinuity of clinical care data are also missing in the sense that they are only recorded during healthcare episodes which usually correspond to illness in addition much informa tion is implicit under the assumption that the human reader will infer the missing information eg pertinent negative findings the result is a time series that is very far from the rigorous data collection normally employed in formal experi ments referring to the statistical taxonomy of missingness health record data are certainly not missing at random and might facetiously even be referred to as almost completely missing accuracy the data are frequently inaccurate resulting in a loss of predictive power errors can occur any where in the process from observing the patient to conceptualizing the results to recording them in the record and recording is influenced by billing requirements and avoidance of liability whereas some errors may be treated as random many errors such as influence from billing are system atic in addition there is often mismatch between the nominal definition of a concept and the intent of the author for example perrla is an acronym commonly used in the eye examination that stands for pupils equal round and reactive to light and accommodation it is unclear however how often clinicians actually test for each of the properties in the cumc database of patients who were missing one eye were documented in a narrative note as being perrla an impossibility because two eyes are required to have equal pupils and another of those patients were documented as being perrla on the left or on the right which is a misuse of the term a researcher looking for subjects whose pupils were equal or accommodated normally could not rely on a nota tion of perrla in the chart complexity healthcare is highly complex it includes a mixture of many continuous variables and a large number of discrete concepts for example at cumc there are different concepts that may be stored in the database there is an enormous amount of work being done to create knowledge structures to define the data including formal definitions classi fication hierarchies and inter concept relationships eg clinical element model maintaining such a structure will remain a challenge however there may also be local variation both in and in definition and use and even within an institu tion definitions vary over time much of the most important data in the record such as symptoms and thought processes are stored as narrative notes which require natural language to generate a computable form temporal attri butes are highly complex with time scales from seconds to years and with different levels of uncertainty bias the above challenges including systematic errors can result in significant bias when health record data are used naively for clinical research for j am med inform assoc doi amiajnl perspectives example in one ehr study of community acquired pneumo step so that large ehr databases can become large research nia patients who came to the emergency department and databases that can then undergo traditional analysis died quickly did not have many symptoms entered into the studies employing large scale ehr data have begun to ehr as a result an attempt to repeat fine pneumonia appear and most of them employ this two step approach using ehr data showed that the apparently healthiest the state of the art in feature extraction is to use a heuristic patients died at a higher rate than sicker patients ultimately iterative approach to generate queries that run across the entire healthcare data reflect a complex set of figure ehr database for example clinical experts may read each with many feedback loops for example physicians request record for a subset of subjects and create a curated dataset a tests relevant to the patient current condition and testing knowledge engineer generates a heuristic rule that maps record guides the diagnosis which determines the treatment and data to each variable in the study eg physician notes billing future testing such feedback loops produce non linear recording codes and medications may all be used to infer the presence of effects that do not reflect the underlying physiology that a disease the rule is tested on the curated subset and the rule researchers may be attempting to study put another way ehr is modified iteratively until sensitivity and specificity reach data are not merely research data with noise and missing values some threshold the rule is then applied to the entire cohort the extent and bias of the noise and missingness are sufficient while this avoids most case by case review it still requires to require fundamentally different methods to analyse the data feature by feature authoring of queries these methods are themselves time consuming furthermore there is much state of the art potentially useful information that is not used the queries fortunately it appears that ehr do contain sufficient informa may be time consuming to maintain and knowledge engineers tion clinicians generally use health records effectively they and clinical experts bring their own biases to draw an analogy learn to navigate the complexity of the record and to fill in with computational biology imagine attempting high through implicit information reusing the information for research put research in which each investigator had to spend months should be possible but having a clinician interpret the record verifying each of thousands of variables before collecting data for every case is infeasible for large studies as we move to large scale mining of the ehr defining the to address the challenges the task is generally broken into queries has become a bottleneck efforts like are two steps the first step which can be called phenotyping or showing significant progress in generating and sharing queries feature extraction transforms the raw ehr data into clinically across institutions but local variations remain and defin relevant features the second step uses these features for trad ing even a small number of phenotypes can take a group of itional research tasks such as measuring associations for institutions years despite advances in ontologies and language discovery or assessing eligibility for a trial as if a research processing the process remains largely unchanged since the coordinator had manually entered and verified the features for earliest days using detective work and alchemy to get golden the most part the ehr challenges are addressed in the first phenotypes from base data figure feedback loops in the electronic health record the state of the patient varies and it determines not only the value of the measurements in the record but also the type and timing of the measurements downloaded by on university july from of https academic oup com jamia article abstract alberta library user j am med inform assoc doi amiajnl perspectives next generation phenotyping there are several ways to improve on the current state one approach improves on the current phenotyping process either by making it more accurate or by reducing the knowledge engin eering effort we refer to the latter as high throughput pheno typing the term could be applied to the current state of the art because even a manually generated query can be run on a large database but we suggest reserving the term for truly high throughput approaches that do not require years to generate a handful of phenotypes a high throughput approach should generate thousands of phenotypes with minimal human inter vention such that they could be maintained over time to improve phenotyping substantially we believe that there needs to be a radical shift in approach and that the answer lies in a familiar place for informatics a combination of top down knowledge engineering and bottom up learning from the data in particular we believe that we need a better understanding of the ehr the ehr is not a direct reflection of the patient and physiology but a reflection of the recording process inherent in healthcare with noise and feedback loops we must study the ehr as an object in itself as if it were a natural system this better understanding will then naturally support both broad based outcome oriented research and physiological research one component is a healthcare process model that represents how processes occur and how data are recorded figure some aspects of the healthcare process model are being defined for example through research related to snomed health level and the clinical element model but they do not directly address the recording process so additional modeling efforts are likely to be needed such efforts might group figure phenotyping and discovery the raw electronic health record ehr data are an indirect reflection of the true patient state due to the recording process attempts to create phenotypes and discover knowledge must account for the recording the healthcare process model represents the salient features of the recording process and informs the phenotyping and discovery downloaded by on university july j from am of med https academic oup com jamia article abstract alberta inform library assoc user doi amiajnl variables into types and might include temporal patterns of data capture given the complexity of healthcare and the number of human and organizational influences a top down model is unlikely to be sufficient therefore a second compo nent is also needed we must mine the ehr data to learn the idiosyncrasies of the healthcare process and their effects on the recording process that is we believe that the interactions and dependencies are too complex to model and predict at a detailed level eg intention vs definition team interactions so empirical measurement of the relationships among data elements will be essential a rigorous model populated with characteristics learned from the data could improve phenotyping in several ways for example it may be possible to map raw data such as a time series of diagnosis codes to a probability of disease if biases can be quantified for example the degree to which a given variable tends to over or underestimate a feature then one could avoid sources that are most biased or one could combine sources that have bias in opposite directions the process of generating a phenotype query would then become less heuristic and more data driven a full review of the data mining methods appropriate to phenotyping is beyond the scope of a perspective but the fol lowing are particularly relevant first is simply characterizing the raw data with frequencies co occurrences and when possible predictive value with respect to desired phenotypes eg how accurate are international classification of disease version codes dimension reduction using algorithms like principal component analysis empirical orthogonal func tions addresses the many disparate variables that comprise an ehr instead of top down defined phenotypes it may be appropriate to define latent variables that have high predictive value using techniques such as latent dirichlet or other methods the ability to find similar cases is often useful to define cohorts for machine learning and has been done with symbolic and computational techniques clinical databases can be stratified into more regular subsets producing more stable results natural language is of course essential to phenotype ehr data due to the narra tive content while time has long been a research topic in informatics further work may be needed this includes temporal modeling and including temporal treatment of narrative data as well as purely numeric approaches including non linear time series analysis drawn from the physics literature the latter includes aggregation of short time series particu larly as applied to health record data and modified to accom modate non equally spaced time series researchers have noted that missingness itself is a useful feature in producing phenotypes we can also improve the use of ehr data at the second step the discovery stage which may include classification eg clin ical trial eligibility prediction eg readmission rate under standing eg physiology and intervention sensitivity to ehr bias may depend on the goal prediction may be accurate even if important confounders are not measured in the ehr but unmeasured confounders could mislead our understanding of physiology even if ehr bias or noise cannot be measured it may be pos sible to factor it out in one study patient data were normal ized to reduce interpatient variance improving the estimation of the correlation among variables in another a derived prop erty mutual information was used in place of traditional para meters such as glucose because they had too much variation perspectives between patients in other cases when the biases and noise cannot be eliminated perhaps they can be understood for example it may be useful to characterize discovered associa tions as being due to the healthcare process eg physician intention versus due to physiology although it is challen ging a number of techniques may be used to infer causation including dynamic bayesian networks granger causality and logic based paradigms recent work demonstrated the control of the confounding effects of with a dem onstration of drugs effects on electrocardiogram qt intervals discussion we believe that the full challenge of phenotyping is not broadly recognized for example one review of mining discusses interoperability and privacy as key challenges but otherwise focuses on the promise of the data rather than the data challenges which are arguably more difficult to solve we believe that the phenotyping process needs to become more data driven and that we need to learn more about the recording process we have sometimes used the phrase the physics of the medical record to point out the likely direction forward it will require study of the ehr as if it were a natural object worthy of study in itself and it may be helpful to employ the general paradigm of physics which involves model ing and aggregation it will be helpful to pull in expertise and algorithms from many fields including non linear time series analysis from physics new directions in causality from phil osophy psychology economics of course our usual collabora tors in computer science and statistics and even new models of research that engage the public our hope is that by exploiting our ample data we can surpass human performance and produce even more reliable phenotypes and accurate associations to draw an analogy a ct scanner uses data that are feasible to collect namely exter nal x ray images and deconvolves them to produce an image that reflects clinically relevant but hidden internal anatomical features similarly we need to use data that are feasible to collect from ehr and deconvolve them to produce clinically relevant phenotypes that are only implicit in the raw data furthermore the advanced use of ehr data which are becom ing both deep in content and broad in coverage of the nation population may open new ways to look at clinical research studying detailed physiology including fine laboratory mea surements over large populations in what might be called population physiology to draw one more analogy from physics we can move from studying weather individual phe notypes to studying climate properties of phenotypes over populations and time systematic changes in the adoption and use of ehr such as those promoted by the hitech incentive program meaning ful use will probably have large effects on how ehr data get used in research for example structured data entry for mean ingful use quality measurement or value based purchasing should improve the volume and quality of data available to research variables that have been notoriously difficult to collect such as smoking history may become more broadly available on the other hand forced data entry can introduce biases that are difficult to detect or correct health information exchange which pulls together not only multiple ehr but also new data sources such as pharmacy fill data should reduce data fragmentation although researchers will need to contend with heterogeneous data definitions and data entry cultures therefore even in a new era of the increased use of ehr a deep understanding of ehr data will be critical downloaded from https academic oup com jamia article abstract by on university july of alberta library user furthermore this must not be a one way street improved understanding of the ehr must be fed back to improve the ehr for example better understanding of missing data inac curacies and biases could lead to improved user interfaces data definitions and even workflows the long term vision of an ehr platform that supports clinical care research and public health will only be achieved with better understanding and true innovation contributors the authors are responsible for the conception and design acquisition of data and analysis and interpretation of data drafting the article and revising it and final approval of the version to be published funding this work was funded by a grant from the national library of medicine discovering and applying knowledge in clinical databases competing interests none provenance and peer review not commissioned externally peer reviewed correction notice this article has been corrected since it was published online first it is now unlocked abstract medical care status and outcomes of a diverse informatics columbia university new york new york usa objective to review the methods and dimensions of data population that is representative of actual patients quality assessment in the context of electronic health the secondary use of data collected in ehrs is record ehr data reuse for research a promising step towards decreasing research costs correspondence to nicole gray weiskopf materials and methods a review of the clinical increasing patient centered research and speeding research literature discussing data quality assessment the rate of new medical discoveries department of biomedical informatics columbia university w street vc new york ny usa nicole weiskopf dbmi methodology for ehr data was performed using an iterative process the aspects of data quality being measured were abstracted and categorized as well as the methods of assessment used despite these benefits reuse of ehr data has been limited by a number of factors including concerns about the quality of the data and their suitability for research it is generally accepted that columbia edu results five dimensions of data quality were identified as a result of differences in priorities between clin received november accepted may which are completeness correctness concordance ical and research settings clinical data are not plausibility and currency and seven broad categories of recorded with the same care as research data published online first june data quality assessment methods comparison with gold moreover stated that the introduction of standards data element agreement data source health information technology like ehrs has led agreement distribution comparison validity checks log not to improvements in the quality of the data review and element presence being recorded but rather to the recording of discussion examination of the methods by which a greater quantity of bad data due to such clinical researchers have investigated the quality and concerns about data quality van der warned suitability of ehr data for research shows that there are specifically against the reuse of clinical data for fundamental features of data quality which may be research and proposed what he called the first law difficult to measure as well as proxy dimensions of informatics d ata shall be used only for the researchers interested in the reuse of ehr data for purpose for which they were collected clinical research are recommended to consider the although such concerns about data quality have adoption of a consistent taxonomy of ehr data quality existed since ehrs were first introduced there to remain aware of the task dependence of data quality remains no consensus as to the quality of electronic to integrate work on data quality assessment from other clinical data or even agreement as to what data fields and to adopt systematic empirically driven quality actually means in the context of ehrs statistically based methods of data quality assessment one of the most broadly adopted conceptualiza conclusion there is currently little consistency or tions of quality comes from juran who said that potential generalizability in the methods used to assess quality is defined through fitness for use in the ehr data quality if the reuse of ehr data for clinical context of data quality this means that data are of research is to become accepted researchers should sufficient quality when they serve the needs of adopt validated systematic methods of ehr data quality a given user pursuing specific goals assessment past study of ehr data quality has revealed highly variable results hogan and wagner in their literature review found that the correctness of as the adoption of electronic health records ehrs data ranged between and and has made it easier to access and aggregate clinical completeness between and depending data there has been growing interest in conducting on the clinical concepts being studied similarly research with data collected during the course of thiru et al in calculating the sensitivity of clinical care the natonal institutes of health different types of ehr data in the literature found has called for increasing the reuse of electronic values ranging between and in a records for research and the clinical research review chan et looked at the quality of the community has been actively seeking methods to same clinical concepts across multiple institutions enable secondary use of clinical data ehrs surpass and still found a great deal of variability the many existing registries and data repositories in completeness of blood pressure recordings for volume and the reuse of these data may diminish example fell anywhere between and the costs and inefficiencies associated with clinical due to differences in measurement recording research like other forms of retrospective research information systems and clinical focus the quality studies that make use of ehr data do not require of ehr data is highly variable therefore it is patient recruitment or data collection both of generally inadvisable to make assumptions about which are expensive and time consuming processes one ehr derived dataset based on another we need the data from ehrs also offer a window into the systematic methods that will allow us to assess the downloaded by on univ july of from alberta https academic oup com jamia article abstract library user j am med inform assoc doi amiajnl review quality of an ehr derived dataset for a given research task our review primarily differs from those highlighted above in its focus the previous reviews looked at data quality findings while ours instead focuses on the methods that have been used to assess data quality in fact the earlier reviews were explicitly limited to studies that relied on the use of a reference standard while we instead explore a range of data quality assessment methods the contributions of this literature review are an empirically based conceptual model of the dimensions of ehr data quality studied by clinical researchers and a summary and critique of the methods that have been used to assess ehr data quality specifically within the context of reusing clinical data for research our goal is to develop a systematic understanding of the approaches that may be used to determine the suitability of ehr data for a specific research goal methods we identified articles in the literature by performing a search of the literature using standard electronic bibliographic tools the literature search was performed by the first author on pubmed in february of as observed by hogan and in their literature review there is no medical subheadings mesh term for data quality so a brief exploratory review was performed to identify relevant keywords the final list included data quality data accuracy data reliability data validity data consis tency data completeness and data error the mesh heading for ehr was not introduced until so the older and more general mesh heading medical record systems computerized was used instead the phrases ehr electronic medical record and computerized medical record were also included in order to capture articles that may not have been tagged correctly we searched for articles including at least one of the quality terms and at least one of the ehr terms results were limited to english language articles the full query is shown below data quality or data accuracy or data reliability or data validity or data consistency or data completeness or data errors or data error and ehr or electronic medical record or computerized medical record or medical records systems computerized mh and english lang this search produced articles all of which were manually reviewed by the first author to determine if they met the selection criteria in particular the articles retained for further review included original research using data quality assess ment methods focused on data derived from an ehr or related system and were published in a peer reviewed journal articles dealing with data from purely administrative systems eg claims databases were not included these inclu sion criteria resulted in relevant articles next we performed an in depth ancestor search reviewing the references of all of the articles in the original pool of this allowed us to identify an additional articles resulting in a final pool of articles meeting our inclusion criteria that were then used to derive results in this study from each article we abstracted the features of data quality examined the methods of assessment used and basic descriptive information including about the article and the type of data being studied through iterative review of the abstracted data we derived broad dimensions of data quality and general cate gories of assessment strategies commonly described in the literature finally we reviewed the articles again categorizing every article based on the dimension or dimensions being assessed as well as the assessment strategies used for each of those dimensions downloaded by on univ july of j from alberta am med https academic oup com jamia article abstract library inform assoc user doi amiajnl before beginning this analysis we searched for preexisting models of ehr data quality but were unable to find any we decided that the potential benefits of adapting a data quality model from another field were outweighed by the risks of approaching our analysis through the lens of a model that had not been validated in the area of ehr data quality furthermore using an existing model to guide analysis is a deductive approach which has the potential to obscure information contained in the data by imposing an existing model from a different discipline we would have run the risk of missing important findings therefore we decided to use an inductive data driven coding approach this approach provides advantages over the deductive approach by allowing us better coverage of the dimensions and methods of data quality assessment results the majority of papers reviewed looked at structured data only or at a combination of structured and unstructured data for our purposes unstructured data types include free entry text while structured data types include coded data values from pre populated lists or data entered into fields requiring specific alphanumeric formats ignoring variations due to lexical categories and negation the articles contained unique terms describing dimensions of data quality features of data quality that were mentioned or described but not assessed were not included in our analysis we grouped the terms together based on shared definitions a few features of good data described in the literature including sufficient granularity and the use of standards were not included in our analyses this decision was made due to the limited discussion of these features the fact that they could be considered traits of good data practice instead of data quality and because no assessment methods were described overall we empirically derived five substantively different dimensions of data quality from the literature the dimensions are defined below completeness is a truth about a patient present in the ehr correctness is an element that is present in the ehr true concordance is there agreement between elements in the ehr or between the ehr and another data source plausibility does an element in the ehr makes sense in light of other knowledge about what that element is measuring currency is an element in the ehr a relevant representation of the patient state at a given point in time the list of data quality terms and their mappings to the five dimensions described above are shown in table the terms chosen to denote each of the dimensions were the clearest and table terms used in the literature to describe the five common dimensions of data quality completeness correctness concordance plausibility currency accessibility accuracy agreement accuracy recency accuracy corrections made consistency believability timeliness availability errors reliability trustworthiness missingness misleading variation validity omission positive predictive value presence quality quality validity rate of recording sensitivity validity review least ambiguous from each of the groups there was a great deal of variability and overlap in the terms used to describe each of these dimensions accuracy for example was sometimes used as a synonym for correctness but in other articles meant both correctness and completeness the dimensions themselves however were abstracted in such a way as to be exhaustive and mutually exclusive based on their definitions every article identified could be matched to one or more of the dimensions a similar process was used to identify the most common methods of data quality assessment the strategies used to assess the dimensions of data quality fell into seven broad categories of methods many of which were used to assess multiple dimensions these general methods are listed and defined below gold standard a dataset drawn from another source or multiple sources with or without information from the ehr is used as a gold standard data element agreement two or more elements within an ehr are compared to see if they report the same or compatible information element presence a determination is made as to whether or not desired or expected data elements are present data source agreement data from the ehr are compared with data from another source to determine if they are in agreement distribution comparison distributions or summary statistics of aggregated data from the ehr are compared with the expected distributions for the clinical concepts of interest validity check data in the ehr are assessed using various techniques that determine if values make sense log review information on the actual data entry practices eg dates times edits is examined a summary of which methods were used to assess which dimensions is shown in table the graph in figure shows the strength of the pairwise relationships between the dimensions and methods some of the methods were used to assess only certain dimensions of data quality whereas other methods were applied more broadly element presence for example was used to assess completeness but none of the other dimensions data element agreement and data source agreement however were applied more broadly most of the dimensions were assessed using an assortment of methods but currency was only measured using a single approach completeness completeness was the most commonly assessed dimension of data quality and was an area of focus in of the articles generally speaking completeness referred to whether or not a truth about a patient was present in the ehr most of the downloaded by on univ july of from alberta https academic oup com jamia article abstract library user j am med inform assoc doi amiajnl articles used the term completeness to describe this dimension but some also referred to data availability or missing data in others completeness was subsumed into more general concepts like accuracy or quality some articles cited the statistical defi nition of completeness suggested by hogan and wagner in which completeness is equivalent to sensitivity many articles assessed ehr data completeness by using another source of data as a gold standard the gold standards used included concurrently kept paper records information supplied by patients review of data by patients clin ical encounters with patients information presented by trained standard patients information requested from the treating physician and alternative data sources from which ehr elements were abstracted a similar approach involved triangulating data from multiple sources within the ehr to create a gold standard other researchers simply looked at the presence or absence of elements in the ehr in some cases these were elements that were expected to be present even if they were not needed for any specific task in other situations the elements examined were dependent upon the task at hand meaning that the researchers determined whether or not the ehr data were table the dimensions of data quality and methods of data quality assessment dimension completeness correctness concordance plausibility currency total method gold standard data element agreement element presence data source agreement 91e96 distribution comparison validity checks log review total in decreasing order of frequency the dimensions are listed from left to right and the methods from top to bottom the numbers in the cells correspond to the article references featuring each dimensionemethod pair figure mapping between dimensions of data quality and data quality assessment methods dimensions are listed on the left and methods of assessment on the right both in decreasing order of frequency from top to bottom the weight of the edge connecting a dimension and method indicates the relative frequency of that combination review complete enough for a specific purpose other data sources common terms used in the literature to describe methods for assessing completeness included looking at agree data concordance include agreement and consistency ment between elements from the same source agreement the most common approach to assessing concordance was to between the ehr and paper records agreement between look at agreement between elements within the ehr the ehr and another electronic source of data and com especially diagnoses and associated information such as medi paring distributions of occurrences of certain elements between cations or procedures the second most common method or with nationally recorded rates used to assess concordance was to look at the agreement of ehr data with data from other sources these other sources included correctness the second most commonly assessed dimension of data quality was correctness which was included in of the articles ehr data were considered correct when the information they contained was true other terms that were commonly used to billing information paper records patient reported data and physician reported data another approach was to compare distributions of data within the ehr with distribu tions of the same information from similar medical or with national rates describe this concept included accuracy error and quality occasionally correctness included completeness due to the fact that some researchers consider missing data to be incorrect ie errors of omission the definition of correctness suggested by hogan and wagner plausibility seven of the articles assessed the plausibility of ehr data in this context data were plausible if they were in agreement states that data correctness is the propor tion of data elements present that are correct which is equiva lent to positive predictive value comparison of ehr data with a gold standard was by far the most frequently used method for assessing correctness these gold standards included paper records information supplied by patients through interviews question naires data review or direct data entry clinical encounters with patients information presented by trained standard patients automatically recorded data contact with the treating physician and alternative data sources from which information matching ehr elements were abstracted some researchers developed gold standards by extracting and triangulating data from within the ehr the second most common approach to assessing correctness with general medical knowledge or information and were therefore feasible in other words assessments of plausibility were intended to determine whether or not data could be trusted or if they were of suspect quality other terms that were used to discuss and describe ehr data plausibility include data validity and integrity the most common approach to assessing the plausibility of ehr data was to perform some sort of validity check to deter mine if specific elements within the ehr were likely to be true or not this included looking for elements with values that were outside biologically plausible ranges or that changed implausibly over or zero valued elements other researchers compared distributions of data values between or with national rates or looked at agreement between related elements was to look at agreement between elements within the ehr usually this involved verifying a diagnosis by looking at associated procedures medications or laboratory values similarly some articles reported on agreement between related and errors identified through the examina tion of the use of copy and paste practices other researchers looked specifically at agreement between structured elements and unstructured data within ehrs one of the more formal approaches described for assessing correctness was the data quality probe proposed by brown and warmington which is a query that when run against an ehr database only returns cases with some disagreement between data elements a few articles described the use of validity checks to assess correctness these included review of changes of sequential data currency the currency of ehr data was assessed in four of the articles currency was often referred to in the literature as timeliness or recency data were considered current if they were recorded in the ehr within a reasonable period of time following measurement or alternatively if they were represen tative of the patient state at a desired time of interest in all four articles currency was assessed through the review of data entry logs in three of the four researchers reviewed whether desired data were entered into the ehr within a set time limit in the fourth researchers considered whether each type of data element was measured recently enough to be considered medi cally relevant over time identifying end digit preferences in blood pressure values and comparing elements with their expected value ranges two other approaches to were using corrections seen in log files as a proxy for correctness and comparing data on the same patients from a registry and an ehr discussion we identified five dimensions of data quality and seven cate gories of data quality assessment methods examination of the types of methods used as well as overlap of the methods between dimensions reveals significant patterns and gaps in knowledge below we explore the major findings of the litera concordance ture review specifically highlighting areas that require further sixteen of the articles reviewed assessed concordance attention and make suggestions for future research data were considered concordant when there was agreement or compatibility between data elements this may mean that two terminology and dimensions of data quality elements recording the same information for a single patient one of the biggest difficulties in conducting this review resulted have the same value or that elements recording different from the inconsistent terminology used to discuss data quality information have values that make sense when considered we had not expected for example the overlap of terms between together eg biological sex is recorded as female and procedure is dimensions or the fact that the language within a single article recorded as gynecological examination measurement of was sometimes inconsistent the clinical research community concordance is generally based on elements contained within the has largely failed to develop or adopt a consistent taxonomy of ehr but some researchers also included information from other data quality downloaded by on univ july of j from alberta am med https academic oup com jamia article abstract library inform assoc user doi amiajnl review downloaded by on univ july of from alberta https academic oup com jamia article abstract library user j am med inform assoc doi amiajnl there is however overlap between the dimensions of data the use of de identified datasets for research becomes more quality identified during this review and those described in common a fitness for purpose approach which suggests that preexisting taxonomies and models of data quality wang and the quality of each dataset compiled for a specific task must be strong conceptual framework of data quality for example assessed necessitates the adoption of alternatives to gold contains dimensions grouped into four categories intrinsic standard based methods contextual representational and accessibility our review in addition to the overreliance on gold standards the majority focused on intrinsic inherent to the data and contextual task of the studies we identified relied upon an intuitive under dependent data quality issues the dimensions we identified standing of data quality and used ad hoc methods to assess data overlapped with two of the intrinsic features accuracy and quality this tendency has also been observed in other fields believability which are equivalent to correctness and plausi most of the studies included in this review presented assessment bility and two of the contextual features timeliness which is methodologies that were developed with a minimal empirical or equivalent to currency and completeness the only dimension theoretical basis only a few researchers made the effort to we identified that does not appear in wang and strong develop generalizable approaches that could be used as a step framework is concordance towards a standard methodology faulconer and de lusignan for the institute of medicine identified four attributes of data example proposed a multistep statistically driven approach to quality relevant to patient records completeness accuracy data quality assessment hogan and suggested specific legibility and meaning related to comprehensibility as the statistical measures of the correctness and completeness of ehr institute of medicine points out electronic records by their elements that have been adopted by other researchers certain nature negate many of the concerns regarding legibility so we methods including comparing distributions of data from the ehr are left with three relevant attributes two of which we identi with expected distributions or looking for agreement between fied through our review meaning is a more abstract concept elements within the ehr lend themselves more readily to and is likely to be difficult to measure objectively which may be generalization brown and warmington data quality probes why we did not observe assessments of this dimension in the for example could be extended to various data elements although literature they require detailed clinical knowledge to implement some although the five dimensions of data quality derived during researchers looking at the quality of research databases pulled our review were treated as mutually exclusive within the liter from general practices in the uk have adopted relatively consis ature we feel that only three can be considered fundamental tent approaches to comparing the distributions of data concerning correctness completeness and currency by this we mean that specific clinical phenomena to information from registries and these dimensions are non reducible and describe core concepts surveys in most cases however the specific assessment of data quality as it relates to ehr data reuse concordance and methods described in the literature would be difficult to apply plausibility on the other hand while discussed as separate to other datasets or research questions if the reuse of ehr data features of data quality appear to serve as proxies for the for clinical research is to become common and feasible develop fundamental dimensions when it is not possible to assess them ment of standardized systematic data quality assessment directly this supposition is supported by the overlap observed methods is vital in the methods used to assess concordance and plausibility with in addition if as a field we intend to adopt the concept of those used to assess correctness and completeness a lack of fitness for purpose it is important to consider the intended concordance between two data sources for example indicates research use of ehr data when determining if they are of error in one or both of those sources an error of omission sufficient quality some dimensions may prove to be more resulting in a lack of completeness or an error of commission task dependent or subjective while others are essentially task resulting in a lack of correctness similarly data that do not independent or objective it will be important to develop appear to be plausible may be incorrect as in the case of a full understanding of the interrelationships of research tasks a measurement that fails a range check or incomplete such as and data characteristics as they relate to data quality for aggregated diagnosis rates within a practice that do not match example the completeness of a set of data elements required by the expected population rates it may be that correctness one research protocol may differ from the completeness required completeness and currency are properties of data quality while for a different protocol many factors including clinical focus plausibility and concordance are methodological approaches to required resolution of clinical information and desired effect assessing data quality in addition researchers may refer to size can affect the suitability of a dataset for a specific research plausibility or concordance when they believe that there are task problems with completeness or correctness but have no way to be certain that errors exist or which data elements might be future directions wrong we believe that efforts to reuse ehr data for clinical research would benefit most from work in a few specific areas adopting data quality assessment methodology a consistent taxonomy of ehr data quality increasing aware we observed a number of noteworthy patterns within the ness of task dependence integrating work on data quality literature in terms of the types of data quality assessments used assessment from other fields and adopting systematic statisti and the manner in which data quality assessment was discussed cally based methods of data quality assessment a taxonomy of for example of the articles in our sample relied on a gold data quality would enable a structured discourse and contextu standard to assess data quality there are a few problems with alize assessment methodologies the findings in this review this approach first the data sources used could rarely be regarding the dimensions of data quality may serve as a stepping considered true gold standards paper records for example may stone towards this goal task dependence is likely to become sometimes be more trusted than electronic records but they a growing issue as efforts to reuse ehr data for research should not be considered entirely correct or complete perhaps increase particularly as data quality assessment does not have more importantly a gold standard for ehr data is simply not a one size fits all solution one approach to addressing the available in most cases this will become more problematic as problem of ehr data quality and suitability for reuse in research center for ehealth research and disease management department of psychology health and technology university of twente enschede netherlands national institute for public health and the environment bilthoven netherlands emgo institute for health and care research department of clinical psychology faculty of psychology and education vu university amsterdam netherlands corresponding author saskia m kelders phd center for ehealth research and disease management department of psychology health and technology university of twente po box enschede ae netherlands phone fax email m kelders utwente nl abstract background although web based interventions for promoting health and health related behavior can be effective poor adherence is a common issue that needs to be addressed technology as a means to communicate the content in web based interventions has been neglected in research indeed technology is often seen as a black box a mere tool that has no effect or value and serves only as a vehicle to deliver intervention content in this paper we examine technology from a holistic perspective we see it as a vital and inseparable aspect of web based interventions to help explain and understand adherence objective this study aims to review the literature on web based health interventions to investigate whether intervention characteristics and persuasive design affect adherence to a web based intervention methods we conducted a systematic review of studies into web based health interventions per intervention intervention characteristics persuasive technology elements and adherence were coded we performed a multiple regression analysis to investigate whether these variables could predict adherence results we included articles on interventions the typical web based intervention is meant to be used once a week is modular in set up is updated once a week lasts for weeks includes interaction with the system and a counselor and peers on the web includes some persuasive technology elements and about of the participants adhere to the intervention regarding persuasive technology we see that primary task support elements are most commonly employed mean out of a possible dialogue support and social support are less commonly employed mean and out of a possible respectively when comparing the interventions of the different health care areas we find significant differences in intended usage p setup p updates p frequency of interaction with a counselor p the system p and peers p duration f p adherence f p and the number of primary task support elements f p our final regression model explained of the variance in adherence in this model a rct study as opposed to an observational study increased interaction with a counselor more frequent intended usage more frequent updates and more extensive employment of dialogue support significantly predicted better adherence conclusions using intervention characteristics and persuasive technology elements a substantial amount of variance in adherence can be explained although there are differences between health care areas on intervention characteristics health care area per se does not predict adherence rather the differences in technology and interaction predict adherence the results of this study can be used to make an informed decision about how to design a web based intervention to which patients are more likely to adhere j med internet res doi jmir http www jmir org j med internet res vol iss p fo page number not for citation purposes renderx kelders journal of medical internet research et al xsl keywords systematic review web based interventions adherence attrition persuasive technology behavior change introduction web based interventions for promoting health and health related behaviors are seen in many variations and health care areas according to barak et al a web based intervention is a primarily self guided intervention program that is executed by means of a prescriptive online program operated through a website and used by consumers seeking health and mental health related assistance the intervention program itself attempts to create positive change and or improve enhance knowledge awareness and understanding via the provision of sound health related material and use of interactive web based components a web based intervention can involve therapy that lasts for a predetermined fixed period of time however it can also be a continuous program with no specific end date that supports self management among patients with a chronic condition it is made up of different inseparable aspects which according to barak et al are as follows program content multimedia choices interactive online activities and guidance and supportive feedback evidence exists to support the effectiveness of web based interventions research has shown these interventions to be effective in different areas of health care however many evaluations of ehealth interventions report either no positive effects at all or only limited ones one of the issues that is frequently addressed is the problem of non adherence which refers to the fact that not all participants use or keep using the intervention in the desired way research suggests that non optimal exposure to the intervention lessens the effect of these interventions gaining an insight into the factors that influence adherence should therefore be one of the main focus areas in any research study into web based interventions in this context it is important to stress the difference between the terms adherence or non usage attrition and dropout dropout or dropout attrition refers to participants in a study who do not fulfill the research protocol eg filling out questionnaires this is not a focus area of this study adherence or non usage attrition refers to the extent to which individuals experience the content of an intervention this is the focus of our study when looking at literature about adherence to a therapeutic regimen adherence is seen as the extent to which the patient behavior matches the recommendations that have been agreed upon with the prescriber the term is often seen as a reaction to the term compliance which has a more coercive connotation consequently in adherence the patient plays an active role in achieving this behavior at the same time there is also a norm or recommendation from a prescriber which the patient tries to match this recommendation is missing from the definitions of both adherence and non usage attrition in this study we elaborate on the definition by introducing the concept of intended usage intended usage is the extent to which individuals should experience the content of the intervention to derive maximum benefit from the intervention as defined or implied by its creators this matches the norm or recommendation from the definition of adherence to a therapeutic regimen by comparing the observed usage of an individual to the intended usage of a web based intervention we can establish whether or not this individual adheres to the intervention in this context adherence is a process that cannot be assessed solely by measuring usage at the beginning and end of the intervention rather it has to be assessed throughout the entire process to establish whether or not an individual adheres to the intervention at each and every step of the way finally by comparing the observed usage of each individual to the intended usage of the web based intervention the percentage of individuals that adhere to the intervention can be calculated this results in a more objective measurement of adherence which can then be compared to other interventions even if the intended usage is different adherence to web based interventions has been the subject of research for some time many studies focus on whether and which respondents characteristics can explain variations in adherence although this is a very important line of study it seems to take the technology of web based interventions for granted technology as a means to communicate the content has been neglected in research indeed this technology is often seen as a black box a mere tool that has no effect or value and serves only as a vehicle for the delivery of intervention content in line with a recent viewpoint paper we propose to examine the technology from a holistic perspective and see it as a vital and inseparable aspect of the web based intervention this approach has been recommended in recent literature and has been the key point in the field of persuasive technology where there are examples of studies on the persuasive capacities of technology to support web based interventions in the health care domain recently two systematic reviews on the influence of intervention factors on adherence to web based interventions were published although both reviews provide valuable insights we feel that there are shortcomings that limit the applicability of these results for our objectives first with regard to adherence the study of brouwer takes exposure to interventions delivered via the internet as the outcome measure exposure is seen as the number of times the user or patient logged on the time spent on site page views etc but these are static measurements unrelated to the usage intended by these interventions this gives limited insights into the process of usage and adherence which makes it difficult to compare different interventions and specify how well certain interventions are doing a review by schubart fails to distinguish between dropout and adherence this approach limits the applicability of the results because in real life implementation of web based interventions there is no research protocol to adhere to only the intervention the results of schubart review cannot http www jmir org j med internet res vol iss p fo page number not for citation purposes renderx kelders journal of medical internet research et al xsl be generalized to these situations because we do not know whether engagement is due to the research or the intervention furthermore regarding the intervention factors both studies use an ad hoc classification of these factors without a theoretical foundation which makes it difficult to generalize and explain the results we consider a web based intervention as consisting of content interaction and technology and although these aspects are inseparable they can be looked at in a structured manner both earlier reviews use a classification that in our opinion has substantial overlap in the goals to be achieved with these aspects for example in the review by brouwer a distinction is made between interactive behavior change strategies and interactive elements it is stated that the goal of interactive elements is to improve the attractiveness of the intervention or to provide the option for more information but this is not mutually exclusive with interactive behavior change strategies for example a quiz is seen as an interactive element but in our opinion it can also be used as a means of receiving tailored feedback or as a way to self monitor your knowledge or behavior allocating a quiz to one of these categories is therefore problematic the categorization of intervention factors in the review by schubart lacks depth and tries to encompass in one single categorization both modality ie the channel through which content is delivered for example email or telephone and strategy eg feedback the current study attempts to overcome these shortcomings by employing a more objective and comparable measurement of adherence to web based interventions and a classification of technology based on persuasive technology literature from the field of persuasive technology we learn that technology has the capacity to be persuasive through its role as a tool a medium and a creator of experiences fogg definition of persuasive technology limits this field to human computer interaction and does not include computer mediated communication ie including interaction with a person however we feel that it is unnecessary and undesirable to separate these two aspects of technology particularly in the area of health care because a web based intervention is made up of different inseparable aspects we therefore propose a broader application of the term persuasive technology to include both human computer interaction and computer mediated communication accordingly regarding the aspects of a web based intervention we propose a more pragmatic conceptual division between technology ie all the features of the web based intervention including multimedia and online activities and interaction ie all interactions between the user or patient and the intervention a counselor or peers which is slightly different from the aspects proposed by barak following fogg work oinas kukkonen introduces a framework to classify technology in its persuasive functions this persuasive system design psd model which is used for example in a study by lehto and colleagues classifies features of the technology as primary task support dialogue support social support and credibility support by applying this model to web based interventions we can systematically look at how persuasive system design categories are used and investigate their possible influence on adherence this study investigates whether intervention characteristics and persuasive design affect adherence to a web based intervention web based interventions are applied in various health care domains and intuitively it seems that there are differences between web based interventions aimed at people with a chronic condition at lifestyle change or at mental health because of the target group involvement with a health care professional and duration of the interventions however the underlying principles may well be the same therefore from an intervention perspective there is no absolute need to see these areas as being so different from each other that they cannot be compared consequently it is interesting to see whether the preconceptions about the differences can be confirmed and whether there is added value for researchers and designers in one area to look at interventions from a different area our systematic review aims to answer the following research questions what are the key characteristics of web based interventions in terms of technology and interaction are there any differences in intervention characteristics between web based interventions aimed at chronic conditions lifestyle or mental health what percentage of participants adhere to web based interventions which characteristics of web based interventions related to technology and interaction are linked to better adherence these insights can help us understand and reduce the impact of non adherence methods search strategy we conducted a comprehensive literature search using the following bibliographic databases web of knowledge ebscohost picarta sciverse scopus and sciencedirect we used a combination of the constructs web based intervention adherence and health for each construct we used several keywords see multimedia appendix to ensure a broad coverage of published studies in our review following this search strategy we identified articles published up to oct see figure for the full flow diagram of article selection http www jmir org j med internet res vol iss p fo page number not for citation purposes renderx kelders journal of medical internet research et al xsl figure flow diagram of study selection eligibility criteria were as follows dropout attrition and non adherence were indistinguishable the intervention was aimed at care the review is limited to studies of web based interventions in providers or relatives of the patient the description of the the health care domain the criteria used for including a study intervention did not include information about the applied were it involved a web based intervention for promoting persuasive features of the technology and the web based health through behavioral change the web based intervention intervention was not primarily intended to be used through a was intended to be visited and used on more than one occasion computer or laptop at the user or patient home in addition the research included an assessment of the effect of the we only included peer reviewed published articles intervention the study reported objective quantifiable measurements of usage for the intervention and the study was published in either english or dutch exclusion criteria http www jmir org j med internet res vol iss p fo page number not for citation purposes renderx kelders journal of medical internet research et al xsl study selection and data collection the study selection was done in three steps first the titles of all retrieved articles were screened for eligibility by two authors sk and rk second the abstracts of all initially relevant articles were screened for eligibility by the same authors finally the full text of all remaining publications was checked for inclusion by two authors sk and rk or sk and jvg in cases where the suitability of a study came into question during one of the steps it was included in the next step disagreements about including the full text publication were discussed until agreement was reached to check whether any eligible publications had been overlooked during the initial search process the reference lists of all systematic reviews that were identified in the original search were checked to find additional publications that met our inclusion criteria the characteristics of all of the interventions that were included were coded by two researchers sk and rk using a data extraction form based on a protocol for the systematic review of ehealth technologies where possible data was extracted using the consort ehealth checklist for the extraction we relied on information that was available in the published literature the basis of the data extraction was the intervention not the study itself this meant that for some interventions data from more than one article was used furthermore when a study described more than one web based intervention eg a comparison of two web based interventions all web based interventions were coded separately data items the following characteristics were coded intervention name the name of the intervention was recorded if the intervention had no name the intervention was named after the first author of the primary article about the intervention behavior or condition the targeted behavior or condition of each intervention was recorded furthermore we recorded the area of health care targeted by the intervention chronic condition lifestyle or mental health studies and study design for each intervention the studies that were used to code the characteristics of the intervention were recorded furthermore we also recorded whether these studies were randomized controlled trials rcts or observational studies without randomized control groups intended usage intended usage was defined as the extent to which the developers of the intervention felt that the intervention should be used to achieve the desired effect when this information was not reported it was inferred from the description of the intervention for example interventions requiring patients to monitor their behavior and receive feedback once a week to achieve the desired effect were coded as intended to be used once a week actual usage all reported information regarding the usage of the intervention related to its intended usage was collected including the number of times the user or patient logged on and the number of modules completed adherence a percentage of adherence was calculated to enable us to compare the different interventions we did this by calculating the percentage of participants that adhered to the intervention for example when the intended use of an intervention was complete modules and out of participants completed modules the adherence was for each intervention that was included we calculated one overall adherence percentage when more studies about the same intervention yielded different adherence percentages we calculated the overall adherence percentage using a weighted average based on the number of participants in each study furthermore when the study included a waiting list and the respondents in this waiting list received access to the intervention at a later stage the adherence was calculated based on usage data for all participants including the waiting list group updates the frequency of content updates for the web based intervention for a participant was recorded this could be based on new information being uploaded for all participants or on a new lesson becoming available for a specific participant duration the duration of the intervention in weeks was recorded setup for each intervention we created a record indicating whether the setup was modular ie content is delivered in a sequential order whereby new content is made available when the user reaches a certain point or free ie all the content of the intervention is available to the user from the start interaction all information about the interaction with participants was recorded and this interaction could be with the system eg automatic email reminders or a web based automated response to filling out an exercise with a counselor eg through email telephone or face to face meetings or with peers eg through a discussion board chat group or face to face group sessions modality we recorded when interaction with the system counselor or peers took place through a different modality than web based face to face meeting telephone or sms an exception was made when the study protocol included a face to face meeting or telephone intake this was not coded as interaction through a different modality because it was not part of the actual intervention persuasive technology in the intervention the applied principles of persuasive technology within the interventions were coded according to the psd framework of http www jmir org j med internet res vol iss p fo page number not for citation purposes renderx kelders journal of medical internet research et al xsl oinas kukkonen and harjumaa we omitted system credibility support because of an observed lack of reporting of these principles in the studies that were included the elements from the psd framework on primary task dialogue and social support with the definitions and the coding scheme we used are presented in table the coding scheme is somewhat modified for the purpose of this study and to account for the computer mediated communication included however when coding the persuasive technology elements the technology was central not the content of the interaction therefore when computer mediated communication was present the content of this communication was not coded as persuasive technology for example when a feedback message from a care provider contained praise this was not coded as dialogue support when the technology provided a praising message after the user had successfully filled out a diary entry then it was coded for each intervention the elements that were present were coded irrespective of whether the designers of the intervention deliberately included these elements as persuasive technology elements to check for differences in interpretation when coding the persuasive technology elements interventions were coded by researchers sk and lvg the interrater reliability measured by cohen kappa was analyses all data on each intervention was entered in spss version ibm corporation somers ny usa and we treated each intervention as a separate case descriptive data of the combined data of all included interventions on all variables were calculated using spss differences in variables between health care areas were calculated using fisher exact tests because of the small expectation values and one way analyses of variance to investigate whether the characteristics of the included interventions could predict the observed adherence we performed a hierarchical multiple linear regression analysis using a block wise enter method the first block was related to the context of the web based intervention and included the health care area coded as dummy variables and the study design rct vs observational which other researchers have proposed to influence adherence or the effect of web based interventions the second block relates to our concept of interaction as one of the aspects of a web based intervention and consists of the frequency of interaction with a counselor the system and peers as well as the modality employed the third and fourth blocks relate to our concept of technology in a web based intervention where the third block contains the intervention characteristics intended usage setup updates and duration and the last block contains the categories of persuasive system design it is important to note that we chose to include the categories and not the separate elements in the multiple regression because the results could be biased when some elements are hardly used and these elements are entered as predictors entering all elements increases the chance of a type i error and the psd model has grouped the elements on their key benefits when the benefits of the specific elements in a category are similar then looking at the specific elements could cause the overall influence of the category to be missed http www jmir org j med internet res vol iss p fo page number not for citation purposes renderx kelders journal of medical internet research et al xsl table psd framework elements coding scheme principle and definition according to psd framework coded as element included when example the web based intervention primary task support reduction a system that reduces complex specifically divides the target behav a web based intervention for weight behavior into simple tasks helps ior into small simple steps management includes a diary for users perform the target behav recording daily calorie intake thereby ior and it may increase the benefit cost ratio of a behavior dividing the target behavior reducing calorie intake into small simple steps of which one is recording calorie in take tunneling using the system to guide users delivers content in a step by step a web based intervention for the through a process or experience format with a predefined order prevention of depression that delivers provides opportunities to per the content in sequential lessons that suade along the way can only be accessed when the previ ous lesson is completed tailoring information provided by the provides content that is adapted to a web based intervention for support system will be more persuasive factors relevant to a user group or ing self management among patients if it is tailored to the potential when a counselor provides feedback with diabetes provides information needs interests personality based on information filled out by adapted to patients based on whether usage context or other factors a participant relevant to a user group they have diabetes mellitus type i or ii personalization a system that offers personal provides content that is adapted to a web based intervention for increas ized content or services has a one user ie the name of the user is ing physical activity allows users to greater capability for persua mentioned and or the user can adapt choose whether they want to see their sion a part of the intervention weekly activity score on the home page or not self monitoring a system that keeps track of provides the ability to track and a web based intervention for the one own performance or sta view the user behavior perfor treatment of alcohol dependence pro tus supports the user in achiev mance or status vides a diary to track and view daily ing goals alcohol use simulation systems that provide simula provides the ability to observe the a web based intervention for smok tions can persuade by enabling cause and effect relationship of rel ing cessation includes a calculator users to observe immediately evant behavior that shows how much users will save the link between cause and ef when they quit smoking fect rehearsal a system providing means with provides the ability and stimulation a web based intervention for support which to rehearse a behavior to rehearse a behavior or to rehearse ing self management in patients with can enable people to change the content of the intervention epilepsy starts each lesson with the their attitudes or behavior in the same important exercise for stress real world management dialogue support praise by offering praise a system offers praise to the participant on a web based intervention that aims can make users more open to any occasion to promote healthy nutritional habits persuasion compliments participants when they have eaten pieces of fruit for days rewards systems that reward target be offers some kind of reward when a web based intervention for the haviors may have great persua the participant performs a target be treatment of social phobia gives sive powers havior relating to the use or goal of points to participants when they en the intervention gage in exposure exercises reminders if a system reminds users of provides reminders about the use of a web based intervention to support their target behavior the users the intervention or the performance self management among patients with will more likely achieve their of target behavior rheumatic arthritis sends an automatic goals email message to remind the partici pant that the new lesson may begin suggestion systems offering fitting sugges provides a suggestion to help the a web based intervention for weight tions will have greater persua participants reach the target behav management provides low calorie sive powers ior recipes http www jmir org j med internet res vol iss p fo page number not for citation purposes renderx kelders journal of medical internet research et al xsl principle and definition according to psd framework coded as element included when example the web based intervention similarity people are more readily per is designed to look familiar and de a web based intervention for the suaded through systems that signed especially for the participant treatment of panic disorder in teenage remind them of themselves in girls explains the exercises through a some meaningful way teenage girl with panic problems liking a system that is visually attrac is visually designed to be attractive during the design of a web based in tive for its users is likely to be to the participants tervention to increase physical activi more persuasive ty in middle aged women a represen tative group is asked for feedback on the design and their feedback is sub sequently incorporated in the new design social role if a system adopts a social role acts as if it has a social role eg a a web based intervention to support users will more likely use it for coach instructor or buddy self management among patients with persuasive purposes migraine incorporated an avatar to guide the participant through the inter vention social support social learning a person will be more motivat provides the opportunity and stimu a web based intervention for weight ed to perform a target behavior lates participants to see others using management provides the option and if he can use a system to ob the intervention or performing the stresses the importance of posting serve others performing the be target behavior physical activity self monitoring data havior on the discussion board and comment ing on the performance of others social comparison system users will have a provides the opportunity for partici a web based intervention for drug greater motivation to perform pants to compare their behavior to abuse prevention for teenagers auto the target behavior if they can the target behavior of other partici matically compares the response of compare their performance with pants and stimulates them to do this the participant to other users of the the performance of others intervention normative influence a system can leverage norma provides normative information on a web based intervention to promote tive influence or peer pressure the target behavior or the usage of self management among patients with to increase the likelihood that the intervention copd provides feedback on the level a person will adopt a target be of physical activity of the participant havior by comparing it to the physical activ ity of well managed copd patients social facilitation system users are more likely to provides the opportunity to see a web based intervention for smok perform target behavior if they whether there are other participants ing cessation includes a discussion discern via the system that oth using the intervention board for users of the intervention ers are performing the behavior along with them cooperation a system can motivate users to stimulates participants to cooperate a web based intervention for the adopt a target attitude or behav to achieve a target behavior promotion of physical activity stimu ior by leveraging human be lates participants to form groups and ings natural drive to cooperate to achieve the group goal of a certain number of steps each week competition a system can motivate users to stimulates participants to compete a web based intervention for diabetes adopt a target attitude or behav with each other to achieve a target management among children includes ior by leveraging human be behavior a leaderboard in which the children ings natural drive to compete who enter blood glucose levels at the right times receive the highest place recognition by offering public recognition prominently shows former partici a web based intervention treatment for an individual or group a pants who adopted the target behav of anxiety includes a testimonial page system can increase the likeli ior where successful users of the interven hood that a person group will tion tell their story adopt a target behavior http www jmir org j med internet res vol iss p fo page number not for citation purposes renderx kelders journal of medical internet research et al xsl results of the participant with a counselor and a similar percentage included some form of interaction with the system a study selection the search yielded unique titles after title abstract and full text screening articles on interventions were included figure in total articles were excluded based on the full text the most common reason for exclusion was related to usage data the lack thereof n or the presentation little over half of the interventions included interaction with peers with and without counselor interaction the average percentage of participants who adhered to an intervention is min max the values of each of the variables for each included intervention can be found in multimedia appendix of inadequate ie subjective or not usable for calculating differences in intervention characteristics between adherence usage data n other studies were excluded health care areas based on the studied intervention not aimed at health promotion by changing behavior n not primarily meant to be used from a computer or laptop at the user home n not intended to be visited and used on more than one occasion n or not targeted at the patient n twenty seven publications were excluded because the study design did not include an assessment of the effect of the intervention eg when they only presented qualitative data on the design of an intervention or when the study design did not provide unique usage data eg a study about the long term effects of an intervention seven publications were excluded because of the description of the intervention or study in publications no information could be gathered on the applied persuasive features of the technology from the description of the intervention and in publications the data on the number of participants and their usage of the intervention was unclear finally in the case of one citation the full text could not be retrieved this citation was therefore excluded when comparing the interventions of the different health care areas using fisher exact tests we find significant differences on intended usage p setup p updates p frequency of interaction with a counselor p the system p and peers p when looking at the standardized residuals data not shown we can see where these differences are manifested we see that lifestyle interventions are more often intended to be used less than once a month than interventions in the other areas we see that mental health interventions are less often free in terms of their setup than the other two areas lifestyle interventions are more often not updated or updated without a known frequency regarding interaction with a counselor we see that lifestyle interventions more often do not employ this feature furthermore we see that lifestyle interventions more frequently include interaction with the system less than once a week finally on interaction with peers chronic interventions more often have interaction for which the frequency is not specified one way analyses of characteristics of the studies that were included the interventions that were included are presented in multimedia appendix overall interventions targeted a specific chronic condition diabetes was targeted most often with interventions sixteen interventions targeted a lifestyle behavior weight management was targeted most often with interventions smoking cessation was also often seen interventions were targeted solely on smoking cessation and intervention included smoking cessation as one of multiple targeted behaviors finally mental health was targeted most often in the studies that were included of these interventions focused on social phobia although it should be noted that these interventions are only from two research groups that extensively studied their interventions depression panic disorder and anxiety were also targeted frequently in the variance show that there are differences in duration f p and adherence f p bonferroni post hoc analyses show that the difference in duration is between lifestyle and mental health interventions lifestyle interventions are longer whereas the difference in adherence is between lifestyle and chronic condition interventions and between lifestyle and mental health interventions lifestyle interventions have a lower adherence rate in sum lifestyle interventions are longer the intended usage is less frequent they have fewer updates there is less interaction with the system and a counselor and there is lower adherence than interventions aimed at chronic conditions and mental health mental health interventions are less often free in their setup and interventions aimed at a chronic condition include interaction with peers more often for which the frequency is not specified interventions that we included and interventions persuasive technology respectively when examining the persuasive technology elements that are table presents an overview of the variables of the interventions that were coded and their distribution over the different areas chronic condition lifestyle and mental health overall we can see that most interventions were meant to be used once a week were set up in a modular way were updated once a week and lasted for approximately weeks median duration weeks face to face telephone and sms support or a combination of these modes were infrequently used with interventions combining face to face and telephone support interventions and and interventions combining telephone and sms support interventions and seventy six per cent of the interventions included interaction presented in table we see that a mean of median out of a possible elements were used within a web based intervention primary task support shows the highest mean out of a possible median while social support shows the lowest mean out of a possible median one way analyses of variance show that there is a significant difference between the use of persuasive technology elements for primary task support f p a bonferroni post hoc analysis shows that this difference is between lifestyle and mental health interventions where lifestyle interventions employ a higher mean of elements than mental health interventions furthermore we can see that in primary task support tunneling http www jmir org j med internet res vol iss p fo page number not for citation purposes renderx kelders journal of medical internet research et al xsl is used most often n closely followed by tailoring n tunneling is used in all included mental health interventions but only in of lifestyle interventions significant difference p reduction and self monitoring are less often used in mental health interventions than in the other areas significant difference reduction p and self monitoring p this is most strikingly seen in self monitoring which is used in of lifestyle interventions as opposed to in the mental health interventions overall rehearsal and simulation are used least of all out of the primary task support elements from the dialogue support elements reminders are most often used n across all areas suggestion is the second most frequently used element n although this is used more often in web based interventions targeted at chronic conditions than in mental health p praise was not used in any of the interventions and rewards were used only in interventions in social support we see that social facilitation is most often used n with a significant difference between interventions aimed at a chronic condition n including social facilitation and at lifestyle n p furthermore social learning and social comparison are used reasonably frequently respectively n and n with mental health interventions predominantly contributing to these numbers with a significant difference only for social learning p cooperation on the other hand is used in lifestyle interventions and chronic intervention but in none of the mental health interventions significant difference p the other elements normative influence competition and recognition are hardly used in sum primary task support is most extensively employed while dialogue support and social support are sparsely employed tunneling tailoring primary task support reminders dialogue support and social facilitation social support support are the most frequently used elements on average lifestyle interventions employ more primary task support elements than mental health interventions predictors of adherence we performed a hierarchical multiple linear regression using a block wise enter method to explore the predictors of adherence variables expected to predict adherence were entered in the analysis in blocks of related constructs as specified in the methods section the final model explained of the variance in adherence in this model interventions studied with a rct design instead of an observational study increased interaction with a counselor more frequent intended usage more frequent updates and more extensive employment of dialogue support significantly predicted better adherence http www jmir org j med internet res vol iss p fo page number not for citation purposes renderx kelders journal of medical internet research et al xsl table descriptive variables of the included interventions per health care area variable chronic lifestyle mental total n n n n n n n n intended usage month month week week week setup free modular updates none yes fns a month month week week week duration weeks mean sd b median interaction with counselor none yes fns week week week interaction with system none yes fns week week week interaction with peers none yes fns week week week face to face included phone included sms included adherence mean sd a fns frequency not specified b based on interventions three interventions and did not specify duration http www jmir org j med internet res vol iss p fo page number not for citation purposes renderx kelders journal of medical internet research et al xsl table persuasive technology in web based interventions included in this study per health care area variable chronic lifestyle n n n mental n total n p a n n n primary task support mean sd median reduction tunneling tailoring personalization self monitoring simulation rehearsal dialogue support mean sd median praise rewards reminders suggestion similarity liking social role social support mean sd median social learning social comparison normative influence social facilitation cooperation competition recognition total mean sd a based on fisher exact test note results in italics are the mean sd and median number of elements used per intervention other results are presented as the number of interventions that include a certain element http www jmir org j med internet res vol iss p fo page number not for citation purposes renderx kelders journal of medical internet research et al xsl table predictors of adherence in a hierarchical multiple linear regression step variable b se b beta p constant chronic lifestyle study design constant chronic lifestyle study design freq interaction with counselor freq interaction with system freq interaction with peers phone face to face sms constant chronic lifestyle study design freq interaction with counselor freq interaction with system freq interaction with peers phone face to face sms intended usage setup updates duration constant chronic lifestyle study design freq interaction with counselor freq interaction with system freq interaction with peers phone 06 face to face sms 08 intended usage 04 setup updates http www jmir org j med internet res vol iss p fo page number not for citation purposes renderx kelders journal of medical internet research et al xsl step variable b se b beta p duration primary task support dialogue support 09 social support 04 note r for step p 08 r for step p r for step p r for step p cumulative variance explained in the final step model r p discussion in this systematic review we have attempted to synthesize the combined knowledge of ehealth researchers to gain insights into the factors that affect adherence to web based interventions in the areas of chronic conditions lifestyle and mental health in this study we viewed technology from a theoretical perspective and conceived adherence as an objective measurement that allows for comparison between different interventions principal results we included publications describing research into interventions mental health interventions n constituted the largest part of these interventions looking at the key characteristics of web based interventions in terms of technology and interaction it appears that the typical web based intervention is meant to be used once a week is modular in setup is updated once a week lasts for weeks includes interaction with the system a counselor and peers on the web includes some persuasive technology elements and results in about of the participants adhering to the intervention however to answer our second research question there do appear to be differences between health care areas overall lifestyle interventions are longer and less strict more employ a free setup less frequent intended usage fewer updates and less interaction than interventions aimed at chronic conditions and mental health which seems to result in lower adherence with lifestyle interventions mental health interventions follow the weekly modular format the most with only one intervention using a free setup this may be explained by the difference in scope of lifestyle and mental health interventions lifestyle interventions may be more oriented towards long term changes while mental health interventions are often aimed at treatment that is delivered in a short strict format however interventions for a chronic condition are also aimed at a long term change or goal but these interventions are on average more strict than lifestyle interventions more counselor involvement is likely to be an explanation because these interventions are often offered in a health care setting and we saw a significant difference between these areas regarding persuasive technology we see that primary task support elements are most commonly employed especially in interventions aimed at chronic conditions and lifestyle tunneling which is a technological result of a modular setup is employed most often in mental health interventions and less frequently in lifestyle interventions this difference is a logical result of the differences in setup between interventions in these areas this finding is not surprising taking into account that most mental health interventions are based on regular face to face therapy where psycho education and behavior modification is usually delivered step wise see tailoring which is widely recognized as an important feature of effective health communication is used in one form or another in of the interventions strikingly rehearsal which is also seen as very important in learning and behavior change is seldom employed it may be that rehearsal is seen by the authors of the articles reviewed as such an obvious part of an intervention that a description of this process is omitted from the description of the interventions if not this should be a point of particular interest when re designing web based interventions only a mean of out of a possible dialogue support elements are employed per web based intervention it should be noted that we have not coded the elements that may be present in email like messages sent by a counselor because we feel that this is part of the counselor interaction and not so much a part of the dialogue support that oinas kukkonen and fogg describe reminders are the most frequently employed element studies have shown the importance of reminders in increasing adherence and in increasing the effectiveness of web based interventions therefore we found it striking that of the interventions did not include reminders in some way suggestion was the second most frequently used element and was employed more in interventions aimed at chronic conditions than mental health this seems likely to be due to the focus of the interventions for chronic conditions being on coping with a condition and giving suggestions or strategies to achieve this whereas in mental health interventions the focus is often more curative to solve a certain problem praise and rewards are seldom used which may be a shortcoming when looking at the recent literature into serious gaming and gamification where employing game like strategies such as praise and rewards are expected to have positive effects on the outcomes of health interventions social support is widely recognized as an important strategy in behavior change and it might be disappointing to see that on average only out of a possible elements are used per web based intervention social facilitation was used in more than half of the interventions it must be noted that here social facilitation means providing the opportunity to contact others using the same intervention it does not say anything about whether the opportunity is actually used in practical terms this means that when an intervention includes a discussion board social facilitation is employed even when there are no posts on http www jmir org j med internet res vol iss p fo page number not for citation purposes renderx kelders journal of medical internet research et al xsl the discussion board social learning and social comparison were employed through for example obligatory posts of exercise answers on a discussion board or by providing a story by a user real or fictive including how he or she dealt with the situation cooperation competition normative influence and recognition are seldom used and therefore provide areas in which web based interventions might be improved however in this study social support did not affect adherence so more research is needed to investigate whether or not this area provides added value our third research question was about the percentage of participants that adhere to web based interventions we found an average adherence of which confirms that non adherence is an issue in web based interventions there was a wide range in the level of adherence with interventions scoring below adherence and interventions scoring adherence or higher our last research question was aimed at determining which characteristics of web based interventions relating to technology and interaction are related to better adherence using a hierarchical multiple linear regression our final model explains of the variance in adherence which in our view is a substantial amount that provides valuable insights into the issue of adherence interestingly the first two models including the context of the intervention and the interaction within the intervention were not significant it was only when aspects relating to the format of the intervention and the technology employed were entered that the model reached significance in the final model an rct as opposed to an observational study significantly predicted better adherence a likely explanation is that the observational studies in our review were mainly small pilot studies and large real life studies pilot studies are likely to show lower adherence rates because the interventions are not fully tested and are improved after the outcomes of the pilot are known real life observational studies have been shown to have lower adherence rates which suggests that the formal structure of a trial is important for participants to adhere to an intervention furthermore the selection processes of many rcts make it likely that there is a difference in the participants in both settings which contributes to the difference in adherence the frequency of interaction with a counselor was a significant predictor of adherence this finding concurs with reviews of brouwer schubart and other studies for an overview see that conclude that counselor or clinician support is related to greater exposure and engagement of the significant predictors in our study this variable contributes the least in our review we have found no evidence that the frequency of interaction with peers is related to adherence this is somewhat contrary to the results of brouwer who concluded that peer support was related to greater exposure in that study exposure was seen as the time visitors spend on the website which is very different from our definition of adherence furthermore in this study we coded the frequency of interaction not merely whether there was any interaction or not this resulted in of interventions being coded as yes there is interaction with peers but the frequency is unknown this frequency may vary to a large degree between these interventions but without clear information we cannot make a distinction which may have influenced our results in the final model the frequency of interaction with the system seems to negatively influence adherence although not significantly this surprising finding may be explained by the fact that more interaction with the system meant in many cases that there was no interaction with a counselor more frequent intended usage also predicts better adherence this might seem counterintuitive but might also mean that when people are expected to be more active they become more engaged with the system moreover more frequent intended usage will in many cases lead to more frequent reminders and we know that reminders can positively influence adherence that the provision of frequent updates is important was also seen in the review of brouwer and is confirmed in this study finally more extensive employment of dialogue support is related to better adherence this outcome was predicted by the persuasive system design model but this study is to our knowledge the first to confirm this outcome related to adherence in a health setting when looking at the other persuasive technology categories we see that social support shows a trend towards a significant contribution to better adherence we feel that this trend warrants further investigation it might be that it has no significant predictive value in this study because of the limited use of social support elements in the included interventions interestingly primary task support does not show any predictive value for adherence this may well be explained by the purpose of the employment of primary task support as indicated in the name these elements make the primary task ie the goal of the intervention easier and are not so much focused on the process ie using the intervention or adhering to the intervention it seems likely that these elements play a more important role in the effect of the intervention than in the adherence a final comment on the model for the prediction of adherence is on the different health care areas we see that in the first model lifestyle interventions as opposed to mental health interventions predict a lower adherence but when adding the characteristics of the interventions in the model this predictive value is negated it seems that the health care area per se does not predict adherence but the differences in the characteristics of the interventions in these areas do predict adherence implications and recommendations taking into account the results of this study it seems reasonable to not only hope for adherence but to plan for adherence when designing web based interventions although studies that are included in this review state that they have planned for adherence it is remarkable that state that encouraging adherence is a task for the counselor and one study included monetary incentives to promote adherence of the studies that mention adapting the design of the intervention to increase adherence studies do so without any theoretical basis or reference studies make the adaptation the focus of their study and studies have adapted the design based on a prior study on the same intervention overall it seems that adapting web based interventions to promote adherence is done in an ad hoc manner http www jmir org j med internet res vol iss p fo page number not for citation purposes renderx kelders journal of medical internet research et al xsl and that a framework to guide researchers and developers in this area is needed the psd model may provide such a framework for the design of web based interventions moreover it seems valuable to look much further than the health care area for which the intervention is being designed although each health care area has its own demands and limitations the different areas might learn from each other strong points lifestyle interventions although aimed at long term goals might benefit from incorporating segments with a more strict format and shorter duration mental health interventions might be extended to aim at more long term goals like relapse prevention they may therefore employ a less strict format while being aware that adherence might become a larger problem moreover mental health interventions might include the primary task support elements used in chronic condition and lifestyle interventions furthermore we now have evidence that certain intervention characteristics and persuasive technology can improve adherence it seems that expecting a certain amount of engagement from the target group can actually be helpful in promoting adherence and is something that seems to be easy to implement in new and existing web based interventions we must keep in mind that the effect of intended usage might also be due to a bias among the participants when only those participants who agree in advance with a high level of engagement participate in such interventions duration seems harder to change cutting an intervention into shorter segments may be enough to improve adherence but this should be investigated further including and possibly increasing the frequency of interaction with a counselor seems a more costly way to improve adherence and might therefore be a less than optimal starting point when specifically used as a strategy to increase adherence increasing dialogue support using persuasive technology seems to be a more cost effective vantage point in this respect and may even be enhanced by the increasing use of mobile technology which seems likely to in turn offer a valuable platform for introducing on the spot reminders and feedback additionally our results can be of value for blended care ie a combination of online and face to face care by clarifying the crucial aspects for promoting adherence in web based interventions when it is not possible to adapt a web based intervention to promote adherence it may be feasible to include a face to face segment in the overall intervention at a crucial stage to make up for the predicted loss of adherence the results of this study can be used to make an informed decision about how to design a web based intervention that has a greater likelihood of patient adherence it must be noted however that we do not advocate a so called technology push where technology is introduced only for the sake of the technology and the ability to create the technology it should always be created in close collaboration with the target audience and with a clear goal to create a viable ehealth technology this study provides insights into the choices one can make with the target audience in this study we defined adherence as being the proportion of participants who use the intervention as it is intended to be used by doing this we have created an adherence measurement from objective data that is comparable between interventions we feel that the study shows that this is a promising approach and this adherence measurement can be used for a wide variety of studies however to date few studies report adherence as the measurement we have chosen to use for review studies this means that researchers have to define the intended use search for the usage data that corresponded to this intended use and then calculate the adherence this might lead to a different interpretation of the usage data than the original authors intended however from our experience we can say that as long as there is enough information on the intervention and the usage it is feasible to calculate an objective and comparable adherence measurement for intervention studies we would advise researchers to at least provide the information needed ie intended usage and usage data related to this intended usage to calculate this adherence measurement and preferably to state the calculated adherence percentage for easy comparison between interventions limitations in this study we have excluded many interventions because data about usage was absent or the usage data that was presented had no direct relationship to the intended use for example we excluded studies that only presented mean login data per week for all respondents and had an intended usage of once a week because these data do not show us which percentage of respondents logged in each week this strict selection based on usage data might have introduced a bias in our included studies we have coded the web based interventions included in this study based on the descriptions in the published literature although we have made an effort to find all the information in the published literature about each intervention our coding was limited by the description of the interventions on paper as is noted by other authors the description of these interventions is varied which makes it difficult to capture all the characteristics of each intervention and this might have influenced our results initiatives to standardize and improve the description of web based interventions like the consort statement for ehealth a protocol for systematic reviews in ehealth and guidelines for executing and reporting internet intervention research are therefore very necessary and will hopefully improve the possibility to compare ehealth technologies and learn from each other lastly a limitation of this review might be that we have only focused on the published literature we have not included grey literature and have therefore included little real life adherence data as noted by christensen there is a difference between the usage of web based interventions in a research setting and in a more real life setting we have tried to cope with this by using a strict definition of adherence separating it from following the research protocol and filling out questionnaires and by coding all interaction that might be the result of being part of a study as part of the intervention nonetheless the limited amount of real life data in our review might have influenced the results overall our results confirm the conclusions of prior studies that interaction with a counselor and regular updates promote http www jmir org j med internet res vol iss p fo page number not for citation purposes renderx kelders journal of medical internet research et al xsl adherence furthermore the results of this review elaborate on the role of intervention characteristics duration setup and intended usage and persuasive technology especially elements to support the dialogue finally this study has provided practical recommendations to increase adherence when re designing a web based intervention future research the data and results from this study provide numerous points of departure for future research to increase our understanding of the characteristics of web based interventions and their effect on adherence it would be interesting to compare interventions that show high adherence with interventions that show low adherence using in depth qualitative analyses the positive deviance approach used by schubart seems appropriate for this goal furthermore it is interesting to test our statistical adherence model in experimental studies additionally expanding the model by including the characteristics of participants seems to be relevant finally exploring the relationship between persuasive technology especially primary task support and clinical outcomes of an intervention is likely to be a worthwhile line of research conflicts of interest none declared background there is currently a lack of information about the uses benefits and limitations of social media for health communication among the general public patients and health professionals from primary research objective to review the current published literature to identify the uses benefits and limitations of social media for health communication among the general public patients and health professionals and identify current gaps in the literature to provide recommendations for future health communication research methods this paper is a review using a systematic approach a systematic search of the literature was conducted using nine electronic databases and manual searches to locate peer reviewed studies published between january and february results the search identified original research studies that included the uses benefits and or limitations of social media for health communication among the general public patients and health professionals the methodological quality of the studies assessed using the downs and black instrument was low this was mainly due to the fact that the vast majority of the studies in this review included limited methodologies and was mainly exploratory and descriptive in nature seven main uses of social media for health communication were identified including focusing on increasing interactions with others and facilitating sharing and obtaining health messages the six key overarching benefits were identified as increased interactions with others more available shared and tailored information increased accessibility and widening access to health information peer social emotional support public health surveillance and potential to influence health policy twelve limitations were identified primarily consisting of quality concerns and lack of reliability confidentiality and privacy conclusions social media brings a new dimension to health care as it offers a medium to be used by the public patients and health professionals to communicate about health issues with the possibility of potentially improving health outcomes social media is a powerful tool which offers collaboration between users and is a social interaction mechanism for a range of individuals although there are several benefits to the use of social media for health communication the information exchanged needs to be monitored for quality and reliability and the users confidentiality and privacy need to be maintained eight gaps in the literature and key recommendations for future health communication research were provided examples of these recommendations include the need to determine the relative effectiveness of different types of social media for health communication using randomized control trials and to explore potential mechanisms for monitoring and enhancing the quality and reliability of health communication using social media further robust and comprehensive evaluation and review using a range of methodologies are required to establish whether social media improves health communication practice both in the short and long terms http www jmir org j med internet res vol iss p fo page number not for citation purposes renderx moorhead journal of medical internet research et al xsl j med internet res doi jmir keywords health communication social media review introduction there is an ongoing increase in the use of social media globally including in health care contexts when focusing on social media for health communication it is useful to first outline the general characteristics of social media kaplan and haenlein defined social media as a group of internet based applications that build on the ideological and technological foundations of web and that allow the creation and exchange of user generated content they suggested that social media can be classified as two components media related and social dimension the media related component involves how close to synchronous face to face communication different types of social media come and how well they reduce ambiguity and uncertainty the social dimension is based on goffman notion of self presentation whereby individuals interactions have the purpose of trying to control others impressions of them social media provides opportunities for users to generate share receive and comment on social content among multiusers through multisensory communication although the terms social media and social networking are often used interchangeably and have some overlaps they are not really the same social media functions as a communication channel that delivers a message which involves asking for something social networking is two way and direct communication that includes sharing of information between several parties social media can be classified in a number of ways to reflect the diverse range of social media platforms such as collaborative projects eg wikipedia content communities eg youtube social networking sites eg facebook and virtual game and social worlds eg world of warcraft second life the relationship between personality traits and engagement with social media has been reported gender is a factor in that extraverted women and men are equally likely to engage but emotional instability increases usage only for men age is also a factor in that extraversion is particularly important in younger users while openness to new experiences is particularly important in older users lenhart and colleagues explored various types of internet usage among teens and young adults in the united states between and during this time social networking sites experienced the biggest rise an average of around and the key shift in use came at age years with almost double the number of teens and years old using them as those years and over compared with social media is changing the nature and speed of health care interaction between individuals and health organizations the general public patients and health professionals are using social media to communicate about health issues in the united states of adults search online and use social media such as facebook for health information social media adoption rates vary in europe for example the percentage of german hospitals using social networks is in single figures whereas approximately of norwegian and swedish hospitals are using linkedin and of norwegian hospitals use facebook for health communication recent uk statistics reported facebook as the fourth most popular source of health information there have been many applications of social media within health contexts ranging from the world health organization using twitter during the influenza a pandemic with more than followers to medical practices and health professionals obtaining information to inform their clinical practice to explore the diversity in form and function of different social media platforms keitzmann and colleagues presented the social media ecology a honeycomb framework of seven building blocks that are configured by different social media platforms and have different implications for organizations such as health care providers in developing their model they have drawn on butterfield morville webb and smith the building blocks are identity the extent to which users reveal themselves conversations the extent to which users communicate with each other sharing the extent to which users exchange distribute and receive content presence the extent to which users know if others are available relationships the extent to which users relate to each other reputation the extent to which users know the social standing of others and content and groups the extent to which users are ordered or form communities thus organizations including health care providers need to recognize and understand the social media landscape where the conversations about them are already being held and develop their own strategies where suitable similarly mangold and faulds highlighted that social media is changing the relationship between producers and consumers of a message this suggests that health care providers may need to take a certain degree of control over online health communication to maintain validity and reliability in this paper social media for health communication refers to the general public patients and health professionals communicating about health issues using social media platforms such as facebook and twitter currently there is a lack of information about the uses benefits and limitations of social media for health communication among the general public patients and health professionals from primary research the objective of this paper was to review the current published literature to identify the uses benefits and limitations of social media for health communication among the general public patients and health professionals and to identify current gaps in the literature to provide recommendations for future health communication research this is important in order to establish whether social media improves health communication practices http www jmir org j med internet res vol iss p fo page number not for citation purposes renderx moorhead journal of medical internet research et al xsl methods and between the general public and or patients and or health professionals about health issues using social media this review paper followed the prisma guidelines and used a systematic approach to retrieve the relevant research studies the review included all study designs in order to identify the best evidence available to address the research objective the literature search was conducted on february using the following electronic databases csa illumina cochrane library communication abstracts ebsco host cinahl isi web of knowledge web of science ovidsp embase ovidsp medline ovidsp psycinfo and pubmeb central the searches were performed using the following defined search terms social media or social network or social networking or web or facebook or twitter or myspace and health from the above database searches hits were identified manual searches were conducted in the journal of medical internet research january to february where papers were identified thus papers were identified in total the papers titles and abstracts were screened for relevance duplication and the selection criteria the inclusion criteria were primary focus on all communication interactions within including the uses and or benefits and or limitations of social media for health communication original research studies published between january and february and all study designs the exclusion criteria were studies not in english literature reviews dissertation theses review papers reports conference papers or abstracts letters to the editor commentaries and feature articles studies only on web ie traditional internet use and studies with a primary marketing or advertising focus in total original research studies that included the use and or benefits and or limitations of social media for health communication among the general public patients and health professionals were selected for this review see figure excluded studies and the reasons for exclusion are listed in multimedia appendix two researchers am lh independently reviewed and evaluated the studies and reached consensus on the inclusion for the analysis the interrater reliability between them was indicating strong agreement any discrepancies were discussed with reference to the research objective until consensus was reached figure prisma flow diagram illustrating the study selection procedure http www jmir org j med internet res vol iss p fo page number not for citation purposes renderx moorhead journal of medical internet research et al xsl results twitter and youtube the full list is provided in table the study samples included blogs forum discussions in which the the selected studies are summarized by study design social media tool application study purpose participants sample and sample size measurement tools results conclusion and use of social media in multimedia appendix the diverse studies included the use of a range of social media tools applications the most reported being facebook blogs participants were the general public patients and or health professionals multimedia appendix there was a wide range of health topics but the most frequently reported on were sexual health diabetes flu and mental health issues such as stress or depression table social media tools applications within the studies a facebook n farmer et al ahmed et al greene et al bender et al egan moreno egan moreno frimmings et al gajara et al garcia romero et al jent et al kukreja et al lord et al sajadi goldman blogs n adams kovic et al lagu et al tan denecke nedjl keelan et al kim adams clauson et al hu sundar sanford shah robinson marcus et al twitter n chew eysenbach scanfeld et al heavillin et al kukreja et al sajadi goldman salathe khandelwal signorini et al turner mcgrievy tate youtube n freeman chapman fernandez luque et al lo et al tian chou et al sajadi goldman fernandez luque et al myspace n moreno et al moreno et al moreno et al versteeg et al ralph et al patientslikeme n frost et al wicks et al doing harris zeng treitler frost et al wikipedia n clauson et al morturu liu rajagopalan et al wiki n denecke nedjl adams quitnet online smoking cessa cobb et al selby et al tion support group n physician rating website not lagu kadry et al specified n second life n beard et al daily strength n morturu liu arboantwoord n rhebergen et al social media tool not specified chou et al jennings et al takahashi et al avery et al colineau n paris corley et al ding zhang hwang et al kim kwon kontos et al lariscy et al orizio et al rice et al adrie et al baptist et al bosslett et al dowdell et al friedman et al hanson et al kishimoto fukushmima lariscy et al liang scammon o dea campbell omurtag et al selkie et al setoyama et al shrank et al veinot et al weitzman et al young rice o grady et al web application not speci scotch et al timpka et al hughes et al lupianez villanueva et al fied n moen et al nordqvist et al ekberg et al nordfeldt et al lau usher et al van uden kraan a some studies included more than one social media tool application methodological quality of studies from the searches between january and february the selected studies in this review were published from to with the vast majority in the last years table from the available methodology of bias tools quality scales the downs and black instrument has been previously identified as a recommended tool to evaluate the quality of both quantitative randomized and nonrandomized studies as there are no standard accepted quality scales for studies of proportions only quantitative studies including mixed methods were evaluated using this downs and black instrument the maximum total score that could be achieved was but the scores of the studies in this review ranged from to overall the studies scored low using this scale as they were mainly exploratory and descriptive with three intervention studies and one randomized controlled trial rct from the studies were applied http www jmir org j med internet res vol iss p fo page number not for citation purposes renderx moorhead journal of medical internet research et al xsl quantitative qualitative including studies with content analysis presenting data with descriptive statistics and mixed methods both quantitative and quantitative these studies are presented by methodology in table methodological bias of the selected studies using the downs and black instrument is presented in multimedia appendix characteristics profile of users accessing social media for health communication the characteristics of users of social media for health communication in the selected studies were diverse covering a range of different population groups the age of the social media users ranged from school children to older adults aged years and up but the majority of the reported ages were years some studies reported that there were more female than male users of social network sites a few studies found that social media users were disproportionately from lower income households studies within the united states reported that more social media users were african americans than nonhispanic whites chou et al concluded that the population is accessing social media regardless of education and race ethnicity uses of social media for health communication from the selected studies seven key uses of social media for health communication were identified for the general public patients and health professionals table social media provided health information on a range of conditions to the general public patients and health professionals this communication can provide answers to medical questions social media allows information to be presented in modes other than text and can bring health information to audiences with special needs for example videos can be used to supplement or replace text and can be useful when literacy is low a range of social media platforms can facilitate dialogue between patients and patients and patients and health professionals sites such as patientslikeme enable patients to engage in dialogue with each other and share health information and advice including information on treatment and medication youtube has been used by the general public to share health information on medications symptoms and diagnoses and by patients to share personal cancer stories blog sites create a space where individuals can access tailored resources and provide health professionals with an opportunity to share information with patients and members of the public facebook is being used by the general public patients carers and health professionals to share their experience of disease management exploration and diagnosis asthma groups are using myspace to share health information in particular personal stories and experiences social media can be used to collect data on patient experiences and opinions such as physician performance social media have been used for health promotion and health education and for delivering a health intervention by providing social support influence to promote smoking cessation and abstinence a study has shown that social media can reduce stigma about certain conditions such as epilepsy in addition there were some opportunities for health professionals to have online consultations benefits of social media for health communication six overarching benefits of social media for health communication were identified for the general public patients and health professionals table social media users have the potential to increase the number of interactions and thus are provided with more available shared and tailored information social media can generate more available health information as users create and share medical information online blog sites create a space where individuals can access tailored resources to deal with health issues social media can widen access to those who may not easily access health information via traditional methods such as younger people ethnic minorities and lower socioeconomic groups an important aspect of using social media for health communication is that it can provide valuable peer social and emotional support for the general public and patients for example social media can aid health behavior change such as smoking cessation and patientslikeme enables patients to communicate with other patients and share information about health issues colineau and paris reported that people used health related social networking sites to discuss sensitive issues and complex information with health professionals in public health surveillance social media can provide communication in real time and at relatively low cost social media can monitor public response to health issues track and monitor disease outbreak identify misinformation of health information identify target areas for intervention efforts and disseminate pertinent health information to targeted communities health professionals can aggregate data about patient experiences from blogs and monitor public reaction to health issues social media may have particular potential for risk communications as they can be used to disseminate personalized messages immediately thus making outreach more effective there is the potential that information on social media may contribute to health care policy making as medical blogs are frequently viewed by mainstream media http www jmir org j med internet res vol iss p fo page number not for citation purposes renderx moorhead journal of medical internet research et al xsl table list of studies by methodology quantitative qualitative or both n quantitative n qualitative n mixed methods n kovic et al freeman chapman a clauson et al chou et al moreno et al a timpka et al moreno et al adams hughes et al avery et al frost et al jennings et al chew eysenbach lagu et al a lupianez villanueva et al cobb et al scotch et al takahashi et al colineau paris tan hwang et al hu sundar beard et al ralph et al kim kwon denecke nedjl a selkie et al kontos et al farmer et al a o grady et al lariscy et al fernandez luque et al keelan et al a lo et al a rice et al kim wicks et al moen et al adrie et al moreno et al baptist et al nordqvist et al versteeg et al a bosslett et al dowdell et al adams ahmed et al a frimmings et al a garcia romero et al clauson et al hanson et al corley et al jent et al ding zhang kadry et al ekberg greene et al a kishimoto fukushmima lagu a kukreja et al a lau nordfeldt et al lord et al orizio et al morturu liu sanford o dea campbell scanfeld et al selby et al a omurtag et al a rajagopalan et al tian setoyama et al bender et al chou et al a signorini et al b turner mcgrievy tate doing harris zeng treitler usher et al egan moreno a van uden kraan egan moreno friedman et al a weitzman et al a young rice frost et al http www jmir org j med internet res vol iss p fo page number not for citation purposes renderx moorhead journal of medical internet research et al xsl quantitative n qualitative n mixed methods n fernandez luque et al gajaria et al rhebergen et al heavillin et al lariscy et al liang scammon sajadi goldman salthe khandelwal a shah robinson shrank et al a veinot et al marcus et al a qualitative study using content analysis with some findings reported as descriptive statistics b descriptive statistics limitations of using social media for health communication there were limitations of social media for health communication table the main recurring limitations of social media are quality concerns and the lack of reliability of the health information the authors of websites are often unidentifiable or there can be numerous authors or the line between producer and audience is blurred thus it is more difficult for individuals to discern the reliability of information found online regulations may not facilitate health professionals to communicate with patients online for example email is not an official medical record and could be vulnerable to security breaches policy reactions to address concerns include providing training in how to use and navigate social media technologies and validate accuracy of information found or bringing more credible sites into the mainstream and making them fully accessible the large volume of information available through social media and the possibility for inaccuracies posted on these sites presents challenges when validating information several studies highlighted concerns about privacy and confidentiality data security and the potential harms that emerge when personal data are indexed social media users are often unaware of the risks of disclosing personal information online and with communicating harmful or incorrect advice using social media as information is readily available there is the potential of information overload for the user the general public may not know how to correctly apply information found online to their personal health situation there is the potential that adverse health consequences can result from information found on social media sites for example pro smoking imagery in addition there may be negative health risk behaviors displayed online such as unsafe sexual behavior there is limited evidence that engaging in online communities positively impacts people health health professionals may not often use social media to communicate with their patients there is also the possibility that social media may act as a deterrent for patients from visiting health professionals table uses of social media for health communication among the general public patients and health professionals uses of social media for health communication social media user general public patients health professionals provide health information on a range of conditions provide answers to medical questions facilitate dialogue between patients to patients and patients and health professionals collect data on patient experiences and opinions used for health intervention health promotion and health education reduce stigma provide online consultations http www jmir org j med internet res vol iss p fo page number not for citation purposes renderx moorhead journal of medical internet research et al xsl table benefits of using social media for health communication for the general public patients and health professionals benefits of social media for health communication social media user general public patients health professionals increase interactions with others more available shared and tailored information increase accessibility widening access peer social emotional support public health surveillance potential to influence health policy table limitations of social media for health communication among the general public patients and health professionals limitations of social media for health communication social media user general public patients health professionals lack of reliability quality concerns lack of confidentiality privacy often unaware of the risks of disclosing personal information online risks associated with communicating harmful or incorrect advice using social media information overload not sure how to correctly apply information found online to their personal health situation certain social media technologies may be more effective in behavior change than others adverse health consequences negative health behaviors social media may act as a deterrent for patients from visiting health professionals currently may not often use social media to communicate to patients discussion the research studies in this review provided evidence that social media most reported applications were facebook blogs twitter and youtube can create a space to share comment and discuss health information on a diverse range of health issues such as sexual health diabetes flu and mental health issues social media attracts a large number of users thus creating a platform for mass health communication with identified uses benefits and limitations for the general public patients and health professionals uses of social media for health communication the main uses of social media focus on increasing interactions with others and facilitating sharing and obtaining health messages the general public mainly use social media for themselves family members and or friends to obtain and share information on a wide range of health issues 103 patients can share their experiences through discussion forums chat rooms and instant messaging or online consultation with a qualified clinician some health professionals were reported to use social media to collect data on patients and to communicate with patients using online consultations however this latest use is limited recent research reported that female health professionals in quebec canada believed that web may be a useful mechanism for knowledge transfer but is limited due to their lack of time and technological skills perhaps in light of kaplan and haenlein classifications of social media further work on improving the social presence the closeness to synchronous face to face communication of such online consultations would contribute to improving communication between health professionals and patients another recent study applied social network analysis to understand the knowledge sharing behavior of practitioners in a clinical online discussion forum and found that although their number is limited interprofessional and interinstitutional ties are strong this relates to gilbert and karahalios social tie analysis and suggests that development of mechanisms that evaluate tie strength in social media that in turn impact on its functionality may be useful for health communication further technological advances will provide more opportunities http www jmir org j med internet res vol iss p fo page number not for citation purposes renderx moorhead journal of medical internet research et al xsl to use social media in health care in the future especially between patients and patients and also health professionals and patients however both patients and health professionals may require training to fully maximize the uses of using social media in health care benefits of social media for health communication numerous benefits of using social media for health communication were reported for the general public patients and health professionals a major benefit of social media for health communication is the accessibility and widening access of health information to various population groups regardless of age education race or ethnicity and locality compared to traditional communication methods while these changing patterns may lessen health disparities traditional inequalities in overall internet access remain furthermore variation in social media engagement according to personality traits age and gender suggests the need for ongoing scrutiny regarding equality of access and effectiveness for different users social media can be used to provide a valuable and useful source of peer social and emotional support to individuals including those with various conditions illnesses hwang and colleagues reported that encouragement motivation and shared experience were important social support features of social media sites social media allows users to generate peer to peer discussion in a way not enabled by traditional websites however this may challenge expectations relationships quality and consistent health care practice as moen et al explain current patterns of collaboration tend to produce an asymmetric patient health care provider relationship this highlights a strong need for health providers to maintain a role within social media health communication that is not simply the same as that of patient and general public users keitzmann et al have suggested that organizations need to recognize and understand the social media landscape and where the conversations about them are already being held cognize develop strategies that are suitable work out how often and when they should enter into conversations and be aware of what others are doing and act accordingly this review highlights clearly that social media has benefits for health communication but the long term effects are not known as the use of social media is expected to increase in the future there may be further benefits of using social media in health care it is not yet known how effective social media applications are in health communications which warrants further research limitations of social media for health communication social media tools remain informal unregulated mechanisms for information collection sharing and promotion so the information is of varying quality and consistency similar issues exist with traditional internet sites but these issues are being heightened by the interactive nature of social media which allows lay users to upload information regardless of quality reliability may be monitored by responsible bodies using automated processes employed to signal when content has been significantly edited and progress is being made in automated quality detection further work to improve the media richness of social media for health communication that is how they may reduce ambiguity and uncertainty would be valuable in addition combining more resources in one site could improve reliability of information as patients interact and share links they could compare numerous social media sites and triangulate information to help them discern correct from incorrect information despite concerns information found on some websites is reported to be generally factually accurate a further limitation is that postings can be a permanent record and be viewed by an increasing audience and perhaps users are unaware of the potential size of the audience base regulatory and security issues must be addressed to broach a way forward for best practice that allows the benefits of social media to be utilized yet still protects patients privacy and to therefore improve use of these media in routine clinical care this is a public policy issue and is already being contested in the united states public education is required for the general public patients and health professionals to make them more aware of the nature of using social media consideration of the variation in social media engagement according to personality traits age and gender will be valuable in tailoring education to meet the needs of population groups gaps in the research literature and recommendations of research into social media for health communication this literature review has shown that the general public patients and health professionals use social media in health care for various purposes with numerous benefits and limitations the current research methodological scoring was low this was mainly due to the fact that the vast majority of the studies in this review were exploratory and descriptive to date there is very limited evidence from rcts and longitudinal studies to more fully determine the role of social media for health communication further research with larger sample sizes and more robust methodologies are required based on this review several gaps in the literature have been identified that need to be addressed the impact of social media for health communication in specific population groups such as minority groups patients groups culture differences the relative effectiveness of different applications of social media for health communication the longer term impact on the effectiveness of social media for health communication the most suitable mechanisms to monitor and enhance the quality and reliability of health communication using social media the risks arising from sharing information online the consequences for confidentiality and privacy and the most suitable mechanisms for effectively educating users in the maintenance of their confidentiality and privacy the full potential of social media in effectively supporting the patient health professional relationship the impact of peer to peer support for the general public patients and health professionals to enhance their interpersonal communication http www jmir org j med internet res vol iss p fo page number not for citation purposes renderx moorhead journal of medical internet research et al xsl the impact of social media on behavior change for healthy lifestyles to address these gaps in the literature the key recommendations for future health communication research focus on robust and comprehensive evaluation and review using a range of methodologies the research priorities are highlighted below to determine the impact of social media for health communication in specific population groups with large sample sizes representation of population groups to determine the relative effectiveness of different social media applications for health communication using rcts to determine the longer term impact on the effectiveness of social media for health communication using longitudinal studies to explore potential mechanisms for monitoring and enhancing the quality and reliability of health communication using social media to investigate the risks arising from sharing information online and the consequences for confidentiality and privacy coupled with developing the most suitable mechanisms to effectively educate users in the maintenance of their confidentiality and privacy to determine how social media can be effectively used to support the patient health professional relationship to determine the impact of peer to peer support for the general public patients and health professionals to enhance their interpersonal communication to explore the potential for social media to lead to behavior change for healthy lifestyles to inform health communication practice conclusions social media brings a new dimension to health care offering a platform used by the public patients and health professionals to communicate about health issues with the possibility of potentially improving health outcomes although there are benefits to using social media for health communication the information needs to be monitored for quality and reliability and the users confidentiality and privacy need to be maintained social media is a powerful tool that offers collaboration between users and a social interaction mechanism for a range of individuals with increasing use of social media there will be further opportunities in health care research into the application of social media for health communication purposes is an expanding area because increasing general use of social media necessitates that health communication researchers match the pace of development further robust research is required to establish whether social media improves health communication practices in both the short and long terms authors contributions dr anne moorhead developed the concept of this paper and selected and evaluated papers and led this manuscript laura harrison conducted the searches for studies and dr anne moorhead and laura harrison evaluated the papers all authors evaluated the studies and contributed to this manuscript conflicts of interest none declared multimedia appendix excluded studies purpose implementations of health information technologies are notoriously difficult which is due to a range of inter related technical social and organizational factors that need to be considered in the light of an apparent lack of empirically based integrated accounts surrounding these issues this interpretative review aims to provide an overview and extract potentially generalizable findings across settings methods we conducted a systematic search and critique of the empirical literature published between and in doing so we searched a range of medical databases to identify review papers that related to the implementation and adoption of ehealth applications in organizational settings we qualitatively synthesized this literature extracting data relating to technologies contexts stakeholders and their inter relationships results from a total body of systematic reviews we identified systematic reviews encompassing organizational issues surrounding health information technology implemen tations by and large the evidence indicates that there are a range of technical social and organizational considerations that need to be deliberated when attempting to ensure that technological innovations are useful for both individuals and organizational processes however these dimensions are inter related requiring a careful balancing act of strategic implementation decisions in order to ensure that unintended consequences resulting from technology introduction do not pose a threat to patients conclusions organizational issues surrounding technology implementations in healthcare settings are crucially important but have as yet not received adequate research attention this may in part be due to the subjective nature of factors but also due to a lack of coor dinated efforts toward more theoretically informed work our findings may be used as the basis for the development of best practice guidelines in this area elsevier ireland ltd all rights reserved corresponding author tel e mail address kathrin beyer ed ac uk k cresswell see front matter elsevier ireland ltd all rights reserved http dx doi org j ijmedinf i n t e r n a t i o n a l j o u r n a l o f m e d i c a l i n f o r m a t i c j ourna l h omepage www ijmijournal com i n t e r n a t i o n a l j o u r n a l o f m e d i c a l i n f o r m a t i c contents introduction methods results technical characteristics social aspects organizational factors ensuring fit between these technical social and organizational dimensions a chronological perspective a range of inter related factors over time discussion conclusions authors contributions conflicts of interests funding acknowledgements references introduction drawing on health information technology hit innova tions to improve the quality and safety of care is now firmly established as a priority area throughout much of the economically developed world however healthcare is when compared to other industries slow to adopt technol ogy underlying this is a complex web of inter related social and technical issues situated within a wider organiza tional environment there is increasing appreciation that introducing technology within complex organizational systems such as healthcare is not a straightforward lin ear process rather it is dynamic in nature involving often various cycles of iteration as technological social and orga nizational dimensions gradually align or not over time organizational dimensions surrounding hit introduction have been the subject of much empirical activity but progress is hampered by the use of inter related terms that are often used synonymously consequently navigating and interpre ting the surrounding body of evidence is somewhat difficult resulting in a lack of integrated accounts of the most important factors associated with implementation existing concepts include adoption deployment diffusion implemen tation infusion integration normalization and routinization box in essence these all relate to the processes by which innovations are introduced and then incorporated or not into routine care by professionals and or patients within organiza tional settings keeping in mind that technological innovation in health care also requires expertise in technical considerations and clinical practice the study of organizational dimensions in relation to hit innovations is not a clearly defined area of interest rather it is a problem based approach centering on the interaction between organizations or more accu rately the people working within these organizations and with technology the field may therefore encompass human factors considerations but can also include issues that go beyond the direct human computer interface such as strate gies employed to introduce systems and the way these are adopted by various stakeholders within organizational box examples of concepts surrounding organizational considerations in hit innovations adoption construed as the acceptance and incorpo ration of hit applications into everyday practice deployment the process of putting technology into use in the organization diffusion the study of how why and at what rate new ideas and technology spread through organizations implementation the consideration and the intro duction of hit applications procurement decisions and development pathways can in some cases impact on implementation considerations infusion the degree of comprehensiveness or sophistication of use of an innovation and the degree to which it is embedded within an organization integration the process by which technology becomes incorporated within organizational practices normalization the process by which an innovation becomes routine routinization the process by which using an inno vation becomes part of regular organizational practice settings similarly social aspects such as individual attitudes and behaviors of groups are integral to organizational issues we summarize the existing bodies of knowledge that may be potentially useful in contributing to the understanding of organizational issues in the context of hit implementation and usage in box perhaps as a result of these different bodies of knowledge there are also a range of theoretical approaches that can help to conceptualize the interaction between technology humans and the organizations in which they function some of these are outlined in box our list in box is by no means exhaustive but our inten tion is to illustrate the range of different existing theoretical lenses surrounding the introduction of hit overall there is no overarching conceptual framework in relation to the implementation and adoption of hit innovations the main tensions of various theoretical considerations seem to be a box examples of bodies of knowledge surrounding organizational issues in hit innovation human factors systems ergonomics all embracing terms that cover the science of understanding the properties of human capability the application of this understanding to the design and development of inno vations and the art of ensuring successful application of human factors engineering to information technol ogy organizational occupational social psychology a subset of psychology that is concerned with the application of psychological theories research meth ods and intervention strategies to workplace issues relevant topics include personnel psychology e g behavior and attitudes changes in what jobs entail working patterns and effects on the individual moti vation and leadership employee selection training and development organizational development and guided change organizational behavior and work and family issues management and in particular organizational change management a structured approach to change in individuals teams organizations and societies that enables the transition from a current state to a desired future state it often focuses on increasing organiza tional effectiveness and on identifying barriers and facilitators to reaching a desired future state information systems an academic discipline that is concerned with the uses of information and informa tion technology in organizations and more generally society this area emerged from systems theory which assumes that the world consists of complex sys tems which are inter related with each other and the world at large the defining feature here is that a sys tem is viewed as being more than the sum of its parts focus on relatively linear stages and integration of technology over time with some models focusing on exploring one partic ular aspect of the lifecycle in detail a focus on individual adopters in isolation a focus on complexity and unpre dictability characterizing the change process and a mixture of the above with models trying to be as inclusive as possible which in turn makes them less specific with the importance of the wider organizational consider ations associated with hit deployment in mind we conducted a secondary review of data obtained during related work the rationale for focusing on this particular topic of interest is an apparent lack of integrated accounts surround ing the issue as outlined above this may be due to social and organizational issues being experienced subjectively and in different ways by different actors but hampers obtain ing insights into potentially generalizable findings across settings i n t e r n a t i o n a l j o u r n a l o f m e d i c a l i n f o r m a t i c box examples of theoretical approaches conceptual izing the interaction between technology humans and organizations diffusion of innovations these are approaches that focus on how innovations spread in and across organizations over time normalization process theory this describes how complex interventions in healthcare are routinely incorporated into the day to day work of health care staff or normalized the model highlights the importance of social processes and the organizational context in shaping outcomes sensemaking this approach assumes that indi viduals in organizations discover meanings of the status quo frequently as a result of some kind of change often by transforming situations into words expressed in language or texts and then displaying a resulting action as a consequence of their interpreta tions the underlying assumption is that organizations are not existing entities as such but are talked into action or produced by sensemaking activities and also the other way around the very way in which they are talked about defines their existence social shaping of technology this approach high lights the importance of wider macro environmental factors in influencing technology and its implemen tation into organizations it emerged as a response to studies focusing on the social consequences of tech nology implementation and in doing so increasingly shifts the focus to viewing technology itself as being shaped by social processes sociotechnical changing these approaches conceptualize change as a non linear unpredictable and context dependent process they assume that both social and technical dimensions shape each other over time in a complex and itself evolving environ ment technology acceptance model this assumes that individual adoption usage of a system is deter mined by the attitude toward use perceived useful ness and perceived ease of use of the application the notion of fit these models emphasize that one not only needs to consider social technolog ical and work process factors in isolation but also the extent to which these align with each other the better the fit the more likely the implementation is assumed to be successful and the higher levels of adoption amongst users are likely to be methods this work is a subset analysis of a recently completed sys tematic review of the literature examining the effectiveness of ehealth applications to improve the quality and safety of healthcare as part of this work and in addition to investi gating clinical outcomes we examined evidence relating to i n t e r n a t i o n a l j o u r n a l o f m e d i c a l i n f o r m a t i c ways of promoting the effective development deployment and routine use of ehealth applications in healthcare sett ings in doing so we searched for systematic reviews relating to organizational issues in hit innovations published between and we developed a comprehensive search strategy and an associated list of search terms drawing on medical subject headings mesh and free text searches this involved combining terms relating to ehealth applications imple mented in organizational settings such as computerized decision support electronic prescribing electronic health records with organizational and implementation related terms such as those outlined in box we examined papers published in medline embase the cochrane database of systematic reviews database of abstracts of reviews of effects the cochrane central regis ter of controlled trials the cochrane methodology register the health technology assessment database google lilacs indmed pakmedinet the national research register clinical trials gov current controlled trials and the national health service nhs economic evaluation database papers were scored by two independent reviewers applying relevant methodological filters to identify systematic reviews this involved initially screening abstracts and subse quently potentially relevant full text papers for empirical work associated with ehealth applications and organizational implementation and adoption processes quality assessments of included studies were conducted by two independent reviewers drawing on relevant instruments which we adapted for ehealth systematic reviews as the overall body of literature identified was too diverse to make any quantitative synthesis of the literature meaningful we chose to qualitatively synthesize retrieved studies drawing on relevant conceptual work to guide this narrative synthesis in doing so we extracted data relating to specific care sett ings and contexts skills knowledge experience attitudes and values of individuals clinicians healthcare managers and patients the characteristics of tools such as adap tiveness and environmental factors tasks goals and their inter relationships results overall our initial searches generated systematic reviews investigating ehealth applications applying our inclusion criteria we found systematic reviews focusing on organiza tional issues surrounding the implementation and adoption of hit and two systematic reviews which focused more generally on related questions of innovation in healthcare settings fig depicts a flow diagram of the screening and selection process and table summarizes the main find ings of individual reviews overall the evidence from systematic reviews draws atten tion to the importance of a number of inter related technical social and organizational factors that can help describe and explain potential underlying causes for success and failure or the perceptions of these acknowledging that these factors are inter related we have organized our results along these dimensions to illustrate the particularities of each cochra ne medli ne embase person al librar y n n data bases n n titles identifi ed for revi ew n titles and abstracts revi ewed removed dupli cates n where unclear full articles obtained for scree ning n rejecte d n include d systematic revi ews n update d sear ches to march n included systematic reviews n update d sear ches april october n rejecte d n systematic revi ews relati ng to ehealth systematic revi ews relati ng to organ ization al iss ues surr ounding t he implementati on a nd adop tion of hit four published in three in three in one in o ne in one in two from human fac tors syste ms ergonomics t wo from organ ization al occ upational social psycho logy sev en from managem ent stu dies t wo from inform ation systems fig flow diagram of the screening and selection process adapted from cresswell k majeed a bates dw sheikh a computerized decision support systems for healthcare professionals an interpretative review in press informatics in primary care before moving toward examining inter relationships between them technical characteristics the literature consistently indicates that the majority of end users are not averse to technology per se however they are likely to resist use of systems that are viewed as inadequate or worse still as interfering with their values aspirations and roles a key feature of technology should therefore be as demon strated by a review of studies of adoption by gagnon et al that it is useful and offers relative advantages over existing practices this is most commonly conceptualized in relation to speed a new system needs to be at least as quick as the system that was previously operational i e not significantly slow down users in their everyday work other features of technology that are repeatedly found to facilitate adoption include early demonstrable benefits perceived ease of use costs the extent to which a system is table summary of main findings from included studies author and year key findings alexander and staggers reviewed the literature for human factors related research in nursing found the following to be important effectiveness of user interfaces e g simple easy to navigate reducing cognitive loads graphical heuristic compliance information density information presented in line with importance including users in development and design effective integration with existing work practices impact of system on user workload customizability of systems in line with user needs flexibility of systems ease of learning how to use a system boonstra and broekhuis systematic literature review to identify barriers to electronic medical record emr adoption amongst primary care physicians identified the following categories financial this includes high perceived start up costs high on going costs uncertainty surrounding return of investment lack of financial resources technical includes a lack of computer skills amongst users lack of training and support complexity of the system resulting in issues with usability perceived limitations of the system e g it may not address all needs or become obsolete lack of customizability resulting in a system that does not meet the needs of users lack of reliability e g crashes interconnectivity with existing systems also includes fear that functioning existing systems may need to be replaced lack of hardware to support emr time slowing workflow and increasing time time to select and implement a system time to learn how to use a system time needed to enter data into a system increase in time spent on care due to disruptions in workload time to convert existing records into an electronic format psychological lack of belief that emrs improve patient care fear that emrs may lead to a loss of professional control over patient information social uncertainty about credibility and reliability of vendor perceived lack of support from other parties e g policy makers other organizations perceived impact on dynamics of doctor patient relationship perceived lack of support from other staff lack of support from management legal fear that data may be accessible to unauthorized third parties lack of standards and guidance organizational size larger organizations find it easier to implement emrs may be due to better resources type change process organizational culture needs to be supportive lack of incentives i e benefits to individual clinicians lack of participation from other staff lack of leadership this includes the role of champions factors are inter related some factors organizational and change process are mediating others gagnon et al identified nine randomized controlled trials investigating the effectiveness of interventions increasing the use of clinical information retrieval technologies by healthcare professionals different studies investigated the following types of interventions educational meetings educational materials educational outreach audit and feedback multifaceted and financial one study showed that the introduction of user fees significantly reduced the number of medline searches mixed evidence three studies indicated a positive impact of interventions on use and four did not show significant effects tendency to improve searching skills and use of electronic databases overall educational meetings were the only type of intervention reporting consistent positive effects on adoption gagnon et al review investigating facilitators and barriers to hit implementation variety of inter related technological human and organizational factors play a role factors may belong to more than one category and overlap facilitators perceived benefits system usefulness ease of use compatibility with tasks and work processes user training and support champions user involvement in design strategy organizational support and management barriers design technical concerns familiarity with technology time consuming nature of use or increased workload lack of compatibility with existing work practices interoperability concerns about validity of resources cost legal issues patient health professional interaction applicability to patients attitude of colleagues toward technology role boundaries and changes in tasks material resources greenhalgh et al the authors drew on several research traditions to develop a multi faceted model of the socio cultural dimensions of organizational change in healthcare organizations they divided existing research traditions into the following three broad categories early diffusion research including rural sociology medical sociology communication studies and marketing later diffusion research including development studies health promotion evidence based medicine research from the organization and management literature including studies of the structural determinants of organizational innovativeness studies of organizational process context and culture inter organizational studies knowledge based approaches to innovation in organizations narrative organizational studies complexity studies and organizational psychology table continued author and year key findings they then further considered factors that can facilitate the successful implementation of innovations and proposed a framework of socio cultural dimensions that need to be considered in this context this framework suggests several key aspects which include the nature of the innovation as perceived by end users strategies by which potential adopters can be targeted the role of effective communication in introducing innovations the importance and nature of both organizational and environmental context how implementation is done most effectively and change is sustained the role of external agencies in influencing successful implementation gruber et al systematic review investigating processes and outcomes associated with hit implementation categorized important outcomes on different levels system user management and patient outcomes system outcomes user friendly meaningful screens and lists system performance functionality integration with other systems accessibility decision tools data availability user outcomes acceptance motivation to use system confidence and self efficacy satisfaction support data quality and integrity sharing of information to improve communication efficiency and patient care management outcomes data for secondary uses and to facilitate decision making quality control compliance of staff with standards leadership efficiency of care processes and operational processes patient outcomes satisfaction regarding relationship with provider improved communication important overall factors attention to clinical context of use implementation go live maintenance phases important end user support gurses and xiao systematic review investigating information tools that support information exchange and communication through multidisciplinary rounds divide existing tools into patient centric information tools decision support tools process oriented tools overall found that information tools improved situational awareness of providers efficiency and length of hospital stay identify a range of needs of clinicians using tools clinical information needs e g results decision information needs e g decision tools social and organizational information needs e g protocols authors suggest that positive impact may be improved by using process oriented information tools i e those that help information organization communication and work management identify a range of technical features that are important summary and display of up to date information supporting different users use of mobile technologies to increase flexibility checklists supporting informal communication keshavjee et al canadian review of what makes emr implementation successful developed a framework based on review of qualitative implementation literature followed principles of systematic review included qualitative articles high incidence of failure in emr implementation there are several existing models that describe factors for successful implementation but none of these is inclusive enough technology is implemented over a certain amount of time people processes and technology are involved strong leadership is important stakeholder communication and engagement is important implementation is a dynamic and evolving learning process usability is important framework divides implementation into three time periods pre implementation implementation and post implementation any factors identified can be in either one some or all of the phases identified factors can have relationship with each other factors divided into categories of people process and technology table continued author and year key findings factors important in the pre implementation phase governance investment in implementation from senior management includes vision and organizational mission allocation of resources leadership implementation team needs to consist of experienced project manager and champion representing the users role of implementation team is to be a link between management and users characteristics of a good manager include effective planning and communication participation of stakeholders conflict resolution motivation of users needs to be realistic involving stakeholders relates to organizational readiness for change this needs to be examined and involves addressing user concerns and needs communicating how the new solution can address and fit in with these needs communicating the vision an understanding that change is difficult but can be overcome address barriers communicate benefits of the new solution involve a variety of different stakeholders to reduce resistance and increase acceptance system needs to perform to fulfill users needs users are important for success of implementation choosing the software software needs to fit in with what the organization requires need to assess cost and usability and supplier related issues integration best if system integrates effectively with existing systems need to determine how paper records will be entered issues surrounding standardization usability need to consider both hardware and software usability both need to fit in with existing work processes emr systems are often complex resulting in reduced usability this requires extra time to do some actions and involves intense learning on part of the user usability can be improved by flexible technology e g tablet computers factors important in the implementation phase the new system needs to fit with clinical workflows this requires a thorough understanding of existing work processes needs to be iterative training needs to be hands on and close to go live needs to be on going and tailored to different pre existing levels of experience among users need to have good working relationship with supplier supplier needs to incorporate changes suggested by users to improve usability ideally have staff on site helpdesk support robust contracts necessary dedicated person in organization often useful to communicate with supplier can utilize influence of super users local champion support on going support during and after implementation so that arising problems can be dealt with effectively and do not compromise care communication and feedback regular meetings with users allowing users to voice concerns evaluation of the implementation dealing with problems need to be flexible and recognize that technology and organization evolve together need to address issues surrounding confidentiality and security consider the relationship between these factors and ease of user access to emr need to minimize risks and address both patient and user concerns through appropriate systems and communication factors important in the post implementation phase recognition that implementation is on going is important technical support if things go wrong on going training of users and over a prolonged period of time to increase acceptance on going input into system usability by users to facilitate adoption incentives for on going use of the system highlight improvements in care and demonstrate to users ludwick and doucette canadian review of emr adoption in primary care looking at articles from a range of countries with a view to identify lessons learned from emr implementations but examined evidence from a range of care settings found that focus of articles was on sociotechnical factors similar factors seemed important across care settings also included grey literature government and professional bodies literature found that sociotechnical factors were most important for successful implementation important that the new system fits in with existing organizational goals and practices barriers were identified to be perceived negative impact on patient safety privacy impact on healthcare professional patient relationship reservations from users implementation time needed cost issues mitigating factors good project management and leadership training standardization management support and clinical champions were success factors design and implementation need to be informed by users focus on different user perspectives and needs important barriers user concerns e g changes in work practices may lead to resistance to adoption resistance especially strong if users perceive change to be imposed on them concerns need to be addressed to facilitate implementation implementation management important can either implement using big bang or incremental approach incremental approach better for complex organizations need to align system so it fits with existing work processes users working with suppliers to design systems accordingly important users previous experience with computers important and can influence adoption the higher the usability of the system e g intuitiveness the higher adoption rates training important affects adoption important that length is adequate support after implementation is adequate and the timing is adequate computers can affect the doctor patient relationship as the consultation is done through the computer and conversation flows are interrupted found decrease in productivity immediately after implementation found that once users get used to system there can be productivity improvements but mixed results into whether system results in time savings no research on exactly how long it takes for organization to get used to system cost concerns common barrier to adoption returns of investment important maximum benefits only if connected systems across healthcare community financial support from government can facilitate adoption table continued author and year key findings found that with adoption there are improvements in patient safety but there can be initial adverse impact on patient safety due to sociotechnical issues training and strong management can mitigate privacy concerns but no study actually assessed impact of systems on privacy mair et al examined barriers and facilitators to the implementation of hit and found technology design factors health professional interactions and organizational factors to be important key barriers include inadequate information management inadequate inter agency cooperation intrusive technology rigidity of system cost lack of testing key facilitators include positive inter agency co operation flexibility ease of use organizational willingness ability to order information other factors health professional patient relationships and security robert et al review of organizational factors and processes affecting the implementation of hit innovation attributes actors and organizational contexts are inter related and have important consequences for implementation and adoption adoption and implementation needs to be viewed as a process consisting of both formal and informal components importance of interactions between groups in organizations organizational history and decision making power relationships professionalism influence of social groups importance of key individuals relationship between organization and environment in which it is situated overall both pre existing conditions as well as actions to facilitate implementation are important lack of theoretical grounding in existing evidence base yarbrough and smith systematic literature review on technology acceptance by physicians focus on the technology acceptance model tam in healthcare resistance by users is the greatest barrier to emr implementation other barriers lack of financial incentives to use lack of empirical evidence of effectiveness concerns surrounding confidentiality and security argue that tam needs to be expanded to include organizational system specific and healthcare specific factors barriers to acceptance disruption of existing work practices systems often slow down care processes and are often viewed as less efficient especially an increase of physician time on administrative tasks seems to be a barrier cost of increased time spend by physicians lack of empirical evidence supporting the effectiveness of systems in relation to cost and quality of care organizational issues organizational support including training and resources important size of organization local policies demographic and individual factors such as level of experience with technology and salary status and value placed on relationship with patient existing norms needs to be collaborative and emphasis on teamwork system issues reliability and dependability flexibility important that physicians can adapt technology to suit needs and fit circumstances some barriers are important only for certain settings and in certain groups of users e g in some healthcare settings cost may be a barrier to individual adoption e g when providers have to pay for the system but in some instances it may not yusof et al identified inter related critical adoption factors relating to hit technology human organization information quality system use and organizational environment success factors greater time efficiency for users system flexibility and information accessibility continuous user training and support firm leadership ease of use system usefulness system flexibility technical support response time turnaround time information accessibility information relevance clarity of system purpose user involvement user training user perception user skills knowledge user roles clinical process champion medical sponsorship internal communication barrier hierarchical structure of the organization i n t e r n a t i o n a l j o u r n a l o f m e d i c a l i n f o r m a t i c interoperable with existing technology in the organization and fits in with existing organizational processes and the extent to which it can be trialed given the constantly changing nature leadership and priorities of complex systems such as health service provision it is also important that the technol ogy has the potential to be adapted or customized to support changing needs and individual organizational contexts of use social aspects a number of social aspects surrounding technological innova tion are highlighted throughout the literature as increasing the chances of successful implementation these include infor mation technology literacy and general competencies of users personal and peer attitudes toward an innovation including colleagues and patients financial consid erations and the extent to which the technology supports inter professional roles and working conversely technologies which inadvertently undermine perceived social standing or professional autonomy are likely to be resisted by users on going involvement of key stakeholders including man agement developers and users at the conception and design stages and an opportunity for field testing of early prototypes and open communication channels can help to ensure that systems are likely to be valued and used by professionals and patients organizational factors larger more complex health systems have proven particularly receptive to the introduction of technological innovation this is in part because of their large human organizational and financial capital but also their complex management structures with great degrees of hierarchy as such avail able evidence highlights the importance of senior leadership and lead professional or champion support resulting in greater ownership surrounding implementation activities these champions frequently need to act as boundary spanners bridging the gulfs that often exist between and within information technology staff manage ment and clinicians they can also facilitate the re design of workflows provide adequate training and support to users and highlight problematic issues the initial implementation can be disruptive for organi zational functioning and individual ways of working as staff attempt to make sense of new workflows making additional time available to individual users for example by proactively reducing workloads during this time period and or introducing the technology when there are no other major upheavals in the organization can help to mitigate the risk of unintended consequences strong organizational leadership and management are necessary to ensure strategic consistency i e to that individ uals within organizations are working toward the common goal of successfully utilizing the technology the literature shows that a pragmatic assessment of the likely benefits and trade offs needs to be conveyed to users as part of this including anticipated timeframes additional considerations should comprise the avoidance of scope creep interoperability considerations and the appro priate implementation approach suited to the technology and organization in question for example a slow and incremen tal soft landing or a one off big bang throughout this process management also needs to plan for potentially extreme contingencies such as the technology failing ensuring fit between these technical social and organizational dimensions the three dimensions discussed above are closely related which means that achieving a certain degree of alignment or fit between them is of prime importance this align ment may be understood as a certain type of balance that needs to be in place to achieve one or more goals if the goal is an implementation that satisfies the majority of stakehol ders a system needs to be not only usable i e technically sound and useful e g fulfilling the needs of users organi zations and or patients but it also needs to be appropriately introduced by organizations e g through engaging with train ing and responding to the needs and expectations of relevant stakeholders for instance picture archiving and communi cation systems in united kingdom secondary care fulfilled all these roles and as a result were readily adopted by users and organizations this point is exemplified in a systematic review by yusof et al who after reviewing studies using the human social organization and technology fit framework conclude that all three technology human and organizational fac tors are equally important in addition to the fit between them although it is not particularly useful to be prescriptive about the nature of this fit such convergence appears much easier to achieve in relation to the organic incrementally devel oped home grown systems that have been developed for use by relatively small teams organizations than is possible in the larger more ambitious hit projects that are now increas ingly being parachuted into complex environments this may be due to the fact that in smaller scale deployments technical characteristics are easier to tailor to end user and organizational needs it may also be facilitated by greater orga nizational responsiveness to user needs and more effective communication between various structures and actors there is furthermore a growing realization that a new technology is easier to embed in an organization if there is a reciprocal or mutually supporting relationship between technical social and organizational factors in which new often unanticipated ways of working are allowed to emerge this presupposes an acknowledgment that changes in any of these dimensions be they small or large may affect the implementation process and the use design of technology in important ways in such situations other factors may therefore have to adapt accordingly to compen sate for the change for example workarounds employed by users social to cope with perceived shortcomings in a tech nical system technical may in some cases result in more efficient ways of working this perspective will be challeng ing for those pursuing linear implementation strategies but unless such experimentation and re invention is allowed and indeed encouraged by organizational strategies e g by mak ing certain workarounds official see box for examples i n t e r n a t i o n a l j o u r n a l o f m e d i c a l i n f o r m a t i c box examples of how organizational strategies can support innovative uses of hit general organizational characteristics organizational history and decision making character ized by supportive organizational culture recognition that technology and the organization evolve together a common goal and purpose specific organizational strategies educational sessions about innovative uses champions to facilitate innovative uses in different specialties professions effective communication and feedback about innova tive uses throughout planning and implementation involving suppliers and working together to discover and disseminate innovative uses technology may never fulfill its potential although a time consuming and expensive process evaluation of unan ticipated consequences is therefore important this should include evaluating consequences which may ultimately prove to be advantageous and also those which may inadvertently increase the risk of harm a chronological perspective a range of inter related factors over time the literature further shows that technology implementa tion is characterized by a range of factors which are of varying importance during the diverse stages of implemen tation for instance a systematic review by keshavjee and colleagues takes a chronological perspective focusing on the significance of pre during and post implementation con siderations this shifts the focus toward studying the interplay between these different dimensions at different stages of the implementation journey based on a critique of reports of implementations of electronic health record sys tems they identified how the focus of management activity changes as implementation proceeds for example activity should begin with extensive stakeholder discussions when making key decisions on software procurement in the pre implementation phase and follow through to the creation and nurturing of user support groups during the early post implementation phase extending this chronological view gruber and colleagues found that the availability of end user support during go live was particularly important discussion overall our review has indicated that appropriate apprecia tion of the importance of technical social and organizational considerations is essential in ensuring that technological innovations are not only useful and usable i e care provision pre implementation post implementation adequ ate technology individual and implementation stakeholder involvement organi sationa l benefits organizational leadership fig inter related technical social and organizational factors over time in hit innovation but that they also support the organizations or systems within which patients and professionals operate i e organizational functioning however it also needs to be kept in mind that these dimensions are inter related resulting in a need to pay attention to the reciprocal relation ship of different technical social and organizational aspects at different stages of implementation the exact nature of the relationship between these dimensions is less clear highlight ing the need for further work in this area we depict this graphically in fig we have reviewed synthesized and interpreted a large body of disparate knowledge of varying methodological quality pertaining to organizational considerations surround ing hit implementations this has allowed producing an integrated account of technical social and organizational dimensions that need to be considered when implementing hit drawing on evidence from disparate bodies of knowledge and varying theoretical backgrounds the factors identified may to some extend help to guide future imple mentations by for example helping to direct attention toward strategic decisions that facilitate involvement of users dur ing the design and implementation process and provide the opportunity for customization of technologies such considerations can help to minimize potential adverse effects whilst at the same time maximizing the chances of success ful integration with individual workflows and organizational requirements table illustrates how the factors identified in this work may be applied to real life contexts relating to the implementation of hit in different countries this shows how the wider strategy can have a bearing on the different dimen sions discussed and how organizational issues can be used to identify plan for and thereby ameliorate risks associated with hit implementation however the complex relationship between different tech nical social and organizational dimensions identified in this work means that there is no prescriptive approach to success ful implementation the emergence of unintended consequences may mean that strategies need to be adapted on an on going basis this is likely to require a careful balanc ing between organizational demands e g resources social demands e g user requirements and technical demands e g interoperability and performance we used a comprehensive strategy for searching the major medical databases to identify work of high quality however despite going beyond searching the relevant quantitative lit erature we cannot in any way claim that our searches are i n t e r n a t i o n a l j o u r n a l o f m e d i c a l i n f o r m a t i c table an illustration of how wider strategic factors across countries may be associated with dimensions identified in this review united kingdom united states of america australia strategy initially a central procurement of standardized hit systems centrally funded incentives to promote implementation of a range of certified systems government investment and guidance combined with local systems choice technology chosen by government so may lack essential technological characteristics useful for individuals and organizations some degree of systems choice so more likely to satisfy user and organizational needs but danger that technology is chosen based on incentives as opposed to needs some degree of systems choice so more likely to satisfy user and organizational needs user involvement limited by standardized software design limited by organizational drivers to choose technology e g financial incentives dependent on individual organizational strategies organizational leadership limited by heavy governmental involvement in strategic directions significant potential of organizational leadership in mitigating risks associated with hit implementation and adoption significant potential of organizational leadership in mitigating risks associated with hit implementation and adoption given the very poor indexing of this literature comprehen sive for example as our focus was on assessing processes involved in the implementation of medical technologies we did not search non medical databases directly related to the topic areas of interest overall much of the available evidence concerning organizational issues in relation to hit innova tions is anecdotal and retrospective in nature stemming from single organizational experiences of implementing a specific application these tend to be descriptive accounts without much attention to relevant theoretical considerations which makes drawing generalizable lessons from such reports diffi cult nevertheless we have provided a starting point for the development of best practice guidelines for implemen tation although this will need to be empirically tested and refined in future work more in depth work is also likely to bring to the fore additional wider contextual factors that go beyond the immediate organizational environment such as for example intra organizational relationships and political developments see table conclusions despite some previous work organizational issues have not received appropriate attention in the literature to date which may be due to them being experienced in different ways by different actors as a result they are difficult to measure objectively difficult to predict and time consuming to plan for nonetheless organizational issues are coming to the forefront of the hit agenda due to a general consensus within the field that technological innovation is not designed developed or deployed in a vacuum the numerous disciplines or bodies of knowledge which contribute to the study of technical social and organiza tional issues are rich in potential to facilitate implementation and adoption of innovations in increasingly complex health service systems research employing expertise in these fields is therefore central to furthering knowledge on organizational adoption and generalizable best practices for implementation authors contributions as conceived this work and together with kc led on drafting this review as and kc are guarantors conflicts of interests all authors declare that they have no conflict of interest funding we gratefully acknowledge funding from the medical research council the chief scientist office of the scottish govern ment the nhs connecting for health evaluation programme and the national institute for health research applied pro gramme grants scheme acknowledgements we gratefully acknowledge the contribution of colleagues who contributed to the nhs connecting for health evalua tion programme and extension funded project including chantelle anandan ashly black josip car akiko hemmi joe liu brian mckinstry susannah mclean mome mukherjee ulugbek nurmatov claudia pagliari yannis pappas and rob procter we also thank the two anonymous expert review ers for their valuable comments on an earlier draft of this manuscript summary points what was already known on the topic the study of organizational issues in health infor mation technology innovations is a multi disciplinary field utilizing bodies of knowledge from organizational psychology change management and human factors there is a general consensus that organizational issues can both facilitate and inhibit the implementation and adoption of technological innovation in healthcare particularly those innovations that are likely to have a major discernible impact on care processes what this study added to our knowledge while there is at present no overarching concep tual framework in relation to the implementation and adoption of health information technology innovations research consistently emphasizes the importance of technical social and organizational fac tors and the inter relationships between these early and on going user involvement relative advan tage of the technology and early demonstrable benefits a close fit with organizational priorities and processes training and support and effective leader ship and change management seem to be particularly important this work has enabled us to produce an integrated account of technical social and organizational dimen sions that need to be considered when implementing hit drawing on evidence from disparate bodies of knowledge and a range of relevant theoretical perspec tives background the proportion of older adults in the population is steadily increasing causing healthcare costs to rise dramatically this situation calls for the implementation of health related information and communication technologies ict to assist in providing more cost effective healthcare to the elderly in order for such a measure to succeed older adults must be prepared to adopt these technologies prior research shows however that this population lags behind in ict adoption although some believe that this is a temporary phenomenon that will soon change objectives to assess use by older adults of technology in general and ict in particular in order to evaluate their readiness to adopt health related ict method we employed the questionnaire used by selwyn et al in in the uk as well as a survey instrument used by morris and venkatesh to examine the validity of the theory of planned behavior tpb in the context of computer use by older employees respondents answered the questions via face to face interviews from the us and from israel spss was used for the data analysis results the results show that although there has been some increase in adoption of modern technologies including ict most of the barriers found by selwyn et al are still valid ict use was determined by accessibility of computers and support and by age marital status education and health health however was found to moderate the effect of age health ier older people being far more likely to use computers than their unhealthy coevals the tpb was only partially supported since only perceived behavioral control pbc emerged as significantly affecting intention to use a computer while age contrary to the findings of morris and venkatesh interacted differently for americans and israelis the main reason for non use was no interest or no need similar to findings from data collected in conclusions adoption of technology by older adults is still limited though it has increased as compared with results of the previous study modern technologies have been adopted albeit selectively by older users who were presumably strongly motivated by perceived usefulness particularly worrying are the effects of health pbc and the fact that many older adults do not share the perception that ict can significantly improve their quality of life we therefore maintain that older adults are not yet ready to adopt health related ict health related ict for the elderly should be kept simple and demonstrate substantial benefits and special attention should be paid to training and support and to specific personal and cultural characteristics these are mandatory conditions for adoption by potential unhealthy and older consumers elsevier ireland ltd all rights reserved corresponding author at ben gurion university of the negev industrial engineering management pob beer sheva israel tel e mail address heart bgu ac il t heart see front matter elsevier ireland ltd all rights reserved doi j ijmedinf international journal of medical informatics journal homepage www ijmijournal com international journal of medical informatics fig average annual health care costs for medicare enrollees aged and over source http www agingstats gov agingstatsdotnet main site default aspx introduction with the dramatic increase of people with chronic conditions and an ageing population there is a need to extend care from the hospital to the home ge healthcare and intel are helping to address these pressing issues the market for telehealth and home health monitoring is predicted to grow from us billion in to an estimated us billion by this quote relates to just one of many ict driven endeavors recently initiated all over the developed world to enhance healthcare provided for elderly people it has been suggested that the use of information and communication technologies ict such as computerized devices home computers inter net and other communication devices could significantly improve the quality of life of elderly people as well as facilitate cost effective care by formal and informal care providers this is particularly desirable in light of the expected growth in the percentage of people and over in the population and in the average annual cost of health care for this cohort fig for such technologies to be efficient and effective elderly people must be willing and able to use them prior research has indicated that age is negatively related to use of ict for such reasons as age related changes and decline of sensory and cognitive abilities as well as difficulty in obtaining technical support it is suggested that these barriers among others explain the imparity in adoption of modern technologies for example personal computer pc use and internet access where senior people are under represented as evident in table although the data in table relate to the us adult pop ulation similar numbers have been reported in other developed countries the picture that emerges casts doubt on the prospects for successful adoption of healthcare related ict among older people it is therefore imperative to investi gate the patterns of ict use by this group in order to determine what needs to be done to ensure effective implementation adoption and use although there is a consensus among researchers as to the effect of age on ict use some maintain that this is a tem porary situation that will significantly change within a few years as the older generation acquires experience in using ict however technological innovations such as the multi touch user interface used in the iphone and other devices or more recently the motion activated user interface are being introduced at a fast pace these innovations significantly change the way people interact with ict is it likely that future generations of older people will adopt technological inno vations more readily than the present generation if indeed impairments inherent to older age are major barriers to adop tion then it is quite likely that the present difficulties will persist into the future as well in order to investigate changes in technology use patterns in general and ict use in particular the present study repli cated the data collection work of selwyn and collaborators in the uk in the following questions were addressed does age still affect ict use among older adults what access do older adults have to ict where can older adults access icts what access do older adults have to ict sup port how does access to new icts such as computers and the internet compare with access to other technologies what factors are associated with older adults access to ict e g gender age educational background health conditions marital status what are older adults using ict for what are the reasons behind older adults non use of ict data for this study were collected in from elderly people in the us and in israel our findings corrob orated those of selwyn et al although the proportion of older adults using ict was higher similar to the earlier results most non users indicated that lack of need for ict was the primary reason for not using a computer suggesting that not much had changed in the years that elapsed since the earlier study age was still found to significantly and negatively affect use with health emerging as a moderating factor that augments the effect of age this evidence is worrying in light of the intensive effort being invested in introducing technologies to improve the quality of life of older people the implications of these findings will be discussed further on the rest of the paper is organized as follows literature sur vey description of the methods employed and results the paper concludes with a discussion and presentation of con clusions international journal of medical informatics table internet use by age years born ages in of total adult population of internet using population 1964 1945 pew internet american life project december survey n total adults and margin of error is n total internet users and margin of error is literature survey the most rapidly growing segment of the population in devel oped countries consists of the elderly particularly the oldest old aged plus for example the uk office for national statistics found that over the last years the percentage of the population aged and over increased from in to in an increase of million people over the same period the percentage of the population aged and under decreased from to this trend is projected to continue so that by of the population will be aged and over versus aged or younger similar fig ures are being reported all over the developed world given this demographic forecast there is a clear and urgent need to provide more effective and efficient healthcare while reducing the number of care providers information and communica tion technologies are seen as the key to achieving these goals yet recent studies showed that acceptance and adoption of ict by the elderly is problematic and particularly so in the healthcare context this evidence is trou bling in light of the extensive investments by governments and institutions in the development of applications and devices to improve healthcare for older people the success of which largely depends on adoption and use by the elderly consumers indeed as reported in a review by marschollek et al most such projects failed for various reasons prominent among which was consumer reluctance studies investigating older ict users found that many of the restrictions and social isolation problems that elderly peo ple face on a daily basis could be alleviated through the use of computers and online technologies indeed older people who used computers thought they enjoyed better social inter action memory enhancement and mental stimulation it is claimed that using the internet can improve the quality of life of ageing people by inducing them to learn new technologies and thus maintain their cognitive capabilities and self esteem as well as by opening new avenues to information and services that would otherwise be difficult to access ict applications in healthcare in former u s surgeon general c everett koop stated cutting edge technology especially in communication and information transfer will enable the greatest advances yet in public health real health care reform will come only from demand reduction as individuals learn to take charge of their health communication technology can work won ders for us in this vital endeavor eventually personal home telemedicine links could provide every home with access to health information h a day days a week encourag ing personal wellness and prevention and leading to better informed decisions about health care p indeed fifteen years later the dominant use of ict in healthcare is in conveying high quality health information and educative materials to consumer patients in order to empower them to make informed decisions and deal more actively with their health some of the relevant websites focus on specific diseases whereas others are general portals such as medlineplus a notable application is nihseniorhealth www nihseniorhealth org which is especially designed for older users while the number of accesses to these applica tions has been growing a study of a german portal revealed that older consumers aged comprised only of users with younger surfers possibly relatives and caregivers con stituting the vast majority of users telemedicine and pervasive and ubiquitous computerized services have mainly been used for monitoring and com munication purposes to facilitate remote care for people at home all report limited success in terms of adoption and use with reasons cited for non use ranging from exces sively complex to offering no advantage over more traditional alternative solutions theories explaining ict adoption one of the fundamental theories that seek to explain par ticular behaviors is the theory of planned behavior tpb which posits that a behavior is the result of an inten tion to carry it out which is in turn influenced by attitude att toward the behavior perceived behavioral control pbc and subjective norm sn pbc reflects perceptions of inter nal and external constraints on behavior and encompasses self efficacy resource facilitating conditions and technology facilitating conditions p while sn is defined as the person perception that most people who are important to him think he should or should not perform the behav ior in question p drawing upon the tpb the technology acceptance model tam has been developed to explain ict use according to tam perceived useful ness pu and perceived ease of use peou are the primary international journal of medical informatics e231 determinants affecting attitude toward use which affects intention to use additionally pu which is impacted by peou was found to directly affect intention to use later research developed which included antecedents to pu and peou and the unified theory of acceptance and use of technol ogy utaut which posited that four categories of variables influence information technology acceptance performance expectancy effort expectancy facilitating conditions and social influence several researchers have adapted the above frameworks to describe adoption of ict by older adults determinants affecting ict use in healthcare a large number of determinants have been posited to affect ict use in the healthcare context the reader is referred to a systematic review of patient acceptance of consumer health information technology chit by or and karsh which lists determinants of which are patient related factors associated with patient health factors socio demographic variables and variables relating to the individual prior experience with computers the remaining factors relate to human computer interaction hci and orga nizational environmental factors such as satisfaction with the health package provided hci factors include pu peou internet dependence self efficacy toward computers com puter anxiety or fear of technology intrinsic motivation perceived information reliability and some others age was the most frequently used factor yet did not show a consis tent effect in contrast other studies showed that relative to younger adults older adults reported less comfort lower efficacy and less control over computing technologies perceptions that are likely to decrease acceptance while gender the second most studied variable in the above review demonstrated no effect in the majority of studies higher education was positively associated with acceptance in of the studies as was prior experience or exposure the role of patient health status in chit acceptance also yielded mixed results as some researchers showed that peo ple experiencing poorer health conditions were more likely to accept chit whereas others e g showed that better health less severely ill patients was associated with increased acceptance having school age children at home was also found to drive acceptance possibly via determinants such as social influence support availability and computer anxiety these findings were further corroborated by studies of specific health it applications ict use by elderly people extensive research has focused on ict use by elderly people in general and in the healthcare con text in particular morris and venkatesh for instance found that age moderated the effects of atti tude pbc and subjective norm on ict use in the workplace with the result that among older users use intention was less affected by attitude than among younger employees and more affected by opinions of important persons and by perceived control but it is important to note that the authors studied users at the workplace where older users are not very old in contrast the review of articles devoted to chit adoption showed mixed results regarding the association between age and technology acceptance however sufficient evidence supports the negative association between age and use of pc and internet declining physical and cognitive capabilities may cause seniors to experience greater difficulties in using computers and these may serve as internal controls or may inhibit conditions that increase effort expectancy associated with it use czaja et al found that adults over years of age had fewer computer skills and had less computer self efficacy than younger adults possibly leading to reduced use or intention to use computer technology these older users however were willing to attempt training and ready to use ict when sufficient instructions on use of the technology were provided table summarizes determinants affecting ict use by elderly people in the healthcare context while designing easy to use systems adapted to specific impairments typical of older age is clearly recommended researchers found that older users tended to focus on bene fits more than on costs and were willing to make the effort to acquire skills if they were convinced of the tech nology advantages and adequacy for their specific needs above and beyond traditional healthcare related means this evidence somewhat contradicts the common assertion that limited use of technology by older adults stems from low self efficacy computer anxiety lack of accessibility or technophobia a negative attitude toward modern technology in general research method this study attempts to answer the research questions out lined above by replicating selwyn et al study and by applying the tpb to a sample of older adults in their natu ral residence setting private or nursing home when nursing homes were the respondents permanent residence sample due to the difficulty of accessing respondents no statistical sampling method was applied rather a sample of conve nience was used although this can be considered a limitation the distribution of the sample in terms of personal charac teristics was sufficiently representative of this population in terms of gender i e a higher percentage of females as is typ ical for this age group education level marital status and health status as can be seen in table data were collected from elderly people in texas usa and in israel in the usa four nursing homes were visited and resi dents who were available in public areas were approached for an interview residents agreed to participate in israel we interviewed persons in two nursing homes and persons living in their private homes all the interviews were carried out in and and lasted min to h each table determinants of older users acceptance of ict determinant description references perceived usefulness perceived impact relevance international journal of medical informatics e231 the degree to which the potential user perceives the technology as beneficial useful and able to significantly contribute to the purpose it is intended for perceived ease of use the degree to which an individual perceives using the technology as free of effort issues associated with the technology factors such as the system cost complexity and safety personal traits factors such as the individual age health condition self actualization self efficacy prior experience with computers computer anxiety social issues factors related to the social reference groups of the individual such as subjective norm and image facilitating issues factors related to the external environment such as availability of support table descriptive statistics for the us and israeli samples parameter u s a israel total mean s d respondents gender female male age total residence private home nursing home education elementary junior high high school bachelor master ph d total mean s d living with a partner no yes severe health problems no yes the research instrument we used selwyn et al questionnaire which is composed of five statements related to accessibility to and use of various technologies statements related to reasons for not using a computer for persons stating they have not used computers in the last months statements related to use of computers for persons stating they have used computers in the last months we refrained from attaching the questionnaire due to its length but it can be secured from the first author statements measuring the tpb model att pbc and sn adapted from morris and venkatesh appendix a personal questions relating to age education computer experience health status marital status etc the questionnaire included questions with nominal or ordinal scales used for all but the demographic questions a nominal scale was used to code values such as places of accessibility perceptions or frequency of use an ordinal point likert scale strongly disagree to strongly agree was used for statements with ascending scales to simplify the data collection process in view of the relatively advanced age of the responding population data were analyzed using spss international journal of medical informatics e231 table access to technologies by location own access at home no access computers and peripherals laptop palmtop pc years old p pc years old printer scanner telephone payphone p videophone landline p fax p mobile phone p tv and vcr cable tv digital cable tv p dvd video recorder player p other entertainment personal music player cd player digital radio analog radio digital camera p video camera p handheld games machines video games machine statistically significant difference between us and israel results the us respondents were all nursing home residents they were significantly better educated p than the israeli respondents and were more likely to live without a partner p these facts however may be mutually dependent as people who enjoy a higher financial status known to pos itively correlate with education are not very healthy and live alone may have a greater tendency to move to rela tively expensive nursing homes such as those where the data were collected in the us therefore we conducted the various tests with due attention to statistically significant differences between the populations table older adults access to technology and support while this paper focuses primarily on ict use by older peo ple use of non ict technologies both traditional and modern was also examined in order to determine whether older peo ple are generally technophobic refrain from adopting any type of modern technology or selectively adopt technologies they perceive as useful for their specific needs therefore respondents were asked about use of and accessibility to both traditional and modern common technologies such as phones and entertainment equipment in addition to computers and peripherals no home access but access from family friends access elsewhere accessibility of technology was graded from more acces sible own at home to less accessible family friends public places to no access the responses listed reflect the highest graded location indicated by the respondent table accessibility of ict twenty nine percent of the respondents had first grade acces sibility owned or had access at home to pcs that were less than years old table of the respondents had the same level of accessibility to pcs that were older than five years and owned a laptop or had one at home assuming that most respondents had either a laptop a newer pc or an older pc but not both these three figures mean that of respondents had access to a pc at home there was no significant differ ence in first and second grade accessibility between israeli and us respondents twenty two percent of the respondents indicated they could access a pc elsewhere mainly ameri cans who referred to public computer rooms at their nursing homes together these two figures indicate that nearly of the respondents had fairly easy access to computers regarding other technologies of the respondents owned or had at home mobile phones had cable tvs and had digital cable tv had digital radios had dvds and had digital cameras israeli respondents had more first grade access to digital cameras 05 df p to fax machines df p to mobile phones df p and to video international journal of medical informatics e231 table older adults potential access to computers site of access a relative home home library a friend home a community center school university work place of study museum science center a pay per use site e g internet café cameras df p than the us respondents for example of israelis owned or could access at home a digital camera whereas this was the case for only of the americans compared to results in selwyn el al study slightly more people in our sample had first grade access to mobile phones vs and significantly more to dvd players vs perhaps due to transition from video recorders players to the more modern technology vs a similar picture emerged in regard to digital cameras and digital radios additionally first grade accessibility to pcs nearly doubled overall from to but nearly tripled to for the israeli respondents the majority of whom lived at home and hence were more comparable to the uk sample this increase in rate of adoption can be attributed to an accel erated pace of ict adoption by older people and the difference between the israeli and american populations possibly results from the technology savvy character of israeli culture although this assumption requires further substantiation the results indicate that modern technologies are indeed adopted by older adults supporting the assumption that this population selectively adopts technologies perceived as bene ficial to them albeit perhaps more slowly than the younger population the results further imply that older people are participating more than ever in the technological era and asso ciated culture respondents were then asked to indicate where they could have accessed a pc had they wanted to table interestingly all respondents indicated they could have done so with no difference between the two populations potential sites for pc access were similar in our study and in selwyn et al with a relative home own home a friend home and a library emerging as the four most accessible sites in descending order overall of the respondents n had used comput ers before n of them at work forty percent n of the americans and n insignificant difference of the israelis total indicated they had used a pc in the last months these will henceforth be termed users as opposed to the rest the non users determinants relating to older adults access to and use of ict will be analyzed next consid ering users and non users separately to gain a clearer picture americans and israelis will be analyzed separately whenever differences are statistically significant accessibility of support as prior research had highlighted lack of access to support as a barrier to ict use by elderly people we asked users about actual sources of support and non users about table actual and potential sources of support source of support users n non users n actual potential own partner children household family neighbors friends others table number of accessible support resources number of accessible support sources users non users actual potential potential sources of support had they needed any table table presents data regarding available sources as indicated by users and non users non users and users alike indicated their main sources of support were their children and other family members yet of the users mentioned that they actually relied on them selves or on a partner the latter however were mentioned as a potential sources by only of non users interestingly all non users mentioned they could enlist support from at least one source yet clearly the level of accessibility was lower than for users determinants of technology use to refine our understanding of the association between acces sibility and use we grouped the respondents according to access to pcs and support following selwyn et al people were classed into five groups core access newer pcs and close support nearby relatives neighbors or friends periph eral home access older pcs and some support remote access access at family or friends homes and support public access and support close or remote and no access the distribution of the populations among the five groups and the level of pc use within each group are displayed in table difference in level of use among the five groups was statistically significant mann whitney u and not unexpectedly showed that accessibility to equipment and sup port is strongly associated with actual use a closer look at the frequency of use of various technologies revealed that as could have been anticipated most respon dents watched tv listened to the radio and in israel also used a mobile phone table thirty three percent of the respon dents indicated they used a pc at home very often more often than they watched video dvd hence in our study use of a pc was by no means a negligible activity in contradistinction with the findings of selwyn et al clearly home was the preferred international journal of medical informatics e231 table level of access to computers by location and support proximity level of accessibility n number of users within core peripheral home remote public no total table frequency of technology use very often fairly often rarely never us israel total us israel total us israel total us israel total watch tv watch video dvd listen to music via hi fi listen to radio play video games talk on mobile phone send receive sms use a pc by location home relatives friends workplace public places statistically significant difference between us and israel test p 05 location for using a pc of the actual users with the pos sibility of accessing a pc at the homes of family or friends rated very low public places in contrast were rated higher by both populations possibly referring to public computer rooms at nursing homes yet perhaps lending some support to the call of selwyn et al to make pcs available in public places for the use of otherwise excluded populations activities comprising ict use by older adults the computer activity cited by users as most common was sending receiving e mails followed by writing documents and playing computer games table while the first two were similarly rated in selwyn study playing games was quite neg ligible thus we see that older adults do indeed make use of computers for social activities and entertainment more fre quently than before as is the case with younger users although the above results generally support selwyn et al conclusions from data collected in we see some improve ment in the rate of pc use particularly among younger olds as well as in the variety of activities performed a closer look at computer use by older adults although all respondents indicated having some level of access to pcs and support and although n indicated they had used computers before only n were actual users i e said they had used a pc in the last months hence some older adults with prior experience in using a pc despite having access to both the technology and related sup port chose not to the question is why we attempted to arrive at an answer by analyzing the effect of personal and social characteristics on pc use and then examining pc use by our respondents under the tpb finally we analyzed the respondents direct answers to this question personal and social characteristics and pc use a binary logistic regression allowing assessment of the sin gular effect of each independent variable on the dependent variable as well as the effect of interactions among variables was employed to gain more insight into the role of personal characteristics in determining use used a pc last year no yes was the dependent variable and country gender age group health status marital status and education were the independent variables all the independent variables were factorial with age grouped into four categories and we used the backward stepwise method with wald as the stop criterion and ran the regression with the independent variables and the interaction between health and age the full results of the binary logistic regression are pre sented in appendix b the model fitted the data well as evidenced by the insignif icant in the model summary and the significant for the omnibus tests of the model coefficients it took five steps to converge explained between cox and snell and nagelkerke r square of the variance in use and substantially improved model predictive power from to multi collinearity was negated since the highest variance inflation factor vif value of the independent variables was well below even the strict value of used as multicollinearity cutoff value having a partner age and being better educated had a sig nificant effect on use table the odds for pc use were table computer based activities activities n very often often rarely never sending receiving e mails writing and editing documents and letters playing games online banking personal financial activities searching information about jobs education business navigate surf without a specific purpose organizing files learning by a computer software watching adult entertainment listening to music on a computer surfing navigating to acquire personal knowledge collecting information about or searching products services creating and manipulating images e g photos downloading software music movies computer pictures watching dvd on the computer participating in chats forums purchasing products services making movies and computer animations programming creating maintaining a personal website taking online courses or lessons making music with a computer times greater an increase of for older adults living with a partner than for persons who lived alone age negatively affected use with odds for computer use among respondents aged and respectively and lower than for a person aged older people with higher education were times an increase of more likely to use a pc as compared with respondents with a lower educa tional level interestingly health on its own was insignificant yet had a major effect when interacting with age thus being and healthy increased the odds of use times while being a healthy year old increased the odds times as compared with being unhealthy being and healthy how ever had no significant effect it therefore seems that health is actually moderating the effect of age on use this result might explain the mixed findings of previous studies con cerning the effect of health on ict use and carries significant implications for the potential adoption of ict by unhealthy older adults international journal of medical informatics e231 tpb and pc use by older adults tpb explains that the behaviors of individuals are positively influenced by a positive attitude toward the behavior as well as by perceived behavioral control over resources and efforts associated with undertaking the behavior these individuals are also positively influenced when important and influential people around them think they should undertake the behavior sn morris and venkatesh supported the conjecture that older employees placed less importance on attitude but more on pbc and sn hence adapting tpb to the present context it was posited that older people with a more positive attitude toward pc use and a higher pbc as well as a more highly valued sn will have a greater tendency to use a pc as stated above we used the measures employed by mor ris and venkatesh though in view of the different context and likert scale we re tested their reliability and validity a principal factor analysis extracted three factors with eigen values greater than as expected and loadings of the items table variables affecting pc use b s e wald df sig exp b c i for exp b lower upper step partnera age age 539 age 008 age 654 higher educationb age healthy 049 age healthy c 986 age healthy age healthy 023 constant 926 a compared to living alone b compared to high school or less c compared to being unhealthy international journal of medical informatics e231 table parameters of the binary logistic regressions us sample n israeli sample n final step final step classification table improvement from to from to r square cox snell to nagelkerke between and between and hosmer lemeshow test df p 379 df p 136 variables in the final equation pbc age att pbc age sn table variables in the binary logistic regression equations b s e wald df sig exp b c i for exp b lower upper us sample step pbc age att 029 age att 279 037 169 age att age att 703 constant 001 465 israeli sample step pbc 011 age sn 041 age sn 161 age sn 139 age sn 896 259 constant on their respective factors were significant and clean with the exception of one item a computer is not compatible with other technologies i use which was culled cronbach alpha values were between and and correlations among the factors were sufficiently small to negate multicollinearity effects with independent variable multicollinearity already ruled out in the previous section appendix c these indicators attest to the convergent and discriminant validity of the factor scales however having noticed a salient difference between responses of americans and israelis we analyzed them sepa rately the full results of the two regressions are displayed in appendices d and e in spite of the sufficient goodness of fit indicators results should be interpreted with caution due to the relatively small sample size of the two sub groups which was reduced further due to omission of cases with missing values intention to use a computer in the forthcoming months scale do not intend perhaps intend definitely intend was the dependent variable while the values of the three fac tors as calculated by spss during the factor analysis were the independent variables age classed into four groups as above was taken as an independent variable as were factors express ing the interaction of age with each of the tpb factors i e age att age pbc age sn both models demonstrated good fit to the data as shown in table and explained about of the variance in intention to use a computer pbc emerged as a strong predictor of intention to use a computer in both populations table each increase by one standard deviation in pbc increased the odds of using a com puter by in the us population and by in the israeli population contrary to tpb the other two factors att and sn were insignificant determinants of use as was age on its own rather age moderated the effect of att in the american table reasons for not using a computer us israel total no need not interesting too old don t know how too difficult no access too expensive medical problem don t like sample and of sn in the israeli sample contrary to the find ings of morris and venkatesh age increased the impact of att though only for age group while age had no effect on att in the israeli sample it moderated the effect of sn though again only for age group thus age reduced the effect of sn on the intention to use a computer among israeli respon dents contrary to the findings of morris and venkatesh although the above results shed light on the barriers to computer use we sought to achieve a deeper understanding by asking non users to indicate reasons for their behavior with multiple reasons allowed see table quite surprisingly answers do not reflect technophobia or an inherent reluctance to use technology but rather a cognitive choice as of non users said they did not use computers because there was no need or it was not interest ing whereas only perceived computer use as too difficult or indicated they were too old to learn no access or don t know how was indicated by whereas mentioned computers were too expensive only one respondent indicated don t like as a reason similar results were found by selwyn et al so evidently there is more to ict non use than generally advocated by prior research international journal of medical informatics e231 table summary of evidence answering the research questions no research question answer age and ict use does age still affect ict use yes although there is an increase in the proportion of older adults who adopt modern technology and ict we still see that older people are less inclined to use technology including ict patterns of use of ict and other technology what access to ict do older adults have most respondents indicated they could access ict and support fairly easily where can older adults access icts what access to support when using ict do older adults have mostly relative homes at own home and at a public library how does access to new icts such as computers and the internet compare with access to other technologies core accessible to modern technologies as well as to ict has increased compared with modern technologies are adopted albeit selectively whereas some modern technologies have been widely adopted ict adoption still lags behind personal characteristics affecting ict use what factors are associated with adults access to ict e g gender age educational background health conditions and marital status education marital status and health emerged as significant determinants of ict use in addition to age health however was found to moderate the effect of age on use use of ict by older adults what are older adults using ict for sending receiving e mails writing and editing documents playing games reasons for non use what are the reasons behind older adults non use of ict perceived behavioral control and no need discussion ict has revolutionized our lives in terms of access to infor mation yet for several segments in population particularly elderly people the digital barrier remains the older people are the less likely they are to use ict is this a tempo rary situation will future older adults more readily adopt ict in general and technology related to healthcare in particu lar before attempting an answer or referring to the research questions a review of the limitations of this study is in order limitations the small sample size was a major limitation because it restricted the statistical tests that could be employed and reduced statistical power in addition generalizability is limited due to the employment of a sample of convenience likewise the salient differences between the two popula tions imply possible cultural effects which merit further study future research should aim to collect data from a larger and more representative sample in various countries to extend the internal and external validity of the results answers to the research questions computers have been part of the industrialized world for more than three decades and today older adults are not as detached from technology as before research supports the importance of prior experience leading several scholars to maintain that the inclusion of older people in the digital world is just a matter of time this issue is addressed by the first question formulated in the present study our results point to answers to this and to the other research questions as summarized in table age remains a significant negative determinant of use of technology in general and ict in particular although there has been an increase in adoption rate as compared with data collected in indeed perceived behavioral control which decreases with age possibly due to cognitive and physical impairments was found to positively affect ict use adoption of ict however lags behind adoption of other types of modern technologies such as digital cameras dvds and cell phones whose rate of use by older adults is quite high albeit different between the two sample populations this difference merits further research the results show that accessibility of ict is a diminishing issue as is the accessibility of support although both still emerged as important determinants of ict use by the elderly people clearly prefer to use computers at home rather than at relatives or in public places as could have been anticipated we therefore conclude that accessibility is no longer a strong barrier to pc use although we acknowledge the importance of close support looking at results obtained about a decade ago it appears there has been an increase in adoption of all forms of technology however many older people are somewhat reluctant to adopt new technologies whether ict or other applications unless they become convinced that these tech nologies confer significant benefits this conclusion follows from the reasons cited by respondents in this study and that of selwyn et al which suggest that older adults refrain from using technologies they are not interested in or have no need for while prior research and current practice http www statistics gov uk cci nugget asp id concur that ict can improve the quality of life of older adults many remain unconvinced it is therefore essential to find ways of bridging the gap between the perceptions of providers and those of older customers regarding potential benefits of ict corroborating prior research education marital status and health emerged as significant determinants of ict use international journal of medical informatics e231 in addition to age health however was found to moder ate the effect of age on use possibly explaining the mixed results cited in the literature it seems that health as such is not so much a barrier as a factor capable of augmenting the effect of age on use healthy older people are many times more likely to use pcs while this is an encouraging find ing if we believe that advances in medicine imply healthier older adults it carries significant implications for providers of healthcare related technologies targeting unhealthy older people an additional concern is the positive effect of living with a partner which suggests that older people living alone for whom ict is posited to facilitate social inclusion and better healthcare may be the very ones less likely to use it although a growing number of elderly have adopted newer technologies their usage level is basic and it seems that depth of use has yet to be addressed for example searching for products or information online purchasing and online bank ing scored low on the usage list this is clearly not a positive sign from the standpoint of penetration of healthcare related ict among older adults contrary to the theory of planned behavior attitude and subjective norm did not emerge as affecting ict use although both significantly interacted with age to affect use albeit dif ferently for the two populations quite intriguing is the fact that the most prevalent direct reason given for non use was no need supporting the assertion that if older adults are to be induced to adopt icts the benefits have to be clearly demonstrated conclusions seen from the perspective of older adults use of ict in the healthcare context the results presented in this study are variously encouraging and discouraging on the positive side we see that the participation rate among the youngest old aged nears and that there is a significant increase in the rate of ict use by older people compared with results from data collected a decade ago most of our respondents had reasonable access to computers and to some degree of support yet the availability of a close source of support is still as important as it was seven years ago nowadays however nearby support seems more readily available on the other hand while older people do adopt modern technology they are quite selective and tend to invest resources only when the expected benefits far exceed those provided by more tradi tional technologies with similar functions we maintain that the prospects for older adults using ict are better than ever since education and health have emerged as significant determinants of pc use on the one hand and on the other it is likely that future older adults in the industri alized countries will be healthier and better educated as well as more proficient in computer use on the less positive side the effects of accessibility of sup port and of health and marital status on ict use may pose significant barriers to adoption of healthcare related ict now and in the future in spite of encouraging advances health status seems to be a strong moderating factor affect ing pc use thus unhealthy people may be less inclined to adopt ict including applications intended to improve their quality of life possibly due to lower perceived behavioral control caused by physical or cognitive weakness therefore providers of such technologies should ensure that potential consumers have first grade access to support from nearby family friends or neighbors potential adopters should clearly comprehend the benefits that can accrue from the new technology the significance of perceived usefulness as a determinant of ict use is another conclusion that emerges from reasons given by respondents for non use similarly to results reported by selwyn et al the majority of respondents in our study claimed that computers were of no interest to them and that they did not feel a need to use them the superior capacity of new healthcare related technology to improve their health status and quality of life compared with simpler traditional technologies has to be made obvious and irrefutable as shown by the results of the tpb analysis attitude toward use becomes more important as people grow older at least as regards the us sample this finding suggests that above and beyond pressure from close relatives and friends efforts to influence positive attitude are warranted the divergences between the american and israeli respondents however may imply cultural differences in this regard such differences if substantiated by further research should be taken into account and suggest that diffusion approaches should be adapted to the specific characteristics of potential consumers future research should investigate these differences in common to both populations pbc emerges as the most important determinant of use among the three tpb fac tors hence every introduction of new healthcare related ict should not only offer straightforward and easy to use technol ogy it must also be bundled with a comprehensive training program any new technology that differs significantly from technologies in which older adults are already proficient is likely to encounter adoption difficulties now and in the future due to the cognitive and physiological impairments that affect pbc among the elderly currently the results do not fully support a bright future for health related ict targeting older people unless the technol ogy is kept simple is seen to be useful and is bundled with first rate support author contributions tsipi heart initiated designed and supervised this research she has also guided the writing of the dissertation verified and improved the data analysis and wrote the submitted manuscript efrat kalderon conducted the research as part of her grad uate studies she collected the data analyzed the data and wrote a dissertation based on the results both authors substantially contributed to the present work competing interests we hereby declare that there is no conflict of interests related to this work international journal of medical informatics e231 summary points what was already known on the topic health expenses for older adults increase ict is believed to assist in providing more cost effective healthcare to this population prior effort in this direction has marginally succeeded older people use of technology lags behind younger people apart of age other personal and social factors such as health education and marital status were found to affect ict use by older adults according to the theory of planned behavior tpb ict use should be determined by attitude toward use att perceived behavioral control pbc and subjec tive norm sn age was found to moderate these factors in a workplace what this study added to our knowledge there is an increase in use of technology in general and of ict use in particular compared with data col lected in nonetheless most findings of the above study were still supported by the present data collected in age was still a barrier to ict use health was found to augment the effect of age on ict use tpb was only partially supported in the present study setting implying that intention to use a computer by older adults is mainly affected by pbc yet not by att and sn age was found to moderate the effects of att in the american sample and of sn in the israeli sam ple possibly implying an association between tpb and culture additionally the effect of age on att and sn was opposite to findings in prior research the results show that older adults are not yet ready to easily adopt health related ict however in order for this to materialize the application should render salient benefits above and beyond traditional technolo gies be simple to use and be provided with adequate support the work was self funded by the authors without involve ment of any institution or external funding resource the first author is a full time employee of an academic institute and the second author is a full time employee of a firm whose area of interest is unrelated to the topic of this paper appendix a tpb items adapted from morris and venkatesh intention to use assuming you had access to a computer would you use it during the forthcoming months do not intend perhaps intend definitely intend 04 attitude att using computers is a bad good idea very bad bad good very good 04 using computers is a foolish wise idea very foolish foolish wise very wise 04 i dislike like the idea of using computers dislike very much dislike like like very much 04 using computers is unpleasant pleasant very unpleasant unpleasant pleasant very pleasant 04 subjective norms sn people who influence my behavior think that i should use a computer strongly do not agree do not agree agree strongly agree 04 people who are important to me think that i should use a computer strongly do not agree do not agree agree strongly agree 04 perceived behavioral control pbc i have control over using a computer strongly do not agree do not agree agree strongly agree 02 04 i have the resources necessary to use a computer strongly do not agree do not agree agree strongly agree 02 04 i have the knowledge necessary to use a computer strongly do not agree do not agree agree strongly agree 02 04 given the resources opportunities and knowledge it takes to use a computer it would be easy for me to use a computer strongly do not agree do not agree agree strongly agree 02 04 a computer is not compatible with other technologies i use strongly do not agree do not agree agree strongly agree 02 04 international journal of medical informatics e231 appendix b results of the binary logistic regression personal characteristics case processing summary unweighted casesa n percent selected cases included in analysis missing cases total unselected cases total a if weight is in effect see classification table for the total number of cases categorical variables codings frequency parameter coding age age age 89 age country u s israel partner gender male female 000 edu2cat high school or lower 000 higher education 000 healthy 97 000 00 000 block beginning block classification tablea b observed predicted used a pc last year percentage correct no yes step last year use computer no yes overall percentage aconstant is included in the model bthe cut value is international journal of medical informatics e231 variables in the equation b s e wald df sig exp b step constant 183 block method backward stepwise wald omnibus tests of model coefficients chi square df sig step step 973 000 block 973 000 model 973 000 step step 154 block 000 model 000 a a negative chi squares value indicates that the chi squares value has decreased from the previous step model summary step log likelihood cox snell r square nagelkerke r square 339 543a a estimation terminated at iteration number because maximum iterations has been reached final solution cannot be found hosmer and lemeshow test step chi square df sig 689 classification tablea observed predicted last year use computer percentage correct no yes step last year use computer no yes overall percentage step last year use computer no yes overall percentage athe cut value is variables in the equation article history received february accepted december available online january keywords electronic health records systematic review privacy confidentiality security standards a b t r a c t objective to report the results of a systematic literature review concerning the security and privacy of electronic health record ehr systems data sources original articles written in english found in medline acm digital library wiley inter science ieee digital library science direct metapress eric cinahl and trip database study selection only those articles dealing with the security and privacy of ehr systems data extraction the extraction of articles using a predefined search string the outcome of which was reviewed by three authors and checked by a fourth results a total of articles were selected of which used standards or regulations related to the pri vacy and security of ehr data the most widely used regulations are the health insurance portability and accountability act hipaa and the european data protection directive ec we found articles that used symmetric key and or asymmetric key schemes and articles that employed the pseudo ano nymity technique in ehr systems a total of articles propose the use of a digital signature scheme based on pki public key infrastructure and articles propose a login password seven of them com bined with a digital certificate or pin for authentication the preferred access control model appears to be role based access control rbac since it is used in studies ten of these studies discuss who should define the ehr systems roles eleven studies discuss who should provide access to ehr data patients or health entities sixteen of the articles reviewed indicate that it is necessary to override defined access policies in the case of an emergency in articles an audit log of the system is produced only four studies mention that system users and or health staff should be trained in security and privacy conclusions recent years have witnessed the design of standards and the promulgation of directives con cerning security and privacy in ehr systems however more work should be done to adopt these regu lations and to deploy secure ehr systems elsevier inc all rights reserved introduction the paper based health records currently in use may generate an extensive paper trail there is consequently a great interest in moving from paper based health records to electronic health re cords ehrs these efforts are principally being made by indepen dent organisations however recent proposals suggest that integrated health records provide many benefits some of which include a reduction in costs improved quality of care the promo tion of evidence based medicine and record keeping and mobility in order to achieve these benefits ehr systems need to satisfy cer tain requirements in term of data completeness resilience to fail ure high availability and the consistency of security policies four great obstacles limit the deployment of ehr systems funding technology attitude and organisational aspects many govern ments rely on integrated ehrs because of the benefits expected from them one example of this interest is that of the us govern ment in the us president decided that the majority of amer icans would be connected to ehrs by in february the us president signed the american recovery and reinvestment act which included the investment of 000 million dollars in the digitalisation of medical records in the usa the member states of the european union also intend to make their health systems compatible before as the vice president of the european commission announced at the high level ehealth conference the eu objective is to share patients ehr data with the objective of free movement and of obtaining quality and effi cient health care however there has been very little activity in policy develop ment involving the numerous significant privacy issues raised by corresponding author fax e mail addresses aleman um es j l fernández alemán mariainmaculada carrion um es i c señor pedroangel oliver um es p á o lozoya atoval um es a toval a shift from a largely disconnected paper based health record sys tem to one that is integrated and electronic moreover the ad vances in information and communications technologies have led to a situation in which patients health data are confronting new see front matter elsevier inc all rights reserved http dx doi org j jbi 003 contents lists available at sciverse sciencedirect journal of biomedical informatics journal homepage www elsevier com locate yjbin security and privacy threats the three fundamental security goals are confidentiality integrity and availability cia the protection and security of personal information is critical in the health sector and it is thus necessary to ensure the cia of personal health information according to the iso standard confidentiality refers to the process that ensures that information is accessible only to those authorised to have access to it integrity refers to the duty to ensure that information is accurate and is not modified in an unauthorised fashion the integrity of health infor mation must therefore be protected to ensure patient safety and one important component of this protection is that of ensuring that the information entire life cycle is fully auditable availability re fers to the property of being accessible and useable upon demand by an authorised entity the availability of health information is also critical to effective healthcare delivery health informatics sys tems must remain operational in the face of natural disasters sys tem failures and denial of service attacks security also involves accountability which refers to people right to criticise or ask why something has occurred health information is also regarded by many as being among the most confidential of all types of personal information protect ing this confidentiality is therefore essential if the privacy of sub jects of care is to be maintained privacy involves access control versus any party not authorised to access the data and has been defined as the claim of individuals groups or institutions to determine for themselves when how and to what extent informa tion about them is communicated to others security and privacy in ehrs can be seriously threatened by hackers viruses and worms many reports of accidental loss or the theft of sensitive clinical data have appeared in recent years knowing the security and privacy features that ehr sys tems have could be critical if these risks are to be confronted and measures to increase the data protection of ehrs are to be adopted according to studies carried out in several countries concerns regarding data security and privacy have appeared a recent study estimated that each year there are million compelled authorisa tions for the disclosure of health records in the united states in studies conducted in denmark germany and new zealand respondents stated their data security concerns as regards ehrs citizens are also aware of the potential risks that shared ehrs may have in austria individuals can even decide whether or not their health related data should be shared with institutions and health care professionals in order to mitigate these con cerns organisations such as the certification commission for healthcare information technology cchit offer a certified pro gram which covers a rigorous inspection of among other things security aspects based on existing standards which is primar ily relevant for the united states cchit has been certifying ehr technology since providing access to ehrs is a vital next step in activating pa tients in their care and improving the health system however this opens new security threats there is a real concern about both people and entities access levels to patients ehrs a patient ehr might be fragmented and accessible from several sites by vis iting different doctors offices hospitals providers etc security defects in some of these systems could cause the disclosure of information to unauthorised persons or companies and health data therefore need protection against manipulations unauthor ised accesses and abuses which includes taking into account pri vacy trustworthiness authentication responsibility and availability issues ehrs also have difficulties in maintain ing data privacy to the extent that administrative staff could for example access information without the patient explicit con sent our objective is to perform a systematic literature review slr related to the security and privacy of ehr systems this article j l fernández alemán et al journal of biomedical informatics analyses security and privacy based on the iso standard in order to answer the following research question what security and privacy features do current ehr sys tems have we have carried out an in depth analysis of all issues related to the security and privacy features of ehr systems reported in pub lished literature using a comparative framework extracted from the iso standard it could be argued that ehr solutions pur chased from vendors often come with a pre set of privacy and security capabilities and this question can only be answered prop erly by analysing real solutions that are being used as ehrs nev ertheless we believe that if the security and privacy proposals found in published literature are identified and analysed they could be used as a proxy for what may or may not be the real ehr security and privacy proposals the review could be a useful contribution for stakeholders in the development implementation selection and use of ehrs this article is also intended for custodi ans who are responsible for overseeing health information secu rity together with security consultants auditors vendors and third party service providers methods systematic review protocol and registration this article has used a systematic review to ensure that both the search and the retrieval process have been accurate and impartial a systematic review is defined as a research technique that at tempts to collect all empirical evidence in a particular field to as sess it critically and to obtain conclusions that summarise the research the objective of an slr is not only to collect all the empir ical evidence of a research question but to support the develop ment of guidelines which can then be used by professionals this systematic review has followed the quality reporting guidelines set by the preferred reporting items for systematic reviews and meta analysis prisma group a review protocol describing each step of the systematic review including eligibility criteria was therefore developed before beginning the search for literature and the data extraction this protocol was reviewed and approved by one of the authors and is described in section eligibility criteria the following inclusion criteria were used articles published in english and articles that deal with the privacy and security in ehr systems only articles written in english were in cluded since this language is favoured by the scientific community in the publication of research studies finally was included to answer the research question information sources the search was applied to medline acm digital library wiley interscience ieee digital library sciencedirect metapress eric cinahl and trip database and was run between july and au gust we also scanned the reference lists included in articles in order to ensure that this review would be more comprehensive study selection the study selection was organised in the following four phases the search for publications from electronic databases related to health and computer science this phase was performed by a using the following search string electronic health record total of main security categories each category contains a and privacy or security which was adapted to the dat description of one or more security controls abases search engines we designed a template based on the requirements defined in exploration of title abstract and key words of identified articles the iso standard containing the characteristics to be ana and selection based on eligibility criteria lysed in each article these characteristics are related to five of complete or partial reading of articles that had not been elimi the eleven original security areas of the standard the justification nated in the previous phase to determine whether they should for considering these characteristics is as follows on the one hand be included in the review in accordance with the eligibility the protection of patients privacy can be achieved with two differ criteria ent techniques anonymisation and encryption on the other scanning the reference lists of articles to discover new studies hand with regard to the key security goals cia access control which were then reviewed as indicated in phases and but policy user access management and monitoring can significantly these articles had to satisfy the inclusion criteria help to ensure the confidentiality and integrity of personal health information we have additionally considered compliance the activities defined in the aforementioned phases were car with legal requirements owing to the importance of the applicable ried out independently by three authors any discrepancies were legislation and standards when dealing with sensitive data resolved by a fourth member of the team the study selection such as medical records finally education training and awareness was developed in an iterative process of individual assessments were selected because they are the greatest non technical until the interrater reliability was acceptable measures available for the purpose of security other important iso security areas such as information security policy organising information security asset management physi data collection process cal and environmental security information security incident management and information security aspects of business continu data collection was carried out by using a data extraction form ity management are outside the scope of this work each potentially relevant article was assessed by one of the authors table shows the areas and the security controls identified of the work presented herein who read the full text signifying that from the iso prefix refers to chapter of the standard only one reviewer extracted data while another checked it any in which these concepts are detailed along with the questions for disagreements were resolved through a discussion between the mulated to consider each security control security controls define two authors who had reviewed the study what is required in terms of information security in healthcare but not how these requirements are to be met our review attempts to understand this second issue data items the iso is a standard which has been specifically tai results lored to healthcare and which defines guidelines to support the interpretation and implementation in health informatics of iso study selection iec this standard addresses the information security man agement needs of the health sector implementing this guidance a total of articles were selected in the review the search of allows healthcare organisations to reduce the number and severity databases provided a total of studies although four were dis of their security incidents and to ensure a minimum level of confi carded because they were not written in english the title ab dentiality integrity and availability of personal health information stract and keywords of the remaining articles were examined this standard provides clear concise and healthcare specific guid and of these were discarded because they did not meet the cri ance on the selection and implementation of security controls for terion of the full text of the remaining studies was exam the protection of health information and is adaptable to the wide ined in greater detail articles were discarded because they did range of sizes locations and service delivery models found in not meet the criterion of and a total of articles were therefore healthcare the iso considers security areas containing included in the review another five articles were also included after table association between security and privacy categories identified in the iso and research questions area security control questions compliance compliance with security policies and standards and technical compliance what standards and regulations do ehrs satisfy data protection and privacy of personal information do ehrs use pseudo anonymity techniques information systems acquisition development maintenance are the users data encrypted access control access control policy what authentication systems are used user registration what access control models are deployed privilege management can access policies be overridden in the case of an emergency user password management if the system needs user roles who defines them who information access restriction grants access to the data communications and operations management policy on the use of cryptographic and key management health information exchange policies and what kind of information is exchanged procedures and exchange agreements audit logging are there audit logs human resources security information security awareness education and training of employees are the ehr users trained in security and privacy issues j l fernández alemán et al journal of biomedical informatics scanning the reference lists of these articles fig shows a prisma flow diagram in which this process is summarised study characteristics in this section we describe the most important features of the studies included in the review table in appendix a provides a summary of the privacy and security characteristics related to ap plied standards and regulations whether users data is encrypted pseudo anonymity techniques and system audit logs table in appendix a shows the characteristics related to authentication the access control models deployed who manages ehr access what occurs in the case of an emergency the training of ehr sys tem users and information exchange techniques compliance what standards and regulations do ehrs satisfy in total of the articles reviewed used standards or regulations concerning privacy or security to design their ehr systems of these studies use the health insurance portability and accountability act hipaa of and this is therefore the regula j l fernández alemán et al journal of biomedical informatics fig prisma flow diagram tion which is most frequently used the hipaa defines the privacy rules of usa health informatics the studies reviewed also included the eu data protection directive ec this directive which is employed in four articles regulates the protection of individuals with regard to the processing of personal data and the free movement of this data it is applied to personal data privacy in general and is therefore applied to ehr data other standards and regulations also appeared the role based access control standard of the american national standard for information technology the national institute of standards and technology nist rbac reference model the iso ts health informatics privilege management and access control the astm standard guide for information access privileges to health information the iso dts health informatics roles of persons the recommendations for the interpretation and application of the personal information protection and electronic documents act the recommenda tion r in europe on the protection of medical data the env health informatics secure user identification strong authentication microprocessor cards and the privacy code in new zealand do ehrs use pseudo anonymity techniques only articles present the pseudo anonym ity technique which allows third parties to access patients health data without disclosing patients personal data for example an identifier is shown rather than patients personal data of these eight studies propose collecting all of a patient ehr data with a patient identifier hash this identifier hash is a to ken which is derived from applying a hash function to the patient identifier a hash function ensures that it is difficult to compute the patient identifier from the token when this is on the net elger et al use an approach for reversible pseudonym generation the standard symmetric encryption algorithm aes advanced encryption standard was used to generate the pseudo nym integrity protection is also incorporated into the pseudonym in order to have proof that the pseudonym is unaltered before reverting it to support future inter clinic patients a dual pass pseudonymisation scheme was developed signifying that a pa tient identity will result in the same pseudonym regardless of which participating study centre collects that patient data quantin et al use a robust cryptographic hash function to anonymise information related to the patient identity a revers ible pseudonym generation method is used the authors propose a list of pseudonymous partial identifiers for each patient the risk of collision is solved by giving a linkage probability level high medium or low to each record this level is obtained from the probabilistic modelling performed on observed data riedl et al propose the possibility of sharing pseud onyms based on the threshold scheme of shamir and provide a mechanism with which to recover lost or destroyed keys alhaq bani and fidge define a pseudonym tree for each patient each patient can therefore have a different pseudonym in each health provider information systems acquisition development maintenance are the users data encrypted a total of articles re port that systems must encrypt ehr data in order to increase secu rity in addition to data identifiers pseudonyms keys and data attributes metadata are also encrypted eight articles use both symmetric key and public key schemes to store encrypted data seven articles store only public key schemes and eight articles store only symmetric key schemes while three of the latter use the aes a symmetric key algorithm adopted by the us government in encryption keys are en crypted and stored along with encrypted ehr or are stored in a separate database communications are securely encrypted and the server authentication using a mutually trusted certification authority is achieved via ssl secure sockets layer via tsl transport layer security or via other secure protocols three studies tackle encryption in the cloud haas et al and zhang and liu consider that patients should not trust that the cloud provider cannot access their ehr data par ticularly when the cloud provider is unrelated to patient or health institutions narayan et al propose the use of ciphertext pol icy attribute based encryption cp abe to ensure that the cloud provider cannot see or copy ehr data access control what authentication systems are used eleven studies propose the use of a digital signature scheme based on pki jafari et al use drm dig ital rights management to control access to ehrs by licenses two certificates are employed a security processor certificate that con tains a key pair which is used for the cryptographic authentication of the machine and is bound to its unique hardware features and a separate certificate called a rights management account certificate table number of selected studies by source of publisher source selected studies quality jcr international journal of medical informatics journal of biomedical informatics journal of medical systems computer methods and programs in biomedicine computers security ieee transactions on parallel and distributed systems computer standards interfaces methods of information in medicine electronic notes in theoretical computer science quality sjr it professional german medical science studies in health technology and informatics international journal of bio medical computing quality core ieee international conference on availability reliability and security b acm workshop on cloud computing security international e health networking application and services conference c annual hawaii int conf system sciences a international digital information management conference annual computer security applications conference a digital rights management workshop c international congress series acm international health informatics symposium c ieee international world of wireless mobile and multimedia networks wowmom symposium ieee international cloud computing cloud conference b ieee consumer communications and networking conference b pacific rim int symposium on dependable computing b world congress privacy security trust and the management of e business c amia symposium j l fernández alemán et al journal of biomedical informatics which contains a key pair used for the authentication of the user and is bound to the user unique identifier and email address other access mechanisms presented in the studies are username pass word login password combined with a digital certificate password and pin a smart card and its pin a smart card its pin and a fingerprint and access policy spaces daglish and archer use a username and a key by employing one of the following methods physical location as part of authentication the use of the web and a security certificate of a trusted organization hu et al propose a pki based authentication protocol but a biometric authentication system can also be embedded for stronger security authentication in distributed ehr systems has been also consid ered sun proposes cross domain authentication based on hier archical identity based public key infrastructure hib pki to take advantage of the benefits of identity based pki in entities from the two domains hib pki avoids certificate based pki induced costs such as revocation storage distribution and certificate verification van der linden et al propose two means of authenticating external access in inter organisational ehr systems user iden tification credentials are registered in the system thus implying a separate procedure to register the credentials the system relies on credentials that are issued by systems from a different organisa tion choe and yoo have designed a multi agent architecture which permits access to authorised users and the safe exchange of patients data based on web services zhang and liu advocate the use of anonymous digital cre dentials in healthcare clouds the authors use a signature scheme called a group signature to allow a member of a group to anony mously sign an ehr j l fernández alemán et al journal of biomedical informatics table summary of slr studies assessing applied standards and regulations whether users data is encrypted pseudo anonymity techniques and system audit logs authors year standards and regulations users data encrypted pseudo anonymity techniques audit log win et al hipaa a bit strong encryption is used to make interpretation and interference of information extremely difficult not indicated yes rostad and edsberg not indicated not indicated not indicated yes audit log is performed in order to define better access policies lovis et al not indicated not indicated not indicated not indicated agrawal and johnson european union directive on data protection of and hipaa commutative encryption de identification of personal health data using an optimal method of k anonymisation not indicated falcão reis et al eurosocap european directive ec personal data protection and the guidelines of oecd privacy protection encryption of data not indicated yes auditing who accesses ehr and with what aim röstad role based access control standard of american national standard for information technology not indicated not indicated yes accessible and understandable by patients kahn and sheshadri hipaa standard specification for continuity of care record ccr of astm encryption of data not indicated not indicated choe and yoo not indicated architecture that uses xml for user authentication data trustworthiness and selective encryption of patient data the user authentication is based on public key infrastructure pki and a certificate for encryption the rivest shamir and adleman rsa algorithm is used to protect the session key during session initiation between the client and the cac and the triple data encryption standard des algorithm is used for selective data encryption not indicated not indicated daglish and archer not indicated encryption of data not indicated not indicated benaloh et al authors detect the lack of interoperability standards the ehr system encrypts health records the authors propose a design for secure and private storage of patients ehr data hierarchical encryption system and partitioned record in which patient distributes keys for decryption of each part communications are securely encrypted via ssl both symmetric key and public key schemes are used to store encrypted data not indicated not indicated farzandipour et al hipaa encryption of data and communications not indicated not indicated hu et al hipaa encryption for health data confidentiality during storage and transmission hybrid public key infrastructure hpki solution in order to satisfy hipaa rules both symmetric key and public key schemes are used to store encrypted data not indicated yes van der linden et al 2009 cen iso ts astm and iso dts data encryption with the patient key not indicated not indicated elger et al data protection directive ec and hipaa authors use the symmetric encryption standard of aes algorithm and pseudonym generation systems dual pass in their project neurist they indicate that a hybrid security model is the basis of a combination of local and distributed models because each health institution has a security authors deal with pseudo anonymity method and a classification of pseudonym generation systems depending on whether or not they are reversible and the number of steps required to generate a pseudonym not indicated j l fernández alemán et al journal of biomedical informatics table continued authors year standards and regulations users data encrypted pseudo anonymity techniques audit log system an access right management and privacy protection policies according to user role authors argue some vulnerabilities of neurist project related to security assessment narayan et al hipaa recommendations for the interpretation and application of the personal information protection and electronic documents act ehr data are stored in cloud ehr data and metadata are encrypted using attribute based encryption abe scheme that uses public and private keys these keys are managed by trusted authority ta which can access all encrypted ehrs the safe keyword search is permitted by an encrypted scheme peks the data is encrypted using efficient symmetric key cryptography and the attribute based encryption is used to make the symmetric keys accessible to authorised users the private key is communicated to the users via a secure link such as ssl thereby preventing eavesdropper from learning anything about the key not indicated not indicated hembroff and muftic hipaa the recommendation r in europe and the privacy code in new zealand authors design an ehr system in which patient ehr data are encrypted and stored on health smart card a bit key length aes cipher is used to encrypt software on the samson card and aes cipher used within hardware components of smart card the piv applet which supports rsa encryption using public keys rsa signing using user private key and rsa key generation creates three pairs of rsa keys in the card not indicated yes system audits each reading update or deletion on each card and who performs them zhang and liu hipaa authors describe a security model for healthcare application clouds they consider that it is necessary to verify authenticity and trustworthiness for each ehr maintaining them encrypted and using ehr owner public key the ehr owner is the health professional that created it each ehr is linked to an identifier the patient has in order to maintain his her anonymity a hash table is used to provide authenticity integrity and non repudiation authors use an anonymous signature scheme called group signature threshold signature and a digital credential scheme yes person who accesses ehr is audited sun and fang hipaa patients health data are encrypted using the peks scheme the delegatee access right is restricted to only those data that are actively and effectively required not indicated yes the delegator delegatee policy servers and storage servers need to maintain an audit trail recording interaction histories faresi et al hipaa not indicated not indicated yes authors perform an audit log to comply with hipaa ardagna et al hipaa not indicated not indicated yes every access granted must be recorded to prevent or discover possible abuses later this process allows the supervisor to analyse access requests to identify common practice that should be explicitly permitted or denied by defining appropriate policies jafari et al not indicated the cryptographic key with which the content is encrypted this key is in turn encrypted with the recipient public key so that it can only be used by the particular user not indicated not indicated haas et al hipaa imia code of ethics for health information professionals and european data protection ehr data are transmitted through secure connections when the receiver authenticates them and not indicated yes authors propose to audit access to health data and its disclosure to third parties continued on next page j l fernández alemán et al journal of biomedical informatics table continued authors year standards and regulations users data encrypted pseudo anonymity techniques audit log directive ec are stored encrypted quantin et al not indicated all communications are encrypted asymmetric encryption algorithm patient anonymity is maintained throughout all communications each ehr in a hospital is associated with a patient id hash h id to maintain his her anonymity each health care institution must sign its ehrs to verify their authenticity and integrity not indicated to ensure transmission security confidential medical information such as the hashed patient identity h pi and the patient medical record are asymmetrically encrypted with the medical practitioner public key horvath et al all developers were trained in hipaa regulations regarding patient privacy all data transfers are subject to encryption via ssl certificates not indicated all data created in the course of a query are stored electronically on servers belonging to the duhs and all login activities in addition to the sql executed are logged in perpetual audit trails jian et al not indicated the phr data is kept secure by encrypting the zip files using a bit symmetric advanced encryption standard key not indicated not indicated dekker and etalle 2007 the hipaa act stresses that patients have the right to justifications of past disclosures of their medical records not indicated not indicated audit logic an a posteriori access control framework is used in an ehr setting once an action is executed its occurrence is safely stored in an audit trail along with time stamping of actions lemaire et al not indicated bit ssl encryption was used for all data transmissions encrypted passwords were used for all users not indicated service provider access to the system was logged and timestamped these logs included information on when a user viewed and edited client records and program information peleg et al legislation concerning healthcare privacy preservation us privacy rule which resulted primarily from the requirements of hipaa not indicated not indicated not indicated bos digital signature guidelines of the american bar association aba rsa is an algorithm for public key cryptography not indicated not indicated bakker not indicated not indicated not indicated in situations of medical audit law suits quality control and self assessment it may be necessary to be able to replay the ehr output as it was or would have been at a certain moment in the past jin et al not indicated not indicated not indicated not indicated tsiknakis et al the national institute of standards and technology nist rbac reference model not indicated not indicated audit control is necessary sucurovic 2007 medis to meet the requirements of cen env and cen env standards tripledes ssl and j for client server communication digital signature rsa the medis project implements web service security between the clinical and central server and ssl between the central server and a client not indicated not indicated kalra et al 2005 security policies and technical measures for the supervision and maintenance of the pseudonymous ehr repository as if it contained identified patient records in conformance with nhs and international standards including privacy enhancing techniques and methods to reduce the risk of re security of data transmissions pseudonymisation and depersonalization reliable identification and traceability what access control models are deployed a total of articles use access control models of which re fer to rbac which is therefore the access control model par excellence each of the users who access the system thus has a role which defines his permissions and restrictions one article presents a superset of rbac named sitbac model which de fines scenarios in which a patient data access is permitted or de j l fernández alemán et al journal of biomedical informatics 2013 562 table continued authors year standards and regulations users data encrypted pseudo anonymity techniques audit log identification ueckert et al not indicated ssl encryption is applied in the kind of storage and transport of data all data in the two databases are encrypted with symmetric keys user authorisation table only the version encoded by a one way hash function is stored akteonline has been split into two different logical databases the first database contains patient identification and demographic data name address etc and links it to an internally generated patient id but no sensitive clinical information the second contains the actual clinical information indexed by the patient id but without any personal data all of the data updates user and date are logged huang et al 2009 following hipaa guidelines the method is designed to protect recover and verify patients identifiers in portable ehrs a smartcard to input the key for the encryption and decryption work both symmetrical and asymmetrical algorithms are used de identification and pseudonymity the recovery process includes re identification and verification not indicated neubauer and heurix not indicated the client side cryptographic operations required for the challenge response style authentication procedure are carried out with a user owned secured contact micro controller smart card acting as secure keystore for the authentication credentials and as trusted client side cryptographic module both symmetric key and public key schemes are used to store encrypted data shared pseudonyms are shared between the data owner and the authorised person and are encrypted with both their inner symmetric keys so that both are able to decrypt this authorisation relation not indicated reni et al de facto standards not indicated not indicated an audit trail of all accesses to any clinical record is kept by marking the name of the person accessing the record in addition to the date and time regardless of whether any information is altered ruotsalainen 2004 not indicated not indicated not indicated audit of defined services pki ttp is performed france et al 2007 not indicated encrypted data and passwords an asymmetric cryptographic system is used not indicated needed to control emergency situations al zharani et al 2006 not indicated encrypted data not indicated not indicated yu and chekhanovskiy 2006 not indicated encrypted data with aes advanced encryption standard secure communication protocol using tls not indicated not indicated riedl et al 2007 not indicated data identifiers pseudonyms keys and data attributes metadata are encrypted secure communication protocol using tls both symmetric key and public key schemes are used to store encrypted data pseudonymisation a hash algorithm to compute a unique identifier for the patient is used not indicated bouwman not indicated encrypted data not indicated audit logs is included alhaqbani and fidge not indicated not indicated a pseudonym tree for each patient is defined each patient can have a different pseudonym in each health provider accesses are audited nied this is the outcome of analysing the scenarios that the authors had elicited via qualitative research the main concept underlying this model is the situation schema which is a pattern consisting of the entities along with their properties and relations ardagna et al introduce an access control system that reg ulates access to medical data based on policies that are modelled as a set of authorisations stating who can or cannot execute which ac tion on which resource policies can be further combined to define complex policies regulating access to resources their proposal is aimed at limiting accesses that break the glass by classifying a subset of these access requests as abuses or planned exceptions and by defining specific policies regulating them jin et al present a unified access control scheme that sup ports the patient centric selective sharing of virtual composite ehrs using different levels of granularity accommodating data aggregation and privacy protection requirements can access policies be overridden in the case of an emergency sixteen of the articles included in our review indicate that in the case of an emergency when the patient life is at risk it is necessary to over ride defined access policies two articles indicate that a security committee must verify access properness to ensure that the confidentiality of the personal health data in the health system is being respected hembroff and muftic propose two methods to ensure the patient safety and security in the case of an emergency the first method was created to have a read access open to anyone who is in possession of a card medical information for emergencies emer gency contacts allergies medications and existing conditions can be obtained using this method the second method is used when a patient cannot provide her pin for example when the pa tient is disoriented or unconscious in these cases fingerprint only authentication protocol along with another security smart card is sued to medical personnel that have proper emergency authorisa tion credentials to access patients medical data must be used ardagna et al propose the definition of four different policy spaces authorised accesses denied accesses planned exceptions and unplanned exceptions the planned exceptions space allows the definition of policies which are used to regulate emergency re quests that include all the accesses that are necessary to preserve patients health and are inherently different from normal routine rostad and edsberg analyse an installation of doculive epr a product of siemens medical solutions in this system which is used by many of the largest hospitals in norway there is an extensive use of exception based access control emergency access is assigned to users as roles and not all documents in the ehr are accessible through emergency access since there are no prede fined reasons for using emergency access the user always has to manually provide a reason the time interval in which the docu ment remains accessible after using emergency access is estab lished beforehand peleg et al presents an emergency authentication process in two levels the system verifies the authenticity of the emergency situation it authenticates a person responder for each activity in the mitigation process such that the person can assume the de fault situation role for the emergency based on his credentials if the system needs user roles who defines them eleven articles deal with who must define the roles i e patients or health organisations and what roles are created in an ehr sys tem of these eight studies pro pose that roles should be previously defined by institutions hospitals or some institutional committee although five articles propose that the pa tient should be able to include refinements or restrictions in order to customize the roles røstad affirms that a complete history in both user defined and system defined roles should be main tained to ensure that the user can perform auditing and examine who had what permission to access her information at any given time røstad presents an access control model for indivo in which a role assignment is labelled with a start time and an end time jafari et al suggest that the leader of the project should estab lish the roles of researcher or assistant in a health research facility j l fernández alemán et al journal of biomedical informatics 2013 562 who grants the access to the data a total of studies deal with ehr data access 62 of these articles indicate that the patient should grant access permissions 62 three articles indicate that access is granted by authorised health professionals narayan et al propose that health professionals should be able to delegate access to other health professionals if they have previously obtained the patient authorisation and faresi et al indicate that there should be data guardians i e doctors nurses who are responsible for the micromanaging of data deciding whether the patient preferences can be applied on the basis of the hipaa in one study patients and administrative staff define what kind of data are available to what kind of user and another de scribes two approaches implicit consent which signifies that the patient consents to predefined rules unless otherwise indi cated and explicit consent which signifies that the patient forbids access unless he grants it in two articles patients and health providers grant access to health information whereas in one article only health providers grant access to health information communications and operations management what kind of information is exchanged a total of studies show that health data are exchanged between organisations one of these by using the cloud pseudonyms and even policies can also be interchanged by using a script language choe and yoo propose a multi agent architecture based on web ser vices to access the authoritative data and to exchange patients data safely van der linden et al identify and analyse the privacy and security related to problems that occur when health data are ex changed between health organisations according to these authors a standardisation of security measures that goes beyond organisa tional boundaries is required such as global definitions of profes sional roles global standards for patient consent and semantic interoperable audit logs sun and fhan have designed an ehr system that allows data sharing between health institutions whilst protecting patient privacy their ehr system incorporates advanced mechanisms for fine grained access control and on demand revocation as enhancements to the basic access control provided by the delega tion mechanism and the basic revocation mechanism respec tively quantin et al have designed a search engine for a distributed database belonging to several health institutions the ehr system can gather together the different parts of a patient medical record that are located in different hospitals without any risk of breaching confidentiality this system allows patient ano nymity to be achieved in all communications are there audit logs twenty five articles 67 believe that it is important to include audit logs these logs included information on who accesses ehr with what aim and the time stamping of actions 74 two articles claim that the audit logs should be accessible and understandable to the patient four articles 67 indicate that it is necessary to audit the access when access policies are overridden in the case of an emergency in this case the patient is notified in order to avoid malicious use audit trail is also used to comply with the hipaa and existing laws to prevent or discover possible abuses later and misuse exception mechanisms and to define better access poli cies some authors advocate that audit logs should be accessible and understandable by patients falcão reis et al argue that each patient should have the capacity to monitor her own audit data and determine who has accessed her ehr what information has been accessed for how long and for what purpose patients should have information related to the creation of the record spe cific instances of how the record is used the process or processes by which the record is updated and eventually deleted haas et al propose auditing ehr accesses and all data flows in order to allow the patient to be able to verify whether privacy policies are being enforced in the case of unwanted information flow pa tients are able to identify the data source or leak the samson sys tem provides the patient with the ability to view audit logs which display names roles dates times and fields of data that have been read written or deleted when a physician initiates a break the glass procedure in emergency circumstances that phy sician data name national provider identifier name of health care organisation is copied from the physician card into the pa tient medical card audit log the architecture proposed by faresi et al includes a component named audit logger which logs every request response of patients health data patients can access audit logs of their records saved in a repository audit in distributed ehr systems is also tackled sun and fang propose maintaining an audit trail in order to record interac tion histories authentication delegation proxy signing search able public key encryption and retrieval and revocation between the ehr system and other entities zhang and liu propose maintaining a log of every access to and modification of data in their ehr security reference model for managing security issues in healthcare clouds human resources security are the ehr users trained in security and privacy issues staff training in security and privacy rarely appears in the arti cles reviewed four studies indicate that it is necessary three of these propose that only health staff training is necessary while falcão reis et al point out that system user training is necessary for both health staff and patients patients should be provided with a general education in their pri vacy rights and duties including clinical data privacy farz andipour et al also propose that all third party users of an organisation information should be trained in security proce dures specification and guidelines for handling patients medical records should include the requirements of using content encryp tion and secure key management solutions 69 healthcare profes sionals must also receive affordable security guidance kahn and sheshadri consider that the best solution for security compli ance in a digital ambulatory medical practice is to offer healthcare professionals educational tools with which to implement security policies and procedures quality assessment in order to assess the quality of the studies reviewed we have considered where they were published table in total of the studies reviewed were published in journals a total of arti cles were published in journals indexed in the journal citation re port jcr of which appeared in first quartile journals in appeared in second quartile journals in and ap peared in third quartile journals in three studies were pub lished in second quartile journals indexed in the scimago journal rank sjr in j l fernández alemán et al journal of biomedical informatics 2013 562 the remaining articles were presented in conferences according to the computing research and education core con ference classification which appeared in 2010 two articles were published in core a conferences five in core b conferences and another five in core c conferences six articles were presented in conferences which are not indexed but were published by ieee and acm and are in other ranks of computer science conferences as is shown above the quality of the review studies is high since of them appertain to top category journals and conferences discussion summary of evidence the main characteristics included in the studies reviewed are summarised and discussed below these characteristics answer our research question what security and privacy features do current ehr sys tems have compliance what standards and regulations do ehrs satisfy about half of the studies reviewed in this article are based on standards or regulations which shows that the application of stan dards and regulations is necessary in any ehr system that guaran tees the privacy and security of patients data the most frequently referenced regulation is the hipaa which is used in the us the hi paa is a federal law which protects health information and ensures that patients have access to their own medical records while giv ing new responsibilities to those in charge of protecting this infor mation another very important standard in europe is the cen iso part iv which includes privacy and security directives in chapter four this is an incipient standard that was developed by the cen in approved by the iso and updated in 2010 its objective is not to describe how ehr systems must be developed but rather to propose common rules for all in order to achieve interoperable ehr systems based on the findings of our study we have noted that the use of cen iso and iso is low this can be explained by observing that these standards were published in and thus from our point of view not en ough time has elapsed for it to have been widely adopted there are practical reasons for using standards including expectations of efficiency cost saving and risk avoidance since standards summarise the relevant aspects of security and privacy process clearly and concisely in a structured manner how ever the lack of a harmonised policy on trust privacy and con fidentiality and common security standards are important barriers for secure inner organisational and cross organisational communication new challenges in privacy and security emerge when genetic genomic data are collected for research purposes genetic genomic data access should also be treated as sensitive information in the ehr the personalized health care workgroup of the ameri can health information community has provided materials that discuss confidentiality privacy and security issues related to ge netic genomic test information in the ehr do ehrs use pseudo anonymity techniques de identification is the process of removing or modifying identifiers from the health personal data so that identification is not reasonably possible this technique is used to prevent the mis table summary of slr assessing authentication access control models deployed access management what occurs in the case of an emergency the training of ehr system users and information exchange techniques authors authentication access control models deployed access management case of an emergency user training techniques of information exchange win et al pin username password credentials not indicated not indicated not indicated not indicated not indicated rostad and edsberg not indicated rbac not indicated not indicated not indicated not indicated lovis et al use of a personal smartcard coupled with a personal identification number rbac access permissions are granted by health professionals roles are defined by an international committee but the patient is able to grant or forbid access to some health professionals in a future trans institutional network not indicated not indicated not indicated agrawal and johnson not indicated rbac the patient grants access permissions in accordance with health organisations policies not indicated not indicated not indicated falcão reis et al not indicated rbac roles are hierarchical following three rules assignment authentication and authorisation the patients grant permission to access their ehr health staff with a special role could bypass access policies in the case of an emergency yes system users in general not indicated röstad not indicated rbac there are two kinds of roles system roles patient provider and researcher and user roles defined by user the patients have the control over who accesses their data not indicated not indicated not indicated kahn and sheshadri not indicated rbac employees administrative staff health staff it staff roles are created by health care organisation individual and non shared identification not indicated not indicated yes the training of professionals is necessary not indicated choe and yoo the patients grant permission to access parts of their ehr not indicated not indicated a multi agent architecture based on web services is proposed to access the authoritative data and to exchange patients data safely daglish and archer digital certificate public key infrastructure pki is used to issue and revoke public keys and public key certificates rbac roles are assigned based on security policies of each hospital username and key using one of following methods physical location as part of authentication use of web and security certificate of a trusted organisation rbac previously defined roles researcher patient primary care second care emergencies and administration the patient may define refinement rules for roles patients and administrative staff define what kind of data are available to what kind of user not indicated not indicated not indicated benaloh et al username and password not indicated the patients share their ehr selectively and choose with whom to do so not indicated not indicated not indicated farzandipour et al not indicated not indicated not indicated not indicated yes staff training not indicated hu et al not indicated not indicated the patient grants access to health provider not indicated not indicated not indicated van der linden et al not indicated rbac creation is not indicated explicitly but it seems that roles are defined by health organisation although the patient can include restrictions to specific users there are two approaches implicit consent where it is assumed that patients have consented unless otherwise indicated and explicit consent where patients forbid access unless otherwise indicated not indicated not indicated not indicated elger et al authentication mechanism based on tokens rbac the patient grants access permissions but exceptions exist not indicated yes it is necessary not indicated narayan et al mutual authentication each user has a username used to define access policies that are managed by patient the patient adds and revokes permissions health professionals can delegate access to other health not indicated not indicated not indicated professionals hembroff and muftic not indicated health data are exchanged between organisations because the patient takes the card to different health institutions zhang and liu health card pin and patient fingerprint rbac the patients grant permission to access their data by entering pin and allowing fingerprints to be read not indicated access is allowed in the case of an emergency by using pin of health professional health card and patient fingerprint not indicated not indicated the data exchange is performed through secure connection ssl tsl or ipsec sun and fang 38 anonymous digital credentials in rbac roles are not indicated access permission is granted by healthcare clouds explicitly but are related to the authorised health professional kind of health professional usually the creator of ehr authors achieve data authenticity rbac authors use an approach roles define the function of the not indicated not indicated authors design a distributed ehr and integrity with the hierarchical based on roles to access kind of health professional that can system that allows data to be id based signature hids and delegation and revocation and access ehrs a health professional shared to permit institutions to standard id based signature ibs use proxy signature to achieve a delegates access to qualified cooperate and to protect data fine grained access control professionals privacy faresi et al not indicated modified rbac based on purpose not indicated not indicated not indicated of access and hipaa privacy regulations ardagna et al patients manage the access to their data the data guardians doctors nurses perform a micromanagement of data to decide whether patient preferences are to be applied based on hipaa regulations when the decision cannot be automated not indicated rbac access control using policy the patients should be able to to guarantee the care comes first not indicated not indicated spaces access their health data and to ccf principle it is necessary to manage access control by a proper provide a means to bypass the notification mechanism prior access control models in cases of approval and explicit consent emergency break the glass jafari et al not indicated rbac and licenses that indicate patients have to give their explicit not indicated not indicated authors propose use of digital access permissions leader consent for their data to be used in rights management drm to share researcher and assistant are the research by use of a list of ehrs in research scenarios roles defined keywords haas et al not indicated not indicated each patient declares privacy policies and checks that they are being complied not indicated not indicated quantin et al access policies are bypassed in the case of emergencies digital signature not indicated not indicated not indicated not indicated authors design a search engine for a distributed database between health care institutions horvath et al deduce authenticates using rbac when logging into the roles are pre established in the not indicated not indicated not indicated microsoft windows server system users must choose one of ehr system redmond wa usa active four user role types directory accounts as these are employees primary means of accessing workstations and clinical applications jian et al 62 the patients are given an initial password that allows them to access the files from the tmt viewer in addition to being password protected the zip file also contains a digital signature from the hospital and a checksum not indicated not indicated not indicated dekker and etalle not indicated persons unauthorized by the patient are not able to view the data authentication of users and not indicated not indicated not indicated not indicated not indicated objects an authorisation request or an authentication credential corresponds to a logical formula and the authorisation or authentication decision corresponds to a proof of the formula lemaire et al password and individual bit rbac the kind of access or roles roles are pre established in the not indicated not indicated not indicated continued on next page table continued authors authentication access control models deployed access management case of an emergency user training techniques of information exchange digital certificates were generated from windows server and installed in all service providers computers certificate authentication was performed upon log in access granted to client profile access granted to client profile data no access to client profile data ehr system peleg et al not indicated situation based access control sitbac model based on situations which are dependent on the context the same task to be performed by the same user can require different authorisations mentioned as a situation not indicated not indicated bos digital signature in information exchange not indicated not indicated not indicated not indicated the use of digital signature is performed in information exchange context between ehrs bakker not indicated not indicated not indicated not indicated not indicated not indicated jin et al not indicated unified access control scheme that supports patient centric selective sharing of virtual composite ehrs using different levels of granularity accommodating data aggregation and privacy protection requirements unified access control scheme that supports patient centric selective sharing of virtual composite ehrs using different levels of granularity accommodating data aggregation and privacy protection requirements in order to accommodate the emergency situations a break of glass policy bg policy for simplicity can be specified to allow staff in emergency rooms to access the patient medical information without the patient explicit authorisations not indicated not indicated tsiknakis et al not indicated rbac patients can grant access to their ehrs not indicated not indicated not indicated sucurovic digital signature pkc public key certificate rsa open source api for x authentication and authorisation xml as the language for developing constrained hierarchical role based access control rbac and at the same time has its focus on decomposing policy engines into components authorisation policy for hierarchy it defines hierarchies of how when where why and who attributes hierarchy of roles professions regions etc not indicated not indicated not indicated kalra et al login password assignment of grid access control levels not indicated not indicated not indicated not indicated ueckert et al password and pin granular access control mechanisms the user authorises persons or institutions to read and or write data in his personal ehr read access to an emergency subset of patient ehr can be enabled and defined not indicated the european union eu directive canada response personal information protection and electronic documents act pipeda and more recently the final privacy rule of the health insurance portability and accountability act hipaa in the usa which became enforceable on april huang et al authentication digital signature and login password not indicated not indicated not indicated not indicated not indicated neubauer and heurix login password rbac three roles are pre established in the ehr system the data owner the affiliated and the authorised person the patient who is in full control of her health data in that she can create data access authorisations for specific health records for authorised users in addition to granting full access rights equivalent to root access for affiliated users a private ticket toolkit is generated for each entry relying on an asymmetric keypair stored on an emergency card e g a relative card not indicated not indicated reni et al 74 login password rbac the chief medical officer cmo emergency read only access is not indicated not indicated defines the roles patient and cmo grant access to personal health information always allowed in the case of someone gaining emergency access a warning message is issued to the cmo who must then investigate to verify access properness ruotsalainen pki services for authentication not indicated to make it possible for external dynamic users to access any of the ehrs inside connected domains the platform offers automatic security negotiation services not indicated not indicated different health applications can run securely and exchange sensitive data by using pki france et al 67 digital signature and electronic identification card not indicated health providers grant access to health information the patient record is open from days before a planned admission to days after discharge there is a security committee to check that the confidentiality of the personal health data in the health system is being respected not indicated exchange of health information concerning identifiable patients by using digital signature al zharani et al authentication digital signature and login password access control module but type is not indicated not indicated not indicated not indicated not indicated yu and chekhanovskiy 69 authentication digital signature and pki rbac patients can grant access to their ehrs not indicated not indicated not indicated riedl et al user authentication using a security token that contains the access keys for example a smart card and pin rbac health provider defines the roles not indicated explicitly but it seems that the patient grants access permissions not indicated not indicated exchange of pseudonyms bouwman digital certificate rbac patients can grant access to their ehrs a specific role is defined for emergencies not indicated not indicated alhaqbani and fidge authentication digital signature and pki not indicated patients and health providers grant access to health information emergency access policy defined between patient and health provider is used not indicated information exchange between health organisations data and policies are interchanged by using a script language use of health data for example employers or insurance companies could use health information to refuse employment or health coverage pseudonymisation consists of transforming and then replacing personal data with a pseudonym that cannot be associ ated with the identification data without knowing a certain secret pseudonymisation therefore allows the data to be associated with a patient under specified and controlled circumstances thus allow ing both primary use of the records by health care providers and secondary use for clinical research although the confidentiality of medical data can be improved through de identification de identified data does not guarantee confidentiality complete anonymisation of the extract of clin ical records is unlikely to be feasible while the original clinical re cord exists the anonymity in ehr databases can be reversed through the disambiguation process nevertheless de identi fication should be as complete as possible such that the extract in isolation from the original clinical record would be reasonably anonymised important ethical and legal consequences are associated with de identification when using data for the purposes of research depending on the kind of research network and its requirements distinct procedures for pseudonymisation are appropriate 89 the one way single pass pseudononymisation method places non reversible codes on the data for research use and yet still al lows researchers to update the research record however there are cases in clinical research in which the research subject must be re contacted and reversible systems are therefore often pre ferred e g reversible single pass reversible dual pass a rigorous review of strategies for the generation and reversal of pseudonyms can be found in 90 data that is reversibly coded is viewed as personal data in the majority of countries and must therefore be protected indepen dent trusted third parties ttps could apply a code to the data and hold the pseudonyms the european medicines agency emea suggests that the ttp could be an external entity such as a governmental agency legal counsel or other qualified third party not involved with the research there is a growing body of literature investigating the risks of person re identification through data mining and probabilistic techniques and a similarly expanding set of algorithmic tech niques have been proposed for the profiling and monitoring of se rial queries and result sets to detect attempts to triangulate towards unique person characteristics a social engineering approach can also be used by an attacker to gain illegal access to the pseudononymisation algorithm or the patient list thus com promising the system new methods are being investigated and developed that can further protect identifiable information one innovative approach 97 aims to protect the data from being viewed by others while the user reads it a hiding function allows identifiable information to be made invisible thus preventing the disclosure of any per sonal and sensitive information while an ehr is being viewed the pre processing of common pattern matching dictionary and predefined area searches to recognise identifiers in the free form condition are used in the application of this method true anonymisation is challenging and further work is needed in the areas of de identification of datasets and protection of ge netic information and in those scenarios in which the ehr storage is in the cloud in order to prevent the cloud provider from being able to see patients data information systems acquisition development maintenance are the users data encrypted j l fernández alemán et al journal of biomedical informatics 2013 562 a proper encryption scheme must be introduced in order to achieve confidentiality in an ehr system both symmetric key and public key schemes are equally used in the studies reviewed in general public key operations are slower than symmetric key primitives and when searchability or hidden labels are required they appear to have inherent privacy weaknesses on the other hand if the server itself holds the private key then key symmetric schemes are also vulnerable as the key might be stolen along with the encrypted data encryption keys can be also hosted on a sepa rate physical server to prevent decryption of patient data if the data storage machine is ever compromised in most of the studies reviewed 38 67 69 the data is encrypted in the servers and the provider itself stores the encryption keys moreover in order to guarantee data trustworthi ness and authentication the review articles advocate that the health staff who create or update the ehr should sign it digitally however some proposals enable patients to generate and store encryption keys and patients can therefore control the access to their phi personal health information in these ehr sys tems each patient grants access to specific portions of his ehr data benaloh et al claim that if the server holds the decryption keys then it will be vulnerable to theft they therefore propose that each patient generates her own decryption key and uses that key to encrypt her records encryption schemes will guarantee that the patient privacy is protected assuming that the patient stores the key safely choe and yoo also propose the selective encryption of patient data to reduce the computational burden the encryp tion is then applied only to those items selected by the patient a number of desired features for appropriate key management are proposed the number of keys held by both patients and doctors should not be large the keys should be easy to store and consume low space complexity the updating of keys should be convenient and efficient in terms of time complexity none of the keys should contain any private information of any parties all the keys should be traced and revoked when they expire or when a user leaves the group when data volume is large e g data image asymmetric cryp tography may be inefficient this problem can be particularly se vere if health records with imaging data are accessed through a low performance computing device such as a portable digital assistant pda a symmetric cryptography method is thus ad vised for the purpose of efficiency this is the case of hu et al who present a hybrid public key infrastructure solution hpki to comply with the hipaa regulations they offer the option of using a cryptographically strong pki scheme for sensitive yet computa tional non intensive textual phi data while efficient symmetric cryptographic technology is deployed for the storage and transmis sion of resources demanding phi image data the hpki proposal is contract oriented a period of time rather than session oriented it should be noted that even with an efficient advanced encryption standard scheme and a powerful desktop pc encryption and decryption of a very high volume of digital medical images within low performance computing devices is still challenging prom ising results concerning encryption theory based on the features of medical images have been reported the most common communication protocol used to establish a secure connection is secure socket layers ssls this method guar anties the secure low cost end to end transmission of information over the potentially insecure internet implementing a fire wall and antivirus protection through security policies will further provide a more secure internet connection access control what authentication systems are used two kinds of authentication have been distinguished user authentication and data authentication user authentication can be defined as the way in which users prove their authenticity to the ehr username or identity id with an associated password have been the most common user authentication mechanisms in ehrs 103 the process used to ensure the origin of a data source is data authentication our findings reveal that the most fre quent data authentication method in current ehr systems consists of a digital signature scheme it should be noted that when physicians 104 patients or other authorised users are able to see or modify ehrs a privacy and security problem could occur if the user access data pass word or other access mechanism were to be stolen by unautho rized parties most ehr users believe that the password checking included in the system will ensure the system security of ehrs however only checking a password to ensure access restriction does not ensure adequate security for ehrs it has been noted that the use of passwords as authentication mechanisms is ex posed to multiple types of attacks such as electronic monitoring to listen to network traffic in order to capture information or unauthorized access to the password file in particular man in the middle mitm attacks are often implemented with authenti cating identity although most cryptographic protocols include some form of endpoint authentication specifically to prevent these attacks moreover passwords can be copied shared or cracked by using debuggers and disassemblers thus in addition to the password there should be some other mechanisms to enhance information security logins passwords have been superseded by other more robust methods the hipaa security guidance report advises using two factor authentication designers should therefore incorporate another authentication system in order to provide strong authenti cation two of the following three methods are recom mended for inclusion in an identification system something a person knows such as a login id email address password pin something a person has such as a key swipe card access card digital certificate or something that identifies a person such as biometrics face and voice pattern identification retinal pattern analysis hand characteristics or automated fingerprint analysis based on pattern recognition the insertion of an rfid radio fre quency identification chip is an invasive identification mecha nism but it can be also used for securing health information a sufficiently secure solution to user authentication is a creden tial system in which only the user who holds a legitimate creden tial issued by a trusted authority can gain access to the health record the user who has obtained a credential can perform cryptographic operations such as signing or decryption authentication solutions based on smartcards combining a to ken and a pin are also employed in the studies reviewed they have a fundamental weakness the presenter of the token cannot be authenticated tokens and pins can be lost and stolen biomet rics such as fingerprints can provide stronger authentication mech anisms however the mobile biometrics template embedded in the smartcard also runs a high risk of being compromised once the smartcard is stolen or lost recent research indicates that the fingerprint minutiae can be used to recover fingerprints it is currently preferable to use a central matching scheme for the biometric authentication in e health security systems owing to the fact that a physically secure organisational infrastructure can offer better computing resources cross organisation authentication can be addressed by means of hib pki or the use of federated identity management such as the liberty alliance project federation technologies provide secure methods for a service provider to identify users who are authenticated by an identity provider the security assertion markup language saml is a federation standard approved by the j l fernández alemán et al journal of biomedical informatics 2013 562 organisation for the advancement of structured information stan dards oasis and backed by the liberty alliance interoperability testing saml defines standardised mechanisms for the communi cation of security and identity information between business partners in order to enable information sharing among a network of healthcare organisations research should define extensible trust hierarchies and authentication standards 33 one solution con sists of creating central registries that can be used to grant or re voke credentials for a specific professional based on his professional behaviour however legal procedures for updating and querying the status of the credentials should be created what access control models are deployed the findings of this review have shown that the majority of the ehrs analysed used the rbac model thus confirming previous studies 113 in fact rbac is the most common access con trol model and is considered to be particularly well suited to health care systems an access control system designed to operate in the healthcare scenario should be flexible and extensi ble it should not be limited to a particular model or language should protect the privacy of the patients and should not allow the exchange of identity data in compliance with government leg islation one of the main pros of rbac is the flexibility it provides roles are assigned to users to allow them to interact with the system thus determining what resource can be accessed by the user and in what situations in order to update the permissions of many users it is only necessary to change the role to which they are all assigned an administrator is responsible for role creation certain models propose allowing the user some control over role assign ment and delegation 115 116 but not over role creation or adaption in contrast ehrs using rbac are not well suited to handling unplanned and dynamic events e g doctors asking for second opinions from colleagues or unplanned patient arrivals most of these ehrs therefore have exception mechanisms in addition to the normal role based access control to handle these situations however the use of exceptions leads to security threats and a need to perform regular auditing to ensure that the exceptions mecha nism is not misused in order to mitigate the complexity of rbac management it would be convenient for a healthcare worker with a specific role to have access to similar kinds of information in various systems a universally applicable model of role definitions that is adopted by all health organisations may be an important step forward the american astm standard 117 has defined an american list of roles iso dts defines a similar set of struc tural and functional roles which are referred to in the international labour organisation however these proposals do not pro vide definitions of each role and its policies rbac has been implemented in many commercial systems and an rbac standard has therefore been created to ensure that the main principles remain equal across different implementations 119 nevertheless limitations design flaws and technical er rors have been identified an alternative to rbac is attri bute based access control abac 121 this model for ehr offers context aware authorisation in order to provide the capabil ity to define different policies for different contexts which can be distinguished by contextual data or environmental attributes abac improves the efficiency of search and scalability of policy with regard to rbac rbac has been further extended to a contextual rbac and situation based access control sbac 122 can access policies be overridden in the case of an emergency access policies are bypassed in the case of emergencies when the patient life is at risk this is bindingly ruled by the eu direc tive ec and by the fourth part of the en standard which defines emergency procedures with which to override ac cess restrictions there are undoubtedly some situations in which the bypassing of access policies is justified for example ehr systems should be flexible enough to allow for the emergency treatment of minors in which the parent or legal guardian may be absent and the usual procedures for consent must therefore change in emergency situations policies that apply in normal circum stances can be overridden special roles in the ehr access control model are sometimes defined to deal with emergency cases all these overriding roles must be widely audited and their actions fully justified however there are some accesses that should al ways be prevented they cannot help in managing such emer gency situations and represent abuses that should never be permitted it should be noted that exception mechanisms increase the threats to patient privacy and their use should therefore be limited and subject to auditing performing an audit log is ex tremely important and it is crucial that the system user checks whether privacy and access policies are being enforced if the system needs user roles who defines them it is often the case that users very specific tasks cannot be mod elled by generic roles or policies adherence to the rbac mod el will result in a large number of roles unless an organisational policy to conform to generic roles is defined and rigorously imple mented several authors therefore advocate that roles can be defined or refined by the patient however care should be taken that the result does not interfere with good clinical practices and consensus on the definition of roles and profiles across organisations should be reached in distributed ehrs systems who grants the access to the data with regard to the principles guidelines and recommendations compiled by the oecd organisation for economic co operation and development protection of privacy and trans border flow of personal data within health information system development the patients are the owners of their health records and should thus have the ability to monitor and control which entities have access to their personal ehrs while a patient might wish to share her entire record with her doctor she might not wish to allow pharmacists billing staff or lab technicians to see any more infor mation than is necessary this issue is addressed in several studies that present a system in which a patient can grant access to specific portions of his health data using a system based on a hier archical encryption system or attribute based cryptography other works go one step further and allow patients to determine the time conditions under which the rights are granted moreover patients can be endowed with a policy manager to view express and alter privacy policies on the usage of their data and check their enforcement communications and operations management what kind of information is exchanged security and privacy issues can often be managed quite simply when dealing with data residing in systems in a single organisa tion but in the case of ensuring secure health information ex change across organisations these categories may be more challenging some authors advocate that complex environments such as health would benefit from a policy driven rbac this iso j l fernández alemán et al journal of biomedical informatics 2013 562 specification 126 extends rbac by defining a framework to represent and manage computable policy agreements between parties in order to exchange and use information the policy agree ments specify which information can be exchanged and under which security related circumstances in this context the term profile is used as the set of constraints regarding the permissions assigned to users usually represented through their roles and as a corresponding set of policies are there audit logs audit trails are an important tool for data security in ehr systems however audit trails are only a palliative mea sure since the confidentiality integrity of the information can be violated before countermeasures have been taken our re view shows that many systems rely on the auditing of log data as a security mechanism audit trails can serve as proofs when disputes arise regarding serious issues such as abuse of permis sions illegal access attempts and the improper disclosure of pa tients health data 38 many countries have brought regulations into force that make their inclusion mandatory such as the security rule of the hipaa which requires healthcare organisations to retain access logs for a minimum of some access control models insert exceptional accesses into an auditing log for their analysis a posteriori an auditing process al lows the supervisor to analyse access requests in order to identify common practices that should be explicitly permitted or denied by defining appropriate policies for example the analysis of ac cess logs could be a very useful tool for learning how to reduce the need for exception based access 77 audit trails can become a fundamental data security tool as some security breaches have resulted from the misuse of access privileges by authorised persons however one study 77 indicates that these log data are seldom used examining access logs is often an overwhelming task audit trails may not be practi cal since they can exceed the size of the original file by orders of magnitude hash chains are currently the most promising approach for storing authentic log files with a reasonably small overhead the workload can be reduced by distributing this task in or der to allow each patient to be responsible for auditing the ac cess log for his own record log data should therefore be accessible and understandable by patients event histories and an alert system can help the patient focus his attention on potentially illegal access to his data this system can mark entities or users that have accessed the patient data and were defined as being suspicious by the patient monthly reports along with statistics should appear as options in the service interface menu all features implemented in this service should be customised by the patient the current practice of auditing access logs involves identifying suspicious accesses to records based on known and simple pat terns many research articles that tackle the development of algo rithms modelling and the definition of information sources used to determine the appropriateness of access architectures for auditing systems and the application of business intelligence platforms can be found in literature human resources security are the ehr users trained in security and privacy issues all organisations that process personal health information should ensure that information security education and training along with regular updates in organisational security policies and procedures are provided to all professional health staff as recommended by the iso standard some authors consider that it is necessary to train health staff and patients in ehr system security and privacy in order to pre vent sensitive data from being exposed a security official should be responsible for all training activities hipaa has also estab lished a set of healthcare provider rules and regulations requiring that all employees in the entities covered defined as those organ isations that during the course of providing services come in contact with or use personal health records be educated in pri vacy however in the light of the articles reviewed security and privacy training fades into the background the reason for this resides in the fact that education is not considered to be as important for security and privacy as is for example creating security algorithms for anonymity authentication and access control the findings of a comparative study concerning the ehr infor mation security requirements of australia canada britain and the usa showed that it is critical that every computer user be aware of her information security studies using semi struc tured interviews with ambulatory care network and information systems leadership medical directors practice managers and ven dors perceived training as pivotal for successful ehr implementa tion in an academic ambulatory setting physicians have also expressed their concern as regards the introduction of medical errors resulting from complex technical capabilities and the level of technical support for clinical information technology solutions poor security and privacy train ing can affect care work in a study of clini cians situated in large australian hospitals fernando and dawson discovered that the clinicians did not understand how to use specific privacy and security implementations effectively and were not able to identify confidential information complex security controls such as fine grained rbac with emergency access may therefore make this problem worse thus leading to suboptimal patient care especially without the existence of appropriate training in the use of complex features such as break the glass ahima research indicated that of the organisation new employees were trained in privacy rules in house by the privacy or education officer the investment in training the work force both at the beginning of employment and during the job de creases potential risk and damage to the organization moreover standardised educational materials in relation to all ele ments of the ehr should be adopted limitations our study may have several limitations the comparative framework was limited to security controls belonging to of security areas identified in the iso standard the search was organised as a manual search process of several databases the search string may not have included words that would have selected other relevant studies the authors have only included articles in english signifying that these results must be considered within the scope of eng lish literature the authors have not included studies published after the search date one researcher extracted data from each article and another checked them the reviewers may have omitted relevant secu rity and privacy data the evaluation criteria used might not have been appropriate j l fernández alemán et al journal of biomedical informatics 2013 562 the authors reviewed articles published in scientific literature to discover how security and privacy are managed in an ehr one in depth endeavour by which to answer the question posed would be to analyse real solutions that are being used in ehr conclusions ehrs allow structured medical data to be shared between authorised health stakeholders in order to improve the quality of healthcare delivery and to achieve massive savings in these systems privacy and security concerns are tremendously impor tant since the patient may encounter serious problems if sensitive information is disclosed in this article we have identified and ana lysed critical privacy and security aspects of the ehrs systems based on the study of research articles from the articles in our review and based on the five security areas analysed we can conclude the following compliance eleven different standards and regulations related to security and privacy have been used in the ehr systems found in articles harmonisation is required to resolve possible inconsistencies and conflicts among standards information systems acquisition development maintenance various encryption algorithms have been proposed in articles it is advisable to use an efficient encryption scheme that is easy to use by both patients and healthcare pro fessionals is easily extensible to include new ehr records and has a reduced number of keys held by each party access control the preferred access control model in ehr sys tems is rbac the most common authentication mecha nisms are digital signature schemes based on pki and logins passwords in order to support patient empower ment 136 ehr systems allow patients to grant access to specific portions of their health data communications and operations management the recording of communications with the electronic health record system is found in articles it was observed that audit is partic ularly useful to identify suspicious accesses and common access practice human resources security health staff training in security and privacy rarely appears in only of the articles reviewed however it is recognised that educational programs which address issues of privacy and security for healthcare profession als and health organisations should be developed we have also perceived that most of the articles in the review defined ehr system security controls but these are not fully de ployed in actual tools an example of this has been provided by a recent survey conducted in spain which illustrates a gloomy state of patient information security of state hospitals do not include on their forms the standard legal wording which explains how and why patients data is stored more than of public hos pitals do not have measures in place to prevent unauthorised ac cess to patients data while they are being transported the number of state hospitals which do not carry out a security audit on their records is as high as centralised european and american health record systems will become a reality in the near future a centralised supranational central server which stores electronic medical records from differ ent health providers will make sensitive data more easily and rap idly accessible to a wider audience but this also will increase the risk that health data could be accidentally exposed or easily dis tributed to unauthorised parties security and privacy related issues are thus becoming even more important in such a cross organisational environment finally although communications in a wireless environment are not within the scope of our review the increasing use of wire less and internet technologies in healthcare delivery is an unques tionable fact a problem with security could therefore occur if the net is not protected when users access health data using wire less devices in the future we hope to carry out a systematic review concerning the privacy and security in wireless devices connected to ehr systems a b t r a c t mobile phones are becoming an increasingly important platform for the delivery of health interventions in recent years researchers have used mobile phones as tools for encouraging physical activity and healthy diets for symptom monitoring in asthma and heart disease for sending patients reminders about upcoming appointments for supporting smoking cessation and for a range of other health problems this paper provides an overview of this rapidly growing body of work we describe the features of mobile phones that make them a particularly promising platform for health interventions and we identify five basic intervention strategies that have been used in mobile phone health applications across different health conditions finally we outline the directions for future research that could increase our under standing of functional and design requirements for the development of highly effective mobile phone health interventions elsevier inc all rights reserved introduction in recent years researchers have increasingly begun to use mo bile phones as platforms for delivery of health interventions this research has targeted a wide range of health conditions and has come both from health sciences and from disciplines in computer science such as human computer interaction hci and ubiquitous computing text messaging interventions for example have been used to facilitate smoking cessation to provide diabetes edu cation to encourage attendance of primary care appointments and even to encourage sunscreen application mobile phone applications allow physicians to monitor patients with chronic heart failure and to detect early signs of arrhythmia or ischemia that can indicate an imminent heart attack in addition patients themselves can use phone applications and sensing and measure ment devices to keep track of their physical activities and to monitor physiological markers relevant to their health status our first goal in this paper is to map out the current state of this rapidly growing body of work rather than focus on a particular condition or health objective e g encouraging weight loss we present a taxonomy of the strategies and types of interventions that have been implemented with mobile phones this approach we hope will both help the readers to understand the design space of mobile phone health interventions and to construct new inter ventions by choosing elements that are most appropriate for their own application domains our second and related goal is to iden tify opportunities for improved and new interventions enabled by the ongoing developments in mobile phone technologies although when possible we highlight the outcomes from the reviewed interventions we do not offer a systematic review of effectiveness of mobile phone interventions the variety and com binations of intervention strategies and the diversity of the condi tions that have been targeted make it hard to determine which features of the mobile phone interventions used in different studies influenced their effectiveness consequently effectiveness reviews can be best done at the level of a particular pathology such as krishna and boren recent review of interventions for diabetes such effectiveness reviews are clearly needed but they must be more narrowly scoped than the work we take up in this paper the rest of this paper is organized as follows after describing our methods in section in section we briefly discuss the rea sons why mobile phones are a promising platform for delivering health interventions in section we review the mobile phone technologies that are commonly used to create health interven tions and the types of interventions that these technologies enable in section the main part of this paper we identify and provide examples of five general health intervention strategies for which mobile phones have been used and the types of mobile phone interventions that have been designed in service of these strate gies in section we discuss the opportunities for future work in this domain made possible by emerging forms of technology final ly in section we briefly discuss the limitations of this review corresponding author fax 5149 e mail addresses klasnja uw edu p klasnja wpratt uw edu w pratt see front matter elsevier inc all rights reserved doi j jbi 08 contents lists available at sciverse sciencedirect journal of biomedical informatics journal homepage www elsevier com locate yjbin methods the review we present in this paper is based on the literature identified through a search of pubmed acm digital library and ieee explore the main databases that catalog the research litera ture on mobile phone based health interventions we searched pubmed for the following terms mobile phone cell phone sms and text message the acm and ieee databases were searched for combinations of the term health and the terms listed above based on the abstracts retrieved through these que ries and through pubmed related articles feature we identified articles that described uses of mobile phones as a means of deliv ering health interventions we eliminated articles that discussed mobile phones in other contexts e g studies of the biological ef fects of radiation emitted by mobile phones for each article in the resulting set along with articles found through the citations in our search results we identified how mobile phones were used to deliver a health intervention what population was targeted and for articles that described an evaluation what results were ob tained we then iteratively clustered the interventions described in the literature until we arrived at the taxonomy described in this paper because our goal is to provide a review of the mobile phone de sign space rather than a systematic review of efficacy evaluations we present only illustrative examples of systems that use each type of intervention that we identified thus although our review covers every intervention strategy and type that we identified in our search it does not touch on every mobile phone intervention found in the literature why use mobile phones for health interventions mobile phones are a particularly attractive avenue for deliver ing health interventions because of the widespread adoption of phones with increasingly powerful technical capabilities people tendency to carry their phones with them everywhere people attachment to their phones and context awareness features enabled through sensing and phone based per sonal information we briefly review these reasons below first over the past years mobile phones have become ubiq uitous according to the latest report from the pew internet american life project of american adults have an active mo bile phone of these over a third use their phones not only to make calls and send text messages but also to access the internet and this trend is accelerating according to the data from international data corporation the worldwide market for smart phones feature rich phones capable of running third party appli cations such as iphones and android devices has grown by in the last year these numbers indicate not only that the adop tion of mobile phones has become nearly universal but also that the market is rapidly shifting toward phones that are de facto pock et computers powerful computational devices that can access the internet and run a variety of complex applications the deep pen etration and technical capabilities of modern phones make sophis ticated phone based health interventions appealing and widely applicable second unlike desktop computers or even laptops mobile phones are nearly always with the person many of us are rarely more than a few feet away from our mobile phones and more of ten than not they are in our hand or in our pocket or purse a study found that even in 2006 individuals were within arm reach of their phones on average 58 of the time as fogg noted we spend more time with our phones than we do with our partners or even at our workplace the fact that the phones are so close at hand makes it possible to drastically increase the number of p klasnja w pratt journal of biomedical informatics points of care from clinics in standard practice and patients homes in traditional telemedicine to nearly any place and time when the patient needs support third people relationships with their mobile phones are often deeply personal phones are customized with user selected ringtones and notification sounds with images of loved ones and with cases and decorations that express their owners style mobile phones are also used for a range of activities throughout the day from calendaring and email to social networking financial track ing and playing games as a result phones often contain highly personal information including pictures intimate text messages and financial information the personal nature of mobile phones can reduce the barriers to adoption and increase acceptance of phone based health interventions by integrating health education and other forms of health promotion with a tool that is an integral part of individuals daily routines and to which they often have po sitive emotional attachment finally the combination of their technical capabilities and the proximity to their owners means that phones can know a great deal about the users current situation through embedded sensing such as gps location tracking accelerometer based exer cise detection and access to the user calendar contacts and other personal information mobile phone applications can infer where their users are and what they are doing this knowledge in turn makes it possible to create just in time interventions that provide users with support at times when that support is most needed such interventions can be particularly useful for discouraging unhealthy behaviors such as eating unhealthy foods or smoking where target behaviors are often strongly tied to particular contexts e g snacking while watching tv at night the ability to detect such contexts could enable us to provide users with assistance before they engage in such unhealthy behaviors as well as to provide post facto feedback potentially greatly increas ing the effectiveness of behavior change interventions technology behind mobile phone health interventions not all mobile phone health interventions rely on the same type of technology technical capabilities of mobile phones vary widely from the support for only voice and text messaging on ba sic feature phones to the support for third party applications sens ing internet access and wireless connectivity with other devices on smartphones such as iphones blackberries windows phones and android phones health interventions have taken advantage to a greater or lesser degree of all these capabilities in this section we provide a brief overview of the technical features of mobile phones and ways in which those features have been used in health interventions text messaging sms text messaging short message service or sms is a service that enables character messages to be sent from one phone to vir tually any other mobile phone in the world because it is supported by even the simplest mobile phones sms is widely used not only in the first world but in the developing countries as well where mo bile phone penetration has reached this widespread use makes text messaging the most universally accessible technology for phone based health interventions and possibly the most acces sible way of delivering health interventions of any kind beyond its accessibility text messaging has seen broad adop tion as a health technology for two other reasons first text mes sages are a push technology allowing intervention messages to be delivered without any effort on the part of the recipient as such text messages are often used for sending reminders e g tips and other educational content e g and for main taining users awareness of their health goals second because text messages can be sent and received by both phones and com puters text messaging provides a way for users to log their health related activities and physiological parameters e g exer cise peak flow in asthma management etc and to receive customized feedback based on these data the ability to process text messages automatically makes it possible for the complete information exchange in an intervention to be done via sms users can be sent reminders to log relevant data they can reply to remin der messages with the requested information and the system can process these responses and send users feedback customized to their current situation for example haug et al smoking ces sation intervention for young adults uses precisely this kind of sms information loop the diversity of these interactions provides evidence for just how flexible text messaging can be as a system for delivering health interventions cameras in recent years cameras have become a standard feature on all but the most basic mobile phones the quality of phone cameras is still below the quality of dedicated digital cameras but their con stant availability makes phone cameras a useful tool for collecting health related data throughout the day so far health interventions have used cameras in three primary ways as an alternative way to journal health related behaviors such as food consumption as a way to provide healthcare providers with addi tional information about a condition such as the appearance of psoriasis lesions and as a way to document circumstances relevant to the self management process such as the contextual factors that might influence diabetic patients ability to effectively manage their blood glucose levels although photos cannot be automatically processed as easily as text can in cases where a goal of an intervention is to support reflection or learning through ac tive engagement with the user data e g phone cameras can be a valuable tool for low effort collection of health related information native applications all major smartphone platforms ios android symbian black berry webos and windows phone provide developers with pro gramming interfaces apis that can be used to build special purpose applications the apis provide access to interface controls e g dialog boxes menus calendar pickers to the phone hard ware features e g accelerometers cameras and to other data and applications on the phone e g contact list calendar email enabling creation of complex and sophisticated applications researchers and commercial companies have leveraged these capabilities to build several different types of health applications journaling applications that enable users to log and chart data about their diet exercise blood glucose levels and other health related behaviors and measures patient ter minals for telemonitoring of conditions such as hypertension and chronic heart failure applications that receive data from pedometers blood pressure monitors and other devices games that teach health related skills and so on we will review many of these application types below automated sensing nearly all contemporary mobile phones can connect to sensing devices over bluetooth or another form of personal area network ing in this way mobile phones can connect to digital scales blood p klasnja w pratt journal of biomedical informatics pressure monitors glucose meters portable electrocardiograms ecg pedometers and gym equipment among other sensors mo bile phones connected to such devices can act as receivers and data stores for the collection of a variety of health intervention data in addition over the last years an increasing number of mo bile phones have begun to ship with sensors such as accelerome ters and gps built in enabling detection of users behaviors even without the use of an external device for example the iphone application runkeeper http runkeeper com uses built in gps to automatically track how far users run or cycle to create maps of their exercise routes and to calculate how many calories were burned during these workouts the trend toward sensors built into mobile phones will likely increase user acceptance of health inter ventions by freeing users from the need to keep track of charge and wear an additional device internet access finally one of the most important capabilities of mobile phones from the standpoint of health interventions is their ability to use the cellular network to connect to the internet from nearly anywhere this always on connectivity means that users data such as blood glucose levels or peak flow readings can be uploaded to providers servers as soon as they are captured thus enabling early detection of critical events the data can also be uploaded to websites where users can easily view chart and edit their infor mation lastly always on connectivity makes it possible to include web pages and online audio and video as part of phone interven tions the use of online resources makes it easier to keep the con tent of an intervention up to date without requiring users to repeatedly install updated versions of the application design space of mobile phone health interventions researchers have taken advantage of the technical capabilities described above to develop a wide range of different health inter ventions some interventions are relatively simple for example a number of interventions use sms to send reminders to patients about upcoming appointments but many are complex both in terms of the technology they use and the intervention strategies on which they rely in this section we aim to map out the design space of mobile phone health interventions by identifying the intervention strate gies that have been used in phone based interventions and the specific forms that the interventions embodying these strategies take specifically we have identified five key intervention strate gies tracking health information involving the healthcare team leveraging social influence increasing the accessibil ity of health information and utilizing entertainment table summarizes examples of these intervention strategies and phone features used the rest of this section reviews the five intervention strategies in depth tracking health information at the core of many mobile phone health applications is a single strategy using the phone to track health related behaviors phys iological states symptoms and other parameters relevant to health tracked health data has many uses but just the process of tracking itself often referred to as self monitoring can provide many benefits including increased frequency of desired behaviors and decreased frequency of undesired behaviors 35 38 better understanding and awareness of one behavior and health pat terns e g and opportunistic engagement in desired behaviors in mobile phone health interventions self monitoring has been implemented in three primary ways through native journal ing applications via text messaging and through automated sens ing and recording native journaling applications native journaling applications are custom designed to support logging of one or more health related behaviors e g physical activity or food intake and relevant measures e g blood glucose levels blood pressure or stress levels they also typically provide ways to chart these data to foster reflection on trends over time for example ubifit an application for encouraging physical activity enables tracking of cardiovascular exercise strength train ing different types of stretching as well as of other activities that users consider to be physically demanding such as scrubbing floors or chopping wood ubifit users can see daily and weekly views of their data as well as their proximity to their weekly phys ical activity goals similarly wellness diary a journaling application developed by nokia research supports logging of a wide range of health related states and activities including among others physical activity food intake weight mood blood pressure stress and amount of sleep each of the measures can also be charted so that users can see trends in their data over time in the context of chronic disease management kollmann and colleagues developed a java mobile phone application called diab memory to enable diabetic patients to self monitor blood glucose levels insulin use intake of carbohydrates and physical activity the phone application synchronizes with a website where users can chart their data in different ways to better understand how various factors affect their blood glucose levels similarly an intervention by walters et al uses wellness diary to help pa tients who are undergoing cardiac rehabilitation to monitor their physical activity diet and risk behaviors such as smoking and alcohol use the data can be charted on the phone and on the com panion website wellness diary connected to which they are auto matically synchronized table examples of mobile phone health interventions broken by intervention strategies and phone features that they employ phone feature intervention strategy text messaging cameras native applications automated internet access sensing tracking health information holtz and whitten anhøj and møldrup franklin et al and haug et al mamykina et al kollmann et al walters et al kim and kim and park et al involving the healthcare team brown et al mamykina et al mamykina et al mamykina et al denning et al lee et al denning et al and mattila consolvo et al and consolvo et al mattila et al gasser et al et al kollmann et al 40 and walters et al obermayer et al haug mamykina et al mamykina et al rubel et al rubel et al et al holtz and and schreier walters et al farmer mamykina et al mamykina et al whitten franklin et al et al kearney et al farmer et al farmer et al et al kim and kim and larsen et al villalba et al villalba et al and park et al and morris et al 62 and scherr and scherr et al et al leveraging social influence obermayer et al consolvo et al franklin et al roger et al riley et al 74 and whittaker et al increasing the accessibility of health information gasser et al and consolvo et al wangberg et al consolvo et al and consolvo et al armstrong et al leong chiu et al and chiu et al et al 33 roger et al gerber et al curioso et al strandbygaard et al and castaño and martínez utilizing entertainment armstrong et al and deshazo et al and rogers et al grimes et al p klasnja w pratt journal of biomedical informatics studies suggest that self monitoring applications can have posi tive effects on users health kollmann et al month feasibility trial of diab memory with ten diabetes mellitus type patients showed that the system helped the participants achieve a statisti cally significant improvement in levels from to p 02 as well as a slight albeit not significant de crease in blood glucose levels 40 similarly the initial study of the wellness diary indicated that phone based journaling could be maintained over months and that active use of the sys tem helped individuals lose weight designers of journaling applications can take several ap proaches to recording of health information each of which pre sents a different set of trade offs with the mobile lifestyle coach for instance users log their physical activity and diet using a simplified point system a user gets one lifestyle point for min of moderate or vigorous physical activity or one serving of fruit or vegetables the system thus allows tracking of a variety of physical activities and foods without requiring effortful logging of every single type of activity or food individually similarly with wellness diary users can log their food intake by selecting from a simple list that includes light snack heavy snack light meal and heavy meal the convenience and speed of entry afforded by such simplified journaling is achieved at the cost of precision of the logged data although wellness diary diet log ging might be sufficient to help users get a better sense of their overall eating patterns it is not sufficiently detailed for instance to help diabetic patients to understand the effect of different foods on their blood glucose levels other applications such as pmeb and balance take a different approach to journaling they incorporate caloric infor mation for a variety of foods and activity types enabling users to more accurately monitor their daily caloric expenditure but at the cost of higher journaling effort a variety of commercial appli cations such as lose it http www freshapps com lose it take this same approach of supporting detailed logging the decision to adopt detailed or simplified journaling depends on several factors including how granular the data need to be how many parameters need to be tracked and how long tracking needs to continue finally native applications do not need to rely on textual entry for journaling a number of systems attempt to reduce the effort involved in self monitoring by enabling users to record their activ ities by taking photographs with the phone camera brown et al system for encouraging physical activity and healthy diet in college students allows users to use the phone camera to take pic tures of their food and activities throughout the day to increase users awareness of their eating and exercise patterns these cap tured images are automatically shown in daily and weekly timeline views displayed on the users phones and computers similarly both wellness diary and the diabetes management system mahi let users take pictures with the phone camera to log their food intake such photo based journaling enables quick capture of com plex data e g a meal consisting of multiple foods but makes it more difficult to graph the data and to automatically provide users with feedback on their activities and health parameters tracking through text messaging in addition to dedicated journaling applications self monitor ing on mobile phones can also be implemented using text messag ing the nature of sms limits how complex tracked data can be making this strategy feasible only for conditions where individuals need to log a small set of well defined parameters within these constraints however sms can be a useful medium for self moni toring because processing text messages on the server makes it possible to automatically analyze logged data and provide users with feedback on their activities and measures one area where sms based self monitoring has been success fully used is in chronic disease management in holtz and whit ten asthma intervention participants use sms to submit peak flow readings every morning the system sends participants automatic feedback on whether the submitted readings are within a normal range and it provides a web interface where they can see and chart all of their submitted data participants in holtz and whitten pilot study gave the sms asthma monitoring system high satisfaction scores and expressed that they felt that the sys tem provided them with an effective way of managing their condi tion anhøj and møldrup asthma intervention uses a similar strategy participants are sent three text messages a day and they reply to these messages to log their peak flow readings medica tions they have taken and any sleep interruptions they had the previous night sms has also been used for self monitoring in diabetes users of the sweet talk system can use text messaging to log their glu cose level readings text messages sent by patients are processed by the sweet talk web application and the reported values are added into the user journal if a text message contains text in addition to a numerical value representing the glucose reading the message text is added to the reading as a note by accessing the website users are able to see charts of submitted data and read any associated notes for conditions where tracked information can take the form of simple numerical values such as glucose levels and peak flow read ings text messaging provides a lightweight way of implementing self monitoring although limited in scope the low cost high prevalence and low technical requirements of sms make it a good match for large scale deployments of self monitoring systems automated sensing for tracking one of the biggest downsides of self monitoring has always been the effort involved in tracking one activities for that reason individuals rarely manage to keep up with traditional paper based self monitoring for longer than a few weeks 38 one way to re p klasnja w pratt journal of biomedical informatics duce that effort is to use sensors and connected measurement de vices to automate logging of at least a subset of relevant activities and states wellness diary automates recording of how much the user walks by connecting to a pedometer ubifit uses the mobile sensing platform a pager sized wearable sensing de vice to automatically detect walking running biking using a stair machine and using an elliptical trainer as long as the msp is turned on and is worn on the user waist these activities are de tected without any further user involvement they are then trans ferred to the phone over bluetooth and added to the user activity journal results from a month field study of ubifit indicated that automatic activity detection was very positively received although study participants disliked having to wear a bulky sens ing device the follow up work by saponas et al has shown that recognition of a similar range of activities can be achieved through the use of accelerometers built into mobile phones obvi ating the need for a second device the use of such embedded sens ing could help increase user acceptance of interventions that incorporate activity detection because it removes the need to wear charge and manage an additional sensing device in addition to automatic detection of health related behaviors a number of phone based health applications can connect often wirelessly to devices for measuring and uploading physiological data devices that support external connections include digital scales e g glucometers e g blood pressure cuffs e g and even sophisticated portable electrocardiograms con necting such devices to a mobile phone reduces the effort involved in transferring readings into a self monitoring application and it decreases errors that can occur during manual transfers although the use of sensing holds great potential for enabling long term self monitoring interventions it is not a panacea mis calibration and incorrect use of measurement devices can intro duce errors into the log similarly automatic activity detection can fail both by not detecting or inaccurately detecting activities that the user has performed and by erroneously detecting activities that the user has not done 28 other challenges remain as well finding ways to indicate to users the level of uncertainty of de tected information and the ability to correct incorrectly detected activities is important for designing interventions that users can trust and are willing to continue to use similarly finding ways to automatically summarize large sets of sensed data as well as to collect and present data in ways that respect users privacy and levels of comfort with being monitored will be important for wide adoption of such systems self monitoring is an essential component of a large number of health interventions from applications that facilitate health behavior change to those that help with symptom monitoring in chronic conditions mobile phones can ease the effort involved in self monitoring by putting sophisticated journaling applications close at hand using photos to quickly document complex behav iors and using measuring devices and sensors to automate logging of behaviors and physiological states involving the healthcare team although tracking health information can provide useful sup port for encouraging health behavior change and for chronic dis ease self care effective management of many conditions requires support from the patient healthcare team keeping the healthcare team informed of the patient symptoms activities and physio logical parameters can greatly enhance how effectively and rap idly care can be provided in such circumstances two types of interventions remote coaching and remote symp tom monitoring leverage mobile phones to keep the healthcare providers informed of the patient condition and to facilitate provider patient care interactions a third type of intervention automated feedback allows indirect clinician feedback through automatically activated rules or guidelines that people with clini cal expertise can define we review these three types of phone interventions next remote coaching remote coaching interventions use the data that patients col lect on mobile phones to enable learning interactions in these interventions tracking data is uploaded from the phone to a web site where the data is reviewed by a member of the patient healthcare team based on the data clinicians then work with pa tients to help them learn to manage their conditions more effec tively the coaching interaction itself typically takes place in one of three ways over the phone via sms or through a website one common strategy is for the data to be collected using a phone based journaling application or sms but for the clinician patient interaction to take place over the phone for example farmer et al describe a mobile phone application for young people with diabetes patients used the application to upload their glucose readings from a bluetooth connected glucometer and to annotate these readings with information about what amount of insulin they injected what they were about to eat and how phys ically active they planned on being in the next few hours based on the data a nurse worked with each patient during weekly phone calls to discuss the patient particular issues for example how much insulin the patient should be taking at night or how different types of meals seemed to affect her glucose levels a randomized controlled trial rct with 94 type patients showed that the intervention group decreased their levels significantly more than the controls a similar intervention by walters et al used the mobile and web versions of the wellness diary to provide coaching around health related activities and goal setting for pa tients undergoing cardiac rehabilitation in other interventions feedback on the patients tracked data is delivered using text messages although this modality cannot pro vide the same kind of rich interaction that is possible over the phone text messaging can deliver to patients highly specific and tailored feedback that can help them manage their conditions more effectively for example in kim and kim intervention for obese patients with type diabetes patients used a mobile phone or a web browser to log their glucose readings insulin doses and the amount of medication they were taking a diabetes educator reviewed these data once a week and sent patients cus tomized feedback using text messages and the website the sms feedback was highly specific kim and kim write that examples of feedback included messages such as please decrease the long acting insulin by two units and please add one tablet of sulfonyl urea in the evening p their results show that these brief coaching messages can be very effective the intervention group in their study had significantly better levels than the controls at and follow ups with the glycemic control continuing to improve over that whole period park et al have recently used a very similar intervention with obese patients with hypertension also with positive results the sweet talk system for young adults with diabetes also includes a sms coaching component in sweet talk users are able to send questions to their healthcare team via sms these questions are forwarded to the best person to answer them e g a doctor or a dietician who then writes a response that gets forwarded to the user phone as a text message as with kim and kim study a year long rct of sweet talk showed that sms based coaching interventions can have meaningful clinical impact over the course of the sweet talk trial the intervention arm significantly decreased their levels from to p 001 while the standard treatment arm showed no improvement p klasnja w pratt journal of biomedical informatics 198 finally in some interventions clinician patient interactions take place through a web application for example mahi mam ykina et al mobile phone application for people who are newly diagnosed with diabetes lets users upload glucose readings and audio and photo diary entries about their diabetes manage ment to a secure website on the website patients can reflect and comment on their data and can discuss the data with a diabe tes educator mamykina et al report that many of their partic ipants found the ability to get feedback about the concrete self management problems they were struggling with to be the most valuable aspect of mahi the use of photographs worked well for this purpose similarly an earlier intervention by smith et al showed positive results for the use of photographs to foster reflection in diabetes self management for visual data such as photos a web application can provide a much richer interaction environment than can text messages or even a phone call at the same time a mobile phone provides a nearly perfect tool for cap turing such data enabling quicker and richer data acquisition than is possible through traditional journaling each of the three modalities for providing coaching feedback has its own benefits and downsides telephone calls are highly interactive enabling in depth discussions and problem solving consequently they can be very effective for helping patients to de velop self management skills however phone calls are labor intensive for healthcare providers and require coordination of pa tients and clinicians busy schedules sms can be effective for rou tine troubleshooting adjusting the dose of a medication providing tips for changes in specific daily activities etc but they are less well suited for helping newly diagnosed patients to learn how to manage their illness or to find solutions for persistent self management problems online interactions through forums and secure messaging such as those used by mamykina et al might provide a happy medium in this regard although they can be labor intensive the asynchronous nature of web based commu nication makes such interactions more flexible than telephone calls the ability to share multimedia content and links to educa tional and problem solving resources makes such web based clini cian patient communication very well suited for facilitating self management skills while also supporting brief interactions when those are sufficient for ongoing health management mobile phones enable effective remote coaching interventions that can help chronic disease patients to learn a range of complex skills that they need to manage their condition phones serve this purpose both by enabling patients to collect data about their activ ities and physiological measures and in sms coaching interven tions by providing a way for them to receive concrete tailored feedback about how to manage their illness more effectively remote symptom monitoring in addition to remote coaching mobile phones are commonly used to monitor patients health and to alert the healthcare team if dangerous symptoms develop timely detection of such symp toms and worrisome physiological parameters is key to prevention of potentially serious deterioration in patients health thus many interventions use data captured with mobile phones to look for such indicators and to alert appropriate healthcare providers depending on the condition symptom monitoring applications can take advantage of sensing self report or a combination of the two for monitoring patients with heart disease for example phys iological data are key and remote monitoring applications often connect to devices that can measure and upload these data auto matically for instance villalba et al system combines a mobile phone application and a variety of bluetooth connected de vices a blood pressure cuff respiration and ecg sensors embed ded in an instrumented shirt accelerometers and a digital scale to enable chronic heart failure patients to collect data about their health status patients data are automatically uploaded to a server and are continuously monitored for signs of decompensa tion if decompensation is detected the patient physician is immediately notified by pager and email similarly rubel et al system uses a mobile phone connected to a portable device with an electrocardiogram heart disease patients use the sys tem called pam to monitor the condition of their heart over time every time a patient records an ecg reading the pem soft ware compares the new reading to the person baseline and recent history ecg readings outside the patient normal range generate alerts and if the system determines that a heart attack is imminent both the patient physician and the nearest emergency call center are immediately notified patients can also report their symptoms using self report ques tionnaires built into a phone application for example an applica tion developed by researchers in the uk 56 facilitates monitoring of toxicity symptoms in cancer patients undergoing chemotherapy the system uses phone based questionnaires to as sess six toxicity indicators nausea vomiting mucositis hand foot symptom diarrhea and fatigue patients responses are uploaded to a server and if the software determines that reported symptoms are potentially alarming the system generates amber or red alerts to the patient healthcare team patients who use these symptom monitoring systems consis tently report feeling reassured with the knowledge that their health is being monitored 58 and with the fast response from the healthcare team in cases when the system detects that some thing might be wrong 58 in addition scherr et al work with chronic heart failure patients has shown that such sys tems can have measurable positive effects on health and clinical care chf patients in scherr 60 study used a mobile phone application a bluetooth enabled blood pressure monitor and a digital scale to transmit their blood pressure weight and medica tion dosages every day study physicians had access to the data through a secure website and data that fell outside of a predeter mined range generated automatic alerts that were sent to the phy sicians scherr et al results show that the study group that used the intervention had both fewer hospitalizations than the controls over a month period and that their hospitalizations were signif icantly shorter median of days for the intervention group vs days for the controls the design of symptom monitoring systems remains challeng ing to be effective symptoms need to be assessed and monitored regularly but this need must be balanced against the burden of monitoring for example excessive reminders to report symptoms can be negatively perceived e g potentially leading to system abandonment similarly sensitivity is needed to balance clinician burden for example in logan et al work on remote moni toring of hypertensive diabetic patients physicians were very con cerned about the negative impact of such systems on their workload as well as the legal liability that this new data stream might introduce thus they requested that the symptom reports be sent to their offices only by fax although remote monitoring systems can be hugely beneficial especially for life threatening conditions such burden and workflow issues need to be carefully considered automated feedback to increase patients access to health expertise and to make it available when patients most need it automated systems can en code the types of feedback clinicians could provide if they were gi ven the patients tracked data the types and complexity of such feedback vary greatly at the simple end in holtz and whitten intervention for self management of asthma users log their peak flow readings using sms and the system sends back one of three possible responses that the patient peak flow value is p klasnja w pratt journal of biomedical informatics 198 ok that the patient should follow her asthma action plan or that the patient should call her physician at the complex end larsen et al and kearney et al 56 chemotherapy symptom management system provides sophisticated and tailored automated feedback depending on the symptoms that the patient reports the system suggests specific strategies for managing those symptoms for example if the patient reports diarrhea she might be told to drink fluids use rehydration sachets or take an anti diarrheal medication depending on reported severity 57 as an other complex example morris et al have developed a system for managing stress and negative emotions 62 throughout the day the phone prompts users to report how they are feeling users who indicate intense negative emotions such as anger or anxiety are prompted with an option to do an appropriate coping exercise drawn from cognitive psychology for example the appli cation can present the user with an animated breathing exercise or ask a set of questions aimed at deescalating the user intense an ger morris et al data from a field study of this application indi cate that using the system helped their participants significantly reduce their anger anxiety and sadness ratings over the course of the study 62 the feedback that a phone based intervention provides can be further customized by taking into account not only the information that the user is tracking but also the user health profile and other relevant information in haug et al sms coach smoking ces sation intervention for example individuals are prompted over sms with weekly questionnaires that assess their smoking status and intention to quit the system combines these data with the information about the user stage of change smoking history self efficacy and reasons for smoking to select customized motiva tional and educational messages that are sent to each person three times a week a similar intervention by obermayer et al uses user profiles to send coping messages at times of the day when individuals trying to quit reported that they were most likely to smoke e g after dinner automated feedback can make it easier for patients to get timely and relevant health management guidance that does not depend on the availability of healthcare professionals however to be effective automated feedback needs to be sufficiently tai lored for example obermayer et al participants reported that the coping messages they were receiving were not customized enough for the situations in which they were tempted to smoke and haug et al participants desired more diverse feedback than the intervention provided how to design highly customized and effective types of automated feedback remains an important open research question even simple representations of progress and goal attainment however can enhance motivation and facili tate the difficult behavior change process leveraging social influence effective health management depends not only on the patients and their clinicians but also on the patients social environment for example wang and fenske found that chronic dis ease patients who received support from family and friends en gaged in significantly more self care behaviors than those who did not similarly in a study of predictors of women continued exercise after cardiac rehabilitation of all the examined factors only social support predicted exercise persistence support from family and friends has been associated with lower hospital readmission rates 67 as well as longer survival times in pa tients with heart failure in addition epidemiological studies by christakis and fowler 69 have indicated that the structure of individuals social networks influences not only their health risks e g probability of becoming obese but also their ability to adopt health promoting behaviors such as quitting smoking for such reasons a number of recent mobile phone interven tions leverage social influence to promote health behavior change and effective management of chronic diseases these interventions have pursued this strategy in three different ways by facilitat ing social support or competition among individuals who share the same health goal i e peer to peer influence by facilitating so cial support from family and friends and by leveraging peers who have succeeding in accomplishing similar health goals i e peer modeling we review these intervention types next peer to peer influence several mobile phone interventions try to facilitate social influ ence among people who are working on the same health goal such as increasing physical activity or managing diabetes these systems have used two types of social influence to further this goal social support i e providing encouragement reassurance and empathy and competition consolvo et al application houston uses peer to peer so cial support to help individuals to be more physically active indi viduals use houston to share their daily step counts and progress toward their daily step goals with a group of friends fitness bud dies and to send messages of encouragement to each other users can also annotate their step count records with additional informa tion e g that a person went hiking or had the flu to make it easier for their fitness buddies to understand the context of a particular daily step count to help users manage how they share their infor mation houston provides controls to make it possible for a user to share a step count record with only certain buddies with everyone or with no one results from a pilot evaluation of houston revealed that messages of social support were very motivating and that competition among buddies was often seen as helpful study par ticipants who used the social version of houston were significantly more likely to reach their daily step goals than participants who used a version of houston that supported step count tracking but did not support sharing however in spite of its benefits sharing sometimes also introduced discomfort especially when a partici pant was not being very active 71 rodger et al text messaging intervention uses a similar peer to peer support strategy their system for smoking cessation used user profiles to connect individuals in the intervention with unfamiliar quit buddies who shared similar characteristics how ever rodger et al provide no detail about how well this part of the intervention worked or how and to what extent system assigned buddies interacted franklin et al sweet talk system implements an impersonal version of peer social support in sweet talk an indi vidual in the intervention can send a message that is anonymously forwarded to all other participants in the intervention sweet talk users used this mechanism to send tips success stories or purely social messages the purpose of this intervention component franklin et al 44 write was to create a sense of community while protecting the group from text bullying through moderation of for warded messages franklin et al 53 analysis of messages sent by participants during a month randomized controlled trial showed that the community building aspect of the intervention was at least partly successful messages intended to be forwarded to others with tips frustrations with diabetes self management personal experiences etc were the second most common type of message that study participants created right after submissions of blood glucose readings indicating that the participants found such social messages to be valuable in contrast to consolvo rodger and franklin systems that encourage social support gasser mobile lifestyle coach application attempted to use competition as a way to encourage individuals to be physically active and eat healthily in the compet itive social version of the application users were assigned into p klasnja w pratt journal of biomedical informatics 198 teams of five the mobile phone interface made an individual pro gress toward her daily goal visible to the whole team and the pro gress of the team shown as each member individual progress as well as in cumulative terms was displayed alongside the progress of the opposing team gasser et al report that individuals in the competitive social condition of their study did not do any better than the participants who took part in the condition that tested the individual version of the system although in this case the ele ment of competition did not help it also did not seem to hurt or be perceived negatively this result contrasts with results in studies by consolvo et al 71 and lin et al both of which found that although liked by some users competition hurt other participants motivation such mixed results suggest that if competition is going to be used as a motivation tool in mobile health technologies it should be an optional element of these systems social support from family and friends an alternative form of social influence can come through sup port from family and friends for example obermayer et al intervention for smoking cessation enabled individuals participat ing in the intervention to nominate a support person from their personal social networks who could provide social support during the quitting process using the web component of the intervention system a support person could monitor the progress of the quitter and send messages of encouragement and support that were for warded to the quitter phone via sms interestingly only a minor ity of participants in either obermayer et al original study or in riley et al 74 follow up study of the system chose to nomi nate a support person it is not clear whether this lack of interest in having a support person was due to some aspect of this particular intervention or if it reflects a general weakness of this approach to facilitating social support a similar result reported by franklin et al 53 that none of the participants in their study of sweet talk nominated a friend or family member to receive their goal messages suggests that sharing with people who are not pursuing the same health goal even those close to the person might be per ceived as problematic or at least not very helpful future work is needed to understand the desires and needs of patients for such so cial support in mobile phone health interventions peer modeling in addition to motivating individuals through encouragement praise and competition some mobile phone health interventions provide users with support through modeling and exchange of information such as tips and pointers to health related resources from successful peers the social cognitive theory has shown that such social learning practices are a key ingredient of successful behavior change as such social learning strategies are partic ularly useful for helping individuals to adopt health promoting behaviors such as healthy diets whittaker et al have used modeling in a smoking cessation sms intervention for maori youth for the intervention whittaker et al created a set of videos featuring role models who have successfully quit smoking in the video clips role models talk about their experiences with quitting and encourage viewers that they too can quit participants in whittaker et al intervention were sent text messages with links to these motivational video clips they could then click on the links and watch the videos directly on their mobile phones the participants in the intervention were overwhelmingly positive about the videos stating that the use of video made the stories more real and made it easier for them to re late to the role models an important aspect of the success of whit taker et al intervention was that the role models were selected from the local maori community making it easier for participants to identify with them the use of video was key in conveying this aspect of the intervention while video imposes additional technical requirements that could potentially limit the reach of an intervention for modeling interventions the immediacy provided by video can greatly enhance the connection that users feel with peer models grimes et al 77 used a related strategy in their interven tion for improving diet in a low income african american commu nity grimes et al eatwell application enabled users to use their mobile phones to call a system where they could record and listen to memories about eating healthily in their local community users left stories about their successes and challenges and with tips about healthy recipes and local places where one could get tasty but healthy meals as with whittaker et al intervention the local nature of eatwell was key to its acceptance by users the tips that were exchanged through eatwell were both highly relevant because they were about local food establishments and stores and were perceived as being culturally appropriate and helpful because they were coming from other community mem bers with similar eating patterns and tastes whittaker et al and grimes et al results indicate that mo bile phone health interventions can effectively facilitate social learning to encourage health promoting behaviors insofar as so cial learning is strengthened by tailoring the intervention to local and cultural resources technologies such as gps and geo tagged social networking updates e g twitter could enable researchers to customize their phone based interventions to specific user groups and contexts even further results from studies reviewed in this section suggest that mo bile phone interventions that incorporate social influence can be both effective and well liked by users however they also indicate that precisely how the social component is designed can signifi cantly affect the usefulness and acceptability of an intervention a careful design process and preliminary testing of a new system in small scale field studies is therefore necessary for this type of phone intervention increasing the accessibility of health information one of the chief advantages of using mobile phones for health interventions is that intervention content can be delivered to indi viduals without any effort on their own part by using text mes sages for example an intervention can be pushed to individuals phones providing reminders health information motivational messages and other kinds of content that can help them manage their health such interventions can provide infor mation at the right time for example a reminder shortly before a person should take her medications and can help individuals to maintain persistent awareness of and commitment to their health goals in this way mobile phone health interventions can make it easier for individuals to manage their health more con sistently and thus more effectively recent mobile phone health interventions have used the push model of delivering health related information in three main ways through informational messages about how to manage health more effectively through reminders and through glance able displays we review these intervention types next informational messages a common way of using mobile phones for health is to send individuals information relevant to their health goals or the man agement of their condition such informational messages fall into two primary categories tips and educational content tips provide individuals with concrete strategies they can use to manage their condition or engage in health behavior change smoking cessation interventions for example might send tips on how to avoid weight gain or offer coping strategies for specific situations such as if you typically experience cravings after breakfast try getting p klasnja w pratt journal of biomedical informatics 198 up from the table and taking a walk p a weight loss inter vention would send users tips on diet and exercise such as a re minder to drink more water although in most cases such tips are drawn from a database of predefined messages in some interventions users can contribute their own tips e g 53 results suggest that text messages with health tips are seen as being help ful gerber et al report for instance that of women in their weight loss study thought that the tips that they were sent during the intervention could help them lose weight along with concrete tips messages sent in health interventions often include more general educational information related to the target condition messages sent in a smoking intervention can in clude information about symptoms of withdrawal or information aimed at strengthening a person decision to quit such as the amount of money she would save if she were to stop smoking similarly wangberg et al intervention sent text messages to parents of children with diabetes with information about diabe tes management and the nature of the disease as with tips educa tional messages can have positive impact for instance participants in wangberg et al study expressed that the informa tion they received during the intervention made it easier for them to talk with their adolescent children about diabetes informational messages are a lightweight health intervention that can help individuals to learn to better manage their health and stay engaged with their health goals however although infor mational messages can be helpful fjeldsoe et al comprehen sive review of text messaging interventions for behavior change showed that tailored messages are more effective than generic messages participant comments in the studies we reviewed sup port these findings for example participants in obermayer et al smoking cessation intervention wanted feedback that was customized to their own high risk situations in addition fjeldsoe et al found that interventions where users actively com municated with the healthcare team or with each other worked better than interventions where they just passively received infor mation for such reasons informational messages would ideally be a part of a comprehensive intervention and not a full intervention in their own right reminders another simple type of intervention that takes advantage of the push model is to send users reminders such reminders have been used in a variety of applications from those where the reminders are the sole intervention to those where the reminders are one component of a comprehensive health intervention as the sole intervention type one particularly important appli cation of reminders is to increase medication adherence for exam ple curioso and colleagues developed an sms reminder system to increase hiv patients adherence to antiretroviral ther apy art a type of treatment where extremely high levels of adherence over are paramount to both immediate and long term effectiveness similarly strandbygaard and colleagues have used daily sms reminders to increase adherence to asth ma treatment although such reminders can be very effective strandbygaard et al report that the participants in the intervention arm of their trial had higher adherence to anti asthmatic medications after weeks than the controls the results are not always positive for example daily sms reminders used to increase adherence of french soldiers to malaria chemoprophylaxis did not result in increased compliance another common use of reminders is to increase the consis tency of health promoting behaviors that individuals often forget to do for instance reminders are commonly used to increase the attendance of medical appointments even a single text message sent 48h prior to an appointment can significantly reduce the number of appointment no shows sms reminders have been successfully used to increase appointment attendance in primary care 33 in family planning clinics for chronic disease fol low up appointments and for ophthalmology appointments similarly armstrong et al showed that a text messaging reminder can significantly increase regular application of sun screen in a study with participants over weeks adherence to sunscreen application in the group that received daily sms reminders was nearly twice as high 56 as in the control group even though there was no difference between the two groups at baseline castaño and martínez used sms reminders to help sexually active adolescents to regularly take their oral birth con trol for such easy health promoting behaviors where the main barrier to doing them regularly is forgetting to do them reminder interventions using sms can be an excellent fit in multifaceted interventions reminders are often paired with tracking applications where individuals monitor their behaviors or physiological states many remote coaching and automated feedback interventions use reminders to ensure that participants are uploading their data regularly diabetes management interven tions for example have reminders that are triggered if a patient does not upload her glucose readings in a certain amount of time 40 47 51 similarly asthma and hypertnsion manage ment interventions send patients reminders if they have not up loaded their peak flow or blood pressure readings regularly although most health interventions use text messaging as the mechanism for delivering reminders reminding functionality can also be a component of a native phone application for example if users of ubifit 28 do not journal any physical activities in and no activities are automatically detected the system prompts them and asks whether they want to add anything to the exercise journal these reminders are a part of the ubifit appli cation itself and they discreetly wait on the phone screen for the next time the user picks up the phone literature suggests that reminders can be well received by users participants in the ubifit studies 28 were very positive about reminders to journal their activities partially because the reminders were not obtrusive there was no sound and the user just saw the reminder next time she used the phone even the more obtrusive text messaging reminders can be well liked however the design of reminders is important users object to reminders that are perceived as being unnecessarily too fre quent the content of the reminders matters as well for in stance during the participatory design of their reminder system for medication adherence curioso and colleagues found that their users wanted the reminders to be motivational reminding them not only what they need to do but also why they are doing it direct and privacy preserving e g using code words instead of direct references to sensitive health conditions in case someone were to see the recipient phone although the content and fre quency of reminders varies from intervention to intervention such findings suggest that minimizing disruptiveness both of users time and potentially social relationships is an important consider ation for the design of effective reminder based health interven tions at the same time curioso et al findings indicate that thoughtfully designed reminders have a potential to do more than just remind individuals they can also motivate increasing the probability of being effective glanceable displays informational messages and reminders are both push interven tions in the narrow sense they are delivered to the user phone on a schedule determined to maximize the likelihood that the delivered information will be effective in contrast recent work on glanceable displays aims to make health related information visible every time a user picks up the phone glanceable displays provide users with an overview of their health information that p klasnja w pratt journal of biomedical informatics 198 can be interpreted at a glance and that is visible throughout the day in this way glanceable displays support persistent awareness of health goals and increase the likelihood that users will engage in health promoting behavior for example a key component of the ubifit application 28 is the feedback about users physical activities that is displayed on the background screen i e wallpaper of the users mobile phones fig ubifit uses a stylized image of a garden to display physical activities that users perform each week different types of flowers represent different types of physical activity walking cardiovas cular exercise strength training flexibility training and other the user gets a flower for every min of walking or cardiovascu lar exercise and for each session of one of the other types of phys ical activities e g stretching or strength training the garden is automatically updated either when the sensing device detects an activity or when the user manually journals a new activity if users meet their weekly activity goals a large butterfly appears in the garden up to three smaller butterflies represent goal attainment for the previous weeks through the use of the garden display on the background screen ubifit enables users to see at a glance how active they have been this week how varied their activities have been and whether they have attained their weekly goals because the garden is visible each time they use the phone users have frequent encounters with feedback about their physical activity this exposure causes cogni tive activation of physical activity goals making it more likely that users will continue to be active and notice opportunities to engage in physical activity a month trial of ubifit demonstrated the power of this ap proach participants who had the background display managed to maintain their level of physical activity over the course of the month study including the winter holiday season while the activity of participants without the display who used the same journaling application significantly decreased during this period study interviews revealed that seeing the garden display throughout the day kept physical activity in the front of partici pants minds these results suggest that glanceable displays are a promising avenue for facilitating health behavior change chiu et al playful bottle system for motivating water intake uses a related approach they have created a mobile game that provides feedback in the form of a tree that slowly loses its leaves if the user does not drink enough water stylized representations such as those used in ubifit and playful bottle could be an effective way of presenting health information on displays that are likely to be seen by people other than the user they also open another avenue of intervention personalization potentially enabling users to pick themes that most resonate with them one participant in the ubifit trial for instance suggested that for him the system would have been much more motivating if it had used robots instead of flowers to represent different types of physical activity by increasing the accessibility of health information these interventions support individuals ability to stay engaged with their health throughout the day for a class of health promoting activities that require high consistency in order to be effective such as adoption of new healthy habits or adherence to a critical medication regimen this persistent engagement with health information is paramount yet a remaining challenge is how to maximize the effectiveness of the pushed health information while minimizing the burden on the individuals using them utilizing entertainment few people would refer to managing their health as fun yet a growing number of recent phone based health interventions have begun to leverage some form of entertainment to engage individu als with their health goals one common form of this strategy is to make reminders and informational messages more engaging by interspersing them with other interesting or amusing non health related content for in stance both armstrong et al text messaging intervention for sun screen application and rodgers et al smoking cessa tion intervention sent their participants messages with fun con tent such as weather forecasts jokes sports scores or interesting local news in addition to health messages such mes sages were well received keeping individuals interested in the intervention in addition participants in whittaker et al smoking cessation intervention for maori youth explicitly sug gested that the intervention could be improved by including jokes quizzes and other such content although the inclusion of general interest content can give health interventions a lighter touch fu ture work will need to investigate whether certain types of non health content work better than others to keep users engaged and whether there are health non health content ratios that opti mize user engagement while minimizing the burden created by the inclusion of additional messages the second approach to leveraging entertainment that has re cently emerged uses mobile phone based video games to support health management in human computer interaction there is an established literature on exertion games games that require players to invest significant physical effort as a part of gameplay e g 89 however the idea that mobile phones can be used for games that promote health management is very recent in two recent papers deshazo et al 90 describe a set of simple phone games that teach patients with diabetes how to estimate carbohydrate content and caloric value of various foods fig they have adapted familiar games of hangman quizshow and countdown to include questions about food relevant to diabetes and to slowly scaffold users knowledge about nutrition in hang man for example users are given six chances to guess how many carbohydrates are contained in a given food before the stickman is hanged to make them more effective all games can be per sonalized based on whether users are vegetarian or not what their educational goals are i e to learn to estimate carbohy drates calories or energy density and what their current skill level is deshazo et al early results indicate that players perceive the phone games to be enjoyable and that they think that playing the games has increased their knowledge about nutrition p klasnja w pratt journal of biomedical informatics 198 fig ubifit glanceable display on the phone background screen different types of flowers represent different types of physical activities that the user performed this week walking cardio strength training and stretching the butterflies indicate if the weekly activity goal was met this week the large butterfly and during the previous weeks grimes et al have taken a similar approach with their ord erup game for helping african americans learn to eat more health ily in the game users play a waiter at a restaurant and try to recommend from the given choices the healthiest food option for restaurant patrons the healthier recommendations the users make and the more quickly they do it the longer they get to keep the job in the game as with grimes et al eatwell system the food options in orderup were customized to the food choices that were popular and readily available in the community that the game was targeting a week trial of the game showed that participants found the game to be enjoyable as well as educational participants expressed that playing the game both helped them learn about the healthfulness of the foods they typically ate and that it influenced their food choices however the participants thought that the edu cational value of the game was limited by the lack of feedback about why certain foods in the game were healthier than others one particularly attractive feature of games like those devel oped by deshazo et al and grimes et al is that they enable individ uals to use microbreaks brief periods of free time during the day e g while waiting for a bus or at the grocery store to increase their self management knowledge while engaging in an entertain ing relaxing activity both of these games could be played in just a few minutes the use of such frequent breaks to improve health management skills could have far reaching positive health outcomes the use of phone based video games for health is in its infancy but is quickly developing the meeting of the games for health conference www gamesforhealth org for instance had a full day of presentations on mobile serious games many of which are about health this space is wide open for innovative research that can not only support individuals in managing their health but also do so in an enjoyable immersive way discussion in this paper we identified five strategies that have been used in mobile phone health interventions and the different ways in which those strategies have been implemented in systems devel oped in health sciences and human computer interaction recent technological advances are creating opportunities for further developments in interventions based on each of these strategies in this section we briefly review what we see as the main direc tions for future work in this important area of health research most mobile phone interventions based on monitoring of health information focus only on capturing information about the health activities and states that are directly tied to individuals health goals such as the amount of physical activity or peak flow in asthma management yet both in health behavior change and chronic disease management understanding the context that influences these activities and states can be very important for developing effective health management practices mobile phones can make it possible to capture such contextual information to help individuals and their healthcare providers to better under stand the concrete circumstances that influence their health and to develop strategies to address those patterns by automatically associating captured information about health relevant activities and states with location time of day and other information con tained on the phone such as calendar events or recent emails and phone calls phone based interventions could help individuals construct a rich understanding of the range of factors that influ ence their health and health related behavior similarly toolkits like myexperience and open data kit 93 can help make the very capture of self report data smarter by triggering ques tionnaires about health related activities and states not only at predetermined times but also based on users location detected activities and physiological measures and other factors that the phone can sense phone based interventions can capture self re port data at the times that are most critical to individuals health goals e g by capturing a user emotional state when initial signs of stress are detected such information in turn combined with a variety of sensing data could enable truly effective just in time interventions although their potential has been recognized for several years mobile phone technology is only now getting to the point where such interventions are technically feasible improvements in phone technologies are also opening new opportunities for facilitating involvement of clinicians the quality of photo and video recording available on modern mobile phones makes it possible to use these devices as ubiquitous and inexpen sive telemonitoring terminals schreier et al intervention for remote monitoring of psoriasis is a good example of the kinds of p klasnja w pratt journal of biomedical informatics 198 fig deshazo et al phone based games for teaching nutritional skills to diabetic patients users are asked to estimate nutritional content of various foods correct answers earn users points in the games clinical interactions that mobile phones make possible recent developments of sophisticated medical devices that interface with mobile phones such as alivecor iphoneecg system www alive cor com or mobisante mobius smartphone based ultrasound system mobisante com could enable phone based consultations for a range of conditions greatly facilitating specialist care in areas where local healthcare resources are lacking mobile phone interventions are just beginning to explore the potential of leveraging social networks in combination with light weight tagging and automatically added location data social net working tools like twitter and facebook can enable a whole class of new applications that use mobile phones to seek and share ad vice expertise support and local resources for health manage ment such applications could enable creation of ad hoc social networks of people with similar health goals or concerns helping individuals to connect with others and to get health related infor mation and advice that is highly customized to their locations cul tural backgrounds and health situations additionally such looser social networks might avoid some of the social frictions that arise with changes in intensity of sharing in more tightly knit networks such as those utilized by the houston system 71 similarly location and other contextual information such as the user calendar could be used to alert individuals to opportunities for healthy activities and local resources relevant to their health situation e g nearby restaurants that serve healthy food or a sug gestion to take a walk after a long meeting if such prompts are made suitably unobtrusive and diverse potentially by drawing on information mined from social networking tools they could provide a dynamic way to keep individuals engaged with their health goals over extended periods of time further building on benefits provided by reminders and glanceable displays as we already mentioned the field of mobile phone games for health is only starting future work should investigate both new types of games e g augmented reality games that superimpose relevant health information on real world objects and new ways to motivate health through gaming for instance berkovsky et al 94 has investigated providing in game rewards for performance of physical activity and harris et al 90 have experimented with tethering health educational content to standard non health games such as sudoku exploring other strategies for motivating health behavior and education through games and other forms of entertainment should be an important direction of future work lastly the vast majority of phone based interventions we iden tified for this review focus on only three areas chronic disease management supporting health behavior change and monitoring of symptoms and critical events although all three areas are extre mely important other aspects of health management could benefit from mobile phone based solutions key among them we believe is personal health information management the ability to ac cess their health information from anywhere to easily collect and group care related information as the information is first encoun tered and to have relevant health related information integrated with other daily tools e g calendar could greatly benefit pa tients especially those who are undergoing treatment for complex conditions such as cancer the work in this area is just beginning as of mid microsoft just announced a mobile optimized website for microsoft healthvault the preeminent third party personal health record but robust mobile phone applications that use healthvault are yet to be developed even the few patient por tals that currently provide mobile access such as spectrum health myspectrum portal support patients personal health information management needs in only limited ways for instance patient portals typically do not allow patients to enter their own data or annotate their records some of our own ongoing work at tempts to fill this gap 97 but much more work is needed in this area limitations the review presented in this paper has two main limitations that are important to acknowledge the first limitation is that the review is largely based on research literature from health sciences and human computer interaction and does not survey the many commercial mobile phone health interventions that have been developed in recent years one reason for this choice is that unlike for research literature there is no systematic way to get a compre hensive overview of commercial products the other reason how ever is that our goal was to provide an overview of the design space of phone based intervention and that in the commercial products we found we did not identify any intervention strategies that were not also represented in the research literature of course given the sheer number of commercial systems in this area it is certainly possible that we overlooked products that break new ground in spite of our efforts to be comprehensive the other limitation is that the large majority of systems we re viewed here are intended to be used by patients or other individ uals who are working on their own health goals we did not review phone based systems that help individuals to manage the health of another person such as a child or an elderly family mem ber there is exciting new work in this area e g 98 which is likely to lead to the development of novel intervention strategies which are currently not used in phone based interventions for patients themselves we suspect that at least some of those new strategies will be applicable to personal health applications as well further contributing to our understanding of how to develop effective phone based interventions that can help individuals to manage their health more effectively consistently and enjoyably abstract the past decade has witnessed the advent of the smartphone a device armed with computing power mobility and downloadable apps that has become commonplace within the medical field as both a personal and professional tool the popularity of medically related apps suggests that physicians use mobile technology to assist with clinical decision making yet usage patterns have never been quantified a digital survey examining smart phone and associated app usage was administered via email to all acgme training programs data regarding respon dent specialty level of training use of smartphones use of smartphone apps desired apps and commonly used apps were collected and analyzed greater than of respond ents used a smartphone of which the iphone was the most popular 56 over half of the respondents reported using apps in their clinical practice the most commonly used app types were drug guides medical calculators coding and billing apps and pregnancy wheels the most frequently requested app types were textbook reference materials average response classification treatment algorithms and general medical knowledge the clinical use of smartphones and apps will likely continue to increase and we have demonstrated an absence of high quality and popular apps despite a strong desire among physicians and trainees this information should be used to guide the development of future healthcare delivery systems expanded app functionality is almost certain but reliability and ease of use will likely remain major factors in determining the successful integration of apps into clinical practice keywords smartphone app technology mobile computing introduction smartphone apps such as those available in the apple and android app stores are quickly becoming integrated into clinical practice by physicians smartphone apps are self contained software applications that can be downloaded by and run from advanced mobile phones commonly referred to as smartphones the leading smartphone operating systems include iphone android and blackberry and each has an associated app distribution store where individual users can select and download apps of interest electronic supplementary material the online version of this article doi 011 contains supplementary material which is available to authorized users o i franko t f tirrell department of orthopaedic surgery university of california west arbor drive mc san diego ca usa the appeal of apps for consumers rests in their ability to store reference information save critical data perform complex calculations access internet based content and present video and audio media etc all through an intuitive user interface as such apps have broadly appealed to consumers for both social and professional functions e mail ofranko ucsd edu similarly apps appeal to developers because app stores t f tirrell biomedical science graduate program university of california san diego ca usa allow for widespread advertising and distribution of their product apps are usually available either for free or at a price typically ranging from 99 99 with a percentage of profits returned to the developers this incentive for developers has resulted in an impressive and expanding number of high quality and broadly appealing applications the most popular smartphone platform in the us market the iphone has over 000 apps as of july available through its app store the general trend towards increased smartphone and app use has penetrated the medical community the increased popularity of apps among health care providers resulted in a dedicated medical app category created in the apple app store in 2008 as further evidence of the increasing role that apps are playing in medical practitioners lives websites have emerged devoted entirely to reviewing apps designed specifically for physicians while various reports have been published focusing on the utility of apps for various specialties or functions no study has examined the prevalence and type of app use among all physicians fellows and residents nationwide the purpose of this study was to perform a prospec tive nation wide email survey evaluating the use of smartphones and smartphone apps among providers at medical centers recognized by the accreditation council for graduate medical education acgme we hypoth esized that the current use of smartphones and their associated apps is prevalent among all specialties and that providers desire additional apps for use in the medical setting methods data collection for this study was performed via a national survey of acgme accredited residency and fellowship programs prior to initiating the study approval was obtained from the sponsoring institutional review board irb first a novel online digital survey was developed to query respondents regarding their specialty level of training use of smartphones and use of smart phone apps see electronic supplementary material for an example of the survey in addition using a free response section respondents were asked to list the apps that they currently find most useful once the digital survey was complete contact information in the form of email addresses for all program directors was obtained from the acgme website this included different department types and program types among institutions the survey was then emailed to all programs with a letter asking the program director to forward the survey to all faculty fellows and residents in each respective department two additional reminder emails were subsequently sent week apart to increase the response rate the first email was sent on april and responses were collected through june 2011 j med syst respondent characteristics level of training specialty smartphone type etc were collected and the data were analyzed based on these demographics in the free response section many respondents reported the use of a variety of apps with similar functions such as drug reference guides medical calculators and pregnancy wheels thus analyses were performed separately for individual apps as well as based on groups of apps with functional similarities when grouped together drug reference apps included epocrates mobilepdr for prescribers johns hopkins abx guide lexicomp micromedex drug information 2011 emra antibiotic guide the sanford guide to antimicrobial therapy 2011 medical calculator apps included medcalc medical calculator bmi calculators calculate medical calculator by qxmd medcalc pregnancy wheel apps included preg wheel pregnancy wheel perfect ob wheel pregnancy wheel and coding and billing apps included any free response with the phrase icd cpt coding billing or some combination thereof some respondents reported the use of mobile web apps that were also included in the analysis chi squared tests for independence spss chicago il were used to examine associations between various survey parameters the strengths of which were quantified using cramer v results there were a total of 306 unique responses from 397 residents fellows and 385 attending physicians among different specialties table over of respondents used some type of smartphone iphone was the most popular followed by android and blackberry operat ing systems table and fig among all respondents 56 reported they currently use apps in their clinical practice and there was a qualitative trend towards decreas ing app use with increased training level table and fig chi squared test of independence demonstrated that training level and app usage are not independent χ n 214 p 0001 v when asked to select the types of apps they would find most useful on a mobile device the most requested app types included textbook reference materials average re sponse classification treatment algorithms and general medical knowledge there was a greater desire for apps among residents relative to fellows and attending physicians respectively fig respondents were then asked which apps they currently find most useful when grouped according to function the four most commonly reported app types and their respective response rates were drug guides 79 medical calculators coding and billing apps and pregnancy wheels the most popular individual apps as reported by respond ents are listed in table discussion the purpose of this study was to define the prevalence of smartphone and app use among providers at acgme accredited training programs the results demonstrate that a majority of physicians and trainees currently have smart j med syst table survey respondent demographics respondents using smartphone using apps total respondents 306 level of training resident 397 fellow 55 attending for years attending for years 86 49 attending for years 39 specialty emergency medicine 91 family medicine internal medicine 83 60 obstetrics gynecology 83 56 pediatrics 78 49 psychiatry 84 62 radiology 77 30 surgery and subspecialties 98 51 other 80 51 phones and use apps with a trend towards increasing app use that inversely correlates with level of training the strength of this relationship as measured by cramer v was using cohen criteria this coefficient is indicative of a medium effect size of the association between training level and app usage this indicates the presence of other important factors yet to be described that determine whether physicians with smartphones are using apps in their medical practice further these results can shed light on the adoption of technology by physicians and may motivate medical teaching institutions to educate their trainees on the appropriate way to incorporate new technology into medical practice the results from this study are not surprising prior reports focusing on smaller segments of the medical community suggest that the use of smartphones and apps are popular among providers however it also seems that the utility of apps differs between specialists this is reflected by the variable use of apps among different specialists table as well as those reported by other authors we feel that one major limitation that guides app use is their typical sales model most apps must be purchased in full before a user has a chance to try them out as a result potential buyers must rely on screenshots user reviews and brand loyalty to make their purchasing decisions we suspect that one of the reasons that epocrates has been so successful in accessing the physician market is a longtime marketplace presence ever since early versions in the early were available on the palm device in addition by offering free access for medical students they have focused their advertising at the medical community youngest members one contradiction found within the data was the fact that the most requested apps categories reference materials treatment algorithms and general medical knowledge are apparently already available from the app store in various formats we believe the interpretation of this finding fig phone operating system of choice across survey population fig percentage of respondents reporting use of smartphone apps in clinical practice fig categories of apps desired by smartphone users consists of multiple conclusions first it seems that app developers recognize the desire for reference applications and have focused their efforts in this realm however the continued desire for more apps by physicians suggests that the reference apps currently available do not yet meet the needs of physicians thereby leaving much room for improvement user interface is of utmost importance for smartphone users and merely porting all information from a textbook into a mobile version may not provide the optimal experience due to screen size or other restrictions in addition because physicians may not regularly search for apps and because apps are continu ously updated physicians may be unaware of the complete library of apps that are currently available lastly many reference materials cost nearly as much as equivalent print versions in order for an app to be successful in reaching common use by physicians it table most commonly used smartphone apps ranking app name of respondents reporting use epocrates medscape medcalc medical calculator skyscape medical resources lexicomp 6 bmi calculator micromedex drug information dynamed citrix receiver red book 3138 j med syst 3139 must be easy to use and reasonably priced all of these reasons might explain the apparent mismatch between app availability and physician desire despite the increasing popularity of apps concerns exist regarding how the use of apps will influence future medical practice no organizations or governing bodies currently exist to review or validate the content contained within these apps while the fda has considered regulating iphones and apps as medical devices this is not currently performed although our results suggest that apps are widely used among us based medical providers the information contained within those apps may not be based on validated or peer reviewed information considering the desire among providers for more apps as well as the incentive for companies to sell apps for profit or marketing purposes the availability of apps is likely to increase thus providers must be aware of this significant limitation and potential liability when utilizing apps to make medical decisions it will likely become the responsibility of physicians to self regulate and self educate regarding the appropriate and prudent use of apps in clinical practice we designed our study to examine app use among a specific population providers and trainees at acgme affiliated institutions and training programs we assumed that this study population would be more likely to adopt new technologies and thus our study was designed to increase the number of useful responses in addition by contacting acgme programs via emails to program directors we captured a broad and diverse population of potential respondents however we recognize the limita tions to this study design one limitation of this study is that the survey response rate is unknown as stated in the methods a variety of programs at institutions were included in the study resulting in a total of 353 emailed survey requests unfortunately no method exists to reliably track the number of emails that were delivered opened deleted or forwarded this is an unfortunate limitation of email based surveys a second limitation is the potential selection bias inherent in any form of subjective survey we cannot control for the possibility that physicians with an interest in technology may have been more likely to complete the online survey as a result our results may be artificially inflated however these results appear consistent with other prior reports and the habits of users as observed by the authors another limitation of our report was based in the app classification system we adopted to simplify and clarify the analysis while some apps are easy to classify because they serve a single function others are multifunctional and seem to fall into more than one category for example epocrates contains drug information as well as some medical calculation tools when this occurred we selected the primary app function in the case of epocrates it was classified as a drug reference app while this redundancy in function for some programs may result in a decreased accuracy of reported app use the reported trend for commonly used apps is likely still valid due to the large difference in reported use in addition there are some functions such as pregnancy wheel calculations for which many apps exist it is thereby possible that some apps reported in the free response section should have fallen into one of the broader categories drug reference medical calculators pregnancy wheels but were instead only counted individually a final limitation is that this study only examined the use of apps at academic medical centers and may not reflect trends among all physicians as noted above providers at teaching institutions may be more likely to be younger practitioners and early adopters of new technology thereby inflating the prevalence of smartphone and app use a larger broader study focused on examining a cross section of all practicing physicians may more accurately reflect the prevalence of app use nationwide nevertheless the results from this report suggest that apps are widely used among practitioners in all specialties and these limitations should not minimize the concerns that exist regarding the validity of app content if current trends continue smartphone and app use are likely to increase at an accelerated rate app availability has exploded in recent years and we have shown that the desire j med syst 3139 3139 for more high quality apps is strong with much space for further development we recommend that information such as that reported in this study be used to guide the development of forthcoming healthcare delivery systems and their integration with smartphone apps the future will likely bring expanded app functionality but reliability and ease of use will likely remain major factors in determining the successful integration of apps into clinical practice disclosures the authors declare that they have no conflict of interest funding none abstract recent advances in microelectronics and integrated circuits system on chip design wireless communication and intelligent low power sensors have allowed the realization of a wireless body area net work wban a wban is a collection of low power miniaturized invasive non invasive lightweight wire less sensor nodes that monitor the human body func tions and the surrounding environment in addition it supports a number of innovative and interesting appli cations such as ubiquitous healthcare entertainment interactive gaming and military applications in this pa per the fundamental mechanisms of wban including architecture and topology wireless implant communi s ullah s saleem cation low power medium access control mac and routing protocols are reviewed a comprehensive study of the proposed technologies for wban at physical phy mac and network layers is presented and many useful solutions are discussed for each layer finally numerous wban applications are highlighted keywords implant communication physical wban mac networking routing survey acronyms and abbreviations altr adaptive least b k s kwak uwb itrc research center inha university temperature routing aes advanced encryption standard nam gu incheon south korea e mail hotmail com csma ca carrier sense multiple access collision avoidance s ullah e mail sanajcs hotmail com cap contention access period cfp contention free period h higgins cca clear channel assessment zarlink semiconductors wiltshire uk control channels b braem c blondia department of mathematics and computer science university of antwerp ibbt middelheimlaan ce consumer electronics ctr counter cbc cipher block chaining antwerp belgium ccm counter with cbc b latre i moerman department of information technology ghent crc cab university ibbt gaston crommenlaan box cyclic redundancy check coefficient of absorption and bioeffects gent belgium cicada cascading information z rahman department of biological sciences korea advanced institute of science and technology kaist daejeon south korea cbr retrieval by controlling access with distributed slot assignment xprotocol constant bit rate j med syst dtdma reservation based dynamic tdma protocol erp effective radiated power ecg electrocardiogram fcc federal communication commission fdtd finite difference time domain gdp gross domestic product gts guaranteed time slot h mac heart beat driven mac protocol hec hydroxyl ethyl cellulose h v horizontal vertical polarisation h h horizontal horizontal polarisation ieee institute of electrical and electronics engineers ism industrial scientific and medical band lpl low power listening lbt listen before talking los line of sight ltr least temperature routing ltrt least total route temperature mac medium access control mics medical implant communications service mac bold letters message authentication code mn master node ms monitoring station nist national institute of standards and technology nlos non line of sight network simulator phy physical layer pb tdma preamble based tdma protocol qos quality of service rf radio frequency remcom a software company http www remcom com sar specific absorption rate tdma time division multiple access tsrp time slot reserved for periodic traffic tsrb time slot reserved for bursty traffic tara thermal aware routing algorithm tip temperature increase potential uwb ultra wide band v v vertical vertical polarisation v h vertical horizontal polarisation wban wireless body area network wmts wireless medical telemetry services wasp wireless autonomous spanning tree protocol wsn wireless sensor network xfdtd a electromagnetic simulation software package xor exclusive or introduction current healthcare systems are facing new challenges due to the rate of growth of the elderly population persons years old and over and limited financial resources according to the us bureau of the census the number of old people 84 years old is predicted to double from 35 million to million by this trend shows that the world elderly population will double from million in to million in furthermore overall healthcare expenditure in the us was trillion in 2004 and this number is projected to be triple by or of the us gross domestic product gdp http www who int the impend ing health crisis attracts researchers industrialists and economists toward optimal and quick health solutions the non intrusive and ambulatory health monitoring of patient vital signs with real time updates of medical records via the internet provides economical solutions to the challenges that health care systems face the remote monitoring of body status and the surrounding environment is therefore becoming more important for sporting activities members of emergency military and health care services the levels of fitness required for the very competitive international sporting events require athletes to be at the very pinnacle of fitness with every muscle used to its utmost furthermore many body functions are traditionally monitored and separated by a considerable period of time this can give an incomplete picture of what is really happening j med syst consider a patient visiting a doctor for a blood pressure check he she may be anxious and thus have elevated pressure resulting in an inaccurate diagnosis if how ever the patient can be fitted with a simple monitoring system that requires no intervention then a picture can be built up of how the pressure changes throughout the day when he she goes about their normal business this gives a better picture of what is happening and remove inaccurate results caused by going to visit the doctor to achieve these requirements monitoring of movement and body functions are essential this mon itoring requires the sensors and wireless system to be very lightweight and to be integrated unobtrusively into the clothing a wireless body area network wban allows the integration of intelligent miniaturized low power sensor nodes in on or around a human body to monitor body functions and the surrounding environment each intelligent node has enough capability to process and forward information to a base station for diagnosis and prescription a wban provides long term health mon itoring of patients under natural physiological states without constraining their normal activities it can be used to develop a smart and affordable health care system and can be a part of diagnostic procedure maintenance of chronic condition supervised recovery from a surgical procedure and can handle emergency events generally wban consists of in body and on body area networks an in body area network al lows communication between invasive implanted de vices and a base station an on body area network on the other hand allows communication between non invasive wearable devices and a base station in this pa per we present a comprehensive study of the proposed technologies for wban at physical phy medium access control mac and network layers to the best of our knowledge no study has been conducted to analyze the behaviour of these layers for wban different technologies proposed at different layers are thoroughly studied and many useful solutions are dis cussed for each layer since many researchers includ ing those involved in the ieee 6 are focusing on phy mac standardization the network layer has received limited concentration we therefore study the importance of network protocols i e routing proto cols for wban and explain how these protocols can be used to improve the lifetime of a network the rest of the paper is categorized into six sections section wban architecture introduces wban ar chitecture and the traffic model section phy layer communication is related to the implant communica tion at the phy layer and presents a brief discussion fig on body nodes distribution on in body rf communication and the propagation pattern in or around a human body section mac layer communication discusses low power mecha nisms and the proposed mac protocols for wban section network layer communication presents a discussion on routing protocols for wban followed by future research directions the final section concludes our work wban architecture a wban consists of in body and on body nodes that continuously monitor a patient vital information for diagnosis and prescription some on body nodes can be used for multimedia and gaming applications these nodes can have different topologies such as star tree and mesh topologies however the most common is a star topology where the nodes are connected to a central coordinator in star manner depending on the application several nodes are sometimes combined to process and transfer data to a central coordinator since table on body nodes links description link a b description through the hand c d through the wrist e f torso front to back g h through the thigh i j through the ankle k l left ear to right ear m n glucose sensor to glucose pump j med syst fig wban architecture for medical and non medical applications some parts of the human body move relative to each other this trend should be considered when deploying on body nodes figure shows a simple example of on body nodes deployed on a human body the nodes for audio and video transmission should be carefully deployed keeping in consideration the sensitivity of head nerves furthermore sensitivity of eye to specific absorption rate sar should be considered the nodes located on the torso and head do not move much relative to each other however the nodes located on extremities such as legs and arms torso and the head may move relative to each other table shows the possible links between on body nodes the in body and on body nodes are quite low power and are not able to generate power available for the whole body sar however these nodes are in close proximity to or inside the human body and therefore the localized sar could be quite large if all the available power is deposited in a small volume as a result the localized sar into the body must be minimized a wban uses wireless medical telemetry ser vices wmts unlicensed industrial scientific and medical ism ultra wideband uwb and medical implant communications service mics bands for data transmission wmts is a licensed band used for medical telemetry system the federal communica tion commission fcc urges the use of wmts for medical applications due to fewer interfering sources absorbed by the body when exposed to rf waves and is measured in watts per kilogram http wireless fcc gov services however only autho rized users such as physicians and trained technicians are eligible to use this band furthermore the restricted wmts mhz bandwidth cannot support video and voice transmissions the alternative spectrum for med ical applications is the ghz ism band that includes guard bands to protect adjacent channel interference but this band is also used by other technologies such as bluetooth zigbee and wifi a licensed mics band mhz is dedicated to implant communication figure shows the wban architecture for medical and non medical applications where the wban traffic is classified into on demand emergency and normal traffic on demand traffic is initiated by the coordina tor or doctor to acquire certain information mostly for the purpose of diagnostic recommendations this is fur ther divided into continuous in case of surgical events and discontinuous when occasional information is re quired emergency traffic is initiated by the nodes when they exceed a predefined threshold and should be accommodated in less than one second this kind of traffic is not generated on regular intervals and is totally unpredictable normal traffic is the data traffic in a normal condition with no time critical and on demand events this includes unobtrusive and routine health monitoring of a patient and treatment of many diseases such as gastrointestinal tract neurological disorders cancer detection handicap rehabilitation and the most threatening heart disease the normal data is collected and processed by the coordinator depending on the application requirements the coordinator may con tain a wakeup radio circuit to accomodate life critical j med syst events and an additional circuit to connect multiple physical layers see section a power efficient mac protocol for wban the coordinator is further con nected to telemedicine and medical servers for relevant recommendations phy layer communication there are several ways to communicate with a human body implant including methods that use electromag netic coupling and radio frequency rf communica tion both are wireless and their use depends on the applications comprehensive details about the implant communication are presented in in this section we briefly discuss electromagnetic coupling in body rf communication antenna design and the propagation pattern in or around a human body this section is concluded with useful remarks electromagnetic coupling electromagnetic coupling means that the transponder and the antenna are coupled by the magnetic flux through coils much like a transformer different ap plications still use electromagnetic coupling to provide a communication link to implanted devices with an external coil held very close to the patient that cou ples to a coil implanted just below the skin surface the implant is powered by the coupled magnetic field and requires no battery for communication data is transferred from the implanted device by altering the impedance of the implanted loop that is detected by the external coil and electronics this type of communi cation is commonly used to identify animals that have been injected with an electronic tag electromagnetic induction is commonly used when continuous long term communication is required such as for a cochlear implant used to restore hearing it achieves the best power transfer when using large transmit and receive coils however it is impractical when space is an issue or devices are implanted deep within the patient this technique does not support very high data rate applica tions and cannot initiate a communication session from inside of the body for further details see 6 in body rf communication rf communication enables a two way data link that allows an implant to initiate a communication session this requires an implanted battery electronics and a suitable antenna earlier some in body communication systems used the ism bands but the mics band is gain ing worldwide acceptance for in body communication systems this band has a power limit of μw in air and is split into ten wide channels where each channel has khz bandwidth the human body is a medium that poses numerous wireless transmission challenges the body is composed of various components that are not predictable and will change as the patient ages gains or loses weight or even changes posture while there are simple formu las for designing free space communications it is very difficult to calculate the performance of an in body communication system as each individual is different generally the implant operates in a wide variety of environments and positions that change with time before considering any in body data transmission the effects of the human body on the rf signal must be understood unlike the usual communication through constant air the various tissues and organs within the body have their own unique conductivity dielectric constant and characteristic impedance as a result sig nal level and propagation from an implanted device to a remote receiver is unpredictable typical dielectric con stant ε r conductivity ρ and characteristic impedance z properties of muscle and fat are shown in table not only do these values vary from person to person they also change as a patient moves changes weight and ages the high dielectric constant ε r works to reduce the physical size of any antenna antenna design according to an in body antenna needs to be tune able using an intelligent transceiver and routine this enables the antenna coupling circuit to be optimized and to obtain the best signal strength often size con straints dictate the choice of a non resonant antenna a non resonant antenna has lower gain and therefore is less sensitive on the receiving side and radiates low power generated by the transmitter this makes design of the antenna coupling circuit even more important a patch antenna can be used in the implants with a rf backplane generally patch antennas are com prised of a flat substrate coated on both sides with a con ductor the substrate is typically alumina or a similar table body electrical properties frequency muscle fat ε r ρ s m z ε r ρ s m z 31 6 12 07 92 4 58 82 6 08 108 56 97 1070 j med syst fig patch with top surface connection body compatible material with a platinum iridium coa ting on both surfaces the upper surface is the active face and is connected to the transceiver the back face is typically connected to the implant the connec tion to the transceiver needs to pass through the case a b fig 4 a patch antenna gain in free space b patch antenna gain inside body where a hermetic seal is maintained requiring a feed through the feed through must have no filter capaci tors present the connection to the top active surface can be established by a hole through the substrate or by a wire connected to the top as given in fig the back face can be connected to the case with conductive epoxy if it is attached to remcom conducted several experiments to com pute the input impedance radiation gain pattern and sar of a patch antenna inside a human body using xfdtd http www remcom com xfdtd they con sidered a patch antenna of mm mm dimen sions and mm thickness with a substrate of lossless dielectric permittivity 9 initial calculations for the patch antenna were performed in free space resulting an input impedance of 175 j ohms with efficiency as given in fig however when the same patch antenna was embedded inside the human body it resulted in 4 05 j ohms impedance with an efficiency of as given in fig the reduction in the gain was due to the loss in the body tissues another option is to use a loop antenna in the implants the loop antenna operates mostly in the magnetic field whereas the patch operates mostly in the electric field the loop antenna delivers comparable performance to that of a dipole but with smaller size j med syst also the magnetic permeability of muscle or fat is very similar to that of air except the dielectric constant that varies considerably this property enables an antenna to be built and used with much less need for retuning a loop antenna can be mounted on the case in a biocompatible structure further details of antenna design can be found in 7 9 and generally the performance of an implant inside a human body is difficult to predict or simulate there are no reliable equations to consider and there fore only limited simulation models can be used the simulations provide a guide to the propagation in or around a human body but do not guarantee perfor mance accurate approximation to a human body can be made using a body phantom see section implant materials filled with a liquid that mimics the electerial properties of the human body antenna testing before designing a matching network for the antenna or transceiver interface it is necessary to measure the impedance of the antenna within a representative medium testing an implant antenna in air does not represent the in body impedance to measure the in body impedance a phantom comprising a tank of liquid is used the liquid is a mixture of water sodium chlo ride sugar and hydroxyl ethyl cellulose hec which mimics muscle or brain tissue 11 in the frequency range 100 mhz to ghz as given in table a way of measuring an antenna with a low radiation resistance is described in 12 matching network once the impedance of the antenna is known it can be matched to the transceiver when implanted the trans ceiver needs to be capable of optimization by having access to an array of capacitors that can be switched in or out across rf terminals these can be controlled by an automatic routine within the transceiver or by a mi crocontroller this enables the transceiver performance table body tissue recipes ingredient of weight of weight 100 mhz to ghz ghz water 4 sugar salt nacl 0 0 hec 0 4 to be optimized antenna tuning circuits are required to present optimum load impedance to the transmitter it should be noted that this does not necessarily lead to a conjugate impedance match the design of matching networks is discussed in 13 base station antennas the implant and the base station antennas are often very different a base station with a ghz wakeup transmitter and mhz communication function typ ically uses two separate antennas as shown in fig 5 these can be as large as aesthetics can allow base station antennas result in a higher gain than the small implanted antenna the base station could also use more than one antenna to overcome the effect of multi path fading and polarization as discussed in and to reduce the signal strength if space permits an arrangement of four antennas with suitable switching and software optimization can be employed implant materials an implant case is typically made of titanium or implant grade stainless steel in body wires are made of platinum and iridium which have conductivities in the order of 9 ms m and 5 ms m respec tively the conductivity of copper is 58 ms m at present these are the only two metals that can be used for conductors to contact the body the low value of electrical conductivity should be compensated by using the thickest material possible the substrate needs to be non toxic mechanically stable and insoluble in blood fig 5 base station and pc j med syst or other body fluids alumina is a material found to be acceptable earlier titania ziconia and multi layer substrates have been considered the entire implant is often coated in a passive material such as parylene table 4 shows parylene has good water resistant prop erties compared to other materials and is acceptable for in body use typical coatings are in the order of a few microns thick coating cannot be used to isolate a conductor from the body as blood will dissolve most plastics or coating and will become porous signal propagation the propagation pattern of the antenna is required to predict the performance of an implant measurements can be made using a body phantom and immersing a battery test implant into it the authors of conducted several experiments to analyze the perfor mance of an implant inside a human body phantom the phantom was filled with a liquid that mimicked the electrical properties of the human body tissues the distance from the body phantom to the base station was m further details can be found in where the authors made useful measurements over a set distance with all combinations of implant and test antenna polarisations i e vertical vertical v v horizontal vertical h v vertical horizontal v h and horizontal horizontal h h polarisations typ ical results are shown in fig 6 where the effective radiated power erp is calculated from the received signal power and the antenna characteristics it can be seen that there is a significant difference in signal levels with polarisation combinations and depth from the results and known antenna parameters the erp can be calculated the v v polarisation combination shows an increase in signal level at 4 cm depth and then shows a decline from the figure it can be seen that signal level is affected by the depth and different polarisation combinations such a test needs to be done with an antenna that is to be used in the final product although most of the performance of an implant link is assessed by measurement using the body phantom simulation can also help to understand the propagation table 4 water uptake and other parameters of various polymers noting these are not all biocompatible 5 material ε r loss water tangent absorption parylene c type 9 0 013 0 01 polyether ketone 4 0 005 0 11 polyether imide 3 0 0026 0 polyether ether ketone 3 3 0 0035 0 11 fig 6 erp vs depth of polarisation combinations pattern k sayrafian et al carried out extensive simula tions at national institute of standards and technology nist to characterize the mics path loss for commu nication to from an implant or between two implants inside a human body the model was based on 4 near surface implants and deep tissue implants for a typical male body figure 7 shows the mics path loss for different communication scenarios where the near surface implants reside within a definable distance mm in the figure from the body surface and the deep tissue implants completely reside inside the body it can be seen that direct communication between two deep implants results in an increasing path loss there fig 7 path loss model vs distance scatter plot j med syst fore peer to peer communication between the implants should not considered however an indirect communi cation through a central coordinator is possible in case of on body communication systems signals often propagate across the body surface this propa gation may be a combination of surface waves creep ing waves diffracted waves scattered waves and free space propagation depending on the antenna position 17 like implantable antennas the design of on body antennas and the signal propagation on the human body is becoming increasingly important unlike in body communication antennas required for on body communication should direct the radiation along the body surface with the appropriate polarisation sev eral groups studied the nature of on body propaga tion where characterization of path loss with distance and path gain are examined at several frequencies kamarudin et al performed the measurements of the path loss of various antennas for on body communi cation and concluded that for the majority of antenna positions and body postures monopole antennas yield the lowest loss however thin wire monopoles are susceptible to damage and therefore antennas in the form of buttons are developed for 4 ghz and 5 8 ghz ism bands and for the uwb band respectively a study conducted at the university of birmingham concluded that some on body links achieve benefit from diversity 19 the stationary body results in some multipath fading but body movement gives rise to fad ing table 5 shows the diversity gain in two channels obtained by using a diversity antenna consisting of two quarter wavelength monopoles spaced by 5 3 cm on a continuous ground plane the transmit antenna was fixed on the belt the table shows that diversity gain is greater for the belt to head channel than that of the belt to ankle for other postures the diversity gain is comparatively less further details about on body antennas and propagation are present in and discussion while designing an implant careful consideration must be given to propagation pattern of the antenna the implant case and the materials used there are a table 5 diversity gain at 4 ghz ism band for jogging postures diversity gain rx placement rx placement right head ankle selection combining 8 57 5 equal gain combining 9 62 6 13 mrc 28 6 number of antenna options to be used in a given net work testing to determine antenna characteristics is important to ensure the effective design of a matching network additionally multiple antennas can be used if there is a polarisation or multi path fading problem with careful measurement and observation including a clear understanding of the above topics a number of implants can be effectively designed in high volume and can be integrated into a patient monitoring system for unobtrusive health monitoring the above discussion only considered communi cation to or from a single implant when multiple implants are deployed in a human body it requires energy conserving mechanisms to efficiently utilize and share the channel in other words it requires a power efficient mac protocol that should control the channel access and the dominant sources of energy waste the next sections present a comprehensive discussion on mac layer communication in wban mac layer communication in this section we discuss the role and importance of mac protocols for wban we first outline ma jor mac requirements of wban then we analyze and compare many existing low power mechanisms such as low power listening lpl contention and scheduled contention and time division multiple ac cess tdma mechanisms for wban additionally we overview a number of proposed mac protocols for wban including a case study of our proposed protocol and discuss their strengths and weaknesses the final section presents the discussion general overview in wban the rf part of the sensor consumes most of the energy and hence becomes one of most important entities to be considered the mac protocol plays a significant role in controlling duty cycling the rf mod ule and in reducing the average energy consumption of the sensor node in other words the mac protocol is required to achieve maximum throughput minimum delay and to maximize the network lifetime by control ling the main sources of energy waste i e collision idle listening overhearing and control packet overhead a collision occurs when more than one packet transmits data at the same time the collided packets have to be retransmitted which consumes extra energy the second source of energy waste is idle listening meaning that a node listens to an idle channel to receive data the third source is overhearing i e to receive packets j med syst table 6 csma ca vs tdma protocols performance metric csma ca tdma power consumption high low traffic level low high bandwidth utilisation low maximum scalability good poor effect of packet failure low latency synchronisation not applicable required that are destined to other nodes the last source is con trol packet overhead meaning that control information are added to the payload a minimal number of control packets should be used for data transmission generally mac protocols are grouped into contention based and schedule based mac protocols in contention based mac protocols such as carrier sense multiple access collision avoidance csma ca protocol nodes contend for the channel to transmit data if the channel is busy the node defers its transmission until it becomes idle these protocols are scalable with no strict time synchronization constraint however they incur significant protocol overhead in schedule based protocols such as tdma protocol the channel is divided into time slots of fixed or variable duration these slots are assigned to nodes and each node transmits during its own slot period these protocols are energy conserving protocols the duty cycle of the radio is reduced and there is no contention idle listening and overhearing problems however these protocols require frequent synchronization table 6 compares csma ca and tdma protocols mac requirements as comprehensively discussed in the most impor tant attribute of a good mac protocol for wban is en ergy efficiency in some applications the device should support a battery lifetime of months or years without interventions while others may require a battery life of tens of hours due to the nature of the applications for example cardiac defibrillators and pacemakers have a lifetime of more than 5 years while swallowable camera pills have a lifetime of 12 h 4 power efficient and flexible duty cycling techniques are required to solve the idle listening overhearing and packet collisions problems furthermore low duty cycle nodes should not receive frequent synchronization control packets beacon frames if they have no data to send or receive the wban mac should satisfy the mac transparency requirements i e to operate on multiple physical layers mics ism and wmts simultane ously figure 8 shows some of the potential issues of a mac protocol for wbans the quality of service qos is also an impor tant factor of a good mac protocol for wban this includes point to point delay and delay variation in some cases real time communication is required for many applications such as fitness and medical surgery monitoring applications for multimedia applications latency should be less than ms and jitter should be less than ms however reliability latency and jitter requirements depend on the nature of the applications for emergency applications the mac protocol should allow in body or on body nodes to get quick access to the channel in less than one second and to send the emergency data to the coordinator one such example is the detection of irregular heartbeat high or low blood pressure or temperature and excessively low or high blood glucose level in a diabetic patient another example is when the node is dying reporting medical emergency events should have a higher priority than non medical emergency battery dying events since most of the traffic in wban is correlated a single physiological fluctuation triggers many fig 8 potential issues of a mac protocol for wban j med syst sensors at the same time in this case a csma ca protocol encounters heavy collisions and extra energy consumption additionally in csma ca protocol the nodes are required to perform clear channel assess ment cca before transmission however the cca is not always guaranteed in the mics band since the path loss inside the human body due to tissue heating is much higher than in free space bin et al studied the unreliability of a cca in wban and concluded that for a given dbm cca threshold the on body nodes cannot see the activity of the in body nodes when they are away at 3 m distance from the surface of the body 25 the behavior of the csma ca protocol for wban is studied in where the authors concluded that the csma ca protocol encounters heavy collision problems for high traffic nodes tdma based proto cols provide good solutions to the traffic correlation heavy collision and cca problems these protocols are energy conserving protocols because the duty cycle is reduced and there are no contention idle listening and overhearing problems however common tdma needs extra energy for periodic time synchronization all the sensors with and without data are required to receive control packets periodically in order to synchro nize their clocks low power mechanisms in wban low power mechanisms play an important role in the performance of a good mac protocol for wban they are categorized into low power listening lpl contention and scheduled contention and tdma mechanisms the following sections briefly explain each mechanism in the context of wban with exam ples further details about these mechanisms can be found in low power listening in the low power listening lpl mechanism nodes wake up for a short duration to check the channel activity without receiving any control packet if the channel is idle the nodes go into sleep mode otherwise they stays on the channel to receive the data this is also called channel polling the lpl is performed on a regular basis regardless of synchronization among nodes the sender sends a long preamble before each message in order to detect the polling at the receiver the bmac protocol is based on the lpl mecha nism where an adaptive preamble sampling technique is used to minimize idle listening and overhearing the lpl method has several advantages and disadvantages the periodic sampling is efficient for high traffic nodes and performs well under variable traffic conditions however it is ineffective for low traffic nodes espe cially for in body nodes where periodic sampling is not preferred due to strict power constraints since the wban topology is a star topology and most of the traffic is uplink using lpl mechanism is not an optimal solution to support both in body and on body communication simultaneously contention and scheduled contention in the contention based mechanism nodes contend for the channel to transmit data regardless of any pre defined schedule the csma ca protocol is a best example of the contention based mechanism however in some cases we need to use a hybrid approach to ac cess the channel i e a combination of contention and scheduling mechanisms called a scheduled contention mechanism in this mechanism scheduled and con tention based schemes are combined to incur scalability and collision avoidance the nodes adapt a common schedule for data communication the schedules are exchanged periodically during a synchronization pe riod if two neighbouring nodes reside in two different clusters they keep the schedules of both clusters which results in extra energy consumption the smac 28 protocol is a good example of a scheduled contention mechanism designed for multi hop wireless sensor networks wsns the scheduled contention mecha nism reduces idle listening using sleep schedules and performs well for multi hop wsns however consid ering this mechanism for wban reveals several prob lems for low power in body on body nodes such as pacemakers and defibrillator implants which should not wake up periodically in order to exchange their schedules with other nodes furthermore scheduled contention mechanism may perform well for on body applications but it does not provide reliable solutions to handle sporadic events including emergency and on demand events handling sporadic events emergency require innovative solutions that allow in body on body nodes to update the coordinator within strictly limited amount of time tdma in the tdma mechanism the channel is bounded by a superframe structure that consists of a num ber of time slots allocated by a base station or a coordinator each node is assigned at least one slot enough to complete the transmission multiple slots can be assigned depending on the data volume this mechanism is probably the best for wban since the j med syst time slots can be allocated to the nodes according to their traffic requirements although it performs well in terms of power consumption but consumes limited energy due to frequent synchronization the preamble based tdma pb tdma protocol is based on the tdma mechanism 29 where nodes are as signed specified slots for collision free data transmis sion using the preamble as discussed in section mac requirements the csma ca protocol is not a re liable protocol for wban due to unreliable cca traffic correlation and heavy collision problems the alternative is to adapt a tdma protocol that can solve the aforementioned problems in a power efficient manner however traditional tdma protocols such as pb tdma have several problems e g preamble overhearing and limitation of handling sporadic events solving these problems including many others for wban can reliably accommodate the heterogeneous traffic requirements comparison of low power mechanisms table 7 presents the characteristics of the lpl schedule contention and tdma mechanisms for wban it shows that the lpl and the scheduled contention are unable to accommodate the hetero geneous wban traffic including sporadic events although it is possible to develop new mac pro tocols based on these mechanisms they will not be able to satisfy all the requirements for example lpl mechanisms may perform well in case of periodic traffic but they are unable to accommodate aperiodic unpredictable sporadic events traffic and low duty cycle nodes in wban furthermore the scheduled contention mechanisms are unable to accommodate in body nodes that do not require frequent synchro nization or exchange of their schedules the tdma mechanisms provide good solutions to the variable wban traffic the slots can be assigned according to the traffic volume of a node although in traditional tdma protocols nodes are required to synchronize at the beginning of each superframe boundary this approach can be optimized for nodes that do not re quire frequent synchronization one of the design ap proaches is to skip the synchronization control packets such as beacons and receive them whenever the nodes have data to send receive a detailed comparison of mac protocols based on lpl scheduled contention and tdma mechanisms for wban is presented in and summarized in appendix a proposed mac protocols for wban the following sections give a brief overview of different mac protocols proposed for wban and highlight their strengths and weaknesses ieee 4 mac protocol ieee 4 is a low power standard designed for low data rate applications 30 it offers three op erational frequency bands mhz mhz and 2 4 ghz bands there are 27 sub channels allocated in ieee 4 i e sub channels in 2 4 ghz table 7 comparison of lpl scheduled contention and tdma mechanisms for wban lpl scheduled contention tdma times less expensive than listening for full contention period low duty cycle listening for full contention period asynchronous synchronous synchronous fine grained time synchronisation sensitive to tuning for neighbourhood sensitive to clock drift very sensitive to clock drift size and traffic rate poor performance when traffic rates improved performance with increase limited throughput and number of active nodes vary greatly optimised for know in traffic periodic traffic receiver and polling efficiency similar cost incurred by sender and require clustering cost incurred more on is gained at the much greater receiver cluster head cost of senders challenging to adapt lpl directly scalable adaptive and flexible limited scalability and adaptability to changes to new radios like ieee 4 on number of nodes unable to accommodate aperiodic low duty cycle nodes do not require low duty cycle nodes do not require frequent traffic unpredictable sporadic events frequent synchronization exchange synchronization at the beginning of each and low duty cycle nodes in wban of schedules in wban hard to superframe easy to satisfy the wban traffic very hard to satisfy the wban traffic satisfy the wban traffic heterogeneity requirements heterogeneity requirements heterogeneity requirements j med syst band sub channels in mhz band and one sub channel in the mhz band ieee 4 mac has two operational modes a beacon enabled mode and a non beacon enabled mode in a beacon enabled mode the network is controlled by a coordinator which regularly transmits beacons for device synchronization and association control the channel is bounded by a superframe structure as illustrated in fig the superframe consists of both active and inactive periods the active period contains three components a bea con a contention access period cap and a con tention free period cfp the coordinator interacts with nodes during the active period and sleeps during inactive period there are maximum seven guaranteed time slots gts in the cfp period to support time critical traffic in the beacon enabled mode a slotted csma ca protocol is used in the cap period while in the non beacon enabled mode unslotted csma ca protocol is used ieee 4 has remained the main focus of re search during the past few years some of the main reasons of selecting ieee 4 for wban are low power communication and support of low data rate wban applications nicolas et al investigated the performance of a non beacon ieee 4 for low upload download rates mostly per hour 31 they concluded that the non beacon ieee 4 results in to years sensor lifetime for low data rate fig 9 a ieee 4 superframe structure b tdma superframe structure of battery aware tdma protocol c tdma timing of energy efficient tdma mac protocol 38 d superframe structure of priority guaranteed mac protocol 39 a b c d j med syst 1094 and asymmetric wban traffic however their work considered data transmission on the basis of periodic intervals which is not a real wban scenario further more the data rate of in body and on body nodes varies ranging from kbps to mbps which reduces the lifetime of the sensor nodes li et al studied the behav iour of slotted and unslotted csma ca mechanisms and concluded that the unslotted mechanism performs better than the slotted one in terms of throughput and latency but with high cost of power consumption additionally dave et al studied the energy efficiency and qos performance of ieee 4 and ieee 33 mac protocols under two generic ap plications a wave form real time stream and a real time parameter measurement stream 34 a series of experiments to evaluate the impact of ieee 11 and microwave ovens on the ieee 4 transmis sion are carried out in 35 other details about ieee 4 for wban can be found in 23 battery aware tdma protocol the authors of 36 proposed a battery aware tdma protocol for wban which takes into account the bat tery discharge dynamics wireless channel models and packet queuing characteristics the battery recovery capacity effects 37 are considered to maximize the idle periods of sensor nodes while maintaining the required qos in addition the proposed protocol utilizes the electrochemical properties of batteries and the char acteristics of wireless fading channels the channel is bounded by tdma superframe structures as given in fig where each superframe consists of a beacon an active period and an inactive period the beacon is used to indicate the length of the frame period and helps to estimate the channel information the sen sor nodes receive the beacons periodically and sub sequently transmit data in the active period no data transmission takes place in the inactive period the low duty cycle nodes can decide whether or not to receive beacons and to utilize the channel at different superframes although this protocol efficiently recov ers the battery capacity and prolongs the lifespan of sensor nodes it has several drawbacks first the average delay and the packet drop rate are high since in some cases the nodes hold the packets in their buffers for long time second there is no reliable mechanism for life critical sporadic or emergency events third the current version of the protocol is not suitable for implants however it can be easily improved to achieve reliable communication in the mics band finally the protocol considers nakagami distribution which does not accurately represent the complexity of real time wban applications energy ef f icient tdma based mac protocol this protocol 38 is mainly developed for streaming large amount of data the authors of the paper ex ploited the fixed network structure static topology of wban and implemented an effective tdma strat egy with little amount of overhead and no overhear ing since the target topology is static and does not change frequently in time no complex synchronization mechanisms are required the network topology is hierarchical with a number of sensor nodes a master node mn that coordinates the synchronization and transmission to the sensor nodes and a monitoring station ms that gathers data from the mns directly for further analysis figure shows the tdma frame structure of the protocol the timeslots s n are allocated to n different nodes the slots are separated by a guard interval which is necessary to prevent overlap ping of transmission form different sensor nodes extra slots rs n are reserved for retransmission the proto col defines two options for communication between mn and ms first when the mn has one transceiver enough time should be given to complete communica tion between mn and ms second when the mn has two transceivers this allows the mn to communicate both with the sensor nodes and the ms simultaneously the second option is complex in terms of implemen tation however it can be used to support communi cation on multiple physical layers simultaneously and can easily satisfy the mac transparency requirement of wban it is shown in the paper that the proposed protocol is energy efficient for streaming communica tion as well as for sending short bursts of data the shortcomings include first it has limited superframe structure only cfp based on pure tdma mechanism with no cap period the cap period can be used to send short data frames when tdma slot duration exceeds the data transmission duration including com mand frames second wban topology is not always static as considered by the protocol and sometimes sensor nodes can be deployed for short period of time e g an endoscope which is expelled probably after 12 h the proposed protocol does not respond well when the wban topology is dynamic however this can happen very often and is not a serious problem finally the protocol lacks a proper wake up mechanism for low duty cycle nodes in case of on demand events j med syst 36 1094 priority guaranteed mac protocol in this protocol 39 the superframe structure is a combination of cap and cfp periods and is defined by the beacon boundaries as given in fig the cap period is further divided into two control pe riods and which are used for life critical medical applications and consumer electronics ce applications respectively the and periods use randomized slotted aloha for resource allocation the use of two separate and dedicated control periods isolates life critical medical communication from much busier ce traffic the cfp period is divided into time slot reserved for periodic traffic tsrp and time slot reserved for bursty traffic tsrb periods the tsrp and tsrb periods are used for periodic and bursty traffic respectively the tdma slots in the tsrp and tsrb periods are allocated on demand using the control periods as demonstrated by the simulations this protocol showed significant improve ments on throughput and power consumption as com pared to ieee 4 mac protocol the drawbacks of this protocol are its complex superframe structure and its inadaptability to emergency and on demand traffic others mac protocols for wban such as heart beat driven mac h mac 40 reservation based dynamic tdma dtdma and bodymac 42 are briefly discussed in 23 a power efficient mac protocol for wban in this section we discuss our proposed power efficient mac protocol for wban this protocol accommo dates normal emergency and on demand traffic in a reliable manner 43 additionally it has two wakeup mechanisms a traffic based wakeup mechanism which accommodates normal traffic by exploiting the traffic patterns of the nodes and a wakeup radio mecha nism which accommodates emergency and on demand traffic by using a wakeup radio like the previous protocols the channel is bounded by the superframe structures the superframe is divided into cap and cfp periods the cap period uses slotted aloha and is used for resource allocation the cfp period is used for data transmission including real time commu nication in the traffic based wakeup mechanism the operation of each node is based on traffic patterns the initial traffic pattern is defined by the manufac turer coordinator and can be changed later these pat terns are repeated per ban day ban hour ban minute ban second and ban millisecond this cat egorization allows simple representation of the traffic levels for example a high traffic node sends data x times per ban minute or ban millisecond the co ordinator can change the traffic levels from low to high vice versa by simply changing the traffic patterns the traffic patterns of all nodes are organized into a table called traffic based wakeup table in the wakeup radio mechanism a separate control channel is used to send a wakeup radio signal the coordinator and the node initiates sending the wakeup radio signal in on demand and emergency case respectively further details can be found in 43 we simulated the protocol using the monte carlo method poisson and deterministic traffic generators were used to generate aperiodic emergency and on demand and periodic normal traffic respectively for the deterministic traffic the gaussian distribution was used to incorporate randomness in the relative offsets of the nodes the average power consumption and delay were presented as a function of packet and beacon inter arrival time if more than one node ap peared to have the same traffic patterns the priority concept was used to ensure fair resource allocation the performance of the protocol was compared to the low power wisemac 44 protocol figure shows the average power consumption of the proposed pro tocol and compares it with the wisemac protocol it can be seen that the average power consumption of the proposed protocol outperforms wisemac protocol since the nodes are required to wake up whenever they have data to send receive the extra power con sumption used for preamble sampling in the wisemac protocol is avoided since we used two generators de terministic and poisson the arrival of poisson traffic affects the average power consumption of the protocol as indicated by a slight curve change in the figure for normal traffic only with no emergency and on demand events the proposed protocol provides rela tively low power consumption as shown in the figure in the wisemac protocol if a node has a packet to send receive it waits until the medium is sampled this increases the delay of the wisemac protocol if the medium is busy or if the sampling period is high however in the proposed protocol a node wakes up whenever it has a packet to send receive it does not have to wait for resource allocation information beacon since the traffic patterns are pre defined and known to the coordinator this results in a reasonable delay as shown in fig j med syst 36 1094 a b 2 d wisemac proposed d n o c e n i y a l e 0 0 2 beacon inter arrival time fig a average power consumption of the proposed protocol b average delay of the proposed protocol mac security the deployment of wban for medical and non medical applications must satisfy stringent security and privacy requirements 45 these requirements are based on different applications ranging from medical heart monitoring to non medical listening to applications in the case of medical applications secu rity threats may lead a patient to dangerous conditions and sometimes to death thus a strict and scalable security mechanism is required to prevent malicious interaction with wban a secure wban should in clude confidentiality and privacy integrity and authen tication key establishment and trust set up secure group management and data aggregation however the integration of a high level security mechanism in a low power and resource constrained sensor increases computational communication and management costs in wban both security and system performance are equally important and thus designing a low power and secure mac protocol for wban is a fundamental challenge to the designers generally the application layer is used to explic itly enable the security by adjusting certain control parameters for example in ieee 4 the ap plication layer has a choice of security modes that control different security levels each security mode has different security properties protection levels and frame formats the ieee 4 based security modes can be used improved for wban according to the application requirements table 8 lists different se curity modes described in the ieee 4 standard broadly classified into no security encryption only aes ctr authentication only aes cbc and en cryption and authentication aes ccm modes the following sections shortly discuss aes ctr aes cbc and aes ccm modes for wban aes ctr the counter ctr also known as integer counter mode mode can be used in wban in order to en crypt data it breaks the cleartext into byte blocks b b 2 b n and computes c i b i e k x i where c i is the cipher text b i is the data block and e k x i is the encryption of the counter x i the coordinator recovers the plaintext by computing b i c i e k x i figure shows the ctr encryption and decryption process table 8 security modes in ieee 4 name description null no security aes ctr encryption only ctr mode aes cbc mac bit mac aes cbc mac bit mac aes cbc mac bit mac aes ccm encryption and bit mac aes ccm encryption and bit mac aes ccm encryption and bit mac j med syst 36 1094 a b c fig 11 a ctr encryption and decryption b cbc mac opera tionc c energy cost of security protocols 46 aes cbc mac in the cipher block chaining message authentication code cbc mac 2 mode the plaintext is xored with the previous cipher text until the final encryption is achieved this mode provides authentication and mes sage integrity by allowing the wban nodes to compute either 32 bits 64 bits or bits message authentica tion code mac the coordinator computes its own mac and compares it with the node mac the coor dinator accepts the packet if both macs are equal the mathematical representation of the cbc mac is given by c i e k b i c i for generating ciphertexts and b i d k c i c i for generating plaintexts figure shows a block diagram of the cbc mac operation differentiate it from the term medium access control mac message authentication code mac is represented in bold letters aes ccm the counter with cbc mac ccm mode combines ctr and cbc modes in order to ensure high level security that includes both data integrity and encryp tion the nodes first apply the integrity protection to the mac frames using cbc mac mode and then encrypts the frames using ctr mode this mode can be used to send or receive sensitive information such as updating programs in pacemakers and implantable cardiac defibrillators generally the addition of security protocols to wban consumes extra energy due to overhead trans mission required by the protocols as given in fig 46 the best way is to use a stream cipher for en cryption where the size of the ciphertext is exactly the same as the plaintext in this case the mac uses bytes of 60 bytes data frame moreover the cyclic redundancy check crc is not required since the macitself achieves data integrity law et al concluded that the most energy efficient cipher is rijndael 47 they examined the number of cpu cycles during they key setup and encryption decryption procedures the summary of different ciphers is given in appendix b discussion mac protocols play a significant role in determining the energy consumption in wban the existing low power mac mechanisms do not satisfy the wban mac requirements the lpl and schedule contention mechanisms are unable to accommodate unpredicted sporadic events as well as low duty cycle nodes the tdma mechanism on the other hand is a good al ternative to be used in wban therefore most of the existing mac protocols for wban are based on tdma mechanisms as discussed above these proto cols have several pros and cons in the context of a real wban system none of them considered the mac transparency requirement i e to allow communication on multiple physical layers simultaneously in addition these protocols are not targeted for mics band perhaps due to listen before talking lbt criteria and fcc restrictions 23 security is also one of the most impor tant issues in wban since the devices are used to col lect sensitive life critical information and may operate in hostile environments the current ieee 6 is working on the standardization of wban including the development of a unified mac protocol the new standard will target all the requirements mentioned in section mac requirements 1082 j med syst 36 1094 although mac protocols can solve a variety of problems they do not consider addressing nor do they ensure end to end packet delivery it is therefore important to consider the network layer and more specifically the routing in wban the next sections focus on network layer communication in wban network layer communication developing efficient routing protocols in wbans is a nontrivial task because of the specific characteristics of the wireless environment on the human body the available bandwidth is limited shared and can vary due to fading noise and interference as a re sult the network control generated by the protocol should be limited 2 the nodes that form the network can be very het erogeneous in terms of available energy or comput ing power as a result node energy should also be taken into account 3 an extremely low transmit power per node is needed to minimize interference to cope with health concerns and to avoid tissue heating 48 4 the devices are located on the human body that can be in motion wbans should therefore be robust against frequent changes in the network topology in this section we give an extensive overview of existing solutions for routing in wban first we dis cuss its relationship to wsns second the most ideal topology is discussed third an overview of existing wban routing solutions is given the routing solutions are grouped in three different strategies temperature aware routing cluster based routing and cross layer routing the cross layer protocol is discussed in more detail finally this section is concluded with future research directions routing wsn vs wban a lot of research is being done towards energy efficient routing in ad hoc networks and wsns 49 but the proposed solutions are inadequate for wbans for example in wsns maximal throughput and minimal routing overhead are considered to be more important than minimal energy consumption energy efficient ad hoc network protocols only attempt to find routes in the network that minimize energy consumption in ter minals with small energy resources thereby neglecting parameters such as the amount of operations measure ments data processing access to memory and energy required to transmit and receive a useful bit over the wireless link even worse loss of a sensor is not con sidered to be problematic in wban the number of devices worn by a patient should be very low in order to have good patient comfort most protocols for wsns only consider networks with homogeneous sensors an assumption that is incorrect when considering different devices in wban each with their different required data rate in many cases the network is considered as static in contrast wban has heterogeneous mobile devices with stringent real time requirements due to the sensor actuator communication the mobility in sensor networks is considered on a scale of meters or tens of meters however movements of tens of centime ters can cause mobility in wbans in brief although challenges faced by wbans are in many ways similar to wsns there are intrinsic differences between the two requiring special attention network topologies network topology is defined as the logical organiza tion or arrangement of communication devices in the system the selection of a proper network topology in wban is important as it significantly affects the overall system performance and protocol design it influences the system in many ways for example in power con sumption the ability to handle heterogeneity the ro bustness against failures and the routing of data etc there has been very little research about the most optimal network architectures in wban most re searchers assume that a single hop topology where every sensor directly communicates with the personal device is the best solution hence almost all current implementations in wban assume single hop commu nication however the devices used in these implemen tations are not fitted for wban as they are too big to allow easy use by the person wearing the wban in this overview we will focus on energy efficiency and reliability of topologies energy ef f iciency in 51 a first attempt is done to justify the use of multi hop in wban where intermediate hops are used for reaching the receiver they use an energy model that is only applicable for an energy usage comparison of uwb communication from a node on the back of a person to a node on the chest it is concluded that whether to use a multi hop strategy or not de pends on the ratio of the energy consumption needed for decoding coding and receiving generating a uwb pulse in 52 a preliminary study based on simulations of the physical layer showed the necessity for multi j med syst 36 1094 hop networking this work only considered the energy consumption of the entire network not taking into account the individual nodes both studies focused on maximizing the lifetime of the network and concluded that in some cases a multi hop topology is more energy efficient in 53 different topologies are evaluated for a tdma based mac protocol for wban it is concluded that a cluster based topology is more energy efficient however they use a simplified propagation protocol which does not take into account the large losses experienced on the human body a more profound study about energy efficiency in wbans has been performed in 54 where the authors showed that single hop communication is inefficient especially for nodes located far away from the sink their analysis is based on path loss models derived for communication on the human body where path loss is very high compared to free space propagation when the sender and receiver are in line of sight los a path loss exponent of about 3 to 4 is found in 55 on the other hand when the communication is non line of sight nlos e g when the sender is placed on the back and the receiver on the chest a path loss exponent of 7 is found in 56 as given in fig and b this means that the path loss around the human body may thus tremendously exceed the path loss for propagation in free space where usually a path loss exponent of 2 is considered due to these high losses in nlos situations direct communication between the sensors and the sink will not always be possible this is especially the case when one wants to lower the radio transmission power hence multi hop networking becomes advantageous and sometimes even an absolute requirement to ensure connectivity of the network these models are used in 54 for determining whether multi hop or single hop is the most energy efficient approach in a line or a tree topology figure 13 shows the ratio of multi hop energy usage over single hop energy usage for a scenario with 4 nodes and changing distances between the nodes the results show that the nodes closest to the sink do not perform well a b fig 12 a communication path on the body b the resulting line topology fig 13 comparison of the energy usage when using single hop and multi hop communication in the line scenario with 4 nodes the distance d between the nodes is varied from 0 0 4 m the ratio of the energy usage of the multi hop and single hop communication is given when the ratio is above the horizontal line single hop is the most energy efficient when using multi hop they become hot spots using more than times the energy of single hop this is shown in fig which explicitly plots the energy consumption in single hop and multi hop in the single hop architecture nodes and 2 consume more energy when they send their data directly to the central device the proposed solution is to use the residual energy of nodes 3 and 4 for relaying data from other nodes stated otherwise to let the nodes cooperate in the network indeed by breaking into this energy supply the lifetime of the network will be higher as the advantages of a fig 14 energy usage when using cooperation compared to single hop and multi hop energy consumption nodes c and d will hear many of the packets sent from a to b and it is wasteful that node b forwards these packets this example shows the trade off between the reliability and energy efficiency in general we consider wban to be a converge cast network all data is generated at the nodes and sent to a central device the sink from a network traffic point of view this can be considered as an asymmetric structure most of the data will flow to the sink while a limited amount of control traffic will go in the other direction routing algorithms can exploit this very specific topol ogy a more dynamic wban should also make sensor to sensor or sensor to actuator traffic feasible in a basic asymmetric converge cast network this is possi ble by routing all traffic between two nodes over the sink routing should be optimized to avoid this trian gular routing situation overall based on the previous it can be concluded that a multi hop architecture is the best choice for wban and that one has to deal with a trade off between energy efficiency and reliability routing strategies in wban in this section we give an overview of existing rout ing strategies for wban these can be categorized as follows temperature based routing cluster based routing and cross layer based routing an overview can be found in fig temperature routing when considering wireless transmission around and on the body important issues are radiation absorption and heating effects on the human body 58 to avoid heat generation five thermal aware routing protocols are proposed in the existing literature fig 16 schematic overview of routing protocols in wban j med syst 36 1094 fig example of a connection probability in a single hop and a multi hop scenario the distance between node a and b is cm and between node c and d multi hop network for nodes and 2 is combined with the available energy at nodes 3 and 4 thus instead of only forwarding data to the most nearby node the data is transported more intelligently and the burden of forwarding the data is more equally spread among the nodes the architecture no longer only considers the naive approach of only sending data to the direct neighbours of a node reliability earlier reliability was not considered in 57 reliability is experimentally investigated by measuring the packet delivery ratio without imposing a mac protocol a multi hop strategy turns out to be the most reliable in order to develop an intuition for why there might be a room for improvement in multi hop routing it is helpful to consider fig different nodes are placed on one line and different routes are shown for communication between nodes a and d the numbers above the communication links show the link probability between the 2 nodes at one extreme node a could send directly to d in one hop and at the other extreme a could use the 3 hop route through b and c in the example it is clear that the 3 hop communication has a communication probability of 63 7 whereas the single hop communication only is on the other hand in multi hop communication j med syst 36 1094 to reduce tissue heating the radio transmission power can be limited or traffic control algorithms can be used in 59 the authors proposed a model for the bioeffect caused by biosensors to the human body both for near field and far field communication using the sar they showed that the bioeffects caused by radio frequency radiation are highly related to the incident power density network traffic and tissue characteris tics based on this observation they derived a guideline for designing wban the normalized bioeffect metric or coefficient of absorption and bioeffects cab a price based rate allocation algorithm further showed that the bioeffects can be reduced via power scheduling and traffic control algorithms another approach is to balance the communica tion over the sensor nodes which is used by thermal aware routing algorithm tara least tempera ture routing ltr and adaptive least temperature routing altr the tara routes data away from high temperature areas due to focusing data communi cations defined as hot spots 60 in order to quickly calculate the temperature increase the authors defined the temperature increase potential tip which is based on the sar each node monitors neighbours packet counts and calculates the communication radia tion and power consumption to derive the current tem perature of the neighbours when the temperature of a neighbouring node is above a certain threshold i e the node is becoming a hot spot the packets will no longer be forwarded to the node but will be withdrawn and rerouted through alternate paths the algorithm leads to a better temperature distribution over all the nodes in the network however tara only considers the temperature as a metric consequently it suffers from low network lifetime a high ratio of dropped packets and low reliability which is problematic for wban improvements of tara are ltr and altr unlike tara ltr always chooses the neighbouring node with the lowest temperature as the next hop for routing in order to maintain the network bandwidth a predefined maximum hop count is used when the number of hops exceeds this maximum the packet is discarded loops are avoided by maintaining a list in the packet with the recently visited nodes thus when the coolest neighbour is already on the list the packet will be forwarded to the second coolest neighbour if it is not in the list altr is similar to ltr with the dif ference that when the packets exceeds the maximum hop count it can use shortest hop routing to take the packet to the destination as quickly as possible an ex ample of ltr and altr is given in fig 17 both ltr and altr suffer from the same problem as tara they do not optimize their routing in terms of energy efficiency reliability or delay in larger sensor networks almost half of the generated packets are be dropped due to excess hop count 62 leading to low packet de livery ratio wastage of energy and even rise in the tem perature in the network a ltr can be considered as a greedy algorithm that is not globally optimized the simulations in 61 further showed that tara ltr and altr have shorter lifetime than shortest path routing due to the rerouting of the packets a smarter combination of ltr and shortest path routing is least total route temperature ltrt 62 ltrt selects a least temperature route instead of only considering the next hop the node temperatures are converted into graph weights and minimum temper ature routes are obtained using dijkstra algorithm the node temperature rises by unit when a packet is received and decreases by unit when no packet is received during a predefined time interval ltrt experiences a lower hop count per packet a lower number of packets dropped and a lower temperature rise is obtained ltrt has as main disadvantage that a node needs to know the temperature of all nodes in the network neither the overhead of obtaining this data nor energy consumption was investigated in general temperature routing can be considered as a specific case of weight based routing results are promising but reliability and energy efficiency can be hard to guarantee cluster based routing anybody 63 is a data gathering protocol that uses clustering to reduce the number of direct transmissions to the remote base station it is based on leach 64 that randomly selects a cluster head at regular time fig 17 an example of ltr and altr the white arrows indicate the ltr path the shaded arrows show the adapted path of altr when the path has three hops the routing algorithm switches to shortest path routing remark that if the maximum hop count were limited to five or less the packet would have been dropped 1086 j med syst 36 1094 intervals in order to spread the energy dissipation the cluster head aggregates all data and sends it to the base station leach assumes that all nodes are within the sending range of the base station anybody solves this problem by changing the cluster head selection and constructing a virtual backbone network of the cluster heads energy efficiency is not thoroughly investigated and reliability is not considered the authors only focus on the number of cluster heads depending on the scale a more profound problem lies in the fact that simulations are based on networks where nodes have hundreds of neighbours and communication ranges of up to m another improvement of leach is hybrid indirect transmissions hit 65 which combines clustering with forming chains in doing so the energy efficiency and as a consequence network lifetime is improved specifically in sparse wban reliability however is not considered another open problem with hit is the conflicting interaction between its communication routes and those desirable for application of the defined fusion operator as a result hit requires more com munication energy in a dense network cross layer protocols cross layer design is a way to improve the efficiency of and interaction between the protocols in a wsn by combining two or more layers from the protocol stack this research has gained a lot of interest in wsns 66 67 however little research has been done for wbans ruzelli et al proposed a cross layer energy efficient multi hop protocol built on ieee 4 68 the network is divided into timezones where each timezone takes turns in the transmission the nodes in the farthest timezone start the transmission in the next slot the farthest one sends its data and so on until the sink is reached the protocol almost doubles the lifetime compared to regular ieee 15 4 the protocol is developed for regular wsns but the au thors claim its usefulness for wbans the primary reason is the absence of direct connectivity between nodes when located at opposite sides of a human body or when radio transmission power is limited to save energy the wireless autonomous spanning tree pro tocol wasp 69 sets up a spanning tree and divides the time axis in slots the slot allocation is done in a distributed manner every node sends out a propri etary wasp scheme to its children to inform them of the following level when they are allowed to send it achieves a high packet delivery ratio while keeping the energy efficiency low two way communication is not supported an improvement is the cascading informa tion retrieval by controlling access with distributed slot assignment protocol cicada explained in more detail in the following section another approach for cross layering is completely discarding the layered structure and implementing the required functionality in different modules which in teract with each other 71 the authors claim that the modular approach has the following advantages duplication of functionality can be avoided 2 het erogeneity is supported as more modules can be added depending on the capabilities of the node 3 cross layer optimizations are possible leading to more energy efficient protocols 4 modules can be easily adapted or replaced a first attempt to use the modular approach for wbans is described in the authors presented a modular framework and describe the most important modules however the discussion is only theoretical and the framework has not been implemented and properly evaluated the discussion above clearly shows that the devel opment of routing protocols for wbans is an emerg ing area of research the protocols described above were only developed in the last four years several ap proaches exist each with their own advantages and dis advantages combining the positive elements of these approaches will lead to better routing protocols cicada a cross layer protocol for wban as an example of a cross layer protocol specifically designed for wbans we briefly describe cicada in this section general overview cicada is a cross layer protocol specifically designed for wbans based on a multi hop tdma scheduling approach 70 it uses the same packets to take care of both medium access as well as routing the packets are used to detect the presence or absence of the children and to control medium access the protocol sets up a spanning tree and divides the time axis in slots grouped in cycles in order to lower the interference and to avoid idle listening the assignment of the slots is done in a distributed way synchronization of slots is possible because a node knows the length of each cycle with the tree structure each node informs its children when they are allowed to send their data routing itself is not complicated in cicada as data packets are routed up the tree which is set up to control the medium access the network is considered to be convergecast but traffic between individual nodes in the network is possible as well by routing over the sink j med syst 36 1094 slot assignment and data transfer in cicada are defined by a sequence of cycles at the beginning of each cycle the slots in the remainder of the cycle are assigned the slot allocation is done by sending a scheme from a parent node to a child node a node calculates its own scheme based on the scheme it has received from his parent each cycle is divided in two parts the control subcycle and the data subcycle each subcycle has its own scheme for slot allocation the control scheme and the data scheme respectively these schemes are both sent in the control subcycle this subcycle is used to propagate the schemes from the parents to their children to assign all slots in the cycle when all nodes have received their scheme the control cycle ends and the data cycle starts each node is assigned one slot in the control subcy cle when a node has received a scheme packet from its parent in a slot in the control subcycle it can sleep as no more packets will arrive in that slot furthermore in the control subcycle it will only be awake in the slot where it sends its own scheme to its children the data subcycle is used to forward the data from the nodes to the sink the first nodes to start sending data are the nodes at the bottom of the tree in doing so all data can be sent to the sink in one cycle this lowers the end to end delay tremendously in fig 18 a small example and its corresponding tree structure is given table 9 gives an overview of which slots each node is allowed to send in during the control and data subcycle the arrows show that during the control subcycle control information is sent downwards from the sink to all nodes in the control subcycle in the data subcycle all data is sent upwards to the sink the last slot of each data scheme is a contention slot represented by x which is used to allow new children to join the network cicada can add an inactive period after the data subcycle for example when the data traffic in the net work is low in these inactive periods the radios can be shut down resulting in a better energy efficiency the length of the active period is expressed by where a cycle without an inactive period has a 100 and a cycle where the active period and the inactive period have the same length as joining and leaving the network in each data subcycle a contention based slot is in cluded to allow nodes to join the tree a new node will send a join request packet in the contention slot of its desired parent using in slot aloha this join request packet contains the number of slots a node needs to transmit its own data to enable immediate e d s a d e level 0 b s a b level level 2 c fig 18 example network the lines indicate the tree structure optimal resource allocation when the parent receives the join it will include the node in the next cycle with this simple join mechanism quickly adding nodes is possible this also gives cicada basic support for mobility and changing topologies to support leaving and detection of loss of a connec tion each node will send at least two packets per cycle its schemes in the control subcycle and a data packet or a hello packet in the data subcycle if a parent does not receive any packet from a child for 2 or more consecutive cycles the parent will assume that the child is lost likewise if a child does not receive packets from its parent for 2 or more consecutive cycles the child will assume that the parent is gone and will try to join a new parent results cicada is evaluated on an example network where some sensors are placed on the body as can be seen in table 9 steady state cycle scheme for the example network for both the control and data subcycle 2 3 4 5 2 3 4 5 6 7 8 9 s a x b x c x d x e x the arrows indicate in which slot a node sends where down means the node broadcasts to its children and up means it uni casts to its parent the x es represent contention slots used for joining the network c 1088 j med syst 36 1094 fig figure shows the generic tree view of this network thirteen nodes each are sending a constant bit rate cbr stream to the sink with radios capable of transmitting up to mbps cicada is compared with smac a frequently used csma style protocol for sensor networks in smac nodes synchronize time by building virtual clusters and employ a fixed duty cycle to reduce idle listening overhead smac includes carrier sensing col lision avoidance rts cts signaling and overhearing avoidance the smac available in is considered with the default parameters a duty cycle of and with static optimal routing in doing so any overhead and delay caused by the routing protocol is avoided the sleep ratio is shown in fig the sleep ratio drops for high packet rates which is expected as more traffic is sent in the network however when the packet rate is too high the sleep ratio rises again more packets are lost due to buffer overflows thus fewer packets are actually sent for lower duty cycles the sleep ratio rises up to 95 compared with smac duty cycle of cicada performs better when the duty cycle is set to or lower the example network is also used to evaluate the packet loss for varying packet rates and different duty cycles packet generation time is defined as the time between the reception of packets from the applica tion layer in fig it can be seen that compared with smac cicada performs a lot better further cicada has a packet loss of 0 as long as at most one packet per cycle is generated the main reason for packet loss when too much data is generated is that the node is not aware of the application s traffic pattern i e the sensor rate the node can only reserve slots for a b fig 19 a sensors place on the body b resulting network topol ogy used in the simulation fig sleep ratio experienced by smac and cicada for the network of fig the duty cycle and the number of packets sent per second are varied data received from the application layer for example when the nodes start transmitting packets two packets will be reserved in the first cycle as at least two packets are generated per cycle the cycle length increases but the traffic rate stays the same as a result more packets are generated during the cycle so more slots have to be reserved this mechanism creates an avalanche effect in the length of the duty cycle currently cicada does not take this problem into account a possible solution is to implement a traffic predictor which can more precisely predict the packet rate the results above show that cicada has a high sleep ratio especially when the duty cycle is reduced fig 21 packet loss experienced by smac and cicada for the network of fig the duty cycle and the number of packets sent per second are varied j med syst 36 1094 to 50 or lower longer duty cycles however lead to packet loss when the packet generation rate is too high when deploying wban using cicada one thus has to balance the desired throughput and the energy efficiency discussion currently not many protocols for wbans exist pro tocols specifically designed for wbans like anybody and cicada do resolve some of the issues that are typically encountered in wbans however there is still a lot of work to do as shown before tempera ture aware routing protocols are very interesting for wban incorporating energy efficiency and reliability support would largely improve the applicability of these results as now the development is much focused on temperature one important issue from an application point of view is offering qos currently protocols like bodyqos 72 do offer basic qos however integra tion into existing mac and especially routing protocols is not optimal when tackling the specific qos require ments of wbans integration with all layers should be strong given the difficult environment where the path loss is high and as a result bandwidth is low qos protocols should expect and use all information about the network that is available another interesting research challenge is energy efficient routing in mobile networks because of the small scale and the low allowed transmission power a simple movement of the human body can cause large mobility at the network level suddenly links will break and new links will appear existing routing protocols are either well suited for mobility but highly energy inefficient from a sensor point of view or they do not support fast mobility we believe this is still a large hur dle to be overcome to completely start using wbans the complex trade off between energy efficiency and fast routing in this mobile network should be overcome wban applications wbans have great potential for several applications including remote medical diagnosis interactive gaming and military applications table shows some of the in body and on body applications in body applications include monitoring and program changes for pacemak ers and implantable cardiac defibrillators control of bladder function and restoration of limb movement 73 on body medical applications include monitor ing ecg blood pressure temperature and respira tion furthermore on body non medical applications include monitoring forgotten things establishing a so cial network and assessing soldier fatigue and battle readiness some of the wban applications are dis cussed below cardiovascular diseases traditionally holter monitors were used to collect car dio rhythm disturbances for offline processing with out real time feedback also transient abnormalities are sometimes difficult to capture for instance many cardiac diseases are associated with episodic rather than continuous abnormalities such as transient surges in blood pressure paroxysmal arrhythmias or induced episodes of myocardial ischemia and their time cannot be accurately predicated 74 wban is a key tech nology that can be used to prevent the occurrence of myocardial infarction monitor episodic events or any other abnormal condition and also can be used for ambulatory health monitoring table 10 in body and on body applications 73 application type sensor node date rate duty cycle power consumption qos privacy per device sensitive to latency per time in body applications glucose sensor few kbps extremely low yes high pacemaker few kbps 1 low yes high endoscope capsule 2 mbps 50 low yes medium on body medical ecg 3 kbps 10 low yes high applications 32 bps 1 low yes high blood pressure 10 bps 1 high yes high on body non medical music for headsets 1 4 mbps high relatively high yes low applications forgotten things kbps medium low no low monitor social networking kbps 1 low low high 1090 j med syst 36 1094 diabetes worldwide more than million people suffer from diabetes a number that is expected to rise to million by frequent monitoring enables proper dosing of medicines and reduces the risk of fainting and in later life blindness loss of circulation and other complications cancer detection cancer remains one of the biggest threats to the human life according to national center for health statis tics about 9 million people had cancer diagnosis in http www cdc gov nchs default htm a set of miniaturised sensors capable of monitoring cancer cells can be seamlessly integrated in wban this allows a physician to diagnose tumours without biopsy asthma wban helps millions of patients suffering from asthma by monitoring allergic agents in the air and providing real time feedback to the physician chu et al pro posed a gps based device that monitors environmental factors and triggers an alarm in case of detecting an environment the patient is allergic to 75 telemedicine systems existing telemedicine systems either use dedicated wireless channels to transfer information to the remote stations or power demanding protocols such bluetooth that are open to interference by other devices working in the same frequency band these characteristics limit prolonged health monitoring wban can be integrated into a telemedicine system that supports unobtrusive ambulatory health monitoring for long period of time figure shows a real time telemedicine infrastructure for patient rehabilitation artificial retina retina prosthesis chips can be implanted in the human eye that assists patient with limited or no vision to see at an adequate level 76 battlefield wbans can be used to connect soldiers in a battlefield and report their activities to the commander i e run ning firing and digging the soldiers should have fig 22 a real time telemedicine infrastructure for patient rehabilitation a secure communication channel in order to prevent ambushes conclusions in this paper we studied the fundamental mechanisms of wban at phy mac and network layers each section was concluded separately with useful remarks starting from the system architecture for the phy layer we reviewed different methodologies of wire less communication to from an implant including rf communication in body antennas and propagation patterns for the mac layer we discussed several low power mechanisms in the context of wban and concluded that the tdma mechanism is suitable and more appropriate for wban we further discussed our proposed low power mac protocol for wban followed by important suggestions for the network layer the possible network topologies for wban were discussed taking into account the required energy efficiency and reliability then a classification of ex isting routing strategies was given with future research directions finally numerous wban applications were presented acknowledgements the authors would like to thank mr michael j hladik mr niamat ullah and mr pervez khan inha university for their insightful comments on the manuscript this work was supported by the national research foundation of korea nrf grant funded by the korea government mest no no 2010 0018116 appendix a table 11 ________________ j med syst 36 1094 table 11 comparison of low power mac protocols for wban 26 low power protocols channels organization and advantages adaptability to mechanisms basic operation and disadvantages wbans comments low power wisemac 1 organized randomly scalable and adaptive to good for high traffic listening and operation is traffic load support applications not suitable based on listening mobility low and high for low duty cycle in body power consumption on body nodes in low and high traffic conditions and low delay bmac 1 organized in slots flexible high throughput good for high traffic and operation is tolerable latency and applications based on schedules low power consumption stem 2 organized randomly suitable for events good for periodic traffic having two channels based applications especially for low traffic control data applications suitable to channel and handle sporadic events due to operation is based a separate control channel on wakeup schedules but hard to handle sporadic events when the traffic load is high scheduled smac 1 organized in slots high transmission latency good for high traffic contention and operation is loosely synchronized applications suitable based on schedules low throughput for applications where throughput is not a primary concern such as in body medical applications tmac 1 organized in slots queued packets are sent good for high traffic and operation is in a burst thus achieve applications early sleep based on schedules better delay performance problems allow the nodes loosely synchronized to loose synchronization pmac 1 organized in hybrid adaptation to changes good for delay sensitive mode and operation is might be slow loosely applications based on listening synchronized high throughput under heavy traffic dmac 1 organized in slots better delay performance on body nodes can be and operation is due to sleep schedules prioritized according to based on schedules loosely synchronized their application optimized for data requirements and a data forwarding sink tree can be built where the wban coordinator will be a cluster node tdma flama 1 organized in frames better end to end good for low power and operation is reliability and energy applications adaptable based on schedules saving smaller delays to high traffic improved energy applications saving high reliability leach 1 organized in clusters and distributed protocol tdma schedules should be operations is based on no global knowledge created by the wban tdma scheme required extra overhead coordinator cluster head for dynamic clustering should not change depending on minimum communication energy as in the traditional leach heed 1 organized in clusters good for energy the wban coordinator acts and operations is efficiency scalability as a cluster head unlike based on tdma prolonged network traditional heed the scheme lifetime load balancing wban network size is often defined by the physician biochar is used for soil conditioning remediation carbon sequestration and water remediation biochar application to water and wastewater has never been reviewed previously this review focuses on recent applications of biochars produced from biomass pyrolysis slow and fast in water and wastewater treatment slow and fast pyrolysis biochar production is brieﬂy discussed the literature on sorption of organic and inorganic contaminants by biochars is surveyed and reviewed adsorption capacities for organic and inorganic contaminants by different biochars under different operating conditions are summarized and where possible compared mechanisms responsible for contaminant remediation are brieﬂy discussed finally a few recommendations for further research have been made in the area of biochar development for application to water ﬁltration ó elsevier ltd all rights reserved introduction biochar is the charred organic matter produced with the intent to deliberately apply to soils to sequester c and improve soil properties lehmann and joseph the international biochar initiative ibi http www biochar international org biochar states biochar is a solid material obtained from the carbonization of biomass various degrees of carbonization produces an inﬁnite corresponding author tel fax e mail address hotmail com d mohan http dx doi org j biortech ó elsevier ltd all rights reserved variety of biochars for use as fuel and adsorbents biochar may be added as a modiﬁer or carbon sink to reduce greenhouse emissions from decaying biomass biochar has appreciable carbon sequestration value biochar has a long history as a soil amendment in japanese horticulture and carbon black exists from wildﬁres in terra preta sites throughout the central amazon brewer et al lehmann lehmann et al biochar use in soil remediation carbon sequestration climate change mitigation and carbon farming have been critically reviewed ahmad et al lehmann lehmann et al sohi et al biochar sequestration does not require a fundamental scientiﬁc d mohan et al bioresource technology advance the production technology is robust simple and appropriate for many regions of the world but optimization and economic evaluation of large scale development are required biochar generally increases nutrient availability microbial activity soil organic matter water retention and crop yields in soils while decreasing its fertilizer needs greenhouse gas emissions nutrient leaching and erosion sohi et al woolf et al pyrolysis dates back at least to ancient egypt when tar for caulking boats and certain embalming agents were made using pyrolysis mohan et al pyrolysis processes have been continuously improved and are widely used for coke and charcoal production in the plant pyrolysis liquid yields were increased by employing fast pyrolysis where the biomass is heated at a rapid rate in a few seconds to c producing chars gases and vapor aerosol that is condensed rapidly to biooil mohan et al this review focuses only on use and opportunities for biochar in water treatment metal ions organics and anions from industrial efﬂuents have been removed by chemical and biological methods chemical precipitation is the most commonly used method precipitations employ hydroxide sulﬁde carbonate and phosphate but sludge production becomes a disposal problem adsorption has evolved as a front line of defense for pollutants which are hard to remove by other methods selective adsorption by biological materials mineral oxides activated carbon or polymer resins has generated excitement activated carbon often thought of as a universal adsorbent for water treatment is frequently made from biomass or coal mohan and pittman activated carbon is ideal for removing contaminants from water but costly to make on the other hand sustainable biochar requires less investment typical biochar is less carbonized than activated carbon more hydrogen and oxygen remain in its structure along with the ash originating from the biomass biochars absorb hydrocarbons other organics and some inorganic metal ions hale et al mohan et al exhibiting potential for water puriﬁcation and soil amelioration biochar could replace coal coconut shell and wood based activated carbons as a low cost sorbent for contaminants and pathogens biochar might be used for removing contaminants from water while also being loaded with nutrients for subsequent use as a soil amendment providing long term sorption capacity and a fertilizer bernd et al this review covers the use of slow and fast pyrolysis biochars for removing contaminants from water emphasizing publications mainly from the last years efforts are also made to differentiate among biochars from slow pyrolysis fast pyrolysis gasiﬁcation and hydothermal carbonization htc biomass conversion technologies a number of conversion schemes have been developed to capitalize on biomass feed properties and reviewed czernik and bridgwater mohan et al both biological anaerobic digestion hydrolysis and fermentation and thermal combustion pyrolysis liquefaction torrifaction and gasiﬁcation methods are used for biomass conversion into fuel and byproducts only thermal processes to produce adsorbent chars are covered here biochar from thermal treatment also has a high energy density typically kj g biomass prolysis pyrolysis is the thermal decomposition of materials in the absence of oxygen or when signiﬁcantly less oxygen is present than required for complete combustion fig table pyrolysis should be differentiated from gasiﬁcation where biomass is reacted with steam or air gasiﬁcation converts biomass into syngas by careful control of the oxygen amount present pyrolysis covers a range of thermal decomposition processes and is difﬁcult to precisely deﬁne the older literature equates pyrolysis to carbonization where char charcoal is the principal product fig table today pyrolysis often describes processes in which liquids biooils are preferred products liquid production is favored in short pyrolysis times fast pyrolysis conventional slow or fast ﬂash pyrolysis depend upon the operating conditions used temperature heating rate and vapor residence time czernik and bridgwater mohan et al the feed heating rate residence time and pyrolysis temperature distinguish the pyrolysis processes conventional pyrolysis is slow the terms slow and fast pyrolysis are somewhat arbitrary and not precisely deﬁned many pyrolyses have been performed at rates that are not fast or slow but are in a broad range between these extremes a key point is whether or not vapors and aerosol components are rapidly removed to optimize liquid formation fast pyrolysis ﬂash vacuum pyrolysis or remain in contact with the solid undergoing secondary reactions which produce added carbonaceous solids operating parameters for slow and fast pyrolysis to biochars are brieﬂy discussed below conventional slow pyrolysis slow pyrolysis has been employed for thousands of years to produce charcoal production and charcoal property knowledge accumulated over the past millenia have been reviewed antal and grønli biomass is heated slowly to about c in absence of air vapor residence times vary from to min vapors in conventional pyrolysis do not escape rapidly unlike in fast pyrolysis fast pyrolysis fast pyrolysis requires dry feedstock wt moisture rapid heat transfer fast temperature increase by heating small biomass particles mm to c and vapor residence times of maximum lima et al fast pyrolysis differs fundamentally from slow pyrolysis gasiﬁcation gasiﬁcation produces a gas fuel that can be used for direct heat generation or electricity generation gasiﬁcation is a partial combustion of solid the product mix gas liquid and solid is controlled by altering temperature particle size residence time pressure gas composition under which the biomass is treated and the catalyst identity if one is used brewer et al characterized the chars from fast pyrolysis and gasiﬁcation of switchgrass and corn stover brewer et al higher char aromaticity was obtained in slow pyrolysis than in fast pyrolysis or gasiﬁcation fused aromatic ring compound sizes in fast and slow pyrolysis chars were similar rings per compound while gasiﬁcation char was more highly condensed rings per compound brewer et al hydrochar it is important to differentiate biochars from hydrochars hydrochars form by hydrothermal carbonization htc of biomass at high temperature and pressure in water producing a char water slurry libra et al the solid char is easily separated its chemical and physical properties differ signiﬁcantly from the starting biomass conversely biochars are produced by pyrolysis slow or fast or gasiﬁcation at various temperatures pressures and carbonization times hydrochars from agricultural residues were characterized and used for water and soil remediation d mohan et al bioresource technology fig thermochemical conversion of biomass into biooil biochar and gases table thermochemical processes reactions residence time and primary products modiﬁed brewer thermochemical process temp range c heating rate pressure residence time desired product slow pyrolysis torrefaction fast pyrolysis gasiﬁcation htc slow c min slow c min very fast c moderate very fast slow atmospheric atmospheric vacuum atmospheric atmospheric elevated hours seconds minutes hours seconds seconds minutes hours biochar stabilized friable biomass biooil syngas producer gas hydrochar wiedner et al chemical properties of biochars from gasiﬁcation or pyrolysis were compared to hydrochars from htc wiedner et al hydrochars are less stable dominated by alkyl moieties than biochars dominated by aromatics hydrochars are not included in the european biochar certiﬁcate ebc standardization due to their different chemical properties wiedner et al torrefaction and retiﬁcation wood retiﬁcation takes place at c the wood color becomes reddish brown chocolate this product is considered to be resistant towards biological attack similarly torrefaction occurs between c at low heating rates and creates a brown or black product with little strength torreﬁed wood yields range from to with energy yields of antal and grønli torrefaction increases the biomass energy density greatly reduces weight enhances hydrophobic nature and improves wood commercial use for energy production by reducing transportation costs typically torrefaction and retiﬁcation do not create adsorbent chars since only partial biomass decomposition occurs to prevent rot and induce some water loss development of biochars different feeds and reactors are employed to make biochars by pyrolysis slow or fast gasiﬁcation or htc important feedstocks and operating conditions are listed in the supplementary material table commonly used reactors include auger mohan et al mohan et al mohan et al mohan et al well swept ﬁxed bed bench scale ﬁxed bed ates and un vertical tubular hameed and el khaiary and ﬂuidized bed types han et al feed stock pyrolyses into biochars have been conducted on debarked loblolly pine pinus taeda chips park et al oak wood pine wood oak bark and pine bark mohan et al pine needles ahmad et al corn stover and apple wood sun et al among many others characterization of biochars biochar physical chemical and mechanical properties can vary with production conditions making it challenging to engineer biochars that are simultaneously optimized for carbon sequestration nutrient storage water holding capacity and adsorption sun et al the ibi has graded biochar into three classes based on carbon content http www biochar international org sites default ﬁles pdf these include class biochar contains carbon or more class biochar between and carbon and class biochar between and carbon thermal conversion of charcoal derived from ﬂuidized bed fast pyrolysis oil production of switchgrass has been reported boateng biochars pyrolyzed under several highly controlled conditions from corn stover and apple wood were characterized particle size pyrolysis heating rate and residence time had signiﬁcant effects on biochar chemical composition aromaticity and pore structure sun et al in fast pyrolysis the shorter the vapor residence time less important are the secondary reactions of vapor with the developing char thus reactions in the vapor and aerosol phases mainly inﬂuence chemical composition of the biooil liquid phase and the gases produced in slow pyrolysis the vapors are conﬁned and react extensively with the solid phase more mass ends up as char increasing both pyrolysis time and temperature increases the c in the char and decreases both the o and surface hydrophilicity the effect of these parameters on adsorption is discussed in subsequent paragraphs applications of biochars in water remediation biochar applications have been divided into subsections on organics and inorganics remediation organics are divided into a dyes b phenolics pesticides polynucelar aromatics and antibiotics removal inorganics are divided into a cations and b anions removal table brieﬂy summarizes example adsorptions of inorganic species on various biochars while a comprehensive listing is given in supplementary material table the ﬁrst column of table lists the biochar type employed while column four gives the adsorbed species this organization is d mohan et al bioresource technology table adsorption capacities of different biochars for removing metal ions from watera type of biochars metal ions pine wood char pine wood char pine wood char oak wood char oak wood char oak wood char pine bark char pine bark char pine bark char oak bark char oak bark char oak bark char hard wood char hard wood char corn straw char corn straw char peanut straw char peanut straw char soybean straw char soybean straw char canola straw char canola straw char rice husks pinewood biochar pinewood biochar pinewood biochar rice husk biochar 00 66 92 rice husk biochar rice husk biochar digested sugarcane bagasse biochar raw sugarcane bagasse biochar 81 dried olive pomace orange waste compost dairy manure biochar a adsorption capacity mg g references type of biochars metal ions adsorption capacity mg g references mohan et al buffalo weed biochar yakkala et al dong et al kong et al 03 03 08 81 97 sugar beet tailing biochar soybean stalk based biochar empty fruit branch magnetic biochar oak wood char oak bark char chen et al rice straw biochar pinewood biochar dairy manure biochar tong et al pig manure biochar pellera et al cow manure biochar cao et al liu and zhang digested dairy waste biochar digested sugar beet biochar mubarak et al mohan et al han et al liu et al xu et al kołodyn ska et al inyang et al inyang et al complete table is available in supplementary material as table somewhat arbitrary because the same general biochar types were prepared in different ways by different authors also several different adsorbates were targeted for adsorption by the same biochar types made by different research groups biochar applications in organics remediation color dye removal the disposal of dye industry wastewater poses major problems because efﬂuents contain acids or bases dissolved solids toxic compounds and they have color color is instantly recognized because it is visible many textile dyes are difﬁcult to destroy by conventional waste treatment methods since they are stable to light oxidizing agents and resist aerobic digestion biochars from straw hameed and el khaiary qiu et al xu et al and bamboo mui et al yang et al were developed to remove dyes from water and wastewater slow pyrolysis rice straw biochar was applied to malachite green mg adsorption hameed and el khaiary rice straw biochar was made in a vertical tubular reactor for h from c under nitrogen maximum mg was removed at ph at c about of the mg was removed within min from a solution with mg l of initial mg kinetics ﬁtted best to a pseudo ﬁrst order model suggesting the adsorption rate is initially d mohan et al bioresource technology controlled by external mass transfer intraparticle diffusion was controlling at a later stage the monolayer langmuir adsorption capacity was mg g at c hameed and el khaiary table straw biochar was acid treated m hcl to remove metals followed by demineralization using hcl hf qiu et al according to chun method chun et al this biochar bc and a commercial activated carbon darco g sigma aldrich ac were used to remove reactive brilliant blue knr and rhodamine b rb dyes from water surprisingly the bc surface area g was higher than the ac table bc was less carbonized than ac as evidenced by its high surface acidity biochar at ph adsorbed more rb than the commercial ac at ph because the dye molecules more easily accessed the larger micropores of bc than the ﬁne pores of ac knr adsorption increased as its concentration increased at ph due to negative surface charge neutralization by na cations of the dye two main ir bands observed after dye adsorption suggest intermolecular hydrogen bonding between o atoms of char functional groups and aoh groups of the dye high rb adsorption ph with increasing ionic strength was possibly due to the formation of rb dimers also c o and aoh groups participated in dipole dipole attractions between rb and bc canola straw cs peanut straw ps soybean straw ss and rice hulls rh were each slowly pyrolyzed for h at c in a mufﬂe furnace generating their respective biochars xu et al the ramp heating rate was c min these biochars removed methyl violet from water sorption capacities followed the order cs ps ss rh which was parallel to their respective cation exchange capacities methyl violet at higher concentrations adsorbed on biochar due to its low water solubility zeta potentials of the cs and ps chars showed their surfaces were acidic changes in the ft ir phenolic aoh stretching and carboxylate asymmetric stretching peaks of methyl violet occur after adsorption a drop in band intensities at and cm indicated the char surface carbonates interacted with methyl violet monolayer adsorption capacities of and mg g were obtained for ps ss and rh chars respectively slow pyrolysis of waste bamboo scaffolding under for h at c gave biochars mui et al char yields decreased with increasing temperature speciﬁcally a sharp yield decrease from to c was due to lignin and hemicelluloses partial gasiﬁcation the char h c ratios decreased versus o c in van krevlen plots showing high pyrolysis temperatures caused progressive aromatization surface area increased with higher pyrolysis temperatures reaching g at c char yields and h and o dropped on longer pyrolysis times whereas surface area increased high heating rates resulted in lower surface areas pore volumes and yields due to rapid depolymerization at char surfaces acid blue acid yellow and methylene blue mb adsorption occurred this bamboo char had a higher adsorption capacity for mb than and bamboo biochar also adsorbs metal complex dye acid black yang et al kenaf hibiscus cannabinus ﬁber char kfc supplied by kenaf fiber industries sdn bhd malaysia was slowly pyrolyzed to acid treated biochar hkfc at c mahmoud et al this hkfc adsorbed methylene blue mb in a honeycomb pore network observed by sem char surface area increased on acid treatment hkfc a mesoporous solid average pore dia nm had almost double the ﬁxed carbon and a higher oxygen content than kfc hkfc had a higher mb removal efﬁciency than kfc the langmuir adsorption capacity at ph was mg g at c mb conc of mg l sorption followed pseudo second order kinetics both intraparticle diffusion and boundary layer diffusion controlled adsorption hornbeam sawdust biochars were made in a ﬁxed bed reactor at and c under an inert atmosphere to adsorb orange ates and un optimum adsorption occurred at ph on all chars adsorption capacity was highest on the char made at c phenols removal phenolic compounds are manufactured for plastics dyes drugs antioxidants and pesticides they pose serious danger when entering the food chain as water pollutants phenols affect the taste and odor of ﬁsh and drinking water at very low concentration also nitrophenols and chlorophenols are priority pollutants poly acrylamide chicken wood and tire biochars p aam cb p aam wb and p aam tb were developed as hydrogel composites using acrylamide aam monomer with n methylenebisacrylamide mba as crosslinker and ammonium persulfate aps as initiator p aam cb p aam wb and p aam tb were utilized for aqueous phenol removal karakoyun et al high sorption capacity rice husk and corncob biochars were prepared at ﬁxed temperatures and different residence times biochar prepared within exhibited a higher phenol adsorption capacity mg g adsorption via acid base interaction and hydrogen binding between phenol and the functional groups was proposed to explain the process liu et al catechol adsorption on oak pine and grass biochars prepared at and c was reported catechol sorption capacity increased with rise in biochar pyrolysis temperature kasozi et al pesticides and polynuclear aromatics removal pesticide and pah remediation has attracted great attention they are introduced into the environment from economic production and wide application in agriculture important pesticide remediation targets include organophoshorous organochlorine carbamate triazine and chlorophenoxy acid compounds dibromochloropropane a soil fumigant used to control nematodes was adsorbed from well waters onto almond shell activated biochars klasson et al almond shells were slowly pyrolyzed at c for h under in a lyndberg furnace with a retort further steam activation at c for min gave a speciﬁc surface area of g the maximum adsorption capacity was mg g field studies were also carried out successfully klasson et al orange peel biochars from slow pyrolysis ranging from to c were used for naphthalene and naphthol adsorption chen and chen maximum naphthol and naphthalene uptake was achieved by and respectively naphthalene adsorption was controlled by surface coverage and partition whereas naphthnol adsorption was controlled by partition surface coverage and surface interactions raw orange peels underwent large weight loses from to c the o c ratio decreased with a rise in pyrolysis temperature chun et al reported a similar trend chun et al solvents removal biochars wc wc wc wc and wc were generated by pyrolyzing a wheat residue triticum aestivum l for h between c and c and analyzing for their elemental compositions surface areas and surface functional groups chun et al these chars removed benzene and nitrobenzene from water the samples made at c were well carbonized with high surface areas g little organic matter and low oxygen content chars formed at c were only partially carbonized and exhibited g surface areas organic carbon and oxygen chun et al hightemperature chars acted via adsorption on their carbonized surfaces low temperature char sorption occurred by surface adsorption and some concurrent partition into the residual organic matter nitrobenzene had higher surface afﬁnities than nonpolar benzene char wc was highly carbonized low h c and low o c versus chars formed at lower temperatures maximum surface area was achieved d mohan et al bioresource technology at c total acidity decreased as pyrolysis temperature rose soybean stover peanut shells and pine needles were pyrolyzed ahmad et al soybean stover and peanut shells were charred at and c and used to remove trichloroethylene tce from water chars produced at c had higher surface areas and g than those at c and g and had higher maximum adsorption capacities mg g for than the other chars tce adsorption correlated well with high carbon contents and negatively with higher oxygen content table miscellaneous poultry litter t pl and wheat straw t ws biochars produced at c over min and hydrothermal poultry litter and swine solids chars at c under autogenic pressures for h were made to remove phenanthrene phen bisphenol a bpa and ethinyl esteradiol from water sun et al lower h c and o c ratios were reported for the thermal biochars indicating more carbonization than in the hydrothermal chars thermal biochars were more hydrophobic than hydrothermal chars sun et al and exhibited mostly aromatic carbons with small amounts of alkyl carbons hydrothermal chars had carboxyl methoxyl o aryl and alkyl carbons in their c nuclear magnetic resonance spectra more and bpa was adsorbed onto hydrothermal biochars prepared at c than onto thermal biochars the oxygen containing polar functional groups of hydrothermal biochars hydrogen bond to and bpa organic carbon normalized distribution coefﬁcients log koc of the hydrothermal biochars were higher than for thermal biochars log koc values of phen and bpa followed the same order as their hydrophobicities phen bpa bpa possesses two phenol rings hence weak p h bonding with the char and phenolic hydroxyl hydrogen bonding with the hydrothermal char oxygen functions occurs phen adsorption on chars occurs by extensive p p interactions sun et al biochar applications in inorganic remediation metal ion removal heavy metals pose serious health threats even at very low concentrations some are cumulative poisons capable of assimilation storage and concentration by organisms exposed long periods to low concentrations eventual metal built up in tissues can cause harmful physiological effects the heavy metals appear among the main pollutants in this century davydova discharged heavy metals present a serious threat to human health and natural waters important biochar adsorption studies have been made with cr cu pb cd hg fe zn and as ions activated carbon has long been used to remove metal ions but only a few milligrams of metal ions are typically adsorbed per gram of activated carbon regeneration problems also exist this makes activated carbon expensive for treating wastewater so its use in developing countries is more problematic low cost locally available materials with adsorption capacities comparable to activated carbon are needed solid biomass derived waste is also a vexing problem recycling requires a suitable recycled product quality if possible lignocellulosic wastes have fuel value so complete combustion fast pyrolysis to biooil or gasiﬁcation to syn gas are options biochar is a byproduct of biooil production in yields if biooil production becomes widespread its resulting char would be widely available for water remediation use slow pyrolysis to biochars also converts lignocellulosic wastes to biochars industrial wastewater and ground surface waters could then be widely treated with biochars to decrease metal ion removal costs biochars from slow pyrolysis and hydrothermal treating of rice husks olive pomace orange wastes and compost were used for remediation pellera et al slow pyrolysis under limited oxygen at and c for h was followed by demineralization by acid htc produced chars in a high pressure reactor by heating to c for min followed by acetone extraction to remove oils pellera et al slow c pyrolysis chars were less efﬁcient for removal than those produced at c but slow pyrolysis chars removed more than the hydrothermal chars pellera et al table and table peanut canola and soybean straw biochars prepared in a c mufﬂe furnace ramp rate of c min for h under limited oxygen were also used for adsorption tong et al all three biochars had higher adsorption capacities than commercial activated carbon at ph sorption involved electrostatic and non electrostatic adsorption sorption capacity rose as ph went up as strong complexes formed between cuaoh and char surface functions aoh and acooh higher phosphate contents of soybean and canola straw chars versus peanut straw char caused cu phosphate formation and precipitation tong et al desorption rates were canola straw soybean straw peanut straw leguminous peanut and soybean straw chars had higher capacities than that of nonleguminous canola straw char peanut straw char had a maximum capacity of mol kg at ph tong et al each of these three straw feeds were also pyrolyzed at and c for use to remove from water tong et al adsorption rates followed the order peanut straw char soybean straw char canola straw char rice straw char biochars formed at c gave the best sorption the sorption occurred by both adsorption and surface precipitation tong et al tables and fast and slow pyrolyzed hardwood and corn straw biochars were reported chen et al fast hardwood pyrolysis was made at c in a residence time at dynamotive inc vancouver canada slow pyrolysis biochars were made at c residence time h at best energies inc madison wisconsin usa was acidic due to organic acids and phenolic compounds above c alkali salts separate raising the char ph high h c ratios of these biochars indicate lower carbonization and aromaticity than activated carbon had a greater o c ratio and higher polarity than possesses a higher surface area 08 g than g comparing surface areas of many biochars illustrated that feed composition can play as an important role in surface area as the synthetic technique employed surface area plays a critical role sa can dominate as conﬁrmed by the higher and uptake by verses highly surface functiminalized maximum langmuir adsorption capacities of mg g for and mg g for were achieved using char tables and pig and cow manure biochars were made at and c at ambient pressure under nitrogen followed by chemical and mechanical treatments kołodyn ska et al the chemically treated c pig manure char had the highest surface area g these chars were used for and removal optimum ph values were for and and for and adsorption sorption kinetics for and was a complex combination of intraparticle pore diffusion external mass transfer and sorption processes kołodyn ska et al table and table metal ion sorption was governed by inner sphere ion complexation with surface functional groups kołodyn ska et al competitive adsorption of and was adversely affected by the presence of and and phenol adsorption was studied on switchgrass sg hardwood hw and softwood sw derived slow and fast pyrolysis biochars han et al fast pyrolysis was carried out at in a ﬂuidized bed reactor at c slow pyrolysis was done at and c for and h respectively chars made at c were steam activated at c for min using as a carrier gas surface areas of steam activated slow pyrolysis chars g for sg hw and sw chars respectively were higher than d mohan et al bioresource technology fast pyrolysis chars and g for sg hw and sw respectively biochars adsorbed more ph than ph adsorption capacities reﬂected char surface areas and availability of surface functional groups sorption by metals at ph decreased in the order c steam activated char c char c char fast pyrolysis char table alamo switchgrass biochar was produced via hydrothermal carbonization at c in a high pressure batch reactor regmi et al this biochar htb was activated htcb using koh to enhance porosity and clear its partially blocked pores htb and htcb removed and from aqueous solutions almost complete removal of was achieved at ph from an initial concentration of mg l htcb showed higher afﬁnity mg g than htb mg g whereas htb showed greater afﬁnity mg g for than for htbc mg g table buffalo weed biochars were prepared at and c by h of slow pyrolysis under for and removal yakkala et al the bet surface area was far higher g for c char than the other two and g for and c chars respectively yakkala et al more surface availability for complexation and cation exchange leads to a high langmuir adsorption capacities by the c biochar 63 and mg g for and respectively ion exchange and metal ion surface complexation dominated the mechanism yakkala et al tables and lead adsorption by slow c pyrolysis biochars from raw bc and anaerobically digested sugarcane bagasse dbc was studied inyang et al these biochars possessed far lower surface areas than activated carbon however dbc sorption capacity mmol kg was twice that of ac mmol kg and twenty times higher than bc mmol kg tables and dbc had higher cation and anion exchange capacities than bc and activated carbon negative zeta potentials indicated these adsorbents had strong negatively charged surfaces lead minerals hydrocerrusite and cerrusite detected on dbc by xrd after sorption conﬁrmed lead precipitation disappearance of acoo carbonyl ir peaks after adsorption suggested insoluble lead carboxylates formed on dbc surfaces bc surface hydroxyl oxygens coordinate with lead cations forming aoapb bonds and a proton is released anaerobically digested dairy waste residue dawc and digested whole sugar beets dwsbc were lowly pyrolyzed to biochars at c for h under inyang et al tables and dawc possessed higher surface area g than dwsbc g sorption capacities were dwsbc and dawc mmol kg so dwsbc was times better a sorbent per unit of surface area aerobically composted swine manure was converted to slow pyrolysis biochars at and c for removal meng et al a higher char yield occurred at c because more cellulose and hemicellulose carbonized at c surface areas and pore sizes decreased from to c due to the pore blockage by inorganic components of the high ash content during pyrolysis alkali salts separate and increase biochar ph h c o c and o n c ratios decreased at c the maximum uptake was mg g table pine wood and rice husk hydrothermal biochars formed at c gave maximum aqueous lead removal capacities of and mg g respectively liu and zhang capacity increased on raising adsorption temperature tables and biproduct chars from pine wood pine bark oak wood and oak bark fast pyrolysis in an auger fed reactor at and c during bio oil production were characterized mohan et al without activation they successfully remediate aqueous and oak bark char offers great potential for and adsorption the signiﬁcantly higher adsorption on oak bark char versus pine wood oak wood and pine bark chars was partially due to its higher surface area and pore volume mohan et al these chars have very small g surface areas versus the high commercial activated carbon values g but remove the metal ions well ion exchange dominated the metal ion adsorption mode mohan et al table and sugarcane pulp residue biochar from slow pyrolysis h at c under gave maximum uptake mg g at ph zhi hui et al table slow pyrolysis c h in nitrogen of oven dried sugar beet tailings in a mufﬂe furnace gave chars used for removal dong et al a maximum langmuir adsorption capacity mg g was achieved at ph tables and oak wood and bark chars from fast pyrolysis in an auger biooil reactor at c were characterized and used for aqueous remediation mohan et al maximum chromium capacity occurred at ph removal increased with rising temperature q wood c 03 mg g c 08 mg g c mg g and q bark c mg g c mg g c mg g more chromium was removed with bark than wood char remarkably oak chars sbet g removed similar amounts of as activated carbon sbet g mohan et al tables and water swelled these chars creating more internal char water contact functional groups within the swollen solid volume can complex or react with leading to greater adsorption capacity char successfully remediated chromium from contaminated surface water with dissolved interfering ions these pyrolytic chars readily reduce and bind aromatic ortho and para dihydroxy compounds reduce to while being oxidized to ortho or para quinones respectively subsequent chelation of occurs mohan et al anion removal low cost pine wood and pine bark chars by products from fast pyrolysis in an auger bio oil production reactor at and c were used as received for water deﬂuoridation mohan et al pine chars successfully treated ﬂuoride contaminated ground water at ph all chars swelled in water due to their high oxygen content opening new internal pore volume mohan et al fluoride also diffused into the chars subsurface solid volume promoting further adsorption ion exchange and metal ﬂuoride precipitation from ash components are adsorption modes remarkably these low surface area chars sbet g can remove similar amounts or more ﬂuoride than activated carbon sbet g more water imbibed in the chars than is possible by ﬁlling only the measured pore volume of dry adsorbent weight loss on removing imbibed water from pine wood char was g g of char and from pine bark char was g g of char mohan et al thus water occupied about 38 cc g and cc g of these two chars some of this volume was due to diffusion into pore walls and by expansion of internal pore structure this swelling contrasts with the behavior of almost fully carbonized carbon blacks the char has by wt oxygen throughout its chemical structure so it is more hygroscopic than carbon black mohan et al during fast pyrolytic decomposition gases and steam rapidly generated inside the wood or bark particles are exploded outward as these escape from the pyrolyzing decomposing particles more porosity is generated these internal pore networks partially collapse or close on cooling mohan et al slow pyrolysis chars prepared from orange peels and water treatment sludge at and c were also used for ﬂuoride uptake oh et al sludge chars had high ash and low carbon contents h c and o c ratios decreased with increased pyrolysis temperatures orange peel biochars made at and c adsorbed more ﬂuoride than those made at c ph sludge based chars had maximum ﬂuoride d mohan et al bioresource technology uptake at ph fluoride forms complexes to oxidized aluminum and iron species on sludge char surfaces fluoroaluminates are released into solution at low ph decreasing the sludge char sorption efﬁciency bench scale slow pyrolysis c h under of anaerobically digested and undigested sugar beet tailings gave dstc and char yields respectively yao et al the dstc surface g was negatively charged magnesium silicon and calcium were present dstc removed signiﬁcantly more phosphate than many adsorbents in most natural aqueous conditions positive mgo surfaces high phzpc actively bind negative phosphate forming mono and polynuclear complexes surface diffusion in dstc large mesopores is important with phosphate the langmuir sorption capacity was mg g at ph tables and modiﬁed biochars for water ﬁltration magnetic biochars innovations incorporating engineered nanoparticles into biochar production could improve biochar functioning in soil fertility enhancement carbon sequestration and wastewater treatment applications inyang et al adsorbent magnetization is an emerging water remediation area to overcome ﬁltration problems of non magnetic adsorbents magnetic adsorbents can easily be recovered from contaminated water containing suspended solids oil and grease using low strength external magnetic ﬁelds impurities may cause adsorbent fouling requiring frequent separation regeneration magnetic separation simpliﬁes isolation and washing followed by redispersion few papers have appeared where fast or slow pyrolysis biochars were magnetized characterized and use to treat water magnetic oak wood mowbc and oak bark mobbc biochars were obtained by fast pyrolysis and c during bio oil production in an auger fed reactor and used for aqueous and remediation mohan et al aqueous biochar suspensions were magnetized by mixing with aqueous solutions followed by naoh causing mixed iron oxides to nucleate and bind the sbet of the magnetic oak wood and bark chars were and g respectively magnetic chars remediated and cd2 better q cd2 mg g q mg g than the nonmagnetic biochars q cd2 mg g q 13 mg g previously reported mohan et al tables and more adsorption occurred than expected based on the chars sbet values mohan et al adsorption of and cd2 was largest at the highest ph values because carboxylic acids anhydrides and phenols become carboxylate and phenoxide anions columbic attractions bound free and cd2 and other hydrated pb and cd cations three magnetic biochars were formed by chemical co precipitation of on orange peel powder and subsequest and c pyrolysis chen et al iron oxide magnetite formation occurred in onestep magnetic biochar preparation these were applied to aqueous phosphate naphthalene and p nitrotoluene remediation a higher organic content remained in the magnetic versus nonmagnetic biochars after pyrolysis iron oxide in the char did not contribute to naphthalene or p nitrotoluene sorption chen et al naphthalene and p nitrotoluene sorption on magnetic biochars increased with higher pyrolysis temperatures sorption was a combination of adsorption and partition magnetic orange peel biochars had higher phosphate sorption efﬁciencies than their nonmagnetic analogs indicating bound iron oxide aggregates assisted in phosphate removal chen et al chemically modiﬁed biochars nanocomposites containing mgo were made by pyrolyzing c with biomass feedstocks sugar beet tailings sugarcane bagasse cottonwoods pine woods and peanut shells zhang et al mgo particles were well dispersed over the biochar surface micropores predominated these nanocomposites were used to remove phosphate and nitrate from water an adsorption capacity of mg g phosphate was achieved by sugar beet tailing composites peanut shell mgo biochar adsorbed nitrate from water highest among these chars with an adsorption capacity of mg g these high nitrate and phosphate capacities may be due to surface area and porosity enhancement by introducing mgo zhang et al hybrid multi walled carbon nanotube cnt coated biochars were made by dip coating biomass into varying concentrations of carboxyl functionalized cnt solutions 01 and w w prior to slow pyrolysis tubular furnace at c h at c min under inyang et al untreated hickory hc and bagasse biochars bc and cnt biochar composites hc cnt and bc cnt were characterized and used for methylene blue mb adsorption cnt addition signiﬁcantly enhanced the hc cnt and bc cnt thermal stabilities surface areas and g and pore volumes and cc g respectively inyang et al electrostatic attraction dominated mb sorption and diffusion controlled mb adsorption rate inyang et al a comparative study of the adsorption capacity of functionalized carbon nanotubes cnts and magnetic biochar from empty fruit branches for removal was reported mubarak et al maximum adsorption capacities were 05 and mg g for functionalized cnt and magnetic biochar respectively mubarak et al tables and a biochar alooh nano ﬂake nanocomposite was fabricated from pretreated biomass through slow pyrolysis in at c for h zhang and gao this was a highly effective adsorbent to remove arsenic methylene blue and phosphate the langmuir capacity of methylene blue adsorption on the biochar alooh mg kg was times higher than that of the corresponding biochar without aluminum treatment mg kg zhang et al the langmuir adsorption capacity of phosphate and arsenic on the biochar alooh was and mg kg respectively zhang and gao magnetic aerobically digested sewage sludge sbet g and magnetic undigested sewage sludge sbet g biochars were prepared in a horizontal furnace at a c min heating rate for h at c under and successfully applied to for diazo naphthol sulfonic acid adsorption gu et al another magnetic saturation magnetization of emu g biochar with colloidal or nanosized c particles embedded in the porous matrix was fabricated via treated cottonwood pyrolysis at c in environment for h zhang et al a large quantity of c particles with sizes from hundreds of nanometers to several micrometers grew within the porous biochar its sorption capacity for as v removal was mg kg zhang et al tables and biochar as an activated carbon precursor biochar is a high heating value solid fuel commonly used in kilns and boilers it was evaluated as a feed to produce activated carbons at c azargohar and dalai activated carbons that resulted had internal surface areas g versus g for the precursor biochar this activated carbon was highly microporous conﬁrmed by sem analysis ft ir spectroscopy proved aromatization had occurred the bet surface area of luscar char increased more than fold upon steam activation azargohar and dalai this adsorbent high micropore to mesopore area d mohan et al bioresource technology 202 its methylene blue adsorption number pore volume and average pore diameter demonstrated a broad potential operating conditions for biochar activation were investigated using a central composite design ccd study azargohar and dalai the relationship between biochar based activated carbon physicochemical properties and phenanthrene adsorption was investigated park et al steam activation ability to add value to fast pyrolysis bio chars was studied lima et al broiler litter alfalfa stems switchgrass corn cob corn stover guayule bagasse guayule shrub soybean straw were all converted to chars by fast pyrolysis in a ﬂuidized bed the surface areas of these biochars and their corresponding steam activated counterparts were determined all were used for aqueous copper cadmium nickel and zinc removal surface areas increased with steam activation from negligible to m2 g with concomitant pore development metal ion adsorption varied with feedstock but always increased with steam activation lima et al tables and comparative evaluation of biochars adsorption capacities of many biochars for different contaminants are summarized in table and the comprehensive table it is very difﬁcult to directly compare adsorption capacities due to a lack of consistency in the literature data sorption capacities were reported at different phs temperatures adsorbate concentration ranges biochar doses particle sizes and surface areas the biochars have been used to treat ground water drinking water synthetic industrial wastewater and actual wastewater the types and concentrations of interfering ions are different and seldom documented some adsorption capacities were reported in batch experiments and others in column modes these cannot be readily compared in batch sorption experiments the sorption capacities were computed by the langmuir or freundlich isotherms or experimentally this makes comparisons more complicated to pursue recycling studies after desorption steps are largely missing in the literature in other words direct comparisons of the tested adsorbents are largely impossible keeping these caveats in mind some of the best biochars having high capacities for selected contaminants were chosen and compared using a bar diagram fig of the biochars compared in this review bamboo char was best for removing methylene blue dye cow manure pig manure peanut straw biochars offered excellent adsorption capacity mg g for cu2 fig maximum lead mg g was removed by cow manure biochar fig surprisingly very high zinc adsorption capacity mg g was achieved by softwood biochar fig cow manure and pig manure biochars also performed better versus other biochars maximum and removal was achieved with sugar beet tailing biochar and sugar pulp biochars respectively tables and highest ﬂuoride removal mg g occurred with pine bark biochar no single biochar removed all the contaminants from water but the conditions employed in those studies can be simulated for large scale applications for drinking water puriﬁcation table and another reason of biochar adsorbents are hard to compare is that they are often prepared under different conditions temperature time atmosphere etc studies of biochar preparation by several methods from the same feedstock followed by adsorption of the same adsorbents are rare likewise identical biochar preparations from the same feed followed by adsorption studies of the same adsorbents are needed to assess reproducibility cost estimation cost studies of biochars from different precursors rarely appear individual biochar costs depend on local precursor availability processing requirements pyrolysis conditions reactor availability recycling and lifetime issues these are not in the literature costs for biochars vary in different countries costs will depend on whether pyrolysis is part of an existing bioreﬁnery and if valueadded coproducts are produced the biochar could be the major targeted product or simply a byproduct as in the case of bioreﬁneries one economic analysis of fischer tropsch ft liquid fuel production including diesel fuel from crop residues appeared manganaro and lawal this was thermochemical based involving fast pyrolysis autothermal reforming atr followed by ft synthesis a simple transparent spreadsheet for estimating economics was presented the sale of pyrolysis char byproduct for soil enhancement at t had a large favorable impact on the economics reducing diesel price by gal the cost of biomass including its transport to pyrolysis site is the largest single contributor to the ﬁnal price of biomass derived fuel becoming more so as plant capacity increases this suggests the need to improve methods of gathering and delivering biomass for each dry t increase in biomass price the sales price of ft fuel is estimated to increase by gal pyrolyzer collectives miles square on a side would reduce diesel price by gal as compared to those on a side manganaro and lawal the cost effects of using mobile pyrolyzers that could go to the biomass sources has not been carefully analyzed based on the literature reviewed following is recommended for future biochar research serious economic studies of biochar production in various sized bioreﬁneries and at distributed minisources connected with agriculture and forestry economic studies of how biochar costs would be impacted by both energy crop growth and large scale char uses for carbon sequestration how large scale char production could use arid and other lands poorly productive for traditional agriculture what are water and fertilizer mineral needs for large scale char production this is not a big issue for adsorbent use only but it is critical when coupled to energy fuels char byproducts and c sequestration on large scales major biochars made by fast or slow pyrolysis with systematically varied o and carbonization from major crop precursors each need to be studied for many adsorption remediation uses some chars will serve only a small number of uses while some may have wide applicability detailed knowledge is needed to compare with other adsorbents low cost activation studies are needed how do high medium and lower o chars function adsorption responses should be analyzed versus costs activation versus non activation should be better understood more understanding of the d aspects of biochar adsorption is needed low surface area but large capacities what does swelling actually do or allow what role does it play in kinetics how are various types of adsorbates distributed throughout the biochars on all dimensional levels pore inside and below pore surfaces throughout the solid material etc stripping pollutants contaminants from biochar adsorbents followed by recycling needs to be far more widely studied one important advantage of really low cost biochar adsorbents could be the ability to use them once and then dispose of them without stripping could be used only a few times disposal could involve combustion to use their fuel value depending on what had been adsorbed many metals would end up as oxide ash and many organic adsorbates would simply burn however chlorinated organics or volatile metal products would require more care the streptococcus pyogenes nuclease can be efficiently targeted to genomic loci by means of singleguide rnas sgrnas to enable genome here we characterize targeting specificity in human cells to inform the selection of target sites and avoid off target effects our study evaluates guide rna variants and indel mutation levels at predicted genomic off target loci in and cells we find that tolerates mismatches between guide rna and target dna at different positions in a sequence dependent manner sensitive to the number position and distribution of mismatches we also show that mediated cleavage is unaffected by dna methylation and that the dosage of and sgrna can be titrated to minimize off target modification to facilitate mammalian genome engineering applications we provide a web based software tool to guide the selection and validation of target sequences as well as off target analyses the bacterial type ii clustered regularly interspaced short palindromic repeats crispr system from s pyogenes can be reconstituted in mammalian cells using three minimal components the crispr associated nuclease a specificity determining crispr rna crrna and an auxiliary trans activating crrna tracrrna following crrna and tracrrna hybridization is targeted to genomic loci matching a nt guide sequence within the crrna immediately upstream of a required ngg protospacer adjacent motif pam crrna and tracrrna duplexes can also be fused to generate a chimeric that mimics the natural crrna tracrrna hybrid both crrna tracrrna duplexes and sgrnas can be used to target for multiplexed genome editing in eukaryotic although an sgrna design consisting of a truncated crrna and tracrrna had been previously shown to mediate efficient cleavage in it failed to achieve detectable cleavage at several loci that were efficiently modified by crrna tracrrna duplexes bearing i dentical guide because the major difference between this sgrna design and the native crrna tracrrna duplex is the length of the tracrrna sequence we tested whether extension of the tracrrna tail would improve activity we generated a set of sgrnas targeting multiple sites within the human and pvalb loci with different tracrrna truncations fig using the surveyor nuclease we assessed the ability of each sgrna complex to generate indels in human embryonic kidney hek cells through the induction of dna doublestranded breaks dsbs and subsequent nonhomologous end joining nhej dna damage repair online methods sgrnas with or nucleotide nt tracrrna tails mediated dna cleavage at all target sites tested with up to fivefold higher levels of indels than the corresponding crrna tracrrna duplexes fig and supplementary fig furthermore both sgrna designs efficiently modified pvalb loci that were previously not targetable using crrna tracrrna fig and supplementary fig for all five tested targets we observed a consistent increase in modification efficiency with increasing tracrrna length we performed northern blot analyses for the guide rna truncations and found increased levels of expression for the longer tracrrna sequences suggesting that improved target cleavage was at least partially due to higher sgrna expression or stability fig taken together these data indicate that the tracrrna tail is important for optimal expression and activity in vivo we further investigated the sgrna architecture by extending the duplex length from to the nt found in the native crrnatracrrna duplex supplementary fig we also mutated the sequence encoding the sgrnas to abolish any poly t tracts that could serve as premature transcriptional terminators for driven we tested these new sgrna scaffolds on three targets within the human gene supplementary fig and observed only modest changes in modification efficiency thus we established sgrna as a minimum effective guide rna architecture and for all subsequent studies we used the most active sgrna architecture institute of mit and harvard cambridge massachusetts usa institute for brain research department of brain and cognitive sciences department of biological engineering massachusetts institute of technology cambridge massachusetts usa of molecular and cellular biology harvard university cambridge massachusetts usa in biophysics harvard university cambridge massachusetts usa mit division of health sciences and technology mit cambridge ma usa of biomedical engineering georgia institute of technology and emory university atlanta georgia usa and systems biology graduate program koch institute for integrative cancer research massachusetts institute of technology cambridge massachusetts usa of bacteriology the rockefeller university new york new york usa authors contributed equally to this work correspondence should be addressed to f z zhang broadinstitute org received march accepted june published online july corrected online august doi nbt nature biotechnology volume number september letters a cbh nls bgh pa b target c target pvalb bp cr r n a cr r n a sgrna sgrna sgrna sgrna architecture sgrna truncations bp guide sequence bp nnnnnnnnnnnnnnnnnnnnguuuuagagcuag a a guucaacuauugccugaucggaauaaaauu cgaua a gaa a aaaguggcaccga g uuucguggcu d wt antisense wt bp indel sense wt human locus antisense wt pam tss me me me me me f distance from tss 05 hr g a n sg r a n r r sg we have previously shown that a catalytic mutant of nickase can mediate gene editing by homology directed repair without detectable indel given its higher cleavage efficiency we tested whether sgrna in complex with the nickase can likewise facilitate homology directed repair without incurring ontarget nhej using single stranded oligonucleotides as repair templates we observed that both the wild type and the mediate homology directed repair in hek cells whereas only the former does so in human embryonic stem cells hescs fig and supplementary fig c we further confirmed using surveyor assay that no target indel mutations are induced by the nickase supplementary fig to explore whether the genome targeting ability of sgrna is influenced by epigenetic that constrain the alternative transcription activator like effector nuclease talens and potentially also zinc finger nuclease zfns technologies we further tested the ability of to cleave methylated dna using either unmethylated or m sssi methylated as dna targets supplementary fig b in a cell free cleavage assay we showed that efficiently cleaves regardless of cpg methylation status in either the bp target sequence or the pam supplementary fig to test whether this is also true in vivo we designed sgrnas sg n a indel figure optimization of guide rna architecture for mediated mammalian genome editing a schematic of bicistronic expression vector for promoter driven sgrna and cbh promoter driven human codon optimized s pyogenes used for all subsequent experiments the sgrna consists of a nt guide sequence blue and scaffold red truncated at various positions as indicated b surveyor assay for mediated indels at the human and pvalb loci arrowheads indicate the expected surveyor fragments n c northern blot analysis for the four sgrna truncation architectures with as loading control d both wild type wt or nickase mutant of promoted 13 insertion of a hindiii site into the human gene single stranded oligonucleotides oriented in either the sense or antisense direction relative to genome sequence were used as homologous recombination templates supplementary fig e schematic of the human locus sgrnas and pams are indicated by colored bars above sequence methylcytosine me are highlighted pink and numbered relative to the transcriptional start site tss f methylation status of assayed by bisulfite sequencing of clones filled circles methylated cpg open circles unmethylated cpg g modification efficiency by three sgrnas targeting the methylated region of assayed by deep sequencing n error bars indicate wilson intervals online methods clone number nature america inc all rights reserved sgrna sgrna sgrna npg e hesc hek sense ssodn bp to target a highly methylated region of the human locus fig f all three sgrnas tested were able to mediate indel mutations in endogenously methylated targets fig having established the optimal guide rna architecture for and having demonstrated its insensitivity to genomic cpg methylation we sought to conduct a comprehensive characterization of the dna targeting specificity of previous studies on cleavage were limited to a small set of single nucleotide mismatches between the guide sequence and dna target suggesting that perfect base pairing within bp directly of the pam pam proximal determines specificity whereas multiple pamdistal mismatches can be tolerated in addition a recent study using catalytically inactive as a transcriptional repressor found no significant off target effects throughout the escherichia coli however a systematic analysis of specificity within the context of a larger mammalian genome has not yet been reported to address this we first evaluated the effect of imperfect complementarity between the guide rna and its genomic target on activity and then assessed the cleavage activity resulting from a single sgrna on multiple genomic off target loci with sequence similarity to facilitate large scale testing of mismatched guide sequences we developed a simple sgrna testing assay by generating expression volume number september nature biotechnology letters fig for two independent biological replicates from these data we applied a binomial model to detect true indel events resulting from cleavage and nhej misrepair and calculated confidence cassettes encoding driven sgrnas using pcr and transfecting the resulting amplicons supplementary fig we then performed deep sequencing of the region flanking each target site supplementary figure single nucleotide specificity of a schematic of the experimental design sgrnas carrying all possible single base pair mismatches blue ns throughout the guide sequence were tested for each target site target site shown as example b heatmap representation of relative cleavage efficiency by single mutated and nonmutated sgrna each for four target sites aggregated from supplementary table for each target the identities of single base pair substitutions are indicated on the left original guide sequence is shown above and highlighted in the heatmap gray squares modification efficiencies increasing from white to dark blue are normalized to the original guide sequence sequence logo representation of the same data can be found in supplementary figure c heatmap for relative cleavage efficiency for each possible rna dna base pair compiled from aggregate data from single mismatch guide rnas for targets supplementary fig mean cleavage levels were calculated for the pam proximal bases right bar and across all substitutions at each position bottom bar positions in gray were not covered by the single mutated and unmutated sgrnas tested supplementary table d mediated indel frequencies at targets with all possible pam sequences determined using the surveyor nuclease assay two target sites from the locus were tested for each pam supplementary table e histogram of distances between nrg pam occurrences within the human genome putative targets were identified using both strands of human chromosomal sequences figure multiple mismatch specificity of a c cleavage efficiency with guide rnas containing consecutive mismatches of or bases a or multiple mismatches separated by different numbers of unmutated bases for targets and b c rows represent each mutated guide rna nucleotide substitutions are shown in white cells gray cells denote unmutated bases all indel frequencies are absolute and analyzed by deep sequencing from two biological replicas error bars indicate wilson intervals online methods intervals for all reported nhej frequencies online methods and supplementary tables we systematically investigated the effect of base pairing mismatches between guide rna sequences and target dna on target modification efficiency we chose four target sites within the human gene and and for each generated a set of 57 different guide rnas containing all possible single nucleotide substitutions in positions directly of the requisite ngg pam fig the guanine at position is preserved given that the promoter requires guanine as the first base of its transcript these off target guide rnas were then assessed for cleavage activity at the on target genomic locus consistent with previous tolerates singlebase mismatches in the pam distal region to a greater extent than in the pam proximal region in contrast to a model that implies that a prototypical bp pam proximal seed sequence largely determines target we found that most bases within the bp target site provide varying degrees of specificity single base specificity generally ranges from to bp immediately upstream of the pam indicating a sequence dependent mismatch sensitive boundary that varies in length fig supplementary fig and supplementary table to further investigate the contributions of base identity and position within the guide rna to specificity we generated additional sets of mismatched guide rnas for more target sites within the locus supplementary fig totaling over sgrnas these guide rnas were designed to cover all possible rna dna mismatches for each position in the guide sequence with at least coverage for positions our aggregate singlemismatch data reveal multiple exceptions to the seed sequence model of fig and supplementary table within the pam proximal region the degree of tolerance varied with the identity of a particular mismatch with rc dc base pairing exhibiting the highest level of disruption to cleavage activity fig in addition to the target specificity we also investigated the ngg pam requirement of to vary the second and third positions of pam we selected target sites within the locus encompassing all possible alternate pams with coverage supplementary table using the surveyor assay we showed that also cleaves targets with nag pams albeit with one fifth of the efficiency for target sites with ngg pams fig the tolerance for an nag pam is in agreement with previous bacterial and expands the s pyogenes target space to every bp on average within the human genome not accounting for constraining factors such as guide rna secondary structure or certain epigenetic modifications fig although we have shown here that methylated dna sequences can be cleaved by further characterization of the implications of epigenetic factors on crispr editing efficiency are needed we next explored the effect of multiple base mismatches on target activity for four targets within the gene we designed sets of guide rnas that contained varying combinations of mismatches to investigate the effect of mismatch number position and spacing on target cleavage activity fig b and supplementary table in general we observed that the total number of mismatched base pairs is a key determinant for cleavage efficiency two mismatches particularly those occurring in a pam proximal region considerably reduced activity whether these mismatches are concatenated or interspaced fig b this effect is further magnified for three concatenated mismatches fig furthermore three or more interspaced fig and five concatenated fig mismatches eliminated detectable cleavage in the vast majority of loci the position of mismatches within the guide sequence also affected the activity of pam proximal mismatches are less tolerated than pam distal counterparts fig recapitulating our volume number september nature biotechnology figure mediated indel frequencies at predicted genomic off target loci a b cleavage levels at putative genomic off target loci containing two or three individual mismatches white cells for target and target are analyzed by deep sequencing list of off target sites are ordered by median position of mutations putative off target sites with additional mutations did not have detectable indels supplementary table the dosage was nmol cell with equimolar sgrna delivery error bars indicate wilson intervals online methods c d indel frequencies for targets and and selected off target loci ot as a function of and sgrna dosage n wilson intervals ng to ng of sgrna plasmid corresponds to to nmol cell cleavage specificity is measured as a ratio of on to off target cleavage amplify off target one potential strategy for minimizing nonspecific cleavage is to limit the enzyme concentration namely the level of sgrna complex cleavage specificity measured as the ratio of on to off target cleavage increased dramatically as we decreased the equimolar amounts of and sgrna transfected into cells fig d from to nmol cell ng to ng of sgrna plasmid qrt pcr assay confirmed that the level of mrna and sgrna decreased proportionally to the amount of transfected dna supplementary fig whereas specificity increased gradually by nearly fourfold as we decreased the transfected dna amount from to nmol cell ng to ng plasmid we observed a notable additional sevenfold increase in specificity upon further decreasing transfected dna from to nmol cell ng to ng plasmid fig these findings suggest that we can minimize the level of off target activity by titrating the amount of and sgrna dna delivered however increasing specificity by reducing the amount of transfected dna also leads to a reduction in on target cleavage these measurements enable quantitative integration of specificity and efficiency criteria into dosage choice to optimize activity for different applications additional work to explore modifications in and sgrna design may improve intrinsic specificity without sacrificing cleavage efficiency the ability to program to target specific sites in the genome by simply designing a short guide rna complementary to the desired target site holds enormous potential for applications throughout biology and medicine our results demonstrate that the specificity of mediated dna cleavage is sequence and locus dependent and npg nature america inc all rights reserved letters governed by the quantity position and identity of mismatching bases whereas the pam proximal bp of the guide sequence generally defines specificity the pam distal sequences also contribute to the overall specificity of mediated dna cleavage although there may be off target cleavage for a given guide sequence they can be predicted and likely minimized by following general design guidelines to maximize specificity for editing a particular gene one should identify potential off target genomic sequences by considering the following four constraints first and foremost they should not be followed by a pam with either ngg or nag sequences second their global sequence similarity to the target sequence should be minimized and guide sequences with genomic off target loci that have fewer than three mismatches should be avoided third at least two mismatches should lie within the pam proximal region of the off target site fourth a maximal number of mismatches should be consecutive or spaced less than four bases apart finally the amount of and sgrna can be titrated to optimize on to off target cleavage ratio using these criteria we formulated a scoring algorithm to integrate and quantify the contributions of mismatch location density and identity on on target and off target cleavage we applied the aggregate cleavage efficiencies of single mismatch guide rnas to test this scoring scheme separately on genome wide targets and found that these factors taken together accounted for of the variance in cutting frequency rank among the genome wide targets studied supplementary fig implementing the guidelines delineated above we designed a computational tool to facilitate the selection and validation of sgrnas as well as to predict off target loci for specificity analyses this tool can be accessed at http www genome engineering org these results and tools further extend the system as a versatile alternative to zfns and talens for genome editing applications further work examining the thermodynamics and in vivo stability of sgrna dna duplexes will likely yield additional predictive power for off target activity whereas exploration of mutants and orthologs may yield novel variants with improved specificity methods methods and any associated references are available in the online version of the paper accession codes all raw reads can be accessed at ncbi bioproject accession number indices are described in supplementary tables note any supplementary information and source data files are available in the online version of the paper acknowledgments we thank a shalek e stamenova and d gray for expert help with dna sequencing r barretto for genome wide pam analysis as well as d altshuler p a sharp and the entire zhang lab for their support and advice p d h is a james mills pierce fellow d a s is an national science foundation pre doctoral fellow and j a w is supported by a life science research foundation fellowship x w is a howard hughes medical institute international student research fellow and is supported by national institutes of health nih grants and to p a sharp x w thesis advisor g b is supported by an nih nanomedicine development center award this work is supported by an nih director pioneer award an nih transformative grant to d altshuler the keck mcknight damon runyon searle scholars klingenstein and simons foundations and bob metcalfe and jane pauley the authors wish to dedicate this paper to the memory of officer sean collier for his caring service to the mit community and for his sacrifice reagents are available to the academic community through addgene and associated protocols support forums and computational tools are available through the zhang lab website http www genome engineering org author contributions j a w and f a r contributed equally to this work p d h d a s f a r s k and f z designed and performed the experiments p d h d a s j a w y l s k f a r and f z analyzed the data v a and o s contributed computational prediction of crispr off target sites and x w performed the northern blot analysis p d h f a r d a s and f z wrote the manuscript with help from all authors competing financial interests the authors declare competing financial interests details are available in the online version of the paper reprints and permissions information is available online at http www nature com reprints index html cong l et al multiplex genome engineering using crispr cas systems science jiang w bikard d cox d zhang f marraffini l a rna guided editing of bacterial genomes using crispr cas systems nat biotechnol wang h et al one step generation of mice carrying mutations in multiple genes by crispr cas mediated genome engineering cell mali p et al rna guided human genome engineering via science jinek m et al rna programmed genome editing in human cells elife cho s w kim s kim j m kim j s targeted genome engineering in human cells with the rna guided endonuclease nat biotechnol chang n et al genome editing with rna guided nuclease in zebrafish embryos cell res hwang w y et al efficient genome editing in zebrafish using a crispr cas system nat biotechnol shen b et al generation of gene modified mice via rna mediated gene targeting cell res gratz s j et al genome engineering of drosophila with the crispr rna guided nuclease genetics doi genetics july 11 deltcheva e et al crispr rna maturation by trans encoded small rna and host factor rnase iii nature jinek m et al a programmable dual rna guided dna endonuclease in adaptive bacterial immunity science 13 guschin d y et al a rapid and general assay for monitoring endogenous gene modification methods mol biol 14 bogenhagen d f brown d d nucleotide sequences in xenopus dna required for transcription termination cell 1981 bultmann s et al targeted transcriptional activation of silent pluripotency gene by combining designer tales and inhibition of epigenetic modifiers nucleic acids res 5368 valton j et al overcoming transcription activator like effector tale dna binding domain sensitivity to cytosine methylation j biol chem christian m et al targeting dna double strand breaks with tal effector nucleases genetics miller j c et al a tale nuclease architecture for efficient genome editing nat biotechnol mussolino c et al a novel tale nuclease scaffold enables high genome editing activity in combination with low toxicity nucleic acids res hsu p d zhang f dissecting neural function using targeted genome engineering technologies acs chem neurosci sanjana n e et al a transcription activator like effector toolbox for genome engineering nat protoc 171 22 porteus m h baltimore d chimeric nucleases stimulate gene targeting in human cells science miller j c et al an improved zinc finger nuclease architecture for highly specific genome editing nat biotechnol sander j d et al selection free zinc finger nuclease engineering by contextdependent assembly coda nat methods 67 wood a j et al targeted genome editing across species using zfns and talens science 26 bobis wozowicz s osiak a rahman s h cathomen t targeted genome editing in pluripotent stem cells using zinc finger nucleases methods 27 qi l s et al repurposing crispr as an rna guided platform for sequence specific control of gene expression cell michaelis l m maud die kinetik der invertinwirkung biochemistry zeitung 333 mahfouz m m et al de novo engineered transcription activator like effector tale hybrid nuclease with novel dna binding specificity creates double strand breaks proc natl acad sci usa volume number september 2013 nature biotechnology online methods npg 2013 nature america inc all rights reserved cell culture and transfection human embryonic kidney hek cell line life technologies was maintained in dulbecco modified eagle medium dmem supplemented with fbs hyclone mm glutamax life technologies u ml penicillin and µg ml streptomycin at c with incubation cells were seeded onto well plates well plates or well plates corning h before transfection cells were transfected using lipofectamine life technologies at confluency following the manufacturer recommended protocol for each well of a well plate a total of µg of sgrna plasmid was used for each well of a well plate a total of ng sgrna plasmid was used unless otherwise indicated for each well of a well plate ng of plasmid was used at a molar ratio to the sgrna pcr product human embryonic stem cell line harvard stem cell institute core was maintained in feeder free conditions on geltrex life technologies in mtesr medium stemcell technologies supplemented with µg ml normocin invivogen cells were transfected with amaxa primary cell d nucleofector kit lonza following the manufacturer protocol surveyor nuclease assay for genome modification and cells were transfected with dna as described above cells were incubated at 37 c for h post transfection before genomic dna extraction genomic dna was extracted using the quickextract dna extraction solution epicentre following the manufacturer protocol briefly pelleted cells were resuspended in quickextract solution and incubated at c for min c for min and c for min the genomic region flanking the crispr target site for each gene was pcr amplified target sites and primers listed in supplementary tables and and products were purified using qiaquick spin column qiagen following the manufacturer protocol ng total of the purified pcr products were mixed with µl taq dna polymerase pcr buffer enzymatics and ultrapure water to a final volume of µl and subjected to a re annealing process to enable heteroduplex formation c for min c to c ramping at c c to c at c and c hold for min after re annealing products were treated with surveyor nuclease and surveyor enhancer s transgenomics following the manufacturer recommended protocol and analyzed on novex tbe polyacrylamide gels life technologies gels were stained with sybr gold dna stain life technologies for min and imaged with a gel doc gel imaging system bio rad quantification was based on relative band intensities indel percentage was determined by the formula b c a b c where a is the integrated intensity of the undigested pcr product and b and c are the integrated intensities of each cleavage product northern blot analysis of tracrrna expression in human cells northern blots were done as previously briefly rnas were extracted using the mirpremier microrna isolation kit sigma and heated to c for min before loading on denaturing polyacrylamide gels sequagel national diagnostics afterwards rna was transferred to a hybond n membrane ge healthcare and crosslinked with stratagene uv crosslinker stratagene probes were labeled with gamma atp perkinelmer with polynucleo tide kinase new england biolabs after washing membrane was exposed to phosphor screen for h and scanned with phosphorimager typhoon bisulfite sequencing to assess dna methylation status genomic dna from cells was isolated with the dneasy blood tissue kit qiagen and bisulfite converted with ez dna methylation lightning kit zymo research bisulfite pcr was conducted using robust hotstart dna polymerase kapa biosystems with primers designed using the bisulfite primer seeker zymo research supplementary table resulting pcr amplicons were gelpurified digested with ecori and hindiii and ligated into a backbone before transformation individual clones were then sanger sequenced to assess dna methylation status in vitro transcription and cleavage assay whole cell lysates from cells were prepared with lysis buffer mm hepes mm kcl mm doi 1038 nbt mm dtt glycerol triton x supplemented with protease inhibitor cocktail roche driven sgrna was transcribed in vitro using custom oligos supplementary sequences and hiscribe in vitro transcription kit neb following the manufacturer recommended protocol to prepare methylated target sites plasmid was methylated by m sssi and tested by digestion with hpaii unmethylated and successfully methylated plasmids were linearized by nhei the in vitro cleavage assay was carried out as follows for a µl cleavage reaction µl of cell lysate was incubated with µl cleavage buffer mm hepes mm kcl mm mm dtt glycerol µg in vitro transcribed rna and ng plasmid dna deep sequencing to assess targeting specificity hek cells plated in well plates were transfected with plasmid dna and sgrna pcr cassette h before genomic dna extraction supplementary fig the genomic region flanking the crispr target site for each gene was amplified supplementary fig supplementary table and supplementary sequences by a fusion pcr method to attach the illumina adapters as well as unique sample specific barcodes to the target amplicons schematic described in supplementary fig pcr products were purified using econospin well filter plates epoch life sciences following the manufacturer recommended protocol barcoded and purified dna samples were quantified by quant it picogreen dsdna assay kit or qubit fluorometer life technologies and pooled in an equimolar ratio sequencing libraries were then sequenced with the illumina miseq personal sequencer life technologies sequencing data analysis and indel detection miseq reads were filtered by requiring an average phred quality q score of at least as well as perfect sequence matches to barcodes and amplicon forward primers reads from on and off target loci were analyzed by first performing smith waterman alignments against amplicon sequences that included nucleotides upstream and downstream of the target site a total of bp alignments meanwhile were analyzed for indels from nucleotides upstream to nucleotides downstream of the target site a total of bp analyzed target regions were discarded if part of their alignment fell outside the miseq read itself or if matched base pairs comprised less than of their total length negative controls for each sample provided a gauge for the inclusion or exclusion of indels as putative cutting events for each sample an indel was counted only if its quality score exceeded µ σ where µ was the mean qualityscore of the negative control corresponding to that sample and σ was the d of the same this yielded whole target region indel rates for both negative controls and their corresponding samples using the negative control pertarget region per read error rate q the sample observed indel count n and its read count r a maximum likelihood estimate for the fraction of reads having target regions with true indels p was derived by applying a binomial error model as follows letting the unknown number of reads in a sample having target regions incorrectly counted as having at least indel be e we can write without making any assumptions about the number of true indels r p e r p e prob e p q q e as r p is the number of reads having target regions with no true indels meanwhile because the number of reads observed to have indels is n n e rp that is the number of reads having target regions with errors but no true indels plus the number of reads whose target regions correctly have indels we can then rewrite the above r p n rp r n prob e p prob n e rp p q q n rp taking all values of the frequency of target regions with true indels p to be equally probable a priori prob n p prob p n the maximum likelihood estimate mle for the frequency of target regions with true nature biotechnology in bacteria foreign nucleic acids are silenced by clustered regularly interspaced short palindromic repeats crispr crispr associated cas systems bacterial type ii crispr systems have been adapted to create guide rnas that direct site specific dna cleavage by the endonuclease in cultured cells here we show that the crispr cas system functions in vivo to induce targeted genetic modifications in zebrafish embryos with efficiencies similar to those obtained using zinc finger nucleases and transcription activator like effector nucleases bacteria and archaea have evolved an adaptive defense mechanism that uses crispr together with cas proteins to provide themselves with acquired resistance to invading viruses and the type ii crispr cas system relies on uptake of foreign dna fragments into crispr and subsequent transcription and processing of these rna transcripts into short crispr rnas crrnas which in turn anneal to a trans activating crrna tracrrna and direct sequencespecific silencing of foreign nucleic acids by cas fig recent in vitro work showed that a synthetic single guide rna sgrna consisting of a fusion of crrna and tracrrna can direct endonuclease mediated cleavage of target fig in addition can function with either crrna and tracrrna together or sgrna to efficiently induce targeted alterations in cultured human however whether crispr cas based rna guided endo nucleases rgens can be used like zinc finger nucleases zfns or transcription activator like effector nucleases talens 11 for genome editing in whole organisms is not known here we show that customizable sgrnas can direct endonuclease mediated alteration of endogenous genes in zebrafish embryos first we constructed expression vectors that enable rna polymerase mediated production of a capped polyadenylated mrna encoding the monomeric endonuclease and of a customizable sgrna bearing nucleotides nts of sequence complementary to a target site fig the sequence of our sgrna like that of another recently differs from a sgrna used in in that our sgrna contains additional tracrrna derived sequences at its end fig c and supplementary table for initial experiments we designed a sgrna with a targeting region complementary to a sequence in the fh gene site no supplementary table to determine the optimal quantity of each rna species to use for genome editing we microinjected varying amounts of fh targeted sgrna and encoding mrna into one cell stage zebrafish embryos we then assessed the frequency of altered alleles in single embryos using a endonuclease i assay supplementary methods we observed targeted insertion deletion mutations indels at all concentrations of rnas examined and in nearly all embryos tested supplementary table however the highest mean frequency of mutations was obtained with a solution containing ng µl sgrna and ng µl encoding mrna supplementary table so we used these concentrations for all subsequent experiments sequencing of mutated fh alleles revealed indels that begin within or encompass the end of the dna sequence complementary to the sgrna supplementary fig this pattern of mutations is consistent with the expected induction of a induced double stranded break at this within the genomic fh target site followed by error prone nonhomologous end joining mediated repair to test the robustness of the sgrna system in zebrafish we constructed ten additional sgrnas one targeted to another sequence in the fh gene site no and the remaining nine targeted to sites in nine other endogenous genes supplementary table we detected high frequencies means of of targeted indels at eight of these ten sites in all individual embryos tested table frequencies of mutagenesis did not appear to depend upon which dna strand sense or anti sense was targeted by the sgrna notably we detected highly efficiencient mutagenesis at sites in the and genes which we were not able to alter using talens supplementary table mutation rates in the other six successfully targeted sites were similar to those seen in these same genes using zfns and or talens table and supplementary table sequencing of all eight of these target sites confirmed the efficient introduction of targeted indels at the expected genomic locations fig supplementary table and supplementary fig the lengths of indel mutations induced by these sgrna constructs are similar to those of mutations induced by zfns and talens previously made by our groups supplementary fig furthermore the nature of the mutations that is the relative abundance of insertions and deletions also appears to be similar among all three systems supplementary fig thus the sgrna system successfully targeted of the sites we tested in zebrafish research center massachusetts general hospital charlestown massachusetts usa pathology unit center for cancer research and center for computational and integrative biology massachusetts general hospital charlestown massachusetts usa of pathology harvard medical school boston massachusetts usa in biological and biomedical sciences harvard medical school boston massachusetts usa of medicine harvard medical school boston massachusetts usa institute cambridge massachusetts usa authors contributed equally to this work figure schematic illustrating naturally occurring and engineered crispr cas systems a naturally occurring dual rna endonuclease crrna interacts with the complementary strand of the dna target site harboring an adjacent pam sequence green and red text respectively tracrrna base pairs with the crrna and the overall complex is recognized and cleaved by nuclease light blue shape folding of the crrna and tracrrna molecules is depicted as predicted by and the association of the crrna to the tracrrna is depicted partially based on the model previously proposed b engineered sgrna system previously used in vitro sgrna composed of portions of the crrna and tracrrna from a is illustrated interacting with the dna target site folding of sgrna is as predicted by c modified engineered sgrna system used in vivo in this study components are illustrated the same way as in b except the sgrna contains additional sequence from the end of the tracrrna folding of sgrna is as predicted by the sgrna depicted is essentially identical to that previously d targeted indel mutations induced by engineered sgrna at the and genes for each gene the wild type sequence is shown at the top with the target sites highlighted in yellow and the pam sequence highlighted as red underlined text deletions are shown as red dashes highlighted in gray and insertions as lower case letters highlighted in blue the net change in length caused by each indel mutation is to the right of each sequence insertion deletion note that some alterations have both insertions and deletions of sequence and in these instances the alterations are enumerated in the parentheses the number of times each mutant allele was isolated is shown in brackets for each gene locus dna fragments were amplified by pcr from genomic dna of ten pooled modified embryos and then sequenced as described in supplementary methods see also supplementary fig with the crispr cas system used in this study only one customized sgrna is required to target a specific sequence and the same cas enzyme is suitable for all sequences in contrast talens or zfns require the design and assembly of two nucleases for each target site in addition sgrnas are encoded by short bp sequences and are therefore much simpler and easier to construct than talens or zfns the short length of sgrna sequences also avoids complications associated with longer typically kb or more and highly repetitive talen encoding vectors e g delivery using viral challenges with dna sequencing potential for recombination it will be of interest to determine why our sgrna reagents efficiently mutagenized sites in endogenous zebrafish genes that we could not alter using talens the sgrna system described here can in principle target any sequence of the form gg ngg such sites occur once in every bp of random dna sequence constraints on the range of targetable sequences are due to sequence requirements imposed by the promoter used to make sgrnas gg at the end of the transcript supplementary fig and by the requirement for a protospacer adjacent motif pam sequence ngg in genomic dna just to the target previous studies suggest that the promoter requirement for a pair of guanines at the end of the transcript could be relaxed to allow for an adenine at either loosening this constraint would enable targeting of sequences of the form g a g a ngg which occur once in every bp of random dna sequence in addition it is also possible that mismatches between the two sgrna bases nearest to the end and the target site might be tolerated although as this may enable targeting of additional sites it may have implications for off target effects future studies should perform larger scale tests of the targeting range of the sgrna system for both matched and mismatched target sites volume number march 2013 nature biotechnology for each target site up to ten individual embryos were assessed for indel mutation frequency using the pcr based assay supplementary methods mean frequencies of mutation for the ten embryos assayed for each target site are also shown with e m n a pcr reaction failed data from such experiments may also yield insights into why some sgrnas fail to mediate efficient sequence alterations and whether such failures can be predicted in advance the modified crispr cas system described here may be adapted to modify the genome of any organism into which rna can be introduced the plasmids expressing short nt sgrnas with customized targeting regions can be easily and rapidly assembled simply by ligating pairs of short annealed oligonucleotides into our promoter based sgrna vector supplementary methods and supplementary figure this process is considerably simpler than other methods for assembling talen or zfn encoding and therefore should be readily amenable to automation and high throughput use we have updated our web based zifit targeter to enable identification of sgrna targetable sites and to generate the sequences of oligonucleotides required to construct customized sgrnas http zifit partners org all plasmids described in this report are available through the nonprofit reagent distribution service addgene http www addgene org crispr jounglab any updates to reagents protocols and software will be made available on the website http www crispr cas org previous studies showed efficient germline transmission of all zfnand talen engineered somatic mutations present at rates of or greater in zebrafish embryos that develop because all of the active sgrna endonuclease combinations described here induced somatic mutation rates well above and these mutations were detected in normally developing embryos we expect that germline transmission of sgrna induced mutations will be as efficient as those induced by zfns or talens based on the rates of mutagenesis we observed we calculate that the mean frequency of cells bearing bi allelic alterations in these embryos would range from to supplementary discussion thus a proportion of the cells in many founders is likely to experience complete loss of target gene function although these rates might allow screening for certain phenotypes in the founders themselves germline transmission will still be desirable in most instances to create nonmosaic knockout animals additional work is needed to determine the off target effects of the sgrna system used here previous work in vitro with purified components and in cultured human cells has suggested that the end of the sgrna target recognition sequence may be the most critical with regard to but whether this will also be true in vivo remains to be determined the toxicity induced by sgrna encoding mrna in zebrafish as judged by the numbers of deformed and dead embryos supplementary fig appeared to vary among the different sgrnas but did not correlate with their abilities to induce indels at the intended target sites a phenomenon we nature biotechnology volume number march 2013 have also observed with zfns and the frequencies of deformed or dead embryos are similar to what we have observed in previous experiments using 19 and this demonstration that a customized crispr cas system can be used to efficiently induce site specific modifications in zebrafish should encourage wider use of this robust and easy to use technology potentially in a variety of other organisms note supplementary information is available in the online version of the paper abstract few developments in microbiological diagnostics have had such a rapid impact on species level identification of microorganisms as matrix assisted laser desorption ionization time of flight mass spectrometry maldi tof ms conventional differentiation methods rely on biochemical criteria and require additional pre testing and lengthy incubation procedures in comparison maldi tof ms can identify bacteria and yeast within minutes directly from colonies grown on culture plates this radically new methodically simple approach profoundly reduces the cost of consumables and time spent on diagnostics the reliability and accuracy of the method have been demonstrated in numerous studies and different systems are already commercially available novel applications of the system besides microbial species level identification are also being explored this includes identification of pathogens from positive blood cultures or directly from patient samples such as urine currently intriguing maldi tof ms developments are being made regarding the phenotypic detection of certain antibiotic resistance mechanisms e g β lactamases and carbapenemases this mini review provides an overview of the literature in the field and also includes our own data and experiences gathered from over years of routine maldi tof ms use in a university hospital microbiological diagnostics facility keywords maldi tof microbiological diagnostic pathogen identification blood cultures antibiotic resistance strain typing a wieser l schneider j jung s schubert max von pettenkofer institut für hygiene und medizinische mikrobiologie ludwig maximilians universität münchen marchioninistr munich germany e mail schubert med uni muenchen de introduction mass spectrometry is the analytic technique used to analyze the mass to charge ratio of various compounds different techniques based on various ionization and detection systems have been developed the most widely used method to date for the analysis of biomolecules is matrix assisted laser desorption ionization time of flight mass spectrometry maldi tof ms it is based on the ionization of cocrystallized sample material by short laser pulses the ions are accelerated and their time of flight is measured in a vacuum flight tube maldi tof ms has successfully been used in research to determine the mass of proteins and peptides in addition to identifying previously unknown proteins marvin et al maldi tof ms contributed to the diagnosis of tumors rheumatoid arthritis alzheimer disease and allergies through the identification of specific biochemical markers marvin et al the first attempts to identify microorganisms using mass spectrometry were performed as early as anhalt and fenselau however these experiments suffered from irreproducible results due to the variabilities caused by growth conditions and media only with the discovery of maldi tof ms in the the analysis of relatively large biomolecules including larger ribosomal proteins became possible hillenkamp and karas the latter are less influenced by culture conditions allowing maldi tof ms to be consistently used to differentiate bacterial species claydon et al holland et al demirev et al fenselau and demirev krishnamurthy and ross krishnamurthy et al in recent years maldi tof ms has been implemented in routine laboratories and utilized as a completely new approach for the identification of bacteria and yeast fig fig principle of maldi tof ms identification of bacteria and yeast in schematic diagram laser impact causes thermal desorption of ribosomal proteins of bacteria yeast embedded in matrix material and applied to the target plate analytes shown as red light blue and orange spheres the matrix is given as green spheres in an electric field ions are accelerated according to their mass and electric charge the drift path allows further separation and leads to measurable differences in time of flight of the desorbed particles that are detected on top of the vacuum tube from the time of flight the exact mass of the polypeptides can be calculated procedural overview mass spectrometer a high vacuum has to be continuously maintained however upon insertion of the loaded target plate air is introduced into the system and the vacuum must be reestablished before sample analysis can be performed once a sufficient vacuum has been created the individual samples are exposed to short laser pulses the laser energy vaporizes the microorganism together with the matrix leading to ionization of the ribosomal proteins an electromagnetic field created by a potential of about kv accelerates the ions before they enter the flight tube the time of flight tof of the analytes to reach the detector at the end of the flight tube is precisely measured the degree of ionization as since maldi tof ms is a very sensitive technique only a small amount of microbial biomass is required for analysis for bacteria to cfu to identify a microorganism the sample is mixed with μl of matrix solution and placed on the steel surface of the target plate to dry the matrix solution cinnamic acid or a benzoic acid derivate cocrystallizes with the sample on the target plate fig a typical target plate can hold between and samples the loaded target plate is inserted into the machine where it is then transported to the measuring chamber within the normalize d peak heights well as the mass of the proteins determines their individual tof based on this tof information a characteristic spectrum is recorded and constitutes a specific sample fingerprint which is unique for a given species for species level identification the size range generally used is between and kda as it was found to be very stable and with a strong signal to noise ratio interestingly this size range is dominated by ribosomal proteins which ionize well provide accurate spectra and are only minimally influenced by microbial growth conditions fig the computer software automatically compares the collected spectra with a reference databank containing a wide variety of medically relevant isolates fig the measured spectra are subject to method inherent noise and therefore will never be exactly identical for an individual isolate the software which compares the spectra generates a numerical value score value based on the similarities between the observed and stored data sets fig this score value provides information about the validity of the identification a score value above is generally considered to be a valid species level identification values between and represent reliable genus level identifications furthermore the software displays additional results next to the best match for plausibility checks fig current algorithms allow the entire computational analysis to be performed in near real time jarman et al sauer et al therefore if only one sample is to be measured it can be processed in min and provides a species level identification if a target plate containing isolates is used results can be obtained in about h starting from the time point the first sample is loaded on the plate after the analysis process in the maldi tof ms the used target plate is removed from the machine disposable use of maldi tof ms in microbiological diagnostics identification and differentiation of bacterial and fungal isolates grown on agar plates the main goal of microbiological diagnostics is to identify the causative agents of infectious diseases such as bacteria fungi and parasites classically microbiological diagnostics is based on microscopy of specimens and culturing on media which may also include antimicrobial susceptibility testing for more than years differentiation of bacteria and fungi has mainly been accomplished through microscopy and analyzing metabolic traits using biochemical reaction profiles studies have evaluated the potential of maldi tof ms to perform these identification procedures it could be demonstrated that maldi tof ms and conventional biochemical differentiation methods are both highly accurate for the identification of isolated subcultured bacteria and yeast a retrospective study of clinical isolates comparing maldi tof ms with conventional biochemical testing systems showed correct species identification of bacteria in intens a u fig maldi tof mass spectrum of enterococcus faecium the measured range of to 11 da is displayed the characteristic mass peaks are predominantly ribosomal proteins subsequently the integrated maldi software matches the pattern with entries of a database target plates can be discarded in the regular laboratory waste while reusable versions are cleansed for further use most recommended cleaning procedures start with treating the plate with ethanol and tca solutions and include mechanical cleansing steps in our hands a quick cleaning protocol with min of ethanol and subsequent mechanical cleaning with detergent and cloth is sufficient for regular workup and produces significantly less chemical waste own validated protocol further the more aggressive cleaning protocols can be performed on a weekly basis own validated protocol species identification fig computer display of identification results after automatic comparison of the generated spectrum with the maldi tof database the ten best matching entries are shown in a tabular form the degree of of the cases eigner et al in a similar prospective study including bacterial isolates from different species 84 were correctly identified by maldi tof ms at the species level seng et al at the time missing database entries were at fault for the majority of unsuccessfully identified samples our validation studies with the bruker daltonik bremen germany maldi tof ms system have also confirmed the validity of the system the mass spectrometry results have been compared with those achieved by biochemical identification systems phoenix becton dickinson heidelberg germany and api biomérieux nürtingen germany in the case of discrepancies rdna rdna sequencing was performed unpublished data out of prospectively analyzed isolates of the identifications were in accordance with the results achieved through biochemical methods the isolates included enterobacteriaceae nonfermenters other gram negatives grampositives and yeast when rdna sequences were used as the gold standard biochemical differentiation was superior in of the cases whereas maldi tof ms provided a more accurate identification result in unpublished data in many cases where the malditof ms yielded no result the relevant species information was absent from the database by now most of the clinically relevant organisms are included in the spectra database and have nearly closed the diagnostic gap score value ncbi code similarity to the reference spectrum is represented by a score value identification results with score values above are considered to be correct for determination of the respective species with the corresponding additions to the reference database the identification rate was almost for neisseria ilina et al ilina et al clostridia grosse herrenthey et al mycobacteria pignone et al salmonella dieckmann et al viridans group streptococci friedrichs et al helicobacter pylori ilina et al and campylobacter martiny et al the results were very similar when comparing the maldi tof ms system with biochemical methods to identify enterobacteriaceae staphylococci and streptococci holler et al the maldi tof ms showed advantages in the identification of gram positive rods barbuddhe et al anaerobes and some nonfermenters mellmann et al vanlaere et al maldi tof ms has also been used successfully by several groups to differentiate yeast and fungi marklein et al proved that out of clinical candida isolates from different species could be correctly identified by maldi tof ms marklein et al two prospective studies looking at the identification of yeast showed comparably high results of identification bizzini et al van veen et al far less data are available for the differentiation of molds like aspergillus sp penicillium sp fusarium sp and dermatophytes amiri eliasi and fenselau fenselau and demirev erhard et al hettick et al hettick et al marinach appl microbiol biotechnol 93 patrice et al putignani et al as maldi tof ms identification can only be performed from cultured fungi the various growth forms of molds such as mycelium and conidia complicate the analysis due to differences in protein composition adjustments and optimizations are needed to enhance the performance of maldi tof ms based identification systems for routine diagnostics of molds furthermore the cultivation of various types of fungi such as dermatophytes is relatively time consuming and no data exist regarding the potential benefits of malditof ms in this setting in many cases there is no clinical relevance for exact identification e g athlete foot and therefore many routine diagnostic laboratories do not even perform dermatophyte differentiation currently used maldi tof ms techniques are nearly independent of culture conditions selective media such as mcconkey and xld agar can also be used in addition to standard media formulations such as columbia and chocolate agar since small amounts of culture material are enough for successful analysis it is often possible to use a colony from a culture or subculture only a few hours after inoculation this is why it is possible to obtain a pure culture identification from a mixed culture within the same day besides culturing the organism sample preparation is an important factor contributing to analysis quality in most cases the maldi tof ms analysis can be performed with little sample preparation other than streaking a colony on the target plate directly streaked colonies used for analysis should be as fresh as possible not more than h because with increasing cultivation time weaker and less distinguished peaks will appear in the spectra unpublished observation this effect is probably due to ribosomal protein degradation and leads to less efficient species identification in some instances a sample may have a strong cell wall e g yeast and may require a short extraction procedure to render the ribosomal proteins available for analysis and to provide a more defined profile for database matching and differentiation with higher confidence for this the colony to be analyzed is suspended in an ethanol solution centrifuged and resuspended in an acetonitrile and formic acid solution the resulting supernatant is again centrifuged and placed on the target plate for analysis an efficient quick extraction protocol can be performed directly on the target plate by adding μl of neat formic acid on the dried crude sample spot haigh et al direct identification of bacteria and yeast from positive blood cultures using maldi tof ms early on attempts were made to use the maldi tof ms for differentiation of pathogens directly from positive blood cultures la scola and raoult the problem with this application is that a high concentration of host proteins from the patient blood sample interferes with the detection of specific bacterial and fungal proteins as a result several research groups have developed and studied preparation protocols that separate bacterial and host proteins through cell lysis or differential centrifugation steps christner et al ferroni et al stevenson et al schubert et al in 95 of tests performed correct species level identification could be achieved prod hom et al ferreira et al for use in a routine diagnostics laboratory protocols need to be accurate and concise many of the published procedures are time consuming and difficult to incorporate into the laboratory workflow therefore cell lysis methods are generally favored due to their simplicity and effectivity fig such protocols allow about of positive blood cultures to be successfully analyzed within min to obtain such high levels of identification modifications to the software thresholds have been implemented stevenson et al schubert et al moussaoui et al for example score values lower than are considered valid if the first three results in the list presented for each sample analysis refer to the same species secondly the protein mass range used for homology search in the database is changed to remove unstable spectra caused by contaminations introduced by the patient blood and the blood culture system these modifications do not abrogate the inherent limitations of the system such as difficulties distinguishing between streptococcus pneumoniae and streptococcus mitis oralis maldi tof ms enables the identification of pathogenic bacteria and yeast directly from patients urine samples analysis of pure isolated cultures from agarose plates is routinely performed with simple protocols identification of germs from a blood culture system requires extraction and enrichment protocols the most challenging task however is to directly analyze patient samples without a cultural enrichment step such sample material is most often rich in host proteins and normal flora which both overlay a pathogen mass spectrum urine is a good candidate as it does not contain normal flora and there are almost no host proteins in the sample at the same time urinary tract infections are normally monomicrobial and exhibit high concentrations of pathogens during the course of infection ferreira et al ferreira et al to prepare the samples for analysis they are slowly centrifuged to pellet out the leukocytes afterwards a fast centrifugation step will pellet the pathogen it is then washed with distilled water to remove residual contaminants these refined samples can be directly analyzed with a maldi tof ms in a large study using this protocol it total time min fig workflow of sample preparation for identification of bacteria and yeasts directly from positive blood cultures ml of blood culture liquid is filled into an eppendorf tube a lysis buffer is added b sample is centrifuged for min at and subsequently supernatant is discarded c washing buffer is added to the pellet and mixed carefully d centrifugation step is repeated e was possible to correctly identify pathogens at the species level directly from urine samples at rates of significant bacterial load cfu ml ferreira et al thus maldi tof ms presents itself as a true alternative to more lengthy and expensive culture based microbial identification systems especially in the outpatient setting where resistance testing is not required pathways and can therefore misclassify isolates in contrast maldi tof ms does not depend on metabolic reactions further maldi tof ms can perform species level identification without the need for predifferentiation a single maldi tof ms system can be used for gram positive bacteria gram negative bacteria and yeast which is not the case with biochemical differentiation methods fast pathogen identification is critical in clinical diagnostics with maldi tof ms species identifications can be obtained from a colony within a few minutes so valuable differentiation results can be communicated to the responsible physician right after the cultures have been reviewed for the first time h after receiving the sample further sample preparation can be automated allowing high sample throughput analysis of clinical samples kiestra wasp another advantage of maldi tof ms is that the reference spectra database can be amended and edited by either commercially available software updates or by internal laboratory personnel an open source platform can be established as a way for users to exchange spectra of isolates to increase their own reference databases however the quality of the entries must be controlled to ensure that no wrong data are distributed the flexibility of the system is attractive to users in various fields because it provides a high level of customized features pertinent to their respective interests as for any diagnostic system there are limitations to maldi tof ms to date in nearly all cases a preculture is required for successful analysis of most patient samples further maldi tof ms has only limited ability to detect bacterial resistance mechanisms maldi tof ms in microbiological diagnostics advantages opportunities and limitations maldi tof ms is increasingly used for microbiological diagnostics and has already replaced conventional biochemical differentiation methods in some laboratories minor discrepancies between biochemical molecular and malditof ms based differentiation results have been observed differences are seen because maldi tof ms pathogen identification is based on the analysis of ribosomal protein spectra such differentiation results therefore are closely related to the results of the rdna sequence database comparisons consequently species which do not differ sufficiently in their ribosomal protein sequences such as shigella spp and escherichia coli or s pneumoniae pneumococcus and members of the s oralis mitis group cannot be distinguished by maldi tof ms this is where classical biochemical tests antigen detection or molecular methods are required on the other hand maldi tof ms reveals the shortcomings of the conventional metabolic testing methods the latter are very vulnerable to the gain and loss of function in metabolic appl microbiol biotechnol 93 costs are an important factor when considering integrating new systems into a laboratory a typical maldi tof ms system used in routine diagnostics currently costs in the range of us 000 000 including analysis equipment computers relevant software and integrated databases these initial investments are offset by low overall operating costs due to high sample throughput analysis and the low amount of consumables required expenditures per analysis can be as low or even lower than cents in the near future it is expected that technological improvements in semiconductor lasers will allow for longer lifespan and faster pulse frequencies further reducing running costs while improving speed and efficiency the introduction of automatic heat cleaning systems for the maldi tof ms can extend the intervals between routine cleaning decreasing maintenance costs and downtime in the study by seng et al seng et al biochemical testing was found to be four times more expensive than maldi tof ms analysis in this calculation the authors included expenses for consumables salaries of employees and equipment depreciation over a year period similar savings were identified in a study by cherkaoui et al cherkaoui et al however each laboratory must carefully calculate the cost reduction for their specific situation as not all study results can easily be transferred another important diagnostic challenge is the detection of β lactamase production in enterobacteriaceae the main mechanism for bacterial resistance to carbapenem and β lactam antibiotics a promising approach for identification of β lactam resistant strains using maldi tof ms is the detection of cleavage products of the respective compounds fig these products are produced by hydrolysis of β lactam rings by the bacterial enzymes in case of the β lactam antibiotic ampicillin the molecule will be decarboxylated after the addition of a these chemical modifications change the mass of the antibiotic compound which can be detected in the mass spectrometer and prove the presence of a β lactamase to produce enough cleavage products for successful detection the bacterial strain needs to be co incubated with the antibiotic for a period of h using this method resistance to various penicillins cephalosporins and carbapenems could be successfully detected burckhardt and zimmermann hrabak et al sparbier et al a further application of maldi tof ms is the differentiation of isolates below the species level this allows epidemiological analysis of patient and environmental samples the idea is to use discrete differences between the protein spectra for individual strain typing and to save the co future applications of maldi tof for microbiological diagnostics currently further applications of maldi tof ms are being developed for clinical microbiological diagnostics one major area of interest is detecting antimicrobial resistance mechanisms one of the organisms in focus is methicillin resistant staphylococcus aureus mrsa studies were performed to detect the penicillin binding protein in mrsa using maldi tof ms to rapidly confirm the methicillin resistant phenotype some trials found differences in protein patterns between methicillin susceptible mssa and resistant mrsa s aureus strains however the detected differences were not due to the resistance determinant but due to the clonality of the s aureus isolates wolters et al bernardo et al this means typical mrsa clones were separated from typical mssa clones as a consequence isolates of a typical mrsa clone can be misidentified as resistant to methicillin although they have lost a functional resistance determinant conversely susceptible clones which have newly acquired the resistance genes can also be misidentified as susceptible wolters et al jackson et al friedrichs et al majcherczyk et al given this it is not surprising that conflicting data have been published bernardo et al du et al e coli isolates after coincubation with ampicillin for h the mass peak of ampicillin is detectable only in the supernatant of the βlactamase negative control strain a in contrast the supernatant of the esbl positive e coli strain reveals the mass peaks of the hydrolyzed ampicillin b with a mass of plus da according to the addition of the further decarboxylated product is visible as a lower mass peak c modified according to sparbier et al poster no at the msacl conference san diego time and costs for molecular analysis such as pcr and restriction fragment length polymorphism analysis recent studies on salmonellae francisella tularensis bacteroides fragilis and streptococcus agalactiae demonstrated the potential of maldi tof ms as a tool to differentiate bacteria on a subspecies level dieckmann et al dieckmann and malorny lartigue et al seibold et al nagy et al other examples for subspecies typing include the accurate identification of genomic species from the acinetobacter baumannii group and the rapid subtyping of yersinia enterocolitica isolates stephan et al espinal et al one of the current limitations of the maldi tof system is the inability to detect pathogens directly from patient material with the exception of urine see above various attempts are made to directly analyze patient material from other sources such as cerebrospinal fluid and blood however no working protocols have been published to date an increasing number of laboratories with high sample throughput are pushing towards full laboratory automation this concept includes automatic inoculation of patient material on growth media as well as automated cultivation growth detection and digital photography of the sample cultures the next goal is to have colonies automatically picked and differentiated to the species level systems incorporating an automatically loaded maldi tof ms for microbial identification are in the promising stages of development and are expected to be fully operational in the near future wasp kiestra maldi tof ms has only recently been introduced into the field of microbiological diagnostics and many innovations and new developments point to a promising future for its use in the coming years acknowledgments the work to refine the diagnostic maldi tof ms technology in our lab was supported by bayerische forschungsstiftung forprotect protection against infectious diseases new diagnostic and therapeutic approaches while pyrolysis of livestock manures generates nutrient rich biochars with potential agronomic uses studies are needed to clarify biochar properties across manure varieties under similar controlled conditions this paper reports selected physicochemical results for ﬁve manure based biochars pyrolyzed at and c swine separated solids paved feedlot manure dairy manure poultry litter and turkey litter elemental and ftir analyses of these alkaline biochars demonstrated variations and similarities in physicochemical characteristics the ftir spectra were similar for turkey and poultry and feedlot and dairy but were distinct for swine biochars dairy biochars contained the greatest volatile matter c and energy content and lowest ash n and s contents swine biochars had the greatest p n and s contents alongside the lowest ph and ec values poultry litter biochars exhibited the greatest ec values with the greatest ash contents turkey litter biochars had the greatest biochar mass recoveries whereas feedlot biochars demonstrated the lowest published by elsevier ltd introduction utilization of livestock manures as feedstocks for thermochemical conversion technologies has advantages of high temperature elimination of pathogens drastically reducing waste stream volume extracting useful energy from a livestock operation and the production of value added products cantrell et al ro et al pyrolysis is the major anaerobic thermochemical conversion process leading to three product phases noncondensable gas condensable vapors liquids bio oil and tars and solid char or ash for targeting char production slow pyrolysis is a technique that uses temperature ranges of c slow heating rates of c and long residence times minutes hours spokas et al when this char product is applied to soils with the intent to sequester carbon and maintain or improve soil fertility it is commonly termed a biochar lehmann and joseph spokas et al biochars have been generated from a range of agricultural and organic materials spokas et al feedstock characteristics and pyrolysis conditions e g maximum exposure temperature affect the biochars physical and chemical characteristics antal and grønli gaskin et al novak et al singh et al plant derived biochars e g woody biomass and grasses often have a low nutrient content this low nutrient content is in part corresponding author tel 5203x113 fax 6970 e mail address keri cantrell ars usda gov k b cantrell 8524 see front matter published by elsevier ltd doi 1016 j biortech 11 due to pyrolytic nitrogen losses sheth and bagchi and the low initial ash content of the feedstock as well as ash composition often high in and cao that are of lower nutrient values bourke et al with low nitrogen phosphorous and potassium contents these plant based biochars are disadvantageous when compared to traditional fertilizers for supplying crop nutrients conversely manures are nutrient rich materials it correspondingly follows that pyrolysis of manures would produce a nutrientrich biochar while there are an increasing number of papers reporting on the physical and chemical characteristics of biochar for use as soil amendments e g novak et al spokas et al and soil contaminant remediation e g cao and harris uchimiya et al literature is limited on biochar characteristics derived from the wide selection of livestock animal manures and their structural diversity due to differences in pyrolytic temperature reported results have predominately pertained to poultry broiler litter biochar e g gaskin et al novak et al singh et al uchimiya et al other elemental and structural characterizations have been reported for swine and dairy cow manure based biochars e g cantrell and martin cao and harris singh et al even though these studies have shown existence of both elemental and structural dissimilarities the biochars were not produced under similar pyrolytic conditions as such there is a possibility that biochar property differences may be due to an array of production conditions to more fully clarify biochar properties between different manure k b cantrell et al bioresource technology 428 feedstocks under more controlled conditions we present selected physicochemical and thermodynamical characteristics of manure based biochar ﬁve varieties created under two temperature regimes material just prior to pyrolysis as pyrolyzed ranged between and 49 materials triplicate pyrolytic runs were performed at two temperatures and c for each run between and kg of prepared material was loaded onto a stainless steel tray and placed into a lindburg electric box furnace equipped with a gas tight retort model lindburg mph riverside mi the control scheme of this system was modiﬁed to that of a stochastic state space regulator in this scheme temperature control was based on four input temperatures allowing for accurate control of pyrolytic exposure temperature details of this system and the control system can be found elsewhere cantrell and martin samples were pyrolyzed under the following temperature schedule min equilibration hold at c ramp to desired pyrolytic temperature within min c min for c runs c min for c runs min equilibration hold at desired temperature 25 c min cool down to c during the c hold the retort was purged using an industrial grade gas ﬂow at l min the ﬂow for the remaining operation was set to l min equivalent to and 04 retort chamber exchanges per min respectively to maintain anoxic conditions after charring the samples remained in an inert atmosphere but were allowed to cool to room temperature for subsequent removal from the retort they were then homogeneously subsampled for analyzes biomass dairy manure dairy manure samples md were obtained from a head milking facility in harford county md the dairy manure was collected within the milking parlor in a holding area this parlor was a double parabone that milked 12 cows at a time the samples were collected across three milking replications cow rations consisted of a mixture of corn silage alfalfa hay as silage corn grain and a protein concentrate the as received moisture content was determined to be wt feedlot manure feedlot manure fl was obtained from a commercial deepbedded facility in sioux county ia that was equipped with concrete ﬂoors in pens and barns within these facilities shredded corn stalks were used as bedding material and the pens were typically cleaned and re bedded one two times per week the removed manure and bedding material was either stockpiled temporarily or applied directly to cropland the as received moisture was determined to be wt poultry litter poultry litter pl was collected from the top 7 cm depth along two transects longitudinally at locations in a commercial poultry house in orangeburg county sc this facility used soft wood shavings as bedding material at the time of sampling the house was empty of birds but the bedding received excrement from between the and ﬂocks that were raised for weeks ﬂocks per year thus the bedding was approximately 12 months old the litter had an as received moisture of 7 wt separated swine solids separated swine solids sw were obtained from a polyacrylamide pam polymer injected solid liquid separation system as part of a larger three process waste treatment system on a head ﬁnishing swine operation in sampson county nc the pam separation system was separating a homogenized mixture of one week old ﬂushed swine manure and phosphate precipitate sludge stream the as received moisture content was determined to be wt turkey litter turkey litter tl used for this experiment was collected from a bird house in lancaster county sc at the time of sampling the house was on its second consecutive ﬂock ﬂocks per year the house was cleaned once a year with soft wood shavings serving as the bedding material approximately l of litter was collected along the center transect of the house the as received moisture content was calculated to be 26 preparation upon receipt the md and fl samples underwent initial convection drying at c the sw samples underwent solar drying in a greenhouse all initially dried md fl and sw samples along with the pl and tl were ground using a wiley mill equipped with a mm screen sample grinding of all feedstocks was followed by overnight oven drying at c the moisture content of ground pyrolysis system and process feedstock and biochar analyzes physical chemical and energetic properties both feedstock and biochar subsamples underwent the following analyzes ph electrical conductivity ec surface area ultimate proximate energy content and plant relevant minerals and metals results were reported for triplicate feedstock subsamples and an average value for the triplicate pyrolytic runs the ph and ec of the feedstock manures and biochars were measured in a w v suspension in deionized water prepared by shaking at rpm for h the bet brunauer emmett teller surface areas were measured via adsorption multilayer theory using a nova surface area analyzer quantachrome boynton beach fl the analysis was performed by hazen research inc golden co following astm d chns standard method astm the oxygen contents o for the feedstock and biochar subsamples were directly measured by dry combustion using a perkin elmer series ii chns o analyzer perkin elmer shelton ct these results were used to calculate atomic h c o c and o n c ratios to evaluate relationships between pyrolysis temperature and the relative degree of aromaticity h c ratio and polarity o c and o n c ratios of each biochar chen et al for the proximate analysis the ash content at c was determined by hazen research inc the volatile matter vm was determined using a thermogravimetric analyzer tga mettler toledo international inc columbus oh following a recommended method in cantrell and martin and ﬁxed carbon content was determined following astm d as the difference between and the additive of vm and ash astm the energy content or higher heating value hhv was determined using an isoperibol calorimeter leco corp st joseph mi following astm d standard method and corrected for n and s content before conversion to a dry basis as well as a dry ash free basis astm mineral and metal analyzes were performed for al as cd ca cr cu fe k mg mn mo na ni p pb and zn these analyzes were performed by the agricultural service laboratory at clemson university clemson k b cantrell et al bioresource technology 428 sc using wet acid digestion conc and quantiﬁed using inductive coupled plasma atomic emission spectroscopy icp aes this laboratory also analyzed samples for n and n and soluble p concentrations using standard manure methods atr ftir analysis further structural analysis was performed using fourier transform infrared spectroscopy ftir using a bruker vertex spectrometer bruker optics billerica ma this instrument was ﬁtted with a miracle attenuated total reﬂectance atr accessory pike technologies madison wi and with a diamond crystal plate the spectra were obtained at cm resolution from to cm using a combined scans all feedstock and biochar samples were analyzed without pretreatments the ftir spectral peak assignments were interpreted based on characteristic vibrations for wood and grass biochars keiluweit et al dairy manure caldero n et al dairy manure biochars cao and harris bio oil obtained from fast pyrolysis of manures das et al xiu et al natural organic matter wen et al and bacterial surfaces jiang et al recovery yields four recovery yields were determined for each pyrolytic run and were expressed on a c ash biochar weight basis and also a biochar energy basis biochar recovery rbc was the percentage ratio of biochar mass to feedstock mass mb mf on a dry ash free basis the biochar recovery rbcdaf was dependent on the feedstock and biochar ash contents af and ab eq demirbasß the c recovery rc was calculated as the product of rbc and ratio of biochar c content to feedstock c content the ash and biochar energy recoveries were calculated similarly rbcdaf af ab af af statistical analyzes when required data were analyzed by proc glimmix general linear mixed model in version of statistical analysis system sas institute inc cary nc signiﬁcant differences between treatments were based on pyrolytic temperature feedstock type or their interactions using an f test p value 05 any tested correlations were identiﬁed using proc corr and p values less than 05 were used for the pearson s correlation test results and discussion physical characteristics our results conform to well known progressions antal and grønli bourke et al keiluweit et al where more vm was removed with increasing temperature resulting in a stable ﬁxed carbon tables and comparing the raw feedstocks and biochars within a pyrolysis temperature treatment the vm contents had some differences and some similarities tables and the md feedstock with the greatest vm generated biochars with the greatest vm content on the other hand sw feedstock produced a c biochar with the lowest vm content table the other feedstocks and biochars were considered to be statistically similar compared to their feedstocks there were one to three fold increases in the biochar ash content all ash contents were signiﬁcantly different with values ranging from to wt db among the c biochars and wt db for the c biochars tables and similarly the ﬁxed carbon content increased times compared to the feedstock tables and all manure based biochars produced at c had statistically similar ﬁxed carbon contents with an average of wt db except for sw tables and the sw c biochar had the lowest ﬁxed carbon value of 7 wt db at c pyrolysis of tl feedstock produced biochar with the lowest concentration of ﬁxed carbon 8 wt db this was statistically similar to sw c biochar fc at 33 8 wt db tables and in comparison this was lower than the other manure based biochars with an average value of 8 wt db from a carbon sequestration perspective sw was the least suited among the manures to preserve carbon at low pyrolytic temperatures along with tl at high pyrolytic temperatures pyrolyzing all of the raw feedstocks increased their ph and produced alkaline biochars tables and the heightened ph was related to pyrolysis temperature with the c biochars being more alkaline than the c biochars contrastingly the increase in alkalinity with temperature was not observed for cow manure biochars at and c they maintained a ph range of 8 singh et al this is different from the current study where both bovine varieties had the greatest ph among the c manure chars both md and fl ph continued to increase at c in excess of the sw biochars were consistently lower in ph than the other manure biochars at 8 and 9 respectively for and c with alkaline ph values as high as for the pl and fl c biochars these manure based biochar additions to soils have been reported to have negative consequences on the soil chemistry of low buffer capacity sandy soils novak et al as such their impact could be mediated through overall soil application rates it was also appropriate that the ec values of the biochars should be characterized to avoid creating unwanted salt effects in soils especially at high biochar application rates 30 tones ha lehmann and joseph biochar ec values varied from to ls cm table biochars produced from pl had the greatest ec among all examined varieties this was not surprising considering that pl is typically high in ec inﬂuencing elements from incomplete assimilation of nutrients by poultry consequently pl ec values exhibited a continual increase associated with higher pyrolysis temperatures this was likely due to the loss of volatile material resulting in concentration of elements in the ash fraction in contrast the ec values for sw biochars were significantly lower than their raw feedstock an average of compared to ls cm the lower ec value may be attributed to the pam addition during the solid liquid separation process although the dosage of the cationic ﬂocculent was very low in aqueous phase the polymer fraction can concentrate in dried sample its subsequent pyrolytic degradation in the solid phase may be inhibiting the dissociation of ionic compounds the nondissociative phases may be originated by binding inorganics with oxides or other anions formation of sulﬁdes or chelations with the ﬁxed carbon forms after pyrolysis of pam sewage sludge an increased association of cu and zn was observed with crystalline metal oxides residual fraction organic matter and with sulﬁdes oxidizable fraction there was a decreased association in exchangeable fractions that will contribute to the dissolved free hydroxide and carbonate species he et al the pam additive was also observed to have an interactive affect in the thermal degradation of swine manure in their comparative thermogravimetric analyzes of ﬂushed and solid separated swine manure samples ro et al for the current study these hypotheses require further experimental study correlation analyzes were conducted on the relationship between ec and ash contents concentrations of k na and k na for the biochars by pyrolysis temperature irrespective of sorting the biochars by pyrolysis temperature there was an extremely low correlation between 005 and 13 between ash and ec this implied that some elements in the ash probably occurred as insoluble oxides or hydroxides that were not capable of conducting electricity in contrast there was a close relationship between the ec values table when regressed against the concentrations of k na table and k na in fact the best predictor for biochar ec values was k na combined producing value of 84 the bet surface area was found to increase with increasing temperature table the most dramatic increases in bet surface area for a pyrolytic temperature of c were for the bovine varieties the dairy md and paved feedlot fl ranging from to m2 g the pl and tl biochars were in a middle grouping and lastly swine manure c biochar exhibiting the lowest surface k b cantrell et al bioresource technology 2012 428 area of 11 m2 g the low surface area for the sw biochars may be attributed to the degradation behavior of the pam considering potential applications the wide range of bet among the manure biochars would potentially result in varied adsorption responses of contaminants from soils uchimiya et al and may affect nutrient plant availabilities of co applied fertilizers spokas et al when compared to the raw feedstock increasing the pyrolytic temperature slightly increased the hhv of manure biochars when processed at c table a further pyrolytic temperature increase caused signiﬁcant decreases in the biochar energy content tables and initially pyrolysis increased the energy content likely due to loses of lighter low energy density compounds along with changes in the feedstock s carbon structure combined these collectively produced a more energy dense product further increases in temperature naturally cause additional breakdown of the carbon structure dairy md biochars were statistically the most energy dense which correlated to the higher carbon and lowest ash contents nonetheless the manure biochars in general may be unsuited for fuel use primarily due to the high ash contents of note are pl and tl biochars consistently exhibiting hhv values on the lower end correlated to their high ash contents recoveries biochar recoveries were found to positively correlate to the feedstock ash content and negatively correlate to the feedstock vm c and n contents p value 0 05 for pearson s correlation test with pyrolysis primarily degrading a feedstock s vm the more available vm then the greater the mass losses accordingly this leads to a lower biochar recovery likewise high initial ash concentrations resulted in greater biochar recovery the sw feedstock with the lowest vm and greatest ash contents generated the greatest biochar recovery of 62 wt db at c tables and despite the initial feedstock having the greatest vm and lowest ash contents the md c biochar recovery results were within the range to the other manures tables and different distributions were noted among the c biochars fl was the lowest wt db md pl and sw were similar with an average value of 0 wt db and tl generated the greatest biochar recovery 9 wt db recoveries of lignocellulosic biochars like wheat wood and olive husk across similar temperatures were reported to range from daf to daf demirbasß lang et al in our study pyrolysis of manure feedstocks generated 5 10 more biochar product ranging 8 daf table our higher results were likely attributed to the increased ash content of the manure feedstocks interfering with the carbon reactions as with biochar mass recoveries carbon and energy recoveries table both decreased with pyrolysis temperature increases turkey litter tl was the best feedstock to preferentially protect carbon from thermal decomposition it exhibited the greatest carbon recoveries at both and c at 8 and 2 wt db respectively greater carbon recoveries were noted for the manure based biochars when compared to pyrolyzed lignocellulosic feedstocks lang et al therefore pyrolyzing manure feedstocks resulted in lesser amounts of volatile carbon released compared to pyrolyzing grasses and woods keiluweit et al this suggested that manures had a greater propensity to retain carbon when pyrolyzed this may be due to protective mechanisms brought on by the various inherent metals thereby changing the bond dissociation energies of inorganic and organic carbon bonds in support it has been shown that treating lignocellulosic biomass with inorganic salt solutions increased char production credited to alterations of reaction pathways white et al similarly lower char yields for washed rice hulls vs unwashed were attributed to the loss of hydrocarbon moieties capable of promoting cross linking reactions that foster char production teng and wei higher levels of k and zn were documented to contribute to increased char yield for rice husk coir pith and groundnut shell raveendran et al the amount of energy remaining in the biochar useful for energy balances of engineered systems varied by feedstock type and pyrolysis temperature table 2 poultry litter pl and sw retained the most energy in the biochar at c as temperature increased to c md and tl biochars retained the greatest amount of energy conversely sw biochar retained the least energy at c ash recoveries for both of the tested pyrolysis temperatures ranged from 7 to 7 table 2 while the ash recovery would be a measure of the reproducibility and reliability of the pyrolysis procedures and system less than ash recovery was expected for the higher temperature pyrolytic runs this was because of factors such as vaporization of p containing compounds known to occur when temperatures approach c knicker in fact mass recovery of p in the biochar ranged from to among all manure biochars likewise heavy metals accumulated in plants used in phytoremediation e g birch and sunﬂower have been reported to be lost from the solid phase during fast pyrolysis of these plants lievens et al with manure feedstocks containing elevated ash contents compared to woods and grasses one would expect increased amounts of metal transferred from the solid phase to the gas and liquid pyrolytic products elemental characteristics pyrolysis at c resulted in a signiﬁcant increase in c table the raw feedstocks average c content was 2 9 wt db and increased to 52 2 2 5 wt db additional increases in the pyrolytic temperature however slightly diminished the c content of the biochars to an average 8 5 5 wt db dairy md and fl biochars statistically had greater c contents among the different temperature biochars tables and for reason well known antal and grønli the changes in c content occurred concurrently with the h and o losses more than of h on a mass basis was removed from the original feedstocks when pyrolyzing at c furthermore pyrolyzing at c resulted in total h losses mass basis exceeding interestingly there was variation in the h content among the c biochars with sw having the greatest tables and 3 the additional increase in pyrolytic temperature caused all manure biochars to have statistically similar h contents an average 10 0 wt db total n content in the biochars increased initially with pyrolysis at c there was one exception with sw tables and 3 initial n content ranged from 2 29 0 03 wt db md to 11 0 78 wt db sw the n content increased for the c md pl and tl biochars with an average of 8 tables and 3 the greatest increase was observed for the fl c biochar this may be related to recalcitrant n occurring in heterocyclic compounds kazi et al on the other hand the sw biochars exhibited a decrease in n concentration however the mechanism for this observation is not well understood all n concentrations decreased for the c pyrolytic temperature to values less than the initial feedstock table 3 there were statistical variations among n contents across the biochars md biochars consistently had the lowest n content whereas sw c was similar to fl on the lower concentrations but statistically had the greatest n concentration among the c biochars on a mass basis n mass losses for the c biochars ranged between for fl and 4 for sw as temperature increased to c these losses increased to an average 5 relative to the feedstocks nitrogen losses may have been due to emission of ammonia and other volatile organic compounds containing n during pyrolysis as well as novak et al ro et al sheth and bagchi where all molar ratios demonstrated a decreasing trend with increasing pyrolysis temperature these trends were attributed to removal of polar surface functional groups and 2 instances of higher degree of carbonization resulting in formation of more aromatic structures which are more recalcitrant oc structures chen et al keiluweit et al lehmann and joseph uchimiya et al following that logic the higher temperature biochars compared to their lower temperature counterparts would be less polar and have greater aromaticity leading to a more hydrophobic character of note are the trends for md fl and tl these ratios continually overlapped suggesting similar behaviors fig among the c biochars pl with the greatest h c ratio would be expected to have the least aromaticity among the c biochars sw with a lower o c ratio would be expected to be the least polar 3 4 nutrient characteristics pyrolysis concentrated mineral and heavy metals with some notable exceptions for cd and pb tables 4 and 5 within a pyro lysis temperature the order of concentration of an individual component e g greatest to lowest remained constant when compared to the order of the concentrations in the raw feedstocks using p as an example the concentrations from greatest to least for the raw feedstocks were sw tl pl fl md a similar ranking occurred within the and c biochars consequently there was no correlation between initial feedstock nutrient concentrations and the increase in concentration thus under the pyrolysis conditions employed in this study the lack of correlation implied that initial feedstock nutrient composition did not affect pyrolysis reactions equally and had no signiﬁcant effect on ﬁnal biochar compositions as such initial concentrations could not be used to quantitatively predict ﬁnal biochar nutrient contents by being rich in minerals important for plant growth like p manure based biochars may be better suited as an alternative fertilizer the p content of all manure c biochars increased from the feedstock concentrations by 50 80 for the c biochars compared to feedstocks p content increased 202 in spite of these increases further studies are required to determine their plant p availability status this is especially true when water table 5 mean heavy metal analyzes of manure feedstocks pyrolysis temperature 0 and biochars feedstock soluble p continually decreased to values 0 5 of raw feedstock soluble p values table 4 the sw feedstock had the second greatest contribution of water soluble p to total p at 5 tl feedstock was at 6 this was largely attributed to the solid liquid separation system separating both ﬂushed material and a phosphate precipitated sludge the sw biochars had less than contribution of soluble p to total p which was the lowest among all the biochars this is interesting and suggests some investigations are required to fully understand the impact of pyrolyzed pam on pyrolysis mechanisms and biochar characteristics the concentrations for elements regulated under c f r e g as cd cu mo ni zn were lower than listed ceiling concentrations table 5 epa the one exception was for tl biochars the as concentration in the raw feedstock was already above acceptable concentrations thus pyrolyzing turkey litter tl could worsen the situation regardless annual loading rates should be monitored if there are intentions of long term repeated soil applications of manure biochars in some heavy metal cases the biochar concentrations decreased with increases in temperature e g cd for all but tl biochars and pb for sw and tl biochars this was not an unusual occurrence because pyrolysis of heavy metal contaminated plants used in phytoremediation revealed that cd can be lost in gas and oil phases at c up to at c lievens et al in contrast pb zn and cu did not exhibit losses to these same phases however pb vaporization has been reported during incineration of municipal waste at temperatures as low as c asthana et al as such the heavy metal concentration would need to be monitored for the other product phases of pyrolysis fortunately with respect to heavy metal concentrations in the biochar phase the low concentrations implied that the biochars generated at both temperatures except as for the tl biochars would have minimal impact on increasing soil heavy metal concentrations in a singular short term application however suitability of other biochars as a soil amendment would depend on feedstock selection and initial nutrient concentrations 3 5 ftir characteristics the ftir spectra are presented in supplementary material supplementary fig c for raw manure feedstocks and biochars produced at and c descriptions for peak assignments are provided in table 6 for the feedstocks the broad band near cm 1 was attributed to the stretching vibration of hydrogen bonded hydroxyl groups keiluweit et al the asymmetric cm 1 and symmetric cm 1 c h stretching bands were associated with aliphatic functional groups relative to the other manures investigated sharper c h stretching bands for the sw feedstock suggested a more aliphatic nature the c o stretching for carboxyl aldehyde ketone and ester were identiﬁed table 6 pyrolysis temperature effects on relative change in ftir wave number identiﬁcation of manure feedstocks pyrolysis temp 0 and biochars feedstocks md dairy fl paved feedlot pl poultry litter sw separated swine solids and tl turkey litter wave number the rapid increase of concentration in the atmosphere combined with depleted supplies of fossil fuels has led to an increased commercial interest in renewable fuels due to their high biomass productivity rapid lipid accumulation and ability to survive in saline water microalgae have been identiﬁed as promising feedstocks for industrial scale production of carbon neutral biodiesel this study examines the principles involved in lipid extraction from microalgal cells a crucial downstream processing step in the production of microalgal biodiesel we analyze the different technological options currently available for laboratoryscale microalgal lipid extraction with a primary focus on the prospect of organic solvent and supercritical ﬂuid extraction the study also provides an assessment of recent breakthroughs in this rapidly developing ﬁeld and reports on the suitability of microalgal lipid compositions for biodiesel conversion elsevier inc all rights reserved contents introduction microalgal lipid composition overview of downstream processes lipid extraction organic solvent extraction basic principles solubility parameters selection of organic solvents operating variables modiﬁcations to organic solvent extraction supercritical ﬂuid extraction basic principles operating variables comparison between organic solvent extraction and extraction effect of cellular pre treatment on lipid extraction simultaneous extraction and transesteriﬁcation of microalgal lipids microalgal bioreﬁnery originoil single step extraction of microalgal lipids conclusions acknowledgments references introduction the search for sustainable and renewable fuels is becoming increasingly important as a direct result of climate change and rising fossil fuel prices current commercial production of biodiesel or fatty acid methyl ester fame involves alkaline catalyzed transesteriﬁcation of triglycerides found in oleaginous food crops with methanol however cultivation of these food crops for biodiesel mainly rapeseed in europe and soybean in the us is no longer sustainable as it requires substantial arable land and consumes large amounts of freshwater chisti microalgae are currently considered to be one of the most romising alternative sources for biodiesel sheehan et al since many microalgal strains can be cultivated on non arable land in a saline water medium their mass farming does not place additional strains on food production widjaja et al their high photosynthetic rates often ascribed to their simplistic unicellular structures enable microalgae not only to serve as an effective carbon sequestration platform but also to rapidly accumulate lipids in their biomass up to of dry cell mass even using a conservative scenario microalgae are still predicted to produce about times more biodiesel per unit area of land than a typical terrestrial oleaginous crop chisti rosenberg et al sheehan et al shenk et al there are however various technological and economic obstacles which have to be overcome before industrial scale production of microalgal biodiesel can take place the selection and successful outdoor large scale cultivation of a robust microalgal strain which has optimum neutral lipid content possesses an elevated growth rate and is immune towards invasion by local microbes remain a major upstream challenge sheehan et al on the other hand the development of an effective and energetically efﬁcient lipid extraction process from the microalgal cells is critical for the successful upscaling of the downstream processes despite the routine use of laboratory scale extraction protocols to determine microalgal lipid contents the variables affecting lipid extraction from microalgal cells are not well understood and no method for industrial scale extraction is currently established halim et al this paper attempts to address the knowledge gap surrounding microalgal lipid extraction by summarizing and critiquing recent studies in the ﬁeld we report on the suitability of microalgal lipid compositions for biodiesel conversion and review the different conventional downstream bioprocessing steps required for microalgal biodiesel production we then examine the technologies currently available for laboratory scale microalgal lipid extraction paying special attention to the use of organic solvent extraction and supercritical ﬂuid extraction we conclude with an assessment on how different cellular pre treatment processes can effect microalgal lipid extraction as well as with an update on the recent advances in the ﬁeld such as the development of a simultaneous microalgal lipid extractionmethylation method and the establishment of a novel single step microalgal lipid extraction method by originoil inc microalgal lipid composition a fatty acid fa molecule consists of a hydrophilic carboxylate group attached to one end of a hydrophobic hydrocarbon chain fig fatty acids are constituents of lipid molecules both neutral and polar and designated based on their two most important features the total number of carbon atoms in the hydrocarbon chain the number of double bonds along the hydrocarbon chain saturated fatty acids have no double bond while unsaturated fatty fig a fatty acid chains saturated fatty acid or stearic acid on the left unsaturated fatty acid or oleic acid on the right oleic acid is of cis isomerism b lipid molecules triacylglycerol neutral lipid on the left phospholipid polar lipid on the right r r r in the triacylglycerol molecule represent fatty acid chains phospholipid molecule is negatively charged modiﬁed from nelson and cox r halim et al biotechnology advances acids consist of at least one double bond nelson and cox when the carboxylate end of the fatty acid molecule is bonded to an uncharged head group e g glycerol a neutral lipid molecule is formed e g triacylglycerol on the other hand the association of a fatty acid molecule to a charged head group e g glycerol and phosphate complex forms a polar lipid molecule e g phospholipid lipids can be deﬁned as any biological molecule which is soluble in an organic solvent as mentioned above most lipids contain fatty acids fig and can generally be classiﬁed into two categories based on the polarity of the molecular head group kates neutral lipids which comprise acylglycerols and free fatty acids ffa and polar lipids which can be further sub categorized into phospholipids pl and glycolipids gl neutral lipids are used primarily in the microalgal cells as energy storage while polar lipids pack in parallel to form bilayer cell membranes acylglycerol consists of fatty acids ester bonded to a glycerol backbone and is categorized according to its number of fatty acids triacylglycerols tg diacylglycerols dg monoacylglycerols mg ffa is a fatty acid bonded to a hydrogen atom it is noted that there are also some types of neutral lipids that do not contain fatty acids such as hydrocarbons hc sterols st ketones k pigments carotenes and chlorophylls even though these lipid fractions are soluble in organic solvents hence ﬁtting the deﬁnition of lipids they are not convertible to biodiesel readers are referred to other sources for a more comprehensive description regarding lipids their varieties and their molecular structures kates mathews and van holde volkman et al the term oil is often used to refer to any lipid fraction that exists as a liquid at ambient conditions since lipids especially those obtained from microalgae are extracted as composite mixtures consisting of various fractions they do not always exist as liquids as such the term oil is not used in any part of this study microalgal lipid content varies considerably from one species to another and could range in terms of dry biomass from to wt brown et al chisti brown et al studied the nutritional properties of different australian microalgal species and concluded that they comprise as a weight fraction of dry cell mass between and lipids microalgal lipid composition also varies considerably from one species to another brown et al during their study investigating the lipid compositions of various microalgal species lv et al demonstrated that some microalgal species are richer in neutral lipids than other species the composition and fatty acid proﬁle of lipids extracted from a particular species is further affected by the microalgal life cycle as well as the cultivation conditions such as medium composition temperature illumination intensity ratio of light dark cycle and aeration rate guzman et al ota et al ramadan et al rao et al microalgal cells harvested during the stationary phase have lower polar lipid contents than the same species obtained during the logarithmic phase dunstan et al some microalgal species have been known to increase their lipid contents from wt to almost wt during oxygen deprivation dunstan et al microalgal cells generally respond to nutrient starvation by intensifying the metabolic pathway which synthesizes neutral lipids however this increase in cellular lipid production usually does not result in an overall increase in oil productivity per unit mass as it is often performed by sacriﬁcing growth and through the cessation of cell division due to the aforementioned inter and intraspeciﬁc variations the suitability of microalgal lipids for biodiesel production is difﬁcult to assess and often needs to be examined on a case by case basis acylglycerols are desirable for commercial scale biodiesel production for two main reasons firstly industrial scale alkaline catalyzed transesteriﬁcation is designed to process acylglycerols tg dg and mg and has limited efﬁcacies on other lipid fractions such as polar lipids and free fatty acids christie lang et al secondly since acylglycerols generally have a lower degree of unsaturation than other lipid fractions i e polar lipids they produce fame with higher oxidation stability as shown by the lipid proﬁles of three microalgal species nannochloropsis oculata pavlova lutheri isochrysis sp in fig microalgal lipids usually comprise high levels of polar lipids and non acylglycerol neutral lipids hc st k ffa as such they often need to be puriﬁed before they can be transesteriﬁed when comparing lipids obtained from logarithmic and stationary growth phase stationary phased lipids despite having an abundance of polar lipids at wt contain higher levels of tg wt of total lipid and appear more attractive for biodiesel processing than logarithmic phased lipids dunstan et al microalgal fatty acids range from to carbons in length and can be either saturated or unsaturated the number of double bonds in the fatty acid chains however never exceeds and almost all of the unsaturated fatty acids are cis isomers medina et al the fatty acid proﬁle of lipid extracted from tetraselmis suecica during early stationary phase is shown in fig volkman et al t suecica is a common green microalga and its fatty acid proﬁle is used here as an example to illustrate the suitability of microalgal lipids for biodiesel synthesis having and as its the principal fatty acids tetraselmis lipid appears to have the required fatty acid proﬁle for conversion to high quality biodiesel saturated neutral lipids breakdown neutral lipids breakdown neutral lipids breakdown fig compositions of crude lipids extracted from three microalgal species during logarithmic phase and stationary phase top nannochloropsis oculata middle pavlova lutheri bottom isochrysis sp for neutral lipids tg triacylglycerols hc hydrocarbons st sterols k ketones ffa free fatty acids modiﬁed from dunstan et al fig fatty acid composition of crude lipid extracted from the species tetraselmis suecica at the end of logarithmic phase the beginning of stationary phase a in terms of fatty acid chain b in terms of number of double bonds in the fatty acid chain in a the letter t after the fatty acid name denotes trans isomerism when no letter t appears fatty acid is of cis isomerism in b the word trans after the number of double bonds denotes that fatty acids are of trans isomerism when no isomerism is mentioned fatty acid is of cis conﬁguration modiﬁed from volkman et al fatty acid content wt is relatively low when compared to the total cis unsaturated content wt this is desirable as fame derived from cis unsaturated fatty acids often has advantageous cold ﬂow properties a low cloud point and a low pour point in contrast to saturated chains which pack rapidly upon temperature decrease to form tight semicrystalline structures cis unsaturated fatty acids are prevented from forming regular molecular packing due to the bends imposed by the cis double bonds and consequentially freeze at a much lower temperature lang et al mathews and van holde the extracted lipid contains a relatively modest amount of polyunsaturated fatty acids pufa with or more double bonds being the most abundant at wt this is desirable as highly unsaturated pufas are known to be responsible for the poor volatility the low oxidation stability and the tendency for gum formation observed in some oilseed derived biodiesel lang et al in terms of lipid classes it is noted that acylglycerols generally have a lower degree of unsaturation than polar lipids and as such are more suited for biodiesel conversion overview of downstream processes fig shows the downstream processing steps required to produce biodiesel from microalgal biomass halim et al table lists the different laboratory scale technological options currently available for each step the table also examines the scale up potential of each technology after the microalgal culture is harvested from the bioreactor it is concentrated in a dewatering step the concentrated microalgal culture is then processed in a pre treatment step to prepare it for lipid extraction during lipid extraction lipids are extracted out of the cellular matrices with an extraction solvent the lipids are then separated from the cellular debris isolated from the extraction solvent and any residual water and ﬁnally converted to biodiesel in the transesteriﬁcation step each of the downstream processes is explained in more detail below cultivation of microalgae is performed in either an indoor or an outdoor system chisti indoor cultivation systems normally use photobioreactors while outdoor systems employ either raceway ponds or photobioreactors in an outdoor system the microalgae are grown in the open environment where cultivation parameters temperature and light intensity are dependant on day to day weather conditions the microalgae grown in such a system often suffer from inconsistent growth rates and are more susceptible to local species invasion on the other hand the microalgae grown in an indoor system are placed in a greenhouse type structure where cultivation conditions can be tightly controlled despite providing better protection against local species invasion the indoor system is not preferred due to its high operating cost chisti throughout its cultivation microalgal culture needs to be aerated with supply and replenished with growth medium consisting of essential elements such as nitrogen phosphorous and iron chisti harvested microalgal culture exists as a dilute aqueous suspension from to g dried microalgal biomass l culture depending on cultivation method and needs to be concentrated in order to reduce the cost of downstream processing danquah et al molina grima et al solid liquid separation methods such as centrifugation ﬁltration and ﬂocculation are used to dewater the microalgal culture to a concentration between and g dried microalgal biomass l culture concentrated microalgal culture is referred to as concentrate when dewatered beyond g dried microalgal biomass l culture the concentrate is transformed to a sludge suspension and is often referred to as paste or pellet developing a cost viable and an energy efﬁcient dewatering technology is currently an active ﬁeld of research among the myriads of dewatering technologies table ﬂocculation appears to be the most advantageous due to its low energy requirement uduman et al wijffels and barbosa during ﬂocculation microalgal cells adhere to one another to form heavy aggregates which then settle to become concentrate cationic anionic and non ionic polyelectrolytes or polymer are typically used to ﬂocculate microalgal cells more details on microalgal dewatering and ﬂocculation can be found elsewhere uduman et al post dewatering the concentrate undergoes pre treatment process aimed at enhancing the efﬁciency of subsequent lipid extraction lee et al as shown in fig the pre treatment process can take different pathways depending on the desired biomass alterations the pre treatment can be performed in either a single step or multiple steps sometimes no pre treatment is performed and the concentrate is directly processed for lipid extraction in one option of the pre treatment pathways the concentrate is exposed to a cell disruption method such as high pressure homogenization which ruptures the cellular structures this pre treatment forces the release of intracellular lipids to the surrounding medium thus assisting the lipid extraction process microalgal concentrate that has been subjected to cell disruption process is referred to as disrupted concentrate in a typical step laboratory scale pre treatment pathway the concentrate is completely dried and then milled into ﬁne powders this pre treatment eliminates residual water known to prohibit effective mass transfer during the lipid extraction step and results in the formation of dried powder detailed discussion on the types and effects of pre treatment are deferred to section during lipid extraction the pre treated microalgal biomass existing as one of the following physical states concentrate or disrupted concentrate or dried powder is exposed to an eluting extraction solvent which extracts the lipids out of the cellular matrices concentrate or disrupted concentrate still contains a certain level of residual water while dried powder will be completely devoid of residual water the principles and processes involved in lipid extraction are discussed in detail in the following section section r halim et al biotechnology advances fig process ﬂow diagram showing the downstream processing steps needed to produce biodiesel from microalgal biomass microalgal lipid extraction generally uses either organic solvent or supercritical ﬂuid such as supercritical carbon dioxide as an extraction solvent after lipid extraction the resulting mixture consisting of extraction solvent residual water only when extraction is performed on concentrate or disrupted concentrate lipids and cell debris is submitted to a solid liquid separation method such as ﬁltration to remove the cell debris for organic solvent extraction a liquid liquid separation method such as distillation vacuum evaporation or solidphase solvent adsorption is then employed to remove the extraction r halim et al biotechnology advances table different laboratory scale technologies available for each downstream processing step required to produce biodiesel from microalgae the scale up potential of each technology is examined highly scalable lack scalability process step cultivation dewatering pre treatment cell disruption pre treatment drying pre treatment particulate size reduction lipid extraction removal of cell debris from extraction solventresidual water lipids mixture removal of extraction solvent and residual water from lipids this step is not applicable to supercritical fluid extraction lipid fractionation transesterification technologies scale up potential raceway ponds photobioreactors agglomeration centrifugation filtration flocculation pressure dewatering ultrasonication high pressure homogenization french pressing bead miling microwave chemical lysis acids enzymes osmotic shock oven drying freeze drying spray drying milling with specific sieve crushing with pestle and mortar organic solvent extraction supercritical fluid extraction organic solvent extraction with soxhlet apparatus ultrasound assisted organic solvent extraction microwave assisted organic solvent extraction accelerated organic solvent extraction sub critical organic solvent extraction filtration centrifugation not applicable to supercritical fluid extraction distillation vacuum evaporation solid phase solvent adsorption liquid chromatography silicic acid column chromatography acid precipitation urea crystallization acid catalyst alkali catalyst solvent and the residual water from the lipids when non polar polar organic solvent mixture is used refer to section residual water is removed from the extraction solvent and the lipids via biphasic separation and decantation the liquid liquid separation then removes the extraction solvent from the lipids on the other hand for supercritical ﬂuid extraction pressure decompression returns the extraction solvent as well as the residual water to their gaseous states and results in forced precipitation of the lipids refer to section as such no extra step is needed for the removal of extraction solvent and residual water the isolated lipids referred to as crude lipids or total lipids can now be gravimetrically quantiﬁed the term total lipids is primarily used for analytical purposes as previously mentioned in section in addition to acylglycerols crude lipids obtained from microalgal biomass frequently contain polar lipids and non acylglycerol neutral lipids such as free fatty acids hydrocarbons sterols ketones carotenes and chlorophylls from the perspective of biodiesel production any non acylglycerol biochemical fraction is a contaminant and will have to be removed from the crude lipids as such crude lipids arising from microalgal biomass are often subjected to a fractionation step before they are transesteriﬁed different puriﬁcation methods such as liquid chromatography acid precipitation and urea crystallizations are used for lipid fractionation medina et al during transesteriﬁcation the fatty acid containing lipid fractions in the crude lipids are reacted with alcohol methanol ethanol isopropanol butanol and converted to fatty acid alkyl esters when methanol is used the reaction produces fatty acid methyl ester fame or biodiesel either an acid such as or an alkali such as naoh or koh can be used as a catalyst for transesteriﬁcation christie volkman et al since alkali catalysts have faster reaction rates estimated at faster and higher conversions than acid catalysts for the transesteriﬁcation of acylglycerols they are commercially used in the chemical industry for conversion of plant and animal oils to biodiesel huang et al as previously noted in section alkaline catalyzed transesteriﬁcation has limited efﬁcacies when applied to non acylglycerol fatty acid containing lipid fractions such as polar lipids and free fatty acids during alkaline transesteriﬁcation of acylglycerols the catalyst cleaves the ester bonds holding the fatty acids to the glycerol backbone fig chisti the liberated fatty acids are then reacted with methanol to form fame in lab scale experiments where only small amounts of crude microalgal lipids are available a large amount of methanol substantially in excess of stoichiometric requirement is often added to ensure quantitative transesteriﬁcation once transesteriﬁcation is completed the reaction mixture containing biodiesel glycerol reformed alkali catalyst excess methanol and un transesteriﬁed lipids then undergoes posttransesteriﬁcation puriﬁcation to remove by product contaminants glycerol alkali catalyst and excess methanol a laboratory scale post transesteriﬁcation puriﬁcation method typically consists of steps the reaction mixture is left to settle under gravity to induce biphasic partitioning top biodiesel un transesteriﬁed lipids phase and bottom glycerol phase once the biodiesel un transesteriﬁed lipids phase is decanted off it is washed repeatedly with water to eliminate any alkali catalyst and excess methanol chisti demirbas demirbas and karslioglu more details on alkaline transesteriﬁcation of acylglycerols and post transesteriﬁcation puriﬁcation can be found elsewhere lang et al wahlen et al analyses of the fame composition of the puriﬁed biodiesel untransesteriﬁed lipids phase are carried out using a gas chromatography gc system variables that affect fame conversion during alkaline transesteriﬁcation include the molar ratio of acylglycerol to methanol the molar ratio of acylglycerol to catalyst the reaction temperature the reaction time the ffa content of the crude lipids and the water content of the r halim et al biotechnology advances triacylglycerol methanol fame biodiesel glycerol diacylglycerol methanol fame biodiesel glycerol monoacylglycerol methanol free fatty acid potassium hydroxide monoacylglycerol water fame biodiesel soap free fatty acid glycerol water glycerol fig various reactions involving the alkali catalyst koh or potassium hydroxide and illustrate the alkaline transesteriﬁcation of acylglycerol with methanol to produce biodiesel as a main product and glycerol as a by product illustrates the undesirable reaction between free fatty acid and koh to form soap and water saponiﬁcation illustrates the undesirable reaction between acylglycerol monacylglycerol is used as a representation and water under an alkaline condition to from free fatty acid and glycerol modiﬁed from chisti and huang et al crude lipids as illustrated in fig ffa reacts with the alkali catalyst to form soap and water saponiﬁcation as such if the crude lipids to be reacted have a high ffa content excess alkali catalyst must always be added in order to compensate for the saponiﬁcation loss huang et al past works on alkaline transesteriﬁcation of vegetable oil have shown that abundant presence of water in the crude lipids had an adverse effect on the reaction kinetics christie lang et al as depicted in fig water under alkaline conditions irreversibly reacts with acylglycerol to from free fatty acid the formation of which as mentioned above consumes the alkali catalyst for its elimination in a study by lepage and roy fame recoveries during the transesteriﬁcation of tg standards were found to substantially decrease once water content of the standards exceeded wt in order for microalgal biodiesel to be environmentally sustainable the total emitted in the downstream processing steps must be lower than or at least equal to the total originally captured by the microalgal cells during their cultivation therefore processes selected in each step should aim at minimizing energy consumption lipid extraction depending on its pre treatment pathway microalgal biomass to be submitted to lipid extraction can assume one of the following physical states concentrate or disrupted concentrate or dried powder during lipid extraction the microalgal biomass is exposed to an eluting extraction solvent which extracts the lipids out of the cellular matrices once the crude lipids are separated from the cell debris the extraction solvent and water only when extraction is performed on concentrate or disrupted concentrate their mass can be measured gravimetrically ideally a lipid extraction technology for microalgal biodiesel production needs to display a high level of speciﬁcity r halim et al biotechnology advances towards lipids in order to minimize the co extraction of non lipid contaminants such as protein and carbohydrates to reduce downstream fractionation puriﬁcation the lipid extraction technology should also be more selective towards acylglycerols than other lipid fractions that are not as readily convertible to biodiesel i e polar lipids and non acylglycerol neutral lipids free fatty acids hydrocarbons sterols ketones carotenes and chlorophylls medina et al additionally the selected technology should be efﬁcient both in terms of time and energy non reactive with the lipids relatively cheap both in terms of capital cost and operating cost and safe kates since dewatering the microalgal biomass beyond a paste consistency g dried microalgal biomass l culture is energy intensive it will be economically beneﬁcial if the selected lipid extraction technology is effective when directly applied to wet feedstock i e concentrate or disrupted concentrate with concentrations between and g dried microalgal biomass l culture halim et al in this section we will review the use of organic solvent extraction for routine lab scale determination of microalgal lipid contents we will also review the application of supercritical ﬂuid extraction an emerging green technology that is currently gaining considerable research attention for microalgal lipid quantiﬁcation organic solvent extraction basic principles the principles underlying organic solvent extraction of microalgal lipids are anchored on the basic chemistry concept of like dissolving like due to the interactions between their long hydrophobic fatty acid chains neutral lipids participate in weak van der waals attractions between one another and form globules in the cytoplasm kates medina et al the proposed mechanism for organic solvent extraction is shown in fig and can be divided into steps when a microalgal cell is exposed to a non polar organic solvent such as hexane or chloroform the organic solvent penetrates through the cell membrane into the cytoplasm step and interacts with the neutral lipids using similar van der waals forces step to form an organic solvent lipids complex step this organic solvent lipids complex driven by a concentration gradient diffuses across the cell membrane step and the static organic solvent ﬁlm surrounding the cell step into the bulk organic solvent as a result the neutral lipids are extracted out of the cells and remain dissolved in the non polar organic solvent a static organic solvent ﬁlm is formed due to the interaction between organic solvent and cell wall this ﬁlm surrounds the microalgal cell and remains undisturbed by any solvent ﬂow or agitation some neutral lipids are however found in the cytoplasm as a complex with polar lipids this complex is strongly linked via hydrogen bonds to proteins in the cell membrane the van der waals interactions formed between non polar organic solvent and neutral lipids in the complex are inadequate to disrupt these membrane based lipid protein associations on the other hand polar organic solvent such as methanol or isopropanol is able to disrupt the lipid protein associations by forming hydrogen bonds with the polar lipids in the complex kates medina et al the mechanism in which the non polar polar organic solvent mixture extracts membrane associated lipid complexes is also proposed in lower half of fig and can be divided into steps the organic solvent both non polar and polar penetrates through the cell membrane into the cytoplasm step and interacts with the lipid complex step during this interaction the non polar organic solvent surrounds the lipid complex and forms van der waals associations with the neutral lipids in the complex while the polar organic solvent also surrounds the lipid complex and forms hydrogen bonds with the polar lipids in the complex the hydrogen bonds are strong enough to displace the lipid protein associations binding the lipid complex to the cell membrane an organic solvent lipids complex is formed and dissociates away from the cell membrane step the static organic solvent film bulk organic solvent cell membrane and cell wall cytoplasm nucleus fig schematic diagram of the proposed organic solvent extraction mechanisms pathway shown at the top of the cell mechanism for non polar organic solvent pathway shown at the bottom of the cell mechanism for non polar polar organic solvent mixture lipids non polar organic solvent polar organic solvent both mechanisms can be described in steps step penetration of organic solvent through the cell membrane step interaction of organic solvent with the lipids step formation of organic solvent lipids complex step diffusion of organic solvent lipids complex across the cell membrane step diffusion of organic solvent lipids complex across the static organic solvent ﬁlm into the bulk organic solvent r halim et al biotechnology advances hexane system g lipid g dried microalgal biomass ﬁnal total lipid yield of hexane isopropanol system v v g lipid g dried microalgal biomass when a non polar polar organic solvent mixture such as hexane isopropanol or chloroform methanol is used both solvents are added simultaneously to the microalgal biomass existing as one of the following physical states concentrate or disrupted concentrate or dried powder in the desired volumetric ratio once cell debris is removed using a solid liquid separation method such as ﬁltration biphasic separation of the initially single phase organic solvent mixture is induced by roughly equivolume addition of the non polar organic solvent hexane for hexane isopropanol mixture and chloroform for chloroform methanol mixture and water upon complete biphasic separation neutral and polar lipids will mainly partition in organic solvent lipids complex then diffuses across the cell membrane step and the static organic solvent ﬁlm surrounding the cell step into the bulk organic solvent as such the addition of a polar organic solvent to a non polar organic solvent facilitates the extraction of membrane associated neutral lipid complexes however the process also inevitably leads to the co extraction of polar lipids in most laboratory practices both non polar organic solvent and polar organic solvent are added to the microalgal cells to ensure the complete extraction of all neutral lipids both in the form of freestanding globules and in the form of membrane associated complexes during our previous study investigating lipid extraction from chlorococcum sp halim et al the inclusion of isopropanol as a co solvent was shown to improve the total lipid yield of pure hexane system by more than ﬁnal total lipid yield of pure a b c concentrate microalgal culture dried e hexane ipa v v powder f d cell debris hexane ipa methanol h g koh crude lipids biodiesel hexane ipa glycerol un transesterified hot plate stirrer lipids fig schematic diagram showing the experimental steps typically undertaken for laboratory scale production of microalgal biodiesel using an organic solvent mixture as a lipid extraction technology hexane isopropanol ipa v v mixture is used for lipid extraction cell disruption lipid fractionation and post transesteriﬁcation puriﬁcation are not performed a cultivation with photobioreactors b dewatering with a centrifuge c drying pre treatment with an oven d particulate size reduction pre treatment with a pestle and a mortar e lipid extraction with hexane isopropanol mixture f removal of cell debris with a ﬁlter g removal of extraction solvent with a distillation unit h transesteriﬁcation with an alkali catalyst r halim et al biotechnology advances the organic phase a mixture of non polar organic solvent and polar organic solvent while the aqueous phase a mixture of water and polar organic solvent will contain primarily non lipid contaminants proteins and carbohydrates kates medina et al as such biphasic separation removes not only residual water but also non lipid contaminants from the mixture of organic solvents and lipids the organic phase is decanted and evaporated to yield dry crude lipids which are then fractionated and transesteriﬁed fig shows the experimental steps typically undertaken for laboratory scale production of microalgal biodiesel using an organic solvent mixture as a lipid extraction technology table summarizes the methods and the ﬁndings of recent studies which investigated organic solvent extraction of microalgal lipids solubility parameters among the myriad of thermodynamic parameters polarity index kauri butanol value and hildebrand solubility parameter attempting to predict the solubility of a solute in a solvent hansen solubility parameters hsp appears to be one of the more widely accepted and promising systems gupta et al hansen snyder hsp characterization predicts that a solute will dissolve in a solvent if the molecules of either substance have similar force of interaction with hsp system total energy of cohesion e can be quantitatively divided into three components hansen these components account for the atomic dispersion or van der waals interactions ed the molecular dipolar interactions ep and the molecular hydrogen bonding electron exchange interactions eh e ed þ ep þ eh e is experimentally determined by measuring the energy required to evaporate the liquid solute or solvent thus breaking all of its cohesive bonds dividing eq by the molar volume v yields the three components of hansen solubility parameters e v ðed vþ þ ðep vþ þ ðeh vþ where e v δ ed v δd ep v δp and eh v δh δ ðδd þ þ ðδp þ þ ðδh þ where δ is the hildebrand total solubility parameter δd is the hansen dispersion parameter δp is the hansen dipolar parameter and δh is the hansen hydrogen bond parameter the si unit for all of the parameters is mpa hsp characterization can be conveniently visualized with a spherical representation hansen solubility parameters of the solute are at the center of the solubility sphere and the radius of the solubility sphere ro indicates the extent of interaction for solubilization to occur good solvents lie within the solubility sphere and poor ones lie outside ra is the distance of the solvent from the center of the solubility sphere ra δdp þ þ ðδps δpp þ þ ðδhs δhp þ subscript is for the solvent and subscript p is for the solute if ra b ro the solvent lies within the solubility sphere and the solute is soluble in the solvent relative energy difference red is numerical representation of this graphical visualization the smaller its red value the better the solvent is at dissolving the solute red ra ro for a solvent mixture composite hansen solubility parameter δi mix is calculated with δi mix δi þ δi þ subscript i signiﬁes one of the three components of hansen solubility parameters φ is volume fraction of each solvent in the mixture we have computed the hsp characterizations for organic solvents and organic solvent mixtures commonly employed to extract lipids from microalgal biomass in table all of the hansen solubility parameters of pure organic solvents were obtained from hansen eq was used to calculate composite hansen solubility parameters for organic solvent mixtures triacetin was used as a representative for all types of triacylglycerols ra was calculated with triacetin as a solute eq ro value of mpa indicates marginal solubility of a solute in a solvent and was assigned for red estimation eq as can be seen from table triacetin appears to be highly soluble in hexane isopropanol mixture v v red and chloroform red on the other hand chloroform methanol water mixture v v v appears to be poor solvents for triacetin red such a ﬁnding is contradictory to previous lipid extraction works which recommend the use of chloroform methanol water mixture molina grima et al we note that the current triacetinbased hsp characterizations must be treated with caution triacylglycerols with higher carbon number will have different hansen solubility parameters to triacetin additionally limited availability of hansen solubility parameters for other desirable neutral lipid derivatives such as diacylglycerols monoacylglycerols and neutral lipids polar lipids complexes prevents the complete characterization of microalgal lipids more research is needed before the mechanisms underlying organic solvent extraction of microalgal lipids as outlined in section can be fully explained with hsp characterization selection of organic solvents in addition to satisfying the previously mentioned criteria for an ideal lipid extraction technology the selected organic solvents should table hsp characterizations for organic solvents and organic solvent mixtures commonly employed to extract lipids from microalgal biomass ra was calculated with triacetin as a solute hansen solubility parameters for triacetin δd δp δh organic solvent or organic solvent mixture hansen dispersion parameter or δd hansen dipolar parameter or δp hansen hydrogen bond parameter or δh afﬁnity of solvent with triacetin or ra relative energy difference or red solubility of triacetin in the solvent chloroform methanol water chloroform methanol v v chloroform methanol water v v v hexane isopropanol hexane isopropanol v v ethanol r halim et al biotechnology advances preferably be volatile for low energy distillation from the crude lipids kates medina et al chloroform methanol v v is the most frequently used organic solvent mixture for lipid extraction from any living tissue using this organic solvent system residual endogenous water in the microalgal cells acts as a ternary component that enables the complete extraction of both neutral and polar lipids it is noted that this method does not require the complete drying of microalgal biomass once the cell debris is removed more chloroform and water are added to induce biphasic partitioning the lower organic phase chloroform with some methanol contains most of the lipids both neutral and polar while the upper aqueous phase water with some methanol constitutes most of the non lipids proteins and carbohydrates medina et al extraction using chloroform methanol v v is fast and quantitative chloroform however is highly toxic and its usage is undesirable the method was originally developed by folch et al for the isolation of total lipids from brain tissues for this reason its efﬁcacy in extracting lipids from microalgal biomass still needs further assessment in a study by lee et al the performance of ﬁve different organic solvent mixtures in extracting lipids from bead beaten botryococcus braunii cells was compared as can be seen in table chloroform methanol obtained the highest ﬁnal total lipid yield at g g dried microalgal biomass on the other hand dichloroethane based organic solvent mixtures dichloroethane methanol and dichloroethane ethanol previously recommended for lipid extraction from the green algae cladofora were found to have limited efﬁcacies when applied to b braunii hexane isopropanol v v mixture has been suggested as a low toxicity substitute to chloroform methanol system halim et al the mixture works in a similar fashion with chloroform methanol system upon biphasic separation the upper organic phase hexane with some isopropanol contains most of the lipids both neutral and polar while the lower aqueous phase water with some isopropanol contains most of the non lipids proteins and carbohydrates when evaluated for microalgal lipid extraction hexane isopropanol mixture was found to be more selective towards neutral lipids compared to chloroform methanol system guckert et al lee et al nagle and lemke as previously mentioned segregation of neutral lipid class at the lipid extraction step is highly desirable as it would allow microalgal biodiesel production to occur with minimal downstream puriﬁcation guckert et al attributed the neutral lipid selectivity of hexane isopropanol mixture to its inability to extract the polar lipid constituents of microalgal membranes chloroplast membranes contain glycolipids and cell membranes contain phospholipids the hexane isopropanol system however yielded a surprisingly low total lipid recovery when applied to b braunii lee et al pure alcohol such as butanol isopropanol and ethanol is cheap volatile and has a strong afﬁnity to membrane associated lipid complex due to its ability to form hydrogen bonds however its polar nature is also a disadvantage as it limits interaction with freestanding neutral lipid globules for this reason when used as a microalgal lipid extraction solvent alcohol is almost always combined with a non polar organic solvent such as hexane or chloroform to ensure the total extraction of both forms of neutral lipids freestanding globules and as membrane associated complexes halim et al in their study nagle and lemke evaluated the efﬁciencies of three organic solvents butanol hexane propanol mixture ethanol in extracting crude lipids from chaetoceros muelleri and compared them to a control water methanol chloroform mixture table even though the control polar non polar mixture was veriﬁed to be the most effective organic solvent system assigned an arbitrary extraction efﬁciency of butanol with an average extraction efﬁciency of was found to be highly promising with a ﬁnal total lipid yield consistently higher than hexane propanol mixture or ethanol in all triplicates the authors argued that even though all of the organic solvents used were safe to handle the consistency of butanol yields from one replicate to another standard deviation of indicated a lower sensitivity to changes in the extraction procedure a beneﬁcial attribute if the process were to be scaled up due to its propensity to inactivate many phosphatidases and lipases kates recommended the use of isopropanolcontaining organic solvent mixture to extract lipids from unicellular algal species that produces lipid degradative enzymes operating variables the evolution of lipids during the organic solvent extraction on microalgal biomass is observed to follow a ﬁrst order kinetics equation halim et al harrison et al kt me ms o e where me is the amount of lipid extracted in the organic solvent at time t g lipid g dried microalgal biomass ms o is the amount of lipid originally present in the cells g lipid g dried microalgal biomass k is the lipid mass transfer coefﬁcient from the microalgal cells into the organic solvent min and t is the extraction time min the parameter k itself is a function of several operating variables and can be described generally as k f ðag b tþ where ag is the degree of agitation revolution per minute b is the ratio of organic solvent to dried microalgal biomass ml organic solvent g dried microalgal biomass and t is the extraction temperature c table provides a summary of the levels of operating variables used in various studies investigating organic solvent extraction of microalgal lipids fig fajardo et al shows typical extraction curves obtained when using an organic solvent to extract lipids from microalgal biomass these curves conform to the model of eq where the rate of lipid recovery is observed to decrease with extraction time during the hour extraction a majority of the lipids is recovered within the ﬁrst h of all extractable lipids and extending the extraction time beyond h does not seem to make any signiﬁcant contribution to the ﬁnal total lipid yield this kind of asymptotic behavior is attributed to the diffusion driven nature of lipid extraction where the rate of lipid evolution is controlled by the total lipid yield wt of dried microalgal biomass time h fig typical ﬁrst order extraction curves obtained for organic solvent extraction of microalgal lipids microalgae phaeodactylum tricornutum organic solvent ethanol ratio of organic solvent to dried microalgal biomass b was optimized x ml ethanol g dried microalgal biomass ml ethanol g dried microalgal biomass ml ethanol g dried microalgal biomass modiﬁed from fajardo et al r halim et al biotechnology advances lipid concentration gradient between the microalgal cells and the organic solvent extraction is most rapid in the beginning when concentration gradient is at its steepest as lipids are removed from the microalgal cells into the organic solvent the concentration gradient diminishes and lipid extraction slows down as can be seen in table every study indicates a different ratio of organic solvent to dried microalgal biomass b the appropriate b value for each microalgal strain varies depending on its lipid content and its intrinsic solvent cellular interaction during their investigation of lipid extraction from phaeodactylum tricornutum fajardo et al found extraction efﬁciency to increase with decreasing b ratio fig the ﬁnal total lipid yields obtained with and ml ethanol per gram of dried microalgal biomass were around wt with ml ethanol per gram of dried microalgal biomass the ﬁnal total lipid yield was roughly wt even though this claim was rather counter intuitive the authors ascribed the higher total lipid yield at lower organic solvent ratio to its higher agitation intensity per volume unit it is important to ﬁnd the optimum b value for a speciﬁc microalgal strain an b value that is too high results in excessive consumption of organic solvent while a value that is too low leads to handling difﬁculties and incomplete extraction variation in the extraction temperature has been reported to inﬂuence lipid yield increasing temperature from c to c was observed to enhance lipid extraction rate from animal tissues fajardo et al however a rise beyond c led to an oxidative degradation of thermo labile components which resulted in a lower lipid yield in a study by balasubramanian et al increasing temperature during lipid extraction from scenedesmus obliquus resulted in a signiﬁcant increase in the ﬁnal total lipid yield the authors attributed this increase to enhanced mass transfer kinetics at higher temperature the kinetics and the mechanism underlying organic solvent extraction of microalgal lipids are not yet well understood and require further research it is noted that organic solvent extraction has several disadvantages the method generally uses large amounts of toxic organic solvents is slow and requires energy intensive evaporation for solvent removal the extent of lipid extraction by a volume of water out condenser water in soxhlet extractor with a thimble to hold the microalgal biomass heating mantle round bottom flask containing the organic solvent fig the soxhlet apparatus modiﬁed from de castro and ayuso organic solvent is also thermodynamically restricted by the lipid mass transfer equilibrium wang and weller once lipid concentration between the bulk organic solvent and the cellular matrices has reached its partition equilibrium level no further transfer of lipids from the cells to the organic solvent will take place modiﬁcations to organic solvent extraction a majority of the laboratory scale organic solvent extractions reported in the literature have been performed as a batch process even though batch extraction is limited by lipid mass transfer equilibrium a continuous organic solvent extraction able to overcome this limitation requires a large amount of organic solvent and becomes too expensive through its ingenious cycles of solvent evaporation and condensation the soxhlet apparatus continuously replenishes cells with fresh organic solvent hence evading equilibrium limitation while simultaneously minimizing solvent consumption luque de castro and garcia ayuso wang and weller the apparatus is shown in fig and has compartments a continuously heated roundbottom ﬂask to store the extracting organic solvent the soxhlet extractor to hold the microalgal biomass existing as concentrate or disrupted concentrate or dried powder and the continuously cooled condenser organic solvent from the heated round bottom ﬂask enters the condenser and is immediately channeled into the soxhlet extractor the organic solvent comes in contact with the microalgal biomass and performs lipid extraction the thimble in the extractor prevents the microalgal biomass from being carried away by the organic solvent ﬂow and as such serves as a ﬁlter to remove cell debris once the organic solvent in the extractor reaches the overﬂow level a siphon unloads the organic solvent lipids mixture from the extractor back into the round bottom ﬂask the organic solvent is heated and evaporates again while the extracted crude lipids remain in the round bottom ﬂask this cycle is repeated until no more crude lipids are extracted in the soxhlet extractor despite its advantageous design in avoiding equilibrium limitation the soxhlet apparatus suffers from high energy requirement for continuous distillation luque de castro and garciaayuso wang and weller independent studies by guckert et al and halim et al veriﬁed the superior efﬁcacy of soxhlet extraction when compared to a batch extraction among the three systems experimented by guckert et al to extract lipids from chlorella sp soxhlet extraction using methylene chloride methanol v v mixture obtained the highest ﬁnal total lipid yield table in terms of the dry microalgal weight the ﬁnal total lipid recovered was approximately by soxhlet extraction using methylene chloride methanol by batch extraction using chloroform methanol mm phosphate buffer and by batch extraction using n hexane isopropanol distilled water halim et al found soxhlet operation of hexane extraction to be signiﬁcantly more efﬁcient than its batch counterpart when used to extract lipids from chlorococcum sp ﬁnal total lipid yield of batch extraction g lipid g dried microalgal biomass ﬁnal total lipid yield of soxhlet extraction g lipid g dried microalgal biomass despite its improved total lipid recovery soxhlet extraction potentially suffered from lipid degradation resulting from the use of elevated temperature throughout the process guckert et al noted that the crude lipids recovered using a soxhlet system contained less pufas then those obtained by batch extractions and ascribed this observation to potential thermal degradation due to the harshness of the soxhlet method a couple of modiﬁcations to organic solvent extraction have also been introduced microwave assisted organic solvent extraction and accelerated or subcritical organic solvent extraction each modiﬁcation utilizes an auxiliary process that enhances the kinetics of lipid extraction by the organic solvent through speedy disruption of the cellular structures luque de castro and garcia ayuso wang and weller modiﬁed organic solvent extraction synergistically r halim et al biotechnology advances combines cell disruption pre treatment to be reviewed in section and lipid extraction as a single step microwave assisted organic solvent extraction uses the aid of electromagnetic radiation within a speciﬁc frequency range to deliver large amount of thermal energy to the microalgal cells balasubramanian et al when the cells receive this energy local internal superheating occurs leading to instantaneous temperature rise within the matrices and rapid pressure effects on the cell wall membrane structure cell structures are immediately ruptured forcing cell constituents to spill out this effective expulsion of cell materials facilitates a more rapid diffusion of microalgal lipids into the extracting organic solvent microwave assisted heating is substantially more rapid than conventional heating as heat is delivered via radiation rather than convection and conduction balasubramanian et al examined the use of microwave assisted hexane extraction to recover lipids from s obliquus microwave assisted hexane extractions were found to result in higher oil yields compared to conventionally waterheated hexane extraction control methods at all extraction temperatures and times during accelerated or subcritical organic solvent extraction lipid extraction is performed at an elevated pressure and temperature in order to accelerate extraction kinetics and to disintegrate cellular structures subcritical organic solvent extraction has some of the beneﬁts of supercritical ﬂuid extraction described in section but is still performed below critical conditions in order to minimize operating cost herrero et al chen et al examined the use of subcritical ethanol to extract lipids from wet microalgal paste of nannochloropsis sp and found the extraction process to be highly efﬁcient maximum ﬁnal lipid recovery of total lipids neither of the modiﬁcations described above microwave assisted or subcritical organic solvent extraction has been applied to an industrial scale due to their high energy requirements it is also noted that there is currently limited understanding on the key variables affecting the performances of these modiﬁed extraction processes luque de castro and garcia ayuso wang and weller the scale up potentials of organic solvent extraction and its modiﬁcations are examined in table supercritical ﬂuid extraction supercritical ﬂuid extraction sfe is an emerging green technology that has the potential to replace traditional organic solvent extraction basic principles when the temperature and the pressure of a ﬂuid are raised over their critical values tc and pc the ﬂuid enters the supercritical region fig pourmortazavi and hajimirsadeghi reverchon and de marco taylor supercritical ﬂuid appears suitable to be used as an extraction solvent for lipid recovery from microalgal biomass due to the following reasons mendes et al taylor tunable solvent power fig p t phase diagram for carbon dioxide showing the supercritical region table physical properties of a typical ﬂuid in different states modiﬁed from taylor gas supercritical ﬂuid liquid density kg viscosity μpa diffusion coefﬁcient the solvent power of a supercritical ﬂuid is a function of its density which can be continuously adjusted by changing the extraction pressure and the extraction temperature as such solvent power of the ﬂuid can be tuned such that it interacts primarily with neutral lipids i e acylglycerols favorable mass transfer as shown in table supercritical ﬂuid displays physical properties intermediate to a liquid and a gas taylor these transitional properties allow for rapid penetration of the ﬂuid through cellular matrices thus resulting in a higher total lipid yield and a shorter extraction time production of solvent free crude lipids crude lipids obtained from supercritical ﬂuid extraction are free from extraction solvent therefore no energy is expended for extraction solvent removal supercritical carbon dioxide is the primary solvent used in the majority of supercritical ﬂuid extractions its moderate critical pressure atm allows for a modest compression cost while its low critical temperature c enables successful extraction of thermally sensitive lipid fractions without degradation facilitates a safe extraction due to its low toxicity low ﬂammability and lack of reactivity macias sanchez et al taylor if the microalgal cells are to be cultivated at a coal ﬁred power station the required for supercritical conversion can be conveniently obtained from the scrubbed ﬂue gas of the station this paper will focus on the use of for microalgal lipid extraction fig shows a lab scale supercritical carbon dioxide extraction unit used for the recovery of microalgal lipids applied separations a mixture of microalgal biomass existing as concentrate or disrupted concentrate or dried powder and packing materials normally diatomaceous earth or diatoms in a speciﬁc ratio is placed inside the extraction vessel equipped with a heating element a feed pump delivers from its source to the extraction vessel at a pressure greater than pc as soon as the vessel is heated t tc the compressed is converted to its supercritical state and performs lipid extraction on the microalgal biomass fig during the lipid extraction process the microalgal biomass and diatoms are packed tightly inside the cylindrical extraction vessel the supercritical carbon dioxide travels on the surface of the packed mixture and lipids are desorbed from the microalgal biomass immediately upon dissolution the encloses the lipids to form a lipids complex the complex driven by concentration gradient diffuses across the static ﬁlm and enters the bulk ﬂow the frits placed at both ends of the extraction vessel prevent the mixture of microalgal biomass and diatoms from being carried away by the ﬂow as such the frits serve as a ﬁlter to remove cell debris the lipids mixture as well as some residual water if lipid extraction was performed on concentrate or disrupted concentrate then leaves the extraction vessel to enter the collection vessel where a micrometering valve is used to rapidly depressurize the incoming ﬂuid fig upon complete depressurization the together with any residual water returns to gaseous state and the extracted crude lipids precipitate in the collection vessel as such r halim et al biotechnology advances fig schematic diagram of a laboratory scale extraction system the unit is used to extract lipids from microalgal biomass modiﬁed from applied separations sfe derived crude lipids are free from any extraction solvent and do not need to undergo an extraction solvent removal step even though extraction can be operated as either a batch static or a continuous dynamic process it is generally exercised as a continuous extraction as this often results in an improved yield taylor the process described above is based on a dynamic extraction the feasibility of applying process to extract microalgal lipids has been demonstrated andrich et al canela et al cheung halim et al herrero et al mendes et al mendiola et al sajilata et al thana et al operating variables operating variables which inﬂuence the performance of extraction of microalgal lipids include pressure temperature modiﬁer addition and ﬂuid ﬂow rate pourmortazavi and hajimirsadeghi table summarizes the levels of operating variables and the ﬁndings of recent studies investigating extraction of microalgal lipids fig schematic diagram of the proposed supercritical carbon dioxide extraction mechanism microalgal biomass and diatoms are packed tightly as a mixture inside the cylindrical extraction vessel supercritical carbon dioxide ﬂows on the surface of the packed mixture lipids static ﬁlm enclosing the packed mixture is formed due to the interaction between and microalgal biomass the mechanism can be described in steps step desorption of lipids from the microalgal biomass into the static ﬁlm step solubilization of the released lipids by step formation of a lipids complex step diffusion of the complex across the static ﬁlm into the bulk ﬂow the evolution of lipids during extraction on microalgal biomass can be described by the following ﬁrst order kinetics equation goto et al halim et al ozkal et al kt me ms o e where me is the amount of lipid extracted by the at time t g lipid g dried microalgal biomass ms o is the amount of lipid originally present in the microalgal cells g lipid g dried microalgal biomass k is the lipid mass transfer coefﬁcient from the microalgal cells into the eluting min and t is the extraction time min the parameter k itself is a function of several operating variables and can be described generally as k f ðp t q þ where p is the extraction pressure bar t is the extraction temperature c m is the concentration of polar modiﬁer mol of q is the ﬂow rate l min fig andrich et al shows typical extraction curves obtained when using to extract lipids from microalgal biomass these curves conform to the model proposed in eq where the rate of lipid recovery is observed to decrease with extraction time during their studies investigating lipid extraction from nannochloropsis sp andrich et al found majority of the total lipids to be extracted within continuing the extraction run beyond did not seem to dramatically increase the total lipid yield this kind of asymptotic behavior is ascribed to the diffusion driven nature of lipid extraction where the rate of lipid evolution is controlled by the lipid concentration gradient between the microalgal biomass and the in our previous study evaluating the feasibility of process to extract lipids from chlorococcum sp halim et al we found the ﬁrst kinetic model described by eq to accurately describe experimental data minimum r value for all extraction curves solvent power of during lipid extraction is a direct function of the extraction pressure and the extraction temperature taylor higher extraction pressure leads to a higher ﬂuid density and thus to an increase in solvent power however increasing extraction pressure also increases operating cost lowers selectivity and often results in the co extraction of unwanted cellular components temperature increase leads to two competing phenomena the decrease in ﬂuid density lowers solvent power while a simultaneous increase in lipid volatility enhances the lipid mass transfer into the bulk ﬂow cheung soares et al r halim et al biotechnology advances table methods and results summary of recent studies investigating supercritical carbon dioxide extraction of microalgal lipids special attention is given to the effect of pressure change and of temperature change on the total lipid yield optimum condition is deﬁned as the experimental condition that produces the highest ﬁnal total lipid yield study microalgal species ﬂow rate polar modiﬁer results and optimum condition extraction extraction temperature extraction quantity of pressure polar modiﬁer or p bar or t c duration min sajilata et al spirulina platensis andrich et al mendes et al final total lipid yield at the optimum condition wt of dried microalgal biomass l min ethanol ml total lipid yield increased with p optimum condition was found at bar min and ml ethanol nannochloropsis sp kg min none at constant t lipid extraction rate increased with p at constant p lipid extraction rate slightly increased with t final total lipid yield was the same at any t and p spirulina maxima not speciﬁed not speciﬁed ethanol at constant t total lipid yield increased with p mol of at constant p total lipid yield decreased with t at constant t and p polar modiﬁer addition signiﬁcantly increased total lipid yield optimum condition was found at bar c with ethanol addition mol cheung hypnea charoides l min none at constant t total lipid yield increased with p at low p bar total lipid yield decreased with t at medium to high p and bar total lipid yield increased with t optimum condition was found at bar and c l min none at constant t total lipid yield increased with p at low p bar total lipid yield decreased with t at high p bar total lipid yield increased with t optimum condition was found at bar and c mendes et al chlorella vulgaris taylor table compiles ﬁndings from previous studies on the effect of pressure change and of temperature change on lipid extraction from microalgal biomass because of its non polar nature is unable to interact with either polar lipids or neutral lipids that form complexes with polar lipids the addition of a polar modiﬁer often referred to as co solvent or entrainer enhances the ﬂuid afﬁnity towards polar lipids as well as lipid complexes that contain both neutral lipids and polar lipids it further facilitates complete lipid extraction by diminishing the ﬂuid viscosity and allowing the to rapidly permeate through the cellular matrices pourmortazavi and hajimirsadeghi taylor common polar modiﬁers include methanol ethanol toluene and methanol water mixture mendes et al demonstrated that the addition of methanol to signiﬁcantly increased the extraction of γ linolenic acid from wt of dried microalgal biomass to wt from spirulina maxima the ﬂow rate of through the extraction vessel affects lipid extraction kinetics even though increasing ﬂow rate enables fig typical ﬁrst order extraction curves obtained for extraction of microalgal lipids microalgae nannochloropsis sp all extractions were performed at a constant temperature c pressure was optimized mpa mpa mpa modiﬁed from andrich et al more effective contact between the extraction ﬂuid and the lipids it often results in uneven ﬂuid penetration and dead volumes within the extraction vessel pourmortazavi and hajimirsadeghi the density in which microalgal biomass is packed to form a ﬁxed bed within the extraction vessel plays an important role in inﬂuencing extraction efﬁciency pourmortazavi and hajimirsadeghi in the case of extraction from dried microalgal powder packing density is directly related to microalgal powder particulate size and the volumetric ratio of packing materials normally diatomaceous earth or diatoms to microalgal powder even though higher packing density increases the amount of lipids in the vessel it reduces the vessel porosity and can adversely affect the extraction kinetics via ﬂuid channeling effects pourmortazavi and hajimirsadeghi comparison between organic solvent extraction and extraction table provides a comparison between organic solvent extraction and extraction when they are used to extract lipids from microalgae despite having low reactivity with lipids and being effective when directly applied to a wet feedstock concentrate or disrupted concentrate organic solvent extraction is slow and uses large amounts of expensive toxic solvents it has a limited selectivity towards biodiesel desirable lipid fractions acylglycerols containing mainly cis unsaturated fatty acids with less than double bonds and requires energy intensive liquid liquid separation method such as distillation to remove the organic solvent from the lipids on the other hand extraction is rapid and non toxic it has high selectivity towards biodiesel desirable lipid fractions due to tuneable density and produces solvent free crude lipids it is also non reactive with the lipids it remains effective when applied to a wet feedstock concentrate or disrupted concentrate though high residual water content in the microalgal biomass tends to lead to ﬂow impedance and restrictor plugging high installation costs of the extraction pressure vessel as well as unfavorable energy requirements for the ﬂuid compression and heating remain the primary obstacles for scaling up extraction crespo and yusty table comparison between organic solvent extraction and extraction for microalgal lipid extraction good poor organic solvent extraction extraction selectivity is not easily tuned when non polar organic solvent is used only limited amount of neutral lipids can be extracted when non polar polar organic solvent mixture is used both neutral lipids and polar lipids are extracted tunable selectivity when combined with ﬂexible polar modiﬁer arrangement should enable speciﬁc extraction of acylglycerols and minimize co extraction of contaminants polar lipids and non acylglycerol neutral lipids total lipid yield due to its intermediate liquid gas properties can penetrate through cellular matrices rapidly and produces a higher total lipid yield extraction time lipid extraction rate is slow and lipid extraction requires a long time for completion due to its intermediate liquid gaseous properties can penetrate through cellular matrices rapidly as such lipid extraction rate is fast and lipid extraction can be completed within a short period energy requirement it consumes little energy as lipid extraction is conducted near ambient conditions however organic solvent needs to be removed from the lipids via energy intensive liquid liquid separation method such as distillation it is highly energy intensive as ﬂuid compression and heating are needed to convert to supercritical state however crude lipids are free from extraction solvent and do not need to undergo an extraction solvent removal installation and operating non energy related cost expensive organic solvent is needed not all of the organic solvents can be recycled the pressure vessel needed for extraction can be extremely expensive to install lipid extraction can be applied to microalgal concentrate without additional pre treatment step and with minimal loss of efﬁciency high residual water content within the microalgal biomass results in ﬂow impedance and restrictor plugging hazard and toxicity toxic organic solvents are used reactivity with lipids organic solvent is non reactive with lipids however distillation carried out to remove the organic solvent from the lipids exposes the lipids to high temperature and possible artifact formation is non reactive with lipids applicability to microalgal concentrate or wet feedstock r halim et al biotechnology advances criteria neutral lipids selectivity r halim et al biotechnology advances cloud point and a high pour point while biodiesel made from pufa tends to be volatile and has a low oxidation stability schematic diagrams for an industrial scale organic solvent extraction system and an industrial scale extraction system are proposed in fig for extraction compressor is used to pressurize the ﬂuid to a supercritical state the scale up potential of each lipid extraction technology is examined in table effect of cellular pre treatment on lipid extraction fig comparison between dynamic extraction and dynamic hexane extraction using a soxhlet apparatus a total lipid yield b fame composition of the crude lipids microalgae chlorococcum sp in a extraction hexane extraction using a soxhlet apparatus in b the letter t after the fatty acid name denotes trans isomerism when no letter t appears fatty acid is of cis isomerism for extraction mass of microalgal dried powder g dried powder diatomaceous earth w w t c p mpa for hexane extraction using a soxhlet apparatus mass of microalgal dried powder g total number of cycles or equilibrium establishments modiﬁed from halim et al halim et al for a more extensive evaluation than table a thorough understanding of mass transfer mechanisms and of kinetic parameters involved in lipid extraction is required in our previous study extracting lipids from dried chlorococcum sp halim et al we compared the performance of dynamic extraction with that of dynamic hexane extraction using a soxhlet apparatus as shown in fig extraction was found to be more efﬁcient than hexane extraction eighty minutes of extraction total lipid yield g lipid g dried microalgal biomass achieved a higher total lipid yield than h of soxhlet extraction total lipid yield g lipid g dried microalgal biomass this outcome was expected since supercritical ﬂuid has more favorable physicochemical properties and facilitates more rapid cellular permeation cheung mendes et al andrich et al reported similar results despite demonstrating that both extractions eventually obtained equivalent ﬁnal total lipid yields from nannochloropsis sp they measured a lower lipid mass transfer coefﬁcient for extraction with hexane than with in terms of fatty acid composition fig chlorococcum crude lipids extracted by comprised a substantially higher quantity of than the corresponding crude lipids extracted by hexane such selectivity was beneﬁcial since as a cis unsaturated fatty acid with less than double bonds is highly desirable for biodiesel production as previously mentioned section biodiesel derived from saturated fatty acids often has disadvantageous cold ﬂow properties a high the effects of cellular pre treatment on microalgal lipid extraction have not been investigated extensively as previously described the pre treatment process can take alternative pathways depending on the desired biomass alterations fig the process can be performed in a single step or multiple steps it is noted that most of the pre treatment steps such as thermal drying for complete water removal or high pressure homogenization for cell disruption are energy intensive and should only be carried out if they substantially enhance the efﬁciency of microalgal lipid extraction based on the combination of technologies available in table the pre treatment process can alter the following conditions of the microalgal biomass degree of cell disruption residual water content and in the case of dried microalgal powder particulate size the efﬁciency of microalgal lipid extraction is known to increase with the degree of cell disruption when intact cells are disintegrated during cell disruption intracellular lipids are liberated from the cellular structures and released into the surrounding medium chisti and moo young gouveia et al lee et al mendes pinto et al during subsequent lipid extraction the eluting extraction solvent can directly interact with these free lipids without penetrating into the cellular structures the lipid extraction process is thus no longer restricted by the transportation of extraction solvent and lipids across the cell membrane it is completed more rapidly and results in a higher lipid recovery it is noted that most cell disruption methods such as bead milling ultrasonication and high pressure homogenization require certain degree of water in the microalgal biomass for their successful operation for this reason cell disruption step in a pre treatment pathway is always performed before the drying step as illustrated in fig additionally most cell disruption methods will not be able to process microalgal concentrate with exceedingly low water contents i e microalgal paste or pellet laboratory scale cell disruption methods fig are classiﬁed based on the manner in which they achieve microalgal cellular disintegration mechanical or non mechanical chisti and moo young harrison et al mechanical methods include bead mill press high pressure homogenization ultrasonication autoclave lyophilization and microwave while non mechanical methods often involve lysing the microalgal cells with acids alkalis enzymes or osmotic shocks chisti and moo young bead mill high pressure homogenization and ultrasonication are three of the most widely used methods for laboratory scale microalgal cell disruption chisti and moo young harrison et al bead mill achieves cellular disruption by physically grinding the microalgal cells against the solid surfaces of glass beads in a violent agitation among the myriad of cell disruption methods bead mill appears most suitable for large scale application due to its low operating cost chisti and moo young high pressure homogenization pumps microalgal concentrate through narrow oriﬁce of a valve under high pressure it then releases the concentrate into a low pressure chamber cellular disintegration is thus achieved through high pressure impingement of accelerated cellular jet on the stationary valve surface as well as through pressure drop induced shear stress that the microalgal concentrate experiences as it passes from the valve to the chamber chisti and moo young ultrasonication disrupts microalgal cells via transmission of sonic r halim et al biotechnology advances fig proposed schematic diagrams of a an industrial scale organic solvent extraction system and b an industrial scale extraction system the systems are intended for lipid extraction from microalgal biomass waves these waves create a series of microbubble cavitations on the cell surface and eventually disintegrate the cell membrane wall chisti and moo young detailed working mechanisms of bead mill high pressure homogenization and ultrasonication can be found elsewhere chisti and moo young harrison et al lee et al assessed the effect of prior mechanical cell disruption on lipid extraction from the species b braunii they used chloroform methanol mixture v v as an extraction solvent and found completely disrupted microalgal cells to yield almost twice the amount of crude lipids of intact microalgal cells also among the different mechanical cell disruption methods investigated fig the classiﬁcation of laboratory scale cell disruption methods r halim et al biotechnology advances botryococcus sp final total lipid yield wt of dried microalgal biomass chlorella vulgaris scenedesmus sp ve wa o r ic g tin lav toc au d n no ing on pti u isr ea a b d be m k oc on ati ic on s c oti sh m os fig effect of prior cell disruption on total lipid yield of organic solvent extraction three microalgal species botryococcus sp chlorella vulgaris and scenedesmus sp were investigated organic solvents chloroform methanol v v mixture modiﬁed from lee et al final total lipid yield g lipid g dried microalgal biomass sonication homogenization high pressure french press bead beating or bead mill and lyophilization mechanical shearing with bead mill obtained the highest ﬁnal total lipid yield in a different study by lee et al the effect of prior cell disruption on lipid 06 04 02 a b organic solvent fig effect of residual water content within the microalgal biomass on total lipid yield of organic solvent extraction microalgae chlorococcum sp a hexane extraction of microalgal dried powder b hexane extraction of microalgal concentrate c hexane isopropanol v v extraction of microalgal dried powder organic phase aqueous phase d hexane isopropanol v v extraction of microalgal concentrate organic phase aqueous phase for a and c mass of dried powder g for b and d mass of concentrate g residual water content wt of concentrate for a b c d mass of dried microalgal biomass g volume of organic solvent mixture ml duration h modiﬁed from halim et al extraction from three microalgal species botryococcus sp chlorella vulgaris and scenedesmus sp was evaluated fig chloroform methanol v v mixture was used as an extraction solvent in all cases among the cell disruption methods assessed autoclave bead beating microwave sonication and osmotic shocks microwave obtained the highest ﬁnal total lipid yield and appeared to be the most efﬁcient for all three microalgal strains for botryococcus sp bead beating and microwave obtained the highest ﬁnal total lipid yields with respectively and g lipid g dried microalgal biomass while sonication seemed to be the least efﬁcient at g lipid g dried microalgal biomass for c vulgaris autoclave and microwave appeared to be the most efﬁcient whereas bead beating produced a low ﬁnal total lipid yield at g lipid g dried microalgal biomass with scenedesmus sp microwave was again found to show the highest extraction efﬁciency while yields from the other methods were similar for all three microalgal species prior disruption of the cells by any of the assessed method was found to improve ﬁnal total lipid yield during the lipid extraction step refer to fig and compare all of the methods with the control non disruption the mechanism in which residual water in the microalgal biomass affects lipid extraction is not well understood and warrants future investigation one hypothesis speculates that the presence of residual water in the microalgal biomass will adversely affect lipid extraction efﬁciency water forms a barrier that prohibits effective lipid mass transfer from the cells to the extraction solvent as such drying of microalgal concentrate is non optional and has to be performed prior to the lipid extraction on the other hand another hypothesis postulates that the presence of residual water in the microalgal biomass will improve lipid extraction efﬁciency water swells the cells and facilitates better solvent access to the lipids drying of microalgal concentrate prior to lipid extraction is deemed unnecessary and may hinder lipid mass transfer various microorganisms bacteria yeasts and viruses have been successfully extracted in their wet state wt water using non polar polar organic solvent mixtures kates medina et al with regards to extraction abundance of residual water in the microalgal biomass results in a highly compacted particle bed within the extraction vessel this often leads to ﬂow impedance and restrictor plugging pourmortazavi and hajimirsadeghi schwartzberg during their investigation of lipid extraction from chlorococcum sp halim et al assessed the effect of residual water content within the microalgal biomass on total lipid yield as shown in fig modiﬁed from halim et al the presence of residual water in the microalgal biomass did not appear to substantially affect total lipid yield hexane extraction of concentrate ﬁnal total lipid yield g lipid g dried microalgal biomass obtained a slightly lower lipid recovery than its dry powder counterpart ﬁnal total lipid yield g lipid g dried microalgal biomass while hexane isopropanol extraction of concentrate ﬁnal total lipid yield g lipid g dried microalgal biomass surprisingly obtained a higher total lipid yield than hexane isopropanol extraction of dried powder ﬁnal total lipid yield g lipid g dried microalgal biomass these ﬁndings were encouraging particularly since the organic solvent extraction of wet biomass did not require any additional pre treatment step among the drying technologies that can be applied to microalgal concentrate freeze drying is preferred for its mild operating conditions thermal drying though commonly used in laboratory practice is not recommended as it degrades thermolabile lipids results in evaporative loss of volatile lipids and yields powder with nonuniform particulate size pourmortazavi and hajimirsadeghi once dried microalgal biomass forms powder or agglomeration that can be milled into different particulate size reducing the particulate size of microalgal powder prior to lipid extraction generally enhances lipid recovery as it increases the interfacial surface area r halim et al biotechnology advances fig alternative process ﬂow diagram showing the downstream processing steps needed with a simultaneous extraction and transesteriﬁcation step to produce biodiesel from microalgae available for biomass solvent contacts and shortens the diffusion pathway of the extraction solvent however exceedingly small particulate size of the microalgal powder may lead to a higher tendency of lipid re adsorption ﬂuid channeling effects in the extraction vessel for extraction and inhomogeneous lipid extraction pourmortazavi and hajimirsadeghi sabio et al pourmortazavi and hajimirsadeghi conducted a study on extraction of oil from tomato skins and veriﬁed that smaller tomato skins resulted in higher oil recoveries simultaneous extraction and transesteriﬁcation of microalgal lipids recent studies investigating biodiesel production from microalgae have focused their efforts on the development of an alternative downstream processing step termed simultaneous extraction and transesteriﬁcation wahlen et al this step also known as direct transesteriﬁcation or in situ transesteriﬁcation combines lipid extraction and transesteriﬁcation in a single step thereby simplifying the downstream pathway required for biodiesel production from microalgal biomass fig the method involves the simultaneous addition of acid catalyst and pure methanol to microalgal biomass generally in the form of dried powder the methanol extracts the lipids from the microalgal biomass and catalyzed by the acid concurrently transesteriﬁes the extracted lipids to produce fatty acid methyl esters similar downstream processing steps as those required in the traditional pathway are then followed where the reaction mixture consisting of methanol biodiesel glycerol reformed acid catalyst un transesteriﬁed lipids and cell debris undergoes cell debris removal and post transesteriﬁcation puriﬁcation r halim et al biotechnology advances fig schematic diagram of a laboratory scale originoil single step extraction system extracted from originoil fig filtration is used for cell debris removal a laboratory scale post transesteriﬁcation puriﬁcation consists of multiple steps the reaction mixture is ﬁrst distilled to remove methanol it is then left to settle under gravity to induce biphasic partitioning top biodiesel un transesteriﬁed lipids phase and bottom glycerol phase the biodiesel un transesteriﬁed lipids phase is decanted off and washed repeatedly with water to eliminate any acid catalyst it is noted that studies investigating direct transesteriﬁcation of microalgal biomass have so far only used acid catalysts acetyl chloride and direct transesteriﬁcation was originally derived by lepage and roy as a rapid method to analyze the fatty acid contents of adipose tissues and milk the method is highly desirable as it reduces the amount of downstream manipulation required for biodiesel production from any given biomass compared to the conventional lipid extraction followed by transesteriﬁcation approach direct transesteriﬁcation has been shown to improve biodiesel yields of various animal and plant tissues however the efﬁcacy of direct transesteriﬁcation on microalgal biomass has not been sufﬁciently investigated parameters that affect the performance of direct transesteriﬁcation include the ratio of methanol to dried microalgal biomass ml methanol g dried microalgal biomass the reaction temperature c wahlen et al examined the effect of residual water content within the microalgal biomass on the kinetics of direct transesteriﬁcation chaetoceros gracilis biomass with residual water contents ranging from to of microalgal paste weight was subjected to direct transesteriﬁcation increasing residual water content within the microalgal biomass was found to progressively decrease fame yield for biomass with residual water content equal to of the microalgal paste weight fame yield was half that obtained from the direct transesteriﬁcation of dried powder biomass with residual water content microalgal bioreﬁnery the cost of producing microalgal biodiesel can theoretically be offset by revenues generated from other co products of the microalgal biomass microalgae contain signiﬁcant quantities of proteins and carbohydrates as well as smaller amounts of high value functional ingredients astaxanthin canthaxanthin carotenes chlorophylls free fatty acids and γ linolenic acid each of these cell components can be appropriately utilized to co generate a useable product in a bioreﬁnery recent studies have concluded that industrial scale production of microalgal biodiesel can only be made economically sustainable if a bioreﬁnery based production strategy is pursued wijffels et al in a bioreﬁnery the crude lipids are to be fractionated into high value functional ingredients and lipids for biodiesel acylglycerols functional ingredients have been linked with the promotion of anti oxidant anti inﬂammatory as well as anticarcinogenic activities in the human bodies and are typically used as food supplements if the microalgal species contains a high level of proteins the residual biomass from biodiesel production processes can be used as livestock feeds if the species has high carbohydrate contents the residual biomass can be fermented to produce bioethanol as such microalgal bioreﬁnery will simultaneously produce biodiesel high value products livestock feeds and bioethanol unfortunately the combination of technologies needed to implement a microalgal bioreﬁnery is still in the early stages of development milder cell disruption lipid extraction process needs to be explored to ensure that the functionalities of different cell components are retained originoil single step extraction of microalgal lipids originoil inc has established a novel method for microalgal lipid extraction originoil instead of following the traditional sequence based downstream processing pathway outlined in fig the method devised by originoil performs three simultaneous functions dewatering cell disruption and lipid extraction in a single downstream step fig this approach is referred to as originoil single step extraction within a single step originoil microalgal concentrate is exposed to quantum fracturing a patent pending technology based on the science of ﬂuid fracturing combined with pulsed r halim et al biotechnology advances electromagnetic ﬁelds and ph modiﬁcation fig microalgal cells are instantly disrupted and intracellular lipids are released from the microalgal biomass the microalgal lipids rise to the top of the gravity clariﬁer for skimming lipid fractionation and transesteriﬁcation to biodiesel while the cell debris settles to the bottom of gravity clariﬁer for further processing as fuel and other valuable by products the water or growth medium in the middle of the gravity clariﬁer can be decanted and recycled for microalgal cultivation there are three components to originoil single step extraction originoil injection lowers ph of the growth medium to optimize electromagnetic ﬁelds delivery and to assist in cell disruption quantum fracturing creates ﬂuid effect to mechanically stress microalgal cells pulsed electromagnetic ﬁelds deliver the force that disrupt the microalgal cells originoil single step extraction has several beneﬁts originoil by combining three functions dewatering cell disruption and lipid extraction in a single downstream step it substantially reduces the energy expenditure required to produce biodiesel from microalgal biomass the lipid extraction method does not use any toxic extraction solvent and thus does not require a solvent recovery step finally the lipid extraction method is highly effective when directly applied to wet feedstock concentrate conclusions the downstream technologies needed for industrial scale production of microalgal biodiesel are still in the early stages of development lipid extraction from microalgal biomass has not received sufﬁcient attention and represents one of the many bottlenecks hindering economic industrial scale production of microalgal biodiesel future research on microalgal biodiesel should focus on developing an effective and energetically efﬁcient lipid extraction process a fundamental understanding of lipid mass transfer mechanisms from the microalgal biomass to the extraction solvent is needed to scale up the lipid extraction process an ideal lipid extraction technology for microalgal biodiesel production needs to be not only lipid speciﬁc in order to minimize the co extraction of non lipid contaminants such as protein and carbohydrates but also selective towards acylglycerols in order to reduce downstream fractionation puriﬁcation fajardo et al medina et al additionally the selected technology should be efﬁcient both in terms of time and energy non reactive with the lipids relatively cheap both in terms of capital cost and operating cost and safe kates microalgal culture is harvested as a dilute aqueous suspension from to g dried microalgal biomass l culture and is substantially concentrated however dewatering the microalgal biomass beyond a paste consistency g dried microalgal biomass l culture is energetically prohibitive for this reason it will be economically beneﬁcial if the selected lipid extraction technology can be directly applied to relatively wet feedstock i e concentrate or disrupted concentrate with concentrations between and g dried microalgal biomass l culture halim et al studies in the past decade have demonstrated the use of organic solvents and the use of supercritical carbon dioxide to extract lipids from microalgal biomass however each technology has its merits and limitations despite having low reactivity with lipids and being directly applicable to relatively wet feedstock organic solvent extraction is slow and uses a large amount of expensive toxic solvents on the other hand supercritical carbon dioxide extraction is a promising green technology that can potentially be used for large scale microalgal lipid extraction it is rapid non toxic has high selectivity towards acylglycerols and produces solvent free lipids its main disadvantages are associated with the high capital cost and the high energy requirement for supercritical ﬂuid compression several studies have shown that the disruption of microalgal cells prior to the lipid extraction step enhances the extraction efﬁciency article history received october received in revised form december accepted january available online january keywords metal nanoparticles nanogold nanosilver plant extracts drug delivery nanoparticle biosynthesis green synthesis a b t r a c t biomolecules present in plant extracts can be used to reduce metal ions to nanoparticles in a single step green synthesis process this biogenic reduction of metal ion to base metal is quite rapid readily conducted at room temperature and pressure and easily scaled up synthesis mediated by plant extracts is environmentally benign the reducing agents involved include the various water soluble plant metabolites e g alkaloids phenolic compounds terpenoids and co enzymes silver ag and gold au nanoparticles have been the particular focus of plant based syntheses extracts of a diverse range of plant species have been successfully used in making nanoparticles in addition to plant extracts live plants can be used for the synthesis here we review the methods of making nanoparticles using plant extracts methods of particle characterization are reviewed and potential applications of the particles in medicine are discussed elsevier inc all rights reserved introduction this review is concerned with the synthesis of metallic nanoparticles using plant extracts the methods used in characterizing the nanoparticles are outlined and the emerging applications of nanoparticles in clinical diagnostics and therapy azzazy et al chen et al doria et al fortina et al larguinho and baptista sahoo et al salata seil and webster wagner et al youns et al zhang et al are discussed although nanoparticles can be made using various physicochemical methods cao sepeur their synthesis using nontoxic and environmentally benign biological methods is attractive specially corresponding author tel fax e mail address ucbanerjee niper ac in u c banerjee see front matter elsevier inc all rights reserved http dx doi org j biotechadv if they are intended for invasive applications in medicine several routes have been developed for biological or biogenic synthesis of nanoparticles from salts of the corresponding metals bar et al dhillon et al duran and seabra gan and li gericke and pinches korbekandi et al luangpipat et al mohanpuria et al mukherjee et al parsons et al ray et al shankar et al singaravelu et al thakkar et al microorganisms whole plants plant tissue and fruits plant extracts and marine algae luangpipat et al rajesh et al singaravelu et al have been used to produce nanoparticles biogenic synthesis is useful not only because of its reduced environmental impact anastas and zimmerman dahl et al shankar et al compared with some of the physicochemical production methods but also because it can be used to produce large quantities of nanoparticles that are free of contamination and have a a k mittal et al biotechnology advances 356 well deﬁned size and morphology hutchison biosynthetic routes can actually provide nanoparticles of a better deﬁned size and morphology than some of the physicochemical methods of production raveendran et al the ability of plant extracts to reduce metal ions has been known since the early although the nature of the reducing agents involved was not well understood in view of its simplicity the use of live plants or whole plant extract and plant tissue for reducing metal salts to nanoparticles has attracted considerable attention within the last years ankamwar armendariz et al beattie and haverkamp gan and li gardea torresdey et al gericke and pinches haverkamp and marshall iravani kandasamy et al kumar and yadav marshall et al park et al parsons et al compared with the use of whole plant extracts and plant tissue the use of plant extracts for making nanoparticles is simpler plant extract mediated synthesis is an increasing focus of attention ali et al ankamwar babu and prabu banerjee bankar et al bar et al baskaralingam et al castro et al chandran et al daisy and saipriya dubey et al kaler et al kesharwani et al lee et al park et al parsons et al singh et al song et al processes for making nanoparticles using plant extracts are readily scalable and may be less expensive iravani compared with the relatively expensive methods based on microbial processes dhillon et al li et al luangpipat et al sastry et al and whole plants armendariz et al beattie and haverkamp haverkamp and marshall kumar and yadav marshall et al plant extracts may act both as reducing agents and stabilizing agents in the synthesis of nanoparticles kumar and yadav the source of the plant extract is known to inﬂuence the characteristics of the nanoparticles kumar and yadav this is because different extracts contain different concentrations and combinations of organic reducing agents mukunthan and balaji typically a plant extract mediated bioreduction involves mixing the aqueous extract with an aqueous solution of the relevant metal salt the reaction occurs at room temperature and is generally complete within a few minutes in view of the number of different chemicals involved the bioreduction process is relatively complex nanoparticles are already used in numerous applications virkutyte and varma including in vitro diagnostics but their use in medicine is mostly on an experimental basis drugs bound to nanoparticles have been claimed to have advantages compared with the conventional forms of the drugs wagner et al nanoparticle bound drugs have an extended half life in vivo longer circulation times and can convey a high concentration of a potent drug to where it is needed sahoo et al the size of the drug nanoparticle and its surface characteristics can be modiﬁed to achieve the desired delivery characteristics mohanraj and chen as the nanoparticlebound drug is not able to circulate broadly its side effects are reduced and a high localized concentration can be achieved where it is needed panyam and labhasetwar in view of the large surface area per unit mass of nanoparticles the drug loading can be relatively high han et al nanoparticle bound drugs are easily suspended in liquids and are able to penetrate deep in organs and tissues characterization of nanoparticles nanoparticles are generally characterized by their size shape surface area and dispersity jiang et al a homogeneity of these properties is important in many applications the common techniques of characterizing nanoparticles are as follows uv visible spectrophotometry dynamic light scattering dls scanning electron microscopy sem transmission electron microscopy tem fourier transform infrared spectroscopy ftir powder x ray diffraction xrd and energy dispersive spectroscopy eds feldheim and foss sepeur shahverdi et al the uv visible spectroscopy is a commonly used techniques pal et al light wavelengths in the nm are generally used for characterizing various metal nanoparticles in the size range of to nm feldheim and foss spectrophotometric absorption measurements in the wavelength ranges of nm huang and yang and 550 nm shankar et al are used in characterizing the silver and gold nanoparticles respectively the dynamic light scattering dls is used to characterize the surface charge and the size distribution of the particles suspended in a liquid jiang et al electron microscopy is another commonly used method of characterization cao scanning electron microscopy and transmission electron microscopy are used for morphological characterization at the nanometer to micrometer scale schaffer et al the transmission electron microscopy has a fold higher resolution compared with the scanning electron microscopy eppler et al ftir spectroscopy is useful for characterizing the surface chemistry chithrani et al organic functional groups e g carbonyls hydroxyls attached to the surface of nanoparticles and the other surface chemical residues are detected using ftir xrd is used for the phase identiﬁcation and characterization of the crystal structure of the nanoparticles sun et al x rays penetrate into the nanomaterial and the resulting diffraction pattern is compared with standards to obtain structural information elemental composition of metal nanoparticles is commonly established using energy dispersive spectroscopy eds strasser et al synthesis of nanoparticles the methods for making nanoparticles can generally involve either a top down approach or a bottom up approach sepeur in top down synthesis fig nanoparticles are produced by size reduction from a suitable starting material meyers et al size reduction is achieved by various physical and chemical treatments fig top down production methods introduce imperfections in the surface structure of the product and this is a major limitation because the surface chemistry and the other physical properties of nanoparticles are highly dependent on the surface structure thakkar et al in bottom up synthesis the nanoparticles are built from smaller entities for example by joining atoms molecules and smaller particles mukherjee et al in bottom up synthesis the nanostructured building blocks of the nanoparticles are formed ﬁrst and then assembled to produce the ﬁnal particle thakkar et al the bottom up synthesis mostly relies on chemical and biological methods of production the probable mechanism of nanoparticle synthesis by bottom up approach is shown in fig of the biological methods of synthesis the methods based on microorganisms have been widely reported dhillon et al gericke and pinches kaler et al korbekandi et al li et al luangpipat et al mohanpuria et al sanghi and verma sastry et al microbial synthesis is of course readily scalable environmentally benign and compatible with the use of the product for medical applications but production of microorganisms is often more expensive than the production of plant extracts plant mediated nanoparticle synthesis using whole plant extract or by living plant were also reported in literature gardea torresdey et al park et al use of plant extracts in nanoparticle synthesis in producing nanoparticles using plant extracts the extract is simply mixed with a solution of the metal salt at room temperature the reaction is complete within minutes nanoparticles of silver gold a k mittal et al biotechnology advances 356 fig various approaches for making nanoparticles and cofactor dependent bioreduction and many other metals have been produced this way li et al fig shows picture of various plants used for the biosynthesis of nanoparticles the nature of the plant extract its concentration the concentration of the metal salt the ph temperature and contact time are known to affect the rate of production of the nanoparticles their quantity and other characteristics dwivedi and gopal table summarizes some of the reports pertaining to nanoparticle synthesis mediated by extracts of various plants synthesis of silver nanoparticles using a leaf extract of polyalthia longifolia was reported by prasad and elumalai an average particle size of about nm was obtained silver and gold ions could be reduced to nanoparticles using a leaf extract of cinnamomum camphora huang et al the reduction was ascribed to the phenolics terpenoids polysaccharides and ﬂavones compounds present in the extract these nanoparticles were found to have a peak bactericidal activity at a concentration of μg ml huang et al they were most active against the yeast candida albicans saxena et al used an aqueous extract of ficus benghalensis leaves to produce silver nanoparticles that had an average diameter of nm fig mechanisms of nanoparticle synthesis m metal ion a k mittal et al biotechnology advances 356 fig various types of plants used for the synthesis of metal nanoparticles in synthesis of silver nanoparticles using a geranium pelargonium graveolens leaf extract the particles formed rapidly and a stable size of nm was achieved shankar et al safaepour et al used geraniol a natural monoterpene alcohol found in some plants and silver nitrate to produce uniformly shaped silver nanoparticles in the size range of nm these nanoparticles were found to inhibit the growth of ﬁbrosarcoma wehi cancer cell line in vitro by up to at a concentration of μg ml safaepour et al kaviya et al reported the synthesis if silver nanoﬂakes using leaf extract of crossandra infundibuliformis reduction of silver ions to nanoparticles using extract of desmodium trifolium was ascribed to the presence of h ions nad and ascorbic acid in the extract ahmad et al synthesis of highly stable silver nanoparticles nm using leaf extract of datura metel has been reported kesharwani et al the extract contained alkaloids proteins enzymes amino acids alcoholic compounds and polysaccharides which were said to be responsible for the reduction of the silver ions to nanoparticles kesharwani et al quinol and chlorophyll pigments present in the extract also contributed to the reduction of silver ions and stabilization of the nanoparticles fig shows the probable chemical constituents present in the plant extract responsible for the bioreduction of metal ions their growth and stabilization a tuber extract of dioscorea bulbifera was used to produce gold and silver nanoparticles of various shapes ghosh et al these nanoparticles in combination with antibiotics were found to have a synergistic antibacterial activity against test microorganisms particularly against pseudomonas aeruginosa escherichia coli and acinetobacter baumannii ghosh et al use of antibiotics in combination with silver nanoparticles has been reported also for effective control of otherwise antibiotic resistant microorganisms sukirtha et al synthesized silver nanoparticles using a leaf extract of melia azedarach and showed them to be active against the hela cervical cancer cell line eclipta prostrata leaf extract mediated synthesis of silver nanoparticles has been reported jha et al silver nanoparticles made using leaf extract of e prostrata were reported to have larvicidal activity against ﬁlariasis and malaria vectors rajakumar and abdul rahuman synthesis of silver nanoparticles using a methanolic extract of eucalyptus hybrida safeda leaves has been reported dubey et al flavonoid and terpenoid compounds present in the extract were claimed to be responsible for the stabilization of nanoparticles cinnamomum zeylanicum bark is rich in linalool methyl chavicol and eugenol and these compounds were said to be responsible for the bioreduction of silver and palladium ions to corresponding nanoparticles in separate experiments sathishkumar et al b the protein from c zeylanicum bark helped to stabilize the synthesized silver nanoparticles sathishkumar et al jacob et al reported the synthesis of silver nanoparticles using piper longum leaf extracts the particles had a uniform spherical shape and ranged in size from about to nm these nanoparticles were found to have a signiﬁcant cytotoxic effect on hep cancer cells panda et al reported the synthesis of silver nanoparticles using a broth prepared from the aromatic spath of male inﬂorescence of screw pine pandanus odorifer forssk these silver nanoparticles were less cytotoxic in comparison with ag ion but were more genotoxic in an assay based on onion plant cells banerjee used an extract of syzygium cumini jambul seeds to produce silver nanoparticles the seed extract had antioxidant properties in in vitro the nanoparticles formed using the extract were found to have higher antioxidant activity compared with the seed extract this may have been due to a preferential adsorption of the antioxidant material from the extract onto the surface of the nanoparticles an extract of ocimum sanctum leaves was reported to reduce silver ions to nanoparticles nm within min mallikarjun et al this high activity of the extract was ascribed to the relatively high levels of ascorbic acid contained in the extract in other studies silver nanoparticles produced using o sanctum leaf extracts have been found to have a high antimicrobial activity against both gram negative e coli and gram positive streptococcus aureus microorganisms singhal et al extract of banana musa paradisiaca peels has been used to produce silver nanoparticles bankar et al these nanoparticles displayed antifungal activity against the yeasts c albicans and candida lipolytica and antibacterial activity against e coli shigella sp klebsiella sp and enterobacter aerogenes ahmad et al used extracts of the legume desmodium triﬂorum to synthesize silver nanoparticles a k mittal et al biotechnology advances 356 table biosynthesis of nanoparticles using plant extracts plant type of nanoparticle size and shape acalypha indica allium sativum garlic clove aloe vera aloe vera aloe barbadensis miller anacardium occidentale apiinextracted from henna lawsonia inermis leaves azadirachta indica neem boswellia ovalifoliolata calotropis procera camelia sinensis carica papaya catharanthus roseus ag ag au ag indium oxide au ag bimetallic au ag ag au bimetallic ag ag ag au ag ag chenopodium album cinnamomum camphora cinnamomum camphora citrus sinensis peel coleus amboinicus lour coleus aromaticus curcuma longa cymbopogon sp lemongrass datura metel desmodium triﬂorum diopyros kaki dioscorea bulbifera eclipta prostrate emblica ofﬁcinalis eucalyptus hybrid euphorbiaceae latex garcinia mangostana mangosteen leaf gelidiella acerosa geranium leaf jatropha curcas l latex memecylon edule melia azedarach mentha piperita peppermint moringa oleifera mucuna pruriens musa paradisiacal nelumbo nucifera lotus parthenium leaf psidium guajava pyrus sp pear fruit extract rhododedendron dauricam rosa rugosa sesuvium portulacastrum swietenia mahogani mahogany in the size range of nm ali et al reported the synthesis of silver and gold nanoparticles using extracts of mentha piperita peppermint plant these nanoparticles had antibacterial activity against clinically isolated human pathogens such as e coli and s aureus prathna et al reported the synthesis of silver nanoparticles using an extract of azadirachta indica leaves and a solution of silver nitrate increasing the reaction time from min to h resulted in a progressive increase in the particle size from around nm to around nm prathna et al prathna et al synthesized nm sized spherical silver nanoparticles using juice of citrus limon lemon the nanoparticles were produced by incubating the juice g l citric acid g l ascorbic acid for h with m silver nitrate solution such that the ratio of the juice to salt solution was by volume citric acid present in the juice was the principal reducing agent extract of rhizome of dioscorea batatas has been used to synthesize silver nanoparticles nagajyothi and lee the nanoparticles were found to be antimicrobial against the yeasts c albicans and saccharomyces cerevisiae narayanan and sakthivel reported the synthesis of silver nanoparticles using a leaf extract of coleus amboinicu the concentration of the extract mixed with the silver salt inﬂuenced the size and shape of the nanoparticles formed narayanan and sakthivel nanoparticles of triangular decahedral hexagonal and spherical shapes could be produced by varying the concentration of the extract narayanan and sakthivel babu and prabu synthesized nm silver nanoparticles using a ﬂower extract of calotropis procera kouvaris et al used a leaf extract of arbutus unedo to produce nanoparticles with a narrow size distribution baskaralingam et al used a leaf extract of calotropis gigantea to produce silver nanoparticles these a k mittal et al biotechnology advances 356 fig possible chemical constituents of plant extract responsible for the bioreduction of metal ions dubey et al huang et al nanoparticles had antibacterial activity against vibrio alginolyticus baskaralingam et al ravindra et al produced silver nanoparticles within cotton ﬁbers loaded with silver ions leaf extract of eucalyptus citriodora neelagiri and ficus bengalensis marri plants were used for the synthesis ravindra et al the average size of the nanoparticles was nm the cotton ﬁbers loaded with the silver nanoparticles were shown to be antibacterial towards e coli ravindra et al patil et al b produced highly stabilized silver nanoparticles nm using a leaf extract ocimum tenuiﬂorum the particles were antibacterial towards gram negative and gram positive bacteria krishnaraj et al synthesized silver nanoparticles nm using a leaf extract of acalypha indica the nanoparticles were shown to be antimicrobial against water borne pathogens such as e coli and vibrio cholerae spherical silver nanoparticles nm have been produced using a leaf extract of euphorbia hirta elumalai et al silver nanoparticles produced using peel extract of citrus sinensis were found to have a broad spectrum antibacterial activity kaviya et al the particles formed at c had an average size of around nm but reducing the reaction temperature to c increased the average size to nm kaviya et al kandasamy et al used a leaf extract of the prosopis chilensis l tree to produce silver nanoparticles that were active against vibrio species in the shrimp penaeus monodon kumar et al and banerjee used extracts of s cumini leaves and seeds to produce silver nanoparticles the nanoparticles produced using the seed extract were found to have stronger antioxidant properties in vitro than the original extract banerjee suggesting a concentration of the polyphenolic antioxidants on the surface of the particles by adsorption vijayaraghavan et al reported a one step synthesis of silver nano microparticles using trachyspermum ammi and papaver somniferum extracts the extracts of t ammi produced nanoparticles 998 nm of various triangular shapes and the extract of p somniferum resulted in spherical shaped microparticles μm li et al b used extracts of capsicum annuum leaf to produce silver nanoparticles and amorphous selenium protein composite nanoparticles a leaf extract of cassia auriculata has been used to synthesize spherical and triangular gold nanoparticles nm within min at room temperature kumar et al raghunandan et al produced irregularly shaped gold nanoparticles using an extract of dried clove syzygium aromaticum buds reduction and stabilization of gold nanoparticles were ascribed to the ﬂavonoids present in the extract parida et al reported the synthesis of gold nanoparticles mediated by an extract of allium cepa the particles had an average size of nm and could be internalized by mcf breast cancer cells via endocytosis parida et al kasthuri et al used a dilute phyllanthin containing extract derived from the plant phyllanthus amarus to produce hexagonal and triangular gold nanoparticles from increasing the concentration of the extract led to the formation of spherical nanoparticles kasthuri et al narayanan and sakthivel produced gold nanoparticles using a leaf extract of coriandrum sativum coriander the particles ranged in size from about to nm and had diverse shapes spherical triangular decahedral edison and sethuraman used an aqueous extract of terminalia chebula to produce gold nanoparticles with sizes ranging from to nm these nanoparticles were active against both gram positive s aureus and gram negative e coli liu et al synthesized gold nanoparticle using extracts of chrysanthemum and tea beverages a nanoparticle based assay was developed for quantifying the antioxidant properties of teas liu et al daisy and saipriya synthesized gold nanoparticles nm using an aqueous extract of cassia ﬁstula extracts of c ﬁstula bark are known to be hypoglycemic gold nanoparticles made using the extract were found to be superior to the extract as hypoglycemic agents in rats for the management of diabetes mellitus liu et al clearly the particles concentrated the hypoglycemic agent from the extract on their surfaces castro et al reported on the use of sugar beet pulp as an effective reductant for making gold nanowires at room temperature the nanoparticles formed initially later joined to form chains and nanowires the formation of the nanowires and nanorods depended on the conditions of the reaction particularly the ph the synthesis of gold and silver nanoparticles using leaf extracts of p graveolens and azadirachta indica was reported by shankar et al respectively bimetallic ag au core shell nanoparticles could be made by the reduction of aqueous ag and au ions using azadirachta indica leaf extract shankar et al presence of a k mittal et al biotechnology advances 356 reducing sugars in the extract was speculated to contribute to stabilizing the nanoparticles shankar et al gold and silver nanoparticles have been produced using extracts of aloe vera chandran et al and camellia sinensis vilchis nestor et al the optical properties of the nanoparticles depended on the initial concentration of the metal salts and the c sinesis extract vilchis nestor et al caffeine and theophylline found in c sinensis extract may have contributed to reduction of the metal ions and formation of the nanoparticles kasthuri et al reported the ability of apiin found in the leaf extract of henna to reduce ions to gold and silver nanoparticles secondary hydroxyl and carbonyl groups of apiin were responsible for the reduction the size and shape of the nanoparticles could be controlled by changing the concentration of apiin kasthuri et al dwivedi and gopal used extracts of chenopodium album leaves to produced silver and gold nanoparticle the particles had quasi spherical shapes and were in the size range of nm production of spherical and triangular shaped silver and gold nanoparticles using fruit extract of tanacetum vulgare has been reported dubey et al a ftir study revealed that the carbonyl groups were involved in the reduction of metal ions to nanoparticles the zeta potential of the silver nanoparticles was shown to vary with ph a low zeta potential at strongly acidic ph dubey et al a larger particle size could be achieved by reducing the ph of the reaction dubey et al njagi et al used an aqueous extract of sorghum bran to produce nanoparticles of iron and silver at room temperature valodkar et al synthesized nanoparticles nm of silver and copper using latex of euphorbiaceae these nanoparticles exhibited excellent bactericidal activity towards both gram negative and gram positive bacteria valodkar et al velayutham et al used a leaf extract of catharanthus roseus to synthesize nanoparticles of titanium dioxide the particles were of irregular shape and ranged in size from to nm suspensions of these nanoparticles were adulticidal and larvicidal against the hematophagous ﬂy hippobosca maculate and the sheep louse bovicola ovis velayutham et al huang et al reported a one step synthesis of au pd core shell nanoparticles using an aqueous solution of bayberry tannin at room temperature the tannin preferentially reduced the au ions to au nanoparticles when a mixture of au and pd was contacted with the tannin huang et al subsequently the gold nanoparticles served as seeds for the growth of a pd shell amarnath et al reported the antibacterial activity of palladium nanoparticles and their stabilization by chitosan and grape polyphenols palladium nanoparticles could be synthesized using coffee and tea extracts nadagouda and varma the nanoparticles were in the size range of nm nadagouda and varma sathishkumar et al produced palladium nanoparticles using an extract of cinnamon c zeylanicum bark as the reducing agent in this process the reaction ph the temperature and the concentration of the extract did not affect the size and shape of the nanoparticles palladium nanoparticles nm have been synthesized using an extract of annona squamosa l peel roopan et al palladium nanoparticles having an average size of nm were synthesized using a leaf extract of soybean glycine max kumar et al lee et al synthesized copper nanoparticles nm using magnolia leaf extract these copper nanoparticles had antimicrobial activity against e coli and were toxic to human adenocarcinomic alveolar basal epithelial cells cells applications of nanoparticles nanoparticles synthesized by the various methods have been used in diverse in vitro diagnostic applications chen et al doria et al fortina et al youns et al both gold and silver nanoparticles have been commonly found to have broad spectrum antimicrobial activity against human and animal pathogens ali et al arulkumar and sabesan duran et al jain et al kandasamy et al kim et al krishnaraj et al lara et al patil et al b prasad and elumalai ravindra et al sathishkumar et al saxena et al seil and webster singh et al singhal et al silver nanoparticles are already widely used as antimicrobial agents in commercial medical and consumer products rai et al ravindra et al fig shows the general applications of metal nanoparticles in biological ﬁeld fig types of metal nanoparticles and their applications in biotechnology a k mittal et al biotechnology advances 356 table various applications of nanoparticles synthesized from plant extracts metal nanoparticle application references silver anti microbial anti cancer anti protozoal anti fungal anti microbial anti bacterial anti microbial anti cancer anti cancer huang et al rajakumar and abdul rahuman sukirtha et al ghosh et al amarnath et al lee et al prasad et al gold palladium copper selenium silver nanoparticles are larvicidal against ﬁlariasis and malaria vectors rajakumar and abdul rahuman santhoshkumar et al and have been found to be active against plasmodial pathogens ponarulselvam et al and cancer cells fortina et al ravindra et al subramanian sukirtha et al antifungal effects of silver nanoparticles have been demonstrated vivek et al extensive literature exists on the mechanisms of antimicrobial action of silver and gold nanoparticles chaloupka et al cui et al li et al silver ion is highly toxic to most microorganisms jung et al and at least one mode of antimicrobial action of nanoparticles is through a slow release of silver ions via oxidation within or outside the cell silver nanoparticles are known to affect the permeability of membranes of microbial and other cells li et al nanoparticles are known to inactivate proteins and interfere with the replication of dna chaloupka et al the applications of metal nanoparticles synthesized from plant extracts are summarized in table applications of nanoparticles are emerging in crop protection and agriculture khot et al nair et al perez de luque and rubiales the use of zinc oxide nanoparticles in antimicrobial food packaging has been reported perez espitia et al several types of nanoparticles have been shown to reduce the microbial loads in treated wastewater efﬂuent duran et al current and potential applications of nanoparticles in biology and medicine are further discussed in the literature fortina et al sahoo et al salata seil and webster wagner et al zhang et al functionalization of nanoparticles for biomedical applications has been discussed by thanh and green and subbiah et al concluding remarks the use of plant extracts for making metallic nanoparticles is inexpensive easily scaled up and environmentally benign it is especially suited for making nanoparticles that must be free of toxic contaminants as required in therapeutic applications the plant extract based synthesis can provide nanoparticles of a controlled size and morphology in medicine nanoparticles are being used as antimicrobial agents in bandages for example applications in targeted drug delivery and clinical diagnostics are developing acknowledgment akm gratefully acknowledges the council for scientiﬁc and industrial research csir new delhi india for the award of a senior research fellowship in support of this work polyphenols as antimicrobial agents maria daglia polyphenols are secondary metabolites produced by higher plants which play multiple essential roles in plant physiology and have potential healthy properties on human organism mainly as antioxidants anti allergic anti inflammatory anticancer antihypertensive and antimicrobial agents in the present review the antibacterial antiviral and antifungal activities of the most active polyphenol classes are reported highlighting where investigated the mechanisms of action and the structure activity relationship moreover considering that the microbial resistance has become an increasing global problem and there is a compulsory need to find out new potent antimicrobial agents as accessories to antibiotic therapy the synergistic effect of polyphenols in combination with conventional antimicrobial agents against clinical multidrug resistant microorganisms is discussed address department of drug sciences university of pavia via taramelli pavia italy subdivided into many subclasses flavonols flavones flavanones anthocyanidins flavanols and also isoflavones figure the main groups of nonflavonoids are first phenolic acids that can be subdivided into derivatives of benzoic acid such as gallic acid and protocatechuic acid and derivatives of cinnamic acid that consist chiefly of coumaric caffeic and ferulic acid second stilbenes whose main representative is resveratrol that exists in both cis and trans isomeric forms and third lignans produced by oxidative dimerization of two phenylpropane units figure in addition to this diversity polyphenols are present in plant tissues mainly as glycosides and or associated with various organic acids and or as complex polymerized molecules with high molecular weights such as tannins corresponding author daglia maria maria daglia unipv it current opinion in biotechnology this review comes from a themed issue on food biotechnology edited by gabriella gazzani and michael grusak available online september see front matter elsevier ltd all rights reserved doi j copbio introduction polyphenols are secondary metabolites ubiquitously distributed in all higher plants which have important roles as defense against plant pathogens and animal herbivore aggression and as response to various abiotic stress conditions such as rainfall and ultraviolet radiation as regard to chemical structure they comprise a wide variety of molecules with polyphenol structure and are generally divided into flavonoids and nonflavonoids flavonoids share a common carbon skeleton of diphenyl propanes two benzene rings ring a and b joined by a linear three carbon chain the central three carbon chain forms a closed pyran ring ring c with a benzene ring more than flavonoids have been identified in fruits vegetables and plant derived beverages such as tea and wine and the list is constantly growing depending on the oxidation state of the central pyran ring flavonoids can themselves be current opinion in biotechnology over the course of the last years polyphenols have been studied for their potential involvement in the prevention of chronic diseases such as cardiovascular disease cancer osteoporosis diabetes mellitus and neurodegenerative diseases their protective activity has been attributed initially to their antioxidant free radical scavenger and metal chelator properties then to the capability of inhibiting or reducing different enzymes such as telomerase cycloxygenase or lipoxygenase and in more recent years to the interaction with signal transduction pathways and cell receptors moreover the antimicrobial activity of polyphenols occurring in vegetable foods and medicinal plants has been extensively investigated against a wide range of microorganisms among polyphenols flavan ols flavonols and tannins received most attention due to their wide spectrum and higher antimicrobial activity in comparison with other polyphenols and to the fact that most of them are able to suppress a number of microbial virulence factors such as inhibition of biofilm formation reduction of host ligands adhesion and neutralization of bacterial toxins and show synergism with antibiotics the antimicrobial properties of certain classes of polyphenols have been proposed either to develop new food preservatives due to the increasing consumer pressure on the food industry to avoid synthetic preservatives or to develop innovative therapies for the treatment of various microbial infections considering the increase in microbial resistance against conventional antibiotic therapy neolignan current opinion in biotechnology chemical structure of the polyphenol classes and microorganisms sensitive to them as reported in the review www sciencedirect com current opinion in biotechnology food biotechnology this review reports the results of the latest investigations regarding the most active antibacterial antiviral and antifungal polyphenols and brings to focus the synergistic activity of some of them in combination with conventional antimicrobials against multiresistance microbial pathogens structure activity studies showed that the galloyl side chain potentiates the parent catechin molecule antiviral activity in fact both egcg and epicatechin gallate ecg were found to be times more active against flu virus than epigallocatechin egc other investigations confirmed egcg antiviral activity against adenovirus and enterovirus infections antimicrobial activity of flavan ols considering flavan ols the antibacterial activity of catechins is known from the when it was demonstrated that these compounds largely present in oolong tea and above all green tea camellia sinensis inhibited the in vitro growth of several bacterial species such as vibrio cholerae streptococcus mutans campilobacter jejuni clostridium perfringes and escherichia coli egcg also exhibited variable time dependent and concentration dependent fungicidal activities several fungi including candida albicans proved sensitive to this compound suggesting that flavan ols may be useful in the treatment of c albicans superinfections of the oral cavities intestine and vagina which may result from an excessive use of antibiotics more recently it was demonstrated that some tea catechins such as gallocatechin gallate epigallocatechin gallate catechin gallate and epicatechin gallate are active at nanomolar levels against some other food borne pathogenic bacteria such as bacillus cereus most of these compounds were found to be more active than antibiotics such as tetracycline or vancomycin at comparable concentrations this suggested that the tested tea catechins could exert a positive effect against gastrointestinal diseases flavonols antimicrobial activity among tea catechins epigallocatechin gallate egcg has received the most attention and has been investigated more deeply in its antibacterial antiviral and antifungal activities as far as the antibacterial activity is concerned clinical isolates of helicobacter pylori a urease producing gastric pathogen that may contribute to the formation of ulcers and gastric cancer in humans including isolates highly resistant to metronidazole and or clarithromycin were used to determine their in vitro egcg sensitivity the minimum inhibitory concentration mic required to inhibit the growth of of organisms was found to be mg ml it is interesting to underline that those clinical isolates highly resistant against antibiotics also showed a similar egcg sensitivity it is worth mentioning the following investigation reporting that rhamnetin myricetin morin and quercetin showed high activity against chlamydia pneumoniae an obligate intracellular gram negative pathogen which is a common cause of acute upper and lower respiratory infections including pharyngitis sinusitis and pneumonia in this study the pretreatment of a human cultured cell line hl cells which is conventionally used in c pneumoniae cultivation with flavonols decreased the infectivity of c pneumoniae by compared to the percentage seen in untreated controls at polyphenol concentrations ranging from to mm when the compounds were continuously present in cell cultures infectivity was clearly lower varying from to all compounds also decreased the infective yields and the most chlamydiosidic compound was rhamnetin which killed c pneumoniae at the tested concentrations as the opportunity to use polyphenols as therapeutic agents is often limited by their bioavailability it is interesting to highlight that due to their hydrophobicity flavonols are capable of penetrating cell phospholipid membranes being therefore able to exert their antibacterial activity also inside the cell moreover rhamnetin resulted to be more active than quercetin and morin probably because of the methoxy group in the a ring which makes this molecule more hydrophobic as reported above tea catechins are active against e coli in particular egcg at sub mic mg ml did not affect e coli growth rate but showed significant antipathogenic effect because it decreased some virulence factors such as biofilm formation and bacterial swarm motility also egcg antiviral activity was discovered in the egcg was found to prevent infection caused by flu virus by binding to the viral hemagglutinin thereby preventing the attachment of viral particles to the target receptor cells other studies showed that modifications of viral membrane properties contributed to tea catechin antiviral effect against flu virus while at the same time current opinion in biotechnology as far as flavonols are concerned we can see a remarkable activity against several gram positive bacteria such as staphylococcus aureus lactobacillus acidophilus and actinomyces naeslundii and gram negative bacteria such as prevotella oralis prevotella melaninogenica porphyromonas gingivalis and fusobacterium nucleatum probably due to different mechanisms of action among which the most convincingly identified is the aggregatory effect on all the bacterial cells recent investigations also pointed out the fungicidal activity of flavonols it was shown that propolis which www sciencedirect com polyphenols as antimicrobial agents daglia is recommended worldwide for external topical use as it relieves various types of bacterial and fungal dermatitis possessed antifungal activity against microsporum gypseum trichophyton mentagrophytes and trichophyton rubrum and the main responsible agents for this activity were identified as flavonols galangin izalpinin and rhamoncitrin among the other propolis polyphenols found active there were flavanone pinocembrin and pinostrobin and chalcones dihydroxychalcone and dihydroxy methoxychalcone that other studies reported to show antimicrobial activity tannins antimicrobial activity tannins are subclassified into proanthocyanidins condensed tannins and gallotannins and ellagitannins hydrolyzable tannins the proanthocyanidins occur in fruits bark leaves and seeds of many plants they are dimers oligomers and polymers of catechins that are bound together by links between and or and are composed of a myriad of oligomeric products that differ first in region and stereochemical configuration of the flavanol interlinkages second in the phenolic hydroxylation pattern and third in the configuration of the hydroxylated c ring center of the flavan ol building block these oligoflavanols are further subdivided into two basic types a type and btype which are characterized by the occurrence of either a double or a single linkage connecting two flavanol units these differences in the chemical structures make investigations directed toward their biological properties or to their structure activity relationships quite challenging the most studied proanthocyanidins are those derived from berries that inhibit the growth of several pathogenic bacteria such as uropathogenic e coli cariogenic s mutans and oxacillin resistant s aureus the cranberry proanthocyanidins consisting primarily of epicatechin tetramers and pentamers with at least one atype linkage were found to be active against the reported pathogenic bacteria several mechanisms could explain the effect of the a type proanthocyanidin in the bacterial growth inhibition such as the destabilization of the cytoplasmic membrane the permeabilization of the cell membrane the inhibition of extracellular microbial enzymes direct actions on microbial metabolism or the deprivation of the substrates required for microbial growth especially essential mineral micronutrients such as iron and zinc via proanthocyanidin chelation with the metals whose depletion can severely limit bacterial growth besides antibacterial activity proanthocyanidins showed antiviral effects against influenza a virus and type herpes simplex virus hsv in this case the mechanism of action seems to consist in preventing the entry of the virus into the host cell which is the first critical step in primary hsv infection www sciencedirect com gallotannins and ellagitannins derived from the metabolism of the shikimate derived gallic acid trihydroxybenzoic acid follow through various esterification and phenolic oxidative coupling reactions to yield numerous near monomeric and oligomeric polyphenolic galloyl ester derivatives of sugar mainly dglucose the antimicrobial activity of hydrolysable tannins is well known ellagitannins the main phenolic compounds of rubus and fragaria genus raspberry cloudberry and strawberry show very interesting properties because they inhibit to different extents the growth of selected gram negative intestinal bacteria strains of salmonella staphylococcus helicobacter e coli clostridium campylobacter and bacillus but they are not active against gram positive beneficial probiotic lactic acid bacteria unfortunately listeria monocytogenes a common bacterium found in the environment and associated with animals that may cause meningitis sepsis or abortion is not affected by these berry compounds as far as gallotannins are concerned penta o galloylglucose hexa o galloylglucose hepta o galloylglucose octa o galloylglucose nona o galloylglucose and decao galloylglucose isolated from mango kernels showed antibacterial activity against food borne bacteria grampositive bacteria were generally more susceptible than gram negative in fact the mics against bacillus subtilis b cereus clostridium botulinum c jejuni l monocytogenes and s aureus were mg ml or less enterotoxigenic e coli and salmonella enterica were inhibited by mg ml also in this case lactic acid bacteria exhibited strong resistance the activity of gallotannins is attributable to their strong affinity for iron and it is also related to the inactivation of membrane bound proteins considering the antifungal activity of ellagitannins the discovery of this property derives from the observation that the durability of hardwoods such as oaks and chestnuts is thought to owe much to the deposition of ellagitannins which are able to precipitate protein and or remove metal cofactors through their strong affinity for metal ions acting as a microbial barrier a recent investigation showed that ellagitannins isolated from ocotea odorifera a medicinal plant commonly used in brazil have potent activity against candida parapsilosis at a concentration level of mm ellagitannins possess antiviral activities in particular against hiv infection and manifest inhibitory effects on hsv and or hsv replication as well as epstein barr virus ellagitannins activity against herpes virus seems due to a marked inhibitory effect on the replication of both hsv and hsv including acyclovir resistant strains with acyclovir being the first effective specific drug against herpes virus made available current opinion in biotechnology 23 food biotechnology nonflavonoid compounds antimicrobial activity as reported above nonflavonoids show weaker antimicrobial activity in comparison with flavonoids nevertheless some investigations are worth mentioning some phenolic acids gallic caffeic and ferulic acids showed antibacterial activity against gram positive s aureus and l monocytogenes and gram negative bacteria e coli and pseudomonas aeruginosa these compounds were found to be more efficient against the reported bacteria than conventional antibiotics such as gentamicin and streptomycin differently chlorogenic acid showed no activity against gram positive bacteria considering another nonflavonoid class of compounds lignans a recent investigation showed that the hexane extract obtained from aristolochia taliscana roots a plant used in traditional mexican medicine contains neolignans among which licarin a was found to be the most active with mics ranging from to mg ml against four mono resistant variants and clinical isolates of mycobacterium tuberculosis strains these results confirm previous investigations on lignans biological properties and suggest that these compounds represent a potentially active agent to fight tuberculosis a pathology that in recent years has become more of a worldwide concern as one third of the world s population is currently infected with m tuberculosis use of polyphenols as a new strategy to fight microbial resistance in combination with antiinfective drugs infectious diseases remain among the leading causes of morbidity and mortality both in developed and developing countries the selective pressure exerted by the use misuse and overuse of anti infective drugs has raised the problem of antibiotic resistant microbes bacteria viruses or parasites that have acquired the ability to survive existing drugs at clinically relevant concentrations and are responsible for very serious diseases such as aids tuberculosis gonorrhea malaria influenza pneumonia diarrhea and the chronic infections caused by bacterial biofilms therapeutic options for these so called community acquired pathogens such as penicillin resistant methicillin resistant and vancomycin resistant s aureus or multidrug resistant v cholera are extremely limited as are prospects for the next generation of antimicrobial drug development recently by considering that antibiotics are biological compounds that are produced by bacteria or other microorganisms and are capable of killing or suppressing the growth and reproduction of other bacteria several investigations have proposed that polyphenols secondary metabolites developed by plants as a strategy of defense against phytophagous insects fungi or bacteria could be used in combination with antibiotics in order to potentiate current opinion in biotechnology 23 181 their efficacy to lower antibiotic dose and therefore to reduce antibiotic adverse reactions the in vitro synergistic effect of two flavonols kaempferol and quercetin in combination with rifampicin a complex macrocyclic antibiotic was demonstrated against clinical rifampicin resistant methicillin resistant s aureus mrsa isolates as regard to the mechanism of action quercetin and kaempferol alone showed slight b lactamase inhibition but when combined with rifampicin the complex exhibited good b lactamase inhibitory effect and respectively in the same study the authors showed that the bactericidal action of ciprofloxacin a fluoroquinolone derivative commonly used in australia to treat staphylococcal infections and associated with a rapid emergence of resistance in gram positive bacteria was greatly enhanced by the sub mic addition of the two polyphenols quinolones mechanism of action is the bond with s aureus topoisomerase iv which causes cell death mainly thanks to dna synthesis inhibition cessation of growth and numerous double stranded dna breaks in the bacterial chromosome both quercetin and kaempferol inhibit the catalytic activity of different bacterial topoisomerases and this might explain some of the synergistic activities between ciprofloxacin and quercetin kaempferol these results were confirmed by a more recent investigation in which kaempferol glycosides isolated from laurus nobilis l reduced the mic of some fluoroquinolones registered against mrsa many papers reported that egcg act synergistically with various b lactam antibiotics against mrsa more recently a korean green tea polyphenol extract containing five main compounds occurring in different percentages egcg egc gallocatechin gallate epicatechin and ecg resulted to have antibacterial effects against strains of mrsa clinical isolates and strains of methicillin susceptible s aureus mssa the mics of oxacillin for each of the mrsa strains were reduced between fold and fold when these strains were coincubated with sub mic mic levels of tea polyphenols demonstrating that the combination of tea polyphenols plus oxacillin was synergistic for all the clinical mrsa isolates as for the mechanism of action two dimensional polyacrylamide gel electrophoresis identified downregulated proteins and three upregulated ones by exposure to polyphenols demonstrating that flavan ols can stimulate in a different way the expression of various proteins in these bacteria the combination of conventional antimicrobials and proanthocyanidins isolated from quercus ilex l not only showed antimicrobial effects against human bacterial species but also against fungal species proanthocyanidins combined with bifonazole and ketoconazole www sciencedirect com polyphenols as antimicrobial agents daglia increased the activity of both of these conventional fungicides rodrı guez vaquero mj aredes ferna ndez pa manca de nadra mc strasser de saad am phenolic compound combinations on escherichia coli viability in a meat system j agric food chem conclusions jayaraman p sakharkar mk lim cs tang th sakharkar kr activity and interactions of antibiotic and phytochemical combinations against pseudomonas aeruginosa in vitro int j biol sci 568 the antimicrobial activities of different classes of polyphenols alone and in combination with antipseudomonal drugs antibiotics against strains of p aeruginosa including clinical strains are reported the present study clearly highlights the low toxic potential of phytochemicals as antibacterial compounds and suggests the possibility of using synergistic drug herb combinations for combating infections caused by this pathogen the in vitro antimicrobial activity of some polyphenol classes has been widely shown by the scientific literature published over the last two decades this activity can be attributable both to direct action against bacteria virus and fungi as well as to the suppression of microbial virulence factors there is growing evidence that flavonoids act synergistically with various antibiotics against multidrug resistant microorganisms these findings suggest that future investigations should be carried out in order to study their in vivo activity toxicity and bioavailability and therefore to determine their actual relevance for treatment of human and animal infection diseases current opinion in biotechnology 23 this review comes from a themed issue on energy biotechnology edited by james c liao and joachim messing because plants have evolved to be recalcitrant to attack by the elements and in particular by microbes and their enzymes the recalcitrance barrier can in principle be overcome by thermochemical technologies involving reactive intermediates other than sugars e g synthesis gas pyrolysis oil fermentative processes e g acid hydrolysis phosphoric acid swelling ionic liquid pretreatment that overcome recalcitrance primarily via innovations in the realm of process engineering and fermentative processes that overcome recalcitrance primarily via innovations in the realm of biotechnology in the latter category processes in which cellulosic biomass is fermented to desired products in one step without adding externally produced enzymes are of obvious appeal indeed such consolidated bioprocessing cbp is widely recognized as the ultimate configuration for low cost hydrolysis and fermentation of cellulosic biomass a cbp enabling microbe must be able to both solubilize a practical biomass substrate and produce desired products at high yield and titer under industrial conditions since microbes with these properties have not been found in nature genetic engineering is required via one of the two strategies the native strategy beginning with microbes that have native ability to utilize insoluble components of cellulosic biomass and the recombinant strategy beginning with microbes that do not have this ability and thus require heterologous expression of a saccharolytic enzyme system the cbp strategy is in principle applicable to production of a broad range of products from plant biomass but has received the most attention with respect to ethanol production and is being implemented commercially for this product first 1669 see front matter elsevier ltd all rights reserved we endeavor here to provide an overview of technological and scientific advances relevant to cbp since our last such review the wholesale price of gasoline has more than doubled underscoring that both the need and opportunity for low cost biomass processing have grown considerably doi j copbio economic motivation available online december introduction cellulosic plant biomass has many desirable features as an energy source but is difficult to efficiently convert into liquid fuels biological processing is a promising technology option for achieving this conversion but still poses great challenges these challenges arise primarily current opinion in biotechnology 23 cbp has potential to lower the cost of biomass processing compared to process configurations featuring a dedicated step for cellulase production due to elimination of operating and capital costs associated with dedicated enzyme production and or more effective biomass solubilization estimates of the cost of added enzymes for lignocellulose conversion vary widely and do not show a decreasing www sciencedirect com recent progress in consolidated bioprocessing olson et al figure pretreatment greater savings due to eliminated enzyme production or a combination of these again depending on the base case to which cbp is compared current opinion in biotechnology historical estimates of the contribution of cellulase enzyme cost to final ethanol cost letters refer to individual references error bars represent the extent of the high and low estimates when given a b c d e f g h i j k l m trend over time figure any cost savings estimate for cbp depends however on the choice of both the advanced technology configuration assumed as well as the base case to which it is compared if the base case has high loading of added enzymes projected savings from eliminated cellulase production by cbp are potentially large if a base case has low loading of added enzyme projected savings from a cbp process achieving high rates and yields are primarily in the form of decreased capital and operating costs related to fermentation and feedstock this dynamic partially explains why enzyme cost estimates differ so widely another implication is that the cost savings of cbp can be much larger than the cost of added enzyme if the basis of comparison is a process designed to minimize enzyme addition in any case all indications are that the cost of added enzyme continues to be a major constraint to cost effective processing of cellulosic biomass figure at the low end of recent estimates cents per gallon ethanol the cost of cellulase is comparable to the purchase cost of feedstock one is hard pressed to come up with an example of a commodity process where the catalyst cost is comparable to that of the raw material more effective biomass solubilization by cbp may arise because cellulase loadings are higher in cbp than is practical in other configurations use of complexed cellulase systems high temperature enzyme microbe synergy or a combination of these economic benefits of more effective cellulose solubilization may be realized as a result of higher rate higher yield less expensive www sciencedirect com progress in the development of genetic tools for fungal systems has recently been reviewed and will not be discussed in detail although to date most engineering efforts have focused on increasing cellulase production there is also interest in engineering biofuel production in fungal systems such as fusarium oxysporum and trichoderma reesei there has also been substantial progress in the development of genetic tools for free enzyme bacterial systems including clostridium phytofermentans clostridium japonicus and thermoanaerobacter and thermanaerobacterium sp thermoanaerobacterium saccharolyticum a thermophilic anaerobe that utilizes a broad range of substrates including xylan and all naturally occurring sugars present in biomass although not crystalline cellulose provides a prominent example of engineering an organism with recently developed genetic tools to produce a biofuel at high yield shaw et al eliminated acetic and lactic acid production resulting in a strain that produces ethanol at a yield of g ethanol g xylose or other sugars such yield was observed under a variety of conditions e g different substrate concentrations batch and continuous culture although it was not demonstrated in industrial growth media in another notable genetic engineering feat reported by cripps et al geobacillus thermoglucosidasius a thermophile capable of oligosaccharide fermentation was engineered by deletion of ldh and pfl and upregulation of pdh to produce industrially relevant yields of g ethanol g hexose sugar although the reported yield was slightly less for pure pentose sugars study of caldicellulosiruptor sp as potential cbp organisms has recently been initiated motivated in part by their having the highest temperature optima among described cellulolytic microbes and the finding that cultures are able to achieve substantial solubilization of lignocellulose without pretreatment the architecture of the cellulase system of members of this genus features multiple catalytic enzyme modules in single enzymes and appears to be different from both the noncomplexed paradigm exemplified by the cellulases of t reesei and other aerobic fungi and cellulosome current opinion in biotechnology 23 guedon et al jin et al agyros et al aem in press guedon et al zambare et al okamoto et al gardner and keating tolonen et al higashide et al gardner and keating agyros et al aem in press berezina et al paradigm exemplified by c thermocellum and other anaerobes active investigation is underway to develop genetic tools for manipulation of this organism with promising results presented at scientific meetings and submitted among cellulosome forming bacteria c thermocellum and c cellulolyticum are the most well developed as potential cbp hosts there has been substantial recent progress with respect to development of tools for genetic manipulation and subsequent metabolic engineering including production of butanol in the case of c cellulolyticum reported values of key performance parameters are shown in tables and which includes both native cellulose utilizing microbes and native hemicellulose utilizing microbes an early comprehensive analysis of the economic benefits of cbp described an advanced scenario with a very low projected cellulosic ethanol selling price based on a titer of g l hydrolysis yield after pretreatment metabolic yield after fermentation and a rate of g l h the argyros et al result with a co culture of c thermocellum and t saccharolyticum is remarkably close to meeting these milestones although it has yet to be shown with a real world substrate under industrial conditions 51 0 0 0 6 0 6 current opinion in biotechnology 23 nd not determined insufficient data a starting substrate ending substrate starting substrate b product titer consumed substrate theoretical yield concentration of total carbohydrates 19 19 cellulose afex corn stover avicel cellulose prairie cord grass rice straw avicel filter paper sigma avicel avicel grass the concept of a titer gap a difference between the maximum concentration of a compound that is tolerated when it is added to a culture and the maximum concentration of that compound that is produced is relevant when considering organism development for cbp via the native strategy the titer gap may be defined with respect to either growth or metabolism and is often not the same for these two points of reference a substantial titer gap is a salient feature of most microbes of interest for the native strategy in the case of c thermocellum for example several studies have established that strains able to grow in the presence of added ethanol at concentrations exceeding g l can readily be obtained by serial transfer and uncoupled metabolism at yet higher concentrations seems reasonable to expect however the maximum concentration of ethanol produced by this organism in pure culture remains at about g l 23 production of ethanol at concentrations at or exceeding the tolerance exhibited in exogenous addition experiments has been observed in the case of engineered strains of t saccharolyticum for which titers g l have been obtained hogsett da abstract biochemical conversion platform peer review denver co february experience with industrial microorganism development provides increasing support for the proposition that with sufficient effort stoichiometric yields of engineered products can be achieved and the titer gap closed www sciencedirect com yao and mikkelsen yao and mikkelsen cai et al prominent examples include ethanol production in yeast and e coli and more recently engineering of e coli to produce propane diol at of theoretical yield and a titer of g l it can be expected that this is also true for less well established organisms of interest for the native cbp development strategy with t saccharolyticum providing the most fully developed example to date hogsett da abstract biochemical conversion platform peer review denver co february progress with hosts for the native cbp strategy will be slower because tools are less developed although this will probably become less true over time the case for eventual success via the native strategy is somewhat less clear with respect to industrial robustness including compatibility with practical pretreatments fermentation at high substrate and hence solids concentration in industrial growth media and strain management and stability we suggest that these and other dimensions of industrial robustness are a key area for investigation aimed at advancing the native strategy product the recombinant strategy the primary challenge for the recombinant strategy is heterologous expression of sufficient quantities of several types of cellulase and or hemicellulase enzymes to permit rapid growth and conversion of pretreated lignocelluloses total enzyme activity produced by the host can be increased by improving both total expression and specific activity of the enzyme system moreover specific activity of the system is a function of both the composition of the system and the specific activity of the components given the expense of aerating large culture volumes as well as loss of product yield and feedstock energy as a result of aerobic respiration non oxidative metabolism is highly desirable and is likely required for many applications the recombinant strategy has been pursued in a number of host organisms including s cerevisiae e coli and bacillus subtilis with work in s cerevisiae the most advanced to date significant advances have recently been made with regard to expression level of cellulases in s cerevisiae as shown in figure the report of ilmen et al represents a large increase in the maximum titer achieved for two critical cellulases and the cellulase expression levels achieved in this study correspond to of total cell protein which meets the calculated levels for growth on cellulose at rates required for an industrial process however data for heterologous cellulase production in yeasts in figure and in the literature in general have been reported for aerobic cultures with cell densities in the range of approximately 5 g l shake flask to g l fed batch bioreactor whereas cbp will involve anaerobic cultures with cell densities at the low end of this range assessing the impact of the advances reported by ilmen et al and future advances will require data obtained under anaerobic conditions current opinion in biotechnology 23 energy biotechnology figure k tcp not as advanced as for non complexed components using the enzymatic components of the c thermocellum system as an example only about of the diversity has been functionally expressed year of publication current opinion in biotechnology comparison of expression levels for and in s cerevisiae in terms of both the titer achieved and the percentage of total cell protein tcp a b c d e f g h i j k note percentage of tcp for in ilmen et al was calculated by assuming a cell yield of 0 45 g cells g glucose and a protein content of dry cell weight of 72 under industrial conditions while such data have not been reported to date in the literature the authors of this study have observed anaerobic production of cbhs at roughly equivalent levels on a percentage to total cell protein tcp basis to the maximum shown in figure unpublished data one approach to increasing the specific activity of recombinantly expressed enzyme systems is to mimic systems already available in nature and in some cases already industrially applied table depicts the ongoing work to create a complete enzymatic system for s cerevisiae relying on data generated by studies that have examined enzymatic systems for the breakdown of lignocellulose via proteomic analysis enzyme diversity from trichoderma reesei an organism whose enzymes will be used to hydrolyze lignocellulose at an industrial scale and from the rapid cellulose utilizer c thermocellum was compared to the enzyme diversity functionally expressed in s cereivisae compared to the herpoel gimbert study of the functional enzyme classes identified in t reesei have been expressed in s cerevisiae whereas of those identified in nagendran et al have been expressed in s cerevisiae of particular note is recent work that has shown expression of many enzyme categories in s cerevisiae utilizing genes in the process brevnova et al patent applied for in contrast but not surprisingly expression of complexed enzyme system components is current opinion in biotechnology 23 another approach to increasing specific activity is to mimic the cellulosome system of complexed enzymes bound to the cell surface this approach has shown promising results with activity enhancement of 5 fold for complexed systems compared with the same enzymes in a non complexed system surface display and endoglucanase activity have been demonstrated in yeast in several instances recombinant cellulosomes capable of solubilizing of g l pasc in hours were developed by expressing a scaffoldin and enzymatic subunits in a single yeast strain by expressing cellulosome components separately a different group was able to achieve solubilization of g l pasc in hours however in both of these cases the cellulosome displaying yeast were unable to grow on pasc and hydrolysis required a high cell density using cells pre cultured in rich media once an enzyme system is created the components of that system must be combined in an organism to allow cellulose and or hemicellulose hydrolysis the first report of anaerobic conversion of phosphoric acid swollen cellulose pasc into ethanol using s cerevisiae at low cell densities to our knowledge was published in where the authors demonstrated conversion of pasc in hours as per recent reports more rapid conversion of pasc into ethanol was achieved by optimizing the level and ratios of cellulase enzymes in s cerevisiae using delta integration of cellulase genes followed by screening for pasc solubilization the yield of of theoretical ethanol suggests that further increases in enzyme production are necessary and or additional enzymes to achieve complete conversion expression of a single engineered endoglucanase at very high levels 5 of total cell protein in bacillus subtilis has been shown to allow conversion of g l reactive amorphous cellulose rac into g l lactate in hours a recent patent application describes the construction of s cerevisiae strains expressing cellulases that can produce small amounts of ethanol directly from avicel a crystalline cellulose by virtue of these expressed enzymes which to our knowledge is the first report of its kind using strains described in this patent application the authors have observed conversion of pasc into ethanol by recombinant s cerevisiae in hours and conversion of bacterial microcrystalline cellulose into ethanol in hours using low inoculums 5 v v unpublished results while these studies demonstrate the principle that cellulose chains can be hydrolyzed by recombinant enzyme systems they have all been carried out on extremely reactive substrates and all but one on amorphous cellulose substrates available in www sciencedirect com energy biotechnology industrial processes are unlikely to be as amorphous in character or as reactive as these model substrates however and development of cbp organisms using pasc may mask the need for enzymes targeting crystalline cellulose in addition to the advances in cellulose utilization there has been some work directed toward obtaining polymeric hemicellulose utilization an obvious prerequisite to hemicellulose utilization is the ability to use the monomer sugars including xylose galactose arabinose mannose and others resulting from hydrolysis it is also often necessary to be able to consume these sugars in the presence of glucose or cello oligomers while several candidate organisms such as e coli and b subtilis already possess this ability it must be engineered into s cerevisiae pentose fermentation by s cerevisiae is relatively advanced and has been reviewed elsewhere however one new direction for the field is the creation of strains that co ferment cellobiose and xylose in these strains a cellodextrin transporter is coupled with xylose fermentation capability relieving inhibition of xylose utilization by glucose because cellobiose is a potent inhibitor of cellulases industrial processes where separate hydrolysis and fermentation are used are unlikely to have high concentrations of cellobiose and xylose but this technology may be applicable to cbp processes if cellobiose or other cello oligomers can be created and rapidly co fermented with xylose as discussed above table many if not all of the necessary enzyme classes for hemicellulose hydrolysis have been expressed in s cerevisiae a recent publication demonstrates that a combination of xylanase b xylosidase and b glucosidase in a xylose utilizing background yielded conversion of oligomers from pretreated rice straw although these results were obtained using very high cell densities g l wet cell weight in addition to this study several xylan fermenting e coli strains have been developed a binary culture of e coli strains expressing xylanase enzymes was shown to allow of the birchwood xylan initially present to be converted into ethanol without the addition of exogenous enzymes another example of hemicellulase engineering in e coli demonstrated the production of fatty acid ethyl esters from birchwood xylan by expressing the endoxylanase and the xylanases xsa in this case the expression of just two enzymes was sufficient to support growth on xylan microbial cellulose utilization fundamentals there are substantial and perhaps underestimated additional phenomena that arise when considering solubilization of cellulose and other insoluble biomass components by cultures of saccharolytic microbes as compared to solubilization by cell free enzymatic systems examples of these phenomena include bioenergetics current opinion in biotechnology 23 42 implications of attachment of cells to the surface of insoluble substrates and expression of saccharolytic enzymes on the cell surface with respect to kinetics and substrate capture and metabolic control related to the choice between investing cellular resources in cellulase or growth there are also some foundational questions of great relevance to cbp for which there are not yet definitive answers notably including under what conditions e g temperature ph substrate particle size presence of multiple microorganisms with complementary capabilities does microbial cellulose utilization proceed fastest how do saccharolytic microbes compare to cell free cellulase systems as agents of biomass solubilization with respect to key variables such as rate yield and the extent of pretreatment required while outstanding issues remain understanding microbial cellulose utilization has received an increased level of effort in the last five years with many exciting findings in the publication pipeline highlights of published work during this period include systematic study of the implications of enzymes displayed on the cell surface indications of cellulose binding triggered gene expression 45 47 50 quantitative demonstration of enzyme microbe synergy 4 the first studies involving targeted knockout of cellulase components in an obligate anaerobe 51 and demonstration of high conversion of unpretreated lignocellulose by microbial cultures a particularly important focus research during the coming years will be to extend fundamentals inclusive studies to include lignocellulosic substrates and operation under conditions approaching those that would occur in an industrial process conclusions during the last five years key advances have been made in organism development for cbp while at the same time remaining barriers have been brought into focus for the native strategy key advances include development of genetic systems for several cellulolytic anaerobic bacteria engineering a cellulolytic host to produce butanol and engineering of a thermophilic bacterium to produce ethanol at commercially attractive yields and titers for the recombinant strategy key advances include highyield conversion of model cellulosic substrates and heterologous expression of and in yeast at levels believed to be sufficient for an industrial process for both strategies increased emphasis on realizing high performance under industrial conditions is needed demonstrating high fermentation yields and titer from practical pretreated lignocellulosic feedstocks is a particular priority for the native strategy co expression of multiple proteins allowing utilization of such pretreated feedstocks with high hydrolysis yields and reasonable rates is a particular priority for the recombinant www sciencedirect com recent progress in consolidated bioprocessing olson et al strategy continued exploration of the underlying fundamentals of microbial cellulose utilization is likely to be useful in order to guide the choice and development of cbp systems abstract recombinant chinese hamster ovary cells rcho cells have been the most commonly used mammalian host for large scale commercial production of therapeutic proteins recent advances in cell culture technology for rcho cells have achieved significant improvement in protein production leading to titer of more than g l to meet the huge demand from market needs this achievement is associated with progression in the establishment of high and stable producer and the optimization of culture process including media development in this review article we focus on current strategies and achievements in cell line development mainly in vector engineering and cell engineering for high and stable protein production in rcho cells the approaches that manipulate various dna elements for gene targeting by site specific integration and cis acting elements to augment and stabilize gene expression are reviewed here the genetic modulation strategy by direct cell engineering with growth promoting and or productivity enhancing factors and omics based approaches involved in transcriptomics proteomics and metabolomics to pursue cell engineering are also presented keywords cho cells cell line development vector engineering cell engineering omics based approaches j y kim g m lee department of biological sciences graduate school of nanoscience technology wcu kaist kusong dong yuseong gu daejeon korea e mail gyunminlee kaist ac kr y g kim biotechnology process engineering center kribb daejeon korea introduction the approval of human tissue plasminogen activator genentech usa as the first therapeutic protein from recombinant mammalian cells in triggered the emergence of mammalian cell culture as the industry s workhorses for biopharmaceutical production although a variety of alternative expression systems exists such as microbial insect transgenic animal and plants mammalian cells are the principle hosts for the commercial production of therapeutic proteins among the biopharmaceuticals approved from to are produced from mammalian cells walsh despite the availability of a number of other mammalian cell lines such as baby hamster kidney mouse myeloma derived human embryonic kidney hek and the human retina derived nearly of all recombinant therapeutic proteins produced today are made in chinese hamster ovary cho cells jayapal et al the popularity of cho cells can be attributed to the following reasons firstly since cho cells have been demonstrated as safe hosts for the past two decades it may be easier to obtain approval to market the therapeutic proteins from regulatory agencies like the fda secondly low specific productivity q which is one of the disadvantages of using mammalian cells for protein production can be overcome by gene amplification in cho cells for cho cells powerful gene amplification systems such as dihydrofolate reductase dhfr mediated or glutamine synthetase gs mediated gene amplification are available thirdly cho cells have the capacity for efficient post translational modification and they produce recombinant proteins with glycoforms that are both compatible with and bioactive in humans finally cho cells can be easily adapted to growth in the regulatory friendly serum free sf suspension conditions a characteristic preferred for large scale culture in bioreactors currently stirred tank bioreactors over 000 l scale are readily used for sf suspension cultures of recombinant cho rcho cells producing therapeutic antibody with an increasing biopharmaceutical market showing an average yearly growth of since the popularity of cho cells as the host for commercial production of therapeutic proteins is likely to persist at least in the near future over the past two decades a more than 100 fold yield improvement of titers in rcho cell culture has been observed and this improved product yield has been largely attributed to the development of serum free medium as well as the optimization of feeding strategies hacker et al nevertheless the demands of the ever increasing highly competitive market still require cells to be more highly productive and to be grown in bioreactors at higher cell densities under rigorous optimization schemes in this review we summarize the current strategy of rcho cell line development and discuss current achievements in vector engineering and cell engineering that make rcho cell lines highly productive and robust for large scale commercial production of therapeutic proteins current strategy for rcho cell line development in many cases a cell line is established based on clones exhibiting high productivity and process development for manufacturing is performed using that cell line however every cho clone reacts differently to different media and culture conditions and significant effort is repeatedly required from process development teams whenever new clones are used to produce new therapeutic proteins accordingly to carry out a specific cell culture process clones suitable for that process should be selected because of clonal variability for example clones suitable for sf suspension fed batch culture are selected for large scale commercial production of therapeutic antibody in the same context many biotech companies have their own therapeutic protein production platform and clones suitable for that platform are selected to facilitate and speed up the process development of new therapeutic proteins figure shows a representative schematic diagram of the steps involved in cell line development for therapeutic antibody production since development of a production cell line requires a considerable investment of time and resources the gene of interest is first transiently expressed in hek or cho cells to test its efficacy and manufacturability once proven the gene of interest is introduced into cho host cell lines such as dhfr deficient cho and and cho mostly by lipofection the cho host cell lines have been adapted for growth in sf suspension to save the appl microbiol biotechnol 93 time and effort of adapting the resulting rcho production cell line to grow in sf suspension culture once the dna enters the host cell nucleus it integrates into the chromosome at a random location if necessary high producing parental clones are subjected to gene amplification for further enhancement of q increased heterogeneity resulting from the random integration of a transfected foreign gene into the chromosome and the altered position of the integrated gene by a gene amplification system contributes to a wide variety of individual clones regarding q since the isolation of clones with a high expression level clearly depends on the number of clones that have been screened screening procedures such as the limiting dilution method are time consuming and labor intensive as a result there has been increasing use of efficient high throughput cell screening systems such as fluorescence activated cell sorting facs the clonepix system genetix the leap system cyntellect and the cellcelector system aviso clones are selected on the basis of their high expression levels and their performance is further tested in sf suspension fed batch culture which is most widely used for large scale commercial production of therapeutic antibody clones showing the best performance in sf suspension fed batch culture in lab scale bioreactors with pre developed feeding cocktails are usually selected as production cell lines vector engineering in cell line development as described earlier the establishment of a high producing stable rcho cell line by gene amplification followed by an extensive screening is time consuming and labor intensive vector engineering which modulates transcriptional activity facilitates rcho cell line development high q can be achieved by modulating transcriptional activity via insertion of various dna elements into a vector in addition the instability of q which often occurs during long term culture may also be overcome by vector engineering although the mechanism responsible for reduced q is not fully understood it is likely to be due to loss of gene copies silencing of a gene by chromosomal rearrangement and or transcriptional inactivation by methylation barnes et al in this section the current vector engineering which includes gene targeting by site specific integration and a cis acting element to augment gene expression is discussed as an efficient and reliable method to establish a high producing stable rcho cell line gene targeting by site specific integration site specific integration of transgenes leads to predictable expression properties and circumvents extensive appl microbiol biotechnol 93 fig schematic diagram of steps involved in cell line development for therapeutic antibody production in rcho cells gene amplification efficacy dna transient expression in hek or cho cells manufacturability stable expression in cho cells adapted to grow in sf suspension culture scale up clone selection large scale l sfsuspension fed batch culture feed media screening this technique however requires information on the chromosomal loci at which a transgene is stably and highly expressed a so called hot spot after screening the hot spot a new desired transgene is introduced into this pre defined position by vector based sitespecific integration successful integration of the desired transgene into the hot spot may guarantee the fast and efficient selection of high producing stable clones for effective recombination and exclusion of unnecessary vector sequences of prokaryotic origin site specific integration is conducted using recombinase mediated cassette exchange strategy among various recombinases cre and flp which respectively recognize loxp and frt sequences are most widely used until now genome engineering by site specific modification has been applied in various fields of biotechnology kito et al first reported the use of gene targeting based on the cre loxp system for human monoclonal antibody production in rcho cells fluorescence in situ hybridization analysis of rcho cells constructed using a loxp and frt strategy revealed that the antibody genes were all located in the original frt tagged locus in the genetargeted and gene amplified cells huang et al because the loxp and frt sequence affects the recombination efficiency of cre and flp recombinase the mutant forms of these sequences such as and in the cre loxp system and in the flp frt system should be evaluated to this end a simple and accurate analysis system was developed to estimate recombination efficiency using facs to compare the efficiency of recombination related to various spacer mutants kim et al kim and lee recently kameyama et al developed an accumulative site specific gene integration system using a series of cre mediated integration reaction with mutated loxps sequences allowing the repeated insertion of multiple genes into one target site besides the cre flp based gene targeting various endonucleases for genome engineering have been also applied the endonucleases include chemical nucleases zinc finger nucleases meganucleases and transcription activators like effectors tale nucleases silva et al because homologous gene targeting is inefficient in mammalian cells meganucleases are applied to make dna double strand break cabaniols et al another recent gene targeting system is the dna recombinase based mammalian artificial chromosome engineering system ace system which offers a clear advantage of large cloning capacity compared to other systems an antibody producing clone with high and stable expression was generated by utilizing this ace system kennard et al cis acting element for augmenting gene expression genetic elements may remodel chromatin to maintain the transgene in an active configuration generally two components of transcription trans acting factors transcription factor and cis acting elements enhancer sequence and stable element for messenger rna mrna are used to augment gene expression until now a number of cis acting elements have been applied to rcho cells to enhance the expression and stability of protein production the most popular cis acting element in rcho cell culture is a scaffold matrix attachment region s mar which maintains the chromatin structure in an active configuration through the creation of chromatin loops this element can increase the expression of a transgene by its boundary effect although the detailed mechanism is currently unknown a variety of s mars such as chicken lysozyme mar human β globin mar and β interferon sar have been tested to evaluate their use to augment expression in rcho cells girod et al kim et al zahnzabal et al interestingly girod et al reported that chicken lysozyme mar acts as a cis acting element on the transgene expression vector and trans acting factors transfected with a separate plasmid furthermore transgene expression was maximized when the chicken lysozyme mar was simultaneously introduced with the cis acting element and trans acting factors ubiquitous chromatin opening elements ucoes are another well known type of cis acting element benton et al evaluated the augmentation effect of ucoes on viral human cytomegalovirus hcmv promoter derived gene expression a dramatic increase in the antibody production of rcho cells was achieved because of the beneficial effect of maintaining chromatin in an open configuration by ucoes cell engineering in cell line development to improve the characteristics of rcho cells in regard to cell growth and foreign protein production numerous cell engineering strategies have been attempted cell engineering has been targeted mainly to increase the time integral of viable cell concentration ivcc and or q figure schematically depicts the effect of cell engineering for improved culture characteristics resulting in enhanced lifespan and production icvv can be increased by enhanced culture longevity improved specific growth rate μ and increased maximum viable cell density this strategy targets to the reduction of cell death at the end of culture and the induction of cell proliferation at the initial stage for the aim of enhancing q the components in regulation of the cell cycle folding transport and secretion are chosen to be key regulators reconstruction of the metabolic pathway also improves protein production anti apoptosis engineering cell death is considered an important issue to be dealt with because it affects the viable cell concentration as well as the product quantity and quality arden and betenbaugh during rcho cell culture cell death is triggered by a variety of stresses including nutrient depletion accumulation of toxic by products elevated osmolarity and shear stress cell death occurs in two forms necrosis or two types of programmed cell death pcd namely apoptosis and autophagy while necrosis is a sudden and passive form pcd is an active genetically controlled process many researchers have recognized pcd as a target to overcome appl microbiol biotechnol 93 the problem mentioned above apoptosis has gained importance in mammalian cell culture including that of rcho cells for therapeutic protein production over the last two decades the prevention of apoptosis by establishing apoptosis resistant rcho cells appears to be beneficial the apoptotic process is regulated by either activation or suppression of members of the bcl family proteins which includes three groups anti apoptotic bcl like proteins bcl bcl xl bcl w and mcl pro apoptotic bax like proteins bax bak and bok and pro apoptotic only proteins bim bad bid puma and noxa adams and cory considering their function as key regulators in apoptosis the activation of anti apoptotic bcl like proteins and suppression of pro apoptotic factors is of particular interest in delaying the onset of apoptosis figure illustrates the genetic engineering strategies employed to overcome apoptosis in rcho cells previously it was found that apoptosis in rcho cells was alleviated by the overexpression of anti apoptotic proteins that include bcl bcl xl and mcl or down regulation of pro apoptotic proteins such as bak and bax resulting in increased protein production chiang and sisk cost et al kim and lee majors et al meents et al tey et al apoptotic signal is mediated by a caspase cascade system which is a series of proteolytic cascade in caspase by other activated cleaved caspases because the caspases play an essential role in apoptosis regulation including induction transduction and amplification of signals the suppression of caspase activation is a promising strategy caspases can be divided functionally into two groups initiator caspase and effector caspase inhibition of caspase 8 caspase 9 fig schematic depiction of the effect of cell engineering on improved culture characteristics cell engineering leads to enhancement of cell growth and q which reflects on the product titer appl microbiol biotechnol 93 fig schematic diagram illustrating the genetic engineering strategies employed to overcome apoptosis in rcho cells initiator caspases by overexpression of dominant negative mutants and caspase caspase effector caspases by introduction of antisense rna or small interfering rna showed a positive effect on cell growth kim and lee sung et al yun et al another strategy for suppressing caspases is the overexpression of intracellular caspase inhibitors such as x linked mammalian inhibitor of apoptosis an inhibitor of caspase caspase caspase 9 and protein cytokine response modifier a a strong inhibitor of caspase caspase 8 and a weak inhibitor of caspase caspase 6 sauerwald et al in addition to cho cell engineering based on bcl family proteins and caspases various other factors have also been applied to inhibiting apoptosis overexpression of adenovirus derived anti apoptotic isolated from silkworm hemolymph and aven the inhibitor of apoptosome activation via interacting apaf successfully blocked apoptosis progression choi et al figueroa et al in addition murine double mutant the inhibitor for the tumor suppressor protein was used to confer apoptosis resistance arden et al human telomerase reverse transcriptase overexpressing rcho cells induced the proliferation and promotion of the resistance to apoptosis crea et al interestingly the reports dealing with two heat shock proteins hsps and taurine transporter proposed that the factors not directly involved in the apoptotic pathway can be good candidates to delay apoptosis with enhanced q lee et al tabuchi et al the beneficial effect of the overexpression of antiapoptotic bcl like proteins on rcho culture longevity is common in contrast there are conflicting reports on its effectiveness on q chiang and sisk lee and lee meents et al tey et al some found a positive effect on q while others found no effect on q these conflicting results may be due to clonal variability which usually occurs in the rcho cell line development inducible overexpression of bcl xl which allows comparison of identical cell populations under different conditions did not affect q of rcho cells producing erythropoietin kim and lee nevertheless overexpression of bcl like proteins does not have a negative effect on q the beneficial effect of overexpression of anti apoptotic bcl like proteins is highlighted with q enhancing factors because the single use of some q enhancing factors such as sodium butyrate nabu and hyperosmolality has a detrimental effect on cell growth the combined use of antiapoptosis engineering with q enhancing factors e g nabu or hyperosmolality results in a dramatic increase in foreign protein production in rcho cell culture kim and lee kim et al sung et al we found that the maintenance of mitochondria is crucial in inhibiting apoptosis by overexpressing an anti apoptotic protein from these studies sung et al appl microbiol biotechnol 93 and fussenegger the effectiveness of targets on cell proliferation discovered via omics based approaches such as valosin containing protein vcp requiem alg and malate dehydrogenase ii have been evaluated chong et al doolan et al wong et al the details of omics based cho cell engineering will be discussed in a later section anti autophagy engineering cell cycle engineering autophagy also known as type ii pcd is a catabolic process that takes place through a caspase independent lysosomal mediated degradation pathway which is distinguished from apoptosis type i pcd levine and klionsky inhibition of apoptosis does not guarantee the blocking of autophagy mediated cell death due to the independency between the two processes until now only apoptosis has been the focus of studies on the alleviation of cell death in rcho cell culture recently the occurrence of autophagy at the end of batch culture of rcho cells was observed when the nutrients were exhausted hwang and lee inversely supplementation of nutrients could reduce cell death mediated by both apoptosis and autophagy han et al in addition the hyperosmotic condition introduced by feeding media and ph control during fed batch culture could induce apoptosis and autophagy in rcho cell culture han et al engineered cho cells with overexpressed bcl xl or a constitutively active form of akt could delay the autophagic cell death induced by nutrient depletion hwang and lee kim et al proliferation engineering successfully engineered rcho cells with proliferationrelated proteins showed an increased growth parameter μ and or maximum viable cell concentration although μ can influence the maximum viable cell concentration the extent of increase in one factor does not correlate with that of the other the rcho cell engineering conducted with cell cycle progression by cyclin dependent kinase like cyclin e and cell cycle transcription factor improved μ and or the maximum viable cell concentration jaluria et al majors et al renner et al a typical oncogenic protein c myc which also has a role in cell cycle progression was successfully applied to generate highly proliferative rcho cells with high μ and maximum viable cell concentration kuystermans and al rubeai recently mammalian target of rapamycin which is known to have various roles in translation vesicle traffic cell survival and cell proliferation was successfully introduced to rcho cells to improve key bioprocess relevant characteristics of rcho cells including proliferation and q dreesen a common feature of q enhanced conditions including physical low temperature and hyperosmolality and chemical nabu perturbations is cell cycle arrest sunley and butler based on this consensus and cell cycle regulating factors have been used in rcho cell engineering to obtain a similar effect fussenegger et al achieved the q enhancement by transiently introducing and into secreted alkaline phosphatase seap producing cho cells the use of cell cycle arrested rcho cells by stably overexpressing showed beneficial q enhancing effects on the production of seap and soluble intracellular adhesion molecule mazur et al meents et al although this enhancing mechanism is not fully understood the expanded metabolic consumption concomitant with increased consumption rates of oxygen glutamine glucose and an intracellular pool of amp adp atp may partly be affected carvalhal et al similarly overexpression can arrest the cell cycle and growth leading to improved q accompanied by increased cell volume and biogenesis of mitochondria and ribosomes bi et al the combinatorial strategy with growth enhancing factor bcl xl and its stabilizer c ebpα ccaat enhancer binding protein α resulted in a further enhancement of cell growth astley and al rubeai fussenegger et al chaperone engineering the bottleneck in enhancing q is believed to be a posttranslational step evidenced by the irrelevance of secreted protein level with increased gene copy number or mrna level mohan et al the endoplasmic reticulum er resident proteins have the major role in protein folding so these proteins mainly chaperone were thought to be a good resource for enhancing q numerous reports dealing with chaperone engineering have revealed mixed results depending on the overexpressed chaperones target therapeutic protein and expression system mohan et al engineered rcho cells with overexpression of and co overexpression of calnexin calreticulin exhibited q enhancing effect on thrombopoietin tpo production chung et al hwang et al overexpression of pdi the most studied chaperone for rcho cell engineering appl microbiol biotechnol 93 could enhance q depending on the target protein it appears to positively affect the antibody producing rcho cells but not the rcho cells producing tpo interleukin or a tumor necrosis factor receptor fc fusion protein borth et al davis et al mohan et al however in the recent report by hayes et al the transient expression of pdi family proteins pdi or pancreatic pdi pdip did not result in any improvement in the antibody productivity of rcho cells although this was not confirmed with stable rcho cells the pdi effectiveness may also depend on the q values of the rcho cells used the extent to which qab was enhanced by the application of low temperature decreased as q of the antibody producing rcho cells used increased yoon et al unfolded protein response based engineering the er has a major role in protein processing and the transmission of signals including signal transduction pathway for unfolded protein response upr to maintain cellular homeostasis ron and walter a common feature of upr signaling pathway is elevated gene transcription encoding various er resident chaperones the regulation of components in the upr signaling pathway has drawn attention in rcho cell engineering considering the important roles of chaperones as described in the earlier section x box binding protein xbp is a protein which has been widely explored in upr signaling pathway tigges and fussenegger revealed that xbp a spliced form of xbp overexpression can increase q of various therapeutic proteins which q is not increased by xbp an unspliced form of xbp in transient and stable expression studies similarly it could be applied to fed batch culture resulting in enhanced q and titer of antibody with physicochemical properties similar to those of non engineered cells becker et al furthermore recent reports suggested that this strategy is specifically effective when it is applied to rcho cells experiencing secretion bottleneck ku et al another possible strategy is to restore the attenuation of translation in response to er stress activating transcription factor one of the key regulators in upr system can return the signal of translational attenuation by phosphorylation of eukaryotic initiation factor via induction of growth arrest and dna damage inducible protein in two reports with and the engineered rcho cells with these proteins showed the enhanced q of antithrombin iii at iii ohya et al omasa et al interestingly overexpression of xbp failed to enhance production in at iii secretion engineering in vesicle trafficking soluble n ethylmaleimide sensitive factor attachment protein receptors snares have a central role in membrane fusion between transport vesicles and the target membrane toonen and verhage the interaction of sm proteins with snares is essential for regulating the snare mediated fusion event in an effort to increase q by relieving the bottleneck in secretion components related to the secretary pathway have been introduced for rcho cells snares snap 23 and and sm protein and based secretary engineering showed similar results peng et al peng and fussenegger the engineered rcho cells by these proteins could increase the antibody titer and specific antibody productivity qab by boosting the secretion although cell growth was negatively affected the secretion engineering strategy with ceramide transfer protein cert which efficiently transfers ceramide from the er to the golgi resulted in an increase in q mutant cert with increased transfer activity showed better enhancement of q than that achieved with wild type cert florin et al metabolic engineering accumulation of ammonia and lactate during rcho cell culture is a major concern because they adversely affect cell growth and product quality lao and toth yang and butler these two toxic waste products are generated from energy sources such as glutamine and glucose the rcho cells genetically modified with relevant metabolic proteins have been developed for efficient utilization of metabolites to reduce the ammonia produced from glutamine a less ammoniagenic substrate glutamate was used as a result rcho cells expressing gs which catalyzes glutamate with ammonia to yield glutamine was generated they showed less ammonia production in the glutamatebased culture zhang et al for a similar purpose carbamoyl phosphate synthetase i and ornithine transcarbamoylase involved in urea cycle were introduced to rcho cells resulting in decreased ammonia accumulation park et al wlaschin and hu evaluated the efficiency of the fructose specific transporter transfected rcho cells for utilizing fructose as an alternative to glucose and its effectiveness on lactate production although all individual clones could utilize fructose as a carbon source only the clone with an appropriate expression level exhibited reduced lactate production several kinds of genetic modulation strategies to change metabolic pathways related with pyruvate such as introducing pyruvate carboxylase or disrupting lactate dehydrogenase a pyruvate dehydrogenase kinases have been investigated kim and lee b zhou et al omics based approach in cell line development omics based approaches such as transcriptomics proteomics and metabolomics have been used in the whole process of developing rcho cell based production in both upstream and downstream processes they range from clone selection cell engineering culture media and culture environments in the upstream process to protein purification and characterization in the downstream process gupta and lee a potential gene of interest has been discovered from basic research so called direct cell engineering however with the recent advances in omics tools the detailed detection of global changes in dna rna protein and metabolites is possible in this section we will review the omicsbased approaches for cho cell engineering transcriptomics transcriptomics is the global analysis of genomic information at the transcription level by chasing mrna expression among various tools for genomic analysis dna microarray is probably the most efficient tool for the cell culture processes considering its massiveness and cost initially due to insufficient chinese hamster sequence information available for probe design non cho derived dna arrays including mouse or rat derived dna array were used and assessed regarding their feasibility for cho cells baik et al de leon gatti et al genomic resources have been mined through the bacterial artificial chromosome based library and expressed sequence tagbased library kantardjieff et al omasa et al as the use of defined sequences in proprietary and nonproprietary cho derived microarray has been increasing transcriptomic analysis has become popular in rcho cell culture the recent public availability of the genome sequence of cho will expedite transcriptomic study xu et al comparative transcriptome analysis using dna microarray in rcho cells has been done under q enhancing culture conditions including low culture temperature baik et al yee et al high osmotic condition shen et al and nabu treatment yee et al in addition similar approaches have been performed with different rcho cell lines with high μ doolan et al or high q nissom et al and recombinant human bone morphogenetic protein rhbmp producing rcho cells with pacesol overexpression doolan et al furthermore appl microbiol biotechnol 93 comparative transcriptomic analysis has been performed to find key factors regulating the apoptosis which occurs at the later stages of batch and fed batch cultures four apoptosisrelated genes fadd faim alg and requiem were identified as a result wong et al these identified proteins were applied to anti apoptotic cho cell engineering by their overexpression or knockdown wong et al nowadays a transcriptomic result is often combined with proteome changes to consider transcriptional changes and post transcriptional alteration baik et al doolan et al kantardjieff et al nissom et al yee et al another fascinating resource in transcriptomics is microrna mirna which is single stranded non coding rna 25 nucleotides in length and complementary to the mrna it can regulate global gene expression at the post transcriptional level by mrna cleavage or translational repression or both the mirna is an attractive alternative in cho cell engineering due to regulation of multiple targets easy introduction into cells and reduction in metabolic burden müller et al gammell et al have identified mir a growth inhibitory mirna as being up regulated during stationary phase growth induced either by temperature shift to low temperature or during normal batch culture similarly introduction of mir and let mirnas considered as significant change in batch culture of cells using mirna microarray could control the cell cycle via downregulating of cell cycle related proteins koh et al furthermore mir was found to be down regulated at low culture temperature and its overexpression could significantly increase q in cho cells at c barron et al with advances in mirna profiling technology the study of mirna to identify cellular targets for cho cell engineering will increase proteomics two dimensional gel electrophoresis de combined with mass spectrometric analysis is a widely used proteomic tool for identifying proteins changed under specific conditions in rcho cells as summarized in table to enhance q several strategies have been attempted in rcho cell culture through physical or chemical manipulation of environmental perturbations o callaghan and james temperature and osmolarity are the key factors in the cell culture process it is well known that rcho cells under hypothermia and hyperosmolarity show enhanced q to understand the intracellular events proteomic studies have been investigated under these conditions baik et al kantardjieff et al kaufmann et al kumar et al lee et al similarly the effects of various chemical components such as nabu dmso zinc sulfate appl microbiol biotechnol 93 table comparative proteomics under specific conditions in rcho cells comparison product culture condition reference temperature shift 30 c temperature shift c temperature shift c seap epo serum adherent t flask batch serum adherent t flask batch serum suspension spinner flask batch kaufmann et al baik et al kumar et al temperature shift c mm butyrate antibody sfm suspension shaking flask batch kantardjieff et al hyperosmotic mosm kg antibody serum adherent t flask batch lee et al 0 5 mm butyrate μm zinc sulfate 5 μg ml tunicamycin hgh sfm adherent t flask batch van dyk et al mm butyrate mm butyrate 5 dmso cytochalasin d insulin or bfgf hydrolysate htpo ifn γ hbsag seap antibody sfm suspension shaker flask batch serum adherent t flask batch serum adherent t flask batch serum adherent t flask batch serum adherent t flask batch sfm suspension shaking flask batch baik et al yee et al li et al hayduk and lee lee et al kim et al 3 mm butyrate bcl xl overexpression epo sfm suspension bioreactor batch baik and lee bcl xl overexpression 1 overexpression pacesol overexpression rhbmp sfm suspension bioreactor fed batch serum adherent t flask batch cdm suspension shaking flask batch carlage et al lee et al meleady et al c myc overexpression with different metabolic profiles with different growth rates with different media compositions antibody antibody epo serum adherent t flask batch sfm suspension bioreactor fed batch cdm suspension shaking flask batch sfm suspension shaker flask batch kuystermans et al pascoe et al doolan et al baik et al tunicamycin cytochalasin d and growth factors which increase μ and or q on the changes of rcho proteome also have also been studied baik et al hayduk and lee kantardjieff et al lee et al li et al van dyk et al yee et al recently differently expressed proteins of rcho cells cultivated in sf medium supplemented with optimized hydrolysates mixtures yielding the highest μ or the highest qab were identified by comparative proteomics kim et al the proteome in rcho cells engineered by overexpressing and or down regulating an effecter protein may be altered proteomic approaches to chase global protein changes by bclxl overexpression have been performed to determine the key components baik and lee carlage et al influences on rcho proteome by overexpression of which has a positive function in the cell proliferation have been evaluated lee et al in addition influences on rcho proteome by the overexpression of soluble paired basic amino acid cleaving enzyme pacesol which affects q of rhbmp have also been evaluated meleady et al recently the differentially expressed intracellular proteins by overexpressing c myc known to be a typical oncogenic protein and enhanced integral viable cell density were investigated kuystermans et al proteomic results have been successfully applied to rcho cell engineering pascoe et al analyzed the protein changes by comparing two different antibody producing rcho cells with different lactate profiles in the same fed batch culture condition similarly the proteome of rcho cells with high μ was compared with that of cells with low μ the proteomic result along with the transcriptomic result could provide a potential candidate for the enhancement of cell growth doolan et al the overexpression of the identified protein vcp could improve cell concentration without affecting cell viability although the effectiveness depended on the clone recently baik et al evaluated the intracellular responses in rcho cells adapted to grow in serum free suspension culture the overexpression of the identified protein heat shock protein kda and or kda showed 10 to enhanced cell concentration during serum free adaptation and to reduction of adaptation time in conventional de a quantitative comparison of the proteins on different gels is challenging due to the high degree of gel to gel variations arising from the heterogeneity of polyacrylamide gel in preparation and fluctuations due to changes in electrical current ph and temperature during the procedure in addition staining methods especially silver staining also contribute to this variation to overcome the gel to gel variation in conventional de two dimensional differential in gel electrophoresis 2 d dige was developed timms and cramer generally the most widely used labeling reagents in the 2 d dige are synthetic n hydroxysuccinimidyl nhs ester derivatives of cyanine dyes and nhs nhs and nhs the mixture of and labeled test samples and internal standard are resolved on a single 2 d gel and three images of the gel are compared the 2 d dige technique has recently been applied to rcho cells to evaluate the altered protein expression level under various culture conditions baik and lee doolan et al kumar et al meleady et al metabolomics another omics based approach attracting attention in rcho cell culture is metabolomics which is coupled with nuclear magnetic resonance nmr or mass spectrometry ms to chase low molecular endogenous metabolites in general the extent of changes in metabolite is significant compared with the extent of changes in dna rna and protein however unlike the dna rna and protein the qualitative analysis system for metabolite has not been fully established yet easy amplification of a sample for dna rna and the existence of specific probe antibody for protein are benefits for qualitative analysis in contrast there is a limitation to analysis of metabolites due to non amplicable characteristics and the lack of public probes for them despite the technical difficulty there are reports to identify crucial extracellular metabolites or to chase intracellular metabolite flux for media development and apoptosis reduction using liquid chromatography ms based or nmr spectroscopybased metabolomics in rcho cell culture bradley et al chong et al goudar et al among them chong et al demonstrated the typical metabolomics based cho cell engineering they found that malate accumulation was most significant in the medium of rcho fed batch culture subsequent cell engineering to overexpress malate dehydrogenase ii resulted in significant growth improvement in ivcc appl microbiol biotechnol 93 concluding remarks significant improvement in therapeutic protein production of rcho cells has been achieved with the development of cell line and culture processes involving media development in particular the establishment of a highly and stably producing cell line is of utmost importance considering the heterogeneity and different responses to the various environments of rcho cells this review focuses on current achievements in vector engineering and cell engineering in cell line development the combinatorial cell engineering strategy with regulation of multiple targets is becoming popular in the same context the use of mirna in cell engineering will increase in the future because mirna can regulate the global gene expression and can easily be introduced into cells in addition combinatorial analysis of transcriptomics with proteomics will enter into general use to compensate the transcriptional changes as well as post transcriptional alteration although the application of vector engineering and cell engineering to rcho cell culture has various benefits the limited success to date in its application may be due to the insufficient genomic information of rcho cells in this regard a recent report dealing with cho genomic sequence can lead to the greatest progression in the current strategy of cell line development xu et al the finding of hot spots and better understanding of its neighboring sequence may help to generate high producers as described in the previous bioinformatics analysis for identifying it in the human genome using a computational method girod et al in addition the complete genomic information of cho cells will be helpful in the identification step in omics based approaches in order to understand the functioning of organisms on the molecular level we need to know which genes are expressed when and where in the organism and to which extent the regulation of gene expression is achieved through genetic regulatory systems structured by networks of interactions between dna rna proteins and small molecules as most genetic regulatory networks of interest involve many components connected through interlocking positive and negative feedback loops an intuitive understanding of their dynamics is hard to obtain as a consequence formal methods and computer tools for the modeling and simulation of genetic regulatory networks will be indispensable this paper reviews formalisms that have been employed in mathematical biology and bioinformatics to describe genetic regulatory systems in particular directed graphs bayesian networks boolean networks and their generalizations ordinary and partial differential equations qualitative differential equations stochastic equations and rule based formalisms in addition the paper discusses how these formalisms have been used in the simulation of the behavior of actual regulatory systems key words genetic regulatory networks mathematical modeling simulation computational biology introduction the genome of an organism plays a central role in the control of cellular processes such as the response of a cell to environmental signals the differentiation of cells and groups of cells in the unfolding of developmental programs and the replication of the dna preceding cell division proteins synthesized from genes may function as transcription factors binding to regulatory sites of other genes as enzymes catalyzing metabolic reactions or as components of signal transduction pathways with few exceptions all cells in an organism contain the same genetic material this implies that in order to understand how genes are implicated in the control of intracellular and intercellular processes the scope should be broadened from sequences of nucleotides coding for proteins to regulatory systems determining which genes are expressed when and where in the organism and to which extent gene expression is a complex process regulated at several stages in the synthesis of proteins lewin apart from the regulation of dna transcription the best studied form of regulation the expression institut national de recherche en informatique et en automatique inria unité de recherche rhône alpes avenue de l europe montbonnot saint ismier cedex france downloaded by university of alberta library from www liebertpub com at for personal use only de jong fig example of a genetic regulatory system consisting of a network of three genes a b and c repressor proteins a b c and d and their mutual interactions the gure distinguishes several types of interaction more complex graphical conventions to represent cellular networks are proposed by kohn of a gene may be controlled during rna processing and transport in eukaryotes rna translation and the posttranslational modi cation of proteins the degradation of proteins and intermediate rna products can also be regulated in the cell the proteins ful lling the above regulatory functions are produced by other genes this gives rise to genetic regulatory systems structured by networks of regulatory interactions between dna rna proteins and small molecules an example of a simple regulatory network involving three genes that code for proteins inhibiting the expression of other genes is shown in fig proteins b and c independently repress gene a by binding to different regulatory sites of the gene while a and d interact to form a heterodimer that binds to a regulatory site of gene b binding of the repressor proteins prevents rna polymerase from transcribing the genes downstream analyses of the huge amounts of data made available by sequencing projects have contributed to the discovery of a large number of genes and their regulatory sites the kegg database for instance contains information on the structure and function of about genes for species kanehisa and goto in some cases the proteins involved in the control of the expression of these genes as well as the molecular mechanisms through which regulation is achieved have been identi ed much less is known however about the functioning of the regulatory systems of which the individual genes and interactions form a part brownstein et al collado vides et al fields et al kanehisa lander loomis and sternberg nowak palsson strohman thieffry gaining an understanding of the emergence of complex patterns of behavior from the interactions between genes in a regulatory network poses a huge scienti c challenge with potentially high industrial pay offs the study of genetic regulatory systems has received a major impetus from the recent development of experimental techniques like cdna microarrays and oligonucleotidechips which permit the spatiotemporal expression levels of genes to be rapidly measured in a massively parallel way brown and botstein lipschutz et al lockhart and winzeler other techniques such as the mass spectrometric identi cation of gel separated proteins allow the state of a cell to be characterized on the proteomic level as well kahn mann pandey and mann zhu and snyder although still in their infancy these techniques have become prominent experimental tools by opening up a window on the dynamics of gene expression in addition to experimental tools formal methods for the modeling and simulation of gene regulation processes are indispensable as most genetic regulatory systems of interest involve many genes connected a notational convention names of genes are printed in italic and names of proteins and other molecules start with a capital downloaded by university of alberta library from www liebertpub com at for personal use only genetic regulatory systems fig analysis of genetic regulatory systems the boxes represent activities the ovals information sources and the arrows information ows through interlocking positive and negative feedback loops an intuitive understanding of their dynamics is hard to obtain using formal methods the structure of regulatory systems can be described unambiguously while predictions of their behavior can be made in a systematic way especially when supported by userfriendly computer tools modeling and simulation methods permit large and complex genetic regulatory systems to be analyzed figure shows the combined application of experimental and computational tools starting from an initial model suggested by knowledge of regulatory mechanisms and available expression data the behavior of the system can be simulated for a variety of experimental conditions comparing the predictions with the observed gene expression pro les gives an indication of the adequacy of the model if the predicted and observed behavior do not match and the experimental data is considered reliable the model must be revised the activities of constructing and revising models of the regulatory network simulating the behavior of the system and testing the resulting predictions are repeated until an adequate model is obtained the formal basis for computer tools supporting the modeling and simulation tasks in fig lies in methods developed in mathematical biology and bioinformatics since the with some notable precursors in the two preceding decades a variety of mathematical formalisms for describing regulatory networks have been proposed these formalisms are complemented by simulation techniques to make behavioral predictions from a model of the system as well as modeling techniques to construct the model from experimental data and knowledge on regulatory mechanisms traditionally the emphasis has been on simulation techniques where the models are assumed to have been hand crafted from the experimental literature with more experimental data becoming available and easily accessible through databases and knowledge bases modeling techniques are currently gaining popularity this paper gives an overview of formalisms to describe genetic regulatory networks and discusses their use in the modeling and simulation of regulatory systems while it was being prepared several other reviews on the modeling and simulation of genetic regulatory systems appeared endy and brent hasty et al mcadams and arkin smolen et al see glass rosen thomas for earlier reviews the present report differs from these reviews in that it focuses on the mathematical methods evaluating their relative strengths and weaknesses rather than on the biological results obtained through their application recently a collection of introductory chapters covering some of the methods discussed below appeared bower and bolouri sections to of this paper give an overview of formalisms proposed in the literature and discuss modeling and simulation techniques appropriate for each of the formalisms formalisms to be discussed include directed graphs bayesian networks boolean networks and their generalizations ordinary and partial differential equations qualitative differential equations stochastic master equations and rule based formalisms it will come as no surprise that the review is not meant to be exhaustive as hinted at above the computational study of gene regulation is a subject with a long history moreover in the last few years the number of papers seems to be growing in an exponential fashion to mention some omissions no attention is paid to petri nets goss and peccoud hofestädt and thelen matsuno et al downloaded by university of alberta library from www liebertpub com at for personal use only de jong reddy et al transformational grammars collado vides collado vides et al and process algebra regev et al the impact of these approaches on the dynamic analysis of genetic regulatory networks has been limited thus far moreover they are related to some of the other formalisms discussed below directed and undirected graphs probably the most straightforward way to model a genetic regulatory network is to view it as a directed graph a directed graph g is de ned as a tuple hv ei with v a set of vertices and e a set of edges a directed edge is a tuple hi j i of vertices where i denotes the head and j the tail of the edge the vertices of a directed graph correspond to genes or other elements of interest in the regulatory system while the edges denote interactions among the genes the graph representation of a regulatory network can be generalized in several ways the vertices and edges could be labeled for instance to allow information about genes and their interactions to be expressed by de ning a directed edge as a tuple hi j si with equal to c or it can be indicated whether i is activated or inhibited by j hypergraphs can be used to deal with situations in which proteins cooperatively regulate the expression of a gene for instance by forming a heterodimer fig the edges are then de ned by hi j si where j represents a list of regulating genes and s a corresponding list of signs indicating their regulatory in uence figure shows simple regulatory networks of three genes current databases and knowledge bases containing information about regulatory interactions can be viewed to a large extent as richly annotated graph representations examples of such databases and knowledge bases are amaze van helden et al ecocyc karp et al genenet kolpakov et al genet samsonova et al gif db jacq et al kegg kanehisa and goto knife euzenat et al and regulondb salgado et al genenet describes cell types and lineages genes with their regulatory features proteins and protein complexes regulatory interactions and other chemical reactions physiological conditions under which the interactions have been observed and primary literature sources the databases and knowledge bases are usually supplemented by applications to compose and edit networks by selecting and manipulating individual interactions at the very least they fig a directed graph representing a genetic regulatory network and b its de nition the plus and minus symbols in the pictorial representation can be omitted and replaced by and a edges respectively c d directed hypergraph representation of a regulatory network with cooperative interactions downloaded by university of alberta library from www liebertpub com at for personal use only genetic regulatory systems permit the user to visualize complete or partial networks often on different levels of detail and to navigate the networks samsonova and serov sanchez et al some current databases and knowledge bases integrate metabolic and signal transduction pathways with gene regulation processes e g kanehisa and goto and karp et al a number of operations on graphs can be carried out to make biologically relevant predictions about regulatory systems a search for paths between two genes for instance may reveal missing regulatory interactions or provide clues about redundancy in the network furthermore cycles in the network point at feedback relations that are important for homeostasis and differentiation section global connectivity characteristics of a network such as the average and the distribution of the number of regulators per gene give an indication of the complexity of the network loosely connected subgraphs point at functional modules of the regulatory system of which the behavior could be considered in isolation from a comparison of regulatory networks of different organisms it might be possible to establish to which extent parts of regulatory networks have been evolutionary conserved the graph models to which the above operations are applied may be composed from information stored in databases and knowledge bases but they can also be obtained from gene expression data through inductive or reverse engineering approaches a variety of clustering algorithms have been used to group together genes with similar temporal expression patterns alon et al ben dor et al brown et al cho et al eisen et al holter et al michaels et al spellman et al tamayo et al wen et al several techniques for clustering expression data time series have been proposed in the literature based on measures like euclidian distance mutual information linear correlation and rank correlation d haeseleer et al the use of clustering algorithms is motivated by the idea that two genes exhibiting similar expression patterns over time may regulate each other or be coregulated by a third gene additional analyses may permit one to extract more information on putative regulatory connections between coexpressed genes in the graph such as the analysis of time lags arkin and ross arkin et al chen et al the performance of perturbation experiments holstege et al hughes et al laub et al spellman et al the mapping of co expressed genes to pathway functions kanehisa and goto zien et al and the screening of upstream regions of coexpressed genes for shared promoters or regulatory sites laub et al spellman et al tavazoie et al bayesian networks in the formalism of bayesian networks friedman et al pearl the structure of a genetic regulatory system is modeled by a directed acyclic graph g d hv ei fig the vertices i v i n represent genes or other elements and correspond to random variables xi if i is a gene then xi will describe the expression level of i for each xi a conditional distribution p xi j parents xi is de ned where parents xi denotes the variables corresponding to the direct regulators of i in g the fig example bayesian network consisting of a graph conditional probability distributions for the random variables the joint probability distribution and conditional independencies friedman et al downloaded by university of alberta library from www liebertpub com at for personal use only de jong graph g and the conditional distributions p xi j parents xi together de ning the bayesian network uniquely specify a joint probability distribution p x let a conditional independency i xi i y j z express the fact that xi is independent of y given z where y and z denote sets of variables the graph encodes the markov assumption stating that for every gene i in g i xii nondescendants xi j parents xi by means of the markov assumption the joint probability distribution can be decomposed into p x d yn p xi j parents xi the graph implies additional conditional independencies as shown in fig for the example network two graphs and hence two bayesian networks are said to be equivalent if they imply the same set of independencies the graphs in an equivalence class cannot be distinguished by observation on x equivalent graphs can be formally characterized as having the same underlying undirected graph but may disagree on the direction of some of the edges see friedman et al for details and references given a set of expression data d in the form of a set of independent values for x learning techniques for bayesian networks heckerman allow one to induce the network or rather the equivalence class of networks that best matches d basically the techniques rely on a matching score to evaluate the networks with respect to the data and search for the network with the optimal score as this optimization problem is known to be np hard heuristic search methods have to be used which are not guaranteed to lead to a globally optimal solution as an additional problem currently available expression data underdetermines the network since at best a few dozen of experiments provide information on the transcription level of thousands of genes friedman and colleagues friedman et al have proposed a heuristic algorithm for the induction of bayesian networks from expression data that is able to deal with this so called dimensionality problem instead of looking for a single network or a single equivalence class of networks they focus on features that are common to high scoring networks in particular they look at markov relations and order relations between pairs of variables xi and xj a markov relation exists if xi is part of the minimal set of variables that shields xj from the rest of the variables while an order relation exists if xi is a parent of xj in all of the graphs in an equivalence class an order relation between two variables may point at a causal relationship between the corresponding genes statistical criteria to assess the con dence in the features have been developed a recent extension of the method pe er et al is able to deal with genetic mutations and considers additional features like activation inhibition and mediation relations between variables markov relations and order relations have been studied in an application of the algorithm to the cell cycle data set of spellman and colleagues see pe er et al for another application this data set contains measurements of the mrna expression level of s cerevisiae orfs included in time series obtained under different cell cycle synchronization methods the bayesian induction algorithm has been applied to the genes whose expression level varied over the cell cycle by inspecting the high con dence order relations in the data friedman and colleagues found that only a few genes dominate the order which indicates that they are potential regulators of the cell cycle process many of these genes are known to be involved in cell cycle control and initiation of the high con dence markov relations most pairs are functionally related some of these relations were not revealed by the cluster analysis of spellman and colleagues a bayesian network approach towards modeling regulatory networks is attractive because of its solid basis in statistics which enables it to deal with the stochastic aspects of gene expression and noisy measurements in a natural way moreover bayesian networks can be used when only incomplete knowledge about the system is available although bayesian networks and the graph models in the previous sections are intuitive representations of genetic regulatory networks they have the disadvantage of leaving dynamical aspects of gene regulation implicit to some extent this can be overcome through generalizations like dynamical bayesian networks which allow feedback relations between genes to be modeled murphy and mian in the next sections formalisms that explicitly take into account the dynamics of regulatory systems will be discussed downloaded by university of alberta library from www liebertpub com at for personal use only genetic regulatory systems boolean networks as a rst approximation the state of a gene can be described by a boolean variable expressing that it is active on or inactive off and hence that its products are present or absent moreover interactions between elements can be represented by boolean functions which calculate the state of a gene from the activation of other genes the result is a boolean network an example of which is shown in fig modeling regulatory networks by means of boolean networks has become popular in the wake of a groundbreaking study by kauffman see sugita and walter et al for early uses of the boolean approximation recent reviews of the use of boolean network models can be found in kauffman book and in a number of articles huang kauffman somogyi and sniegoski let the n vector x of variables in a boolean network represent the state of a regulatory system of n elements each oxi has the value or so that the state space of the system consists of states the state oxi of an element at time point t is computed by means of a boolean function or rule obi from the state of k of the n elements at the previous time point t notice that k may be different for each oxi the variable oxi is also referred to as the output of the element and the k variables from which it is calculated the inputs for k inputs the total number of possible boolean functions obi mapping the inputs to the output is this means that for k d there are possible functions including the nand or and nor in fig in summary the dynamics of a boolean network describing a regulatory system are given by oxi t c d obi x t i n where obi maps k inputs to an output value the structure of a boolean network can be recast in the form of a wiring diagram fig c the upper row lists the state at t and the lower row the state at t c while the boolean function calculating the output from the input is shown below each element the wiring diagram is a convenient representation for computing transitions between states the transition from one state to the next is usually determined in a parallel fashion applying the boolean function of each element to its inputs for instance given a state vector at t d the system in the example will move to a state at the next time point t d that is if all three genes are inactive at t d the second and the third gene will become active at the next time point hence transitions between states in a network are deterministic with a single output state for a given input and synchronous in the sense that the outputs of the elements are updated simultaneously a sequence of states connected by transitions forms a trajectory of the system because the number of states in the state space is nite the number of states in a trajectory will be nite as well more speci cally all initial states of a trajectory will eventually reach a steady state or a state cycle also referred to as fig a example boolean network and b the corresponding equations in this case n d and k d c wiring diagram of the boolean network downloaded by university of alberta library from www liebertpub com at for personal use only de jong point attractor or dynamic attractor respectively the states that are not part of an attractor are called transient states the attractor states and the transient states leading to the attractor together constitute the basin of attraction in the example both attractors have a basin of attraction consisting of four states for simple networks the attractors and their basins of attraction in the state space can be calculated by hand but for larger systems computer programs become inevitable an example of such a program is ddlab developed by wuensche given suf cient memory ddlab allows the basins of attraction of a network to be calculated up to n d individual basins of attraction can be determined for much larger networks another example is the program gene o matic platzer et al which permits differentiation in multicellular systems to be simulated jackson et al boolean networks were among the rst formalisms for which model induction methods were proposed the reveal algorithm developed by liang et al being an example see akutsu et al ideker et al karp et al maki et al noda et al for other examples and thomas for seminal ideas in outline this algorithm uses information theory to establish how given elements are connected in the network and then determines the functions that specify the logic of the interactions from the data for each of the n elements all possible combinations of k inputs are considered k n until a set of inputs is found which fully determines the output of the element in information theoretic terms this means that the mutual information of the output of the element and its inputs equals the information value of the output of the element alone the function for the element is then found by comparing the state transitions in the observed trajectory with the boolean function de nitions an implementation of the algorithm proved to be able to reliably reproduce networks with n d and k d given state transition pairs out of the possible pairs examples of boolean network models of simple regulatory systems can be found in kauffman an interesting application of boolean networks is their use in the study of global properties of large scale regulatory systems see kauffman kauffman somogyi and sniegoski szallasi and liang weisbuch for reviews the basic idea is to generate random boolean networks with local properties that hold for all members of a class of systems properties of interest are for example the number k of regulators of a gene and the type of functions obi through which the regulators in uence gene expression by locating the attractors trajectories and basins of attraction in the state space one can systematically investigate the implications of the local properties for the global dynamics of the networks this approach has been explored by kauffman see kauffman for later work summarized in kauffman he randomly connected each of the n elements in the network to k inputs and randomly selected the boolean function obi computing the output of the element from among the possible functions analysis of networks of up to elements showed that for low k and certain choices of regulatory functions the systems exhibited highly ordered dynamics for example the expected median number of attractors was empirically found to be about pn the square root of the number of elements this means that a network of elements would be expected to have only state cycles and steady states moreover the length of the attractors was found to be restricted as well also proportional to pn interpreting an attractor of the boolean network as a pattern of gene expression corresponding to a cell type kauffman argued that the square root dependence on n of the number of attractors is in accordance with the observation that the number of cell types seems to grow with the square root of the number of genes the high degree of order observed in large random boolean networks has led kauffman to venture that once living systems with certain local properties have evolved under the pressure of natural selection ordered global dynamics necessarily follows kauffman boolean networks allow large regulatory networks to be analyzed in an ef cient way by making strong simplifying assumptions on the structure and dynamics of a genetic regulatory system in the boolean network formalism a gene is considered to be either on or off and intermediate expression levels are neglected also transitions between the activation states of the genes are assumed to occur synchronously when transitions do not take place simultaneously as is usually the case certain behaviors may not be predicted by the simulation algorithm there are situations in which the idealizations underlying boolean networks are not appropriate and more general methods are required ideas have been explored by means of continuous formalisms discussed in later sections alves and savageau bagley and glass glass and hill kauffman newman and rice downloaded by university of alberta library from www liebertpub com at for personal use only genetic regulatory systems generalized logical networks in this section a generalized logical method developed by thomas and colleagues will be discussed the method is based on a formalism that generalizes upon boolean networks in that it allows variables to have more than two values and transitions between states to occur asynchronously since its original conception the method has undergone several extensions the version that will be presented here in outline is more extensively described by thomas thomas and d ari thomas et al see thomas for earlier formulations the formalism of thomas and colleagues uses discrete variables oxi so called logical variables that are abstractions of real concentrations variables xi the possible values of oxi are de ned by comparing the concentrations xi with the thresholds of the in uence of i on other elements of the regulatory system if an element i in uences p other elements of the regulatory system it may have as many as p distinct thresholds van ham i i p i given these thresholds oxi has the possible values pg and is de ned as follows oxi d if xi i oxi d if i xi i oxi d p if xi p i the vector x denotes the logical state of the regulatory system the pattern of regulatory interactions in the system is described by logical equations of the form xo i t d boi x t i n where xo i is called the image of xoi the image is the value towards which xoi tends when the logical state of the system is x the image of oxi is not necessarily its successor value and should therefore be distinguished from ox t c in the logical function obi is a generalization of the boolean function in since the logical variables now have more than two possible values the logical function computes the image of oxi from the logical state of the system more speci cally from the value of k of its n elements in fig an example regulatory network is shown gene regulates genes and so that it has two thresholds and the corresponding logical variable takes its value from similarly and have one and two thresholds respectively and hence possible values and each edge in the graph has been labeled with the rank number of the threshold as well as the sign of the regulatory in uence for instance the label from gene to gene means that inhibits above its second threshold that is when d the graph in fig a can be transformed into logical equations as shown in fig b the functions need to be speci ed such as to be consistent with the threshold restrictions in the graph examples of logical functions allowed by the method are shown in fig c in a notation that emphasizes the correspondence with the boolean functions in the previous section see thomas and d ari thomas et al for a full account of the speci cation of logical functions consider the case of if and so that and have values above their rst threshold the inhibitory in uences of genes and on gene become operative figure c indicates that will tend to that is below the rst threshold of the protein produced by gene if either d or d that is if only one of the inhibitory in uences is operative then gene is moderately expressed this is here represented by the value for the image of in general several logical functions will be consistent with the threshold restrictions exactly which logical function is chosen may be motivated by biological considerations or may be a guess re ecting uncertainty about the structure of the system being studied downloaded by university of alberta library from www liebertpub com at for personal use only de jong fig a example regulatory network in graph notation where the edges are labeled to express the rank number of the threshold as well as the sign of the in uence thomas et al b logical equations corresponding to the graph and c possible de nitions of the logical functions the logical equations form the input for the analysis of the regulatory system in particular the determination of its logical steady states a logical steady state occurs when the logical state of the system equals its image i e x d x since the number of logical states is nite due to the discretization achieved by the introduction of logical variables one can exhaustively test for logical steady states in the case of the logical equations in fig the logical state is the only steady state among the possible logical states the other states are transient logical states if the regulatory system is in a transient logical state it will make a transition to another logical state thomas thomas and d ari since a logical value will move into the direction of its image the possible successor states of a logical state can be deduced by comparing the value of a logical variable with that of its image with the assumption that two logical variables will not change their value simultaneously a maximum of n successor states can be reached from a current state of course if a logical state is steady it has no successor states since the logical variables equal their image the logical states and the transitions among them can be organized in a state transition graph in more advanced analyses of state transitions time delays occasioned by transcription translation and transport can be taken into account thomas thomas and d ari the generalized logical method outlined above has been generalized in several ways notably by the introduction of logical parameters and threshold values for the logical variables logical parameters snoussi thomas and d ari allow classes of logical functions as compared with individual logical functions to be characterized this facilitates the identi cation of logical steady states by introducing logical values that correspond to threshold values in a continuous description snoussi and thomas thomas and d ari it becomes possible to detect additional logical steady states the resulting method integrates elements from the differential equation formalisms discussed in the next sections snoussi thomas and d ari the logical method of thomas has been implemented thieffry et al and its effectiveness demonstrated in the study of a number of genetic regulatory systems of limited scale like phage infection in e coli thieffry and thomas thomas and d ari thomas et al and dorso ventral pattern formation and gap gene control in drosophila sánchez and thieffry sánchez et al another example is a study of the regulatory network controlling ower morphogenesis in arabidopsis thaliana mendoza et al see also mendoza and alvarez buylla in this case a regulatory downloaded by university of alberta library from www liebertpub com at for personal use only genetic regulatory systems network involving ten genes has been derived from published genetic and molecular data and reduced to two subnetworks of two and four genes respectively after choosing appropriate logical functions the system can be shown to have six logical steady states four steady states correspond to the patterns of gene expression found in the oral organs of the plant sepals petals stamens carpels whereas a fth accounts for a non oral state the sixth steady state has not been characterized experimentally thus far consideration of the state transition graph and constraints on the logical parameters leads to the prediction that the gene lfy must have at least one more regulator to account for the transition from the non oral steady state to one of the four owering states nonlinear ordinary differential equations being arguably the most widespread formalism to model dynamical systems in science and engineering ordinary differential equations odes have been widely used to analyze genetic regulatory systems the ode formalism models the concentrations of rnas proteins and other molecules by time dependent variables with values contained in the set of nonnegative real numbers regulatory interactions take the form of functional and differential relations between the concentration variables more speci cally gene regulation is modeled by rate equations expressing the rate of production of a component of the system as a function of the concentrations of other components rate equations have the mathematical form dxi dt d fi x i n where x d xn is the vector of concentrations of proteins mrnas or small molecules and fi rn r a usually nonlinear function the rate of synthesis of i is seen to be dependent upon the concentrations x possibly including xi and fi can be extended to include concentrations u of input components e g externally supplied nutrients discrete time delays arising from the time required to complete transcription translation and diffusion to the place of action of a protein can also be represented dxi dt d fi t xn t in i n where in denote discrete time delays alternatively integrals can be used to model distributed time delays landahl mahaffy macdonald smolen et al powerful mathematical methods for modeling biochemical reaction systems by means of rate equations have been developed in the past century especially in the context of metabolic processes see cornish bowden heinrich and schuster and voit for introductions using these methods kinetic models of genetic regulation processes can be constructed by specifying the functions fi in fig a kinetic model of a simple genetic regulation process is shown going back to early work by goodwin the end product of a metabolic pathway co inhibits the expression of a gene coding for an enzyme that catalyzes a reaction step in the pathway this gives rise to a negative feedback loop involving the mrna concentration the enzyme concentration and the metabolite concentration more generally equations of the form dt d xn dxi dt d i i ixi i n are employed tyson and othmer the parameters n n are production constants and n degradation constants the rate equations express a balance between the number of molecules appearing and disappearing per unit time in the case of the production term involves a nonlinear regulation function r r r whereas the concentrations xi i n increase linearly with xi in order to express that the metabolic product is a co repressor of the gene r needs to be a decreasing downloaded by university of alberta library from www liebertpub com at for personal use only de jong fig a example of a genetic regulatory system involving end product inhibition and b its ode model adapted from goodwin a is an enzyme and c a repressor protein while k and f are metabolites and represent the concentrations of mrna a protein a and metabolite k respectively are production constants degradation constants and r r r a decreasing nonlinear regulation function ranging from to function i e r xn the terms ixi i n state that the concentrations xi decrease through degradation diffusion and growth dilution at a rate proportional to the concentrations themselves a regulation function often found in the literature is the so called hill curve fig a hc xj μj m d xm j xm j c μm j with μj the threshold for the regulatory in uence of xj on a target gene and m a steepness parameter the function ranges from to and increases as xj so that an increase in xj will tend to increase the expression rate of the gene activation in order to express that an increase in xj decreases the expression rate inhibition as in the regulation function hc xj μj m is replaced by h xj μj m d hc xj μj m for m hill curves have a sigmoid shape in agreement with experimental evidence yagil yagil and yagil figure shows two further examples of regulation functions that will be discussed in more detail in a later section due to the nonlinearity of fi analytical solution of the rate equations is not normally possible in special cases qualitative properties of the solutions such as the number and the stability of steady fig examples of regulation functions a hill function hc b heaviside or step function sc and c logoid function lc downloaded by university of alberta library from www liebertpub com at for personal use only genetic regulatory systems states and the occurrence of limit cycles can be established this is illustrated by studies that investigate the relationship between single feedback loops and the dynamics of regulatory systems e g cherry and adler goodwin 1965 grif th keller othmer tyson walter wolf and eeckman reviewed in smolen et al tyson and othmer in the case of a negative feedback loop as in fig the system may approach or oscillate around a single steady state in the presence of a positive feedback loop on the contrary the system tends to settle in one of two stable states depending on whether the initial state is on one or the other side of a separatrix in fact a negative feedback loop is a necessary condition for stable periodicity and a positive feedback loop for multistationarity gouzé plahte et al snoussi thomas thomas and kaufman following seminal ideas in delbrück and monod and jacob thomas has drawn attention to the relation between the feedback structure of a system and the biological phenomena of homeostasis and differentiation thieffry et al thomas thomas and d ari in particular the stable periodicity of negative feedback loops can be interpreted as representing homeostasis whereas the multistationarity occasioned by a positive feedback loop provides a suggestive parallel to differentiation processes observed in development one way to work around the refractoriness of nonlinear rate equations to mathematical analysis is to simplify the models an approach that will be discussed in section alternatively one can take recourse to numerical techniques in numerical simulation the exact solution of the equations is approximated by calculating approximate values xm for x at consecutive time points tm lambert a variety of computer tools speci cally adapted to the simulation and analysis of biochemical reaction systems are available such as dbsolve goryanin et al gepasi mendes mist ehlde and zacchi and scamp sauro numerical simulations of a few well studied regulatory systems have been carried out for instance reinitz and vaisnys discuss a numerical model of the cro ci switch controlling the growth of phage in e coli they study the switch when certain genes have been mutated causing it to be functionally isolated from the rest of the phage genome another simulation of the phage system is described by macadams and shapiro see also ackers et al and shea and ackers and the review in mcadams and arkin an interesting feature of this work is the parallel drawn between genetic regulatory networks and electrical circuits resulting in a hybrid approach that integrates biochemical kinetic modeling within the framework of circuit simulation other well known examples of genetic regulation processes for which numerical models have been developed include the induction of the lac operon in e coli bliss et al carrier and keasling mahaffy sanglier and nicolis wong et al the developmental cycle of bacteriophage endy et al you and yin the synthesis of trp in e coli koh et al prokudina et al santillán and mackey xiu et al the expression of a human immunode ciency virus hiv hammond and circadian rhythms in drosophila and other organisms goldbeter leloup and goldbeter ruoff et al ueda et al simulation of the functioning of a regulatory network is often complemented by bifurcation analysis tools to investigate the sensitivity of steady states and limit cycles to parameter values strogatz borisuk and tyson have applied these techniques to a numerical model of a much studied example of posttranslational modi cation the control of mitosis in the xenopus oocyte novak and tyson see novak and tyson for a variant of this model the model consists of ten odes describing the network of biochemical reactions regulating m phase promoting factor mpf a protein triggering a number of key events of mitosis by introducing additional assumptions about the biochemical properties of the system the m phase control model can be simpli ed to just two odes for the concentrations of active mpf and its regulatory subunit cyclin without losing essential qualitative features of the solutions a rich array of distinct behaviors were found when varying parameter values corresponding to well known physiological states of the cell as well as states that had never been recognized experimentally recently similar models for yeast have been developed by the same group chen et al novak et al novak and tyson see goldbeter obeyesekere et al tyson and tyson et al for reviews of cell cycle models a problem hampering the use of numerical techniques is the lack of in vivo or in vitro measurements of the kinetic parameters in the rate equations numerical parameter values are available for only a handful of well studied systems the phage switch being a rare example in contrast in the cell cycle models downloaded by university of alberta library from www liebertpub com at for personal use only de jong mentioned above in most cases the parameter values had to chosen such that the models are able to reproduce the observed qualitative behavior for larger models nding appropriate values may be dif cult to achieve the growing availability of gene expression data could alleviate the problem to some extent from measurements of the state variables x at several stages of a process of interest possibly under a variety of experimental conditions parameter values might be estimated with the help of system identi cation techniques ljung in a later section this approach will be examined in more detail some recent studies suggest another response to the absence of measured values of the kinetic parameters for instance in a simulation study of the segment polarity network in drosophila it was found that essential properties of the system were quite robust to variations in parameter values and initial conditions over sometimes several orders of magnitude von dassow et al a similar conclusion had been drawn earlier from analysis of a model of bacterial chemotaxis barkai and leibler see savageau for pioneering work these robustness results suggest that it is the network structure rather than the precise value of the parameters that confers stability to the system piecewise linear differential equations in this section a special case of the rate equations will be considered that is based on two simplifying assumptions first the models abstract from the biochemical details of regulatory interactions by directly relating the expression levels of genes in the network second the switch like behavior of genes whose expression is regulated by continuous sigmoid curves is approximated by discontinuous step functions fig the resulting differential equations are piecewise linear and have favorable mathematical properties facilitating their qualitative analysis piecewise linear differential equation plde models are related to the logical models in sections and notwithstanding subtle differences caused by the discrete rather than continuous nature of the latter formalisms we will be concerned with piecewise linear differential equations of the form dxi dt d gi x ixi i n where xi denotes the cellular concentration of the product of gene i and i the degradation rate of xi glass mestl et al thomas and d ari regulation of degradation could be modeled by replacing i with a function similar to gi but this will be omitted here in its most general form the function gi rn r is de ned as gi x d x il bil x where il is a rate parameter bil rn 1g a function de ned in terms of sums and multiplications of step functions and l a possibly empty set of indices the function bil should be seen as the arithmetic equivalent of a boolean function expressing the conditions under which the gene is expressed at a rate il glass plahte et al snoussi the conditions are speci ed by means of step functions sc and de ned by sc xj μj d xj μj xj μj and xj μj d sc xj μj in the example of fig we see that for gene d sc meaning that protein is synthesized at a rate if the concentration of protein is below the threshold concentration gene is repressed by a heterodimer composed of proteins and so that it is expressed at a rate if the concentration of either protein remains below its respective threshold d sc sc more complex combinations of step functions can be used to model the combined effects of several regulatory proteins downloaded by university of alberta library from www liebertpub com at for personal use only genetic regulatory systems fig a example regulatory network of three genes and b corresponding piecewise lineardifferential equations and represent protein or mrna concentrations respectively production constants degradation constants and threshold constants equations have been well studied in mathematical biology and elsewhere edwards edwards and glass edwards et al glass glass and hill glass and pasternack lewis and glass mestl et al plahte et al snoussi snoussi and thomas thomas and d ari consider an n dimensional hyper box of the phase space de ned as follows xi maxi max x gi x i i n where we assume that for all threshold concentrations μik of the protein encoded by gene i it holds that μik maxi the n dimensional threshold hyper planes xi d μik divide the n box into orthants figure a displays the phase space box and orthants corresponding to the example system in fig in each orthant of the n box by evaluation of the step functions reduces to odes with a constant production term composed of rate parameters in bi pxi d ixi i n figure b gives an example of the state equations corresponding to the orthant μ12 and fig a the phase space box of the model in fig divided into d orthants by the threshold planes b the state equations for the orthant μ21 μ12 and the orthant demarcated by bold lines downloaded by university of alberta library from www liebertpub com at for personal use only de jong since equations are linear and orthogonal the behavior inside an orthant is straightforward in fact all trajectories evolve towards a stable steady state x d m g the so called focal state the focal state of an orthant may lie inside or outside the orthant if the focal state lies outside the orthant the trajectories will tend towards one or several of the threshold planes bounding the orthant the trajectories may be continued in an adjacent orthant with a new focal state determined by the reduced state equations in that orthant whether a trajectory can be continued in an adjacent orthant will depend on the precise way in which the pldes are de ned in the threshold planes where the step functions are discontinuous several alternatives have been proposed in the literature glass gouzé and sari plahte et al ranging from restrictions on the function gi to general solutions based on sophisticated mathematical techniques the pldes have two types of steady states in addition to regular steady states lying inside the orthants there are singular steady states situated on one or more threshold planes separating the orthants snoussi and thomas regular steady states can be readily identi ed since they are the focal state of some orthant in the n box singular steady states important as they are as possible homeostatic points of the regulatory system pose a dif culty for the piecewise linear formalism due to the discontinuities at the thresholds techniques to identify these steady states are presented by plahte et al snoussi and thomas and gouzé and sari the global behavior of the pldes can be quite complex and is not well understood in general continuations of trajectories in several orthants may give rise to oscillations towards a singular steady state located at the intersection of threshold planes cycles or limit cycles glass and pasternack lewis and glass mestl et al plahte et al numerical simulation studies have shown that aperiodic chaotic dynamics can occur for n and become quite common for higher dimensions and particular characteristics of the functions gi glass and hill lewis and glass mestl et al mestl et al analyzed the occurrence of chaos in some detail in an example network the use of pldes is exempli ed by the method of generalized threshold models developed by tchuarev and colleagues prokudina et al tchuraev see also kananyan et al ratner and tchuraev and tchuraev and ratner for earlier formulations of threshold models they use pldes that have been embedded in a nite state machine framework allowing regulatory interactions to be modeled in a more explicit and detailed manner than is possible by alone among other things the average time a regulatory protein is bound to the dna can be taken into account as well as the existence of several copies of a gene in different functional states the resulting equations may contain time delays section generalized threshold models have been applied to study the dynamics of the regulation of tryptophan synthesis and arabinose catabolism in e coli the numerical simulation results suggest a number of hypotheses on the functioning of the regulatory systems among other things the relative importance of three different mechanisms controlling the synthesis of tryptophan prokudina et al other applications of plde models can be found in alur et al de jong et al and ghosh and tomlin how much information is lost by the approximation of sigmoid regulation functions by step functions numerical simulation studies by glass and kauffman and glass and pérez show that in most cases there is no difference in the qualitative properties of the solutions when step functions instead of hill functions are used see also plahte et al and thomas and d ari plahte et al discusses situations in which differences do occur in the context of a study on the regulation of free iron level in mammalian cells omholt and colleagues have shown that as the step functions were relaxed to moderately steep sigmoids the singular steady state predicted on the basis of an analysis of the pldes was preserved omholt et al plahte et al the numerical and analytical results turned out to be within the same order of magnitude comparable to the experimental accuracy a major advantage of the use of pldes is the simplicity of the mathematical analysis entailed by the form of the equations in a similar vein savageau has proposed to de ne gi in by means of power law functions giving rise to nonlinear models of the form pxi d xp ik yn x rijk j xp ik yn x sijk j i n downloaded by university of alberta library from www liebertpub com at for personal use only genetic regulatory systems where rij k sij k are kinetic orders for elementary processes contributing to the production degradation of xi and ik ik are rate constants for these processes ik ik by changing to a logarithmic scale becomes a linear system that is much more tractable to analysis than the original nonlinear system see savageau and voit for reviews and a software package among other things the power law formalism has been applied to the study of the functional effectiveness of different types of coupling in regulatory networks savageau qualitative differential equations piecewise linear differential equations of the form can be analyzed qualitativelyby de ning qualitative states that correspond to the orthants of the phase space whenever trajectories can pass from one orthant to another there is a transition between the corresponding qualitative states this results in a transition graph summarizing the qualitative dynamics of the system edwards et al glass a similar idea was encountered in the logical method discussed in section in fact snoussi has demonstrated that the generalized logical formalism of thomas and colleagues can be seen as an abstraction of a special case of the idea of abstracting a discrete description from a continuous model and analyzing the discrete instead of continuous equations to draw conclusions about the dynamics of the system is central to work on qualitative reasoning in arti cial intelligence one of the best known formalisms developed for this purpose are the qualitative differential equations qdes used in the simulation method qsim kuipers qdes are abstractions of odes of the form dxi dt d fi x i n where fi rn r can be any linear or nonlinear function the variables x take a qualitative value composed of a qualitative magnitude and direction the qualitative magnitude of a variable xi is a discrete abstraction of its real value while the qualitative direction is the sign of its derivative the function fi is abstracted into a set of qualitative constraints which restrict the possible qualitative values of the variables given an initial qualitative state consisting of the qualitative values for x at the initial time point the qsim algorithm generates a tree of qualitative behaviors each behavior in the tree describes a possible sequence of state transitions from the initial state it has been proven that every qualitatively distinct behavior of the ode corresponds to a behavior in the tree generated from the qde although the reverse may not be true the incomplete understanding of many genetic regulatory mechanisms on the molecular level and the general absence of quantitative knowledge have stimulated an interest in qualitative simulation techniques an example of their application is given by heidtke and schulze kremer their case study concerns phage growth control in e coli the model consists of seven qdes representing the different stages of phage growth as it follows either the lytic or lysogenic pathway each qde describes in detail the infected bacterium including explicit representation of phages inside and outside the cell viral dna ribosomes mrna and proteins and the way in which they interact to control major cellular events for other examples of the application of qualitative reasoning concepts to gene regulation see akutsu et al de jong et al koile and overton and trelease et al a problem with qualitative simulation approaches is their limited upscalability as a consequence of the weak nature of qualitative constraints and the dif culty to identify implicit constraints behavior trees quickly grow out of bounds this causes the range of application of the methods to be limited to regulatory systems of modest size and complexity systems of even a few genes related by positive and negative feedback loops cannot be handled unless these systems have been so well studied already that behavior predictions can be tightly constrained an attractive feature of the class of pldes discussed in section is that they put strong constraints on local behavior in the phase space by reframing the mathematical analysis of these models in a qualitative simulation framework using a simulation algorithm tailored to the equations larger networks with complex feedback loops can be treated the resulting method has been applied to the simulation of the initiation of sporulation in b subtilis de jong et al downloaded by university of alberta library from www liebertpub com at for personal use only de jong extensions of qualitative simulation methods allow weak numerical information in the form of interval bounds on the qualitative magnitude and direction of the variables and numerical envelopes around the functions fi to be integrated in the simulation process berleant and kuipers this presents another way to restrict the number of behaviors while it may also re ne the predictions of the remaining behaviors heidtke and schulze kremer show how the conclusions of their model can be made more precise by adding semiquantitative information the integration of numerical information is more dif cult to achieve in the logical approaches discussed above in which the relation between the discrete and the underlying continuous models is less direct much work in qualitative reasoning has focused on the automated composition of an appropriate model of a system given a user query from a situation description and a domain knowledge base nayak one of the basic approaches towards automated modeling qualitative process theory forbus has been used by karp in his method for the construction and revision of gene regulation models karp from a taxonomic knowledge base describing biological objects like strains of bacteria genes enzymes and amino acids and a process knowledge base comprising a theory about biochemical reactions the program gensim predicts the outcome of a proposed experiment if the predictions do not match with the observations made while actually carrying out the experiment then the program hypgene generates hypotheses to explain the discrepancies in particular it revises assumptions about the experimental conditions that gensim used to derive its predictions the programs have been able to partially reproduce the experimental reasoning underlying the discovery of the attenuation mechanism regulating the synthesis of tryptophan in e coli other examples of the use of automated modeling techniques in the context of gene regulation can be found in heidtke and schulze kremer and koile and overton see also de jong and rip and kohn partial differential equations and other spatially distributed models differential equations of the form describe genetic regulatory processes while abstracting from spatial dimensions the regulatory systems of interest are assumed implicitly to be spatially homogeneous there are situations in which these assumptions are not appropriate it might be necessary for instance to distinguish between different compartments of a cell say the nucleus and the cytoplasm and to take into account the diffusion of regulatory proteins or metabolites from one compartment to another moreover gradients of protein concentrations across cell tissues are a critical feature in embryonal development the introduction of time delays for diffusion effects allows some aspects of spatial inhomogeneities to be dealt with while preserving the basic form of the rate equations busenberg and mahaffy smolen et al section however in the case that multiple compartments of a cell or multiple cells need to be explicitly modeled a more drastic extension of becomes necessary suppose that a multicellular regulatory system is considered where the p cells are arranged in a row as shown in figure a we introduce a vector x l t which denotes the time varying concentration of gene products in cell l with l a discrete variable ranging from to p within each cell regulation of gene fig examples of spatial con gurations of multicellular genetic regulatory systems downloaded by university of alberta library from www liebertpub com at for personal use only genetic regulatory systems expression occurs in the manner described by equation between pairs of adjacent cells l and l c l p diffusion of gene products is assumed to occur proportionally to the concentration differences x i x l i x l i x l i and a diffusion constant i taken together this leads to a system of coupled odes so called reaction diffusion equations dx l i dt d fi x l c i x i l i c x l i i n l p notice that fi is the same for all l in order to account for the fact that the genetic regulatory network is the same in every individual cell the reaction diffusion equations are valid for the case that cells are arranged in a row but they can be generalized to other one dimensional and higher dimensional spatial con gurations fig in addition the diffusion constants can be made to vary at different locations although has been introduced here with a multicellular regulatory system in mind it is valid for compartmental unicellular systems as well perhaps with some small adaptations see glass and kauffman for an example if the number of cells is large enough the discrete variable l in can be replaced by a continuous variable ranging from to where represents the size of the regulatory system the concentration variables x are now de ned as functions of both l and t and the reaction diffusion equations become partial differential equations pdes xi t d fi x c i l i n if it is assumed that no diffusion occurs across the boundaries l d and l d the boundary conditions become xi t d and xi t d reaction diffusion equations have been widely used in mathematical biology to study pattern formation in development see gierer kauffman maini et al meinhardt and nicolis and prigogine for reviews the equations are also referred to as turing systems after the mathematician who suggested their applicability to developmental phenomena turing turing considered a special case of equations and with two concentration variables and also called morphogens most of the work on reaction diffusion equations is concerned with this special case although higher dimensional systems have been investigated as well e g gierer and lacalli in what follows continuous equations will be considered for the case n d not surprisingly direct analytical solution of this system of nonlinear pdes is not possible in general however suppose that there exists a unique spatially homogeneous steady state x x such that x x fig the spatial homogeneity of the steady state ensures that it is consistent with boundary conditions by linearizing around x x the behavior of the system in response to a small perturbation of the steady state can be predicted let represent the deviation of xi from the homogeneous steady state concentration x i on given the above boundary conditions l t d xi l t x i d cik t cos l i d with cos the modes or eigenfunctions of the laplacian operator on l and cik t the mode amplitudes fig britton nicolis and prigogine segel the steady state x x is stable to perturbations if the mode amplitudes cik all decay exponentially in response to a uctuation decomposable into a large number of weighted modes however if at least one mode amplitude cik k causes the corresponding mode cos to grow exponentially then the steady state is downloaded by university of alberta library from www liebertpub com at for personal use only de jong fig a homogeneous steady state of the reaction diffusion system b c two modes of the solution of the linearized system around the steady state in b k d and in c k d unstable in this case diffusion between adjacent cells does not have a homeogenizing in uence but instead entails spatially heterogeneous gene expression patterns details on the mathematics of the stability analysis can be found in the references listed above gierer and meinhardt have formulated constraints on the functions and parameters in such that an initial steady state can be destabilized by a small uctuation gierer gierer and meinhardt meinhardt among other things the product of gene the activator must positively regulate itself i e while the product of gene the inhibitor must negatively regulate gene i e x2 further the inhibitory effect must be suf ciently strong and be relatively fast compared to the activating effect both the activator and inhibitor diffuse through the system but the latter has to do so more rapidly than the former to achieve the necessary combination of short range activation and long range inhibition another prerequisite for diffusion driven instability is that the size of the domain is larger than some minimum domain size or what comes to the same thing the diffusivity of the inhibitor is smaller than a certain minimum diffusion coef cient or the reactions proceed at a rate faster than a certain maximum reaction rate activator inhibitor systems have been extensively used to study the emergence of segmentation patterns in the early drosophila embryo in the early stage of embryogenesis the embryo forms a syncytial blastoderm a single cell with many nuclei this permits spatial interactions to be conveniently treated in terms of diffusion of gene products goodwin and kauffman and kauffman following earlier suggestions in kauffman et al have noted that the observed spatial and temporal expression patterns of genes involved in the segmentation process much resemble the modes of the linearization of around its equilibrium by taking parameters slowly time varying to mimic developmental effects sequences of sigmoids of increasing frequency increasing k see fig are generated that conform to the expression of the pair rule genes in the mid embryo numerical simulation studies have demonstrated that some aspects of stripe formation in drosophila can indeed be reproduced in this way e g bunow et al lacalli nagorcka other examples of modeling drosophila embryogenesis by means of reaction diffusion systems can be found in hunding hunding et al lacalli et al meinhardt russell see kauffman and meinhardt for reviews the use of reaction diffusion equations in modeling spatially distributed gene expression is compromised by the observation that the predictions are quite sensitive to the shape of the spatial domain the initial and boundary conditions and the chosen parameter values bard and lauder bunow et al this affects the credibility of the models as biological variation among individualmembers of a species or small uctuations in environmental conditions do not generally lead to large deviations of developmental trajectories even more seriously the activator inhibitor models commonly used suffer from the fact that scarce experimental evidence has been found for the hypothesis that two reacting and diffusing morphogens underlie pattern formation from what is known from experimental studies of drosophila development it appears that the situation is much more complex the regulatory system consists of layered networks of downloaded by university of alberta library from www liebertpub com at for personal use only genetic regulatory systems tens or hundreds of developmental genes that mutually interact and whose products diffuse through the embryo it seems therefore preferable to work with instances of the reaction diffusion equations and that re ect the underlying regulatory networks more directly the gene circuit method proposed by reinitz and colleagues mjolsness et al reinitz et al reinitz and sharp reviewed in reinitz and sharp is an example of such an approach see burstein edgar et al hamahashi and kitano kyoda et al sánchez et al and von dassow et al for related work the method employs a variant of dx l i dt ri xn ij xj c u l c i ix l i c i k x i l i c x l i a i n l p where x l is a state vector of protein concentrations in nucleus l and u l an input variable the real parameters ij and describe the contribution of the state and input variables respectively to the expression rate of x l i the sum of the regulatory in uences including a basal expression term i is modulated by a sigmoid regulation function ri r r and a constant indicating the maximum expression rate of gene i the diffusion parameter is the same for every protein but depends on the number k of nuclear divisions that have taken place equations have been used to model the formation of striped patterns of the products of gap genes and pair rule genes in the middle region of the drosophila blastoderm see marnellos et al and marnellos and mjolsness for other applications state variables are introduced for the gap genes kruppel knirps giant and hunchback and for the pair rule gene even skipped n d the input variable represents the maternal bicoid concentration the values of the parameters in can be estimated by means of measurements of protein concentrations in the nuclei at a sequence of time points kosman et al myasnikova et al reinitz and sharp given that the sign of a ij speci es the nature of the interaction between two genes i and j positive negative or no interaction the parameter values thus obtained yield the regulatory network underlying the formation of the striped eve pattern on the basis of a least square t to expression data reinitz and sharp demonstrate that each border in the stripes to is controlled by one of the four gap genes and that for the pattern to emerge the diffusivity of eve should be orders of magnitude lower than the diffusivity of the gap gene proteins reinitz et al reinitz and sharp using the estimated parameter values mutant patterns have also been predicted sharp and reinitz the fact that parameters of the model may represent interactions in the regulatory network like the ij in equation deserves special mention as noted above it implies that an estimation of parameter values simultaneously xes the regulatory structure of the system models of the system could thus be induced from expression data without relying on prior knowledge on the existence of regulatory interactions between genes this idea has already been encountered in sections and of course but is here generalized to the case of continuous dynamical models related work taking ordinary differential equations and difference equations as their point of departure has come up recently chen et al d haeseleer et al noda et al van someren et al weaver et al wessels et al the induction of models from measurements of x at a sequence of time points is made attractive by the growing availability of gene expression data however precise measurements of absolute expression levels are currently dif cult to achieve in addition as a consequence of the dimensionality problem referred to in section the models need to be simple and are usually strong abstractions of biological processes erb and michaels for larger and more complex models the computational costs of nding an optimal t between the parameter values and the data may be prohibitively high de nition of i k reveals a particularity of equation namely that it only holds in interphase the period between two nuclear divisions in which the gene products are synthesized mjolsness reinitz and sharp supplement the model by a transition rule that describes how the model and its parameters need to be changed after division see mjolsness et al for details downloaded by university of alberta library from www liebertpub com at for personal use only de jong stochastic master equations in principle differential equations allow gene regulation to be described in great detail down to the level of individual reaction steps like the binding of a transcription factor to a regulatory site or the transcription of a gene by the stepwise advancement of dna polymerase along the dna molecule fig drew however a number of implicit assumptions underlying de formalisms are no longer valid on the molecular level differential equations presuppose that concentrations of substances vary continuously and deterministically both of which assumptions may be questionable in the case of gene regulation gibson and mjolsness gillespie ko mcadams and arkin nicolis and prigogine rigney szallasi in the rst place the small numbers of molecules of some of the components of the regulatory system compromise the continuity assumption there may be only a few tens of molecules of a transcription factor in the cell nucleus and a single dna molecule carrying the gene second deterministic change presupposed by the use of the differential operator d dt may be questionable due to uctuations in the timing of cellular events such as the delay between start and nish of transcription as a consequence two regulatory systems having the same initial conditions may ultimately settle into different states a phenomenon strengthened by the small numbers of molecules involved instead of taking a continuous and deterministic approach some authors have proposed to use discrete and stochastic models of gene regulation arkin et al gillespie mcadams and arkin morton firth and bray nicolis and prigogine rigney discrete amounts x of molecules are taken as state variables and a joint probability distribution p x t is introduced to express the probability that at time t the cell contains molecules of the rst species molecules of the second species etc the time evolution of the function p x t can now be speci ed as follows p x t c d p x t xm a c xm j where m is the number of reactions that can occur in the system the probability that reaction j will occur in the interval t t c given that the system is in the state x at t and the probability that reaction j will bring the system in state x from another state in t t c gillespie rearranging and taking the limit as gives the master equation van kampen t p x t d xm j jp x t compare this equation with the rate equations in section whereas the latter determine how the state of the system changes with time the former describes how the probability of the system being in a certain state changes with time notice that the state variables in the stochastic formulation can be reformulated as concentrations by dividing the number of molecules xi by a volume factor although the master equation provides an intuitively clear picture of the stochastic processes governing the dynamics of a regulatory system it is even more dif cult to solve by analytical means than the fig example of two reaction steps involved in dna transcription the rst reaction step describes the binding of rna polymerase rnap to the transcription initiation site while the second reaction step describes the advancement of rnap along the dna downloaded by university of alberta library from www liebertpub com at for personal use only genetic regulatory systems deterministic rate equation moreover numerical simulation of the system is complicated by the form of which contains n c independent variables n discrete variables x and a continuous variable t under certain conditions the master equation can be approximated by stochastic differential equations so called langevin equations that consist of a deterministic ode of the type extended with a noise term gillespie nicolis and prigogine van kampen the conditions under which the approximation is valid may not always be possible to satisfy in the case of genetic regulatory systems an alternative approach would be to disregard the master equation altogether and directly simulate the time evolution of the regulatory system this idea underlies the stochastic simulation approach developed by gillespie basically the stochastic simulation algorithm i determines when the next reaction occurs and of which type it will be given that the system is originally in state x at t ii revises the state of the system in accordance with this reaction and iii continues at the resulting next state the stochastic variables and are introduced which represent the time interval until the next reaction occurs and the type of reaction respectively at each state a value for and is randomly chosen from a set of values whose joint probability density function p has been derived from the same principles as those underlying the master equation this guarantees that when a large number of stochastic simulations are carried out the resulting distribution for x at t will approach the distribution implied by the master equation in other words whereas the master equation deals with behavior averages obtained by calculating the averages and the variances of the xi at t from p x t stochastic simulation provides information on individual behaviors gibson and bruck have proposed various improvements of the gillespie algorithm directed at reducing the computational complexity of the procedure morton firth and bray discuss another algorithm that is more ef cient at simulating systems with a large number of reactions the algorithm does so by following individual molecules over time and storing the way they combine into complexes that can have multiple phosphorylration methylation and other activation states the algorithm which is particularly suitable for simulating signal transduction pathways has been implemented in the simulator stochsim stochastic simulation has been used by mcadams and arkin to analyze the interactions controlling the expression of a single prokaryotic gene mcadams and arkin see also barkai and leibler in particular they investigate how the time interval between the activation of one gene and the regulatory action of its product on another gene the so called switching delay is affected by the stochastic nature of the transcription initiation intervals and the numbers of protein molecules produced per transcript using parameter values that approximate those for the expression of the cro gene from the pr promotor in phage they nd that the protein is produced in sharp bursts occurring at random time intervals leading to strong uctuations in switching delays interestingly this may provide an explanation of phenotypic variations in isogenic populations when it is assumed that the variations arise from switching mechanisms which decide between alternative developmental paths depending on the effective concentrations of competitive regulatory proteins the above hypothesis has been pursued in later work which investigates the in uence of uctuations in the rate of gene expression on the choice between lytic and lysogenic growth in phage arkin et al the predicted fraction of infected cells selecting the lysogenic pathway at different infection levels is consistent with experimental observations the predictions of the models have been shown to be relatively insensitive to changes in the translation rate protein dimerization rates and protein degradation rates however they are somewhat sensitive to the transcription rate and quite sensitive to the average number of proteins per mrna transcript gibson and bruck stochastic simulation results in closer approximations to the molecular reality of gene regulation but its use is not always evident in the rst place the approach requires detailed knowledge of the reaction mechanisms to be available including estimates of the probability density function p moreover stochastic simulation is costly due to the large number of simulations that need to be carried out to calculate an approximate value of p x t and due to the large number of reactions that need to be traced in each of these simulations whether the costs always balance the expected bene ts depends on the level of granularity at which one wishes to study regulatory processes on a larger time scale stochastic effects may level out so that continuous and deterministic models form a good approximation see gillespie for a discussion downloaded by university of alberta library from www liebertpub com at for personal use only de jong rule based formalisms the model formalisms discussed so far whether they are continuous or discrete static or dynamic deterministic or stochastic all use a restricted notion of state in fact the state of a regulatory system is equated with the concentration or the number of molecules of proteins mrnas metabolites and other species at a certain time point knowledge based or rule based simulation formalisms developed in the eld of arti cial intelligence permit a richer variety of knowledge about the system to be expressed in a single formalism for instance the logical and physical structure of a gene such as the relative position of the regulatory sites at which transcription is initiated prevented and aborted can be conveniently represented in the rule based approach basically rule based formalisms consist of two components a set of facts and a set of rules that are stored in a knowledge base hayes roth et al facts express knowledge about the objects of a regulatory system each object is described by properties that take their value from well de ned domains the objects are usually structured in a hierarchy of classes for example the class dna sequence could be de ned as consisting of objects with properties that include topology and strandedness a speci c dna sequence might have the value circular for topology and single stranded for strandedness the class dna sequence could be de ned as a subclass of the class sequences other objects of interest in a regulatory system include rnas proteins cells and cell boundaries but also experimental conditions and processes like transcription splicing and translation the rules in the knowledge base consist of two parts a condition part and an action part the condition part expresses conditions in terms of properties of objects while the action part operates upon the objects e g by changing a property of an existing object an example of a rule describing the conditions under which conditions dna polymerase i will bind to bacterial dna adapted from brutlag et al and galper et al is if and temperature range of exp conditions is to ionic strength range of exp conditions is to ph range of exp conditions is to then activity of dna polymerase i is dna binding rule based simulation consists of the repeated process of matching the facts in the knowledge base against the condition parts of the rules and carrying out the action part of the rules whose conditions are satis ed a control strategy determines the order in which the rules are evaluated and resolves the con icts that arise when several rules match at the same time control strategies vary from random selection to complicated priority schemes for instance procedures that take into account estimates of the a priori probability that a rule will re rule based simulation can be used in a forward chaining or backwardchaining mode in the case of forward chaining sketched above one deduces all facts implied by the set of rules and the initial facts in the knowledge base in the case of backward chaining the facts are conclusions to be explained and simulation proceeds in the reverse direction the facts are recursively matched against the action parts of the rules in order to nd all possible combinations of facts that can lead to the conclusions meyers and friedland have applied rule based simulation techniques to model phage growth control meyers and friedland they have performed simulations to study the decision between the lytic and lysogenic pathway for wild type phage and several single and double mutants their simulations produced correct results except in two cases in which the assumption of deterministic behavior proved deleterious the phage example is also used by shimada and colleagues shimada et al a novel aspect of their work being that the regulation process is considered on two levels of abstraction the system uses rule based simulation for the noncritical parts of the decision between the lytic and lysogenic pathway and a qualitative phase space analysis of rate equations for the critical parts the major advantage of rule based formalisms their capability to deal with a richer variety of biological knowledge in an intuitive way is counteracted by dif culties in maintaining the consistency of a revised knowledge base and the problem to incorporate quantitative information although attempts have been made to integrate symbolic and numerical knowledge into a single formalism for instance in the genetic grammar underlying the metabolica system developed by hofestädt and colleagues hofestädt hofestädt and meineke it remains true that in this respect rule based formalisms cannot compete with the continuous formalisms discussed in previous sections downloaded by university of alberta library from www liebertpub com at for personal use only genetic regulatory systems conclusions the formalisms discussed in this review allow genetic regulatory systems to be modeled in quite different ways in table the different approaches are compared on a number of dimensions such as whether the models are discrete or continuous static or dynamic deterministic or stochastic and qualitative or quantitative in addition the approaches are compared with respect to the level of granularity on which they describe regulatory processes apart from constraining the capability of the models to answer certain biological questions the level of granularity to a large extent determines the amount of computer power required to simulate a regulatory network until recently modeling and simulation studies have predominantly used deterministic coarse to average grained models such as logical models and simple differential equation models although quantitative models have been used the conclusions drawn from them have been mostly qualitative this situation can be explained by two major dif culties hampering the modeling and simulation of genetic regulatory networks in the rst place the biochemical reaction mechanisms underlying regulatory interactions are usually not known or are incompletely known this means that detailed kinetic models cannot be built and more approximate models are needed in the second place quantitative information on kinetic parameters and molecular concentrations is only seldom available as a consequence traditional methods for numerical analysis are dif cult to apply not surprisingly the few modeling and simulation studies using ne grained quantitative and stochastic models have been restricted to regulatory networks of small size and modest complexity that have been well characterized already by experimental means for such model systems the paradigm example being phage discussed in section we do have detailed knowledge on regulatory mechanisms and at least partial quantitative information besides a lack of relevant information to build models and test predictions the use of ne grained stochastic models is also hampered by the inherent computational complexity of the simulation problems the expression of a single eukaryotic genemay involve a dozen of transcription factors interacting in complex ways arnone and davidson yuh et al each of these transcription factors may be modi ed by many other proteins thus giving rise to large and complex networks of interactions it can be reasonably expected that at least some of the problems mentioned above will be considerably relieved in the near future the emergence of new experimental techniques along with the development of databases and other infrastructural provisions giving access to published and unpublished experimental data promise to relieve the data bottleneck together with the continuing increase of computer power this might allow hitherto impractical approaches to modeling and simulation to be tried table summary of properties of different modeling formalisms directed graphs dg bayesian networks byn boolean networks bnn generalized logical networks gln nonlinear differential equations nlde piecewise linear differential equations plde partial differential equations pde stochastic master equations sme and rule base formalisms r static discrete d deterministic d qualitative ql coarse c average a dynamic d continuous c stochastic quantitative qn ne f grained dg d ql c byn sa d c qn c bnn d d d ql c gln d d d ql a nlde d c d qn a f plde d c d ql qnc a qde d d d ql a f pde d cb d qn a f sme d d qn f r d d d ql a f ageneralization to dynamic boolean networks is possible bspatial dimension is often discretized cqualitative analysis of models is possible downloaded by university of alberta library from www liebertpub com at for personal use only de jong functional genomics has yielded experimental techniques allowing interactions between genes and gene products to be elucidated in a large scale manner an example is the use of cdna microarrays to monitor protein dna interactions ren et al iyer et al key contributions to the study of protein protein interactions have been made by mass spectrometric protein identi cation and by the yeast twohybrid system pandey and mann tucker et al zhu and snyder increasing knowledge on the molecular mechanisms underlying gene regulation will eventually allow regulatory systems to be modeled on a ner level of granularity than is currently possible other techniques will promote the use of quantitativemodels of gene regulation for example the ability to synthesize regulatory networks in vivo as described by becskei and serrano elowitz and leiber and gardner et al might facilitate the direct measurement of model parameters although the expression pro les provided by cdna microarrays oligonucleotide chips and other tools to measure the evolution of the state of a cell are currently effectively qualitative in nature expected improvements in the re production and the statistical interpretation of the measurements may allow veritable quantitative approaches to take hold the use of quantitative models permits larger systems to be studied at higher precision koshland maddox the above mentioned advances in biology and computer technology may bring us nearer to what seems to be the ultimate goal of modeling and simulation efforts to study the behavior of entire prokaryotic or even eukaryotic organisms in silico that is the use of models that integrate gene regulation with metabolism signal transduction replication and repair and a variety of other cellular processes several initiatives to achieve this have been launched in recent years including the simulation of a virtual cell using the e cell or initial cell environments tomita et al tomita loew and schaff the simulation of the development of the drosophila embryo in the framework of the virtual drosophila project kitano et al and the analysis of metabolic networks in e coli and h in uenzae edwards and palsson schilling et al schilling and palsson even if computer technology would develop to the point that whole cells and organisms can be simulated on the molecular level by tracing hundreds of thousands of reactions occurring in parallel such brute force strategies are not guaranteed to yield insight into the functioning of living systems in fact it may be far from straightfoward to nd meaningful patterns in the masses of data generated by whole cell simulations as an alternative one could focus on smaller subsystems rst that is start with modules that can be studied in relative isolation because they are loosely connected through regulatory interactions or act on different timescales in a second step the analysis of the individual modules could be supplemented by an investigation of the interactions between the modules possibly on a more abstract and hence computationally less demanding level the modular organization of genetic regulatory networks has been put forward in studies of drosophila segmentation von dassow et al and ower morphogenesis in arabidopsis mendoza et al see also hartwell et al and thieffry and romero the overall picture that is emerging may thus not show a big super model covering all aspects of cellular dynamics in great detail but rather a hierarchy of models accounting for different aspects of the cell on different levels of abstraction switching from one level to another one can follow up an analysis of the global regulatory structure of the system by an in depth look at the ne structure of a regulatory module whereas for the rst problem coarse grained models of the type discussed in sections or may be suf cient the latter problem may require detailed models of the form encountered in sections and one of the consequences of this view is the emphasis it puts on modeling a task that has received less systematic attention than simulation thus far the construction of the right model for the job describing relevant aspects of the system on the desired level of granularity demands methods for computer supported modeling that are only in the process of emerging two burgeoning approaches towards computer supported modeling have been encountered in this paper on the one hand models can be composed from knowledge on regulatory interactions stored in databases and knowledge bases as illustrated in section on the other hand models can be induced from expression data by methods like those discussed in sections and each of these approaches has its merits but neither of them seems suf cient in itself rather it can be expected that a combination of the two approaches exploiting a wide range of structural and functional information on regulatory networks will be most effective abstract background variable importance measures for random forests have been receiving increased attention as a means of variable selection in many classification tasks in bioinformatics and related scientific fields for instance to select a subset of genetic markers relevant for the prediction of a certain disease we show that random forest variable importance measures are a sensible means for variable selection in many applications but are not reliable in situations where potential predictor variables vary in their scale of measurement or their number of categories this is particularly important in genomics and computational biology where predictors often include variables of different types for example when predictors include both sequence data and continuous variables such as folding energy or when amino acid sequence data show different numbers of categories results simulation studies are presented illustrating that when random forest variable importance measures are used with data of varying types the results are misleading because suboptimal predictor variables may be artificially preferred in variable selection the two mechanisms underlying this deficiency are biased variable selection in the individual classification trees used to build the random forest on one hand and effects induced by bootstrap sampling with replacement on the other hand conclusion we propose to employ an alternative implementation of random forests that provides unbiased variable selection in the individual classification trees when this method is applied using subsampling without replacement the resulting variable importance measures can be used reliably for variable selection even in situations where the potential predictor variables vary in their scale of measurement or their number of categories the usage of both random forest algorithms and their variable importance measures in the r system for statistical computing is illustrated and documented thoroughly in an application re analyzing data from a study on rna editing therefore the suggested method can be applied straightforwardly by scientists in bioinformatics research page of page number not for citation purposes bmc bioinformatics http www biomedcentral com background in bioinformatics and related scientific fields such as sta tistical genomics and genetic epidemiology an important task is the prediction of a categorical response variable such as the disease status of a patient or the properties of a molecule based on a large number of predictors the aim of this research is on one hand to predict the value of the response variable from the values of the predictors i e to create a diagnostic tool and on the other hand to relia bly identify relevant predictors from a large set of candi date variables from a statistical point of view one of the challenges in identifying these relevant predictor variables is the so called small n large p problem usual data sets in genomics often contain hundreds or thousands of genes or markers that serve as predictor variables xp but only for a comparatively small number n of subjects or tissue types traditional statistical models used in clinical case control studies for predicting the disease status from selected pre dictor variables such as logistic regression are not suita ble for small n large p problems a more appropriate approach from machine learning that has been proposed recently for prediction and variable selec tion in various fields related to bioinformatics and com putational biology is the nonlinear and nonparametric random forest method it also provides variable importance measures for variable selection purposes random forests have been successfully applied to various problems in e g genetic epidemiology and microbiology in general within the last five years within a very short period of time random forests have become a major data analysis tool that performs well in comparison with many standard methods what has greatly contrib uted to the popularity of random forests is the fact that they can be applied to a wide range of prediction prob lems even if they are nonlinear and involve complex high order interaction effects and that random forests produce variable importance measures for each predictor variable applications of random forests in bioinformatics include large scale association studies for complex genetic dis eases as e g lunetta et al and bureau et al who detect snp snp interactions in the case control context by means of computing a random forest variable importance measure for each polymorphism a comparison of the performance of random forests and other classification methods for the analysis of gene expression data is pre sented by díaz uriate and alvarez de andrés who pro pose a new gene selection method based on random forests for sample classification with microarray data we refer to for other applications of the random forest methodology to microarray data prediction of phenotypes based on amino acid or dna sequence is another important area of application of ran dom forests since possibly involving many interactions for example segal et al use random forests to predict the replication capacity of viruses such as hiv based on amino acid sequence from reverse transcriptase and pro tease cummings and segal link the rifampin resist ance in mycobacterium tuberculosis to a few amino acid positions in rpob whereas cummings and myers predict c to u edited sites in plant mitochondrial rna based on sequence regions flanking edited sites and a few other continuous parameters the random forest approach was shown to outperform six other methods in the prediction of protein interactions based on various biological features such as gene expres sion gene ontology go features and sequence data other applications of random forests can be found in fields as different as quantitative structure activity rela tionship qsar modeling nuclear magnetic res onance spectroscopy landscape epidemiology and medicine in general the scope of this paper is to show that the variable impor tance measures of breiman original random forest method based on cart classification trees are a sensible means for variable selection in many of these applications but are not reliable in situations where potential predictor variables vary in their scale of meas urement or their number of categories as e g when both genetic and environmental variables individually and in interactions are considered as potential predictors or pre dictor variables of the same type vary in the number of cat egories present in a certain sample as is often the case in genomics bioinformatics and related disciplines simulation studies are presented illustrating that variable selection with the variable importance measure of the original random forest method bears the risk that subop timal predictor variables are artificially preferred in such scenarios in an extra section further details and explanations of the statistical sources underlying the deficiency of the variable importance measures of the original random forest method namely biased variable selection in the individ ual classification trees used to build the random forest and effects induced by bootstrap sampling with replacement are given we propose to employ an alternative random forest method the variable importance measure of which can be employed to reliably select relevant predictor variables in any data set the performance of this method is compared to that of the original random forest method in simula page of page number not for citation purposes bmc bioinformatics http www biomedcentral com tion studies and is illustrated by an application to the pre diction of c to u edited sites in plant mitochondrial rna re analyzing the data of that were previously ana lyzed with the original random forest method methods here we focus on the use of random forests for classifica tion tasks rather than regression tasks for instance for predicting the disease status from a set of selected genetic and environmental risk factors or for predicting whether a site of interest is edited by means of neighboring sites and other predictor variables as in our application exam ple random forests are an ensemble method that combines several individual classification trees in the following way from the original sample several bootstrap samples are drawn and an unpruned classification tree is fit to each bootstrap sample the variable selection for each split in the classification tree is conducted only from a small ran dom subset of predictor variables so that the small n large p problem is avoided from the complete forest the status of the response variable is predicted as an average or majority vote of the predictions of all trees random forests can highly increase the prediction accu racy as compared to individual classification trees because the ensemble adjusts for the instability of the individual trees induced by small changes in the learning sample that impairs the prediction accuracy in test sam ples however the interpretability of a random forest is not as straightforward as that of an individual classifica tion tree where the influence of a predictor variable directly corresponds to its position in the tree thus alter native measures for variable importance are required for the interpretation of random forests random forest variable importance measures a naive variable importance measure to use in tree based ensemble methods is to merely count the number of times each variable is selected by all individual trees in the ensemble more elaborate variable importance measures incorporate a weighted mean of the individual trees improvement in the splitting criterion produced by each variable an example for such a measure in classification is the gini importance available in random forest implemen tations the gini importance describes the improve ment in the gini gain splitting criterion the most advanced variable importance measure availa ble in random forests is the permutation accuracy impor tance measure its rationale is the following by randomly permuting the predictor variable xj its original association with the response y is broken when the per muted variable xj together with the remaining unper muted predictor variables is used to predict the response the prediction accuracy i e the number of observations classified correctly decreases substantially if the original variable xj was associated with the response thus a rea sonable measure for variable importance is the difference in prediction accuracy before and after permuting xj for variable selection purposes the advantage of the ran dom forest permutation accuracy importance measure as compared to univariate screening methods is that it covers the impact of each predictor variable individually as well as in multivariate interactions with other predictor varia bles for example lunetta et al find that genetic mark ers relevant in interactions with other markers or environmental variables can be detected more efficiently by means of random forests than by means of univariate screening methods like fisher exact test the gini importance and the permutation accuracy importance measures are employed as variable selection criteria in many recent studies in various disciplines related to bioinformatics as outlined in the background section therefore we want to investigate their reliability as variable importance measures in different scenarios in the simulation studies presented in the next section we compare the behavior of all three random forest variable importance measures namely the number of times each variable is selected by all individual trees in the ensemble termed selection frequency in the following the gini importance and the permutation accuracy importance measure termed permutation importance in the fol lowing simulation studies the reference implementation of breiman original ran dom forest method is available in the r system for sta tistical computing via the randomforest add on package by liaw and wiener the behavior of the selection frequency the gini importance and the permu tation importance of the randomforest function is explored in a simulation design where potential predictor variables vary in their scale of measurement and number of categories as an alternative we propose to use the new random for est function cforest available in the r add on package party in such scenarios in contrast to randomforest the cforest function creates random forests not from cart classification trees based on the gini split criterion that are known to prefer variables with e g more catego ries in variable selection but from unbiased classification trees based on a conditional inference page of page number not for citation purposes bmc bioinformatics table simulation design for the simulation studies predictor variables the predictor variables are sampled independently from the following distributions n stands for the standard normal distribution m k stands for the multinomial distribution with values in k and equal probabilities discrete uniform distribution on k b p stands for the binomial distribution bernoulli distribution with probability p thus m equals b framework the problem of biased variable selection in classification trees is covered more thoroughly in a sep arate section below since the cforest function does not employ the gini crite rion we investigate the behavior of the gini importance for the randomforest function only the selection fre quency and the permutation importance is studied for both functions randomforest and cforest in two ways either the individual trees are built on bootstrap samples of the original sample size n drawn with replacement as suggested in or on subsamples drawn without replace ment for sampling without replacement the subsample size here is set to times the original sample size n because in bootstrap sampling with replacement about of the data end up in the bootstrap sample other fractions for the subsample size are possible for instance as suggested by friedman and hall subsampling as an alternative to bootstrap sampling in aggregating e g individual classification trees is investigated further by bühlmann and yu who also coin the term sub agging as an abbreviation for subsample aggregating as opposed to bagging for bootstrap aggregating politis romano and wolf show that for statistical inference in general subsampling works under weaker assumptions http www biomedcentral com than bootstrap sampling and even in situations when bootstrap sampling fails the simulation design used throughout this paper repre sents a scenario where a binary response variable y is sup posed to be predicted from a set of potential predictor variables that vary in their scale of measurement and number of categories the first predictor variable is con tinuous while the other predictor variables are categorical on a nominal scale of measurement with their number of categories between two and up to twenty the simulation designs of both studies are summarized in tables and the sample size for all simulation studies was set to n in the first simulation study the so called null case none of the predictor variables is informative for the response i e all predictor variables and the response are sampled independently in this situation a sensible variable impor tance measure should not prefer any one predictor varia ble over any other in the second simulation study the so called power case the predictor variable is informative for the response i e the distribution of the response depends on the value of this predictor variable the degree of dependence between the informative predictor variable and the response y is regulated by the relevance parameter of the conditional distribution of y given cf table we will later display results for different values of the relevance parameter indicating different degrees of dependence between and y in the power case a sensible variable importance measure should be able to distinguish the informative predictor variable from its uninformative competitors and even more so with increasing degree of dependence results and discussion our simulation studies show that for the randomforest function all three variable importance measures are unre liable and the gini importance is most strongly biased for the cforest function reliable results can be achieved both with the selection frequency and the permutation importance if the function is used together with subsam table simulation design for the simulation studies response variable the response variable is sampled from binomial bernoulli distributions the degree of dependence between the response y and is regulated by the probability p of the binomial distribution b p of y conditional on with the relevance parameter taking values in 05 to model different degrees of dependence page of page number not for citation purposes bmc bioinformatics http www biomedcentral com pling without replacement otherwise the measures are biased as well results of the null case simulation study in the null case when all predictor variables are equally uninformative the selection frequencies as well as the gini importance and the permutation importance of all predictor variables are supposed to be equal however as presented in figure the mean selection frequencies over simulation runs of the predictor variables dif fer substantially when the randomforest function cf top row in figure or the cforest function with bootstrap sampling cf bottom row left plot in figure are used variables with more categories are obviously preferred only when the cforest function is used together with sub sampling without replacement cf bottom row right plot in figure are the variable selection frequencies for the uninformative predictor variables equally low as desired it is obvious that variable importance cannot be repre sented reliably by the selection frequencies that can be considered as very basic variable importance measures if the potential predictor variables vary in their scale of measurement or number of categories when the random forest function or the cforest function with bootstrap sampling is used the mean gini importance over simulation runs that is displayed in figure is biased even stronger like the selection frequencies for the randomforest function cf top row in figure the gini importance shows a strong preference for variables with many categories and the continuous variable the statistical sources of which are explained in the section on variable selection bias in classification trees below we conclude that the gini importance cannot be used to reliably measure variable importance in this situation either we now consider the more advanced permutation impor tance measure we find that here an effect of the scale of measurement or number of categories of the potential predictor variables is less obvious but still severely affects the reliability and interpretability of the variable impor tance measure figure shows boxplots of the distributions over simulation runs of the permutation importance meas ures of both functions for the null case the plots in the top row again display the distribution when the random forest function is used the bottom row when the cforest function is used the left column of plots displays the dis tributions when bootstrap sampling is conducted with replacement while the right column displays the distribu tions when subsampling is conducted without replace ment figure shows boxplots of the distributions of the scaled version of the permutation importance measures of both functions incorporating the standard deviation of the measures the scaled variable importance is the default output of the randomforest function however it has been noted e g by díaz uriate and alvarez de andrés in their supple mentary material that the scaled variable importance of the randomforest function depends on the number of trees grown in the random forest in the cforest function this is not the case therefore we suggest not to interpret the magnitude of the scaled variable importance of the randomforest function the plots show that for the randomforest function cf top row in figures and and less pronounced for the cfor est function with bootstrap sampling cf bottom row left plot in figures and the deviation of the permutation importance measure over the simulation runs is highest for the variable with the highest number of categories and decreases for the variables with less categories and the continuous variable this effect is weakened but not sub stantially altered by scaling the measure cf figure vs figure as opposed to the obvious effect in the selection frequen cies and the gini importance there is no effect in the mean values of the distributions of the permutation importance measures which are in mean close to zero as expected for uninformative variables however the nota ble differences in the variance of the distributions for pre dictor variables with different scale of measurement or number of categories seriously affect the expressiveness of the variable importance measure in a single trial this effect may lead to a severe over or underestimation of the variable importance of variables that have more categories as an artefact of the method even though they are no more or less informative than the other variables only when the cforest function is used together with sub sampling without replacement cf bottom row right plot in figures and does the deviation of the permutation importance measure over the simulation runs not increase substantially with the number of categories or scale of measurement of the predictor variables thus only the variable importance measure available in cforest and only when used together with sampling with out replacement reliably reflects the true importance of potential predictor variables in a scenario where the potential predictor variables vary in their scale of meas urement or number of categories page of page number not for citation purposes bmc bioinformatics http www biomedcentral com figure results of the null case study variable selection frequency mean variable selection frequencies for the null case where none of the predictor variables is informative the plots in the top row display the frequencies when the randomforest function is used the bottom row when the cforest function is used the left column corresponds to bootstrap sampling with replacement the right column to subsampling without replacement results of the power case simulation study in the power case where only the predictor variable is informative a sensible variable importance measure should be able to distinguish the informative predictor variable the following figures display the results of the power case with the highest value of the relevance parameter indi cating a high degree of dependence between and the response in this setting each of the variable importance measures should clearly prefer while the respective val page of page number not for citation purposes bmc bioinformatics http www biomedcentral com figure results of the null case study gini importance mean gini importance for the null case where none of the predictor variables is informative the left plot corresponds to bootstrap sampling with replacement the right plot to subsampling with out replacement ues for the remaining predictor variables should be equally low figure shows that the mean selection frequencies again over simulation runs of the predictor variables again differ substantially when the randomforest func tion cf top row in figure is used and the relevant pre dictor variable cannot be identified with the cforest function with bootstrap sampling cf bottom row left plot in figure there is still bias obvious in the selection frequencies of the categorical predictor variables with many categories only when the cforest function is used together with subsampling without replacement cf bot tom row right plot in figure are the variable selection frequencies for the uninformative predictor variables equally low as desired and the value for the relevant pre dictor variable sticks out the mean gini importance that is displayed in figure again shows a strong bias towards variables with many categories and the continuous variable it completely fails to identify the relevant predictor variable with the mean value for the relevant variable only slightly higher than in the null case figures and show boxplots of the distributions of the unscaled and scaled permutation importance measures of both functions again for the randomforest function cf top row in figures and and less pronounced for the cforest function with bootstrap sampling cf bottom row left plot in figures and the deviation of the permuta tion importance measure over the simulation runs is high est for the variable with the highest number of categories and decreases for the variables with less catego ries and the continuous variable this effect is weakened but not substantially altered by scaling the measure cf figure vs figure as expected the mean value of the permutation impor tance measure for the informative predictor variable is higher than for the uninformative variables however the deviation of the variable importance measure for the uninformative variables with many categories and is so high that in a single trial these uninformative variables may outperform the informative variable as an artefact of the method thus only the variable importance measure computed with the cforest function and only when used together with sampling without replacement is able to reliably detect the informative variable out of a set of uninformative competitors even if the degree of depend ence between and the response is high the rate at which the informative predictor variable is correctly iden tified by producing the highest value of the permutation importance measure increases with the degree of depend page of page number not for citation purposes bmc bioinformatics http www biomedcentral com figure results of the null case study unscaled permutation importance distributions of the unscaled permutation impor tance measures for the null case where none of the predictor variables is informative the plots in the top row display the dis tributions when the randomforest function is used the bottom row when the cforest function is used the left column corresponds to bootstrap sampling with replacement the right column to subsampling without replacement ence between and the response in table the rates of correct identifications over simulation runs for four different degrees of dependence between and the response are summarized for the randomforest and cfor est function with different options for all degrees of dependence between and the response y the cforest function detects the informative variable more reliably than the randomforest function and the cforest function used with subsampling without replacement outperforms the cforest function with boot page of page number not for citation purposes bmc bioinformatics results of the null case study scaled permutation importance distributions of the scaled permutation importance measures for the null case where none of the predictor variables is informative the plots in the top row display the distribu tions when the randomforest function is used the bottom row when the cforest function is used the left column corre sponds to bootstrap sampling with replacement the right column to subsampling without replacement strap sampling with replacement for the randomforest function scaling the permutation importance measure can slightly increase the rates of correct identifications because as shown in figures and scaling weakens the differences in variance of the permutation importance measure for variables of different scale of measurement and number of categories for the cforest function that is not affected by the scale of measurement and number of bmc bioinformatics http www biomedcentral com figure results of the power case study variable selection frequency mean variable selection frequencies for the power case where only the second predictor variable is informative the plots in the top row display the frequencies when the ran domforest function is used the bottom row when the cforest function is used the left column corresponds to bootstrap sampling with replacement the right column to subsampling without replacement categories of the predictor variables both the unscaled and the scaled permutation importance perform equally well so far we have seen that for the assessment of variable importance and variable selection purposes it is impor tant to use a reliable method that is not affected by other page of page number not for citation purposes bmc bioinformatics http www biomedcentral com figure results of the power case study gini importance mean gini importance for the power case where only the second predictor variable is informative the left plot corresponds to bootstrap sampling with replacement the right plot to subsam pling without replacement characteristics of the predictor variables statistical expla nations of our findings are given in a later section in addition to its superiority in the assessment of variable importance the cforest method especially when used together with subsampling without replacement can also be superior to the randomforest method with respect to classification accuracy in situations like that of the power case simulation study where uninformative predictor var iables with many categories fool the randomforest function due to its artificial preference for uninformative predictor variables with many categories the randomforest function can produce a higher mean misclassification rate than the cforest function the mean misclassification rates again over simulation runs for the randomforest and cforest function again for four different degrees of dependence and used with sampling with and without replacement are displayed in table each method was applied to the same simulated test set in each simulation run the test sets were generated from the same data generating process as the learning sets we find that for all degrees of dependence between and the response y the cforest function especially with sampling without replacement outperforms the other methods a similar result is obtained in the application to c to u con version data presented in the next section the differences in classification accuracy are moderate in the latter case however one could think of more extreme situations that would produce even greater differences this shows that the same mechanisms underlying the var iable importance bias can also affect the classification accuracy e g when suboptimal predictor variables that do not add to the classification accuracy are artificially preferred in variable selection merely because they have more categories application to c to u conversion data rna editing is the process whereby rna is modified from the sequence of the corresponding dna template for instance cytidine to uridine conversion abbreviated c to u conversion is common in plant mitochondria the mechanisms of this conversion remain largely unknown although the role of neighboring nucleotides is emphasized cummings and myers suggest to use information from sequence regions flanking the sites of interest to predict editing in arabidopsis thaliana brassica napus and oryza sativa based on random forests the ara bidopsis thaliana data of can be loaded from the jour page of page number not for citation purposes bmc bioinformatics http www biomedcentral com nal homepage for each of the observations the data set gives the response at the site of interest binary edited not edited and as potential predictor variables the nucleotides at positions to relative to the edited site categories the codon position categories page of page number not for citation purposes bmc bioinformatics http www biomedcentral com figure results of the power case study scaled permutation importance distributions of the scaled permutation impor tance measures for the power case where only the second predictor variable is informative the plots in the top row display the distributions when the randomforest function is used the bottom row when the cforest function is used the left column corresponds to bootstrap sampling with replacement the right column to subsampling without replacement the estimated folding energy continuous and the difference in estimated folding energy between pre edited and edited sequences continuous we first derive the permutation importance measure for each of the potential predictor variables with each method as can be seen from the barplot in figure the scaled variable importance measures largely reflect the page of rates of correct identifications of the informative variable with the scaled and unscaled permutation importance of the randomforest method applied with sampling with and without replacement as compared to those of the cforest method applied with sampling with and without replacement as a function of the degree of dependence indicated by the relevance parameter cf table between the informative variable and the response standard errors of the rates of correct identifications r over iterations can easily be computet by se r r results of based on the gini importance measure but differ slightly for the randomforest and cforest function and the different resampling schemes in particular the variable importance measure of the randomforest func tion seems to produce more noise than that of the cfor est function the contrast of amplitudes between irrelevant and relevant predictors is more pronounced when the cforest function is used note however that the the permutation importance val ues for one predictor variable can vary between two com putations because each computation is based on a different random permutation of the variable therefore before interpreting random forest permutation impor tance values the analysis should be repeated with several different random seeds to test the stability of the results similarly to the simulation study we also compared the prediction accuracy of the four approaches for this data set to do so we split the original data set into learning and test sets with size ratio in a standard split sample validation scheme a random forest is grown based on the learning set and subsequently used to predict the observa tions in the test set this procedure is repeated times and the mean misclassification rates over the runs are reported in table again we find a slight superiority of the cforest function especially when sampling is con ducted without replacement differences to the accuracy values reported in are most likely due to their use of a different validation scheme that is not reported in detail in all function calls and all important options of the ran domforest and cforest functions used in the simulation studies and the application to c to u conversion data are documented in the supplement see additional file sources of variable importance bias the main difference between the randomforest function based on cart trees and cforest function based on conditional inference trees is that in randomforest the variable selection in the individual cart trees is biased so that e g variables with more categories are pre ferred this is illustrated in the next section on variable selection bias in individual classification trees however even if the individual trees select variables in an unbiased way as in the cforest function we find that the variable importance measures as well as the selection fre quencies of the variables are affected by the bootstrap sampling with replacement this is explained in the sec tion on effects induced by bootstrapping variable selection bias in the individual classification trees of a random forest let us again consider the null case simulation study design where none of the variables is informative and thus should be selected with equally low probabilities in a classification tree in traditional classification tree algorithms like cart for each variable a split criterion like the gini index is com puted for all possible cutpoints within the range of that variable the variable selected for the next split is the one that produced the highest criterion value overall i e in its best cutpoint obviously variables with more potential cutpoints are more likely to produce a good criterion value by chance as in a multiple testing situation therefore if we compare the highest criterion value of a variable with two catego ries say that provides only one cutpoint from which the criterion was computed with a variable with four catego ries that provides seven cutpoints from which the best cri page of page number not for citation purposes bmc bioinformatics http www biomedcentral com figure results for the c to u conversion data scaled permutation importance scaled variable importance measures for the c to u conversion data the plots in the top row display the measures when the randomforest function is used the bot tom row when the cforest function is used the left column corresponds to bootstrap sampling with replacement the right column to subsampling without replacement in each plot the positions through indicate the nucleotides flanking the site of interest and the last three bars on the right refer to the codon position cp the estimated folding energy fe and the differ ence in estimated folding energy dfe terion value is used the latter is often preferred because the number of cutpoints grows exponentially with the number of categories of unordered categorical predictors we find a preference for variables with more categories in page of mean misclassification rates of the randomforest method applied with sampling with and without replacement as compared to those of the cforest method applied with sampling with and without replacement as a function of the degree of dependence indicated by the relevance parameter cf table between the informative variable and the response standard errors of the mean misclassification rates are given in parentheses cart like classification trees for further reading on vari able selection bias in classification trees see e g the cor responding sections in since the gini importance measure in randomforest is directly derived from the gini index split criterion used in the underlying individual classification trees it carries for ward the same bias as was shown in figures and conditional inference trees that are used to construct the classification trees in cforest are unbiased in variable selection here the variable selection is conducted by minimizing the p value of a conditional inference inde pendence test comparable e g to the test that incorpo rates the number of categories of each variable in the degrees of freedom the mean selection frequencies again over simula tion runs of the five predictor variables of the null case simulation study design for both cart classification trees as implemented in the rpart function and condi tional inference trees function ctree are displayed in fig ure we find that the variable selection with the rpart function is highly biased while for the ctree function it is unbiased the variable selection bias that occurs in every individual tree in the randomforest function also has a direct effect on the variable importance measures of this function pre dictor variables with more categories are artificially pre ferred in variable selection in each splitting decision thus they are selected in more individual classification trees and tend to be situated closer to the root node in each tree the variable selection bias affects the variable importance measures in two respects firstly the variable selection fre quencies over all trees are directly affected by the variable selection bias in each individual tree secondly the effect on the permutation importance is less obvious but just as severe when permuting the variables to compute their permuta tion importance measure the variables that appear in more trees and are situated closer to the root node can affect the prediction accuracy of a larger set of observa tions while variables that appear in fewer trees and are sit uated closer to the bottom nodes affect only small subsets of observations thus the range of possible changes in prediction accuracy in the random forest i e the devia tion of the variable importance measure is higher for var iables that are preferred by the individual trees due to variable selection bias we found in figures through that the effects induced by the differences in scale of measurement of the predictor variables were more pronounced for the randomforest function where variable selection in the individual trees is biased than for the cforest function where the individ ual trees are unbiased however we also found that when the cforest function is used with bootstrap sampling the variable selection frequencies of the categorical predictors still depend on their number of categories cf e g bot tom row left plot in figure and also the deviation of table mean misclassification rates for application to c to u conversion data mean misclassification rates of the randomforest method applied with sampling with and without replacement as compared to those of the cforest method applied with sampling with and without replacement standard errors of the mean misclassification rates are given in parentheses page of page number not for citation purposes bmc bioinformatics http www biomedcentral com figure variable selection bias in individual trees relative selection frequencies for the rpart left and the ctree right classifica tion tree methods all variables are uninformative as in the null case simulation study the permutation importance measure is still affected by the number of categories cf e g bottom row left plot in figures and thus there must be another source of bias besides the variable selection bias in the individual trees that affects the selection frequencies and the deviation of the permu tation importance measure we show in the next section that this additional effect is due to bootstrap sampling with replacement that is tradi tionally employed in random forests effects induced by bootstrapping from the comparison of left and right columns repre senting sampling with and without replacement in fig ures and we learned that the variable selection frequencies in random forest functions are affected by the resampling scheme we found that even when the cforest function based on unbiased classification trees is used variables with more categories are preferred when bootstrap sampling is con ducted with replacement while no bias occurs when sub sampling is conducted without replacement as displayed in the bottom right plot in figures and thus the boot strap sampling induces an effect that is more pronounced for predictor variables with more categories for a better understanding of the underlying mechanism let us consider only the categorical predictor variables through with different numbers of categories from the null case simulation study design rather than trying to explain the effect of bootstrap sam pling in the complex framework of random forests we use a much simpler independence test for the explanation we consider the p values of tests computed from simulated data sets in each simulation run a test is computed for each predictor variable and the binary response y remember that the variables in the null case are not informative i e the response is independent of all variables for independent variables the distribution of the p values of the test is supposed to form a uniform distribution the left plot in figure displays the distribution of the p values of tests from each predictor variable and the response y as boxplots we find that the boxplots range from to with median as expected because the p values of the test form a uniform distribution when computed before bootstrapping however if in each sim ulation run we draw a bootstrap sample from the original sample and then again compute the p values based on the bootstrap sample we find that the distribution of the p page of page number not for citation purposes bmc bioinformatics http www biomedcentral com values is shifted towards zero as displayed in the right plot in figure obviously the bootstrap sampling artificially induces an association between the variables this effect is always present when statistical inference such as an association test is carried out on bootstrap samples bickel and ren point out that bootstrap hypothesis testing fails whenever the distribution of any statistic in the bootstrap sample rather than the distribution of the statistic under the null hypothesis is used for statistical inference we found that this issue directly affects variable selection in random forests because the deviation from the null hypothesis is more pronounced for variables that have more categories the reason for the shift in the distribu tion of the p values displayed in figure is that each original sample even if sampled from theoretically inde pendent distributions may show some minor variations from the null hypothesis of independence these minor variations are aggravated by bootstrap sampling with replacement because the cell counts in the contingency table are affected by observations that are either not included or are doubled or tripled in the bootstrap sam ple and therefore the bootsrap sample deviates notably from the null hypothesis even if the original sample was generated under the null hypothesis this effect is more pronounced for variables with more categories because in larger tables such as the table from the cross classification of and the binary response y the absolute cell counts are smaller than in smaller tables such as the table from the cross classification of and the binary response y with respect to the smaller absolute cell counts excluding or duplicating an observation produces more severe variations from the null hypothesis this effect is not eliminated if the sample size is increased because in bootstrap sampling the size n of the original sample and the bootstrap sample size n increase simulta neously however if subsamples are drawn without replacement the effect disappears the apparent association that is induced by bootstrap sampling and that is more pronounced for predictor var iables with many categories affects both variable impor tance measures the selection frequency is again directly affected and the permutation importance is affected because variables with many categories are selected more often and gain positions closer to the root node in the individual trees together with the mechanisms described in the previous section this explains our findings figure effects induced by bootstrapping distribution of the p values of tests of each categorical variable and the binary response for the null case simulation study where none of the predictor variables is informative the left plots corre spond to the distribution of the p values computed from the original sample before bootstrapping the right plots correspond to the distribution of the p values computed for each variable from the bootstrap sample drawn with replacement page of page number not for citation purposes bmc bioinformatics http www biomedcentral com 25 from our simulation results we can see however that the effect of bootstrap sampling is mostly superposed by the much stronger effect of variable selection bias when com paring the conditions of sampling with and without replacement for the randomforest function only cf fig ures through top row only when variable selection bias is removed by the cforest function the differences between the conditions of sampling with and without replacement are obvious cf figures through bottom row we therefore conclude that in order to be able to reliably interpret the variable importance measures of a random forest the forest must be built from unbiased classification trees and sampling must be conducted without replacement conclusion random forests are a powerful statistical tool that has found many applicants in various scientific areas it has been applied to such a wide variety of problems as large scale association studies for complex genetic diseases the prediction of phenotypes based on amino acid or dna sequences qsar modeling and clinical medicine to name just a few features that have added to the popularity of random for ests especially in bioinformatics and related fields where identifying a subset of relevant predictor variables from very large sets of candidates is the major challenge include its ability to deal with critical small n large p data sets and the variable importance measures it provides for variable selection purposes however when a method is used for variable selection rather than prediction only it is particularly important that the value and interpretation of the variable impor tance measure actually depict the importance of the varia ble and are not affected by any other characteristics we found that for the original random forest method the variable importance measures are affected by the number of categories and scale of measurement of the predictor variables which are no direct indicators of the true impor tance of the variable as long as e g only continuous predictor variables as in most gene expression studies or only variables with the same number of categories are considered in the sample variable selection with random forest variable importance measures is not affected by our findings however in studies where continuous variables such as the folding energy are used in combination with categorical informa tion from the neighboring nucleotides or when categori cal predictors as in amino acid sequence data vary in their number of categories present in the sample variable selection with random forest variable importance meas ures is unreliable and may even be misleading especially informations on clinical and environmental variables are often gathered by means of questionnaires where the number of categories can vary between ques tions the number of categories is typically determined by many different factors but is not necessarily an indicator of variable importance similarly the number of different categories of a predictor actually available in a certain sample is not an indicator of its relevance for predicting the response hence the number of categories of a varia ble should not influence its estimated importance oth erwise the results of a study could easily be distorted when an irrelevant variable with many categories is included in the study design we showed that due to variable selection bias in the indi vidual classification trees and effects induced by bootstrap sampling the variable importance measures of the ran domforest function are not reliable in many scenarios rel evant in applied research as an alternative random forest method we propose to use the cforest function that provides unbiased variable selec tion in the individual classification trees when this method is applied with subsampling without replacement the resulting variable importance measure can be used reliably for variable selection even in situations where the potential predictor variables vary in their scale of meas urement or their number of categories with respect to computation time the cforest function is more expensive than the randomforest function because in order to be unbiased split decisions and stopping rely on time consuming conditional inference to give an impression the computation times of the application to c to u conversion data with observations and predictor variables as stated in the supplementary file for the cforest function used with bootstrap sampling with replacement are in the range of 8 sec while subsam pling without replacement is computationally less expen sive and in the range of since we saw that only subsampling without replacement guarantees reliable variable selection and produces unbi ased variable importance measures the faster version without replacement should be preferred anyway the computation time for the randomforest function is in the range of sec with and sec without replacement however we saw that the randomforest function should not be used when the potential predictor variables vary in their scale of measurement or their number of categories the aim of this paper was to explore the limits of the empirical measures of variable importance provided for page of page number not for citation purposes bmc bioinformatics 8 25 random forests to understand the underlying mecha nisms and to use that understanding to guarantee unbi ased and reliable variable selection in random forests in a more theoretical work van der laan gives a fun damental definition of variable importance as well as a statistical inference framework for estimating and testing variable importance inspired by this approach future research on variable importance measures for variable selection with random forests aims at providing further means of statistical inference that can be used to guide the decision on which and how many predictor variables to select in a certain problem abstract summary the biopython project is a mature open source inter national collaboration of volunteer developers providing python libraries for a wide range of bioinformatics problems biopython includes modules for reading and writing different sequence file formats and multiple sequence alignments dealing with macro molecular structures interacting with common tools such as blast clustalw and emboss accessing key online databases as well as providing numerical methods for statistical learning availability biopython is freely available with documentation and source code at www biopython org under the biopython license contact all queries should be directed to the biopython mailing lists see www biopython org wiki peter cock scri ac uk introduction python www python org and biopython are freely available open source tools available for all the major operating systems python is a very high level programming language in widespread commercial and academic use it features an easy to learn syntax object oriented programming capabilities and a wide array of libraries python can interface to optimized code written in c c or even fortran and together with the numerical python project numpy oliphant makes a good choice for scientific programming oliphant python has even been used in the numerically demanding field of molecular dynamics hinsen there are also high quality plotting libraries such as matplotlib matplotlib sourceforge net available to whom correspondence should be addressed since its founding in chapman and chang biopython has grown into a large collection of modules described briefly below intended for computational biology or bioinformatics programmers to use in scripts or incorporate into their own software our web site lists over publications using or citing biopython the open bioinformatics foundation obf www open bio org hosts our web site source code repository bug tracking database and email mailing lists and also supports the related bioperl stajich et al biojava holland et al bioruby www bioruby org and biosql www biosql org projects biopython features the seq object is biopython core sequence representation it behaves very much like a python string but with the addition of an alphabet allowing explicit declaration of a protein sequence for example and some key biologically relevant methods for example from bio seq import seq from bio alphabet import gene seq atgaaagcaattttcgtactg aaaggttggtggcgcacttga print gene transcribe augaaagcaauuuucguacugaaagguugguggcgcacuuga print gene translate table mkaifvlkgwwrt sequence annotation is represented using seqrecord objects which augment a seq object with properties such as the record name identifier and description and space for additional key value terms the seqrecord can also hold a list of seqfeature the author this is an open access article distributed under the terms of the creative commons attribution non commercial license http creativecommons org licenses by nc uk which permits unrestricted non commercial use distribution and reproduction in any medium provided the original work is properly cited biopython table selected bio seqio or bio alignio file formats where possible our format names column format match bioperl and emboss rice et al column r w denotes support for reading r and writing w conclusions biopython is a large open source application programming interface api used in both bioinformatics software development and in everyday scripts for common bioinformatics tasks the homepage www biopython org provides access to the source code documentation and mailing lists the features described herein are only a subset potential users should refer to the tutorial and api documentation for further information acknowledgements the obf hosts and supports the project the many biopython contributors over the years are warmly thanked a list too long to be reproduced here funding fundacao para a ciencia e tecnologia portugal grant sfrh bd to t a conflict of interest none declared objects which describe sub features of the sequence with their location and their own annotation the bio seqio module provides a simple interface for reading and writing biological sequence files in various formats table where regardless of the file format the information is held as seqrecord objects bio seqio interprets multiple sequence alignment file formats as collections of equal length gapped sequences alternatively bio alignio works directly with alignments including files holding more than one alignment e g re sampled alignments for bootstrapping or multiple pairwise alignments related module bio nexus developed for kauff et al supports phylogenetic tools using the nexus interface maddison et al or the newick standard tree format modules for a number of online databases are included such as the ncbi entrez utilities expasy interpro kegg and scop bio blast can call the ncbi online blast server or a local standalone installation and includes a parser for their xml output biopython has wrapper code for other command line tools too such as clustalw and emboss the bio pdb module provides a pdb file parser and functionality related to macromolecular structure hamelryck and manderick module bio motif provides support for sequence motif analysis searching comparing and de novo learning biopython graphical output capabilities were recently significantly extended by the inclusion of genomediagram pritchard et al biopython contains modules for supervised statistical learning such as bayesian methods and markov models as well as unsu pervised learning such as clustering de hoon et al alignace is a gibbs sampling algorithm for identifying motifs that are over represented in a set of dna sequences when used to search upstream of apparently coregulated genes alignace nds motifs that often correspond to the dna binding preferences of transcription factors we previously used alignace to analyze whole genome mrna expression data here we present a more detailed study of its effective ness as applied to a variety of groups of genes in the saccharomyces cerevi siae genome published functional catalogs of genes and sets of genes grouped by common name provided groups resulting in motifs in conjunction with this analysis we present measures for gau ging the tendency of a motif to target a given set of genes relative to all other genes in the genome and for gauging the degree to which a motif is preferentially located in a certain distance range upstream of transla tional start sites we demonstrate improved methods for comparing and clustering sequence motifs many previously identi ed cis regulatory elements were found we also describe previously unidenti ed motifs one of which has been veri ed by experiments in our laboratory an extensive set of alignace runs on randomly selected sets of genes and on sets of genes whose upstream regions contain known transcription factor binding sites serve as controls academic press keywords bioinformatics computational biology genomics dna regulatory motifs yeast introduction the recent increase in the number of sequenced genomes and the amount of genome scale exper imental data allows the use of computational techniques to investigate cis acting sequences con trolling transcriptional regulation some methods seek to nd new sites for a given transcription fac tor based on a set of known sites often by using online search engines where one may submit sequences to be scanned for known motifs heinemeyer et al zhu zhang others such as alignace seek to nd unknown abbreviations used orf open reading frame sgd saccharomyces genome database ore oleate response element rrpe ribosomal rna processing element pac box polymerase a and c box stre stress response element ecb early cell cycle box e mail address of the corresponding author church arep med harvard edu dna binding motifs for unspeci ed transcription factors by searching the regions upstream of the translational start sites of a set of potentially coregulated genes spellman et al van helden et al brazma et al roth et al alignace is based on a gibbs sampling algor ithm and returns a series of motifs that are over represented in the input set it previously has been used to nd transcriptional regulatory dna motifs in saccharomyces cerevisiae using groups of genes derived from genome wide mrna expression data roth et al tavazoie et al while many known cis acting elements were identi ed alignace returned many more motifs about which no literature information was found a dis tinguishing feature of most of the known motifs was that their corresponding highest scoring geno mic sites tended to be strongly selective for the upstream regions of the genes used to nd them 051205 35 academic press identi cation of regulatory elements in yeast one might expect this to be always true since each motif is itself composed of sites in those regions but we found that the vast majority of the unknown motifs were not very selective in this way also a subset of the known motifs seemed to be preferentially positioned relative to the start of translation here we describe statistics to measure these two motif properties which we call group speci city and positional bias furthermore we present results from the systematic application of alignace to a sample set of functional groups of genes in s cerevisiae as well as positive and nega tive control sets these data sets allow us to cali brate alignace and the associated motif measures so that empirical signi cance thresholds for these statistics may be determined many known cis regulatory elements as well as novel motifs are identi ed by this method results the input sets of genes a total of groups were examined including groups from the database at the munich infor mation center for protein sequences heinemeyer et al groups from the yeast protein data base hodges et al and groups based on common name root as listed in the table of open reading frames orfs from the saccharomyces genome database sgd ftp genome ftp stan ford edu pub yeast sacchdb cherry et al we considered only groups of six or more genes the number of genes in each of these groups range from this minimum of six to as many as with an average of genes per group runs of alignace on the upstream regions of these groups of genes produced motifs since diverse sources of data were used to gener ate these groups of genes no single mechanism of control is expected to exert an in uence over all of the members of any of the groups it is therefore important that each motif may consist of any num ber of sites including zero in each upstream region submitted to alignace furthermore motifs found upstream of only a fraction of the submitted genes may still be considered very sig ni cant according to the measures developed here for example the motif corresponding to binding by the transcription factor was found from an alignace run on the upstream regions of amino acid residue biosynthetic genes as an align ment of sites upstream of only of the genes nevertheless according to the statistics dis cussed below it was one of the strongest motifs found positive and negative controls were also per formed a total of known transcription factors with experimentally validated binding sites were used to create a test set to see how often alignace nds expected sequence motifs to determine the false positive rate a set of control alignace runs were done each with and randomly selected orfs this distribution of group sizes was chosen to be comparable to the functional categories studied here and to span the range of sizes of most gene sets to be analyzed by this method in future applications motif measures to reduce the set of motifs under consider ation we devised two motif measures one related to group speci city the other to positional bias the group speci city score gauges how well a given motif targets the upstream regions of the genes used to nd it relative to the upstream regions of all of the genes in the genome the pos itional bias score indicates the degree to which a motif tends to be preferentially positioned in a par ticular distance range upstream of the translational start see methods these measures are distinct from and supersede those used in the initial report using alignace roth et al in that study the two relevant measures were map score and a general speci city score not to be confused with the new group speci city score the map score measures the degree to which a motif is over represented rela tive to the expectation for the random occurrence of such a motif in the sequence under consider ation it is the central score used by alignace to rate the different alignments it samples see methods the main drawback to the map score is the fact that some motifs occurring ubiquitously in a genome e g a rich motifs in s cerevisiae are scored very highly but are not likely to be relevant to the speci c set of genes in question the general speci city score was designed to give an indication of how frequently a motif occurs in the genome cutoffs based on this score were then used in an attempt to eliminate the ubiquitous motifs with high map scores however many real motifs occur frequently in the genome in fact the more important the motif is in terms of the number of genes it controls the worse it scores by this measure the new measure which we call group speci city does not have this drawback it serves as a powerful adjunct to the map score in that it takes into account the sequence of the entire genome and highlights those motifs that are found preferen tially in association with the genes under consider ation cutoffs based on group speci city serve to eliminate motifs that correspond to sequence features that are over represented throughout the genome it provides better balance between motifs with many genomic sites and motifs with fewer sites since it is only a measure of the degree to which the distribution of sites is skewed toward the input set the greater total number of sites is not as much of an advantage motif clustering many examples of identical or very similar motifs were generated by alignace this occurs when the same motif is found from alignace runs on overlapping or related groups of orfs and also when multiple similar examples of a very strong motif are returned from a single alignace run the latter case is caused by the iterative mask ing procedure used to nd multiple motifs see methods to automatically group very similar motifs together we needed a computational measure for motif similarity while many established tools exist for comparing one sequence with another sequence altschul et al or one sequence with an alignment of many sequences berg von hippel methods for comparing two sequence align ments in a way appropriate to the short dna motifs here are considerably less well developed we previously devised and used one algorithm for this purpose roth et al a modi ed and simpler algorithm is used here which we name compareace a hierarchical clustering technique based on compareace was developed and used to group similar motifs see methods highly group specific motifs in order to present only the strongest of the great number of motifs found we chose a map score cutoff of which reduced the set of motifs under consideration to while largely arbi trary this threshold did not lead to the rejection of any of the best examples of known cis regulatory elements to focus on the most selective motifs a cutoff of for the group speci city score was chosen a total of highly speci c motifs ful lled both criteria and were grouped into 25 distinct motif clusters figure lists a representative motif from each of the motif clusters along with its corre sponding map group speci city and positional bias scores if the motif has been identi ed with the binding preferences of a known transcription factor that is also indicated otherwise a short description is given of the group of genes upstream of which the motif was found known motifs assignment of alignace motifs to known cis regulatory elements from the literature is an ideal application for compareace this algorithm was not used in this case however because databases of known transcription factor binding sites are still incomplete with respect to what is known in the literature the main criterion used to identify an alignace motif as a known cis regulatory element was that the alignace motif matched the litera ture consensus and was found upstream of an appropriate set of genes for motifs with numerous annotated well de ned binding sites this criterion allowed us to easily make the assignment in cases involving very few known sites the criterion used was whether the top genomic sites for the alignace motif included a signi cant fraction of the sites veri ed in the literature we were able to identify the following known motifs from among the 25 highly speci c motif clusters the heat shock element hse the complex the complex the mlui cell cycle box mcb the stress response element stre the complex the carbon source responsive element csre and svetlov cooper warner lundin et al martinez pastor et al blaiseau et al fisher goding becker et al mcintosh karpichev et al caspary et al baur et al delahodde et al known real motifs found with slightly lower group speci city scores include the early cell cycle box ecb and the cell cycle activation cca not shown in figure yamaguchi iwai et al lohr et al mcinerny et al freeman et al the given names correspond either to the known tran scription factor or to an acronym corresponding to the motif function among those motif clusters that were not identi ed with known transcription factors we were generally unable to nd infor mation to indicate a possible cellular function in most assignments the motif found by alignace matched very closely the literature motif but two exceptions are worth noting as they illustrate different interpretation issues with alignace motifs the rst exception involves the assignment to the stre of motif cluster which contains motifs from carbohydrate utilization categories the consensus binding site for the stre is agggg martinez pastor et al the motifs in this cluster are very g rich but include more columns of information than are in this simple con sensus this may indicate that the literature con sensus has ignored information in the anking regions of the motif or it may indicate that alignace has chosen an alignment in which the motif has been overspeci ed another dif cult assignment was motif cluster which was derived from a group of genes involved in peroxisomal organization the motif identi ed by alignace is a superposition of a half site of the oleate response element ore and the multifunctional consensus cggcggc karpichev et al gailus durner et al it therefore demonstrates two possible ways in which alignace can fail to nd the appropriate motif the ore is de ned in the literature as two near palindromic half sites separated by a bp or bp spacer the alignace alignment however only matched one half site alignace is designed to look for compact sites and so penalizes sites that are diffuse this penalty is not so great as to pre clude its nding the binding site a cgg inverted repeat separated by bp it is possible identi cation of regulatory elements in yeast figure motifs ranked by speci city score for each cluster the statistics for the motif member with the best low est speci city score are listed the second third and fourth columns correspond to the map group speci city and positional bias scores respectively the fth column is the number of base pairs upstream of the translational start site that the center of the most enriched bp window is found see methods the sixth column is a sequence logo representation of the motif schneider stephens an algorithm for determining a unique orientation for each motif was developed and applied see methods the last column lists the common name or the binding factor for the motif if known otherwise a short description is given of the group of genes upstream of which the motif was found that in this case the bp spacer incurred too great a penalty for the full site to be considered signi cant that the variability in the length of the spacer prevented alignment of the full site or that the sampling was not suf ciently extensive for even a strong motif to be found from among the large sample space of motifs with bp spacers the alignment was further complicated by the presence of a few sites that were perfect matches to the consensus which is very similar to the ore half site it is not known whether these are func tional sites nevertheless although this motif does not directly correspond to the binding prefer ences for any one transcription factor and the aligned sites seem to match the binding prefer ences for either of two different factors it is encouraging that the sites aligned by alignace largely correspond to functional control elements in the cases where the upstream regions in ques tion have been studied unknown motifs three highly speci c motifs were found to be associated with ribosomal proteins one of these the motif is well known the other two are motifs and which are primarily associated with small and large ribosomal subunits respect ively these ndings are especially interesting since the transcriptional regulation of a number of ribosomal proteins has been studied in detail and the known and sites along with a t rich region are generally found to be suf cient to explain their transcriptional control warner goncalves et al the second motif listed in figure has an extre mely well conserved consensus and is very speci c for genes coding for proteasome subunits the motif also shows a great deal of positional bias with the most signi cant enrichment occurring between approximately and bp relative to the translational start the top genes ranked by the strength of their best site in this upstream window include proteasome subunits ve ubi quitin related genes ve chaperonin genes two mitochrondrial proteases and three nuclear trans port genes the corresponding binding factor for this motif has recently been identi ed as mannhaupt et al this result has been inde pendently veri ed in our lab using a one hybrid selection with con rmation by mrna expression analysis of knockout and overexpressing strains cluster contains a motif that to our knowl edge has not been noted in the literature it is very speci c for genes involved in rrna processing and it demonstrates strong positional bias prefer ring sites between approximately and bp upstream of the translational start we refer to this motif as rrpe which stands for ribosomal rna processing element many other unknown motifs were found see the web site http arep med harvard edu for a complete current list of all known and unknown motifs found by alignace positionally biased motifs to focus on the most positionally biased motifs a map score cutoff of was again applied fol lowed by a positional bias score cutoff of the motifs passing these criteria demonstrated great redundancy and were separated into only distinct clusters see figure the vast majority of these motifs are homopoly meric a rich sequences which are commonly found despite the fact that alignace corrects for the a t content of the yeast genome these are generally the strongest motifs found in a search of any selection of upstream regions in yeast and they demonstrate strong positional bias toward locations between about and relative to the translational start the only known transcrip tion factor that binds such sequences is datin which has been observed to act both as an activa tor and as a repressor moreira et al such sites have also been observed to exert transcrip tional effects that are consistent with their sequence speci c structural properties iyer struhl other motifs found here include at repeat and gt repeat motifs the mcb element and two unknown motifs that were also ranked highly in terms of speci city svetlov cooper these two distinct unknown motifs were found to be positionally biased and speci c for rrna and trna synthesis and processing one is the rrpe motif discussed figure motifs ranked by positional bias score for each cluster the statistics for the motif member with the best lowest positional bias score are listed see the legend to figure for details identi cation of regulatory elements in yeast above and the other is a motif with consensus gatgag that has been noted before it was named the polymerase a and c box pac box because of its association with polymerase a and c subunits dequard chablat et al but as yet neither a function nor a trans acting factor for this motif has been identi ed b we ran alignace on each of randomly cho sen sets of 80 and orfs varying map score and group speci city cutoffs were applied as in the functional categories and motif clustering was performed see table for a com parison of results runs of alignace on the upstream regions of genes in functional categories did result in higher scoring motifs overall there were a number of motifs from the randomly cho sen sets of orfs however that scored well within the range of some real motifs from the functional category runs inspection of the best of these motifs showed no indication that any of them might cor respond to motifs noted in the literature if one considers these motifs to represent the background noise inherent in this method then table may be used to choose cutoffs with pre scribed false positive rates accordingly four fths of the motifs listed in figure should correspond to a real signal above that background in cases in which one searches upstream of genes that are known to be controlled by a common transcription factor the false positive rates estimated in this manner are likely too high greater credence would be given to any motif speci c to a coregu lated group of genes as opposed to a motif speci c to a randomly chosen group of genes the group speci city statistic can be modi ed to compare a motif targets against a set of genes other than that used to nd the motif we refer to a statistic measuring the speci city of a motif for some different group of genes as cross speci city using this measure motifs from the alignace runs on randomly selected groups of orfs are found to have cross speci cities of less than for one or more functional categories this is despite the fact that no randomly chosen set of orfs included more than ve members of any of the smaller functional categories those having less than orfs or of the members of any of the lar ger functional categories those having or more orfs all of these motifs correspond to one of the following rpn4p pac box rrpe and ecb there were numerous matches to each of the rst four motifs but only one match to the ecb motif by comparison no motif found from any functional category had a cross speci city of less than to any of the randomly chosen orf sets the most positionally biased motifs found in the negative controls are very similar to those found analogously in the functional groups analysis figure these include the pac box and rrpe the only novel motif was derived from amino acid repeats in the coding regions of two proteins found very nearby or over lapping one of the randomly chosen orfs many of the motifs found by this method correspond to known real motifs though no biological infor mation beyond the genome sequence and pre dicted translation start sites was used to nd them positive controls motifs from searches upstream of genes with known transcription factor binding sites groups of genes controlled by known transcrip tion factors were used for the positive controls zhu zhang only factors having ve or more unique reported binding sites were con sidered alignace was used as above to search for motifs upstream of the reportedly controlled genes and the resulting alignments were checked for the presence of the sites cited in the literature a motif was considered a match if it contained half or more of the literature sites in its alignment or if half or more of the aligned sites were cited in the literature an alignment corresponding to the lit table comparison between alignace runs on upstream regions of orfs in functional categories and randomly chosen sets of orfs columns list the numbers of motifs found from random groups and functional categories having group speci city scores less than the cutoff listed in the rst column and map scores greater than that listed in the column headings the number of indepen dent motif clusters is listed in parentheses erature motif was found in of the test cases of the eight that were not found here ve were found in appropriate functional category runs therefore it is likely that many of the false nega tives are the result of the limited number of true sites in the small input sequence sets in any case the false negative rate is no more than and with appropriate input data might be much lower discussion we present a set of analytical tools for the com putational discovery and validation of cis acting regulatory elements in a sequenced and annotated genome the group speci city score is a useful statistic for gauging whether a given motif is real in the sense that it describes a sequence feature that is function ally relevant for the genes under consideration this measure is independent of the method being used to nd motifs it works as long as there is a method of ranking potentially regulated gene targets and could therefore serve as an independent measure by which to judge the performance of different motif nding algorithms alternatively different methods of grouping genes could be rated by the ability of those groupings to lead to the discovery of very self speci c motifs the group speci city score also might serve as a new basis on which to design improved motif nding algorithms the observation that some real motifs are prefer entially located in certain distance ranges upstream of translational start sites is intriguing the most positionally biased motifs tend to have sites cen tered around positions between and relative to translational start since the utrs in s cerevisiae are very short this could indicate that a precise positioning relative to the transcriptional start site is necessary for the function of these motifs alternatively since some of these motifs regulate the transcription of multiprotein com plexes one possible explanation for their precise positioning is that nearly identical modes of tran scriptional induction and translational ef ciency are required for the stoichiometric production of the protein subunits the fact that there are many motifs that do not demonstrate this property implies that the reason for this positional bias whatever it may be is not a property of all tran scription factor binding sites the method presented here is applicable to groups of genes other than functional categories possibilities include clusters of genes sharing com mon expression pro les across different conditions sets of genes sharing a common phenotype and genes coding for interacting proteins with the advent of high throughput technologies in many cases it is becoming possible to obtain these types of information on a whole genome basis with only one or a few experiments furthermore although the s cerevisiae genome with its compact upstream regions and independently transcribed genes seems ideal for the approach used here to nd motifs it may prove applicable to many other organisms alignace has already proven useful in bacterial genomes mcguire et al unpublished results though some distinct challenges will be encountered as it is applied to larger eukaryotic organisms as new technologies generate great quantities of data concerning organisms about which little is known the methods presented here perhaps in the context of a more general model of genetic networks could help to piece together much of the functionality of these organisms the analysis performed here will also form the starting point for a database of information about known and hypothetical sequence control features in s cerevisiae and other organisms not only will researchers be able to use these tools to determine the most likely potential regulatory sequences for the genes they are studying but they will be able to quickly determine whether the resulting hypothetical motifs are similar to any known or already suspected motifs methods alignace alignace is an algorithm implemented in c for nding multiple motifs in any given set of dna input sequences we de ne a motif as the characteristic base frequency patterns of the most information rich columns of a set of aligned sites alignace is based on a gibbs sampling algorithm previously used to nd motifs in protein sequences neuwald et al lawrence et al liu et al it differs from this method in the following ways the motif model was changed so that the base frequencies for non site sequence was xed according to the source genome a t in the case of s cerevisiae both strands of the input sequence are simultaneously considered at each step of the algor ithm overlapping sites are not allowed even if the sites are on opposite strands simultaneous multiple motif searching was replaced by an approach in which single motifs were found and iteratively masked the masking is done by determining the most information rich col umn in each motif mapping that column back to the input sequences and placing a marker at each of those positions the sampler is then reinitialized to nd another motif with the stipulation that no sites that con tain a masking marker may be resampled such sites may however be added to any found motif at the end of sampling so that the alignace output includes all rel evant sites for each output motif in the case of a very strong motif it is possible for the motif to have one of its positions masked and yet still retain enough information in its other positions for a variant of the original motif to be found we refer to these as mask variants the near optimum sampling method used by alignace is different from that used by neuwald et al the map score is now the criterion on which the nal output motif is based see below alignace accepts input either as a fasta formatted sequence le or as a list of orf names along with an sgd orf table and a fasta formatted genome sequence in the latter case alignace will take sequence upstream of the translational starts of the listed orfs for identi cation of regulatory elements in yeast motif searching the translational start site is used as a proxy for the transcriptional start site since the latter is dif cult to determine computationally the amount of sequence to be taken is speci ed by parameters such that at least a minimum amount of sequence is taken default bp and as much as a maximum amount is taken default bp so long as sequence belonging to other orfs transfer rnas trnas small nuclear snrnas and transposons are not included in the case that some orf overlaps an orf of interest including part of its upstream region the presence of that orf is ignored it is assumed that only one of such pairs of overlapping orfs in s cerevisiae is real since a number of upstream regions in s cerevisiae are nearly identical alignace also includes the option to purge very similar input sequence before sampling a smith waterman algorithm smith waterman is used to nd such sets of repeated input sequences all but one of which are then removed from consideration the cutoffs used for this are such that at least sequence identity is required for a sequence to be purged all results generated for this work used version of alignace the only non default options used were ÿy automatic selection of upstream regions and ÿe pur ging of input sequences based on smith waterman com parisons map score the map maximum a priori log likelihood score is used by alignace to judge different motifs sampled during the course of the algorithm a crude but useful approximation is given by the formula n log r where n is the number of aligned sites and r is the degree of over representation of the motif in the input sequence in other words if a site matching a given motif is expected to occur once every kilobase according to background geno mic mononucleotide frequencies and ten sites are observed in kb of input sequence then r ˆ a detailed development of the formula is given by liu et al the general properties of map score can be summar ized by stating that all of the following lead to higher scores for otherwise similar motifs greater numbers of aligned sites more tightly conserved motifs less total input sequence more tightly packed infor mation rich positions and enrichment of the motif with nucleotides that are less prevalent in the genome scanace scanace is a program written in c that searches a genome for close matches to a motif found by alignace the scoring method used is identical with that used by alignace to sample sites speci cally the score s for a site q whose sequence as a function of pos ition is given by q p is here fp b is the number of bases of type b aligned at position p n is the number of aligned sites and pb is the genomic background nucleotide frequency for base b the rst term in the second equation above corresponds to the log of the frequency of a given base at a particular position in the motif alignment estimated with a baye sian prior distribution corresponding to the genomic mononucleotide frequencies and a total pseudocount of as is the default for alignace scanace can be set to return all genomic sites scoring better than a cutoff based on the mean and standard deviation of the scores of the aligned sites or it can return a given number of best sites the positions of the sites are returned along with information concerning neighboring genomic features according to the tables of orfs and other features it is given this information may then be used to generate the necessary data for cal culating group speci city and positional bias scores group specificity the group speci city score is a measure of how well a given motif targets the genes whose upstream regions were used to nd it for each motif scanace output is used to rank all orfs according to the strength of the site best matching the scoring matrix in each orfs upstream non coding region between and bp relative to translation start the top orfs in this list are compared to the genes in the group used to nd the motif more than orfs are included in the target list if there are ties according to the ranking criteria the probability that these sets would have the observed intersection or greater is calculated this probability is what we refer to as the group speci city score it is given by the formula where n is the total number of orfs at the time of these calculations and are the numbers of orfs in the group used to nd the motif and in the list of tar get genes respectively and x is the number of orfs in the intersection of the two lists each term of this sum represents the probability of having obtained an intersec tion of i orfs assuming a random sampling of the two sets of orfs being compared the sum s is then the probability of observing this intersection or greater this is the statistic we use to quantify the degree to which a motif is speci c to the orfs from which it was found if the assignment of sites to orfs was not as straightfor ward or if it was believed that the occurrence of multiple sites for a given orf was very signi cant it would be possible to modify this statistic to instead consider speci city between the genomic sites in the input sequence set and the top target sites in the genome this is the method used by mcguire et al unpublished results other variations are also possible where the matrix m is calculated as positional bias a statistic to measure positional bias was constructed as follows the best sites in the genome for a given motif are found and their positions relative to the trans lational start sites of the nearest orfs are extracted from the scanace output more than genomic sites are considered if many equally good sites are tied in the ranking for the best among these sites t are found within bp upstream of some orf the bp window containing the greatest number m of these t sites is considered further the probability of observing m or more sites out of a possible t in a bp window of a bp region is determined by the formula where w ˆ and ˆ to make the expected distri bution of randomly chosen sites as at as possible the presence of sites inside coding regions is ignored that is if a site is inside some orf and yet is also bp upstream of some other orf it is counted as occurring at relative to a start site the only deviations from this presumed at background occur when a complete orf is contained within the bp upstream region of another orf this happens for orfs since a sliding window of bp is being considered the expected distribution of scores for a randomly chosen site distribution is itself not at to determine what threshold score should be considered signi cant sample distributions of sites were randomly generated out of sets of randomly selected sites in a bp range only two scored better than and one better than by this statistic over one third of the motifs con sidered in this study passed a cutoff of indicating a very signi cant degree of positional bias compareace to compare motifs we chose a scoring method based on the pearson correlation coef cient between the nucleotide base frequencies of two motif alignments pietrokovski we decided to consider only align ments that contained at least the most informative six positions of each motif this precludes the possibility of high scores resulting from alignments involving only the weak regions of motifs the region of alignment is allowed to be as wide as necessary to accommodate these positions for each motif but is made no wider positions of unknown sequence are modeled as being 25 each a c g and t the nal score is the maximum value of the correlation coef cient over the space of all allowable alignments this score varies between and and approaches for a perfect match between motifs we have named this algorithm compareace by analogy to the related tools scanace and alignace motif clustering the speci c method used was the simple joining algorithm hartigan in which comparisons between groups of motifs are done by averaging all of the compareace scores between relevant pairs of motifs the purpose of the clustering in this case was not to highlight distant relationships but rather to automati cally group identical motifs a cutoff score of was used to de ne the cluster boundaries the clusters were largely insensitive to cutoffs in the range of 0 to 0 orienting motifs since dna sequences can be read in either of two ways for consistency we designed a method of orienting motifs the information weighted nucleotide base con tent of the motif is calculated with values ia ic ig and it the function ig ia ÿ it ÿ ic is evaluated and the motifs are orientated so that this value is posi tive as a result purine residues are preferentially dis played and g t rich motifs are displayed instead of a c rich motifs abstract background the avogadro project has developed an advanced molecule editor and visualizer designed for cross platform use in computational chemistry molecular modeling bioinformatics materials science and related areas it oﬀers flexible high quality rendering and a powerful plugin architecture typical uses include building molecular structures formatting input files and analyzing output of a wide variety of computational chemistry packages by using the cml file format as its native document type avogadro seeks to enhance the semantic accessibility of chemical data types results the work presented here details the avogadro library which is a framework providing a code library and application programming interface api with three dimensional visualization capabilities and has direct applications to research and education in the fields of chemistry physics materials science and biology the avogadro application provides a rich graphical interface using dynamically loaded plugins through the library itself the application and library can each be extended by implementing a plugin module in c or python to explore diﬀerent visualization techniques build manipulate molecular structures and interact with other programs we describe some example extensions one which uses a genetic algorithm to find stable crystal structures and one which interfaces with the packmol program to create packed solvated structures for molecular dynamics simulations the 0 release series of avogadro is the main focus of the results discussed here conclusions avogadro oﬀers a semantic chemical builder and platform for visualization and analysis for users it oﬀers an easy to use builder integrated support for downloading from common databases such as pubchem and the protein data bank extracting chemical data from a wide variety of formats including computational chemistry output and native semantic support for the cml file format for developers it can be easily extended via a powerful plugin mechanism to support new features in organic chemistry inorganic complexes drug design materials biomolecules and simulations avogadro is freely available under an open source license from http avogadro openmolecules net background many fields such as chemistry materials science physics and biology need eﬃcient computer programs to both build and visualize molecular structures the field of molecular graphics is dominated by viewers with lit tle or no editing capabilities such as rasmol jmol pymol vmd qutemol ballview vesta and xcrysden 8 among many others the correspondence marcus hanwell kitware com department of chemistry university of pittsburgh parkman avenue pittsburgh pa usa of scientific computing kitware inc corporate drive clifton park ny usa full list of author information is available at the end of the article aforementioned viewers are all freely available and most of them are available under open source licenses and work on the most common operating systems linux apple mac os x microsoft windows and bsd the choice of software capable of building chemical structures in three dimensions is far smaller there are existing commercial packages such as cache scigress gaussview hyperchem crystalmaker materials studio and spartan which are polished and capable of constructing many diﬀerent types of molecular structures they are however not available for all operating systems most of them only run on microsoft windows and are not easily extensi ble customized or integrated into automated workflows hanwell et al licensee chemistry central ltd this is an open access article distributed under the terms of the creative commons attribution license http creativecommons org licenses by 0 which permits unrestricted use distribution and reproduction in any medium provided the original work is properly cited hanwell et al journal of cheminformatics page of http www jcheminf com content licensing costs can be prohibitive if the company were to change its direction or focus this can lead to a loss of a significant research investment in a commercial product furthermore in most cases these programs use custom proprietary file formats and semantic and chemical data can be lost in conversion to other data formats the selection of free open source cross platform three dimensional molecular builders was quite limited when the avogadro project was founded in late ghemical was one of the only projects satisfying these needs at the time two of the authors hutchison and curtis contributed to ghemical previously but had found that it was not easily extensible this led them to found a new project to address the issues they had observed in ghemical and other packages the molden applica tion was also available able to build up small molecules and analyze output from several quantum codes how ever it suﬀers from a restrictive license and it uses an antiquated graphical toolkit which is not native on most modern operating systems broad goals for the design of a molecular editor were identified following a case study of the available appli cations one of the main issues with both commercial and open source applications is a lack of extensibility many of the applications also only work on one or two operating systems the creation of an open and exten sible framework that implements many of the necessary foundations for a molecular builder and visualizer would facilitate more eﬀective research in this area further the open standardized chemical markup language cml file format would be used to secure semantic and chemical data and allow easy interoperability with other chemistry software at the time of writing it is apparent that other researchers have perceived similar needs several new applications are available today that focus on both build ing and visualizing molecular structure these include gabedit and some highly specific edi tors such as macmolplt which focus on particular computational packages i e gamess us for macmol plt whilst oﬀering many interesting and useful fea tures these projects suﬀer from the same issues centering around eﬀective reuse of existing code well commented and documented code and easy extension to add new features and adapt for specialized areas implementation the avogadro project was started in earnest in and over the first years of development has been downloaded over times 24 been translated into over lan guages 25 and has over contributors so far it has been cited over times including applications in spectroscopy catalysis materials chemistry theoretical chemistry biochemistry and molecular dynamics among many others from the beginning the project has strived to make a robust flexible framework for both building and visual izing molecular structures much of the initial focus has been placed on preparing input and analyzing output from quantum calculations other applications such as prepar ing input for md simulations and visualizing periodic structures will also be presented demonstrating the flex ibility of the avogadro platform the development team has also been members of the blue obelisk movement fol lowing the three pillars outlined by the group open data open standards and open source software architecture one area that seems to suﬀer in many code bases in chemistry is software architecture this can lead to less maintainable code poor code reuse and a much higher barrier to entry problems were identified in other projects with a view to minimize their impact when developing avogadro modern software design processes were used in the initial planning stages of avogadro along with the choice of modern programming languages and libraries avogadro has close ties to several other free cross platform open source projects to reuse as much code as is practical these projects include qt to pro vide a free cross platform graphical toolkit open babel for chemical file input output geometry optimiza tion and other chemical perception eigen for matrix and vector mathematics opengl glsl for real time three dimensional rendering and pov ray for ray traced rendering based on the previous experience of the authors and a review of available programs at the time several fun damental choices were made the c programming language the qt graphical toolkit opengl for visu alization cmake as the build system and open babel as the chemical library using this combination of languages and libraries requires the project to be licensed under the gnu license and made openly available to all the core of avogadro is written in portable c code with platform specific diﬀerences abstracted away by qt opengl and open babel the cmake build system makes the build process relatively simple on all supported platforms avogadro has been successfully built and tested on linux apple mac os x and microsoft windows in common and bit hardware architectures the avogadro framework uses the model view con troller paradigm the model is comprised of the core data classes such as molecule atom and bond views are made up of the engine display plugins and controllers are the tools interactive mouse and extensions non interactive form based menu based every plugin has full access to the core data model but view and controller hanwell et al journal of cheminformatics page of http www jcheminf com content plugins are conceptually diﬀerent views are responsible for displaying data and controllers are responsible for modifying changing data plugins rely on avogadro set of programming inter faces and almost all functionality is implemented in self contained plugins that are loaded at runtime the major ity of plugins distributed with avogadro are written in c but the api is also available in the python script ing language this allows for a great deal of choice in how plugins are implemented each plugin is a singleton class that implements a particular set of functions depending on the type of plugin which allows for features to be implemented in a very modular way over the last few years avogadro development has started to use nightly builds of the latest version of the code in order to automatically flag issues introduced in new commits code review was also introduced in order to add a review step before new code is merged along with softening the line between someone with commit rights and someone without anyone can propose and upload a patch but a small group can choose if when the patch will be merged some automated testing has been added but coverage at this point remains relatively low api docu mentation is automatically generated from comments in the code using doxygen plugin interface avogadro plugins are divided into four diﬀerent types corresponding to four main classes that derive from this common base class specializing their interface for specific activities figure the avogadro color base class defines the virtual interface for applying colors to atoms bonds and other properties avogadro engine defines the common interface for all display types in avogadro simple ball and stick van der waals visualizations surfaces and force visualizations the avogadro tool base class provides the interface for all interactive tools focusing principally on mouse and keyboard interaction with avogadro examples of tool plugins include the draw tool used to draw molecules atom by atom and the navigation tool used to pan rotate and scale the view of the molecule there are also several specialized tools such as the alignment tool finally there is the avogadro extension class which defines the interface for dialog based plugins these extensions can interact with the molecule and are used for a variety of purposes from molecule prop erties dialogs to input file generation dialogs for many quantum codes including nwchem gaussian gamess and others this class of plugin is also applied to file import and network aware extensions querying web databases for structures given their common name for example at start up several standard directories which may be customized are searched for plugins the qt plugin framework is used to check that the plugins have a recent enough version to be loaded and the plugin type can be deduced once loaded the user interface is then populated with appropriate entries tools are added to the main tool bar using their embedded icons display types are added opengl pov ray manipulate tool painters tools python scripting tools balls engines sticks force extensions fields colors elements open babel qt eigen figure general code architecture of avogadro general code architecture of avogadro indicating major plugin interfaces for colors display engines tools and extensions red boxes indicate code dependencies of avogadro blue boxes indicate plugin api classes and green boxes inidicate examples of each plugin type hanwell et al journal of cheminformatics page of http www jcheminf com content to the display type list and menu entries are added for all loaded extensions the tool and display type plugins can both option ally provide a dialog for configuring the plugin dialogs are specific to each plugin and integrated into the user interface display types display plugins are referred to as engines internally their primary focus is rendering graphics to the screen as is the case with most molecular graphics a large por tion of the geometric primitives are spheres and cylinders typically used to represent atoms and bonds there are many other properties that can be rendered using the dis play type plugins for example some of the engines also convey information about the underlying data the geomet ric primitives represent to allow for the molecule to be edited table shows a summary of the display plugins distributed with avogadro engines are performance critical as the render functions are called each time a frame is requested for display eﬃ cient rendering is also critical since multiple display types table list of default display type engine plugins can be combined to form a composite display for exam ple ball and stick display overlaid with a transparent van der waals space filling display and ring rendering to high light all rings in the structure figure d and f show two such combinations of multiple display types tools the tools are responsible for virtually all mouse and key board interaction with the molecule a list of all tools is given in table the navigation tool provides basic scene navigation implementing rotation panning tilting and zooming sup port the initial point of interaction where the click occurs changes the anchor point for navigation navi gation takes place about the center of molecule when clicking in empty space or about the center of any clicked atom during interaction the navigation tool provides visual cues to show what type of navigation is taking place the navigation tool is also used as the default tool if the currently active tool does not handle the mouse event passed to it one of the other central tools is the draw tool which implements a free hand molecule drawing input method supporting keyboard shortcuts combo boxes and a peri odic table view to select elements the user can use the left mouse button to add new atoms or bonds or click on the bonds to change their order the right mouse button can be used to delete atoms or bonds and the directional keys can be used in combination with the mouse to quickly rotate pan the molecule there are also two tools for adjustment of structures atom or bond centric a selection tool supporting stan dard selection interactions and an auto rotate tool that allows users to set the speed and angles about which to rotate the molecule the interactive auto optimization tool provides a sculpting interaction where the user can begin a continuous geometry optimization and switch back to the draw or adjustment tools and change the shape and structure of the molecule while observing the new structure being optimized this can also be combined with the measurement tool to interactively observe bond lengths and angles evolve as the structure is updated and the geometry minimized if the optimization tool is turned oﬀ the measurement tool also allows the user to precisely adjust bond lengths and or angles using the adjustment tools extensions extensions represent quite a diverse range of plugins including input generation dialogs for various quantum chemistry codes such as gamess molpro nwchem etc animation of the molecule and visualization of molecular orbitals and electron density network aware extensions allow the user to click on a menu item to fetch hanwell et al journal of cheminformatics page of http www jcheminf com content figure standard molecular structure representations several molecular representations of thiophene a wireframe b stick licorice c ball and stick d ball and stick with ring e van der waals cpk and f transparent van der waal with stick by chemical name and search for tnt or propanol and have structures returned by the nih cactus chem ical structure resolver service a summary of the extensions distributed with avogadro is shown in table other extensions translate the entire scene to pov ray input and call pov ray to render the molecule using ray tracing techniques to provide higher quality renderings for publication various molecular property dialogs are also implemented as plugins drawing largely on open babel functionality to provide an overview of the molecule cartesian editors addition and removal of hydrogens fragment smiles and peptide insertion table list of default mouse tool plugins are all implemented as extensions showing up in avo gadro menus more recently a crystallography extension was added giving access to a much wider range to func tionality useful to practitioners in that area including miller plane visualization slab and surface generation new builders for nanotubes nanoparticles and dna are also planned for upcoming releases colors the color plugins primarily take either double precision numbers or integer values and return an rgb value the plugins range from the standard color plugin that takes atomic number and returns the standard rgb value for that element through to mapping things like partial change and index to more easily view various aspects of the molecule structure by defining a plugin interface for coloring atoms bonds or residues developers can easily oﬀer flexible render ing options to highlight important information without requiring a user to tediously set colors on specific atoms or functional groups default color plugins are listed in table illustrating the variety of options each plugin is usually only 50 lines of c code python scripting python bindings are provided for all of the core api python code can be used in two ways the first is the inter active python terminal and the second is to write python plugins extensions tools or display types writing a hanwell et al journal of cheminformatics page of http www jcheminf com content table list of default extension commands python plugin requires the same functionality to be imple mented as a native c plugin the advantage of python plugins is that it easier to make prototypes since no compilation is required python plugins can also easily be shared with other users the python bindings interface with the pyqt python bindings for the qt toolkit which enables python code to use all of qt features when writing a plugin for example a short python script can present a window using qt and render molecules using avogadro avogadro also includes an interactive python console figure which allows users to directly script and manip ulate the avogadro environment results and discussion the graphical user interface the first thing most people will see is the main avo gadro application window as shown in figure binary installers are provided for apple mac os x and microsoft windows along with packages for all of the major linux distributions this means that avogadro can be installed quite easily on most operating systems easy to follow instructions on how to compile the latest source code are also provided on the main avogadro web site for the more adventurous or those using an operating system that is not yet supported the qt toolkit gives avogadro a native look and feel on the three major supported operating systems linux apple mac os x and microsoft windows the basic functionality expected in a molecular builder and viewer has been implemented along with several less common features it is very easy for new users to install avogadro and build their first molecules within minutes thanks to the open babel library avogadro supports a large portion of the chemical file formats that are in common use the vast majority of this functionality has been writ ten using the interface made available to plugin writers and is loaded at runtime we will discuss these plugin interfaces and descriptions of the plugin types later semantic chemistry avogadro has used cml as its default file format from a very early stage this was chosen over other file formats because of the extensible semantic structure pro vided by cml and the support available in open babel the cml format oﬀers a number of advantages over others in common use including the ability to extend the format this allows avogadro and other programs to be future proof adding new information and features neces sary for an advanced semantically aware editor at a later time while still remaining readable in older versions of avogadro through the use of open babel a large array of file formats can be interpreted when extending avogadro to read in larger amounts of the output from quantum codes it was necessary to devote significant develop ment resources to understanding and adding semantic meaning to the quantum code output this work was developed in a plugin which was later split out into table list of default color plugins hanwell et al journal of cheminformatics page of http www jcheminf com content figure python scripting terminal printing atomic numbers a small independent library called openqube more recently a large amount of work has been done by the quixote project jumbo converters and the semantic physical science workshop to augment quantum codes to output more of this data directly from the code since cml can be extended it is possible to reuse exist ing conventions for molecular structure data and add new conventions for the additional quantum data building a molecule atom by atom after opening avogadro a window such as that shown in figure is presented by default the draw tool is selected simply left clicking on the black part of the display allows the user to draw a carbon atom if the user pushes the left mouse button down and drags a bonded carbon atom is drawn between the start point and the final position where the mouse is released a large amount of eﬀort has been expended to create an intuitive tool for drawing small molecules common chemical elements can be selected from a drop down list or a periodic table can be displayed to select less com mon elements clicking on an existing atom changes it to the currently selected element dragging changes the atom back to its previous element and draws a new atom figure the avogadro graphical user interface taken on mac os x showing the editing interface for a molecule hanwell et al journal of cheminformatics page 8 of http www jcheminf com content bonded to the original if the bonds are left clicked then the bond order cycles between single double and triple shortcut keys are also available e g typing the atomic symbol e g c o for cobalt changes the selected ele ment or typing the numbers and changes the bond order right clicking on atoms or bonds deletes them if the adjust hydrogens box is checked the number of hydro gens bonded to each atom is automatically adjusted to satisfy valency alternatively this can also be done at the end of an editing session by using the add hydrogens extension in the build menu in addition to the draw tool there are two tools for adjusting the position of atoms in existing molecules the atom centric manipulate tool can be used to move an atom or a group of selected atoms the bond centric manipulate tool can be used to select a bond and then adjust all atoms positions relative to the selected bond in various ways e g altering the bond length bond angles or dihedral angles these three tools allow for a great deal of flexibility in building small molecules interactively on screen once the molecular structure is complete the force field extension can be used to perform a geometry opti mization by clicking on extensions and optimize geometry a fast geometry optimization is performed on the molecule the force field and calculation parameters can be adjusted but the defaults are adequate for most molecules this workflow is typical when building up a small molecular structures for use as input to quantum calculations or publication quality figures an alternative is to combine the auto optimization tool with the drawing tool this presents a unique way of sculpting the molecule while the geometry is constantly minimized in the background the geometry optimiza tion is animated and the eﬀect of changing bond orders adding new groups or removing groups can be observed interactively several dialogs are implemented to provide information on molecule properties and to precisely change parame ters such as the cartesian coordinates of the atoms in the molecule building a molecule from fragments in addition to building molecules atom by atom users can insert pre built fragments of common molecules ligands or amino acid sequences as shown in figure in all cases after inserting the fragment the atom centered manipulate tool is selected allowing the fragment to be moved or rotated into position easily users can also insert a smiles string for a molecule in this case a rough geometry is generated using open babel and a quick force field optimization preparing input for quantum codes several extensions were developed for avogadro that assist the user in preparing input files for popular quantum codes such as gamess us nwchem gaussian q chem molpro and the graphical dialogs present the fea tures required to run basic quantum calculations some examples are shown in figure the preview of the input file at the bottom of each dialog is updated as options are changed this approach helps new users of quantum codes to learn the syntax of input files for diﬀerent codes and to quickly generate use ful input files as they learn the input can also be edited by hand in the dialog before the file is saved and sub mitted to the quantum code the mopac extension can also run the program directly if it is avail able on the user computer and then reload the output figure 5 dialogs for inserting pre built fragments the left shows molecules and the right amino acid sequences hanwell et al journal of cheminformatics page of http www jcheminf com content figure dialog for generating input for quantum codes dialogs for generating input for q chem nwchem molpro and note that the dialogs are similar in interface allowing users to use multiple computational chemistry packages file into avogadro once the calculation is complete this feature will be extended to other quantum codes in future versions of avogadro the gamess us plugin is one of the most highly developed featuring a basic dialog present in most of the other input deck generators as well as an advanced dialog exposing many of the more unusual and complex calcula tion types in addition to the advanced dialog the input deck can be edited inline and features syntax highlight ing figure as used in many popular editors aimed at software developers this can indicate simple typing errors in keywords as well as harder to spot whitespace errors that would otherwise cause the hand edited input deck to fail when being read by gamess us alignment and measurements one of the specialized tools included in the standard avo gadro distribution is the alignment tool this mouse tool facilitates the alignment of a molecular structure with the coordinate origin if one atom is selected and along the specified axis if two atoms are selected the align ment tool can be combined with the measure select and manipulate tools to create inputs for quantum codes where the position and orientation of the molecule is important one example of this is calculations where an external electric field is applied to the molecule in these types of calculations the alignment of the molecule can have a large eﬀect figure 8 shows the measurement tool in action with the alignment tool configuration dialog visible in the lower left corner more complex alignment tools for specific tasks could be created the alignment tool was created in just a few hours for a specific research project this is a prime exam ple where extensibility was very important for performing research using a graphical computational chemistry tool it would not be worth the investment to create a new application just to align molecular structures to an axis but creating a plugin for an extensible project is not unreasonable hanwell et al journal of cheminformatics page of http www jcheminf com content 1 figure the gamess us input deck generator this input generator has an advanced panel and syntax highlighting visualization the avogadro application uses opengl to render molec ular representations to the screen interactively opengl oﬀers a high level cross platform api for rendering three dimensional images using hardware accelerated graphics opengl 1 1 and below is used in most of the rendering code and so avogadro can be used even on older computer systems or those without more modern accelerated graphics it is capable of taking advantage of some of the newer features available in opengl 0 as described below but this has been kept as an optional extra feature when working on novel visualizations of molecular structure standard representations in chemistry there are several standard representations of molecular structure originally based upon those possible with physical models the avogadro application imple ments each of these representations shown in figure as a plugin these range from the simple wireframe repre sentation stick licorice ball and stick and van der waals spheres it is also possible to combine several representations such as ball and stick with ring rendering figure d and a semi transparent van der waals space filling repre sentation with a stick representation to elucidate molecu lar backbone figure f quantum calculations and electronic structure quantum codes were originally developed for line print ers and unfortunately little has changed since then in the standard log files there are several formats developed for use in other codes and specifically for visualization and analysis but there is little agreement on any standard file format in the computational quantum chemistry commu nity a plugin was developed in avogadro to visualize the output of various quantum codes and get the data into the right format for further visualization and analysis initially support was added and extended in open babel for gaussian cube files this format provides atomic coor dinates and one or more regularly spaced grids of scalar values this can be read in and techniques such as the marching cubes algorithm can be used to compute trian gular meshes of isosurfaces at values of electron density for example once the code has been developed to visu alize these isosurfaces it became clear that it would be useful to be able to calculate these cubes on the fly and at diﬀerent levels of detail depending upon the intended use the first format which was somewhat documented at the time it was developed is the gaussian formatted checkpoint format this format is much easier to parse than the log files generated as the program runs and provides all of the detail needed to calculate scalar val ues of the molecular orbital or electron density at any point in space once a class structure had been developed hanwell et al journal of cheminformatics page of http www jcheminf com content 1 figure 8 the measurement tool the measurement tool being used to measure bond angles and lengths on linux with kde for gaussian type orbitals the approach was extended to read in several other popular output file formats including q chem gamess us nwchem and mol pro support was added later along with support for the aux format and slater type orbitals used in that code all of these codes output their final config urations using the standard linear combination of atomic orbitals meaning that parallelization is extremely simple the plugin was developed to take advantage of the map reduce approach oﬀered by qtconcurrent in order to use all available processor cores this oﬀers almost lin ear scaling as each point in the grid can be calculated independently of all other points the results of which can be seen in figure an alternate approach to calculating the molecular orbitals was developed in a second plu gin that has since been split oﬀ into a separate project named openqube the openqube library has also been added as an optional backend in vtk during the google summer of code bringing support for sev eral output file formats and calculation of cube files that can later be fed into more advanced data pipelines a class hierarchy with a standard api is provided for quantum output adding support for new codes involved developing a new parser and ensuring the gaussian or figure molecular orbitals and surfaces rendering of a molecular orbital isosurface left and an electrostatic surface potential mapped onto the electron density right hanwell et al journal of cheminformatics page of http www jcheminf com content 1 slater set is populated with the correct ordering and the expected normalization scheme the p and d type gaussian orbitals are supported with f and g support planned in order to support the increasing number of calculations using these higher order orbitals the basis set exchange hosted by emsl provides access to the basis sets in common use although at present these basis sets are normally read in directly from the output files there are several related projects for adding semantic meaning to this type of output including the jumbo converters project and quixote it is hoped that more codes will adopt semantic output in the future using a common format so that data exchange validation and analysis become easier across several codes this was the subject of a recent meeting with several computa tional chemistry codes beginning to use fox in order to output cml development has begun on code to read in cml output either directly from the codes or from conversion of other formats using open babel or the jumbo converters if enough semantic structure can be added to cml and the converters support a large enough range of the output this could replace most of the parsing code present in openqube semantic mean ing is one of the most diﬃcult to extract from log files and coming together as a community will help projects like avogadro to derive more meaning from the outputs of these codes secondary biological structure avogadro uses the pdb reader from open babel to read in the secondary biological structure two plugins exist to process and render this information the first is a plu gin which renders a simple tube between the biomolecule backbone atoms a second more advanced plugin calcu lates meshes for the alpha helices and beta sheets while the first plugin is much faster the advanced plugin more accurately produces output expected in the field this allows users flexibility for rendering secondary biological structures glsl novel visualization glsl or opengl shader language is a c like syntax that can be used to develop code that will run on graphics cards and included in the opengl 0 specification it has been used to great eﬀect by the games industry as well as in many areas of data visualization several recent papers highlight the potential in chemistry such as qutemol 5 in adding support for features such as ambient occlusion to add depth to images avogadro has support for vertex and fragment shader programs and several examples are bundled with the package if the user graphics card is capable these pro grams can be loaded at runtime and used to great eﬀect to visualize structure some of these include summariza tion techniques such as isosurface rendering where only the edges orthogonal to the view plane are visible giv ing a much better rendering of both the molecular and electronic structure figure ray tracing avogadro uses a painter abstraction that makes it much easier for developers to add new display types it also abstracts away the renderer making it possible to add support for alternative backends currently only opengl and pov ray are supported due to the abstraction we are able to use the implicit surfaces available in ray trac ers to render molecular structure at very high levels of clarity and with none of the triangle artifacts present in standard opengl rendered images much higher quality transparency and reflection also allow for the images to be used in poster and oral presentations as well as research articles figure this feature is implemented in an extension with an additional painter class deriving from the base class and a dialog allowing the user to edit the basic rendering controls the pov ray input file can also be retained and edited to produce more complex images or to allow for much finer control of the rendering process if desired figure molecular orbitals rendering using glsl shaders rendering of a molecular orbital isosurface using two glsl shaders to highlight the edges of the surfaces the x ray eﬀect left and red and blue right showing the positive and negative molecular orbital shapes hanwell et al journal of cheminformatics page of http www jcheminf com content 1 figure ray traced homo isosurfaces of varying cube density rendering of a molecular orbital isosurface using pov ray with cubes of low left and high right density avogadro library in use the avogadro library first use was the avogadro appli cation closely followed by the kalzium periodic table program that is part of the kde software collection this initial work was funded in part by the google summer of code program in and also resulted in the addi tion of several other features in the avogadro library to support kalzium and general visualization and editing of molecular structure figure the q chem package has developed qui the q chem user interface around avogadro originally as an avogadro extension this is a more advanced ver sion of the input generator developed in avogadro with much tighter integration molpro has also published some results from their development of a molpro interface using the avogadro library packmol packmol is a third party package designed to create ini tial packed configurations of molecules for molecular dynamics or other simulations examples include surrounding a protein with solvent solvent mixtures lipid bilayers spherical micelles placing counterions adding ligands to nanoparticles etc typically users may have equilibrated solvent boxes which have been run for long simulations to ensure proper density and both short and long range interactions between solvent molecules using such solvent boxes allows placing solute molecules such figure the kalzium application in kde using avogadro to render molecular structures hanwell et al journal of cheminformatics page of http www jcheminf com content 1 figure the packmol lipid layer as produced by the packmol extension as proteins in an approximately correct initial structure such as that shown in figure the solute is added into the box and solvent molecules with overlapping atoms are removed while these utilities are often enough creating complex input files is not always easy for more compli cated systems packmol can create an initial configuration based on defined densities geometries e g sphere box etc and the molecules to be placed an avogadro devel oper wrote an external plugin to facilitate use of packmol including estimating the number of molecules in a given volume the plugin is not currently distributed with avogadro as a standard feature although it is planned for some future version it serves as an example of how avogadro can facil itate a workflow with a text oriented package packmol including saving files in the pdb format required by pack mol generating an input file and reading the output for visualization analysis and further simulations figure the xtalopt extension xtalopt extension showing a plot of stability vs search progress for a supercell hanwell et al journal of cheminformatics page of http www jcheminf com content 1 xtalopt the xtalopt software package is implemented as a third party c extension to avogadro and makes heavy use of the libavogadro api the extension implements an evolutionary algorithm tailored for crystal structure prediction the xtalopt development team chose avo gadro as a platform because of its open source license well designed api powerful visualization tools and intu itive user interface xtalopt exists as a dialog window figure and uses the main avogadro window for visu alizing candidate structures as they evolve the api is well suited for xtalopt needs providing a simple mechanism to allow the user to view edit and export the struc tures generated during the search taking advantage of the cross platform capabilities of avogadro and its dependen cies xtalopt is available for linux windows and mac conclusions avogadro has grown over its first six years to become an important tool for building editing visualizing and ana lyzing chemical and molecular data with over downloads language translations and localizations and over citations it has become an integral part of the chemical software toolbox through use of the native cml file format and a wide variety of chemical data import avogadro can provide semantic chemical data editing and conversion we seek to provide an inte grated environment in the simulation and cheminfor matics workflow while more must be done particularly in regards to documentation tutorials ease of use and automation we aim to improve the quality and feature set with each new release currently two upcoming versions of avogadro are under development the first is avogadro version 1 1 which adds additional features and refinement par ticularly including crystallography support developed through the xtalopt project the second is a more sub stantial development for avogadro version 2 0 where many of the core data structures are being rewritten in order to oﬀer greater flexibility and scalability our goal is to support an increasing scope of chemical sys tems including biomolecules dna rna saccarides etc materials crystallography polymers surfaces nanoscience nanoparticles nanotubes graphene etc with improved speed intuitive ease of use and simpler non reciprocal licensing terms avogadro is freely available from http avogadro openmolecules net and new contributors are welcome in all areas users developers testers translators educators students researchers dreamers abstract motivation in silico experiments in bioinformatics in volve the co ordinated use of computational tools and information repos itories a growing number of these resources are being made available with pro grammatic access in the form of web ser vices bio informatics scientists will need to orchestrate these web services in workflows as part of their analyses results the taverna project has developed a tool for the composition and enactment of bioinformat ics workflows for the life sciences community the tool includes a workbench application which pro vides a graphical user interface for the composition of workflows these workflows are written in a new language called the simple conceptual unified flow language scufl where by each step within a work flow rep resents one atomic task two examples are used to illustrate the ease by which in silico ex periments can be represented as scufl workflows using the workbench application availability the taverna workflow system is available as open source and can be downloaded with example scufl workflows from http taverna sourceforge net contact taverna users lists sourceforge net introduction an in silico experiment is a procedure involving the use of local and remote resources to test a hy pothesis derive a sum mary or search for patterns stevens et al in bio informatics resources may be information repositories such astheem blandswiss protdatabases orcomputa tionalanaly sis tools like blast and clustalw the analysis performed during an in silico exper iment frequently involves a combination of these resources which are linked in a specific order thus forming a workflow process for example a workflow to invest igate the evolutionary relationships between proteins might begin with ac quiring amino acid sequences belonging to a pro tein family from swiss prot and then applying the clustalw algorithm to align and iden tify patterns between sequences recently organizations have started to provide program matic access to bioinformatics information repositories and analysis tools based on web services stein a new dis tributed computing architecture which uses existing inter net communication and data exchange standards booth et al http www org tr ws arch resources with web service access provide a web based published applica tion programming interface for interaction with other applica tions web services have wide support in terms of commercial and open source tools for developing web services and client applications that use them examples of bioinformat ics web services include the xembl wang et al openbqs senger http industry ebi ac uk openbqs and soaplab analysis services senger et al hos ted by the european bioinformatics institute ebi the services provided by xml central of ddbj miyazaki and sugawara the kegg api kawashima et al http www genome ad jp kegg soap and a range of analysis services offered by the pathport project eckart and sobral the mygrid e science project aims to provide high level ser vice based middleware to support data intensive in silico bio informatics experiments using distributed resources goble et al these bioinformatics analyses depend on a work flow system which can converse with the interfaces of web services and direct the flow of data between resources there is considerable development in workflow tools in both the e science and e business community but it is a broad t oinn et al area with many competing proposals and no accepted stand ards van der aalst at the time it was re quired no tool satisfied all the mygrid requirements which include open source availability and the provision of a graphical user inter face for composing workflows mygrid also aims to support the recording of results from in silico experiments and proven ance information about how those results were obtained in addition there are existing bioin formatics applications with their own custom invocation mechanisms which life scient ists will want to incorporate into workflows interaction with these applications may be stateful with scripts used to define a series of activities to be performed examples of such tools are talisman appli cations oinn and soaplab services senger et al the latter are web services which have been created using soaplab a tool that can wrap command line pro grams with a web service interface based on a description of the analysis tool to be deployed tools which have been wrapped by soaplab include the programs distrib uted with in the european molecular biology open software suite emboss rice et al and algorithms such as blast altschul et al when scientists use web ser vices for in silico experiments they need to be abstracted from the detail and complexity of web service program ming furthermore scientists will want to integrate new types of ser vices as and when they arise in the bioinfor matics community these requirements have led to the in ception of the taverna project which has developed an open source workflow tool enabling scientists to orchestrate bioinformatics web ser vices and existing bioinformatics applications in workflows the initial users of taverna are bioinformaticians who can both develop and run work flows ongoing developments will address the wider life sci ences community from those wanting to find their group existing workflows run them and browse the results to those who want guidance in the modification of existing work flows to match their specific needs systems and methods the taverna project provides a graphical workbench tool for both creating and running workflows that represent in silico bioinformatics experiments in taverna a workflow is consid ered to be a graph of processors each of which transforms a set of data inputs into a set of data outputs these workflows are represented in the scufl language scufl language specification current languages were deemed unsuitable for composing scientific workflows since the existing standards are in flux and high quality free tools were not available to support stand ards oinn et al in addition web service standards do not have the levels of user abstraction neces sary for most bioinformaticians and do not offer support for the specifica tion of data processes or resources at a semantic level these requirements led to the specification of the simple conceptual unified flow language scufl it is a high level xml based conceptual language in which each processing step of the workflow represents one atomic task a workflow in the scufl language consists of three main entities processors a processor is a transformation that accepts a set of input data and produces a set of output data processors have a name within the scufl model and a set of both input and output ports during the execution of a workflow each processor has a current execution status which is one of initializing waiting run ning complete failed or aborted the main processor types cur rently available are arbitrary wsdl type this type of processor allows a single call on a web service operation the port names are derived from the message names in the web service description language wsdl file of the web service for the identified operation soaplab type this processor type calls a com plete soaplab invocation as one unit the end point should be the full uni form resource locator url of the service endpoint so in cluding e g edit seqret at the end the port names are ex trac ted from the keys of the soaplab input and output map objects talisman type this processor enables the invoca tion of a talisman session as a task in the scufl workflow the talis man processor requires a url of a talisman tscript xml file describing the talisman application definition a set of input and output data and the operational behaviours of this service in terms of talisman trigger invoca tions the port names are taken directly from the tscript file nested workflow type a processor of this type can invoke another child workflow currently only child workflows in scufl are supported the input and output entities of the workflow are the input and output ports respectively of this processor string constant type this type of processor has a single output port on which it returns a con stant string value this processor is of particular use where another processor in the same workflow requires a default value which acts as a pa rameter another use of this processor is the replacement of an input entity in test workflows figs and b local processor type this processor can be used to add new local functions that are coded as classes to comply with a simple java interface the local functions currently availa ble for use in workflows are shown in figure a workflow can also possess input and output data enti ties a workflow input can be considered to be a source composing and enacting workflows using taverna fig two example scufl workflows a the affyidtogeneannotation workflow used for obtaining information for a given gene refer enced by an affymetrix probe set identifier this workflow uses wsdl web service operations green boxes and soaplab services yellow boxes default inputs parameters in the form of string processors are shown as blue boxes and workflow output objects are displayed as blue triangles b the emblacctokeggpathway workflow retrieving pathway information associated with a given gene identified by its embl accession number this workflow integrates two disparate bioinformatics data resources srs and kegg t oinn et al fig the scufl workbench a number of views are provided by this application for the composition and enactment of scufl workflows a workflow can be viewed in its xml format a in graphical format b and as a tree structure using the scufl model explorer which is also used to manipulate the workflow c expanding a processor node reveals its inputs and outputs c the workbench includes a service pal ette for browsing local and remote services and workflows d an enactor launch panel is used to display provenance information and the results generated by enacted workflows e the enactor launch panel e shows the result of the godiagram workflow output b which is a subgraph of the go corresponding to terms associated with a swiss prot identifier processor which executes instantaneously and makes the input value available on its virtual output port a workflow output can be considered as a sink proces sor which receives a value from its virtual input port but never actually executes both workflow sources and sinks can be annotated with metadata three types of metadata can be associated with workflow inputs and outputs a mime type a semantic type based on the mygrid bioinformatics ontology wroe et al and a free textual description data links data links mediate the flow of data be tween a data source and a data sink the data source can be a processor output or a workflow input the data sink can be a processor input port or a workflow output each data sink will re ceive the same value if there are multiple links from a data source coordination constraints a coordination constraint links two pro cessors and controls their execution this level of control is re quired when there is a process where the stages must execute in a certain order and yet there is no direct data dependency between them for example coordination constraints can be used to allow one processor to go from scheduled to running if another processor has status completed in most cases composing and enacting workflows using taverna no concurrency constraints are required since data links will ensure that some processors stay in their waiting state until the data they require is available scufl workbench the taverna tool contains an application called the scufl work bench which enables bioinformaticians to write workflows without having to learn the scufl language figs and this application acts as a container for a number of user interface compo nents which provide read only views and read write controllers views involved in the composition and enactment of scufl workflows the scufl model ex plorer is a controller view that shows the state of the current model as a tree struc ture and is also used for defining the flow of data between processors fig at the top level are the different types of entities within a scufl model overall workflow in puts and outputs processors data links and coordi nation controls pro cessor nodes may be expanded to reveal their inputs and outputs fig the scufl diagram view provides a graphical display of the current workflow figs and b and this view uses the dot tool from graphviz to render the workflow as a png image gansner and north there is a range of display options supported by the graphical view users can view processors with all ports displayed no ports fig or only those ports which are bound to data links fig and b the scufl workflow can also be viewed in its xml representation as xscufl fig the xscufl and graphical displays of the workflow are read only since only the scufl model explorer is used to edit workflows the scufl workbench contains a service browser that provides a palette of processors fig con text menus in the service panel allow new processors to be added to the current scufl workflow model there are two methods of pop ulating the palette with services processors in the current scufl model can be scavenged which involves extracting the set of processors contained within the model and add ing them to the service palette the exact relation ship between a processor and an available service depends on the processor type an arbitrary wsdl processor corresponds to a web ser vice operation and when a wsdl document is scavenged the pal ette is populated with a service entry containing sub entries for each web service operation with soaplab processors a set of soaplab services from the same factory will be added to the palette the palette can also be populated from the web using scavengers for each processor type each scavenger requires a url which when pointed to a directory will perform a naïve search to find files that it can process pointing to a web scavenger at http taverna sourceforge net webservices in dex html will provide access to the xembl path port ddbj and fig an architectural view of the taverna workflow system the taverna workbench is used to compose scufl workflows which are parsed into a form that can be enacted by the freefluo enactor dif ferent types of services can be invoked by freefluo currently the enac tor can invoke wsdl web services soaplab services talisman and local applications workflows when directed with a url to a directory of scufl workflows fig workflows can be executed in the scufl workbench using the enactor launch panel fig this panel allows inputs to be specified for the workflow and launches a local instance of the freefluo enactment engine fig freefluo is a java workflow orchestration tool for web services that supports a subset of the web services flow language as well as scufl addis et al this flexibility of freefluo is provided at its core by a reusable orchestration framework that is not tied to any work flow language or execution architecture the enactor core sup ports an object model of a workflow in the form of a directed graph where each node has a state machine that defines its lifecycle scheduling and state transitions are driven by mes sage passing between nodes as the workflow progresses the core of the enactor is decoupled from both the textual form of a workflow specification and the details of service invocation and data model allowing it to orches trate workflows in a ge neric way fig this flexibility is exploited when taverna is extended to cope with a new pro cessor type the core of the enactor is unchanged but freefluo t oinn et al is extended with a parser for the new xscufl processor and a plug in for the required service invocation results the ease by which bioinformatics web services can be in teg rated with one another using taverna is demonstrated by two exemplar bioinformatics workflows the first work flow is a mygrid annotation pipeline process for obtaining information about a gene in the second workflow third party bioinformatics web services are used to retrieve me ta bolic or signalling pathway maps and associated nucleo tide sequence information for a given gene product a tuto rial and videos demonstrating how workflows can be com posed and enacted using the scufl workbench is available at http taverna sourceforge net gene annotation pipeline the querying of information repositories and the applica tion of analysis programs are frequently combined within an annotation pipeline for investigating the biology of genes an annotation pipeline using disparate bioinformat ics resources is easily composed by taverna using the scufl workbench an example of such a workflow called affy idtogene annotation is shown in figure this was one of a number of workflows written for biologists working on a microarray based approach to the genetic analysis of graves disease to evaluate whether taverna was capable of performing their analyses addis et al stevens et al the workflow obtains information about genes which may be involved in graves disease that have been identi fied by using affymetrix microarray chips three dis tinct resources were used within this annotation pipeline an instance of the sequence retrieval system srs at the ebi a mapping database called affymapper and programs deployed as soaplab services the key functionality in this workflow was the ability to map a candidate gene as refer enced by its affymetrix probe set identifier to entries in var ious bio logical databases the mappings from an affymet rix probe set identifier to its embl accession number and swiss prot iden tifier were obtained from the affymapper database service in addition the srs service was used to link from an embl accession number to medline identifi ers for obtaining biblio graphic citations of the gene in sci entific literature the srs service was also used to obtain the records associated with the embl accession number swiss prot and medline identifiers to obtain sequence and published information about the gene and its gene product identifiers to terms in the gene ontology go ashburner et al associated with the swiss prot identifier were also retrieved in this workflow the goviz service then ex tracted the sub graph of the go associated with the go identifiers to provide a more general view to the function of the gene product fig two bioinformatics tools performed analyses on the nuc leotide sequence associated with embl accession number and the amino acid sequence from the swiss prot record a tblast search with the nucleotide sequence against the protein data bank found proteins with known structures that may be related to the trans lated nucleotide sequence fig the pepstat program in emboss generated protein informa tion such as the molecular weight and isoe lectric point based on the amino acid sequence contained within the swiss prot record which was mapped from the affymetrix probe set identifier pathway map retrieval obtaining information about the metabolic or signalling path ways associated with a gene product is an analysis which is commonly per formed in bioinformatics yanai et al lee and sonnhammer hannenhalli and levy one approach to this analysis is to integrate data available from nucleotide sequence databases with a database contain ing pathway information as shown by the workflow called emblacctokeggpathway in figure this workflow starts by mapping the embl accession number gene handle to kegg gene identifiers using a cus tom keggmapper service the workflow retrieves the embl record associated with the embl accession number using the srs service while entries associated with the kegg gene identifiers are obtained using the kegg web service the kegg gene identifiers are then used to search the pathway dia grams in the kegg pathway database which contains the gene ob jects in the pathway diagrams that correspond to the genes on the map are highlighted and the url for each pathway diagram is returned by the kegg web service on completion of the workflow the scufl workbench displays the pathway diagram specified by each returned url fig this func tionality is based on the ability to associate syntactic and semantic metadata to input and output data objects in scufl workflows for example the x taverna url mime type was associated with each path way url returned by the getpathways task fig any output data object associated with this x taverna url mime type results in the document located at the url being retrieved and displayed within the workbench fig this ability to associate appropriate viewers with workflow outputs based on their types is one of the benefits of the taverna workbench tracking of data provenance the tracking of data provenance is also required during the execution of in silico experiments provenance is information that identifies the source and processing of data by record ing the metadata and inter mediate results associated with the workflow this type of information can be a useful audit trail when investigating how results in particular erroneous or unexpected ones have been produced by workflow pro cesses composing and enacting workflows using taverna fig the tblast report generated by the gene annotation pipeline workflow shown in figure during the enactment of workflows in taverna the proven ance information recorded comprises of technical metadata showing how each task has been performed fig the type of processor sta tus start and end time and a descrip tion of the ser vice operation used are recorded and may be browsed fig in the future we aim to unify the provenance production and collection from a variety of dispar ate systems includ ing workflow database query and manual annotation tools by defining a stand ard set of relations and classes to be used in re source description framework statements generated as a side effect of the primary function of these sys tems state ments can be merged from the different sources that data may have passed through en route to its final form this will allow questions about the origin of any given item of data in a data store to be answered even though such data may be derived from a combination of otherwise distinct op erations discussion with the increasing number of bioinformatics databases and computational programs being made available as web ser vices a workflow system is an essential tool for e scientists if they are to take full advantage of such resources a tool for the creation and enactment of scientific workflows has been developed by the taverna project this project has developed the scufl language for the scripting of workflows and a parser for the freefluo enactor to enable it to execute scufl workflow definitions the scufl workbench provides a t oinn et al fig one of the pathway diagrams retrieved from the kegg pathway database as a result of the workflow shown in figure graphical user interface for the composition of scufl work flow definitions this allows taverna users to concentrate on cap turing their in silico research experiments as work flows rather than having to spend time and effort learning the syntax of the scufl workflow language before they can compose workflows the two workflows presented in figure and b shows how taverna orchestrates data resources and computa tional ser vices to undertake bioinformatics analyses the gene annota tion pipeline workflow demonstrates the ease by which data resources can be integrated with analysis programs fig the interoperability of taverna is high lighted in the pathway retrieval workflow which consumed the kegg web service a data resource located at kyoto university japan that was developed independently of the taverna or mygrid projects fig taverna is also able to consume other third party services such as xembl wang et al and those provided by ddbj and the pathport project kawashima et al eckart and so bral scufl workflows demon strating how these ser vices can be used are available at http taverna sourceforge net webservices workflows the pathway retrieval workflow also demonstrates the benefit of labelling data objects with metadata that describe their syntactic and or semantic type the additional information provided by the syntactic metadata provides applications with a handle to select an appropriate method for viewing data this was shown in the path way retrieval workflow in which the url data product was associated with the x taverna url mime type which subsequently led the scufl workbench to display the pathway maps located at the url fig the taverna workflow system provides a tool which can integrate resources that are shared as web services among the bioinformatics community in a similar fashion the scufl workflows created using taverna are resources in their own right that can be shared among sci entists this is in contrast to perl scripts which can be used to compose workflows but which are not often shared this is due at least in part to the difficulties in knowing the exact environment require ments e g resources invoked via system calls which inhibit the portability and maintenance of perl scripts this issue does not occur with scufl work flows since a common invoca tion mechanism in the form of web services is used and the service interface is usually readily available on the web in the form of a wsdl file however it is not necessary for resources to be exposed with a web service interface for them composing and enacting workflows using taverna fig the provenance log recorded after the enactment of the gene annotation pipeline workflow shown in figure information is dis played about the task performed by each processor in the scufl workflow to be used within scufl workflows in taverna re sources with other invocation mechanisms may be used in taverna by first creating a plug in for the freefluo enactor to access the resource and second implementing a corresponding scufl processor type legacy bioinformatics tools with other invoc ation mechanisms have been incorporated into taverna as talisman oinn and soaplab senger et al scufl processor types fig new features will continuously be added to the taverna workflow tool in response to user feedback the scufl work bench is an area targeted for im provement to a level such that workflows can be composed by laboratory biologists services are cur rently located within the scufl workbench by insti gating a web crawl to locate xscufl wsdl and talisman tscript files at a given url work is in progress to link the scufl workbench with service registries such as those offered by the mygrid lord et al and bio moby wilkinson and links projects in addition both scufl and freefluo aim to support the ability to define an expression that is evaluated before a task is executed if the expression is not true the enactor can either skip the step entirely or wait until the condition is true before executing the step the decision whether a condition is true or false may also require human intervention the act of user intervention in workflows is currently being investigated in the taverna pro ject in the interests of creating a bioinformatics nation whereby biological data can be seamlessly accessed by scientists stein suggested that data and tool providers equip their re sources with a web service interface the web services framework is a standard which is being adop ted by the bioinformatics community and this is shown by the increasing number of bioinformatics web services being made available such as those from the cancer bioinformat ics infrastructure objects project http cabio nci nih gov stein to this end there are open source tools avail able for the generation of web service wrappers such as apache axis http ws apache org axis which was used to web service enable the affymapper and keggmapper data bases for the gene annotation and pathway retrieval workflows fig and b it is hoped that our taverna workflow tool will encourage bioinformaticians to use the resources currently available as web services and also to provide web service access to their databases and programs for sharing within the bi oinformatics community this will enable the shar ing of workflows which further illustrates the value of the co ordinated use of a growing repertoire of bioinformatics web services i tasser as zhang server generated the best struc ture predictions among all automated servers the average or tm score of all targets domains is at least higher than the second best server and compa rable with the best human expert predictions since the first public release in november the i tasser server has generated structure predictions for thousands of modeling requests from various laboratories in the world we have been frequently asked by the users about how the quality of the i tasser models should be annotated because this will essentially decide how they will exploit the predictions in their research the general idea of the modeling quality estimation of models has been pursued by a number of authors which merges as a new research topic of model quality assess ment programs mqap and is assessed in the recent experiment in the category of qa in this work we introduce the on line setting of the i tasser server and develop a confidence scoring system which can provide the users with a simple and reliable assessment of the i tasser models different from most of the mqap programs that assess models purely based on the structure of the final models the confidence scor ing function developed here incorporates the information and parameters of the modeling simulations implementation i tasser method i tasser is a hierarchical protein structure modeling approach based on the secondary structure enhanced pro file profile threading alignment ppa and the itera tive implementation of the threading assembly refinement tasser program the detail of the i tasser method has been described in here we give a brief overview of the method the target sequences are first threaded through a repre sentative pdb structure library with a pair wise sequence identity cut off of to search for the possible folds by four simple variants of ppa methods with different com binations of the hidden markov model and psi blast profiles and the needleman wunsch and smith waterman alignment algorithms the contin uous fragments are then excised from the threading aligned regions which are used to reassemble full length models while the threading unaligned regions mainly loops are built by ab initio modeling the conforma tional space is searched by replica exchange monte carlo simulations the structure trajectories are clustered by spicker and the cluster centroids are obtained by the averaging the coordinates of all clustered structures to rule out the steric clashes on the centroid structures and to refine the models further we implement the frag ment assembly simulation again which starts from the cluster centroid of the first round simulation spatial restraints are extracted from the centroids and the pdb structures searched by the structure alignment program tm align which are used to guide the second round simulation finally the structure decoys are clustered and the lowest energy structure in each cluster is selected which has the cα atoms and the side chain centers of mass specified pulchra is used to add backbone atoms n c o and to build side chain rotamers if any region with residues has no aligned residues in at least two strong ppa alignments of z score see below the target will be judged as a multiple domain protein and domain boundaries are automatically assigned based on the borders of the large gaps i tasser simulations will be run for the full chain as well as the sep arate domains the final full length models are generated by docking the model of domains together the domain docking is performed by a quick metropolis monte carlo simulation where the energy is defined as the rmsd of domain models to the full chain model plus the recipro cal of the number of steric clashes between domains the goal of the docking is to find the domain orientation that is closest to the i tasser full chain model but has the minimum steric clashes this procedure does not influ ence the multiple domain proteins which have all domains completely aligned by the ppas c score the c score of the i tasser models is defined as where m is the multiplicity of structures in the spicker cluster mtot is the total number of the i tasser structure decoys used in the clustering rmsd is the average rmsd of the decoys to the cluster centroid z i is the highest z score the energy to mean in the unit of standard devia tion of the templates by the ith ppa threading program and i is a program specified z score cutoff for distin guishing between good and bad templates i e the first two factors of equation account for the degree of structure convergence in the spicker clustering which correlates with the consistency of the external restraints and the inherent i tasser potential the third factor accounts for the quality of threading alignments the log arithm in equation is to adjust the c score values in an approximately even distribution a previously defined c page of score has been shown to have a strong correlation with the quality of the predicted models here the definition of c score is slightly different first a normalized z score by is used instead of the z score itself which makes it easy to extend the definition to the cases when templates are generated by different threading algorithms second it accounts for the consensus of alignment confidence of multiple threading programs rather than one threading program we also tried other alternatives for the c score definition for example if we add tm score the average tm score of the decoys to the cluster centroid in the numerator of the second factor in equation the correlation between the c score and tm score will increase by but it does not increase the correlation of c score with rmsd and the cal culation of tm score will increase the spicker running time by so we did not include tm score in the c score definition we also attempted to optimize the pow ers of the three factors of equation by maximizing the correlation between c score and the quality of final mod els in the training proteins interestingly the optimized powers of all three factors are close to which indicates that the c score in equation is close to an optimal defi nition if considering these factors tm score tm score is defined to assess the topological similarity of two protein structures where di is the distance of the ith pair of residues between two structures after an optimal superposition l and l is the protein length tm score stays in with higher values indicating better models statistically a tm score corresponds to a similarity between two randomly selected structures from the pdb library a tm score corresponds approxi mately to two structures of the similar topology one advantage of the tm score is that the meaning of the tm score cutoffs is independent of the size of proteins server setting the url address of the on line i tasser server is listed at the end of the paper to use the server what users need to provide is the amino acid sequence of the proteins to be modeled in the fasta format currently the acceptable size range of the targets is between residues depending on the protein size the i tasser modeling procedure takes a maximum of hours typically hours for a sequence around residues after the mod eling is finished an email will be sent to the users which include the pdb format files of up to predicted models c score of the models and the predicted rmsd and tm score of the first model a brief explanation of the rmsd tm score and c score is also provided in the email once a prediction is made a gif visual file is made for each of the i tasser models so that the users can get a quick on line view of how the topology of their models looks like the pdb files and the visual files are kept on our server for days and made publicly downloadable at so that other users can quickly retrieve the mode ling results without resubmitting the jobs when they want to model the same or similar proteins the queue of the jobs is shown on the page as well so that the users can track their submitted jobs finally an about i tasser server webpage is designed to provide a detailed introduction of the server which is kept updated when new features are developed results and discussions for the benchmark of the i tasser server we collect nonhomologous single domain proteins directly from the pdb library which have a pair wise sequence identity with the size ranging from to residues the purpose has been made to have the selected proteins of a balanced distribution in secondary structure classes and modeling difficulty as a result the benchmark set includes α β αβ proteins based on the z scores of the ppa alignments targets are assigned as easy medium hard targets respectively we randomly select proteins as the training set to fit the parameters of the estimated model quality see below the remaining proteins will be used as the test set see when i tasser is used to generate models for the proteins homologous templates with sequence iden tity to the target are excluded from the threading template library it should be mentioned that here we benchmark the i tasser algorithm only on the single domain proteins for multiple domain proteins a small misorientation of the domains may result in dramatic change in tm score and rmsd values even if the topology of the individual domains is unchanged which can result in divergent cor relations of the c score and the overall model qualities consequently the confidence score and quality estima tion of multiple domain models should be understood approximately as those for the individual domain units correlation of c score and model qualities in figure we display the tm score of the first i tasser models of all testing proteins which shows a strong correlation to the c scores with a pearson correlation page of coefficient of if we define a model of tm score as a correct fold and assess the models using a cutoff of c score the false positive and false negative rate are and respectively the correlation of rmsd with the c score is not as strong as that of the tm score figure many high c score models have a big rmsd this is mainly because of the definition of rmsd which averages distances of all resi due pairs with an coefficient of if we define a model of tm score as a correct fold and assess the models using a cutoff of c score the false positive and false negative rate are and respectively the correlation of rmsd with the c score is not as strong as that of the tm score figure many high c score models have a big rmsd this is mainly because of the definition of rmsd which averages distances of all resi due pairs with an equal weight therefore a big local modeling error will result in a high rmsd value even when the global topology is correct for illustration in figure we show two examples of the i tasser mode ling for which has a high c score the core region of the model is very close to the native with a rmsd å but the n terminus of the model is mis ori entated which results in an overall rmsd å a region usually implying wrong folds as defined in equa tion tm score weights the residue pairs of small dis tances stronger than those of large distances which is not sensitive to the local structure errors and has a value of in the example for the global topology of the secondary structure arrangements in the i tasser modeling is incorrect with a tm score of close to random the c score in this case is however the rmsd å is similar as that of therefore the rmsd values in the high rmsd region are not sensitive to the global topology of structures the second reason for the low rmsd c score correlation is due to the inherent size dependence of rmsd in figure we show the tm score and rmsd values of the i tasser models versus the protein length for the test proteins obviously the small proteins tend to have a lower rmsd a tendency also seen in the randomly selected pdb structure pairs which results in a non trivial rmsd length correlation figure since the dis tance in tm score is normalized by a length dependent scale see equation there is no length dependence in the tm score values which have an almost uniform cut near figure in figure we plot the rmsd values versus c score ln l which has an obviously stronger correlation correlation coefficient than that in figure as a control we also calculate the correlation of tm score or rmsd with the sequence identity between the target and the best template which is or the low correlation is not surprising because all homologous tem plates with a high sequence identity have been excluded and the profile profile programs often identify templates of correct topology even when the sequence identity to the target is low quantitative estimate of the quality of i tasser models based on the i tasser models of the training pro teins we fit a two order polynomial to the tm score c score data by the least square fitting method we obtain tm score c c score with a root mean squared tm score deviation rmstd of for the training protein set in figure we show the curve of equation dashed curve which fits very well with the test proteins with a rmstd of if we con sider equation as the estimated tm score the average error of the estimation is in the test set here we note that the rmstd is defined as sqrt tm score tm score and the average error of estimation is tm score tm score where tm score is the average tm score in the training set and the estimated tm score in the test set if we use rmstd as the standard deviation of the tm score estimation there is a probability of that the real tm score will fall in the range of tm score rmstd in the lower part of figure we show the data of rmstd versus c score at each point the rmstd from the esti mated tm score by equation is calculated for the pro teins in a bin of c score c score on average each bin contains proteins the dependence of rmstd with c score is spindle like which indicates that the tm score can be relatively easier predicted in both high and low c score regions compared with that in the medium c score region the data fits well with the gaussian function in the training proteins as an overlap of equation with the rmstd data is shown in figure solid curve since the rmsd of the i tasser models correlates better with c score ln l than with c score we fit the order polynomial with the data of rmsd c score ln l in the training proteins we obtain rmsd c score ln l c score ln l with a root mean squared rmsd deviation rmsrd of å in figure we show the curve of equation dashed curve which fits well with the testing proteins with a rmsrd å the average error of the estimated rmsd using equation is å in the test set in the low part of figure we display the rmsrd value calculated in each bin of c score lnl c score lnl and the gaussian curve fitted from the training proteins i e we develop the i tasser server for the automated full length protein structure prediction a series of accessorial webpages are designed to facilitate the users in submit ting viewing and tracking the predictions based on the statistical significance of the ppa threading alignments and the structure convergence of the monte carlo simula tions a new confidence score c score is introduced and benchmarked for the i tasser server which demon strates a strong correlation with the real quality of the final models the pearson correlation coefficients of the c score with tm score and rmsd are and respec tively the strong correlation data allows us to make quan titative estimates of the accuracy of the i tasser predictions using a order polynomial equation fit from training proteins we can predict the tm score and rmsd of the final models with an average error of and å respectively in a large scale benchmark test page of for each submitted sequence following items will be returned to the users by email after the i tasser mode ling up to five predicted models ranked based on the structure density of the spicker clustering c score of all the i tasser models estimated tm score and rmsd for the first model in the form of estimation devi ation where the values of estimation and deviation are cal culated by equations by definition in of cases the real tm score and rmsd values will fall in this range despite the significant correlation between the c score and the tm score they have been introduced for the different purposes while the c score judges how con fident the server feels about the predictions based on the information from the modeling simulations tm score is a measure of the absolute quality of the final model in comparison with the native structure which is estimated through the calculation of the c score it should be mentioned that the estimated qualities are provided only for the first model although for the pur pose of providing more information the c score of all models are sent to the users the correlation of c score and modeling quality for the lower rank models is much weaker than that for the first model this is understanda ble because the conformational space covered by the i tasser simulations is limited for easy targets almost all decoys are near native and the structures are mainly clus tered in the first cluster after removing the structures in the first cluster the size of the lower rank clusters will be much smaller which may be comparable to that of hard targets but the quality of the lower rank clusters from the easy targets is still on average better than that from the hard targets because most decoys generated in the hard targets are incorrect nevertheless there is a correlation between the rank and the quality of the clusters for the same target in this set of test proteins the average tm score rmsd of the top five models are å å å å and å respectively therefore the c score and predicted data should be considered as an upper limit estimate for the quality of all i tasser models availability and requirements project name i tasser server project home page http zhang bioinformatics ku edu i tasser operating system windows linux mac programming language perl license gpl any restrictions to use by non academics license needed abbreviations i tasser iterative threading assembly refinement algo rithm ppa profile profile alignment threading algorithm rmsd root mean squared deviation rmsrd root mean squared rmsd deviation from aver age or estimated rmsd rmstd root mean squared tm score deviation from average or estimated tm score authors contributions yz developed the i tasser server performed the bench mark calculation and wrote the manuscript he has read and approved the final manuscript acknowledgements the author wants to thank dr sitao wu for help in constructing the web pages the project is partly supported by ku start up fund rationale computation has become an essential tool in life science research this is exemplified in genomics where first microarrays and now massively parallel dna sequen cing have enabled a variety of genome wide functional assays such as chip seq and rna seq and many others that require increasingly complex analysis tools however sudden reliance on computation has created an informatics crisis for life science researchers computational resources can be difficult to use and ensuring that computational experiments are communi cated well and hence reproducible is challenging galaxy helps to address this crisis by providing an open web based platform for performing accessible reproducible and transparent genomic science the problem of accessibility of computational tools has long been recognized without programming or informatics expertise scientists needing to use computa tional approaches are impeded by problems ranging from tool installation to determining which parameter values to use to efficiently combining multiple tools together in an analysis chain the severity of these pro blems is evidenced by the numerous solutions to address them tutorials software libraries such as correspondence anton bx psu edu james taylor emory edu of biology and department of mathematics and computer science emory university clifton road ne atlanta ga usa for comparative genomics and bioinformatics penn state university wartik lab university park pa usa full list of author information is available at the end of the article bioconductor and bioperl and web based inter faces for tools all improve the accessibility of com putation these approaches each have advantages but do not offer a general solution that enables a computa tional tool to be easily included in an analysis chain and run by scientists without programming experience however making tools accessible does not necessarily address the crucial problem of reproducibility reprodu cing experimental results is an essential facet of scienti fic inquiry providing the foundation for understanding integrating and extending results toward new discov eries learning a programming language might enable a scientist to perform a given analysis but ensuring that analysis is documented in a form another scientist can reproduce requires learning and practicing software engineering skills note that neither programming nor software engineering are included in a typical biomedi cal curriculum a recent investigation found that less than half of selected microarray experiments published in nature genetics could be reproduced issues that pre vented reproduction included missing raw data details in processing methods especially computational ones and software and hardware details experiments that employ next generation sequencing ngs will only exacerbate challenges in reproducibility due to a lack of standards exceedingly large dataset sizes and increas ingly complex computational tools in addition integra tive experiments which use multiple data sources and multiple computational tools in their analyses further complicate reproducibility goecks et al licensee biomed central ltd this is an open access article distributed under the terms of the creative commons attribution license http creativecommons org licenses by which permits unrestricted use distribution and reproduction in any medium provided the original work is properly cited to support reproducible computational research the concept of a reproducible research system rrs has been proposed an rrs provides an environment for performing and recording computational analyses and enabling the use or inclusion of these analyses when preparing documents for publications multiple systems provide an environment for recording and repeating computational analyses by automatically track ing the provenance of data and tool usage and enabling users to selectively run and rerun particular analyses and one such system provides a means to inte grate analyses in a word processing document while the concept of an rrs is clearly defined and well motivated there are many open questions about what features an rrs should include and what implementa tion best serves the goals of reproducibility amongst the most important open questions are how user gener ated content can be included in an rrs and how best to publish computational outputs datasets analyses workflows and tools produced from an experiment just because an analysis can be reproduced does not mean it can easily be communicated or understood realizing the potential of computational experiments also requires addressing the challenge of transparency the open sharing and communication of experimental results to promote accountability and collaboration for computational experiments researchers have argued that computational results such as analyses and meth ods are of equal or even greater importance than text and figures as experimental outputs transpar ency has received less attention than accessibility and reproducibility but it may be the most difficult to address current rrss enable users to share outputs in limited ways but no rrs or other system has developed a comprehensive framework for facilitating transparency we have designed and implemented the galaxy plat form to explore how an open web based approach can address these challenges and facilitate genomics research galaxy is a popular web based genomic work bench that enables users to perform computational ana lyses of genomic data the public galaxy service makes analysis tools genomic data tutorial demonstra tions persistent workspaces and publication services available to any scientist that has access to the internet local galaxy servers can be set up by downloading the galaxy application and customizing it to meet parti cular needs galaxy has established a significant commu nity of users and developers here we describe our approach to building a collaborative environment for performing complex analyses with automatic and unob trusive provenance tracking and use this as the basis for a system that allows transparent sharing of not only the precise computational details underlying an analysis but also intent context and narrative galaxy pages are the principal means to communicate research performed in galaxy pages are interactive web based documents that users create to describe a complete genomics experi ment pages allow computational experiments to be documented and published with all computational out puts directly connected allowing readers to view the experiment at any level of detail inspect intermediate data and analysis steps reproduce some or all of the experiment and extract methods to be modified and reused accessibility galaxy approach to making computation accessible has been discussed in detail in previous publications here we briefly review the most relevant aspects of the approach the most important feature of galaxy analy sis workspace is what users do not need to do or learn galaxy users do not need to program nor do they need to learn the implementation details of any single tool galaxy enables users to perform integrative genomic analyses by providing a unified web based interface for obtaining genomic data and applying computational tools to analyze the data figure users can import datasets into their workspaces from many established data warehouses or upload their own datasets interfaces to computational tools are automatically generated from abstract descriptions to ensure a consistent look and feel the galaxy analysis environment is made possible by the model galaxy uses for integrating tools a tool can be any piece of software written in any language for which a command line invocation can be constructed to add a new tool to galaxy a developer writes a con figuration file that describes how to run the tool includ ing detailed specification of input and output parameters this specification allows the galaxy frame work to work with the tool abstractly for example automatically generating web interfaces for tools as described above although this approach is less flexible than working in a programming language directly for researchers that can program it is this precise specifi cation of tool behavior that serves as a substrate for making computation accessible and addressing transpar ency and reproducibility making it ideal for command line averse biomedical researchers reproducibility galaxy enables users to apply tools to datasets and hence perform computational analyses the next step in supporting computational research is ensuring these analyses are reproducible this requires capturing suffi cient metadata descriptive information about datasets tools and their invocations that is a number of sequences in a dataset or a version of genomic assembly figure galaxy analysis workspace the galaxy analysis workspace is where users perform genomic analyses the workspace has four areas the navigation bar tool panel left column detail panel middle column and history panel right column the navigation bar provides links to galaxy major components including the analysis workspace workflows data libraries and user repositories histories workflows pages the tool panel lists the analysis tools and data sources available to the user the detail panel displays interfaces for tools selected by the user the history panel shows data and the results of analyses performed by the user as well as automatically tracked metadata and user generated annotations every action by the user generates a new history item which can then be used in subsequent analyses downloaded or visualized galaxy history panel helps to facilitate reproducibility by showing provenance of data and by enabling users to extract a workflow from a history rerun analysis steps visualize output datasets tag datasets for searching and grouping and annotate steps with information about their purpose or importance here step is being rerun are examples of metadata to repeat an analysis exactly when a user performs an analysis using galaxy it automatically generates metadata for each analysis step galaxy metadata includes every piece of informa tion necessary to track provenance and ensure repeat ability of that step input datasets tools used parameter values and output datasets galaxy groups a series of analysis steps into a history and users can create copy and version histories all datasets in a history initial intermediate and final are viewable and the user can rerun any analysis step while galaxy automatically tracked metadata are sufficient to repeat an analysis it is not sufficient to cap ture the intent of the analysis user annotations descriptions or notes about an analysis step are a criti cal facet of reproducibility because they enable users to explain why a particular step is needed or important automatically tracked metadata record what was done and annotations indicate why it was done galaxy also supports tagging or labeling applying words or phrases to describe an item tagging has proven very useful for categorizing and searching in many web appli cations galaxy uses tags to help users find items easily via search and to show users all items that have a parti cular tag tags support reproducibility because they help users find and reuse datasets histories and analysis steps reuse is an activity that is often necessary for reproducibility annotations and tags are forms of user metadata galaxy history panel provides access to both automatically tracked metadata and user metadata figure within the analysis workspace and hence users can see all reproducibility metadata for a history in a single location users can annotate and tag both complete histories and analysis steps without leaving the analysis workspace reducing the time and effort required for these tasks recording metadata is sufficient to ensure reproduci bility but alone does not make repeating an analysis easy the galaxy workflow system facilitates analysis repeatability and like galaxy accessibility model in a way that is usable even to users that have little program ming experience a galaxy workflow is a reusable tem plate analysis that a user can run repeatedly on different data each time a workflow is run the same tools with the same parameters are executed users can also create a workflow from scratch using galaxy interactive gra phical workflow editor figure nearly any galaxy tool can be added to a workflow users connect tools to form a complete analysis and the workflow editor veri fies for each link between tools that the tools are com patible the workflow editor thus provides a simple and graphical interface for creating complex workflows however this still requires users to plan their analysis upfront to ease workflow creation and facilitate analy sis reuse users can create a workflow by example using an existing analysis history to develop and repeatedly run an analysis on multiple datasets requires only a few steps create and edit a history to develop a satisfac tory set of analysis steps automatically generate a workflow based on the history and use the generated workflow to repeat the analysis for multiple other inputs a workflow is located next to all other tools in galaxy tool menu and behaves the same as all other tools when it is run workflows and all galaxy metadata are integrated executing a workflow generates a group of datasets and corresponding metadata which are placed in the current history users can add annotations and tags to workflows and workflow steps just as they can for histories user annotations are especially valu able for workflows because while workflows are abstract and can be reused in different analyses a workflow will be reused only if it is clear what its purpose is and how it works transparency in the course of performing analysis related to a project galaxy users often generate copious amounts of meta data and numerous histories and workflows the final step for making computational experiments truly useful is facilitating transparency for the experiments enabling users to share and communicate their experimental results and outputs in a meaningful way galaxy pro motes transparency via three methods a sharing model for galaxy items datasets histories and workflows and public repositories of published items a web based framework for displaying shared or published galaxy items and pages custom web based documents that enable users to communicate their experiment at every level of detail and in such a way that readers can view reproduce and extend their experiment without leaving galaxy or their web browser galaxy sharing model public repositories and dis play framework provide users with means to share data sets histories and workflows via web links galaxy sharing model provides progressive levels of sharing including the ability to publish an item publishing an item generates a link to the item and lists it in galaxy public repository figure published items have pre dictable short and clear links in order to facilitate shar ing and recall a user can edit an item link as well users can search sort and filter the public repository by name author tag and annotation to find items of interest galaxy displays all shared or published items as webpages with their automatic and user metadata and with additional links figure an item webpage provides a link so that anyone viewing an item can import the item into his analysis workspace and start using it the page also highlights information about the item and additional links its author links to related items the item community tags the most popular tags that users have applied to the item and the user item tags tags link back to the public repository and show items that share the same tag galaxy pages figure are the principal means for communicating accessible reproducible and transparent computational research through galaxy pages are cus tom web based documents that enable users to commu nicate about an entire computational experiment and pages represent a step towards the next generation of online publication or publication supplement a page like a publication or supplement includes a mix of text and graphs describing the experiment analyses in addition to standard content a page also includes embedded galaxy items from the experiment datasets histories and workflows these embedded items provide an added layer of interactivity providing additional details and links to use the items as well pages enable readers to understand an experiment at every level of detail when a reader first visits a page he can read its text view images and see an overview of embedded items an item name type and annotation should the reader want more detail he can expand an embedded item and view its details for histories and workflows expanding the item shows each step history steps can be individually expanded as well all metadata for both history and workflow steps are included as well hence a reader can view a page in its entirety and then expand embedded items to view every detail of every step in an experiment from parameter settings to annotations without leaving the page currently readers cannot discuss or comment on pages or embedded items though such features are planned pages also enable readers to actively use and reuse embedded items a reader can copy any embedded item into her analysis workspace and begin using that item immediately this functionality makes reproducing an analysis simple a reader can import a history and rerun it or she can import a workflow and input datasets and run the workflow once a history or workflow is imported from a page a reader can also modify or extend the analysis as well or reuse a workflow in another analysis using pages readers can quickly become analysts by importing embedded items and can do so without leaving their web browser or galaxy putting it all together accessible reproducible and transparent metagenomics to demonstrate the utility of our approach we used pages to create an online supplement for a metagenomic study performed in galaxy that surveyed eukaryotic diversity in organic matter collected off the windshield of a motor vehicle the choice of a metagenomic experiment for highlighting the utility of galaxy and pages was not accidental among all applications of ngs technologies metagenomic applications are argu ably one of the least reproducible this is primarily due to the lack of an integrated solution for performing figure galaxy public repositories and published items a galaxy public repository for pages there are also public repositories for histories and workflows repositories can be searched by name annotation owner and community tags b a published galaxy workflow each shared or published item is displayed in a webpage with its metadata for example execution details user annotations a link for copying the item into a user workspace and links for viewing related items metagenomic studies forcing researchers to use various software packages patched together with a variety of in house scripts because phylogenetic profiling is extre mely parameter dependent small changes in parameter settings lead to large discrepancies in phylogenetic pro files of metagenomic samples knowing exact analysis settings are critical with this in mind we designed a complete metagenomic pipeline that accepts ngs reads as the input and generates phylogenetic profiles as the output the galaxy page for this study describes the analyses performed and includes the study datasets histories and workflow so that the study can be rerun in its entirety to reproduce the analyses performed in the study readers can copy the study histories into their own workspace and rerun them readers can also copy the study workflow into their workspace and apply it to other datasets without modification in summary this study demonstrates how galaxy sup ports the complete lifecycle of a computational biology experiment galaxy provides a framework for perform ing computational analyses systematically repeating ana lyses capturing all details of performed analyses and annotating analyses using galaxy pages researchers can communicate all components of an experiment datasets analyses workflows and annotations in a web based interactive format an experiment page enables readers to view an experiment components at any level of detail reproduce any analysis and repur pose the experiment components in their own research all galaxy and page functionality is available using nothing more than a web browser galaxy usage for the approach we have implemented in galaxy to be successful it must truly be usable to experimentalists with limited computational expertise anecdotal evi dence suggests that galaxy is usable for many biologists galaxy public web server processes about jobs per day in addition to the public server there are a number of high profile galaxy servers in use including servers at the cold spring harbor laboratory and the united states department of energy joint genome institute individuals and groups not affiliated with the galaxy team have used galaxy to perform many different types of genomic research including investigations of epige nomics chromatin profiling transcriptional enhancers and genome environment interactions publication venues for these investigations include science nature and other prominent journals despite only recently being introduced galaxy sharing features have been used to make data available from a study published in science all of galaxy operations can be performed using nothing more than a web browser and galaxy user interface follows standard web usability guidelines such as consistency visual feedback and access to help and documentation hence biologists familiar with genomic analysis tools and comfortable using a web browser should be able to learn to use galaxy without difficulty in the future we plan to collect and analyze user data so that we can report quantitative measure ments of how useful and usable galaxy is for biologists and what can be done to make it better comparing galaxy with other genomic research platforms accessibility reproducibility and transparency are useful concepts for organizing and discussing galaxy approach to supporting computational research how ever stepping back and considering galaxy as a com plete platform two themes emerge for advancing computational research one theme concerns the reuse of computational outputs and the other theme concerns meaningful connections between analyses and sharing galaxy enables reuse of datasets tools histories and workflows in many ways automatic and user metadata make it simple for galaxy users to find and reuse their own analysis components galaxy public repository takes an initial step toward helping users publish their analysis components so that others can view and use them reuse is a core facet of software engineering and development enabling large programs to be developed efficiently by leveraging past work and affording the development and sharing of best practices enabling reuse is similarly important for life sciences computation galaxy provides connections that enable users to effectively move between performing a computational experiment and publishing it galaxy users can annotate a history or workflow in the analysis workspace and then share an item or embed the item within a page in just a few actions once shared published or embedded others can view the item or import it into their work space for immediate use galaxy then makes the com plete cycle of item use from creation to annotation to publication to reuse possible using only a web browser making it simple for the majority of users to participate wherever in the cycle that they choose providing mean ingful connections between analyses and publishing can encourage more publishing and a higher quality of pub lishing both for pages and for individual items seeing that published items are used can encourage users to publish more than they otherwise would well regarded published items can serve as models for the develop ment of other items and hence can improve the quality of subsequently published items publishing then is clo sely connected with reusing analysis components keeping these two themes in mind it is useful to con trast galaxy with other genomic workbenches to high light galaxy strengths and weaknesses and suggest future directions of development for platforms support ing computational science currently the most mature rrs platforms complementing galaxy are genepattern and mobyle both are web based frameworks for supporting genomic research and a primary goal of each platform is to enable reproducible research table summarizes galaxy functions and compares them with the functions of genepattern and mobyle all three platforms have features that improve access to computation and facilitate reproducibility each platform has a unified web based interface for working with tools automatically generates metadata when tools are run and provides a framework for adding new tools to the platform in addition all platforms employ the concept of workflows to support repeat ability galaxy also has features that distinguish it from both genepattern and mobyle galaxy has integrated data warehouses that enable users to employ data from these warehouses in integrative analyses in addition galaxy tags and annotations public repository and web based publication framework are also unique these features are essential for supporting both repro ducibility and transparency perhaps the most striking difference between galaxy and genepattern is each platform approach for inte grating analyses and publications galaxy employs a web based approach and enables users to create pages web accessible documents with embedded datasets ana lyses and workflows genepattern provides a microsoft word plugin that enables users to embed analyses and workflows into microsoft word documents both approaches provide similar functions but each platform integration choice yields unique benefits galaxy web based approach ensures that due to the internet open standards all readers can view and inter act with galaxy pages and embedded items in addition table comparing galaxy to other genomic workbenches galaxy functionality description genepattern comparison mobyle comparison making computation accessible unified web based tool interface all tool interface share same style and use web components tool interfaces are generated from tool configuration file same functions as galaxy same functions as galaxy simple tool integration tool developers can integrate tools by writing a tool configuration file and including tool file in galaxy configuration file similar but not as flexible tool configuration file easy installation of selected tools via a web based interface remote services can be added using a server configuration file integrated datasources transparent access to established data warehouses no similar functions no similar functions ensuring reproducibility automatic metadata provenance inputs parameters and outputs for each tool used analysis steps grouped into histories user tags can apply short tags to histories datasets workflows and pages tags are searchable and facilitate reuse same functions as galaxy same functions as galaxy no similar functions no similar functions user annotations can add descriptions or notes to histories datasets workflows workflow steps and pages to aid in understanding analyses cannot annotate a history but can annotate a workflow pipeline with an external document no similar functions creating and running workflows can create either by example or from scratch a workflow that can be repeatedly used to perform a multi step analysis same functions as galaxy although editor is form based rather than graphical in development workflow metadata automatic documentation is generated when a workflow is run users can also tag and annotate workflows and workflow steps same functions as galaxy for generating automatic metadata cannot annotate workflow steps in development promoting transparency sharing model datasets histories workflows and pages can be shared at progressive levels and published to galaxy public repositories datasets have more advanced sharing options including groups can share analyses and workflows with individuals or groups no similar functions item reuse display framework and public repositories pages with embedded items coupling between analysis workspace and publication workspace shared or published items displayed as webpages and can be imported and used immediately public repositories can be searched archives of analyses and workflows for sharing between servers are under development can create custom webpages with embedded galaxy items each page can document a complete experiment providing all details and supporting reuse of experiment outputs can import and immediately start using any shared published or embedded item without leaving web browser or galaxy can create an archive of an analysis or workflow and share that with others author information is included in archive microsoft word plugin enables users to embed analyses and workflows in word documents can run embedded analyses and save results in microsoft word documents can create an archive of an analysis and share that with others no similar functions no similar functions a summary of galaxy functionality and how galaxy functionality compares to the functionality of two other genomic workbenches genepattern and mobyle galaxy novel functionality includes but is not limited to integrated datasources user annotations a graphical workflow editor pages with embedded items and coupling the workspaces for analysis and publication using an open web based model galaxy analysis workspace and publication workspace use the same medium the web and hence users can move between the two workspaces without leaving their web browser galaxy publication media webpages matches the media used by many popular journals and hence can be used as primary or secondary documents for article submissions the main benefit of genepat tern word plugin is its integration into a popular word processor that is often used for preparing articles how ever microsoft word documents are rarely used for archival purposes and can be difficult to view also because genepattern and microsoft word are two dif ferent programs it can be difficult to move between genepattern analysis workspace and word publica tion workspace these constraints limit the value of the genepattern word documents an ideal fully featured platform for integrating ana lyses and publications would likely incorporate both approaches and enable users to create both word pro cessing documents and webpages that share references to analyses and workflows the ideal platform would enable users to embed objects in both a document and webpage simultaneously synchronize a document and webpage so that changes to one are reflected in the other and provide users with an analysis workspace accessible from either a document or a webpage achieving this goal will require the definition of open standards for describing and exchanging documents and analysis components between different systems and we look forward to future developments in this direction for example genomespace it is also useful to compare galaxy with other plat forms that support particular aspects of genomic science and hence are complementary to galaxy approach bioconductor is an open source software project that provides tools for analyzing and understanding genomic data bioconductor and similar platforms such as bioperl and biopython represent an approach to reproducibility that uses libraries and scripts built on top of a fully featured programming language together bioconductor and sweave a literate programming tool for documenting bioconductor analyses can be used to reproduce an analysis if a researcher has the ori ginal data the bioconductor scripts used in the analysis and enough programming expertise to run the scripts because bioconductor is built directly on top of a fully featured programming language it provides more flex ibility and power for performing analyses as compared to galaxy however bioconductor flexibility and power are only available to users with programming experience and hence are not accessible to many biolo gists in addition bioconductor lacks automatic prove nance tracking or a simple sharing model taverna is a workflow system that supports the crea tion and use of workflows for analyzing genomic data taverna users create workflows using web services and connect workflow steps using a graphical user inter face much as users do when creating a galaxy workflow taverna focuses exclusively on workflows this focus makes it more difficult to communicate complete ana lyses in taverna as the data must be handled outside of the system one of tavern most interesting features is its use of the myexperiment platform for sharing work flows myexperiment is a website that enables users to upload and share their workflows with others as well as download and use others workflows both bioconductor and taverna offer features that complement galaxy functionality galaxy framework can accommodate bioconductor tools and scripts with out modification to integrate a bioconductor tool or script all a developer needs to do is write a tool defini tion file for it we are actively working to integrate galaxy workflow sharing functionality with myexperi ment so that galaxy workflows can be shared via myexperiment future directions and challenges galaxy future directions arise from efforts to balance support for cutting edge genomic science with support for accessible reproducible and transparent science the increasingly large size of many datasets is one parti cularly challenging aspect of current and future genomic science it is often prohibitive to move large datasets due to constraints in time and money hence local galaxy installations near the data are likely to become more prevalent because it makes more sense to run galaxy locally as compared to moving the data to a remote galaxy server ensuring that galaxy analyses are accessible repro ducible and transparent as the number of galaxy ser vers grows is a significant challenge it is often difficult to provide easy and persistent access to galaxy analyses on a local server easy access is necessary for collabora tive work and persistent access is needed for published analyses local servers are often difficult to access for example if it is behind a firewall and additional work is often needed to ensure that a local server is function ing well we are pursuing three strategies to ensure that any galaxy analysis and associated objects can be made easily and persistently accessible first we are develop ing export and import support so that galaxy analyses can be stored as files and transferred among different galaxy servers second we are building a community space where users can upload and share galaxy objects third we plan to enable direct export of galaxy pages and analyses associated with publications to a long term searchable data archive such as dryad local installations also pose challenges to galaxy accessibility because it can be difficult to install tools that galaxy runs using web services in galaxy would reduce the need to install tools locally many large life sciences databases such as blast and interproscan provide access via a programmatic web interface however web services can compromise the reproduci bility of an analysis because a researcher cannot deter mine or verify details of the program that is providing a web service also a researcher cannot be assured that a needed web service will be available when trying to reproduce an analysis because web services can signifi cantly compromise reproducibility they are not a viable approach for use in galaxy a related problem is how best to enable researchers to install and choose which version of a tool to run galaxy metadata include the version of each tool run but this information is not yet exposed to users we are extending the galaxy framework to support simulta neously integrating tools that require different versions of an underlying program or library to ease the burden of installing and administering tool dependencies we are pursuing the approach of building virtual machine images that can be used to deploy a personal galaxy server locally or on a cloud computing resource with particular tool suites and tool versions included finally increasing the choices that researchers have when installing and using galaxy leads to a new chal lenge requiring a user to select tool suites during installation and tool versions and parameters during analysis can be problematic presenting users with so many choices can lead to confusion or require users to make choices that they are unsure of workflows pro vide one solution to this problem by predefining para meters and ways of composing tools for specific types of analysis to help users make better and faster choices within galaxy we are extending galaxy sharing model to help the galaxy user community find and highlight useful items ideally the community will identify his tories workflows and other items that represent best practices best practice items can be used to help guide users in their own analyses we have proposed a model for a reproducible research system based on three qualities accessibility reproducibil ity and transparency galaxy implements this model using a web based open framework and users can access all of galaxy features using only a standard web browser galaxy pages draw together much of galaxy functionality to provide a new publishing method galaxy pages enable biologists to describe their experiments using web based documents that include embedded galaxy objects an experiment page communicates all facets of the experi ment via increasing levels of detail and enables readers to reproduce the experiment or reuse the experiment meth ods without leaving galaxy the life sciences community has used galaxy to perform analyses that contributed to numerous publications and we have used galaxy pages to provide supplementary material for a published metage nomics experiment in the future large datasets and increasing access to computation likely means that more biologists will have access to a personal galaxy server a main challenge for galaxy is continuing to enable accessi ble reproducible and transparent genomic science while also facilitating more personal and distributed access to galaxy functionality details of galaxy framework and selected features the galaxy framework is a set of reusable software components that can be integrated into applications encapsulating functionality for describing generic inter faces to computational tools building concrete inter faces for users to interact with tools invoking those tools in various execution environments dealing with general and tool specific dataset formats and conver sions and working with metadata describing datasets tools and their relationships the galaxy application is an application built using this framework that provides access to tools through an interface for example a web based interface and provides features for perform ing reproducible computational research as described in this paper a galaxy server or instance is a deployment of this application with a specific set of tools galaxy is implemented primarily in the python pro gramming language tested on versions through it is distributed as a standalone package that includes an embedded web server and sql structured query lan guage database but can be configured to use an exter nal web server or database regular updates are distributed through a version control system and galaxy automatically manages database and dependency updates a galaxy instance can utilize compute clusters for running jobs and can be easily interfaced with por table batch system pbs or sun grid engine sge clusters the editors for tagging and annotations are integrated into galaxy analysis workspace and are designed to support web based genomic research galaxy tags are hierarchical and can have values and these features make tags amenable to many different metadata voca bularies and navigational techniques for instance the tag encode indicates that the item uses encode cell line the tag is encode line and its value is using this tag galaxy can find all items that have this tag and value encode all items that have this tag regardless of value encode or all items that share a parent tag encode or encode any thing we are currently developing an interface for browsing tagged items we are also implementing item tags for datasets stored in galaxy libraries this is espe cially useful because galaxy libraries are repositories for shared datasets and helping researchers find relevant libraries and library datasets is often difficult users can style their annotations for example use bold and italics and add web links to them because annotations are dis played on webpages via galaxy publication framework it makes sense that users are able to take advantage of the fact that annotations are displayed on webpages galaxy workflow editor provides an interactive gra phical interface that enables users to visually build and connect tools to create workflow a user can add a box to represent any of the tools in galaxy tool panel with the exception of several datasources access tools at the time of writing to the workflow editor canvas the user then connects tools to create a flow of data from one tool to the next and ultimately an analysis chain con necting tools is done by dragging links from one tool to another the workflow editor can determine which tools can be chained together if the output of tool a is com patible with the input of tool b these two can be chained together valid links between tools are green and invalid links are red galaxy sharing model provides three progressive levels of sharing first a user can share an item with other users second a user can make an item accessible making an item accessible generates a web link for the item that a user can share with others unlike when an item is shared with other users an accessible item can be viewed by anyone that knows the item link includ ing non galaxy users third a user can publish an item publishing an item makes the item accessible and lists the item in galaxy public repository accessible or published items have consistent clear links that employ the item owner public username the item type and the item identifier for instance an accessible history owned by a user with the username jgoecks and using the identifier microarray analysis would have the relative url jgoecks h microarray analysis galaxy item links are simple in order to facilitate sharing and recall a user can edit an item identifier as well and hence change its url sharing an item and editing its identifier are done through a simple web based interface galaxy page editor looks and feels like a word pro cessing program the editor enables a galaxy user to create a free form web document using text standard web components for example images links tables web styles for example paragraphs headings and embedded galaxy items embedding galaxy items is done via standard lists and buttons and embedded galaxy items look like colored blocks in the text when a user is editing a page the embedding framework is suf ficiently general to allow other types of items such as visualizations and data libraries to be embedded in pages in the future abstract the bioconductor project is an initiative for the collaborative creation of extensible software for computational biology and bioinformatics the goals of the project include fostering collaborative development and widespread use of innovative software reducing barriers to entry into interdisciplinary scientific research and promoting the achievement of remote reproducibility of research results we describe details of our aims and methods identify current challenges compare bioconductor to other open bioinformatics projects and provide working examples genome biology volume issue article gentleman et al http genomebiology com background the bioconductor project is an initiative for the collabora tive creation of extensible software for computational biology and bioinformatics cbb biology molecular biology in par ticular is undergoing two related transformations first there is a growing awareness of the computational nature of many biological processes and that computational and statis tical models can be used to great benefit second develop ments in high throughput data acquisition produce requirements for computational and statistical sophistication at each stage of the biological research pipeline the main goal of the bioconductor project is creation of a durable and flexible software development and deployment environment that meets these new conceptual computational and inferen tial challenges we strive to reduce barriers to entry to research in cbb a key aim is simplification of the processes by which statistical researchers can explore and interact fruit fully with data resources and algorithms of cbb and by which working biologists obtain access to and use of state of the art statistical methods for accurate inference in cbb among the many challenges that arise for both statisticians and biologists are tasks of data acquisition data manage ment data transformation data modeling combining differ ent data sources making use of evolving machine learning methods and developing new modeling strategies suitable to cbb we have emphasized transparency reproducibility and efficiency of development in our response to these challenges fundamental to all these tasks is the need for software ideas alone cannot solve the substantial problems that arise the primary motivations for an open source computing envi ronment for statistical genomics are transparency pursuit of reproducibility and efficiency of development transparency high throughput methodologies in cbb are extremely com plex and many steps are involved in the conversion of infor mation from low level information structures for example microarray scan images to statistical databases of expression measures coupled with design and covariate data it is not possible to say a priori how sensitive the ultimate analyses are to variations or errors in the many steps in the pipeline credible work in this domain requires exposure of the entire process pursuit of reproducibility experimental protocols in molecular biology are fully pub lished lists of ingredients and algorithms for creating specific substances or processes accuracy of an experimental claim can be checked by complete obedience to the protocol this standard should be adopted for algorithmic work in cbb portable source code should accompany each published anal ysis coupled with the data on which the analysis is based efficiency of development by development we refer not only to the development of the specific computing resource but to the development of com puting methods in cbb as a whole software and data resources in an open source environment can be read by interested investigators and can be modified and extended to achieve new functionalities novices can use the open sources as learning materials this is particularly effective when good documentation protocols are established the open source approach thus aids in recruitment and training of future gen erations of scientists and software developers the rest of this article is devoted to describing the computing science methodology underlying bioconductor the main sec tions detail design methods and specific coding and deploy ment approaches describe specific unmet challenges and review limitations and future aims we then consider a number of other open source projects that provide software solutions for cbb and end with an example of how one might use bioconductor software to analyze microarray data results and discussion methodology the software development strategy we have adopted has sev eral precedents in the mid richard stallman started the free software foundation and the gnu project as an attempt to provide a free and open implementation of the unix operating system one of the major motivations for the project was the idea that for researchers in computational sci ences their creations discoveries software should be avail able for everyone to test justify replicate and work on to boost further scientific innovation together with the linux kernel the gnu linux combination sparked the huge open source movement we know today open source soft ware is no longer viewed with prejudice it has been adopted by major information technology companies and has changed the way we think about computational sciences a large body of literature exists on how to manage open source software projects see hill for a good introduction and a compre hensive bibliography one of the key success factors of the linux kernel is its mod ular design which allows for independent and parallel devel opment of code in a virtual decentralized network developers are not managed within the hierarchy of a com pany but are directly responsible for parts of the project and interact directly where necessary to build a complex system our organization and development model has attempted to follow these principles as well as those that have evolved from the r project in this section we review seven topics important to establish ment of a scientific open source software project and discuss them from a cbb point of view language selection infra structure resources design strategies and commitments http genomebiology com genome biology volume issue article gentleman et al distributed development and recruitment of developers reuse of exogenous resources publication and licensure of code and documentation language selection cbb poses a wide range of challenges and any software devel opment project will need to consider which specific aspects it will address for the bioconductor project we wanted to focus initially on bioinformatics problems in particular we were interested in data management and analysis problems associ ated with dna microarrays this orientation necessitated a programming environment that had good numerical capabil ities flexible visualization capabilities access to databases and a wide range of statistical and mathematical algorithms our collective experience with r suggested that its range of well implemented statistical and visualization tools would decrease development and distribution time for robust soft ware for cbb we also note that r is gaining widespread usage within the cbb community independently of the bio conductor project many other bioinformatics projects and researchers have found r to be a good language and toolset with which to work examples include the spot system maanova and dchip we now briefly enumerate features of the r software environment that are important motivations behind its selection prototyping capabilities r is a high level interpreted language in which one can easily and quickly prototype new computational methods these methods may not run quickly in the interpreted implementa tion and those that are successful and that get widely used will often need to be re implemented to run faster this is often a good compromise we can explore lots of concepts eas ily and put more effort into those that are successful packaging protocol the r environment includes a well established system for packaging together related software components and docu mentation there is a great deal of support in the language for creating testing and distributing software in the form of packages using a package system lets us develop different software modules and distribute them with clear notions of protocol compliance test based validation version identifi cation and package interdependencies the packaging sys tem has been adopted by hundreds of developers around the world and lies at the heart of the comprehensive r archive network where several hundred independent but interoper able packages addressing a wide range of statistical analysis and visualization objectives may be downloaded as open source object oriented programming support the complexity of problems in cbb is often translated into a need for many different software tools to attack a single prob lem thus many software packages are used for a single anal ysis to secure reliable package interoperability we have adopted a formal object oriented programming discipline as encoded in the system of formal classes and methods the bioconductor project was an early adopter of the dis cipline and was the motivation for a number of improvements established by john chambers in object oriented program ming for r www connectivity access to data from on line sources is an essential part of most cbb projects r has a well developed and tested set of functions and packages that provide access to different data bases and to web resources via http for example there is also a package for dealing with xml available from the omegahat project and an early version of a package for a soap client ssoap also available from the omegahat project these are much in line with proposals made by stein and have aided our work towards creating an environ ment in which the user perceives tight integration of diverse data annotation and analysis resources statistical simulation and modeling support among the statistical and numerical algorithms provided by r are its random number generators and machine learning algorithms these have been well tested and are known to be reliable the bioconductor project has been able to adapt these to the requirements in cbb with minimal effort it is also worth noting that a number of innovations and exten sions based on work of researchers involved in the biocon ductor project have been flowing back to the authors of these packages visualization support among the strengths of r are its data and model visualization capabilities like many other areas of r these capabilities are still evolving we have been able to quickly develop plots to render genes at their chromosomal locations a heatmap function along with many other graphical tools there are clear needs to make many of these plots interactive so that users can query them and navigate through them and our future plans involve such developments support for concurrent computation r has also been the basis for pathbreaking research in parallel statistical computing packages such as snow and rpvm sim plify the development of portable interpreted code for com puting on a beowulf or similar computational cluster of workstations these tools provide simple interfaces that allow for high level experimentation in parallel computation by computing on functions and environments in concurrent r sessions on possibly heterogeneous machines the snow package provides a higher level of abstraction that is inde pendent of the communication technology such as the mes sage passing interface mpi or the parallel virtual machine pvm parallel random number generation essential when distributing parts of stochastic simula tions across a cluster is managed by rsprng practical genome biology volume issue article gentleman et al http genomebiology com benefits and problems involved with programming parallel processes in r are described more fully in rossini et al and li and rossini community perhaps the most important aspect of using r is its active user and developer communities this is not a static language r is undergoing major changes that focus on the changing techno logical landscape of scientific computing exposing biologists to these innovations and simultaneously exposing those involved in statistical computing to the needs of the cbb com munity has been very fruitful and we hope beneficial to both communities infrastructure base we began with the perspective that significant investment in software infrastructure would be necessary at the early stages the first two years of the bioconductor project have included significant effort in developing infrastructure in the form of reusable data structures and software documenta tion modules r packages the focus on reusable software components is in sharp contrast to the one off approach that is often adopted in a one off solution to a bioinformatics problem code is written to obtain the answer to a given ques tion the code is not designed to work for variations on that question or to be adaptable for application to distinct ques tions and may indeed only work on the specific dataset to which it was originally applied a researcher who wishes to perform a kindred analysis must typically construct the tools from scratch in this situation the scientific standard of reproducibility of research is not met except via laborious reinvention it is our hope that reuse refinement and exten sion will become the primary software related activities in bioinformatics when reusable components are distributed on a sound platform it becomes feasible to demand that a published novel analysis be accompanied by portable and open software tools that perform all the relevant calculations this will facilitate direct reproducibility and will increase the efficiency of research by making transparent the means to vary or extend the new computational method two examples of the software infrastructure concepts described here are the exprset class of the biobase package and the various bioconductor metadata packages for exam ple an exprset is a data structure that binds together array based expression measurements with covari ate and administrative data for a collection of microarrays based on r data frame and list structures exprsets offer much convenience to programmers and analysts for gene filtering constructing annotation based subsets and for other manipulations of microarray results the exprset design facilitates a three tier architecture for providing anal ysis tools for new microarray platforms low level data are bridged to high level analysis manipulations via the exprset structure the designer of low level processing software can focus on the creation of an exprset instance and need not cater for any particular analysis data structure representa tion the designer of analysis procedures can ignore low level structures and processes and operate directly on the exprset representation this design is responsible for the ease of interoperation of three key bioconductor packages affy marray and limma the package is one of a large collection of related packages that relate manufactured chip components to bio logical metadata concerning sequence gene functionality gene membership in pathways and physical and administra tive information about genes the package includes a number of conventionally named hashed environments providing high performance retrieval of metadata based on probe nomenclature or retrieval of groups of probe names based on metadata specifications both types of information metadata and probe name sets can be used very fruitfully with exprsets for example a vector of probe names immedi ately serves to extract the expression values for the named probes because the exprset structure inherits the named extraction capacity of r data frames design strategies and commitments well designed scientific software should reduce data com plexity ease access to modeling tools and support integrated access to diverse data resources at a variety of levels software infrastructure can form a basis for both good scientific prac tice others should be able to easily replicate experimental results and for innovation the adoption of designing by contract object oriented pro gramming modularization multiscale executable documen tation and automated resource distribution are some of the basic software engineering strategies employed by the bio conductor project designing by contract while we do not employ formal contracting methodologies for example eiffel in our coding disciplines the con tracting metaphor is still useful in characterizing the approach to the creation of interoperable components in bio conductor as an example consider the problem of facilitat ing analysis of expression data stored in a relational database with the constraints that one wants to be able to work with the data as one would with any exprset and one does not want to copy unneeded records into r at any time technically data access could occur in various ways using database connec tions dcom communications or corba to name but a few in a designing by contract discipline the provider of exprset functionality must deliver a specified set of func tionalities whatever object the provider code returns it must satisfy the exprsets contract among other things this means that the object must respond to the application of functions exprs and pdata with objects that satisfy the r matrix and data frame contracts respectively it follows that exprs x i j for example will return the number http genomebiology com genome biology volume issue article gentleman et al encoding the expression level for the ith gene for the jth sam ple in the object x no matter what the underlying representa tion of x here i and j need not denote numerical indices but can hold any vectors suitable for interrogating matrices via the square bracket operator satisfaction of the contract obli gations simplifies specification of analysis procedures which can be written without any concern for the underlying repre sentations for exprset information a basic theme in r development is simplifying the means by which developers can state follow and verify satisfaction of design contracts of this sort environment features that sup port convenient inheritance of behaviors between related classes with minimal recoding are at a premium in this discipline object oriented programming there are various approaches to the object oriented program ming methodology we have encouraged but do not require use of the so called system of formal classes and methods in bioconductor software the object paradigm defined primarily by chambers with modifications embodied in r is similar to that of common lisp and dylan in this system classes are defined to have specified structures in terms of a set of typed slots and inheritance relation ships and methods are defined both generically to specify the basic contract and behavior and specifically to cater for objects of particular classes constraints can be given for objects intended to instantiate a given class and objects can be checked for validity of contract satisfaction the system is a basic tool in carrying out the designing by contract disci pline and has proven quite effective modularization the notion that software should be designed as a system of interacting modules is fairly well established modularization can occur at various levels of system structure we strive for modularization at the data structure r function and r pack age levels this means that data structures are designed to possess minimally sufficient content to have a meaningful role in efficient programming the exprset structure for example contains information on expression levels exprs slot variability se exprs covariate data phenodata slot and several types of metadata slots description annotation and notes the tight binding of covariate data with expression data spares developers the need to track these two types of information separately the exprset structure explicitly excludes information on gene related annotation such as gene symbol or chromosome location because these are potentially volatile and are not needed in many activities involving exprsets modularization at the r function level entails that functions are written to do one meaningful task and no more and that documents help pages are available at the function level with worked exam ples this simplifies debugging and testing modularization at the package level entails that all packages include sufficient functionality and documentation to be used and understood in isolation from most other packages exceptions are for mally encoded in files distributed with the package multiscale and executable documentation accurate and thorough documentation is fundamental to effective software development and use and must be created and maintained in a uniform fashion to have the greatest impact we inherit from r a powerful system for small scale documentation and unit testing in the form of the executable example sections in function oriented manual pages we have also introduced a new concept of large scale documen tation with the vignette concept vignettes go beyond typical man page documentation which generally focuses on docu menting the behavior of a function or small group of func tions the purpose of a vignette is to describe in detail the processing steps required to perform a specific task which generally involves multiple functions and may involve multi ple packages users of a package have interactive access to all vignettes associated with that package the sweave system was adopted for creating and processing vignettes once these have been written users can interact with them on different levels the transformed docu ments are provided in adobe portable document format pdf and access to the code chunks from within r is availa ble through various functions in the tools package however new users will need a simpler interface our first offering in this area is the vignette explorer vexplorer which provides a widget that can be used to navigate the various code chunks each chunk is associated with a button and the code is dis played in a window within the widget when the user clicks on the button the code is evaluated and the output presented in a second window other buttons provide other functional ity such as access to the pdf version of the document we plan to extend this tool greatly in the coming years and to integrate it closely with research into reproducible research see for an illustration automated software distribution the modularity commitment imposes a cost on users who are accustomed to integrated end to end environments users of bioconductor need to be familiar with the existence and functionality of a large number of packages to diminish this cost we have extended the packaging infrastructure of r cran to better support the deployment and management of packages at the user level automatic updating of packages when new versions are available and tools that obtain all package dependencies automatically are among the features provided as part of the repostools package in bioconductor note that new methods in r package design and distribution include the provision of checksums with all packages to help with verification that package contents have not been altered in transit genome biology volume issue article gentleman et al http genomebiology com in conclusion these engineering commitments and develop ments have led to a reasonably harmonious set of tools for cbb it is worth considering how the s language notion that everything is an object impacts our approach we have made use of this notion in our commitment to contracting and object oriented programming and in the automated distribu tion of resources in which package catalogs and biological metadata are all straightforward r objects packages and doc uments are not yet treatable as r objects and this leads to complications we are actively studying methods for simplify ing authoring and use of documentation in a multipackage environment with namespaces that allow symbol reuse and for strengthening the connection between session image and package inventory in use so that saved r images can be restored exactly to their functional state at session close distributed development and recruitment of developers distributed development is the process by which individuals who are significantly geographically separated produce and extend a software project this approach has been used by the r project for approximately years this was necessitated in this case by the fact no institution currently has sufficient numbers of researchers in this area to support a project of this magnitude distributed development facilitates the inclusion of a variety of viewpoints and experiences contributions from individuals outside the project led to the expansion of the core developer group membership in the core depends upon the willingness of the developer to adopt shared objec tives and methods and to submerge personal objectives in preference to creation of software for the greater scientific community distributed development requires the use of tools and strate gies that allow different programmers to work approximately simultaneously on the same components of the project among the more important requirements is for a shared code base or archive that all members of the project can access and modify together with some form of version management system we adopted the concurrent versions system and created a central archive within this system that all members of the team have access to additional discipline is needed to ensure that changes by one programmer should not result in a failure of other code in the system within the r language software components are nat urally broken into packages with a formal protocol for pack age structure and content specified in the r extensions manual each package should represent a single coher ent theme by using well defined applications programming interfaces apis developers of a package are free to modify their internal structures as long as they continue to provide the documented outputs we rely on the testing mechanisms supported by the r pack age testing system to ensure coherent non regressive development each developer is responsible for documenting all functions and for providing examples and possibly other scripts or sets of commands that test the code each developer is responsible for ensuring that all tests run successfully before committing changes back to the central archive thus the person who knows the code best writes the test programs but all are responsible for running them and ensuring that changes they have made do not affect the code of others in some cases changes by one author will necessitate change in the code and tests of others under the system we are using these situations are detected and dealt with when they occur in development reducing the frequency with which error reports come from the field members of the development team communicate via a private mailing list in many cases they also use private email tele phone and meetings at conferences in order to engage in joint projects and to keep informed about the ideas of other members reuse of exogenous resources we now present three arguments in favor of using and adapt ing software from other projects rather than re implementing or reinventing functionality the first argument that we con sider is that writing good software is a challenging problem and any re implementation of existing algorithms should be avoided if possible standard tools and paradigms that have been proven and are well understood should be preferred over new untested approaches all software contains bugs but well used and maintained software tends to contain fewer the second argument is that cbb is an enormous field and that progress will require the coordinated efforts of many projects and software developers thus we will require struc tured paradigms for accessing data and algorithms written in other languages and systems the more structured and inte grated this functionality the easier it will be to use and hence the more it will be used as specific examples we consider our recent development of tools for working with graph or net work structures there are three main packages in biocon ductor of interacting with graphs they are graph rbgl and rgraphviz the first of these provides the class descriptions and basic infrastructure for dealing with graphs in r the sec ond provides access to algorithms on graphs and the third to a rich collection of graph layout algorithms the graph pack age was written from scratch for this project but the other two are interfaces to rich libraries of software routines that have been created by other software projects boost and graphviz respectively both of which are very sub stantial projects with large code bases we have no interest in replicating that work and will wherever possible simply access the functions and libraries produced by other projects there are many benefits from this approach for us and for the other projects for bioinformatics and computational biology we gain rapid access to a variety of graph algorithms includ ing graph layout and traversal the developers in those http genomebiology com genome biology volume issue article gentleman et al communities gain a new user base and a new set of problems that they can consider gaining a new user base is often very useful as new users with previously unanticipated needs tend to expose weaknesses in design and implementation that more sophisticated or experienced users are often able to avoid in a similar vein we plan to develop and encourage collabo ration with other projects including those organized through the open bioinformatics foundation and the international interoperability consortium we have not specifically con centrated on collaboration to this point in part because we have chosen areas for development that do not overlap signif icantly with the tools provided by those projects in this case our philosophy remains one of developing interfaces to the software provided by those projects and not re implementing their work in some cases other projects have recognized the potential gains for collaboration and have started developing interfaces for us to their systems with the intent of making future contributions another argument in favor of standardization and reuse of existing tools is best made with reference to a specific exam ple consider the topic of markup and markup languages for any specific problem one could quickly devise a markup that is sufficient for that problem so why then should we adopt a standard such as xml among the reasons for this choice is the availability of programmers conversant with the para digm and hence lower training costs a second reason is that the xml community is growing and developing and we will get substantial technological improvements without having to initiate them this is not unusual other areas of computa tional research are as vibrant as cbb and by coordinating and sharing ideas and innovations we simplify our own tasks while providing stimulus to these other areas publication and licensing of code modern standards of scientific publication involve peer review and subsequent publication in a journal software publication is a slightly different process with limited involve ment to date of formal peer review or official journal publica tion we release software under an open source license as our main method of publication we do this in the hope that it will encourage reproducibility extension and general adherence to the scientific method this decision also ensures that the code is open to public scrutiny and comment there are many other reasons for deciding to release software under an open source license some of which are listed in table another consideration that arose when determining the form of publication was the need to allow an evolutionary aspect to our own software there are many reasons for adopting a strategy that would permit us to extend and improve our soft ware offerings over time the field of cbb is relatively volatile and as new technologies are developed new software and inferential methods are needed further software technology itself is evolving thus we wanted to have a publication strat egy that could accommodate changes in software at a variety of levels we hope that that strategy will also encourage our users to think of software technology as a dynamic field rather than a static one and to therefore be on the lookout for inno vations in this arena as well as in more traditional biological ones our decision to release software in the form of r packages is an important part of this consideration packages are easy to distribute they have version numbers and define an api a coordinated release of all bioconductor packages occurs twice every year at any given time there is a release version of every package and a development version the only changes allowed to be made on the release version are bug fixes and documentation improvements this ensures that users will not encounter radical new behaviors in code obtained in the release version all other changes such as enhancements or design changes are carried out on the development branch approximately six weeks before a release a major effort is taken to ensure that all packages on the development branch are coordinated and work well together during that period extensive testing is carried out through peer review amongst the bioconductor core at release time all packages on the development branch that are included in the release change modes and are now released packages previous versions of these packages are deprecated in favor of the newly released versions simultaneously a new development branch is made and the developers start to work on packages in the new branch note that these version related administrative oper ations occur with little impact on developers the release manager is responsible for package snapshot and file version modifications the developers source code base is fairly sim ple and need not involve retention of multiple copies of any source code files even though two versions are active at all times we would also like to point out that there are compelling arguments that can be made in favor of choosing different paradigms for software development and deployment we are not attempting at this juncture to convince others to distrib ute software in this way but rather elucidating our views and the reasons that we made our choice under a different set of conditions or with different goals it is entirely likely that we would have chosen a different model special concerns we now consider four specific challenges that are raised by research in computational biology and bioinformatics repro ducibility data evolution and complexity training users and responding to user needs genome biology volume issue article gentleman et al http genomebiology com table reasons for deciding to release software under an open source license to encourage reproducibility extension and general adherence to the scientific method to ensure that the code is open to public scrutiny and comment to provide full access to algorithms and their implementation to provide to users the ability to fix bugs without waiting for the developer and to extend and improve the supplied software to encourage good scientific computing and statistical practice by exhibiting fully appropriate tools and instruction to provide a workbench of tools that allow researchers to explore and expand the methods used to analyze biological data to ensure that the international scientific community is the owner of the software tools needed to carry out research to lead and encourage commercial support and development of those tools that are successful to promote reproducible research by providing open and accessible tools with which to carry out that research reproducible research we would like to address the reproducibility of published work in cbb reproducibility is important in its own right and is the standard for scientific discovery reproducibility is an important step in the process of incremental improvement or refinement in most areas of science researchers continu ally improve and extend the results of others but for scientific computation this is generally the exception rather than the rule buckheit and donoho referring to the work and philos ophy of claerbout state the following principle an article about computational science in a scientific publication is not the scholarship itself it is merely advertising of the scholar ship the actual scholarship is the complete software develop ment environment and that complete set of instructions that generated the figures there are substantial benefits that will come from enabling authors to publish not just an advertisement of their work but rather the work itself a paradigm that fundamentally shifts publication of computational science from an advertisement of scholarship to the scholarship itself will be a welcome addi tion some of the concepts and tools that can be used in this regard are contained in when attempting to re implement computational methodol ogy from a published description many difficulties are encountered schwab et al make the following points indeed the problem occurs wherever traditional methods of scientific publication are used to describe computational research in a traditional article the author merely outlines the relevant computations the limitations of a paper medium prohibit complete documentation including experimental data parameter values and the author programs conse quently the reader has painfully to re implement the author work before verifying and utilizing it the reader must spend valuable time merely rediscovering minutiae which the author was unable to communicate conveniently the development of a system capable of supporting the con venient creation and distribution of reproducible research in cbb is a massive undertaking nevertheless the bioconduc tor project has adopted practices and standards that assist in partial achievement of reproducible cbb publication of the data from which articles are derived is becoming the norm in cbb this practice provides one of the components needed for reproducible research access to the data the other major component that is needed is access to the software and the explicit set of instructions or commands that were used to transform the data to provide the outputs on which the conclusions of the paper rest in this regard pub lishing in cbb has been less successful it is easy to identify major publications in the most prestigious journals that pro vide sketchy or indecipherable characterizations of computa tional and inferential processes underlying basic conclusions this problem could be eliminated if the data housed in public archives were accompanied by portable code and scripts that regenerate the article figures and tables the combination of r well established platform independ ence with bioconductor packaging and documentation standards leads to a system in which distribution of data with working code and scripts can achieve most of the require ments of reproducible and replayable research in cbb the steps leading to the creation of a table or figure can be clearly exposed in an sweave document an r user can export the code for modification or replay with variations on parameter settings to check robustness of the reported calculations or to explore alternative analysis concepts thus we believe that r and bioconductor can provide a start along the path towards generally reproducible research in cbb the infrastructure in r that is used to support replaya bility and remote robustness analysis could be implemented http genomebiology com genome biology volume issue article gentleman et al in other languages such as perl and python all that is needed is some platform independent format for binding together the data software and scripts defining the analysis and a document that can be rendered automatically to a con veniently readable account of the analysis steps and their out comes if the format is an r package this package then constitutes a single distributable software element that embodies the computational science being published this is precisely the compendium concept espoused in dynamics of biological annotation metadata are data about data and their definition depends on the perspective of the investigator metadata for one investi gator may well be experimental data for another there are two major challenges that we will consider first is the evolu tionary nature of the metadata as new experiments are done and as our understanding of the biological processes involved increases the metadata changes and evolves the second major problem that concerns metadata data is its complexity we are trying to develop software tools that make it easier for data analysts and researchers to use the existing metadata appropriately the constant changing and updating of the metadata suggests that we must have a system or a collection process that ensures that any metadata can be updated and the updates can be distributed users of our system will want access to the most recent versions our solution has been to place meta data into r packages these packages are built using a semi automatic process and are distributed and updated using the package distribution tools developed in the repos tools package there is a natural way to apply version num bers so users can determine if their data are up to date or if necessary they can obtain older versions to verify particular analyses further users can synchronize a variety of meta data packages according to a common version of the data sources that they were constructed from there are a number of advantages that come from automating the process of building data packages first the modules are uniform to an extent that would not be possible if the pack ages were human written this means that users of this tech nology need only become acquainted with one package to be acquainted with all such packages second we can create many packages very quickly hence the labor savings are sub stantial for microarray analyses all data packages should have the same information chromosomal location gene ontology categories and so on the only difference between the packages is that each references only the specific set of genes probes that were assayed this means that data ana lysts can easily switch from one type of chip to another it also means that we can develop a single set of tools for manipulat ing the metadata and improvements in those tools are availa ble to all users immediately users are free to extend data packages with data from other potentially proprietary sources treating the data in the same manner that we treat software has also had many advantages on the server side we can use the same software distribution tools indicating updates and improvements with version numbering on the client side the user does not need to learn about the storage or internal details of the data packages they simply install them like other packages and then use them one issue that often arises is whether one should simply rely on online sources for metadata that is given an identifier the user can potentially obtain more up to date information by querying the appropriate databases the data packages we are proposing cannot be as current there are however some disadvantages to the approach of accessing all resources online first users are not always online they are not always aware of all applicable information sources and the invest ment in person time to obtain such information can be high there are also issues of reproducibility that are intractable as the owners of the web resources are free to update and modify their offerings at will some but not all of these difficulties can be alleviated if the data are available in a web services format another argument that can be made in favor of our approach in this context is that it allows the person constructing the data packages to amalgamate disparate information from a number of sources in building metadata packages for bio conductor we find that some data are available from different sources and under those circumstances we look for consen sus if possible the process is quite sophisticated and is detailed in the annbuilder package and paper training most of the projects in cbb require a combination of skills from biology computer science and statistics because the field is new and there has been little specialized training in this area it seems that there is some substantial benefit to be had from paying attention to training from the perspective of the bioconductor project many of our potential users are unfamiliar with the r language and generally are scientifically more aligned with one discipline than all three it is therefore important that we produce documentation for the software modules that is accessible to all we have taken a two pronged approach to this we have developed substantial amounts of course material aimed at all the constituent disci plines and we have developed a system for interactive use of software and documentation in the form of vignettes and more generally in the form of navigable documents with dynamic content course materials have been developed and refined over the past two to three years several members of the bioconductor development team have taught courses and subsequently refined the material based on success and feedback the materials developed are modular and are freely distributed although restrictions on publication are made the focus of genome biology volume issue article gentleman et al http genomebiology com the materials is the introduction and use of software devel oped as part of the bioconductor project but that is not a requirement and merely reflects our own specific purposes and goals in this area we feel that we would benefit greatly from contri butions from those with more experience in technical docu ment authoring there are likely to be strategies concepts and methodologies that are standard practice in that domain that we are largely unaware of however in the short term we rely on the students our colleagues and the users of the bio conductor system to guide us and we hope that many will con tribute others can easily make substantial contributions even those with little or no programming skills what is required is domain knowledge in one field of interest and the recognition of a problem that requires additional domain knowledge from another of the fields of interest our experience has been that many of these new users often transform themselves into developers thus our develop ment of training materials and documentation needs to pay some attention to the needs of this group as well there are many more software components than we can collectively produce attracting others to collaboratively write software is essential to success responding to user needs the success of any software project rests on its ability to both provide solutions to the problems it is addressing and to attract a user community perhaps the most effective way of addressing user needs is through an e mail help list and one was set up as soon as the project became active in addition it is important to keep a searchable archive available so that the system itself has a memory and new users can be referred there for answers to common questions it is also important that members of the project deal with bug reports and feature requests through this public forum as it both broadcasts their intentions and provides a public record of the discussion our mailing list mailto bioconductor stat math ethz ch has been successful there are approximately subscribers and about email messages per year attracting a user community itself requires a method of dis tributing the software and providing sufficient training mate rials to allow potential users to explore the system and determine whether it is sufficient for their purposes an alter nate approach would be to develop a graphical user interface gui that made interactions with the system sufficiently self explanatory that documentation was not needed we note that this solution is generally more applicable to cases where the underlying software tasks are well defined and well known in the present case the software requirements as well as the statistical and biological requirements are con stantly evolving r is primarily command line oriented and we have chosen to follow that paradigm at least for the first few years of development we would of course welcome and collaborate with those whose goal was in gui development but our own forays into this area are limited to the production of a handful of widgets that promote user interaction at spe cific points users have experienced difficulties downloading and install ing both r and the bioconductor modules some of these dif ficulties have been caused by the users local environments firewalls and a lack of direct access to the internet and some by problems with our software bugs which arise in part because it is in general very difficult to adequately test soft ware that interacts over the internet we have however man aged to help every user who was willing to persist get both r and bioconductor properly installed another substantial dif ficulty that we had to overcome was to develop a system that allowed users to download not just the software package that they knew they wanted but additionally and at the same time all other software packages that it relies on with bio conductor software there is a much larger inter reliance on software packages including those that provide machine learning biological metadata and experimental data than for most other uses of r and the r package system the package repostools contains much of the necessary infrastructure for handling these tasks it is a set of functions for dealing with r package repositories which are basically internet locations for collections of r packages once the basic software is installed users will need access to documentation such as the training materials described above and other materials such as the vignettes described in a previous section such materials are most valuable if the user can easily obtain and run the examples on their own computer we note the obvious similarity with this problem and that described in the section on reproducible research again we are in the enjoyable situation of having a paradigm and tools that can serve two purposes other open source bioinformatics software projects the open bioinformatics foundation supports projects simi lar to bioconductor that are nominally rooted in specific pro gramming languages bioperl biopython and biojava are prominent examples of open source lan guage based bioinformatics projects the intentions and design methodologies of the bioperl project have been lucidly described by stajich and colleagues bioperl in this section we consider commonalities and differences between bioperl and bioconductor both projects have com mitments to open source distribution and to community based development with an identified core of developers per forming primary design and maintenance tasks for the project both projects use object oriented programming methodology with the intention of abstracting key structural and functional features of computational workflows in bioin formatics and defining stable application programming http genomebiology com genome biology volume issue article gentleman et al interfaces api that hide implementation details from those who do not need to know them the toolkits are based on highly portable programming languages these languages have extensive software resources developed for non bioin formatic purposes the repositories for r comprehensive r archive network cran and perl comprehensive perl archive network cpan provide mirrored www access to structured collections of software modules and documents for a wide variety of workflow elements development methodol ogies targeted at software reuse can realize large gains in pro ductivity by establishing interfaces to existing cpan or cran procedures instead of reimplementing such procedures for reuse to succeed the maintainer of the external resource must commit to stability of the resource api such stability tends to be the norm for widely used modules finally both languages have considerable interoperability infrastructure one implication is that each project can use software written in unrelated languages r has well established interfaces to perl python java and c r api allows software in r to be called from other languages and the rsperl package facilitates direct calls to r from perl thus there are many opportunities for symbiotic use of code by bioconductor and bioperl developers and users the following script illustrates the use of bioperl in r library rsperl perlpackage bio perl x perl swiss x division human x accession unlist x nuclear protein rna binding repeat ribonucleoprotein methylation transport the perlpackage command brings the bioperl modules into scope perl invokes the bioperl subroutine with arguments swiss and the resulting r object is a reference to a perl hash rsperl infrastructure permits interrogation of the hash via the operator note that rsperl is not a bioconductor supported utility and that installation of the bioperl and rsperl resources to allow interoperation can be complicated key differences between the bioconductor and bioperl projects concern scope approaches to distribution documen tation and testing and important details of object oriented design scope bioperl is clearly slanted towards processing of sequence data and interfacing to sequence databases with support for sequence visualization and queries for external annotation bioconductor is slanted towards statistical analysis of micro array experiments with major concerns for array preprocess ing quality control within and between array normalization binding of covariate and design data to expression data and downstream inference on biological and clinical questions bioconductor has packages devoted to diverse microarray manufacturing and analysis paradigms and to other high throughput assays of interest in computa tional biology including serial analysis of gene expression sage array comparative genomic hybridization array cgh and proteomic time of flight seldi tof data we say the projects are slanted towards these concerns because it is clear that both projects ultimately aim to support general research activities in computational biology distribution documentation and testing bioperl inherits the distribution paradigm supported by cpan software modules can be acquired and installed inter actively using for example perl mcpan e shell this process supports automated retrieval of requested packages and dependencies but is not triggered by runtime events bioconductor has extended the cran distribution function alities so that packages can be obtained and installed just in time as required by a computational request for both perl and r software modules and packages are structured collec tions of files some of which are source code some of which are documents about the code the relationship between doc umentation and testing is somewhat tighter in bioconductor than in bioperl manual pages and vignettes in bioconductor include executable code failure of the code in a man page or vignette is a quality control event experimentation with exe cutable code in manual pages through the example function of r is useful for learning about software behavior in perl tests occupy separate programs and are not typically inte grated with documentation details of object oriented procedure both r and perl are extensible computer languages thus it is possible to introduce software infrastructure supporting dif ferent approaches to object oriented programming oop in various ways in both languages genome biology volume issue article gentleman et al http genomebiology com r80 r core developers have provided two distinct approaches to oop in r these approaches are named and in any object can be assigned to a class or sequence of classes sim ply by setting the class name as the value of the object class attribute class hierarchies are defined implicitly at the object level generic methods are defined as ordinary functions and class specific methods are dispatched according to the class of the object being passed as an argument in formal def inition of class structure is supported and class hierarchy is explicitly defined in class definitions class instances are explicitly constructed and subject to validation at time of con struction generic methods are non standard r functions and metadata on generic methods is established at the package level specific methods are dispatched according to the class signature of the argument list multiple dispatch overall the oop approach embodied in is closer to dylan or scheme than to c or java bioconductor does not require specific oop methodology but encourages the use of and core members have contributed special tools for the docu mentation and testing of oop methods in r oop methodology in perl has a substantial history and is extensively employed in bioperl the basic approach to oop in perl seems to resemble more than in that perl bless operation can associate any perl data instance with any class the cpan class multimethod module can be used to allow multiple dispatch behavior of generic subroutines the specific classes of objects identified in bioperl are targeted at sequence data seq locatableseq relsegment are exam ples location data simple split fuzzy and an important class of objects called interface objects which are classes whose names end in i these objects define what methods can be called on objects of specified classes but do not imple ment any methods biojava biopython gmod and moby other open bioinformatics projects have intentions and methods that are closely linked with those of bioconductor biojava provides dazzle a servlet framework supporting the distributed annotation system specification for sharing sequence data and metadata version of the biojava release includes java classes for general alphabets and sym bol list processing tools for parsing outputs of blast related analyses and software for constructing and fitting hidden markov models in principle any of these resources could be used for analysis in bioconductor r through the sjava inter face biopython provides software for constructing python objects by parsing output of various alignment or clustering algorithms and for a variety of downstream tasks including classification biopython also provides infrastructure for decomposition of parallelizable tasks into separable proc esses for computation on a cluster of workstations the generic model organism database gmod project tar gets construction of reusable components that can be used to reproduce successful creation of open and widely accessible databases of model organisms for example worm fruitfly and yeast the main tasks addressed are genome visualiza tion and annotation literature curation biological ontology activities gene expression analysis and pathway visualization and annotation biomoby provides a framework for developing and cat aloging web services relevant to molecular biology and genomics a basic aim is to provide a central registry of data annotation or analysis services that can be used programmat ically to publish and make use of data and annotation resources pertinent to a wide variety of biological contexts as these diverse projects mature particularly with regard to interoperability we expect to add infrastructure to biocon ductor to simplify the use of these resources in the context of statistical data analysis it is our hope that the r and biocon ductor commitments to interoperability make it feasible for developers in other languages to reuse statistical and visuali zation software already present and tested in r using bioconductor example results of the bioconductor project include an extensive repository of software tools documentation short course materials and biological annotation data at we describe the use of the software and annotation data by description of a concrete analysis of a microarray archive derived from a leukemia study acute lymphocytic leukemia all is a common and difficult to treat malignancy with substantial variability in therapeutic outcomes some all patients have clearly characterized chromosomal aberrations and the functional consequences of these aberrations are not fully understood bioconductor tools were used to develop a new characterization of the con trast in gene expression between all patients with two spe cific forms of chromosomal translocation the most important tasks accomplished with bioconductor employed simple to use tools for state of the art normalization of hun dreds of microarrays clear schematization of normalized expression data bound to detailed covariate data flexible approaches to gene and sample filtering to support drilling down to manageable and interpretable subsets flexible visu alization technologies for exploration and communication of genomic findings and programmatic connection between expression platform metadata and biological annotation data supporting convenient functional interpretation we will illustrate these through a transcript of the actual command output sequence more detailed versions of some of the processing and analysis activities sketched here can be found in the vignettes from the gostats package http genomebiology com r80 genome biology volume issue article r80 gentleman et al r80 13 data all eset all all mol in c bcr abl next we find genes which are differentially expressed between the and bcr abl groups we use the function lmfit from the limma package which can assess differential expression between many different groups and conditions simultaneously the function lmfit accepts a model matrix which describes the experimental design and produces an output object of class marraylm which stores the fitted model information for each gene the fitted model object is further processed by the ebayes function to produce empirical bayes test statistics for each gene including moder ated t statistics p values and log odds of differential expres sion the fold changes average intensites and holm adjusted p values are displayed for the top genes figure figure limma analysis of the all data the leftmost numbers are row indices id is the affymetrix accession number m is the log ratio of expression a is the log average expression and b is the log odds of differential expression the dataset is from the ritz laboratory at the dana farber cancer institute it contains data from patients with all two subgroups are to be compared the first group con sists of patients with a translocation between chromosomes and labeled the second group consists of patients with a translocation between chromosomes and labeled bcr abl these conditions are mutually exclusive in this dataset the affymetrix platform was used and expres sion measures were normalized using gcrma from the affy package the output of this is an object of class exprset which can be used as input for other functions the package provides biological metadata including mappings from the affymetrix identifiers to go chromosomal location and so on these data can of course be obtained from many other sources but there are some advantages to having them as an r package after loading the appropriate packages we first subset the all exprset to extract those samples with the covariates of interest the design of the exprset class includes methods for subsetting both cases and probes by using the square bracket notation on all we derive a new exprset with data on only the desired patients we select those genes that have adjusted p values below the default method of adjusting for multiple comparisons uses holm method to control the family wise error rate we could use a less conservative method such as the false discov ery rate and the multtest package offers other possibilities but for this example we will use the very stringent holm method to select a small number of genes there are genes selected for further analysis a heat map produced by the heatmap function from r allows us to visual ize the differential action of these genes between the two groups of patients note how the different software modules can be integrated to provide a very rich data analysis environ ment figure shows clearly that these two groups can be dis tinguished in terms of gene expression we can carry out many other tests for example whether genes encoded on a particular chromosome or perhaps on a specific strand of a chromosome are over represented amongst those selected by moderated t test many of these questions are normally addressed in terms of a hypergeomet ric distribution but they can also be thought of as two way or multi way tables and alternate statistical tests all readily available in r can be applied to the resulting data we turn our attention briefly to the use of the gene ontology go annotation in conjunction with these data we first identify the set of unique locuslink identifiers among our selected affymetrix probes the function gohyperg is found figure heat map produced by the bioconductor function heatmap of the all leukemia data in the gostats package it carries out a hypergeometric test for an overabundance of genes in our selected list of genes for each term in the go graph that is induced by these genes fig ure the smallest p value found was 8 and it corresponds to the term mhc class ii receptor activity we see that six of the genes with this go annotation have been selected had we used a slightly less conservative gene selection method then the number of selected genes in this go annotation would have been even higher reproducing the above results for any other species or chip for which an annotation package was available would require almost no changes to the code the analyst need only substi tute the references to the data package with those for their array and the basic principles and code are unchanged the group dynamic has also been an important factor in the success of bioconductor a willingness to work together to see that cooperation and coordination in software development yields substantial benefits for the developers and the users and encouraging others to join and contribute to the project are also major factors in our success to date the project provides the following resources an online repository for obtaining software data and metadata papers and training materials a development team that coordinates the discussion of software strategies and develop ment a user community that provides software testing sug gested improvements and self help more than software packages hundreds of metadata packages and a number of experimental data packages at this point it is worth considering the future while many of the packages we have developed have been aimed at particu lar problems there have been others that were designed to support future developments and that future seems very interesting many of the new problems we are encountering in cbb are not easily addressed by technology transfer but rather require new statistical methods and software tools we hope that we can encourage more statisticians to become involved in this area of research and to orient themselves and their research to the mixture of methodology and software development that is necessary in this field figure hypergeometric analysis of molecular function enrichment of genes selected in the analysis described in figure similarly substitution of other algorithms or statistical tests is possible as the data analyst has access to the full and complete source code all tools are modifiable at the source level to suit local requirements abstract comparative analysis of molecular sequence data is essential for reconstructing the evolutionary histories of species and inferring the nature and extent of selective forces shaping the evolution of genes and species here we announce the release of molecular evolutionary genetics analysis version which is a user friendly software for mining online databases building sequence alignments and phylogenetic trees and using methods of evolutionary bioinformatics in basic biology biomedicine and evolution the newest addition in is a collection of maximum likelihood ml analyses for inferring evolutionary trees selecting best fit substitution models nucleotide or amino acid inferring ancestral states and sequences along with probabilities and estimating evolutionary rates site by site in computer simulation analyses ml tree inference algorithms in compared favorably with other software packages in terms of computational efficiency and the accuracy of the estimates of phylogenetic trees substitution parameters and rate variation among sites the mega user interface has now been enhanced to be activity driven to make it easier for the use of both beginners and experienced scientists this version of mega is intended for the windows platform and it has been configured for effective use on mac os x and linux desktops it is available free of charge from http www megasoftware net introduction the molecular evolutionary genetics analysis mega soft ware was developed with the goal of providing a biologist cen tric integrated suite of tools for statistical analyses of dna and protein sequence data from an evolutionary standpoint over the years it has grown to include tools for sequence alignment phylogenetic tree reconstruction and visualization testing an array of evolutionary hypotheses estimating se quence divergences web based acquisition of sequence data and expert systems to generate natural language descriptions of the analysis methods and data chosen by the user kumar et al kumar and dudley with the fifth major release the collection of analysis tools in mega has now broadened to include the maximum likelihood ml methods for molecular evolutionary analysis table contains a summary of all statistical methods and models in with new features marked with an asterisk in the following we provide a brief description of methodo logical advancements along with relevant research results and technical enhancements in model selection for nucleotide and amino acid sequences now contains facilities to evaluate the fit of major models of nucleotide and amino acid substitutions which are frequently desired by researchers posada and crandall nei and kumar yang fig for nucle otide substitutions the gtr and five nested models are available whereas six models with and without empirical frequencies f have been programmed for the amino acid substitutions table provides the goodness of fit see below of the substitution models with and without assuming the existence of evolutionary rate vari ation among sites which is modeled by a discrete gamma distribution g yang and or an allowance for the presence of invariant sites i fitch and margoliash fitch shoemaker and fitch this results in an evaluation of and models for nucleotide and amino acid substitutions respectively for each of these models provides the estimated values of shape parameter of the gamma distribution a the proportion of invariant sites and the substitution rates between bases or residues as applicable depending on the model the assumed or observed values of the base or amino acid frequencies used in the analysis are also provided this information enables researchers to quickly examine the robustness of the esti mates of evolutionary parameters under different models of substitutions and assumptions about the distribution of evolutionary rates among sites fig the goodness of fit of each model to the data is measured by the bayesian information criterion bic schwarz and corrected the author published by oxford university press on behalf of the society for molecular biology and evolution all rights reserved for permissions please e mail journals permissions oup com downloaded frommohlt tbpsio l aecvaodle m1b 1ab1stradcoti 1m advance access publication may table a summary of analyses and substitution models in sequence alignments dna codon and protein alignments both manual and automated alignments with trace file editor built in automated aligners clustalw and muscle major analyses statistical approach in parentheses models and parameters select best fit substitution model ml test pattern homogeneity estimate substitution pattern mcl ml estimate rate variation among sites ml estimate transition transversion bias mcl ml estimate site by site rates ml infer phylogenies infer phylogenetic trees nj ml me mp phylogeny tests bootstrap and branch length tests branch and bound exact search mp heuristic searches nearest neighbor interchange nni ml me mp close neighbor interchange cni ml me mp and max mini mp compute distances pairwise and diversity within and between group distances bootstrap and analytical variances separate distances by site degeneracy codon sites separation of distances in transitions and transversions separate nonsynonymous and synonymous changes tests of selection for complete sequences or set of codons sequence pairs or groups within and between ancestral sequences infer by ml with relative probabilities for bases or residues or by mp all parsimonious pathways molecular clocks tajima sequence clock test likelihood ratio test ml for a topology estimate branch lengths under clock substitution models with empirical frequencies rev reversible dna general time reversible gtr tamura nei hasegawa kishino yano tamura three parameter kimura two parameter tajima nei jukes cantor codons nei gojobori original and modified li wu lou original and modified protein poisson equal input dayhoff jones taylor thornton whelan and goldman mitochondrial rev chloroplast rev reverse transcriptase rev rate variation and base compositions gamma rates g and invariant sites i models incorporate compositional heterogeneity note mcl maximum composite likelihood me minimum evolution mp maximum parsimony an asterix denotes features that are new in akaike information criterion aicc hurvich and tsai see also posada and buckley by default lists models with decreasing bic values see below for the rea son and caveats along with log likelihood as well as aicc values for each model in the ml methods for evaluating the fit of substitution models to the data an evolutionary tree is needed automatically infers the evolutionary tree by the neighbor joining nj algorithm that uses a matrix of pairwise distan ces estimated under the jones thornton taylor jtt model for amino acid sequences or the tamura and nei model for nucleotide sequences saitou and nei jones et al tamura and nei tamura et al branch lengths and substitution rate parame ters are then optimized for each model to fit the data users may provide their own tree topology in the newick new hampshire format for use in this model selection fig however the automatic option is expected to be fre quently used because trees are rarely known a priori we tested the impact of the use of automatically generated trees in on the process of model selection by com puter simulation these simulations used sets of evo lutionary parameters base frequencies sequence length mean evolutionary rate and transition transversion rate ratio derived from real sequence data see rosenberg and kumar and introduced four different levels of rate variation among sites for each parameter set gamma shape parameter a and results showed that the best fit models produced by using auto matically generated trees were the same as those inferred using the true tree for of the data sets according to the bic and aicc criteria fig for an overwhelming majority of data sets aicc selected the most complex model see also ripplinger and sullivan but both bic and aicc selected substitutions models that were more complex than the true model posada and buckley alfaro and huelsenbeck the true model was among the top when bic was used and among the top when aicc was used when the rate variation among sites was extreme a models incorporating invariant sites i along with discrete gamma rate categories g were favored for virtually every data set this means that a discrete gamma g model using a small number of cat egories which is a common practice coupled with an allowance for invariant sites i is better at approximating the continuous gamma distribution used in the simulation when the rate variation among sites is severe this was con firmed by comparing the ml value for the fit of hky g model categories with the ml value for gtr g i model using only four discrete gamma categories the for mer performed slightly better than the latter even though the latter involved a more complex model on the basis of the above observation we pooled g and g i results for each model of substitution and found that bic selects the true model for of the data sets in con trast aicc selects the correct model only of the time therefore we rank the models by bic in fig however the choice of criterion to select the best fit models is rather complicated and researchers should explore model selection based on aicc values and other available methods for evolutionary analyses in which choice of model is known to substantially affect the final result e g tamura et al ripplinger and sullivan to facilitate downstream anal ysis to select the best model provides exporting of results in microsoft excel open office and comma sepa rated values formats these simulation results also provided us with an opportunity to evaluate the estimates of a obtained by us ing the automatically generated tree and to compare them to those obtained by using the true tree under the correct model of substitution the means and standard deviations of these estimates were very similar to the true values and virtually identical for automatically generated and true trees fig similarly the overall estimates of downloaded academic oup com mbe article abstract 973375 fig evaluating the fit of substitution models in a the models menu in the action bar provides access to the facility b an analysis preferences dialog box provides the user withan array of choices including the choice of tree to use and the method to treatmissing data and alignment gaps in addition to the complete deletion and pairwise deletion options now includes a partial deletion option that enables users to exclude positions if they have less than a desired percentage x of site coverage that is no more than x sequences at a site are allowed to have an alignment gap missing datum or ambiguous base amino acid for protein coding nucleotide sequences users can choose to analyze nucleotide or translated amino acid substitutions with a choice of codon positions in the former c the list of evaluated substitution models along with their relative fits number of parameters branch lengths model parameters and estimates of evolutionary parameters for drosophila adh sequence data which are available in the examples directory in installation the note below the table provides a brief description of the results e g ranking of models by bic data subset selected and the analysis option chosen this figure is available in color online and in black and white in print transition transversion ratio r were close to the true value for both automatically generated and true trees fig therefore the use of automatically generated trees with is useful as a first approximation in estimat ing evolutionary substitution parameters and evaluating relative fits of models inferring ml trees now provides the ml method to infer evolutionary trees and conduct the bootstrap test for nucleotide and amino acid alignments felsenstein because the ml method is computationally demanding we provide heuristic methods that search for the ml tree by topological rearrangements of an initial tree swofford nei and kumar guindon and gascuel stamatakis et al the initial tree for the ml search can be supplied by the user newick format or generated automatically by applying nj and bionj algorithms to a matrix of pairwise distances estimated using a maximum composite likelihood approach for nucleotide sequences and a jtt model for amino acid sequences saitou and nei jones et al gascuel tamura et al for the user selected data subset that contains sites with insertion deletions and missing data we begin by temporarily obtaining a site coverage parameter such that the number of ambiguous states and insertion deletions per sequence are the lowest this site coverage parameter is then used to generate a data subset for estimating evolutionary distan ces to build an initial tree along with branch lengths we downloaded from https academic oup com mbe article abstract 973375 fig comparison of the best fit model identified by using automatically generated and true trees for computer simulated sequence data sets a the percentage of datasets for which the use of an automatically generated tree produces the same best fit model as does the use of the true tree results are shown from datasets simulated with four different values of the gamma parameter a for rate variation among sites b the estimates of a when using the automatically generated trees filled bars and the true tree open bars the average a and standard deviation are depicted on each bar discrete gamma categories were used c the relationship of true and estimated transition transversion ratio r when using automatically generated trees for data simulated with a the value of r becomes when the transition transversion rate ratio j is in kimura two parameter model the slope of the linear regression was with the intercept passing through the origin using the true tree slope and values were 007 and 98 respectively the absolute average difference between the two sets of estimates was maximum difference similar results were obtained for data simulated with a and found this approach to produce better initial estimates when there are many insertions deletions and missing data in the sequences after this procedure the user selected data subset is restored and used in all subsequent calculations by default conducts an nni search starting with the initial tree such that the alternative trees differ in one branching pattern one can expand the search space by using the cni option in which alternative trees with two branches differences are evaluated e g nei and kumar p in each case ml values are computed for all the alternative trees produced by the branch swapping and all the branch swaps identified to increase the ml value are made simultaneously if several single rearrangements are found to improve ml values for any branch we choose the rearrangement that leads to the highest improvement in the ml value we do not skip any branch swaps as long as it improves the ml value in order to make major com putational time savings we do skip the evaluation of alter native topologies generated by rearrangements involving branches whose lengths are more than three times longer than their approximate standard errors we use the second derivative of the ml score to generate approximate stan dard errors edwards during the branch length opti mizations therefore starting with systematic topological rearrangements of the initial tree we discover trees with a higher ml value these trees are subjected to new rounds of rearrangements and this iterative process continues until no trees with greater likelihood can be found we tested the performance time and accuracy of the nni and cni searches in by means of computer simulated data sets containing sequences see materials and methods we compared the time taken to complete these heuristic searches with each other and with those needed by phyml version guindon et al and raxml version stamatakis results showed that on average a cni search requires twice the time of an nni search in fig speeds of the nni and cni searches were similar to mix and g respectively but they were faster than nni and spr searches respec tively fig similar trends were observed for another simulated dataset in which an increasingly larger number of sequences were analyzed fig sequence data sets for these data the ml heuristic time increase shows a power trend with the increasing number of sequences fig it is important to note that the raxml will be fast er than if the user machine is equipped with mul tiple processor and or multicore cpus because parallel versions of are yet to be implemented even though different programs and search options show large differences in computational times the average accuracies of the inferred ml trees were found to be rather similar the accuracy difference is less than for the data sets containing sequences fig and sequences fig therefore ml methods in appear to be comparable to other widely used ml implementations in terms of computational time and phylogenetic accuracy in these simulations we also compared the estimates of mlvalues generated by and downloaded academic oup com mbe article abstract 973375 a a b 100 500 time seconds b 100 accuracy c 000 000 15 000 000 100 no of sequences 80 accuracy fig accuracies of heuristic ml trees produced by and programs shown are the proportions of interior branches tree partitions inferred correctly along with standard deviation for simulated data sets containing a sequences and b sequences g gtrgamma with four discrete gamma categories mix mixed method of using cat and gamma models substitutionandthe bayesianposteriorprobabilitiesaregen erated for each possible ancestral state assignment for each node yang et al with this addition users can now explore ancestral sequences inferred using maximum parsi 500 no of sequences x fig comparison of the computational speed of ml heuristic searches a average time taken to complete nni and cni g and mix and nni and spr heuristic searches for 792 simulated data sets containing sequences each bars are shown with standard deviation three data sets were excluded from calculations as the nni search failed b c scatter plots showing the time taken to search for the ml tree for alignments that contain and sequences of 000 base pairs the power trend fits are indicated for and 98 in all cases for direct comparisons all analyses were conducted by using discrete categories for the gamma distribution and a gtr model of nucleotide substitution see materials and methods for simulation procedures analysis descriptions and computer hardware used g gtrgamma with four discrete gamma categories mix mixed method of using cat and gamma models for the true tree and found them to differ less than over all simulation cases inference of ancestral states and sequences now provides inferences of ancestral states and sequences using the empirical bayesian method fig given a phylogenetic tree branch lengths are estimated under a user selected model of nucleotide or amino acid mony and ml methods in however the latter is often preferable because it helps investigators to distinguish among multiple equally likely most parsimonious assign ments by using the posterior probabilities for each possible nucleotide or amino acid assignment furthermore it is expected to produce more accurate results at positions that have undergone multiple substitutions over the whole tree or in specific lineages nei and kumar these ancestral states along with posterior probabilities can be exported in multipleformatsforindividualpositionsandforallcomplete ancestral sequences however note that the reconstructed ancestral sequences are not real observed data and may involve systematic biases and random errors especially for the highly variable positions so caution should be exercised if they are to be used in further statistical analysis position by position evolutionary rates for both nucleotide and amino acid sequence data users can estimate relative rates of molecular evolution position by position in users select the number of discrete categories to approximate the gamma distribution specify whether or not to model invariant positions and choose a nucleotide or amino acid substitution model as men tioned earlier they can use an automatically generated downloaded from https academic oup com mbe article abstract 973375 fig position specific inferred ancestral states in a primate opsin phylogeny and the posterior probabilities of alternative amino acids at that position see examples directory for the data file and nei and kumar p 213 for a description of the data this figure is available in color online and in black and white in print topology but it should be done carefully because the site specific estimates of the evolutionary rate may depend on the evolutionary tree used see mayrose et al no information on sequence divergence times is needed for estimating relative rates of evolution over sites where all individual relative rates are scaled such that the average relative rate over all positions is equal to this means that positions showing a relative rate less than are inferred to be more highly conserved than the average conservation of sites in the alignment whenever available these results are automatically exported directly to statistical analysis soft ware including microsoft excel which can be used to gen erate sequence wide profiles and conduct further analyses ml molecular clocks and linearized tree in addition to tajima nonparametric test of molecular clock for three sequences tajima we have now added a likelihood ratio test of the molecular clock where the ml value for a given tree assuming the rate uniformity among lineages is compared with that without the assump tion in the output primary information along with the p value of rejecting the null hypothesis of equal rates under a distribution is presented this test is expected to reject the null hypothesis when applied to data sets containing many sequences or long sequences as the strict equality of evolutionary rates among lineages is frequently violated on the other hand the estimates of branch lengths and thus interior node depths in a tree obtained under the as sumption of a molecular clock can be useful to generate a rough idea about the relative timing of sequence divergence events e g takezaki et al of course such estimates should be used cautiously therefore now provides estimates of ml branch lengths by assuming equal evolutionary rate among line ages with this addition users can now produce linearized trees using pairwise distances as well as the ml method one can manually calibrate the molecular clock by setting the divergence time for any one node in the tree which produces divergence times for all other nodes in the tree for these divergence times calculates approximate confidence intervals from the variance of the node height computed using the curvature method e g schrago note that this procedure may underestimate the variance considerably due to the violation of the assumed clock con stancy the estimated node heights may be biased because of this reason as well so the confidence intervals presented are not appropriate for hypothesis testing usability enhancements we have also introduced many improvements to enhance mega usability first s central user interface has now become activity driven where a launch bar provides direct access to the growing suite of tools according to the type of analysis needed through the action bar fig once a user selects what they wish to compute prompts for a data file to use and the methods and data subsets to employ this wizard style layout will make easier for beginners and expert users alike in this spirit we have now added native support for the widely used fasta file format for sequence data and se quence data can now be aligned using the muscle soft ware which is very fast and accurate for data sets containing a large number of sequences edgar be cause mega now accepts user trees for heuristic searches for molecular clock tests and for ancestral sequence recon struction we have included a tree topology editor that is useful for creating trees and editing existing topologies by using drag and drop of branches operating systems and platforms in a recent survey of long term mega users we have found that both mac os x and linux platforms are used by a sub stantial number of researchers one out of four therefore we have been optimizing the use of on the mac os x and linux platforms for mac os x we have now devel oped a custom installer that bundles and the wine software so that the installation of is as simple as installing native mac applications wine is a translation layer capable of running native windows applications on posix compatible operating systems such as mac os and linux and has two major advantages over using an emulation layer i e virtualization software first by not using virtualization users are not required to purchase a license for an additional operating system second instal lation is simplified as there is no need to create and or in stall an operating system disk image as a result mac os x users are able to use as seamlessly as if they were operating it on the windows platform for which was originally developed similar provisions have been made for the use of on the linux platform in our tests we found that calculations in on mac os x and linux are 5 slower than windows this difference is rather small because all calculations via wine are executed directly on the cpu like any other downloaded academic oup com mbe article abstract 973375 fig the action bar and associated action menus this figure is available in color online and in black and white in print native application in mac os x and linux in contrast the user interface is rendered via emulation by wine which can sometimes result in a slowdown when drawing on the screen but this is becoming less noticeable with contemporary cpus that are extremely fast this en hancement is likely to make more useable for a greater number of researchers conclusion in summary now provides analysis tools for three major types ml mp and evolutionary distances of statis tical methods of molecular evolution table and fig these facilities not only make mega useful for more re searchers but also enable researchers to evaluate the robust ness of their results by comparing inferences from multiple methods under a variety of statistical models in the future we will continue to develop mega with a focus on imple menting faster algorithms for phylogenetic inference inte grating more third party tools and upgrading the computing core to use multicore and distributed computing effectively as always all versions of mega are available free of charge from http www megasoftware net materials and methods we generated two sets of nucleotide sequence data by using computer simulations in one a taxa tree representing the phylogenetic relationships among mammals was used see fig in rosenberg and kumar we simulated dna evo lution for hypothetical genes along this tree each with an independent set of evolutionary parameter values base fre quencies sequence length mean evolutionary rate and tran sition transversion rate ratio estimated from the real sequence data rosenberg and kumar for each setof evolutionary parameters different sets the branch lengths of the model tree were estimated using the corre sponding evolutionary rate sequence alignments were gener ated under the hasegawa kishino yano hky hasegawa et al model of nucleotide substitution at four different levels of rate variation among sites a 5 0 0 5 0 and 0 thatwereimplementedduringcomputersimulationsviaadis cretized gamma distribution with a very large number of cat egories this resulted in a total of 792 alignment sets we also generated dna sequence alignments containing taxa which were based on the corresponding sized trees derived from a master phylogeny of taxa see supplementary fig supplementary material online in battistuzzi et al this master phylogeny was obtained by pruning taxa and groups from the tree of 1 671 families in the timetree of life hedges and kumar such that the final tree was strictly bifurcating the resultant tree of taxa was scaled to time and spanned billion years of evolution this master topology was subsampled to produce model trees used to generate the sequence alignments containing varying number of taxa 40 500 with one set containing all taxa sequences were simulated using seqgen rambaut and grassly under the hky hasegawa et al model of nucleotide substitution with a g c content of and a transition transversion rate ratio of 1 05 which were esti mated from an alignment of small subunit rrna sequences from taxa of animals fungi plants and archaebacteria in order to make the evolutionary rate heterogeneous among tip and internal lineages rates were varied randomly by drawing them from a uniform distribution with boundaries 5 of the expected rate in each branch independently we used substitution rates of 0 025 0 050 and 0 100 per base pair per billion years to establish branch lengths in total data sets were generated in this way and the results are presented in the main text we also conducted taxa simulations in which sequences evolved four times faster 0 4 substitutions per site per billion years and found the differences between methods were very similar to those reported here results not shown a benchmark comparison of ml phylogenetic inference between and was performed for all simulated datasets by collecting the computational and phylogenetic performance of these programs including downloaded from https academic oup com mbe article abstract 973375 execution time in seconds the estimate of a gamma shape parameter ml values and topological accuracy be cause windows is s native operating system win dows executables were used for phyml version 0 and raxml version 04 all analyses were conducted on computers with identical hardware intel 2 ghz quad core processor and gb ram and operating systems bit windows enterprise edition for direct comparison each program was executed serially in a single thread of execution with one core utilized per dataset in order to generate comparable results on time and accuracy we used identical substitution models and dis crete gamma options across all programs because the fast est heuristic search in raxml mix assumes a general time reversible gtr model with four discrete gamma rate categories we used these options in all cases unless noted otherwise for all three programs analyses were conducted using the automatically generated initial trees and selecting the default options and heuristic searches starting with the initial trees were conducted with two different levels of branch rearrangements quick searches nearest neigh bor interchange nni for and phyml and mix for raxml and slow searches close neighbor interchange cni for subtree pruning regrafting spr for phyml and gtrgamma for raxml the accuracy of phylogenetic tree of n taxa was estimated from the topo logical distance dt between the inferred tree and the true topology was given by n ½dt n personal health record phr is an emerging patient centric model of health information exchange which is often outsourced to be stored at a third party such as cloud providers however there have been wide privacy concerns as personal health information could be exposed to those third party servers and to unauthorized parties to assure the patients control over access to their own phrs it is a promising method to encrypt the phrs before outsourcing yet issues such as risks of privacy exposure scalability in key management flexible access and efficient user revocation have remained the most important challenges toward achieving fine grained cryptographically enforced data access control in this paper we propose a novel patient centric framework and a suite of mechanisms for data access control to phrs stored in semitrusted servers to achieve fine grained and scalable data access control for phrs we leverage attribute based encryption abe techniques to encrypt each patient phr file different from previous works in secure data outsourcing we focus on the multiple data owner scenario and divide the users in the phr system into multiple security domains that greatly reduces the key management complexity for owners and users a high degree of patient privacy is guaranteed simultaneously by exploiting multiauthority abe our scheme also enables dynamic modification of access policies or file attributes supports efficient on demand user attribute revocation and break glass access under emergency scenarios extensive analytical and experimental results are presented which show the security scalability and efficiency of our proposed scheme index terms personal health records cloud computing data privacy fine grained access control attribute based encryption ç i ntroduction i n as recent a patient centric years personal model health of health record information phr has exchange emerged a phr service allows a patient to create manage and control her personal health data in one place through the web which has made the storage retrieval and sharing of the medical information more efficient especially each patient is promised the full control of her medical records and can share her health data with a wide range of users including healthcare providers family members or friends due to the high cost of building and maintaining specialized data centers many phr services are outsourced to or provided by third party service providers for example microsoft healthvault recently architectures of storing phrs in cloud computing have been proposed in m li is with the department of cs utah state university old main hill logan ut e mail ming li usu edu s yu is with the department of cs university of arkansas at little rock s university ave little rock ar e mail ualr edu y zheng and w lou are with the department of cs virginia tech haycock road falls church va e mail zhengyao wjlou vt edu k ren is with the department of computer science and engineering university at buffalo davis hall buffalo ny e mail kuiren buffalo edu manuscript received sept revised jan accepted feb published online mar recommended for acceptance by a nayak for information on obtaining reprints of this article please send e mail to tpds computer org and reference ieeecs log number tpds digital object identifier no tpds while it is exciting to have convenient phr services for everyone there are many security and privacy risks which could impede its wide adoption the main concern is about whether the patients could actually control the sharing of their sensitive personal health information phi especially when they are stored on a third party server which people may not fully trust on the one hand although there exist healthcare regulations such as hipaa which is recently amended to incorporate business associates cloud providers are usually not covered entities on the other hand due to the high value of the sensitive phi the third party storage servers are often the targets of various malicious behaviors which may lead to exposure of the phi as a famous incident a department of veterans affairs http www healthvault com database containing sensitive phi of million military veterans including their social security numbers and health problems was stolen by an employee who took the data home without authorization to ensure patient centric privacy control over their own phrs it is essential to have fine grained data access control mechanisms that work with semitrusted servers a feasible and promising approach would be to encrypt the data before outsourcing basically the phr owner herself should decide how to encrypt her files and to allow which set of users to obtain access to each file a phr file should only be available to the users who are given the corresponding decryption key while remain confidential to the rest of users furthermore the patient shall always retain the right to not only grant but also revoke access privileges when they feel it is necessary however the goal of patient centric privacy ß ieee published by the ieee computer society ieee transactions on parallel and distributed systems vol no january is often in conflict with scalability in a phr system the authorized users may either need to access the phr for personal use or professional purposes examples of the former are family member and friends while the latter can be medical doctors pharmacists and researchers etc we refer to the two categories of users as personal and professional users respectively the latter has potentially large scale should each owner herself be directly responsible for managing all the professional users she will easily be overwhelmed by the key management overhead in addition since those users access requests are generally unpredict able it is difficult for an owner to determine a list of them on the other hand different from the single data owner scenario considered in most of the existing works in a phr system there are multiple owners who may encrypt according to their own ways possibly using different sets of crypto graphic keys letting each user obtain keys from every owner whose phr she wants to read would limit the accessibility since patients are not always online an alternative is to employ a central authority ca to do the key management on behalf of all phr owners but this requires too much trust on a single authority i e cause the key escrow problem in this paper we endeavor to study the patient centric secure sharing of phrs stored on semitrusted servers and focus on addressing the complicated and challenging key management issues in order to protect the personal health data stored on a semitrusted server we adopt attribute based encryption abe as the main encryption primitive using abe access policies are expressed based on the attributes of users or data which enables a patient to selectively share her phr among a set of users by encrypting the file under a set of attributes without the need to know a complete list of users the complexities per encryption key generation and decryption are only linear with the number of attributes involved however to integrate abe into a large scale phr system important issues such as key management scalability dynamic policy updates and efficient on demand revocation are nontrivial to solve and remain largely open up to date to this end we make the following main contributions we propose a novel abe based framework for patient centric secure sharing of phrs in cloud computing environments under the multiowner settings to address the key management challenges we conceptually divide the users in the system into two types of domains namely public and personal domains psds in particular the majority profes sional users are managed distributively by attribute authorities in the former while each owner only needs to manage the keys of a small number of users in her personal domain in this way our framework can simultaneously handle different types of phr sharing applications requirements while incurring minimal key management overhead for both owners and users in the system in addition the framework enforces write access control handles dynamic policy updates and provides break glass access to phrs under emergence scenarios in the public domain we use multiauthority abe ma abe to improve the security and avoid key escrow problem each attribute authority aa in it governs a disjoint subset of user role attributes while none of them alone is able to control the security of the whole system we propose mechanisms for key distribution and encryption so that phr owners can specify personalized fine grained role based access policies during file encryption in the personal domain owners directly assign access privileges for personal users and encrypt a phr file under its data attributes furthermore we enhance ma abe by putting forward an efficient and on demand user attribute revocation scheme and prove its security under standard security assumptions in this way patients have full privacy control over their phrs we provide a thorough analysis of the complexity and scalability of our proposed secure phr sharing solution in terms of multiple metrics in computa tion communication storage and key management we also compare our scheme to several previous ones in complexity scalability and security further more we demonstrate the efficiency of our scheme by implementing it on a modern workstation and performing experiments simulations compared with the preliminary version of this paper there are several main additional contributions we clarify and extend our usage of ma abe in the public domain and formally show how and which types of user defined file access policies are realized we clarify the proposed revocable ma abe scheme and provide a formal security proof for it we carry out both real world experiments and simulations to evaluate the performance of the proposed solution in this paper r elated w ork this paper is mostly related to works in cryptographically enforced access control for outsourced data and attribute based encryption to realize fine grained access control the traditional public key encryption pke based schemes either incur high key management overhead or require encrypting multiple copies of a file using different users keys to improve upon the scalability of the above solutions one to many encryption methods such as abe can be used in goyal et al seminal paper on abe data are encrypted under a set of attributes so that multiple users who possess proper keys can decrypt this potentially makes encryption and key management more efficient a fundamental property of abe is preventing against user collusion in addition the encryptor is not required to know the acl abe for fine grained data access control a number of works used abe to realize fine grained access control for outsourced data especially there has been an increasing interest in applying abe to secure electronic healthcare records ehrs recently narayan et al proposed an attribute based infrastructure for ehr systems where each patient ehr files are encrypted using a broadcast variant of cp abe that allows direct revocation however the ciphertext length grows linearly with the number of unrevoked users in a li et al scalable and secure sharing of personal health records in cloud computing using attribute based variant of abe that allows delegation of access rights is proposed for encrypted ehrs ibraimi et al applied ciphertext policy abe cp abe to manage the sharing table frequently used notations of phrs and introduced the concept of social professional domains in akinyele et al investigated using abe to generate self protecting emrs which can either be stored on cloud servers or cellphones so that emr could be accessed when the health provider is offline however there are several common drawbacks of the above works first they usually assume the use of a single trusted authority ta in the system this not only may create a load bottleneck but also suffers from the key escrow problem since the ta can access all the encrypted files opening the door for potential privacy exposure in addition it is not practical to delegate all attribute management tasks to one ta including certifying all users attributes or roles and generating secret keys in fact different organizations usually form their own sub domains and become suitable authorities to define and certify different sets of attributes belonging to their sub domains i e divide and rule for example a professional association would be responsible for certifying medical specialties while a regional health provider would certify the job ranks of its staffs second there still lacks an efficient and on demand user revocation mechanism for abe with the support for dynamic policy updates changes which are essential parts of secure phr sharing finally most of the existing works do not differentiate between the personal and public domains puds which have different attribute definitions key management requirements and scalability issues our idea of conceptually dividing the system into two types of domains is similar with that in however a key difference is in a single ta is still assumed to govern the whole professional domain recently yu et al ywrl applied key policy abe to secure outsourced data in the cloud where a single data owner can encrypt her data and share with multiple authorized users by distributing keys to them that contain attribute based access privileges they also propose a method for the data owner to revoke a user efficiently by delegating the updates of affected ciphertexts and user secret keys to the cloud server since the key update operations can be aggregated over time their scheme achieves low amortized overhead however in the ywrl scheme the data owner is also a ta at the same time it would be inefficient to be applied to a phr system with multiple data owners and users because then each user would receive many keys from multiple owners even if the keys contain the same sets of attributes on the other hand chase and chow proposed a multiple authority abe cc ma abe solution in which multiple tas each governing a different subset of the system users attributes generate user secret keys collectively a user needs to obtain one part of her key from each ta this scheme prevents against collusion among at most n à tas in addition to user collusion resistance however it is not clear how to realize efficient user revocation in addition since cc ma abe embeds the access policy in users keys rather than the ciphertext a direct application of it to a phr system is nonintuitive as it is not clear how to allow data owners to specify their file access policies we give detailed overviews to the ywrl scheme and cc ma abe scheme in the supplementary material which can be found on the computer society digital library at http doi ieeecomputersociety org tpds revocable abe it is a well known challenging problem to revoke users attributes efficiently and on demand in abe traditionally this is often done by the authority broadcasting periodic key updates to unrevoked users frequently which does not achieve complete backward forward security and is less efficient recently and proposed two cp abe schemes with immediate attribute revocation capability instead of periodical revocation however they were not designed for ma abe in addition ruj et al proposed an alternative solution for the same problem in our paper using lewko and waters lw decentralized abe scheme the main advantage of their solution is each user can obtain secret keys from any subset of the tas in the system in contrast to the cc ma abe the lw abe scheme enjoys better policy expressive ness and it is extended by to support user revocation on the downside the communication overhead of key revocation is still high as it requires a data owner to transmit an updated ciphertext component to every nonrevoked user they also do not differentiate personal and public domains in this paper we bridge the above gaps by proposing a unified security framework for patient centric sharing of phrs in a multidomain multiauthority phr system with many users the framework captures application level requirements of both public and personal use of a patient phrs and distributes users trust to multiple authorities that better reflects reality we also propose a suite of access control mechanisms by uniquely combining the technical strengths of both cc ma abe and the ywrl abe scheme using our scheme patients can choose and enforce their own access policy for each phr file and can revoke a user without involving high overhead we also implement part of our solution in a prototype phr system f ramework for s ecure and p atient c entric s calable phr s haring in this section we describe our novel patient centric secure data sharing framework for cloud based phr systems the main notations are summarized in table problem definition we consider a phr system where there are multiple phr owners and phr users the owners refer to patients who ieee transactions on parallel and distributed systems vol no january have full control over their own phr data i e they can create manage and delete it there is a central server belonging to the phr service provider that stores all the owners phrs the users may come from various aspects for example a friend a caregiver or a researcher users access the phr documents through the server in order to read or write to someone phr and a user can simultaneously have access to multiple owners data a typical phr system uses standard data formats for example continuity of care ccr based on xml data structure which is widely used in representative phr systems including indivo an open source phr system adopted by boston children hospital due to the nature of xml the phr files are logically organized by their categories in a hierarchical way security model in this paper we consider the server to be semitrusted i e honest but curious as those in and that means the server will try to find out as much secret information in the stored phr files as possible but they will honestly follow the protocol in general on the other hand some users will also try to access the files beyond their privileges for example a pharmacy may want to obtain the prescriptions of patients for marketing and boosting its profits to do so they may collude with other users or even with the server in addition we assume each party in our system is preloaded with a public private key pair and entity authentication can be done by traditional challenge response protocols requirements to achieve patient centric phr sharing a core requirement is that each patient can control who are authorized to access to her own phr documents especially user controlled read write access and revocation are the two core security objectives for any electronic health record system pointed out by mandl et al in as early as the security and performance requirements are summarized as follows data confidentiality unauthorized users including the server who do not possess enough attributes satisfying the access policy or do not have proper key access privileges should be prevented from decrypting a phr document even under user collusion fine grained access control should be enforced meaning different users are authorized to read different sets of documents on demand revocation whenever a user attribute is no longer valid the user should not be able to access future phr files using that attribute this is usually called attribute revocation and the corresponding security property is forward secrecy there is also user revocation where all of a user access privileges are revoked write access control we shall prevent the unauthor ized contributors to gain write access to owners phrs while the legitimate contributors should access the server with accountability the data access policies should be flexible i e dynamic changes to the predefined policies shall be allowed especially the phrs should be accessible under emergency scenarios scalability efficiency and usability the phr system should support users from both the personal domain and public domains since the set of users from the public domain may be large in size and unpredict able the system should be highly scalable in terms of complexity in key management communication computation and storage additionally the owners efforts in managing users and keys should be minimized to enjoy usability overview of our framework the main goal of our framework is to provide secure patient centric phr access and efficient key management at the same time the key idea is to divide the system into multiple security domains namely public domains and personal domains according to the different users data access requirements the puds consist of users who make access based on their professional roles such as doctors nurses and medical researchers in practice a pud can be mapped to an independent sector in the society such as the health care government or insurance sector for each psd its users are personally associated with a data owner such as family members or close friends and they make accesses to phrs based on access rights assigned by the owner in both types of security domains we utilize abe to realize cryptographically enforced patient centric phr access especially in a pud multiauthority abe is used in which there are multiple attribute authorities aas each governing a disjoint subset of attributes role attributes are defined for puds representing the professional role or obligations of a pud user users in puds obtain their attribute based secret keys from the aas without directly interacting with the owners to control access from pud users owners are free to specify role based fine grained access policies for her phr files while do not need to know the list of authorized users when doing encryption since the puds contain the majority of users it greatly reduces the key management overhead for both the owners and users each data owner e g patient is a trusted authority of her own psd who uses a kp abe system to manage the secret keys and access rights of users in her psd since the users are personally known by the phr owner to realize patient centric access the owner is at the best position to grant user access privileges on a case by case basis for psd data attributes are defined which refer to the intrinsic properties of the phr data such as the category of a phr file for the purpose of psd access each phr file is labeled with its data attributes while the key size is only linear with the number of file categories a user can access since the number of users in a psd is often small it reduces the burden for the owner when encrypting the data for psd all that the owner needs to know is the intrinsic data properties the multidomain approach best models different user types and access requirements in a phr system the use of abe makes the encrypted phrs self protective i e they can be accessed by only authorized users even when storing on a semitrusted server and when the owner is not online in addition efficient and on demand user revocation is made possible via our abe enhancements li et al scalable and secure sharing of personal health records in cloud computing using attribute based details of the proposed framework in our framework there are multiple sds multiple owners multiple aas and multiple users in addition two abe systems are involved for each psd the ywrl revocable kp abe scheme is adopted for each pud our proposed revocable ma abe scheme described in section is used the framework is illustrated in fig we term the users having read and write access as data readers and contributors respectively system setup and key distribution the system first defines a common universe of data attributes shared by every psd such as basic profile medical history allergies and prescriptions an emergency attribute is also defined for break glass access each phr owner client application generates its corresponding public master keys the public keys can be published via user profile in an online healthcare social network hsn which could be part of the phr service e g the indivo system there are two ways for distributing secret keys first when first using the phr service a phr owner can specify the access privilege of a data reader in her psd and let her application generate and distribute corresponding key to the latter in a way resembling invitations in googledoc second a reader in psd could obtain the secret key by sending a request indicating which types of files she wants to access to the phr owner via hsn and the owner will grant her a subset of requested data types based on that the policy engine of the application automatically derives an access structure and runs keygen of kp abe to generate the user secret key that embeds her access structure in addition the data attributes can be organized in a hierarchical manner for efficient policy generation see fig when the user is granted all the file types under a category her access privilege will be represented by that category instead for the puds the system defines role attributes and a reader in a pud obtains secret key from aas which binds the user to her claimed attributes roles for example a physician in it would receive hospital a physician m d internal medicine as her attributes from the aas in practice there exist multiple aas each governing a different subset of role attributes for instance hospital staffs shall have a different aa from pharmacy specialists this is reflected by in fig ma abe is used to encrypt the data fig the proposed framework for patient centric secure and scalable phr sharing on semitrusted storage under multiowner settings and the concrete mechanism will be presented in section in addition the aas distribute write keys that permit contributors in their pud to write to some patients phr phr encryption and access the owners upload abe encrypted phr files to the server each owner phr file is encrypted both under a certain fine grained and role based access policy for users from the pud to access and under a selected set of data attributes that allows access from users in the psd only authorized users can decrypt the phr files excluding the server for improving efficiency the data attributes will include all the intermediate file types from a leaf node to the root for example in fig an allergy file attributes are fp hr medical history allergyg the data readers download phr files from the server and they can decrypt the files only if they have suitable attribute based keys the data contributors will be granted write access to someone phr if they present proper write keys user revocation here we consider revocation of a data reader or her attributes access privileges there are several possible cases revocation of one or more role attributes of a public domain user revocation of a public domain user which is equiva lent to revoking all of that user attributes these operations are done by the aa that the user belongs to where the actual computations can be delegated to the server to improve efficiency revocation of a personal domain user access privileges revocation of a personal domain user these can be initiated through the phr owner client application in a similar way policy updates a phr owner can update her sharing policy for an existing phr document by updating the attributes or access policy in the ciphertext the supported operations include add delete modify which can be done by the server on behalf of the user break glass when an emergency happens the regular access policies may no longer be applicable to handle this situation break glass access is needed to access the victim phr in our framework each owner phr access right is also delegated to an emergency department ed to prevent from abuse of break glass option the emergency staff needs to contact the ed to verify her identity and the fig the attribute hierarchy of files leaf nodes are atomic file categories while internal nodes are compound categories dark boxes are the categories that a psd data reader have access to ieee transactions on parallel and distributed systems vol no january emergency situation and obtain temporary read keys using ma abe in the public domain after the emergency is over the patient can revoke the for the puds our framework delegates the key management emergent access via the ed functions to multiple attribute authorities in order to an example here we demonstrate how our framework achieve stronger privacy guarantee for data owners the works using a concrete example suppose phr owner alice chase chow cc ma abe scheme is used where each is a patient associated with hospital a after she creates a authority governs a disjoint set of attributes distributively it phr file f labeled as phr medical history allergy emergency in fig she first encrypts it according to both is natural to associate the ciphertext of a phr document with an owner specified access policy for users from pud f data labels under the ywrl kp abe and a role based file access policy p under our revocable ma abe this policy can be decided based on recommended settings by the system or alice own preference it may look like however one technical challenge is that cc ma abe is essentially a kp abe scheme where the access policies are enforced in users secret keys and those key policies do not directly translate to document access policies from the owners points of view by our design we show that by p ðprofession physicianþ ðspecialty internal medicineþ ðorganization hospital aþ agreeing upon the formats of the key policies and the rules of specifying which attributes are required in the ciphertext the cc ma abe can actually support owner specified docu ment access policies with some degree of flexibility such as she also sends the break glass key to the ed in addition alice determines the access rights of users in her psd which can be done either online or offline for example she may approve her friend bob request to access files with labels fpersonal infog or fmedical historyg her client application will distribute a secret key with the access structure ðpersonal info medical historyþ to bob when bob wants to access another file f with labels phr me dical history medications he is able to decrypt f due to the medical history attribute for another user charlie who is a physician specializing in internal medicine in the one in fig i e it functions similar to cp abe in order to allow the owners to specify an access policy for each phr document we exploit the fact that the basic cc ma abe works in a way similar to fuzzy ibe where the threshold policies e g k out of n are supported since the threshold gate has an intrinsic symmetry from both the encryptor and the user point of views we can predefine the formats of the allowed document policies as well as those of the key policies so that an owner can enforce a file access policy through choosing which set of attributes to be included in the ciphertext hospital b in the pud he obtains his secret key from multiple aas such as the american medical association ama the american board of medical specialties abms and the american hospital association aha but he cannot decrypt f basic usage of ma abe setup in particular the aas first generate the mks and pk using setup as in cc ma abe the kth aa defines a disjoint because his role attributes do not satisfy p finally an emergency room staff dorothy who temporarily obtains the break glass key from ed can gain access to f due to the emergency attribute in that key remarks the separation of psd pud and data role attributes reflects the real world situation first in the psd a patient usually only gives personal access of his her sensitive phr to selected users such as family members and close friends rather than all the friends in the social network different psd users can be assigned different access privileges based on their relationships with the owner in this way patients can exert fine control over the access for each user in their psds second by our multi domain and multiauthority framework each public user only needs to contact aas in its own pud who collabora tively generates a secret key for the user which reduces the set of role attributes uu k which are relatively static properties of the public users these attributes are classified by their types such as profession and license status medical specialty and affiliation where each type has multiple possible values basically each aa monitors a disjoint subset of attribute types for example in the healthcare domain the ama may issue medical professional licenses like physi cian m d nurse entry level license etc the abms could certify specialties like internal medicine surgery etc and aha may define user affiliations such as hospital a and pharmacy d in order to represent the do not care option for the owners we add one wildcard attribute ã in each type of the attributes document policy generation and encryption in the basic usage we consider a special class of access policy conjunc tive normal form cnf p ðða a workload per aa since each aa handles fewer number of attributes per key issuing in addition the multiauthority abe is resilient to compromise of up to n à aas in a pud which solves the key escrow problem furthermore in our a could d þþ be á á á ðða and m m a m is the total number m a m d of m attribute þþ where a i j types for such a file access policy an owner encrypts the file as follows all the attributes in this section are role attributes framework user role verification is much easier different organizations can form their own sub domains and become aas to manage and certify different sets of attributes which is similar to divide and rule definition basic encryption rule for pud let p be in cnf form then p is required to contain at least one attribute from each type and the encryptor associates the ciphertext with all the attributes on the leaf of the access tree corresponding to p m ain d esign i ssues key policy generation and key distribution in cc in this section we address several key design issues in the format of the key policies is restricted to conjunctions secure and scalable sharing of phrs in cloud computing under the proposed framework recently lewko and waters proposed a multiauthority cp abe construction but it does not support on demand attribute revocation li et al scalable and secure sharing of personal health records in cloud computing using attribute based table sample secret keys and key policies for three public users in the health care domain among different aas i e p p ááá p n where p k could correspond to arbitrary monotonic access structure to be able to implement the cnf document policy each aa need to follow the rule of key distribution definition basic key policy generation rule for pud letp be in the above form for the secret key of useru aa contain at least one attribute from every type of attributes u k should governed by aa k and always include the wildcard associated with each type in addition the key policy p k of u issued by aa k is indices out of of attribute n k þ ááá types governed out of n by k t þ aa where k n k n k t are the in the above aa k after key distribution aau k is the set of the role aas attributes can remain u obtains offline from for most of the time a detailed key distribution example is given in table the following two properties ensure that the set of users that can decrypt a file with an access policy p is equivalent to the set of users with key access structures such that the ciphertext attribute set p leaf nodes will satisfy definition correctness given a ciphertext and its corresponding file access policy p and its leaf node set lðpþ aac a user access tree t and its leaf node set lðt þ aa u pðlðt þþ tðlðpþþ that is whenever the attributes in user secret key satisfy the file access policy the attributes in the access policy should satisfy the access structure in user secret key definition completeness conversely t ðlðpþþ pðlðt þþ theorem following the above proposed key generation and encryption rules the cnf file access policy achieves both correctness and completeness proof in the following subscript i of an attribute set denotes the subset of attributes belonging to the ith type correctness if pðlðt þþ i e lðt þ satis fies p ith policy term in m p corresponding aac i ðt þ since the to user access tree t is out of n i l i this implies t ðlðpþþ completeness it is easy to see the above is reversible due to the symmetry of set inter section tu the above theorem essentially states the cc ma abe can be used in a fashion like cp abe when the document access policy is cnf in practice the above rules need to be agreed and followed by each owner and aa it is easy to generalize the above conclusions to conjunctive forms with each term being a threshold logic formula which will not be elaborated here achieving more expressive file access policies by enhancing the key policy generation rule we can enable more expressive encryptor access policies we exploit an observation that in practice a user attributes roles belonging to different types assigned by the same aa are often correlated with respect to a primary attribute type in the following an attribute tuple refers to the set of attribute values governed by one aa each of a different type that are correlated with each other definition enhanced key policy generation rule in addition to the basic key policy generation rule the attribute tuples assigned by the same aa for different users do not intersect with each other as long as their primary attribute types are distinct definition enhanced encryption rule in addition to the basic encryption rule as long as there are multiple attributes of the same primary type corresponding nonintersected attribute tuples are included in the ciphertext attribute set this primary type based attribute association is illu strated in fig note that there is a horizontal association between two attributes belonging to different types as signed to each user for example in the first aa ama in table license status is associated with profession and profession is a primary type that means a physician possible set of license status do not intersect with that of a nurse or a pharmacist an m d license is always associated with physician while elderly nursing licence is always associated with nurse thus if the second level key policy within the ama is out of n out of n a physician would receive a key like physician or and m d or recall the assumption that each user can only hold at most one role attribute in each type nurse will be like nurse or and elderly nursing licence or meanwhile the encryptor can be made aware of this correlation so she may include the attribute set physician m d nurse elderly nursing licence during encryption due to the attribute correlation the set of users that can have access to this file can only possess one out of two sets of possible roles which means the following policy is enforced physician and m d or nurse and fig illustration of the enhanced key policy generation rule solid horizontal lines represent possible attribute associations for two users ieee transactions on parallel and distributed systems vol no january elderly nursing licence the direct consequence is it enables a disjunctive normal form dnf encryptor access policy to appear at the second level if the encryptor wants to enforce such a dnf policy under an aa she can simply include all the attributes in that policy in the ciphertext furthermore if one wants to encrypt with wildcard attributes in the policy say physician and m d or nurse and any nursing license the same idea can be used i e we can simply correlate each profession attribute with its proprietary ã attribute so we will have ã nursing license ã physician license etc in the users keys the above discussion is summarized in fig by an example encryptor policy if multiple there are multiple puds then sets of ciphertext components needs pud to j fp be pud included j g and since in reality the number of puds is usually small this method is more efficient and secure than a straightforward application of cp abe in which each organization acts as an authority that governs all types of attributes and the length of ciphertext grows linearly with the number of organizations for efficiency each file is encrypted with a randomly generated file encryption key fek which is then encrypted by abe summary in this above we present a method to enforce owner access policy during encryption which utilizes the ma abe scheme in a way like cp abe the essential idea is to define a set of key generation rules and encryption rules there are two layers in the encryptor access policy the first one is across different attribute authorities while the second is across different attributes governed by the same aa for the first layer conjunctive policy is enabled for the second either k out of n or dnf policy are supported we exploit the correlations among attribute types under an aa to enable the extended second level dnf policy next we summarize the formats of user secret key and ciphertext in our framework a user u in an owner psd has the following keys skpsd u the construction of the ywrl hfd i g abe psd scheme i where shown d i follows in supplementary material available online and aau psd is the attribute set in the key policy for u for a user u in a pud skpud u hd u fd k i g ng k i where d u and d k i are fig an example policy realizable under our framework using ma abe following the enhanced key generation and encryption rules defined according to the ma abe scheme also in supple mentary material available online and aa u k include attri butes in the key policy issued by aa k the ciphertext of file f is abe ðfekþ e fek ðfþi where e fek ðfþ is a symmetric key encryption off and e abe psd ðfekþ e pud ðfekþi where each of the ciphertexts are encrypted using the ywrl abe scheme and ma abe scheme respectively enhancing ma abe for user revocation the original cc ma abe scheme does not enable efficient and on demand user revocation to achieve this for ma abe we combine ideas from ywrl revocable kp abe its details are shown in supplementary material available online and propose an enhanced ma abe scheme in particular an authority can revoke a user or user attributes immediately by reencrypting the cipher texts and updating users secret keys while a major part of these operations can be delegated to the server which enhances efficiency the idea to revoke one attribute of a user in ma abe is as follows the aa who governs this attribute actively updates that attribute for all the affected unrevoked users to this end the following updates should be carried out the public master key components for the affected attribute the secret key component corresponding to that attribute of each unrevoked user also the server shall update all the ciphertexts containing that attribute in order to reduce the potential computational burden for the aas we adopt proxy encryption to delegate operations and to the server and use lazy revocation to reduce the overhead in particular each data attribute i is associated with a version number ver i upon each revocation event if i is an affected attribute to the server who the aa submits a rekey rk i then reencrypts the affected ciphertexts i t i and increases their version numbers the unrevoked users secret key components are updated via a similar operation using the rekey to delegate secret key updates to the server a dummy attribute needs to be additionally defined by each of n à aas which are always anded with each user key policy to prevent the server from grasping the secret keys this also maintains the resistance against up to n à aa collusion of ma abe as will be shown by our security proof using lazy revocation the affected cipher texts and user secret keys are only updated when an affected unrevoked user logs into the system next time by the form of the rekey all the updates can be aggregated from the last login to the most current one to revoke a user in ma abe one needs to find out a minimal subset of attributes such that without it the user secret key access structure aau will never be satisfied because our ma abe scheme requires conjunctive access policy across the aas it suffices to find a minimal subset by each aa k be satisfied and then compute k aa u k the without minimal which set aa u k will not all k the the enhanced aa k min will initiate the revocation operation cc ma abe scheme with k min out of immediate revocation capabilities is formally described in fig it has nine algorithms where minimalset rekeygen reenc and keyupdate are related to user revocation and policyupdate li et al scalable and secure sharing of personal health records in cloud computing using attribute based not be able to write to patients that are not treated by her therefore we combine signatures with the hash chain technique to achieve our goals suppose the time granularity is set to át and the time is divided into periods of át for each working cycle e g a day an organization generates a hash chain h h n g where hðh þ h i i n at time the organization broadcasts a signature of the chain end h n org ðh n þ to all users in its domain where ðáþ stands for an unforgeable signature scheme after that it multicasts h nài to the set of authorized contributors at each time period i note that the above method enables timely revocation of write access i e the authority simply stops issuing hashes for a contributor at the time of revocation in addition an owner could distribute a time related signature owner ðts ttþ to the entities that requests write access which can be delegated to the organization where ts is the start time of the granted time window and tt is the end of the time window for example to enable a billing clerk to add billing information to alice phr alice can specify am to pm as the granted time window at the beginning of a clinical visit note that for contributors in the psd of the owner they only need to obtain signatures from the owner herself generally during time period j an authorized contri butor w submits a ticket to the server after being authenticated to it e pk server ð owner ðtskttþk org ðh n þkh nàj krþ where e pk server is the public key encryption using the server public key and r is a nonce to prevent replay attack the server verifies if the signatures are correct using both org and owner public keys and whether hjðh nàj þ h n where hjðþ means hash j times only if both holds the contributor is granted write access and the server accepts the contents uploaded subsequently fig the enhanced ma abe scheme with on demand revocation capabilities handle dynamic policy changes our scheme should support the dynamic add modify delete of part of the document access policies or data is for handling dynamic policy changes a version number attributes by the owner for example if a patient does not is used to record and differentiate the system states pk want doctors to view her phr after she finishes a visit to a mk sk ct after each revocation operation since this hospital she can simply delete the ciphertext components scheme combines and the differences with respect corresponding to attribute doctor in her phr files adding to each of them are highlighted and modification of attributes access policies can be done by enforce write access control if there is no restrictions on write access anyone may write to someone phr using only public keys which is undesirable by granting write access we mean a data contributor should obtain proper authorization from the organization she is in and or from the targeting owner which shall be able to be verified by the server who grants rejects write access a naive way is to let each contributor obtain a signature from her organization every time she intends to write yet this requires the organizations be always online the observation is that it is desirable and practical to authorize according to time periods whose granularity can be adjusted proxy reencryption techniques however they are expensive to make the computation more efficient each owner could store the random number used in encrypting the of each document on her own computer and construct new ciphertext components corresponding to added changed attributes based on the policyupdate algorithm is shown in fig to reduce the storage cost the owner can merely keep a random seed and generate thesfor each encrypted file from such as using a pseudorandom generator thus the main computational overhead to modify add one attribute in the ciphertext is just one modular exponentiation operation for example a doctor should be permitted to write only during her office hours on the other hand the doctor must the details of the encryption algorithms are shown in supplementary material available online ieee transactions on parallel and distributed systems vol no january table comparison of security deal with break glass access for certain parts of the phr data medical staffs need to have temporary access when an emergency happens to a patient who may become unconscious and is unable to change her access policies beforehand the medical staffs will need some temporary authorization e g emergency key to decrypt those data under our framework this can be naturally achieved by letting each patient delegate her emergency key to an emergency department specifically in the beginning each owner defines an emergency attribute and builds it into the psd part of the ciphertext of each phr document that she allows break glass access she then generates an emergency key sk em using the single node key policy emergency and delegates it to the ed who keeps it in a database of patient directory upon emergency a medical staff authenticates herself to the ed requests and obtains the corresponding patient sk em and then decrypts the phr documents using sk em after the patient recovers from the emergency she can revoke the break glass access by computing a rekey rk em submit it to the ed and the server to update her sk em and ct to their newest versions respectively remarks we note that although using abe and ma abe enhances the system scalability there are some limita tions in the practicality of using them in building phr systems for example in workflow based access control scenarios the data access right could be given based on users identities rather than their attributes while abe does not handle that efficiently in those scenarios one may consider the use of attribute based broadcast encryption abbe in addition the expressibility of our encryptor access policy is somewhat limited by that of ma abe since it only supports conjunctive policy across multiple aas in practice the credentials from different organizations may be considered equally effective in that case distributed abe schemes will be needed we designate those issues as future works s ecurity a nalysis in this section we analyze the security of the proposed phr sharing solution first we show it achieves data confidenti ality i e preventing unauthorized read accesses by proving the enhanced ma abe scheme with efficient revocation to be secure under the attribute based selective set model we have the following main theorem theorem the enhanced ma abe scheme guarantees data confidentiality of the phr data against unauthorized users and the curious cloud service provider while maintaining the collusion resistance against users and up to n à aas in addition our framework achieves forward secrecy and security of write access control for detailed security analysis and proofs please refer to the online supplementary material available online of this paper we also compare the security of our scheme with several existing works in terms of confidentiality guarantee access control granularity and supported revocation method etc we choose four representative state of the art schemes to compare with the vfjps scheme based on access control list acl the bchl scheme based on hibe where each owner acts as a key distribution center the hn revocable cp abe scheme where we adapt it by assuming using one pud with a single authority and multiple psds to fit our setting the ngs scheme in which is a privacy preserving ehr system that adopts attribute based broadcast encryption to achieve data access control the rns scheme in that enhances the lewko waters ma abe with revocation capability for data access control in the cloud the results are shown in table it can be seen that our scheme achieves high privacy guarantee and on demand revocation the conjunctive policy restriction only applies for pud while in psd a user access structure can still be arbitrary monotonic formula in comparison with the rns scheme in rns the aas are independent with each other while in our scheme the aas issue user secret keys collectively and interactively also the rns scheme supports arbitrary monotonic boolean formula as file access policy however our user revocation method is more efficient in terms of communication overhead in rns upon each revocation event the data owner needs to recompute and send new ciphertext components corresponding to revoked attributes to all the remaining users in our scheme such interaction is not needed in addition our proposed frame work specifically addresses the access requirements in cloud based health record management systems by logically dividing the system into pud and psds which considers both personal and professional phr users our revocation methods for abe in both types of domains are consistent the rns scheme only applies to the pud s calability and e fficiency storage and communication costs first we evaluate the scalability and efficiency of our solution in terms of storage communication and computa tion costs we compare with previous schemes in terms of li et al scalable and secure sharing of personal health records in cloud computing using attribute based table notations for efficiency comparison in table for ngs scheme we only listed the efficiency of one of the two constructions in m and l are the maximum number of attributes in a ciphertext policy and user secret key respectively vfjps hn and rns the size of ciphertext is smaller than ngs while being comparable with hn and rns the public key size is smaller than vfjps and bchl and is comparable with that of rns while it seems larger than those of hn and ngs note that we can use the large universe constructions to dramatically reduce the public key size overall compared with non abe schemes our scheme achieves higher scalability in key management compared with existing revocable abe schemes the main advantage of our solution is small rekeying message sizes to revoke a user the maximum rekeying message size is linear with the number of attributes in that user secret key these indicate our scheme is more scalable than existing works to further show the storage and communication costs we provide a numerical analysis using typical parameter ciphertext size user secret key size public key information settings in the supplementary material available online size and revocation rekeying message size our analysis is based on the worst case where each user may potentially access part of every owners data table is a list of notations where in our scheme juj ju d j t c computation costs next we evaluate the computational cost of our scheme through combined implementation and simulation we provide the first implementation of the gpsw kp abe scheme to the best of our knowledge and also integrated the abe algorithms into a prototype phr system indivo the gpsw kp abe scheme is tested on a pc with ghz processor using the pairing based cryptography pbc library the public para meters are chosen to provide bits security level and we use a pairing friendly type a bit elliptic curve group this parameter setting has also been adopted in other related works in abe we then use the abe algorithms to encrypt randomly generated xml formatted files since real phr files are difficult to obtain and implement the user interfaces for data input and output due to space limitations the details of prototype imple mentation are reported in in the supplementary material available online fig we present benchmarks of cryptographic operations and detailed timing results for the two abe algorithms used by our framework it is shown that the decryption operation in our enhanced ma abe scheme is quite fast because it involves only jaa j jaac and t u and pud psd jaa jþjaac note u psd jþjaa that pud j since u pud includes j a the þ ju r one emergency attribute user could be both in a psd hn ngs and rns schemes do not separate and t u u psd and pud their juj ju r j t c jaa c pud j in the pud jaa pud in j however they only apply to phr access while s p addition oðt c logt c þ for the p rest c þ in the rns scheme the results are given in table the ciphertext size only accounts for the encryption of fek in our scheme for simplicity we assume there is only one pud thus the ciphertext includes m additional wildcard attributes and up to n à dummy attributes our scheme requires a secret key size that is linear with jaa u j the number of attributes of each user while in the vfjps and bchl schemes this is linear with n o since a user needs to obtain at least one key from each owner whose phr file the user wants to access for public key size we count the size of the effective information that each user needs to obtain the vfjps scheme requires each owner to publish a directed acyclic graph representing her acl along with key assignments which essentially amounts to oðn u þ per owner this puts a c pud j þ pairing operations in contrast large burden either in communication or storage cost on the the rns scheme involves c pud system for rekeying we consider revocation of one user by an owner in vfjps and bchl in vfjps revoking one user from a file may need overencryption and issuing of new public tokens for all the rest of users in the worst case the ngs scheme achieves direct user revocation using abbe which eliminates the need of rekeying and reencryption however attribute revocation is not achieved and for the revocable abbe in either the ciphertext size is linear with the number of revoked users or the public key is linear with the total number of users in the system for the rns scheme the main drawback is the large size of revocation messages to be transmitted to nonrevoked users in our scheme revocation of one user u requires revoking a minimum set of data attributes that makes her access structure unsatisfiable from table it can be seen that our scheme has much smaller secret key size compared with vfjps and bchl smaller rekeying message size than j þ pairing operations the time costs of key generation encryption and decryp tion processes are all linear with the number of attributes for attributes they all take less than from the system aspect each data owner patient uses the ywrl abe scheme for setup key generation and revocation uses both ywrl and enhanced ma abe for encryption each psd user adopts the ywrl scheme for decryption while each pud user adopts the enhanced ma abe scheme for decryption each aa uses enhanced ma abe for setup key generation and revocation next we provide estimations of computation times of each party in the system in table the values are calculated from the example parameters and benchmark results where ex ponentiation time t p times ms exp ms exp t ms pairing finally we simulate the server computation cost spent in user revocation to evaluate the system performance of user revocation especially the lazy revocation method greatly reduces the cost of revocation because it aggregates multiple ciphertext key update operations which amor tizes the computations over time the details of the ieee transactions on parallel and distributed systems vol no january table comparison of efficiency table computation also used in complexity supplementary for each material party available in the system online and ju d numerical estimation of time costs assuming c psd following parameters jaauj m jlðt þj j a minimal j ju number r j of n attributes number to c pud of aas jaa revoke a user j jaa j experimental simulation evaluation results are presented in the supplementary material available online c onclusion in this paper we have proposed a novel framework of secure sharing of personal health records in cloud comput ing considering partially trustworthy cloud servers we argue that to fully realize the patient centric concept patients shall have complete control of their own privacy through encrypting their phr files to allow fine grained access the framework addresses the unique challenges brought by multiple phr owners and users in that we greatly reduce the complexity of key management while enhance the privacy guarantees compared with previous works we utilize abe to encrypt the phr data so that patients can allow access not only by personal users but also various users from public domains with different professional roles qualifications and affiliations further more we enhance an existing ma abe scheme to handle efficient and on demand user revocation and prove its security through implementation and simulation we show that our solution is both scalable and efficient cloud computing offers utility oriented it services to users worldwide based on a pay as you go model it enables hosting of pervasive applications from consumer scientific and business domains however data centers hosting cloud applications consume huge amounts of electrical energy contributing to high operational costs and carbon footprints to the environment therefore we need green cloud computing solutions that can not only minimize operational costs but also reduce the environmental impact in this paper we define an architectural framework and principles for energy efficient cloud computing based on this architecture we present our vision open research challenges and resource provisioning and allocation algorithms for energy efficient management of cloud computing environments the proposed energy aware allocation heuristics provision data center resources to client applications in a way that improves energy efficiency of the data center while delivering the negotiated quality of service qos in particular in this paper we conduct a survey of research in energy efficient computing and propose a architectural principles for energy efficient management of clouds b energy efficient resource allocation policies and scheduling algorithms considering qos expectations and power usage characteristics of the devices and c a number of open research challenges addressing which can bring substantial benefits to both resource providers and consumers we have validated our approach by conducting a performance evaluation study using the cloudsim toolkit the results demonstrate that cloud computing model has immense potential as it offers significant cost savings and demonstrates high potential for the improvement of energy efficiency under dynamic workload scenarios elsevier b v all rights reserved introduction cloud computing can be classified as a new paradigm for the dynamic provisioning of computing services supported by state of the art data centers that usually employ virtual machine vm technologies for consolidation and environment isolation purposes cloud computing delivers an infrastructure platform and software applications as services that are made available to consumers in a pay as you go model in industry these services are referred to as infrastructure as a service iaas platform as a service paas and software as a service saas respectively many computing service providers including google microsoft yahoo and ibm are rapidly deploying data centers in various locations around the world to deliver cloud computing services a recent berkeley report stated cloud computing the long held dream of computing as a utility has the potential to corresponding author e mail addresses abe csse unimelb edu au a beloglazov jemal abawajy deakin edu au j abawajy raj csse unimelb edu au r buyya transform a large part of the it industry making software even more attractive as a service cloud offers significant benefit to it companies by relieving them from the necessity in setting up basic hardware and software infrastructures and thus enabling more focus on innovation and creating business value for their services moreover developers with innovative ideas for new internet services no longer require large capital outlays in hardware to deploy their service or human expenses to operate it to fully realize the potential of cloud computing cloud service providers have to ensure that they can be flexible in their service delivery to meet various consumer requirements while keeping the consumers isolated from the underlying infrastructure until recently high performance has been the sole concern in data center deployments and this demand has been fulfilled without paying much attention to energy consumption however an average data center consumes as much energy as households as energy costs are increasing while availability dwindles there is a need to shift the focus from optimizing data center resource management for pure performance to optimizing them for energy efficiency while maintaining high service level performance therefore cloud service providers need to adopt measures to ensure that their profit margin is not dramatically reduced due see front matter elsevier b v all rights reserved doi j future contents lists available at sciverse sciencedirect future generation computer systems journal homepage www elsevier com locate fgcs future generation computer systems 756 a beloglazov et al future generation computer systems to high energy costs the rising energy cost is a highly potential threat as it increases the total cost of ownership tco and reduces the return on investment roi of cloud infrastructures there is also increasing pressure from governments worldwide aimed at the reduction of carbon footprints which have a significant impact on the climate change for example the japanese government has established the japan data center council to address the soaring energy consumption of data centers recently leading computing service providers have formed a global consortium known as the green grid to promote energy efficiency for data centers and minimization of the environmental impact thus providers need to minimize energy consumption of cloud infrastructures while ensuring the service delivery lowering the energy usage of data centers is a challenging and complex issue because computing applications and data are growing so quickly that increasingly larger servers and disks are needed to process them fast enough within the required time period green cloud computing is envisioned to achieve not only the efficient processing and utilization of a computing infrastructure but also to minimize energy consumption this is essential for ensuring that the future growth of cloud computing is sustainable otherwise cloud computing with increasingly pervasive front end client devices interacting with back end data centers will cause an enormous escalation of the energy usage to address this problem and drive green cloud computing data center resources need to be managed in an energy efficient manner in particular cloud resources need to be allocated not only to satisfy quality of service qos requirements specified by users via service level agreements slas but also to reduce energy usage the main objective of this work is to present our vision discuss open research challenges in energy aware resource management and develop efficient policies and algorithms for virtualized data centers so that cloud computing can be a more sustainable and eco friendly mainstream technology to drive commercial scientific and technological advancements for future generations specifically our work aims to define an architectural framework and principles for energy efficient cloud computing investigate energy aware resource provisioning and allocation algorithms that provision data center resources to client applications in a way that improves the energy efficiency of a data center without violating the negotiated slas develop autonomic and energy aware mechanisms for self managing changes in the state of resources effectively and efficiently to satisfy service obligations and achieve energy efficiency develop algorithms for energy efficient mapping of vms to suitable cloud resources in addition to dynamic consolidation of vm resource partitions explore open research challenges in energy efficient resource management for virtualized cloud data centers to facilitate ad vancements of the state of the art operational cloud environ ments the rest of the paper is organized as follows section dis cusses related work followed by the green cloud architecture and principles for energy efficient cloud computing presented in sec tion the proposed energy aware resource allocation algorithms are discussed in section a performance analysis of the proposed energy aware resource provisioning and allocation algorithms is presented in section in section we discuss our vision on open research challenges in energy efficient cloud computing section concludes the paper with summary and future research directions related work one of the first works in which power management has been applied at the data center level has been done by pinheiro et al in this work the authors have proposed a technique for minimization of power consumption in a heterogeneous cluster of computing nodes serving multiple web applications the main technique applied to minimize power consumption is concentrating the workload to the minimum of physical nodes and switching idle nodes off this approach requires dealing with the power performance trade off as performance of applications can be degraded due to the workload consolidation requirements to the throughput and execution time of applications are defined in slas to ensure reliable qos the proposed algorithm periodically monitors the load of resources cpu disk storage and network interface and makes decisions on switching nodes on off to minimize the overall power consumption while providing the expected performance the actual load balancing is not handled by the system and has to be managed by the applications the algorithm runs on a master node which creates a single point of failure spf and may become a performance bottleneck in a large system in addition the authors have pointed out that the reconfiguration operations are time consuming and the algorithm adds or removes only one node at a time which may also be a reason for slow reaction in large scale environments the proposed approach can be applied to multi application mixed workload environments with fixed slas chase et al have considered the problem of energy effi cient management of homogeneous resources in internet hosting centers the main challenge is to determine the resource demand of each application at its current request load level and to allocate resources in the most efficient way to deal with this problem the authors have applied an economic framework services bid for resources in terms of volume and quality this enables negotiation of the slas according to the available budget and current qos requirements i e balancing the cost of resource usage energy cost and the benefit gained due to the usage of this resource the system maintains an active set of servers selected to serve requests for each service the network switches are dynamically reconfigured to change the active set of servers when necessary energy consumption is reduced by switching idle servers to power saving modes e g sleep hibernation the system is targeted at the web workload which leads to a noise in the load data the authors have addressed this problem by applying the statistical flip flop filter which reduces the number of unproductive reallocations and leads to a more stable and efficient control the proposed approach is suitable for multi application environments with variable slas and has created a foundation for numerous studies on power efficient resource allocation at the data center level however in contrast to the system deals only with the management of the cpu but does not consider other system resources the latency due to switching nodes on off also is not taken into account the authors have noted that the management algorithm is fast when the workload is stable but turns out to be relatively expensive during significant changes in the workload moreover likewise diverse software configurations are not handled which can be addressed by applying the virtualization technology elnozahy et al have investigated the problem of power efficient resource management in a single web application en vironment with fixed slas response time and load balancing handled by the application as in two power saving techniques are applied switching power of computing nodes on off and dynamic voltage and frequency scaling dvfs the main idea of the policy is to estimate the total cpu frequency required to pro vide the necessary response time determine the optimal number a beloglazov et al future generation computer systems of physical nodes and set the proportional frequency to all the nodes however the transition time for switching the power of a node is not considered only a single application is assumed to be run in the system and like in the load balancing is supposed to be handled by an external system the algorithm is centralized that creates an spf and reduces the scalability despite the vari able nature of the workload unlike the resource usage data are not approximated which results in potentially inefficient de cisions due to fluctuations nathuji and schwan have studied power management techniques in the context of virtualized data centers which has not been done before besides hardware scaling and vms consolidation the authors have introduced and applied a new power management technique called soft resource scaling the idea is to emulate hardware scaling by providing less resource time for a vm using the virtual machine monitor vmm schedul ing capability the authors found that a combination of hard and soft scaling may provide higher power savings due to the limited number of hardware scaling states the authors have proposed an architecture where the resource management is divided into lo cal and global policies at the local level the system leverages the guest os power management strategies however such manage ment may appear to be inefficient as the guest os may be legacy or power unaware raghavendra et al have investigated the problem of power management for a data center environment by combining and coordinating five diverse power management policies the authors explored the problem in terms of control theory and applied a feedback control loop to coordinate the controllers actions it is claimed that similarly to the approach is independent of the workload type like most of the previous works the system deals only with the cpu management the authors have pointed out an interesting outcome of the experiments the actual power savings can vary depending on the workload but the benefits from coordination are qualitatively similar for all classes of workloads however the system fails to support strict slas as well as variable slas for different applications this results in the suitability for enterprise environments but not for cloud computing providers where more comprehensive support for slas is essential kusic et al have defined the problem of power management in virtualized heterogeneous environments as a sequential optimization and addressed it using limited lookahead control llc the objective is to maximize the resource provider profit by minimizing both power consumption and sla violation kalman filter is applied to estimate the number of future requests to predict the future state of the system and perform necessary reallocations however in contrast to heuristic based approaches the proposed model requires simulation based learning for the application specific adjustments moreover due to the complexity of the model the execution time of the optimization controller reaches min even for nodes which is not suitable for large scale real world systems on the contrary our approach is heuristic based allowing the achievement of reasonable performance even for a large scale as shown in our experimental study srikantaiah et al have studied the problem of request scheduling for multi tiered web applications in virtualized hetero geneous systems to minimize energy consumption while meet ing performance requirements the authors have investigated the effect of performance degradation due to high utilization of dif ferent resources when the workload is consolidated they have found that the energy consumption per transaction results in a u shaped curve and it is possible to determine the optimal uti lization point to handle the optimization over multiple resources the authors have proposed a heuristic for the multidimensional bin packing problem as an algorithm for the workload consolida tion however the proposed approach is workload type and ap plication dependent whereas our algorithms are independent of the workload type and thus are suitable for a generic cloud en vironment cardosa et al have proposed an approach for the problem of power efficient allocation of vms in virtualized hetero geneous computing environments they have leveraged the min max and shares parameters of vmm which represent minimum maximum and proportion of the cpu allocated to vms sharing the same resource similarly to the approach suits only enterprise environments as it does not support strict slas and requires the knowledge of application priorities to define the shares parame ter other limitations are that the allocation of vms is not adapted at run time the allocation is static and no other resources except for the cpu are considered during the vm reallocation verma et al have formulated the problem of power aware dynamic placement of applications in virtualized heterogeneous systems as continuous optimization at each time frame the placement of vms is optimized to minimize power consumption and maximize performance like in the authors have applied a heuristic for the bin packing problem with variable bin sizes and costs similarly to live migration of vms is used to achieve a new placement at each time frame the proposed algorithms on the contrary to our approach do not handle strict sla requirements slas can be violated due to variability of the workload gandhi et al have considered the problem of allocating an available power budget among servers in a virtualized heterogeneous server farm while minimizing the mean response time to investigate the effect of different factors on mean response time a queuing theoretic model has been introduced which allows the prediction of the mean response time as a function of the power to frequency relationship arrival rate peak power budget etc the model is used to determine the optimal power allocation for every configuration of the above factors in contrast to the discussed studies we propose efficient heuris tics for dynamic adaption of vm allocation at run time accord ing to the current utilization of resources applying live migration switching idle nodes to the sleep mode and thus minimizing en ergy consumption the proposed approach can effectively handle strict slas heterogeneous infrastructure and heterogeneous vms the algorithms do not depend on a particular type of workload and do not require any knowledge about applications running in vms another resource that has been recognized by the research community as a significant energy consumer is network infrastruc ture gupta et al have suggested putting network interfaces links switches and routers into sleep modes when they are idle in order to save the energy consumed by the internet backbone and consumers based on the foundation laid by gupta et al a number of research works have been done on the energy efficient traffic routing by isps and applying sleep modes and performance scaling of network devices chiaraviglio and matta have proposed cooperation between isps and content providers that allows the achievement of an efficient simultaneous alloca tion of compute resources and network paths that minimizes en ergy consumption under performance constraints koseoglu and karasan have applied a similar approach of joint allocation of computational resources and network paths to grid environments based on the optical burst switching technology with the objective of minimizing job completion times tomas et al have inves tigated the problem of scheduling message passing interface mpi jobs in grids considering network data transfers satisfying the qos requirements dodonov and de mello have proposed an approach to scheduling distributed applications in grids based on predictions of communication events they have proposed the migration of communicating processes if the migration cost is lower than the cost of the predicted communication with the objective of minimizing the total execution time they have shown that the approach can be effectively applied in grids however it is not vi able for virtualized data centers as the vm migration cost is higher a beloglazov et al future generation computer systems than the process migration cost gyarmati and trinh have in vestigated the energy consumption implications of data centers network architectures however optimization of network archi tectures can be applied only at the data center design time and cannot be applied dynamically guo et al have proposed and implemented a virtual cluster management system that allocates the resources in a way satisfying bandwidth guarantees the allo cation is determined by a heuristic that minimizes the total band width utilized the vm allocation is adapted when some of the vms are de allocated however the vm allocation is not dynamically adapted depending on the current network load moreover the ap proach does not explicitly minimize energy consumption by the network rodero merino et al have proposed an additional layer of infrastructure management in clouds with the ability to automatically deploy services with multi vm configurations the proposed infrastructure management system applies the specified rules for scaling vm configurations in and out however the system does not optimize the network communication between vms cal heiros et al have investigated the problem of mapping vms on physical nodes optimizing network communication between vms however the problem has not been explored in the context of the optimization of energy consumption recently a number of research works have been done on the thermal efficient resource management in data centers the studies have shown that the software driven thermal man agement and temperature aware workload placement bring additional energy savings however the problem of thermal man agement in the context of virtualized data centers has not been investigated moreover to the best of our knowledge there is no study on a comprehensive approach that combines optimization of vm placement according to the current utilization of resources with thermal and network optimizations for virtualized data cen ters therefore the exploration of such an approach is timely and crucial considering the proliferation of cloud computing environ ments green cloud architecture architectural framework clouds aim to drive the design of the next generation data centers by architecting them as networks of virtual services hardware database user interface application logic so that users can access and deploy applications from anywhere in the world on demand at competitive costs depending on their qos requirements fig shows the high level architecture for supporting energy efficient service allocation in a green cloud computing infrastructure there are basically four main entities involved consumers brokers cloud consumers or their brokers submit service requests from anywhere in the world to the cloud it is important to notice that there can be a difference between cloud consumers and users of deployed services for instance a consumer can be a company deploying a web application which presents varying workload according to the number of users accessing it green service allocator acts as the interface between the cloud infrastructure and consumers it requires the interaction of the following components to support the energy efficient resource management a green negotiator negotiates with the consumers brokers to finalize the slas with specified prices and penalties for violations of the slas between the cloud provider and consumer depending on the consumer qos requirements and energy saving schemes in case of web applications for instance a qos metric can be of requests being served in less than fig the high level system architecture b service analyzer interprets and analyzes the service re quirements of a submitted request before accepting it hence it needs the latest load and energy information from vm manager and energy monitor respectively c consumer profiler gathers specific characteristics of con sumers so that important consumers can be granted special privileges and prioritized over other consumers d pricing decides how service requests are charged to man age the supply and demand of computing resources and fa cilitate in prioritizing service allocations effectively e energy monitor observes energy consumption caused by vms and physical machines and provides this information to the vm manager to make energy efficient resource allo cation decisions f service scheduler assigns requests to vms and determines resource entitlements for the allocated vms if the auto scaling functionality has been requested by a customer it also decides when vms are to be added or removed to meet the demand g vm manager keeps track of the availability of vms and their resource usage it is in charge of provisioning new vms as well as reallocating vms across physical machines to adapt the placement h accounting monitors the actual usage of resources by vms and accounts for the resource usage costs historical data of the resource usage can be used to improve resource alloca tion decisions vms multiple vms can be dynamically started and stopped on a single physical machine according to incoming requests hence providing the flexibility of configuring various partitions of re sources on the same physical machine to different requirements of service requests multiple vms can concurrently run applica tions based on different operating system environments on a single physical machine by dynamically migrating vms across physical machines workloads can be consolidated and unused resources can be switched to a low power mode turned off or configured to operate at low performance levels e g using dvfs in order to save energy physical machines the underlying physical computing servers provide the hardware infrastructure for creating virtualized resources to meet service demands power model power consumption by computing nodes in data centers is mostly determined by the cpu memory disk storage and network interfaces in comparison to other system resources the cpu consumes the main part of energy and hence in this work we focus on managing its power consumption and efficient usage moreover the cpu utilization is typically proportional to the overall system load recent studies have shown that the application of dvfs on the cpu results in almost linear power to frequency a beloglazov et al future generation computer systems relationship for a server the reason lies in the limited number of states that can be set to the frequency and voltage of the cpu and the fact that dvfs is not applied to other system components apart from the cpu moreover these studies have shown that on average an idle server consumes approximately of the power consumed by the server running at the full cpu speed this fact justifies the technique of switching idle servers to the sleep mode to reduce the total power consumption therefore in this work we use the power model defined in p u k p max bfd algorithm bins where that is shown to use opt is the number of no more than bins given by the opt optimal solution in our modification the modified best fit decreasing mbfd algorithms we sort all vms in decreasing order of their current cpu utilizations and allocate each vm to a host that provides the least increase of power consumption due to this allocation this allows leveraging the heterogeneity of resources by choosing the most power efficient nodes first the pseudo code for the algorithm is presented in algorithm the complexity of the k p max u allocation part of the algorithm is n m where n is the number of vms that have to be allocated and m is the number of hosts p max is the maximum power consumed when the server is fully utilized k is the fraction of power consumed by the idle server algorithm modified best fit decreasing mbfd i e and u is the cpu utilization for our experiments p max is set to w which is a usual value for modern servers for exam ple according to the specpower benchmark for the fourth quar ter of the average power consumption at utilization for servers consuming less than w was approximately w the utilization of the cpu may change over time due to the workload variability thus the cpu utilization is a function of time and is represented asu t therefore the total energy consumption by a physical node e can be defined as an integral of the power consumption function over a period of time as shown in e input hostlist vmlist output allocation of vms vmlist sortdecreasingutilization foreach vm in vmlist do minpower max allocatedhost null foreach host in hostlist do if host has enough resource for vm then power estimatepower host vm if power minpower then allocatedhost host t t minpower power p u t dt if allocatedhost null then energy aware allocation of data center resources recent developments in virtualization have resulted in its proliferation across data centers by supporting the movement of vms between physical nodes it enables dynamic migration of vms according to the performance requirements when vms do not use all the provided resources they can be logically resized and consolidated to the minimum number of physical nodes while idle nodes can be switched to the sleep mode to eliminate the idle power consumption and reduce the total energy consumption by the data center currently resource allocation in a cloud data center aims to provide high performance while meeting slas without focusing on allocating vms to minimize energy consumption to explore both performance and energy efficiency three crucial issues must be addressed first excessive power cycling of a server could reduce its reliability second turning resources off in a dynamic environment is risky from the qos perspective due to the variability of the workload and aggressive consolidation some vms may not obtain required resources under peak load and fail to meet the desired qos third ensuring slas brings challenges to accurate application performance management in virtualized environments all these issues require effective consolidation policies that can minimize energy consumption without compromising the user specified qos requirements vm placement the problem of vm allocation can be divided in two the first part is the admission of new requests for vm provisioning and placing the vms on hosts whereas the second part is the optimization of the current vm allocation the first part can be seen as a bin packing problem with variable bin sizes and prices to solve it we apply a modification of the best fit decreasing the specpower benchmark results for the fourth quarter of http www spec org results allocate vm to allocatedhost return allocation vm selection the optimization of the current vm allocation is carried out in two steps at the first step we select vms that need to be migrated at the second step the chosen vms are placed on the hosts using the mbfd algorithm to determine when and which vms should be migrated we introduce three double threshold vm selection policies the basic idea is to set upper and lower utilization thresholds for hosts and keep the total utilization of the cpu by all the vms allocated to the host between these thresholds if the cpu utilization of a host falls below the lower threshold all vms have to be migrated from this host and the host has to be switched to the sleep mode in order to eliminate the idle power consumption if the utilization exceeds the upper threshold some vms have to be migrated from the host to reduce the utilization the aim is to preserve free resources in order to prevent sla violations due to the consolidation in cases when the utilization by vms increases the difference between the old and new placements forms a set of vms that have to be reallocated the new placement is achieved using live migration of vms in the following sections we discuss the proposed vm selection policies the minimization of migrations policy the minimization of migrations mm policy selects the mini mum number of vms needed to migrate from a host to lower the cpu utilization below the upper utilization threshold if the upper threshold is violated let v j be a set of vms currently allocated to the host j then set r p v j p v j is the power set defined in of v j the mm policy finds a r s s p v j u j v t u v s s min u a if u j t u v j if u j t l otherwise a beloglazov et al future generation computer systems 768 where t u is the upper utilization threshold t l is the lower utilization threshold u j the random choice policy is the current cpu utilization of the host j vm and v u a v is the fraction of the cpu utilization allocated to the the pseudo code for the mm algorithm for the over utilization case is presented in algorithm the algorithm sorts the list of vms in the decreasing order of the cpu utilization then it repeatedly the random choice rc policy relies on a random selection of a number of vms needed to decrease the cpu utilization by a host below the upper utilization threshold according to a uniformly distributed discrete random variable x whose values index subsets of v j looks through the list of vms and finds a vm that is the best to migrate from the host the best vm is the one that satisfies two conditions first the vm should have the utilization higher than the difference between the host overall utilization and the upper utilization threshold second if the vm is migrated from the host the difference between the upper threshold and the new utilization is the minimum across the values provided by all the vms if there is no such a vm the algorithm selects the vm with the highest utilization removes it from the list of vms and proceeds to a new iteration the algorithm stops when the new utilization of the host is below the upper utilization threshold the complexity of the algorithm is proportional to the product of the number of over utilized hosts and the number of vms allocated to these hosts algorithm minimization of migrations mm the policy selects a set r p v j as shown in r s s p v j u j v t u v s x u a d u p v j if u j t u v j if u j t l otherwise where x is a uniformly distributed discrete random variable used to select a subset of v j the results of a simulation based evaluation of the proposed algorithms in terms of power consumption sla violations and the number of vm migrations are presented in section input hostlist output migrationlist foreach h in hostlist do performance analysis vmlist h getvmlist vmlist sortdecreasingutilization hutil h getutil bestfitutil max in this section we discuss a performance analysis of the energy aware allocation heuristics presented in section in our experiments we calculate the time needed to perform a live while hutil do migration of a vm as the size of its memory divided by the available foreach vm in vmlist do network bandwidth this is justified as to enable live migration if vm getutil hutil then the images and data of vms must be stored on a network attached t vm getutil hutil storage nas and therefore copying the vm storage is not if t bestfitutil then required live migration creates an extra cpu load however it has bestfitutil t been shown that the performance overhead is low moreover bestfitvm vm with advancements of the virtualization technology the efficiency else of vm migration is going to be improved for the simulations if bestfitutil max then the utilization of the cpu by a vm is generated as a uniformly bestfitvm vm distributed random variable this is appropriate due to unknown break types of applications running on vms and as it is not possible to hutil hutil bestfitvm getutil build the exact model of such a mixed workload this approach has migrationlist add bestfitvm been justified by verma et al vmlist remove bestfitvm if hutil then migrationlist add h getvmlist performance metrics vmlist remove h getvmlist return migrationlist in order to compare the efficiency of the algorithms we use several metrics to evaluate their performance the first metric is the total energy consumption by the physical resources of the highest potential growth policy a data center caused by the application workloads the second when the upper threshold is violated the highest potential growth hpg policy migrates vms that have the lowest usage performance metric is called the sla violation percentage or simply the sla violations which is defined as the percentage of of the cpu relatively to the cpu capacity defined by the vm sla violation events relatively to the total number of the processed parameters in order to minimize the potential increase of the host time frames we define that an sla violation occurs when a given utilization and prevent an sla violation as formalized in vm cannot get the amount of million instructions per second mips that are requested this can happen in cases when vms sharing the same host require a cpu performance that cannot be provided due to the consolidation this metric shows the level r by which the qos requirements negotiated between the resource provider and consumers are violated due to the energy aware resource management it is assumed that the provider pays a penalty to the client in case of an sla violation the third metric is the number of vm migrations initiated by the vm manager during the adaptation of the vm placement the last performance metric is the average sla violation which represents the average cpu performance that has not been allocated to an application when requested resulting in performance degradation s s p v j u j v t u v s v s u a u a v u r if u j where u r min t u v j v if u j t l otherwise v is the fraction of the cpu capacity initially requested for the vmv and defined as the vm parameter we do not provide the pseudo code for the hpg algorithm as it is similar to the mm algorithm presented earlier a beloglazov et al future generation computer systems 768 experiment setup as the targeted system is a generic cloud computing envi ronment i e iaas it is essential to evaluate it on a large scale virtualized data center infrastructure however it is extremely difficult to conduct repeatable large scale experiments on a real infrastructure which is required to evaluate and compare the pro posed resource management algorithms therefore to ensure the repeatability of experiments simulations have been chosen as a way to evaluate the performance of the proposed heuristics the cloudsim toolkit has been chosen as a simulation platform as it is a modern simulation framework aimed at cloud computing environments in contrast to alternative simulation toolkits e g simgrid gangsim it supports modeling of on demand virtualiza tion enabled resource and application management it has been extended to enable energy aware simulations as the core frame work does not provide this capability apart from the energy con sumption modeling and accounting the ability to simulate service applications with workloads variable over time has been incorpo rated we have simulated a data center comprising heteroge neous physical nodes each node is modeled to have one cpu core with the performance equivalent to or mips gb of ram and tb of storage power consumption by the hosts is de fined according to the model described in section according to this model a host consumes from w with cpu utiliza tion up to w with cpu utilization each vm requires one cpu core with or mips mb of ram and gb of storage the users submit requests for provisioning of heterogeneous vms that fill the full capacity of the simulated data center each vm runs a web application or any kind of application with variable workload which is modeled to generate the utiliza tion of cpu according to a uniformly distributed random variable the application runs for mi that is equal to min of the execution on mips cpu with utilization initially the vms are allocated according to the requested characteristics assuming cpu utilization each experiment has been run times simulation results for the benchmark experimental results we have used the non power aware npa policy this policy does not apply any power aware optimizations and implies that all hosts run at cpu utilization and consume maximum power all the time the second policy applies dvfs but does not perform any adaptation of the vm allocation at run time for the described simulation setup the npa policy leads to the total energy consumption of kwh with confidence interval ci whereas dvfs decreases this value to kwh with ci another benchmark vm selection policy is a vm migration aware policy called single threshold st it is based on the idea of setting the upper utilization threshold for hosts and placing vms while keeping the total utilization of cpu below this threshold at each time frame all vms are reallocated using the mbfd algorithm with additional condition of keeping the upper utilization threshold not violated to evaluate the st policy we have conducted several experiments with different values of the utilization threshold the simulation results are presented in fig the results show that energy consumption can be significantly reduced relatively to the npa and dvfs policies by and respectively with of sla violations the results show that with the growth of the utilization threshold energy consumption decreases whereas the percentage of sla violations increases this is due to the fact that a higher utilization threshold allows more aggressive consolidation of vms by the cost of the increased risk of sla violations fig the energy consumption and sla violations by the st policy fig the mean energy consumption by the mm policy for different values of the utilization thresholds to evaluate the double threshold policies it is necessary to determine the best values for the thresholds in terms of the energy consumption and qos delivered we have chosen the mm policy to conduct the analysis of the utilization thresholds we have simulated the mm policy varying the absolute values of the lower and upper thresholds as well as the interval between them first of all it is important to determine which threshold has higher influence on the energy consumption we have performed a regression analysis of the relationship between the energy consumption and values of the utilization thresholds to achieve the normality of the residuals we have applied the log log x transformation the ryan joiner normality test has resulted in the p value the regression analysis has shown the adjusted is the coefficient for the lower threshold is and the coefficient for the upper threshold is the value of the adjusted shows that the obtained regression represents the relationship with a high precision the values of the coefficients show that the lower threshold has approximately two times higher influence on the energy consumption than the upper threshold this can be explained by the fact that an increase of the lower threshold eliminates the low utilization of the resources leading to higher energy savings however possibly increasing the number of vm migrations and sla violations the results showing the mean energy consumption achieved using the mm policy for different values of the lower utilization threshold and the interval between the thresholds are presented in fig the graph shows that an increase of the lower utilization threshold leads to decreased energy consumption however the low level of energy consumption can be achieved with different intervals between the thresholds therefore to determine the best interval we have to consider another factor the level of sla violations a beloglazov et al future generation computer systems 768 a the mean sla violations by the mm policy b the energy consumption to sla violations by the mm policy fig the energy consumption and sla violation analysis for the mm policy a the energy consumption b the sla violations c the number of vm migrations d the average sla violation fig the comparison of the double threshold algorithms the results presented in fig a show that an increase of the lower utilization threshold leads to an increase of the sla violations level for all the intervals therefore it is necessary to determine the interval that will provide the optimal trade off between energy consumption and sla violations fig b shows that for the considered simulation setup the best combination of energy consumption and sla violations is achieved with the interval between the thresholds we have used the interval between the thresholds of with different values of the lower utilization threshold to compare the mm policy with the hpg and rc policies the graphs with fitted lines of the energy consumption sla violations number of vm migration and average sla violation achieved by the policies with the interval between the thresholds are presented in fig to compare the policies by each of these factors we have performed a two way anova test to meet the assumptions of the model we have transformed the energy consumption using log log x transformation and the sla violations using log x after the transformation the residuals are normally distributed withp value the variances of the distributions are approximately equal according to the plot of the standardized residuals against the fitted values according to the results of the two way anova test the data are consistent with the null hypothesis as p value therefore we can conclude that all the policies produce results with a not statistically significant difference for the comparison of sla violations for the double threshold policies we get p value that allow us to conclude that the data are not consistent with the null hypothesis using pair wise comparisons we have found that the difference between the results obtained using the rc and mm policies is not statistically significant p value however the p value for the difference between the mm and hpg policies is and for the difference between rc and hpg the p value is the difference of means for the mm and hpg policies is with ci 068 the difference of means for the rc and hpg policies is with ci 083 this means that the mm and rc policies lead to significantly less sla violations than the hpg policy a beloglazov et al future generation computer systems 768 table the final experiment results algorithm energy kwh sla violation vm migrations average sla npa 036 dvfs 390 st 81 st 88 mm 480 mm 262 3194 mm 3054 75 a the energy consumption b the sla violations c the number of vm migrations d the average sla violation fig the final experiment results for the analysis of the number of vm migrations the extreme point for the lower utilization threshold at and the interval has been removed the reason is that the usage of these utilization thresholds leads to the same behavior of all the evaluated policies as at of the upper utilization threshold there is no space to handle peak loads the two way anova test for the comparison of the number of vm migrations gives a statistically significant result with p value according to the pair wise comparisons the usage of the rc policy results in the mean of fewer vm migrations than the hpg policy with ci the usage of the mm policy results in the mean of fewer vm migrations than the rc policy with ci from these results we can conclude that the mm policy leads to a significant reduction of the number of vm migrations compared to the other double threshold policies a two way anova test for the comparison of the average sla violation gives a not statistically significant result with p value this means that we accept the null hypothesis the difference in results that can be obtained using the mm hpg and rc policies is negligible in summary all the evaluated policies cause approximately the same energy consumption and average value of sla violation however the mm and rc policies lead to significantly less sla violations than the hpg policy moreover the usage of the mm policy results in a significantly reduced number of vm migrations in comparison to the other policies therefore we have chosen the mm policy as the best among the double threshold policies we have chosen three representative threshold pairs for the mm policy and two values of the threshold for the st policy to conduct a final comparison the mean values of the energy consumption sla violations number of migration and average sla violation along with ci for the npa dvfs st and mm policies are presented in table and in fig the results show that the dynamic reallocation of vms accord ing to the current cpu utilization can bring higher energy savings in comparison to static resource allocation policies according to the t test for the simulated scenario the mm policy leads to kwh less energy consumption on average than st with approximately the same level of sla violations with ci moreover the mm policy leads to more than times less vm migrations than st thet test for the comparison of means of the energy consumption caused by the mm and dvfs policies show that the mm policy on average leads to kwh less energy a beloglazov et al future generation computer systems 768 consumption than dvfs with ci in comparison to the npa policy for the simulated scenario mm on average leads to kwh less energy consumption than npa with ci from the presented results we can conclude that the usage of the mm policy provides the best energy savings with the least sla violations and number of vm migrations among the evaluated policies for the simulated scenario moreover the results show the flexibility of the mm algorithm as the thresholds can be adjusted according to the slas requirements of the sla violations allows the achievement of the energy consumption of 476 kwh 480 however if the slas are relaxed for example allowing performance degradation the energy consumption is further reduced to kwh 134 150 according to our model the service provider pays a penalty to the client in cases of sla violations the actual penalty amount depends on the contract terms negotiated between the provider and the client nevertheless the energy savings of achieved using the mm policy in comparison to the non migration aware dvfs policy justify the penalty caused by sla violations moreover the performance requirements set as the slas for our experiments imply the performance delivery in a real world environment the provider can define in the contract terms the allowed mean performance degradation of and thus avoiding the penalty if the performance degradation does not exceed the specified value transitions to the sleep mode we have collected the data on the number of times the hosts have been switched to the sleep mode caused by the proposed mm algorithm during the simulations the distribution of the number of transitions obtained from simulation runs is depicted in fig according to the data the experiment setup described in section has led to the mean of transitions to the sleep mode with ci the distribution of the time duration until a host is switched to the sleep mode is shown in fig the data show that the mean time before a host is switched to the sleep mode is with ci in other words for our experiment setup and the workload generated on average a host switches to the sleep mode after approximately of being active this value is effective for real world systems as modern servers allow low latency transitions to the sleep mode consuming low power meisner et al have shown that a typical blade server consuming w in the fully utilized state consumes approximately w in the sleep mode while the transition delay is ms open challenges the virtualization technology which cloud computing environ ments heavily rely on provides the ability to transfer vms be tween physical nodes using live or offline migration this enables the technique of dynamic consolidation of vms to the minimum of physical nodes according to the current resource requirements as a result the idle nodes can be switched off or put to a power saving mode e g sleep hibernate to reduce the total energy consump tion by the data center in this paper we have proposed algorithms that leverage this technique showing its efficiency however there are many open challenges that have to be addressed in order to take advantage of the full potential of energy conservation in cloud data centers in this section we identify and discuss key open research challenges that should be addressed at the level of managing data center resources fig the distribution of the number of host transition to the sleep mode fig the distribution of the time until a host is switched to the sleep mode optimization of vm placement according to the utilization of multiple system resources the cpu consumes the major part of power in a server followed by the next largest power consumer memory however modern multi core processors are much more power efficient than previ ous generations whereas the memory technology does not show any significant improvements in energy efficiency the increased number of cores in servers combined with the rapid adoption of virtualization technologies creates the ever growing demand to memory and makes memory one of the most important compo nents of focus in the power and energy usage optimization the same applies to network and disk storage facilities in modern data centers these facts unveil that it is essential to take into ac count the usage of multiple system resources in the energy aware resource management we have already investigated the problem of energy aware dynamic consolidation of vms according to the current cpu utilization however to allow a better vm placement optimization the vms should be reallocated according to the current utilization of multiple system resources including the cpu ram and network bandwidth as shown in fig disk storage is usually centralized e g nas to enable live migration of vms and therefore requires specific energy efficient management techniques the problem arises when it comes to providing strict slas ensuring no performance degradation which is required for a cloud data center a generic cloud computing environment iaas is built to serve multiple applications for multiple users creating mixed workloads and complicating the workload characterization how to predict performance peaks how to determine which vms when and where should be migrated to prevent the performance fig the network optimization degradation considering multiple system resources how to develop fast and effective algorithms for the vm placement optimization across multiple resources for large scale systems these are the challenges that have to be addressed to provide a viable solution for modern cloud computing environments optimization of virtual network topologies in virtualized data centers vms often communicate with each other establishing virtual network topologies however due to vm migrations or a non optimized allocation the communicat ing vms may end up hosted on logically distant physical nodes providing costly data transfers between each other if the com municating vms are allocated to the hosts in different racks or enclosures the network communication may involve additional network switches and links which consume significant amount of energy there have been recent research efforts on the opti mization of the allocation of communicating applications to mini mize the network data transfer overhead however these works have not directly addressed the problem of energy con sumption by the network infrastructure moreover the proposed approaches do not optimize the placement of vms at run time de pending on the current network load which is effective for vari able communication patterns and should be applied to virtualized data centers to eliminate data transfer overheads and minimize energy consumption it is necessary to monitor the communication between vms and dynamically adapt their placement depending on the communication between them as shown in fig to provide effective reallocations we propose the application of the proposed energy models of network devices to the development of vm allocation adaption strategies for cloud data fig the optimization over multiple resources a beloglazov et al future generation computer systems 768 fig the temperature optimization centers as migrations consume additional energy and have a negative impact on the performance before initiating a migration the reallocation controller has to ensure that the cost of migration does not exceed the benefit the energy aware optimization of the network communication between vms in cloud data centers is essential to investigate as modern service applications are often deployed in multi vm configurations the optimal vm placement and its dynamic adaptation can substantially reduce the data transfer overheads and thus energy consumed by the network infrastructure optimization of thermal states and cooling system operation a significant part of electrical energy consumed by computing resources is transformed into heat high temperature leads to a number of problems such as reduced system reliability and availability as well as decreased lifetime of devices in order to keep the system components within their safe operating temperature and prevent failures and crashes the emitted heat must be dissipated the cooling problem becomes extremely important for modern blade and unit rack servers which lead to a high density of computing resources and complicate the heat dissipation for example for a data center with standard computing racks each consuming kw the initial cost of purchasing and installing the infrastructure is million whereas the annual costs for cooling is around million therefore apart from hardware improvements it is essential to optimize the cooling system operation from the software side there has been work on modeling the thermal topology of a data center that can be applied to achieve a more efficient workload placement new challenges include how and when to reallocate vms to minimize the power drawn by the cooling system while preserving a safe temperature of the resources and minimizing the migration overhead and performance degradation we propose the investigation and development of new thermal management algorithms that monitor the thermal state of physical nodes and reallocate the workload vms from overheated nodes as shown in fig in this case the cooling systems of the overheated nodes can be slowed down allowing natural power dissipation the temperature variations caused by different workloads can be leveraged to swap vms at an appropriate time to control the temperature and energy consumption in addition hardware level power management techniques such as dvfs can lower the temperature when it surpasses the thermal threshold to meet the requirements of cloud data centers this problem should be explored for a case when multiple diverse applications with different qos requirements are executing in the system simultaneously a beloglazov et al future generation computer systems 768 efficient consolidation of vms for managing heterogeneous workloads cloud infrastructure services provide users with the ability to provision virtual machines and execute any kinds of applications on them this leads to the fact that different types of applications e g enterprise scientific and social network applications can be allocated to a single physical computing node however it is not obvious how these applications can influence each other as they can be data network or compute intensive thus creating variable or static load on the resources the problem is to determine which kinds of applications can be allocated to a single host to provide the most efficient usage of the resources moreover data center resources may deliver different levels of performance to their clients hence performance aware resource selection plays an important role in cloud computing current approaches to energy efficient consolidation of vms in data centers do not investigate the problem of combining differ ent workload types these approaches usually focus on one par ticular workload type or do not consider different kinds of appli cations assuming a uniform workload in contrast to the previous works we propose an intelligent consolidation of vms with differ ent workload types for example a compute intensive scientific application can be effectively combined with a web application file server as the former mostly relies on cpu performance whereas the latter utilizes disk storage and network bandwidth it is necessary to investigate which kinds of applications can be effectively combined and which parameters influence the effi ciency additionally cloud applications can present varying work loads it is therefore essential to carry out a study of cloud ser vices and their workloads in order to identify common behav iors patterns and explore load forecasting approaches that can potentially lead to more efficient resource provisioning and conse quently improved energy efficiency another research problem is to develop methods for the automatic determination of compatible applications and application of vm migration to adapt the place ment when the application behavior changes in this context we propose the investigation of sample ap plications and correlations between workloads and attempt to build performance models that can help exploring the trade offs between qos and energy savings this knowledge can be used to develop workload aware resource allocation algorithms which can be incorporated into energy efficient resource management strategies in data centers to achieve more optimal allocation of re sources for resource providers the optimal allocation of vms will result in higher utilization of resources and therefore reduced op erational costs end users will benefit from decreased prices for the resource usage knowledge of the efficient combination of different types of workloads will advance resource management strategies in energy aware computing environments where consolidation of vms is one of the most productive energy saving techniques a holistic approach to energy aware resource management although all of the optimization techniques discussed above are important some of them are contradictory for example the technique discussed in section is aimed at the consolidation of vms and increases the amount of physical resources in cases of workload peaks on the other hand the technique discussed in section de consolidates vms in cases of node overheating incorporating additional constraints therefore the problem of combining different optimization techniques presents a significant research challenge creating a multi objective optimization prob lem one of the limitations of current practices is that the optimiza tion algorithm is slow due to complex computations or requires machine learning that is not suitable for a large scale data cen ter environment that has to be able to quickly respond to changes in the workload usually the optimization controller is central ized that creates a single point of failure and limits the scalability the proposed holistic approach can be highly resource intensive as it will incorporate a multi objective optimization therefore to provide scalability and fault tolerance the crucial requirements to the optimization algorithm are decentralization and distributed nature this implies that reallocation controllers are distributed over multiple physical nodes in a data center and do not have the complete view of the system at any point of time the research challenges in this case are how to efficiently propagate the data between controllers how to effectively combine different opti mization techniques how to ensure that the solution is close to the global optimum these questions have to be answered in order to develop an approach suitable for real world cloud data centers concluding remarks and future directions this work advances the cloud computing field in two ways first it plays a significant role in the reduction of data center energy consumption costs and thus helps to develop a strong and competitive cloud computing industry second consumers are increasingly becoming conscious about the environment a recent study shows that data centers represent a large and rapidly growing energy consumption sector of the economy and a significant source of co emissions reducing greenhouse gas emissions is a key energy policy focus of many countries around the world we have presented and evaluated our energy aware resource allocation algorithms utilizing the dynamic consolidation of vms the experiment results have shown that this approach leads to a substantial reduction of energy consumption in cloud data centers in comparison to static resource allocation techniques we are aiming at putting in a strong thrust on open challenges identified in this paper in order enhance the energy efficient management of cloud computing environments the research work is planned to be followed by the develop ment of a software platform that supports the energy efficient management and allocation of cloud data center resources in or der to reduce the cost of software engineering we will extensively reuse existing cloud middleware and associated technologies we will leverage third party cloud technologies and services offer ings including a vm technologies such as open source xen and kvm and commercial products from vmware b amazon elastic compute cloud simple storage service and microsoft azure we will also leverage our own technologies such as aneka which is a net based platform for building enterprise clouds we will implement a generic resource manager and plug in soft ware adaptors to allow the interaction with different cloud man agement systems such as aneka and amazon networks are complex and prone to bugs existing tools that check configuration files and data plane state operate offline at timescales of seconds to hours and cannot detect or prevent bugs as they arise is it possible to check network wide invariants in real time as the network state evolves the key challenge here is to achieve extremely low latency during the checks so that net work performance is not affected in this paper we present a preliminary design veriflow which suggests that this goal is achievable veriflow is a layer between a software defined networking controller and network devices that checks for network wide invariant violations dynamically as each for warding rule is inserted based on an implementation using a mininet openflow network and route views trace data we find that veriflow can perform rigorous checking within hundreds of microseconds per rule insertion categories and subject descriptors c computer communication networks network operations network management network monitoring general terms algorithms design experimentation management perfor mance security verification keywords software defined networking openflow forwarding debug ging real time introduction network forwarding behaviors are complex including code pendent functions running on hundreds or thousands of de vices such as routers switches and firewalls from different vendors as a result a substantial amount of effort is re quired to ensure networks correctness and security how ever faults in the network state arise commonly in practice permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee hotsdn august helsinki finland copyright acm including loops suboptimal routing black holes and access control violations that make services unavailable or prone to attacks e g ddos attacks software defined network ing sdn will ease the development of network applica tions but bugs are likely to remain problematic since the complexity of software will increase moreover sdn allows multiple applications or even multiple users to program the same physical network simultaneously potentially resulting in conflicting rules that alter the intended behavior of one or more applications one solution is to rigorously check network software or configuration for bugs prior to deployment symbolic exe cution can catch bugs through exploration of all possible code paths but is usually not tractable for large software analysis of configuration files is useful but cannot find bugs in router software and must be designed for specific configuration languages and control protocols moreover using these approaches an operator who wants to ensure the network correctness must have access to the software and configuration which may not be true in an sdn net work where controllers can be operated by other parties another approach is to statically analyze snapshots of the network wide data plane state these tools op erate offline and thus only find bugs after they happen this paper studies the following question is it possible to check network wide invariants such as absence of routing loops in real time as the network evolves this would en able us to check updates before they hit the network allow ing us to raise alarms or even prevent bugs as they occur by blocking problematic changes however existing techniques for checking networks are not adequate for this purpose as they operate on timescales of seconds to hours as current sdn controllers are capable of handling around new flow installs per second while maintaining a sub flow install time rule verification latency in the order of seconds is not enough to ensure real time response and will affect controller throughput immensely delaying up dates for processing can harm consistency of network state and reduce reaction time of protocols with real time require ments such as routing and fast failover moreover check ing network wide properties seems to require network wide state and processing churn of a large network could intro duce scaling challenges hence we need some way to per form this checking at very high speeds we present a preliminary design veriflow which demon strates that the goal of real time verification is achievable average run time of reachability tests in is sec onds veriflow leverages sdn to obtain a picture of the network as it evolves by sitting as a layer between the controller and the devices and checks validity of invariants as each rule is inserted however sdn in isolation does not make the problem easy in order to ensure real time response veri flow introduces novel incremental algorithms to search for potential violation of key network invariants for example availability of a path to the destination absence of routing loops access control policies or isolation between virtual networks our prototype implementation of veriflow checks open flow and ip forwarding rules we microbenchmarked veriflow by simulating a real ip network using real bgp traces collected from route views we also evaluated its overhead relative to nox in an emulated openflow network using mininet we find that veriflow is able to verify network invariants within hundreds of microseconds as new rules are introduced into the network veriflow verification phase has little impact on network performance and inflates tcp connection setup latency by a manageable amount around on average in summary our key con tribution is to present the first tool that can check network wide invariants in real time design of veriflow checking network wide invariants in the presence of com plex forwarding elements such as routers firewalls packet transformers can be a hard problem to solve for exam ple in it was shown that packet filters make reacha bility checks np complete and if arbitrary programs are allowed in the data plane then reachability becomes unde cidable aiming to perform these checks in real time makes the problem even harder our design tackles this problem as follows first we monitor all the network update events in a live network second we confine our verification activ ities to only those parts of the network whose actions may be influenced by a new update third rather than check ing invariants with a general purpose tool such as a sat or bdd solver which are generally too slow we use a custom algorithm that is sufficient to verify many kinds of invariants we now discuss each of these design decisions in detail veriflow first job is to track every forwarding state change event for example in an sdn such as openflow a centralized controller issues forwarding rules to the net work devices to handle flows initiated by users veriflow has to intercept all these rules and verify them before they reach the network to achieve this goal veriflow sits as a shim layer between the controller and the network sim ilar to flowvisor and monitors all communication in either direction for every rule insertion deletion message veriflow must verify the effect of the rule on the entire network at very high speeds we solve this problem in three steps first we slice the network into a set of equivalence classes of packets sec tion packets belonging to an equivalence class experi ence the same forwarding actions throughout the network intuitively each change to the network will typically only affect a very small number of equivalence classes there fore we find the set of equivalence classes whose operation could be altered by a rule and verify network invariants only within those classes second veriflow builds individ ual forwarding graphs for every equivalence class using the current network state section third veriflow tra verses these graphs to determine the status of one or more invariants section the following subsections describe these steps in detail figure shows the placement and operations of veriflow in an sdn figure veriflow sits between the sdn applica tions and devices to intercept and check every rule that the network experiences slicing the network into equivalence classes one way to verify network properties is to prepare a model of the entire network using its current data plane state and run queries on this model however checking the en tire network state every time a new flow rule is inserted is wasteful and fails to provide real time response in stead we note that most forwarding rule changes affect only a small subset of all possible packets for example inserting a ip longest prefix match rule will only affect forwarding for packets destined for that prefix however there can be rules in a network device that overlap with a newly inserted rule and match the same set of packets if this happens then it is possible that the new rule may inadvertently alter the path of a set of packets in order to confine our verifica tion activities only on the affected set of packets we slice the network into a set of equivalence classes based on the new rule and the existing rules that overlap with the new rule an equivalence class is a set p of packets such that for any p p p and any network device r the forwarding action is identical for p and p at r separating the entire packet space into individual equivalence classes allows veri flow to pinpoint the affected set of packets if a problem is discovered let us look at an example assume that a switch in an openflow network has two flow rules matching two disjoint sets of packets the first rule matches all packets whose des tination ip address falls within the range and the second rule matches all packets whose destination ip ad dress falls within the range now if a new rule with destination ip address prefix is added into the switch it may affect packets belonging to the range depending on the priority values of these two rules however the new rule will never affect packets that be long to the range therefore veriflow will only consider the new rule and the existing overlap ping rule while analyzing network properties these two overlapping rules will result in the following three equivalence classes to to and to 255 255 veriflow has to utilize an efficient data structure to quickly store new network rules and find overlapping rules we achieve this goal with the help of a prefix tree or trie data structure inspired by the packet classification algorithms presented in a trie is an ordered tree data structure that is used to store an associative array we use the packet header fields as keys in our trie these include the mac and ip addresses both source and destination and trans port protocol ports we use a single trie to store all the rules present in the network in the current version of veri flow we only use the destination ip address to build the forwarding state of the entire network more details on our trie implementation are presented in section modeling forwarding state with forward ing graphs in order to determine the forwarding behavior of each equivalence class veriflow generates individual forwarding graphs for all the equivalence classes computed in the pre vious step this graph is a representation of how packets within an equivalence class will be forwarded through the network it consists of nodes representing network devices and directed edges representing forwarding decisions for that equivalence class at each node hence a node represents an equivalence class at a particular device we put a directed edge from node a to node b if according to the forward ing table at node a the next hop for the equivalence class is node b for each equivalence class we traverse the trie structure to find the devices and rules that match packets from that equivalence class and build the graph using this information a forwarding graph contains all the informa tion needed to answer queries posted by network operators running queries above we described how we model the network behav ior next we need some way to answer queries check invari ants using this model to do this we provide an algorithm which takes as input an invariant to be checked traverses the forwarding graphs of the affected equivalence classes and outputs information about whether the invariant holds there exists a large diversity of queries that can be ex pressed as network reachability e g detecting black holes and routing loops ensuring isolation of multiple vlans verifying access control policies hence we focus on sup porting reachability queries however to support other types of queries it would be straightforward to extend veriflow to accept user provided modules that compute arbitrary prop erties given access to the equivalence classes and their for warding graphs basic reachability algorithm given a snapshot of the network data plane state and a set of new rules network wide invariants can be verified by tracing the traversal paths of all the affected equivalence classes veriflow performs this step by traversing every forwarding graph computed in the previous step using depth first search during this traversal veriflow tries to detect violations of network wide invariants the outcome of this traversal can be a set of possible destinations including none black hole or may result in a routing loop in particular while traversing a forwarding graph if veriflow encounters a node twice then it concludes that insertion of the new rule will result in a routing loop on the other hand if veriflow encounters a node that does not have any outgoing edge and hence does not lead to the intended destination then it concludes that there is a black hole in the network verification actions if veriflow determines that an in variant is violated it executes an associated action that is pre configured for each invariant by the network operator two obvious actions the operator could choose are dropping the rule or installing the rule but generating an alarm for the operator for example the operator could choose to drop rules that cause a security violation such as packets leaking onto a protected vlan but only generate an alarm for a black hole implementation in this section we describe three key implementation chal lenges of our design we start with a description of our inter facing module that helps veriflow to intercept all network events in an openflow network in a transparent manner section section provides some details on the use of our trie structure finally in section we discuss a graph cache based strategy that we use to speed up the ver ification process interfacing with openflow entities in order to ease the deployment of veriflow in any open flow network and use veriflow with unmodified openflow applications we need a mechanism to make veriflow trans parent so that openflow entities remain completely un aware of the presence of veriflow we do this by imple menting veriflow as a proxy application that sits between openflow switches and the controller openflow switches need to be configured to connect to veriflow instead of the openflow controller the switches consider veriflow as their controller for every connection veriflow receives from the openflow switches it initiates a new connection towards the actual controller and simply copies all the bytes sent from the switches to the controller and vice versa how ever simply copying all the bytes from one end to another will not serve our main purpose i e verification of newly inserted rules veriflow has to determine message bound aries within this stream of bytes and filter out rule inser tion deletion messages to achieve this veriflow buffers the bytes it receives from either end and checks whether it received a complete openflow message or not whenever veriflow detects a flow modification message it invokes its rule verification module performing rule verification as we mentioned in section we maintain a trie data structure to store all the forwarding rules present in all the devices in the network this allows us to quickly look up the existing rules that overlap with a newly inserted rule we consider each rule as a binary string and use individual bits to prepare the trie each level in our trie represents a single bit in a particular rule for example for traditional destination prefix based routing there are levels in the trie each node in our trie has three branches the first branch is taken if the corresponding rule bit is the second is taken if the bit is and the third is taken if the bit is don t care i e a wildcard the leaves in the trie store the actual rules that are represented by the path that leads to a particular leaf starting from the root of the trie once we build the trie searching for overlapping rules becomes pretty simple and extremely fast given a new rule we start with the first bit of the rule and traverse the trie starting from its root we examine each bit and take the branch that the bit value points to for don t care bits we explore all the branches of the current node as a don t care bit can take any value for the or bit we explore both the or branch and the don t care branch once we reach the leaves of all the paths that we explore we get a list of rules that overlap with the new rule we use these rules to construct the equivalence classes and forwarding graphs to be used for verifying network properties speeding up veriflow with a graph cache as the network experiences new updates new equivalent classes of packets will be produced due to the interactions between existing and new rules however some equivalent classes will remain common across multiple rule insertions and their forwarding graphs can be reused by updating the graphs with the new rules therefore we maintain a cache of forwarding graphs indexed by their equivalence classes this saves time and allows veriflow to perform rule verification very quickly resulting in real time response evaluation in this section we present performance results of our veri flow implementation as veriflow intercepts every rule in sertion message whenever it is issued by an sdn controller it is crucial to complete the verification process in real time so that network performance is not affected and to ensure scalability of the controller we evaluated the overhead of veriflow operations with the help of two experiments in the first experiment section our goal is to microbench mark different phases of veriflow operations and observe their contribution to the overall run time this allows us to focus on those parts of veriflow that can be further opti mized to reduce the verification latency the goal of the second experiment section is to assess the impact of veriflow on tcp connection setup latency as perceived by end users of an sdn this is important because setting up an end to end tcp connection requires multiple flow rules to be installed in a multi hop network as veri flow will be intercepting each of these rules and verify their effects one by one it is important to keep the overhead as low as possible so that end users do not experience signif icant delay while setting up a new tcp flow both of our experiments were performed on a dell optiplex machine with an intel core cpu with cores at ghz and gb of ram running bit ubuntu linux microbenchmarking veriflow run time in this experiment we simulated a network consisting of routers following a rocketfuel topology as and replayed bgp border gateway protocol rib rout ing information base and update traces collected from the route views project we used an ospf open shortest path first simulator to compute the igp interior gate way protocol path cost between every pair of routers in the network a bgp rib snapshot consisting of million entries was used to initialize the network fib tables then we replayed a bgp update trace containing updates to trigger dynamic changes in the network we randomly mapped route views peers to border routers in our network and then replayed updates so that they originate according to this mapping upon receiving an update from the neigh boring as each border router sends the update to all the other routers in the network using standard bgp polices each router updates its rib using the information present in the update and updates its fib forwarding information base we fed all the fib changes into veriflow to measure the time veriflow takes to complete its individual steps the results from this experiment are shown in figure f d c graph cache update equivalence class graph search build total verification query 100000 time microseconds figure results from simulation using the route views trace total verification time of veriflow re mained well below millisecond for most of the up dates from figure we see that veriflow is able to verify most of the updates within millisecond please note that the x axis in figure is plotted in a log scale the mean verifica tion time for the updates is only microseconds and each query takes only microseconds on an average by limit ing the verification latency within hundreds of microseconds veriflow ensures real time response while processing rule in sertion messages moreover this allows network operators to run multiple queries e g black hole detection isolation of multiple vlans etc within a millisecond time budget effect on tcp connection setup latency in order to evaluate the effect of veriflow operations on user perceived tcp connection setup latency we emulated a node openflow network using mininet mininet cre ates a software defined network sdn with multiple nodes on a single machine our network consists of openflow switches arranged in a chain like topology with a host con nected to every switch we ran a nox controller along with an application that provides the functionality of a learning switch it allows a host to reach any other host in the net work by installing flow rules in the switches we imple mented a simple tcp server program and a simple tcp client program to drive the experiment the server program accepts tcp connections from clients and closes the con nection immediately the client program just connects to a given server and closes the connection immediately we ran the server program at each of the hosts and configured the client programs at all the hosts to connect to the server of a random host excluding itself as many times as possible over a given duration seconds in our experiment we set the rule eviction hard timeout to its minimum possible value second so that veriflow experiences the maximum number of new rules being sent by the controller at the ar rival of new connection requests we measured the time it takes to complete each connection while using veriflow we encounter two types of over head first there is the overhead imposed by the proxy module including overhead to buffer bytes re assemble com plete rule messages and kernel overheads of context switch ing and socket interfaces in order to assess this overhead we first ran our experiment with veriflow placed between the controller and the switches but disabled the verification module the second overhead is the overhead imposed by the ver ification module itself to assess this overhead we ran our experiment a second time with the verification module en abled this allows us to see the delta increase caused by running our verification algorithms as a comparison we ran our experiment a third time without placing veriflow between the controller and the switches in all the runs we varied the number of hosts and ran each combination times figure shows the results from this experiment averaged over runs d n o c e li l i m y c n e t a l p u t e n o i t without with proxy veriflow only with veriflow c e n t number of hosts n o c p c figure tcp connection setup experiment us ing mininet and nox the verification phase of veriflow incurs minimal overhead compared to the proxy operations from figure we can see that in the presence of veriflow the tcp connection setup latency is affected by a signifi cant amount latency is increased by around on aver age compared to the case when veriflow is not in action however we observe that a major share of this overhead is actually contributed by the proxy module increases latency by around on average the overhead imposed by the verification module is rather low and inflates the connection setup latency by only on average from this observation we can conclude that implementing veriflow as a pluggable module for sdn applications or controller frameworks such as nox will allow veriflow to verify network wide invari ants with minimal increase in tcp connection setup latency moreover the overhead caused by proxy operations will be present in any tool that acts as a proxy in an sdn such as flowvisor discussion and future work handling packet transformations we can extend our design to handle rules that perform packet transformation such as network address translation a transformation rule has two parts the match part determines the set of pack ets that will undergo the transformation and the transfor mation part represents the set of packets into which the matched packets will get transformed we can handle this case by generating additional equivalence classes and their corresponding forwarding graphs to address the changes in packet header due to the transformations we leave a full design and implementation to future work deciding when to check veriflow may not know when an invariant violation is a true problem rather than an in termediate state during which the violation is considered acceptable by the operator for example in an sdn ap plications can install rules into a set of switches to build an end to end path from a source host to a destination host however as veriflow is unaware of application semantics it may not be able to determine these rule set boundaries this may cause veriflow to report the presence of tempo rary black holes while processing a set of rules one by one one possible solution is for the sdn application to tell veri flow when to check handling queries other than reachability we can extend our design to answer queries that do not fall into the reachability category for example in a data center the network operator may want to ensure that certain flows do not use the same links or that the number of flows on a link always remains below a threshold as mentioned in section plug in modules could check other properties performance may however depend on the property being checked and its implementation multiple controllers veriflow assumes it has a com plete view of the network to be checked in a multi controller scenario obtaining this view in real time would be difficult checking network wide invariants in real time with multiple controllers is a challenging problem for the future related work recent work on debugging general networks and sdns fo cuses on detecting network anomalies checking open flow applications ensuring data plane consistency and allowing multiple applications to run side by side in a non conflicting manner however unlike veriflow none of the existing solutions provide real time verification of network wide invariants as the network experiences dy namic changes programming openflow networks nox is a network operating system that provides a programming interface to write controller applications for an openflow network nox provides an api that is used by the appli cations to register for openflow events and send openflow commands to the switches frenetic is a high level pro gramming language that can be used to write openflow ap plications running on top of nox frenetic allows openflow application developers to express packet processing policies at a higher level manner than the nox api however fre netic and nox only provide the language and the associated run time unlike veriflow neither nox nor frenetic per form correctness checking of updates limiting their ability to help in detecting bugs in the application code or other issues that may occur while the network is in operation checking openflow applications nice performs symbolic execution of openflow applications and applies model checking to explore the state space of an entire open flow network unlike veriflow nice is a proactive ap proach that tries to figure out invalid system states by using a simplified openflow switch model it is not designed to check network properties in real time flowvisor allows multiple openflow applications to run side by side on the same physical infrastructure without affecting each others actions or performance like veriflow flowvisor acts as a proxy unlike veriflow flowvisor does not look for violations of key network invariants checking network invariants the router configuration checker rcc checks configuration files to detect faults that may cause undesired behavior in the network however rcc cannot detect faults that only manifest themselves in the data plane e g bugs in router software and inconsistencies between the control plane and the data plane see for examples anteater uses data plane information of a network and checks for violations of key network invariants anteater converts the data plane information into boolean expres sions translates network invariants into instances of boolean satisfiability sat problems and checks the resultant sat formulas using a sat solver although anteater can de tect violations of network invariants it is static in nature and does not scale well to dynamic changes in the network taking up to hundreds of seconds to check a single invari ant concurrent with our work is a system with goals similar to anteater and is also not real time configchecker and flowchecker convert network rules configuration and forwarding rules respectively into boolean expressions in order to check network invariants they use binary decision diagram bdd to model the net work state and run queries using computation tree logic ctl wide invariants veriflow and uses handles graph search dynamic techniques changes to in verify real network time moreover unlike previous solutions veriflow can prevent problems from hitting the forwarding plane conclusion in this paper we presented veriflow a network debugging tool to find faulty rules issued by sdn applications and op tionally prevent them from reaching the network and causing anomalous network behavior to the best of our knowledge veriflow is the first tool that can verify network wide in variants in a live network in real time with the help of experiments using a real world network topology real world traces and an emulated openflow network we found that veriflow is capable of processing forwarding table updates in real time due to the increasing popularity of cloud computing more and more data owners are motivated to outsource their data to cloud servers for great convenience and reduced cost in data management however sensitive data should be encrypted before outsourcing for privacy requirements which obsoletes data utilization like keyword based document retrieval in this paper we present a secure multi keyword ranked search scheme over encrypted cloud data which simultaneously supports dynamic update operations like deletion and insertion of documents specifically the vector space model and the widely used tf â idf model are combined in the index construction and query generation we construct a special tree based index structure and propose a greedy depth first search algorithm to provide efficient multi keyword ranked search the secure knn algorithm is utilized to encrypt the index and query vectors and meanwhile ensure accurate relevance score calculation between encrypted index and query vectors in order to resist statistical attacks phantom terms are added to the index vector for blinding search results due to the use of our special tree based index structure the proposed scheme can achieve sub linear search time and deal with the deletion and insertion of documents flexibly extensive experiments are conducted to demonstrate the efficiency of the proposed scheme index terms searchable encryption multi keyword ranked search dynamic update cloud computing ç introduction c loud of computing has been considered as a new model enterprise it infrastructure which can organize huge resource of computing storage and applications and enable users to enjoy ubiquitous convenient and on demand network access to a shared pool of configurable computing resources with great efficiency and minimal eco nomic overhead attracted by these appealing features both individuals and enterprises are motivated to outsource their data to the cloud instead of purchasing software and hardware to manage the data themselves despite of the various advantages of cloud services outsourcing sensitive information such as e mails per sonal health records company finance data government documents etc to remote servers brings privacy con cerns the cloud service providers csps that keep the data for users may access users sensitive information without authorization a general approach to protect the data confidentiality is to encrypt the data before outsourc ing however this will cause a huge cost in terms of data usability for example the existing techniques on keyword based information retrieval which are widely ß ieee personal use is permitted but republication redistribution requires ieee permission see http www ieee org publications rights index html for more information used on the plaintext data cannot be directly applied on the encrypted data downloading all the data from the cloud and decrypt locally is obviously impractical in order to address the above problem researchers have designed some general purpose solutions with fully homo morphic encryption or oblivious rams however these methods are not practical due to their high computa tional overhead for both the cloud sever and user on the contrary more practical special purpose solutions such as searchable encryption se schemes have made specific con tributions in terms of efficiency functionality and security searchable encryption schemes enable the client to store the encrypted data to the cloud and execute keyword search over ciphertext domain so far abundant works have been proposed under different threat models to achieve various search functionality such as single keyword search similar ity search multi keyword boolean search ranked search multi keyword ranked search etc among them multi key word ranked search achieves more and more attention for its practical applicability recently some dynamic schemes have been proposed to support inserting and deleting oper ations on document collection these are significant works as it is highly possible that the data owners need to update z xia x wang and x sun are with the jiangsu engineering center of network monitoring jiangsu collaborative innovation center on atmo spheric environment and equipment technology and school of computer and software nanjing university of information science technology their data on the cloud server but few of the dynamic schemes support efficient multi keyword ranked search this paper proposes a secure tree based search scheme nanjing china e mail sunnudt com over the encrypted cloud data which supports multi key q wang is with the key lab of aerospace information security and trusted computing school of computer wuhan university wuhan china e mail qianwang whu edu cn manuscript received aug revised feb accepted feb date of publication feb date of current version jan word ranked search and dynamic operation on the docu ment collection specifically the vector space model and the widely used term frequency tf â inverse document frequency idf model are combined in the index construc recommended for acceptance by j chen tion and query generation to provide multi keyword ranked for information on obtaining reprints of this article please send e mail to reprints ieee org and reference the digital object identifier below digital object identifier no tpds search in order to obtain high search efficiency we con struct a tree based index structure and propose a greedy depth first search gdfs algorithm based on this index tree due to the special structure of our tree based index the proposed search scheme can flexibly achieve sub linear search time and deal with the deletion and insertion of documents the secure knn algorithm is utilized to encrypt the index and query vectors and meanwhile ensure accu rate relevance score calculation between encrypted index and query vectors to resist different attacks in different threat models we construct two secure search schemes the basic dynamic multi keyword ranked search bdmrs scheme in the known ciphertext model and the enhanced dynamic multi keyword ranked search edmrs scheme in the known background model our contributions are sum marized as follows we design a searchable encryption scheme that sup ports both the accurate multi keyword ranked search and flexible dynamic operation on document collection due to the special structure of our tree based index the search complexity of the proposed scheme is fundamentally kept to logarithmic and in practice the proposed scheme can achieve higher search efficiency by executing our greedy depth first search algorithm moreover parallel search can be flexibly performed to further reduce the time cost of search process the reminder of this paper is organized as follows related work is discussed in section and section gives a brief introduction to the system model threat model the design goals and the preliminaries section describes the schemes in detail section presents the experiments and performance analysis and section covers the conclusion related work searchable encryption schemes enable the clients to store the encrypted data to the cloud and execute keyword search over ciphertext domain due to different cryptography primitives searchable encryption schemes can be con structed using public key based cryptography or symmetric key based cryptography song et al proposed the first symmetric searchable encryption sse scheme and the search time of their scheme is linear to the size of the data collection goh proposed formal security definitions for sse and designed a scheme based on bloom filter the search time of goh scheme is o nð þ where n is the cardinality of the document collection curtmola et al proposed two schemes sse and sse which achieve the optimal search time their sse scheme is secure against chosen keyword attacks and sse is secure against adaptive chosen keyword attacks these early works are single keyword boolean search schemes which are very simple in terms of functionality afterward abundant works have been proposed under dif ferent threat models to achieve various search functionality such as single keyword search similarity search multi keyword boolean search ranked search and multi keyword ranked search etc xia et al a secure and dynamic multi keyword ranked search scheme over encrypted cloud data multi keyword boolean search allows the users to input multiple query keywords to request suitable docu ments among these works conjunctive keyword search schemes only return the documents that contain all of the query keywords disjunctive keyword search schemes return all of the documents that contain a subset of the query keywords predicate search schemes are proposed to support both con junctive and disjunctive search all these multi keyword search schemes retrieve search results based on the exis tence of keywords which cannot provide acceptable result ranking functionality ranked search can enable quick search of the most rele vant data sending back only the top k most relevant docu ments can effectively decrease network traffic some early works have realized the ranked search using order preserving techniques but they are designed only for single keyword search cao et al realized the first pri vacy preserving multi keyword ranked search scheme in which documents and queries are represented as vectors of dictionary size with the coordinate matching the docu ments are ranked according to the number of matched query keywords however cao et al scheme does not con sider the importance of the different keywords and thus is not accurate enough in addition the search efficiency of the scheme is linear with the cardinality of document collection sun et al presented a secure multi keyword search scheme that supports similarity based ranking the authors constructed a searchable index tree based on vector space model and adopted cosine measure together with tfâidf to provide ranking results sun et al search algorithm achieves better than linear search efficiency but results in precision loss orencik et al proposed a secure multi keyword search method which utilized local sensitive hash lsh functions to cluster the similar documents the lsh algorithm is suitable for similar search but cannot provide exact ranking in zhang et al proposed a scheme to deal with secure multi keyword ranked search in a multi owner model in this scheme different data owners use dif ferent secret keys to encrypt their documents and keywords while authorized data users can query without knowing keys of these different data owners the authors proposed an additive order preserving function to retrieve the most relevant search results however these works don t support dynamic operations practically the data owner may need to update the docu ment collection after he upload the collection to the cloud server thus the se schemes are expected to support the insertion and deletion of the documents there are also sev eral dynamic searchable encryption schemes in the work of song et al the each document is considered as a sequence of fixed length words and is individually indexed this scheme supports straightforward update operations but with low efficiency goh proposed a scheme to generate a sub index bloom filter for every doc ument based on keywords then the dynamic operations can be easily realized through updating of a bloom filter along with the corresponding document however goh scheme has linear search time and suffers from false posi tives in kamara et al constructed an encrypted inverted index that can handle dynamic data efficiently but this scheme is very complex to implement subse quently as an improvement kamara and papamanthou proposed a new search scheme based on tree based index which can handle dynamic update on document data stored in leaf nodes however their scheme is designed only for single keyword boolean search in cash et al presented a data structure for keyword identity tuple named t set then a document can be represented by a series of independent t sets based on this structure cash et al proposed a dynamic searchable encryption scheme in their construction newly added tuples are stored in another database in the cloud and deleted tuples are recorded in a revocation list the final search result is achieved through excluding tuples in the revocation list from the ones retrieved from original and newly added tuples yet cash et al dynamic search scheme doesn t real ize the multi keyword ranked search functionality p roblem f ormulation notations and preliminaries w the dictionary namely the set of keywords denoted as w w m g m the total number of keywords in w w q the subset of w representing the keywords in the query f the plaintext document collection denoted as a collection of n documents f f n g each document f in the collection can be considered as a sequence of keywords n the total number of documents in f c the encrypted document collection stored in the cloud server denoted as c c n g t the unencrypted form of index tree for the whole document collection f i the searchable encrypted tree index generated from t q the query vector for keyword set w q td the encrypted form of q which is named as trapdoor for the search request d u the index vector stored in tree node u whose dimension equals to the cardinality of the dictionary w note that the node u can be either a leaf node or an internal node of the tree i u the encrypted form of d u vector space model and relevance score function vector space model along with tfâidf rule is widely used in plaintext information retrieval which efficiently supports ranked multi keyword search here the term frequency is the number of times a given term keyword appears within a document and the inverse document frequency is obtained through dividing the cardinality of document collection by the number of documents containing the keyword in the vector space model each document is denoted by a vector whose elements are the normalized tf values of keywords in this document each query is also denoted as a vector q whose elements are the normalized idf values of query keywords in the document collection naturally the lengths of both the tf vector and the idf vector are equal to the total number of keywords and the dot product of the tf vector d u and the idf vector q can be calculated to quantify ieee transactions on parallel and distributed systems vol no february the relevance between the query and corresponding docu ment following are the notations used in our relevance evaluation function n n the f w i the total number number of of keyword documents w i in document f n word w i the w i number of documents that contain key tf f w i the tf value of w i in document f idf tf stored u w w i i the the in idf value of w i in document collection normalized index vector tf d u value of keyword w i idf document w i the collection normalized idf value of keyword w i in the relevance evaluation function is defined as rscoreðd u qþ d u á q x tf u w i â idf w i w i q if u is an internal index vectors in node of the child the tree tf nodes of u if u w the i is calculated from u is a leaf node tf u w i is calculated as tf u w i q ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi p tf f w i w i where tf f w i à tf f w i á idf w i is calculated þ as lnn f w i and in the search vector q idf w i q ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi p idf w i w i where idf w i à idf w i á q keyword balanced ln à þ binary n n tree w i á the balanced binary tree is widely used to deal with optimization problems the keyword balanced binary kbb tree in our scheme is a dynamic data structure whose node stores a vector d the elements of vector d are the normalized tf values some times we refer the vector d in the node u to d u for simplic ity formally the node u in our kbb tree is defined as follows u hid d p l p r fidi where id denotes the identity of node u p l and p r are respectively the pointers to the left and right child of node u if the node u is a leaf node of the tree fid stores the iden tity of a document and d denotes a vector consisting of the normalized tf values of the keywords to the document if the node u is an internal node fid is set to null and d denotes a vector consisting of the tf values which is calcu lated as follows maxfu p l u p r g i m the detailed construction process of the tree based index is illustrated in section which is denoted as buildindextreeðfþ the system and threat models the system model in this paper involves three different enti ties data owner data user and cloud server as illustrated in fig data owner has a collection of documents f f n g that he wants to outsource to the cloud server in encrypted form while still keeping the capability to search on them for effective utilization in our scheme the data owner first builds a secure searchable tree index i from doc ument collection f and then generates an encrypted docu ment collection c for f afterwards the data owner outsources the encrypted collection c and the secure index i to the cloud server and securely distributes the key infor mation of trapdoor generation including keyword idf val ues and document decryption to the authorized data users besides the data owner is responsible for the update operation of his documents stored in the cloud server while updating the data owner generates the update infor mation locally and sends it to the server data users are authorized ones to access the documents of data owner with t query keywords the authorized user can generate a trapdoor td according to search control mechanisms to fetch k encrypted documents from cloud server then the data user can decrypt the documents with the shared secret key cloud server stores the encrypted document collection c and the encrypted searchable tree index i for data owner upon receiving the trapdoor td from the data user the cloud server executes search over the index tree i and finally returns the corresponding collection of top k ranked encrypted documents besides upon receiving the update information from the data owner the server needs to update the index i and document collection c according to the received information the cloud server in the proposed scheme is considered as honest but curious which is employed by lots of works on secure cloud data search specifically the cloud server honestly and correctly executes instructions in the designated protocol meanwhile it is curious to infer and analyze received data which helps it acquire additional information depending on what information the cloud server knows we adopt the two threat models proposed by cao et al known ciphertext model in this model the cloud server only knows the encrypted document collection c the search able index tree i and the search trapdoor td submitted by fig the architecture of ranked search over encrypted cloud data xia et al a secure and dynamic multi keyword ranked search scheme over encrypted cloud data the authorized user that is to say the cloud server can con duct ciphertext only attack coa in this model known background model compared with known cipher text model the cloud server in this stronger model is equipped with more knowledge such as the term frequency statistics of the document collection this statistical informa tion records how many documents are there for each term frequency of a specific keyword in the whole document col lection as shown in fig which could be used as the key word identity equipped with such statistical information the cloud server can conduct tf statistical attack to deduce or even identify certain keywords through analyzing histo gram and value range of the corresponding frequency dis tributions design goals to enable secure efficient accurate and dynamic multi key word ranked search over outsourced encrypted cloud data under the above models our system has the following design goals dynamic the proposed scheme is designed to provide not only multi keyword query and accurate result ranking but also dynamic update on document collections search efficiency the scheme aims to achieve sublinear search efficiency by exploring a special tree based index and an efficient search algorithm privacy preserving the scheme is designed to prevent the cloud server from learning additional information about the document collection the index tree and the query the spe cific privacy requirements are summarized as follows index confidentiality and query confidentiality the underlying plaintext information including key words in the index and query tf values of key words stored in the index and idf values of query keywords should be protected from cloud server trapdoor unlinkability the cloud server should not be able to determine whether two encrypted queries trapdoors are generated from the same search request keyword privacy the cloud server could not identify the specific keyword in query index or document collection by analyzing the statistical information like term frequency note that our proposed scheme is not designed to protect access pattern i e the sequence of returned documents fig distribution of term frequency tf for a keyword subnet and b keyword host s t he chemes in this section we first describe the unencrypted dynamic multi keyword ranked search udmrs scheme which is constructed on the basis of vector space model and kbb tree based on the udmrs scheme two secure search schemes bdmrs and edmrs schemes are constructed against two threat models respectively index construction of udmrs scheme in section we have briefly introduced the kbb index tree structure which assists us in introducing the index con struction in the process of index construction we first gen erate a tree node for each document in the collection these nodes are the leaf nodes of the index tree then the internal tree nodes are generated based on these leaf nodes the for mal construction process of the index is presented in algorithm an example of our index tree is shown in fig note that the index tree t built here is a plaintext following are some notations for algorithm besides the data structure of the tree node is defined as hid d p l p r fidi where the unique identity id for each tree node is generated through the function genidðþ currentnodeset the set of current processing nodes which have no parents if the number of nodes is even the cardinality of the set is denoted as zþþ else the cardinality is denoted as þ tempnodeset the set of the newly generated nodes in the index if d u for an internal node u there is at least one path from the node u to some leaf which indicates a document containing the keyword w i in addition d u always stores the biggest normalized tf value of w i among its child nodes thus the possible largest relevance score of its children can be easily estimated ieee transactions on parallel and distributed systems vol no february fig an example of the tree based index with the document collection i ji 6g and cardinality of the dictionary m in the con struction process of the tree index we first generate leaf nodes from the documents then the internal tree nodes are generated based on the leaf nodes this figure also shows an example of search process in which the query vector q is equal to in this example we set the parameter k with the meaning that three documents will be returned to the user according to the search algorithm the search starts with the root node and vance score of are successively reaches f reached to the the query with first the is leaf relevance node after f that through scores the r leaf and nodes and r f the rele and next f the the leaf algorithm node f will is reached try to are no reasonable results with score replace search in this subtree subtree rooted because by r the f and in rlist find relevance finally that there score of r is which is smaller than the smallest relevance score in rlist p roposed algorithm buildindextreeðfþ input the document collection f f n g with the identifiers fid ffidjfid ng output the index tree t for each document f fid in f do construct a leaf node u for f fid with u id genidðþ u p l u p r i m null u fid fid and tf f fid for insert u to currentnodeset end for while the number of nodes in currentnodeset is larger than do if the number of nodes in currentnodeset is even i e then for each pair of nodes and in currentnodeset do generate a parent node u for and with u id genidðþ u p l w i u p r u fid and g for each i m insert u to tempnodeset end for else for each pair of nodes and of the former à nodes in currentnodeset do generate a parent node u for and 14 insert u to tempnodeset end for create a parent node u for the à and node and then create a parent node u for u and the þ node insert u to tempnodeset end if replace currentnodeset with tempnodeset and then clear tempnodeset end while return the only node left in currentnodeset namely the root of index tree t search process of udmrs scheme the search process of the udmrs scheme is a recursive pro cedure upon the tree named as greedy depth first search algorithm we construct a result list denoted as rlist whose element is defined as hrscore fidi here the rscore is the relevance score of the document f fid to the query which is calculated according to formula the rlist stores the k accessed documents with the largest rele vance scores to the query the elements of the list are ranked in descending order according to the rscore and will be updated timely during the search process following are some other notations and the gdfs algorithm is described in algorithm rscoreðd u qþ the function to calculate the rele vance score for query vector q and index vector d u stored in node u which is defined in formula kthscore the smallest relevance score in current rlist which is initialized as hchild the child node of a tree node with higher relevance score lchild the child node of a tree node with lower relevance score since the possible largest relevance score of documents rooted by the node u can be predicted only a part of the nodes in the tree are accessed during the search process fig shows an example of search process with the docu ment collection i ji 6g cardinality of the dic tionary m and query vector q 38þ algorithm gdfsðindextreenode uþ if the node u is not a leaf node then if rscoreðd u qþ kthscore then gdfsðu hchildþ gdfsðu lchildþ else return end if else if rscoreðd u qþ kth score then delete the element with the smallest relevance score from rlist insert a new element hrscoreðd u qþ u fidi and sort all the elements of rlist end if return 14 end if bdmrs scheme based on the udmrs scheme we construct the basic dynamic multi keyword ranked search scheme by using the secure knn algorithm the bdmrs scheme is designed to achieve the goal of privacy preserving in the known ciphertext model and the four algorithms included are described as follows sk setupðþ initially the data owner generates the secret key set sk including a randomly generated m bit vector s where m is equal to the cardinality of dictionary and two ðm â mþ invertible matrices m and m namely sk fs m m g i genindexðf skþ first the unencrypted index tree t is built on f by using t buildindextreeðfþ second the data owner generates two random vectors fd u d u for index vector d u in each node u accord ing to the secret vector s specifically and d u will be set equal to d u if d u and d u if d u will be set as two random values whose sum equals to d u finally the encrypted index tree i is built where the node u stores two encrypted index vectors td i u gentrapdoorðw fmt d u mt q skþ d u with keyword set w q the unencrypted query vector q with length of m is generated if w i w q stores the normalized idf value of w i else is set to similarly the query vector q is split into two random vectors and the difference is that if and are set to two random values whose sum equals to 2i else 2i and 2i are set as the same as 2i finally the algorithm returns the trapdoor td relevancescore srscoreði u tdþ with the trap door td the cloud server computes the relevance score of node u in the index tree i to the query note that the relevance score calculated from encrypted xia et al a secure and dynamic multi keyword ranked search scheme over encrypted cloud data vectors is equal to that from unencrypted vectors as follows i à u mt á td á á à á þ à mt á á à á d à mt u á t à d á þ à mt u á t à á d u d u d u þ d u d u á q0 þ d u á q00 d u á q rscoreðd u qþ security analysis we analyze the bdmrs scheme accord ing to the three predefined privacy requirements in the design goals index confidentiality and query confidentiality in the proposed bdmrs scheme i u and td are obfus cated vectors which means the cloud server cannot infer the original vectors d u and q without the secret key set sk the secret keys m and m are gaussian random matrices according to the attacker cloud server of coa cannot calculate the matrices merely with ciphertext thus the bdmrs scheme is resilient against ciphertext only attack and the index confidentiality and the query confidentiality are well protected query unlinkability the trapdoor of query vector is generated from a random splitting operation which means that the same search requests will be trans formed into different query trapdoors and thus the query unlinkability is protected however the cloud server is able to link the same search requests according to the same visited path and the same rel evance scores keyword privacy in this scheme the confidentiality of the index and query are well protected that the origi nal vectors are kept from the cloud server and the search process merely introduces inner product com puting of encrypted vectors which leaks no informa tion about any specific keyword thus the keyword privacy is protected in the known ciphertext model but in the known background model the cloud server is supposed to have more knowledge such as the term frequency statistics of keywords this statis tic information can be visualized as a tf distribution histogram which reveals how many documents are there for every tf value of a specific keyword in the document collection then due to the specificity of the tf distribution histogram like the graph slope and value range the cloud server could conduct tf statistical attack to deduce identify keywords in the worst case when there is only one keyword in the query vector i e the normalized idf value for the keyword is the final relevance score distribution is exactly the normalized tf distribution of this keyword which is directly exposed to cloud server therefore the bdmrs scheme cannot resist tf statistical attack in the known background model edmrs scheme the security analysis above shows that the bdmrs scheme can protect the index confidentiality and query confidentiality in the known ciphertext model however the cloud server is able to link the same search requests by tracking path of visited nodes in addition in the known background model it is possible for the cloud server to identify a keyword as the normalized tf distribution of the keyword can be exactly obtained from the final calculated relevance scores the primary cause is that the relevance score calculated from i u and td is exactly equal to that from d u and q a heuristic method to further improve the security is to break such exact equality thus we can introduce some tunable randomness to disturb the relevance score calculation in addition to suit different users preferences for higher accu rate ranked results or better protected keyword privacy the randomness are set adjustable the enhanced edmrs scheme is almost the same as bdmrs scheme except that sk setupðþ in this algorithm we set the secret vector s as a m bit vector and set m and m are ðm þ þ invertible matrices where is the number of phantom terms i genindexðf skþ before encrypting the index vector d u we extend the vector d u to be a ðm þ dimensional vector each extended ele ment d u þ j j is set as a random number j td gentrapdoorðw extended to be a ðm q skþ þ dimensional the query vector q vector is among the extended elements a number of ele ments are randomly chosen to set as and the rest are set as relevancescore srscoreði u tdþ after the execu tion of relevance evaluation by cloud server the final relevance d u á q þ p score v where for v index vector i u þ j equals to security analysis the security of edmrs scheme is also analyzed according to the three predefined privacy require ments in the design goals index confidentiality and query confidentiality inherited from bdmrs scheme the edmrs scheme can pro tect index confidentiality and query confidentiality in the known background model due to the utiliza tion of phantom terms the confidentiality is further enhanced as the transformation matrices are harder to figure out query unlinkability by introducing the random value the same search requests will generate different query vectors and receive different relevance score distributions thus the query unlinkability is pro tected better however since the proposed scheme is not designed to protect access pattern for efficiency issues the motivated cloud server can analyze the similarity of search results to judge whether the retrieved results come from the same requests in the proposed edmrs scheme the data user can control the p v level of unlinkability by adjusting the this is a trade off between accuracy value of and pri vacy which is determined by the user ieee transactions on parallel and distributed systems vol no february keyword privacy as is discussed in section the bdmrs scheme cannot resist tf statistical attack in the known background model as the cloud server is able to deduce identify keywords through analyzing the tf distribution histogram thus the edmrs scheme is designed keywords with the to randomness obscure the of tf p distributions v in of order to maximize the randomness of relevance butions we need to get as many different score p v as distri sible for given that there are different choices each index vector the possibility that two of p p pos v sharing scheme the the number same value of different is p v in the edmrs which reaches the maximum v is equal to when m hence considering 00 00 we set and v so that the number of differ ent m p v is greater than therefore there are at least dummy elements in every vector and half of p them v in each need query to be randomly selected to generate in addition we set every j to fol low the same uniform according to the distribution à d central limit theorem the þ p dþ v follows the normal distribution nðm where expectation m and standard deviation can be calcu lated as m in the real application we can set m and bal ance the accuracy and privacy by adjusting the vari ance dynamic update operation of dmrs after insertion or deletion of a document we need to update synchronously the index since the index of dmrs scheme is designed as a balanced binary tree the dynamic operation is carried out by updating nodes in the index tree note that the update on index is merely based on document identifies and no access to the content of documents is required the specific process is presented as follows algorithm c i g genupdateinfoðsk t generates which will be i updtypeþþ this the update information sent to the cloud server in order c to i g reduce the communication overhead the data owner stores a copy of unencrypted index tree here the notion updtype fins delg denotes either an inser tion or a deletion for the document f i the notion t denotes the set consisting of the tree nodes that need to be changed during the update for example if we want to delete the document f in fig the subtree t includes a set of nodes fr r rg if updtype is equal to del the data owner deletes from the subtree the leaf node that stores the doc ument identity i and updates the vector d of other nodes in subtree t so as to generate the updated of the subtree leaf node t breaks in particular if the deletion the balance of the binary index tree we replace the deleted node with a fake node whose vector is padded with and file identity is null then the data owner encrypts the vectors stored in the subtree t with the key set sk to generate encrypted the output c i as null subtree and set if updtype is equal to ins the data owner gener ates a tree node u hgenidðþ d null null ii for the document f i where 2j tf f i for j m then the data owner inserts this new node into the subtree t w j as a leaf node and updates the vector d of other nodes in subtree t according to the formula so as to generate the new subtree always preferable t to replace here the data owner is the fake leaf nodes generated by del operation with newly inserted nodes instead of directly inserting new nodes next the data owner encrypts the vectors stored in subtree t with the key set sk as described in section to finally generate encrypted subtree i 0 from c to obtain the new collection similar to the scheme in our scheme can also carry out the update operation without storing the index tree on data owner side we choose to store the unencrypted index tree on the data owner side to tradeoff storage cost for less communi cation burdens in both of the kamara and papamanthou scheme and our design it needs to change a set of nodes to update a leaf node because the vector data of an internal node is computed from its children if the data owner does not store the unencrypted subtree the whole update process needs two rounds of communications between the cloud server and the data owner specifically the data owner should first download the involved subtree in encrypted form from the cloud server second the data owner decrypts the subtree and updates it with the newly added or deleted leaf node third the data owner re encrypts the subtree and uploads the encrypted subtree to the cloud server finally xia et al a secure and dynamic multi keyword ranked search scheme over encrypted cloud data table the change of keyword idf values after updating in a collection with 000 documents keyword no original idf values in the updated collection idf values after deleting after deleting documents documents after adding documents after adding documents 2581 2628 8934 8926 8910 9128 9226 7478 6861 3990 4390 5174 4555 5757 the cloud server replaces the old subtree with the updated one thus to reduce the communication cost we store an unencrypted tree on the data owner side then the data owner can update the subtree directly with the newly added or deleted leaf node and encrypt and upload the updated subtree to the cloud server in this case the update operation can be finished with one round of communication between the cloud server and the data owner as a dynamic scheme it is not reasonable to fix the length of vector as the size of dictionary because the newly added document may contain the keywords out of the dictionary in the proposed scheme we add some blank entries in the dictionary and set corresponding entries in each index vector as 0 if new keywords appear while inserting documents these blank entries are replaced with new keywords then the index vectors of newly added documents are generated based on the updated dictionary while the other index vectors are not affected and remain the same as before after several times of document updating the real idf values of some keywords in the present collection may have obviously changed therefore as the distributor of the idf data the data owner needs to recalculate the idf values for all keywords and distribute them to authorized users in table there are three classes of keywords with different idf value ranges the smaller idf value means the key word appears more frequently table shows that after adding or deleting and documents the idf values do not change a lot thus the data owner is unnecessary to update idf values every time when he executes update operation on the data set the data owner can flexibly choose to check the change of idf values and distribute the new idf values when these values have changed a lot parallel execution of search owing to the tree based index structure the proposed search scheme can be executed in parallel which further improves the search efficiency for example we assume there are a set of processors with the other one if there is no idle processor the current processor is used to deal with the child with larger rele vance score and the other child is put into a waiting queue once there is an idle processor it takes the oldest node in the queue to continue the search note that all the process ors share the same result list rlist p erformance a nalysis we implement the proposed scheme using c language in windows operation system and test its efficiency on a real world document collection the request for comments rfc the test includes the search precision on dif ferent privacy level and the efficiency of index construc tion trapdoor generation search and update most of the experimental results are obtained with an intel core tm duo processor 93 ghz except that the efficiency of search is tested on a server with two intel r xeon r cpu processors 0 ghz which has processor cores and supports parallel threads precision and privacy the search precision of scheme is affected by the dummy keywords in edmrs scheme here the precision is defined as that in p k k where is the number of real top k documents in the retrieved k documents if a smaller p v standard deviation is set for the random variable the edmrs scheme is supposed to obtain higher pre cision and vice versa the results are shown in fig in the edmrs scheme phantom terms are added to the index vector to obscure the relevance score calculation so that the cloud server cannot identify keywords by analyzing the tf distributions of special keywords here we quantify the obscureness of the relevance score by rank privacy which is defined as is its real rank number in the whole rank privacy denotes the higher security of the scheme which is illustrated in fig in the proposed scheme data users can accomplish dif ferent requirements on search precision and privacy by adjusting the standard deviation which can be treated as a balance parameter fig the precision a and rank privacy b of searches with different standard deviation s ieee transactions on parallel and distributed systems vol no february table precision test of s basic scheme no precision no precision 88 94 11 85 14 we compare our schemes with a recent work proposed by sun et al which achieves high search efficiency note that our bdmrs scheme retrieves the search results through exact calculation of document vector and query vector thus top k search precision of the bdmrs scheme is percent but as a similarity based multi keyword ranked search scheme the basic scheme in suffers from precision loss due to the clustering of sub vectors during index construction the precision test of s basic scheme is presented in table in each test five keywords are ran domly chosen as input and the precision of returned top results is observed the test is repeated times and the average precision is percent efficiency index tree construction the process of index tree construction for document collec tion f includes two main steps building an unencrypted kbb tree based on the document collection f and encrypting the index tree with splitting operation and two multiplications of a ðm â mþ matrix the index structure is constructed following a post order traversal of the tree based on the document collection f and oðnþ nodes are generated during the traversal for each node generation of an index vector takes oðmþ time vector splitting process takes oðmþ time and two multiplications of a ðm â mþ matrix takes time as a whole the time complexity for index tree construction is apparently the time cost for building index tree mainly depends on the cardinal ity of document collection f and the number of keywords in dictionary w fig shows that the time cost of index tree fig time cost for index tree construction a for the different sizes of document collection with the fixed dictionary m 4 000 and b for the different sizes of dictionary with the fixed document collection n 4 000 table storage consumption of index tree size of dictionary 000 000 000 4 000 000 bdmrs mb 293 edmrs mb 315 construction is almost linear with the size of document col lection and is proportional to the number of keywords in the dictionary due to the dimension extension the index tree construction of edmrs scheme is slightly more time consuming than that of bdmrs scheme although the index tree construction consumes relatively much time at the data owner side it is noteworthy that this is a one time operation on the other hand since the underlying balanced binary tree has space complexity oðnþ and every node stores two m dimensional vectors the space complexity of the index tree is oðnmþ as listed in table when the document col lection is fixed n 4 000 the storage consumption of the index tree is determined by the size of the dictionary trapdoor generation the generation of a trapdoor incurs a vector splitting opera tion and two multiplications of a ðm â mþ matrix thus the time complexity is as shown in fig typical search requests usually consist of just a few keywords fig shows that the number of query keywords has little influence on the overhead of trapdoor generation when the dictionary size is fixed due to the dimension extension the time cost of edmrs scheme is a little higher than that of bdmrs scheme search efficiency during the search process if the relevance score at node u is larger than the minimum relevance score in result list rlist the cloud server examines the children of the node else it returns thus lots of nodes are not accessed during a real search we denote the number of leaf nodes that contain one or more keywords in the query as u generally u is larger than the number of required documents k but far less than the cardinality of the document collection n as a balanced binary tree the height of the index is maintained to be log n fig time cost for trapdoor generation a for different sizes of dictio nary with the fixed number of query keywords t 4 and b for differ ent numbers of query keywords with the fixed dictionary m 4 4 000 xia et al a secure and dynamic multi keyword ranked search scheme over encrypted cloud data and the complexity of relevance score calculation is oðmþ thus the time complexity of search is oðum log nþ note that the real search time is less than um log n it is because many leaf nodes that contain the queried keywords are not visited according to our search algorithm and the access ing paths of some different leaf nodes share the mutual tra versed parts in addition the parallel execution of search process can increase the efficiency a lot we test the search efficiency of the proposed scheme on a server which supports parallel threads the search performance is tested respectively by starting 4 and threads we compare the search efficiency of our scheme with that of sun et al in the implementation of sun s code we divide 4 000 keywords into levels thus each level contains keywords according to the higher level the query keywords reside the higher the search efficiency is in our experiment we choose keywords from the first level the highest level the opti mal case for search efficiency comparison fig shows that if the query keywords are chosen from the first level our scheme obtains almost the same efficiency as when we start four threads fig also shows that the search efficiency of our scheme increases a lot when we increase the number of threads from to 4 however when we continue to increase the threads the search efficiency is not increased fig the efficiency of a search with keywords of interest as input a for the different sizes of document collection with the same dictio nary m 4 4 000 and b for different numbers of retrieved documents with the same document collection and dictionary n 4 000 and m 4 4 000 remarkably our search algorithm can be executed in par allel to improve the search efficiency but all the started threads will share one result list rlist in mutually exclu sive manner when we start too many threads the threads will spend a lot of time for waiting to read and write the rlist an intuitive method to handle this problem is to con struct multiple result lists however in our scheme it will not help to improve the search efficiency a lot it is because that we need to find k results for each result list and time complexity for retrieving each result list is oðum log n lþ in this case the multiple threads will not save much time and selecting k results from the multiple result list will further increase the time consumption in the fig we show the time consumption when we start multiple threads with multiple result lists the experimental results prove that our scheme will obtain better search efficiency when we start multiple threads with only one result list 4 update efficiency in order to update a leaf node the data owner needs to update logn nodes since it involves an encryption operation for index vector at each node which takes time the time complexity of update operation is thus log nþ we illustrate the time cost for the deletion of a document fig shows that when the size of dictionary is fixed the deletion of a document takes nearly logarithmic time with the size of document collection and fig shows that the update time is proportional to the size of dictionary when the document collection is fixed in addition the space complexity of each node is oðmþ thus space complexity of the communication package of updating a document is oðm log nþ conclusion and future work in this paper a secure efficient and dynamic search scheme is proposed which supports not only the accurate multi keyword ranked search but also the dynamic deletion and insertion of documents we construct a special keyword bal anced binary tree as the index and propose a greedy depth first search algorithm to obtain better efficiency than linear search in addition the parallel search process can be carried out to further reduce the time cost the secu rity of the scheme is protected against two threat models by fig the efficiency of a search with keywords of interest as input a for the different sizes of document collection with the same dictionary m 4 4 000 and b for different numbers of retrieved documents with the same document collection and dictionary n 4 000 and m 4 4 000 ieee transactions on parallel and distributed systems vol no february using the secure knn algorithm experimental results dem onstrate the efficiency of our proposed scheme there are still many challenge problems in symmetric se schemes in the proposed scheme the data owner is respon sible for generating updating information and sending them to the cloud server thus the data owner needs to store the unencrypted index tree and the information that are neces sary to recalculate the idf values such an active data owner may not be very suitable for the cloud computing model it could be a meaningful but difficult future work to design a dynamic searchable encryption scheme whose updating operation can be completed by cloud server only meanwhile reserving the ability to support multi keyword ranked search in addition as the most of works about searchable encryption our scheme mainly considers the challenge from the cloud server actually there are many secure challenges in a multi user scheme first all the users usually keep the same secure key for trapdoor generation in a symmetric se scheme in this case the revocation of the user is big chal lenge if it is needed to revoke a user in this scheme we need to rebuild the index and distribute the new secure keys to all the authorized users second symmetric se schemes usually assume that all the data users are trustworthy it is not practi cal and a dishonest data user will lead to many secure prob lems for example a dishonest data user may search the documents and distribute the decrypted documents to the unauthorized ones even more a dishonest data user may distribute his her secure keys to the unauthorized ones in the future works we will try to improve the se scheme to handle these challenge problems ubiquitous sensing enabled by wireless sensor network wsn technologies cuts across many areas of modern day living this offers the ability to measure infer and understand environmental indicators from delicate ecologies and natural resources to urban environments the proliferation of these devices in a communicating actuating network creates the internet of things iot wherein sensors and actuators blend seamlessly with the environment around us and the information is shared across platforms in order to develop a common operating picture cop fueled by the recent adaptation of a variety of enabling wireless technologies such as rfid tags and embedded sensor and actuator nodes the iot has stepped out of its infancy and is the next revolutionary technology in transforming the internet into a fully integrated future internet as we move from www static pages web to social networking web to ubiquitous computing web the need for data on demand using sophisticated intuitive queries increases significantly this paper presents a cloud centric vision for worldwide implementation of internet of things the key enabling technologies and application domains that are likely to drive iot research in the near future are discussed a cloud implementation using aneka which is based on interaction of private and public clouds is presented we conclude our iot vision by expanding on the need for convergence of wsn the internet and distributed computing directed at technological research community elsevier b v all rights reserved introduction the next wave in the era of computing will be outside the realm of the traditional desktop in the internet of things iot paradigm many of the objects that surround us will be on the network in one form or another radio frequency identification rfid and sensor network technologies will rise to meet this new challenge in which information and communication systems are invisibly embedded in the environment around us this results in the generation of enormous amounts of data which have to be stored processed and presented in a seamless efficient and easily interpretable form this model will consist of services that are commodities and delivered in a manner similar to traditional commodities cloud corresponding author tel fax e mail addresses rbuyya unimelb edu au raj cs mu oz au r buyya url http www buyya com r buyya computing can provide the virtual infrastructure for such utility computing which integrates monitoring devices storage devices analytics tools visualization platforms and client delivery the cost based model that cloud computing offers will enable end to end service provisioning for businesses and users to access applications on demand from anywhere smart connectivity with existing networks and context aware computation using network resources is an indispensable part of iot with the growing presence of wifi and lte wireless inter net access the evolution towards ubiquitous information and com munication networks is already evident however for the internet of things vision to successfully emerge the computing paradigm will need to go beyond traditional mobile computing scenarios that use smart phones and portables and evolve into connect ing everyday existing objects and embedding intelligence into our environment for technology to disappear from the conscious ness of the user the internet of things demands a shared understanding of the situation of its users and their appliances see front matter elsevier b v all rights reserved http dx doi org 1016 j future 01 contents lists available at sciverse sciencedirect future generation computer systems journal homepage www elsevier com locate fgcs future generation computer systems j gubbi et al future generation computer systems software architectures and pervasive communication networks to process and convey the contextual information to where it is rel evant and the analytics tools in the internet of things that aim for autonomous and smart behavior with these three fundamental grounds in place smart connectivity and context aware computa tion can be accomplished the term internet of things was first coined by kevin ashton in in the context of supply chain management however in the past decade the definition has been more inclusive cover ing wide range of applications like healthcare utilities transport etc although the definition of things has changed as tech nology evolved the main goal of making a computer sense infor mation without the aid of human intervention remains the same a radical evolution of the current internet into a network of in terconnected objects that not only harvests information from the environment sensing and interacts with the physical world actu ation command control but also uses existing internet standards to provide services for information transfer analytics applications and communications fueled by the prevalence of devices enabled by open wireless technology such as bluetooth radio frequency identification rfid wi fi and telephonic data services as well as embedded sensor and actuator nodes iot has stepped out of its in fancy and is on the verge of transforming the current static internet into a fully integrated future internet the internet revolution led to the interconnection between people at an unprecedented scale and pace the next revolution will be the interconnection be tween objects to create a smart environment only in did the number of interconnected devices on the planet overtake the ac tual number of people currently there are billion interconnected devices and it is expected to reach billion devices by according to the gsma this amounts to trillion revenue op portunities for mobile network operators alone spanning vertical segments such as health automotive utilities and consumer elec tronics a schematic of the interconnection of objects is depicted in fig where the application domains are chosen based on the scale of the impact of the data generated the users span from individual to national level organizations addressing wide ranging issues this paper presents the current trends in iot research propelled by applications and the need for convergence in several interdisciplinary technologies specifically in section we present the overall iot vision and the technologies that will achieve it followed by some common definitions in the area along with some trends and taxonomy of iot in section we discuss several application domains in iot with a new approach in defining them in section 4 and section provides our cloud centric iot vision a case study of data analytics on the aneka azure cloud platform is given in section and we conclude with discussions on open challenges and future trends in section ubiquitous computing in the next decade the effort by researchers to create a human to human inter face through technology in the late resulted in the creation of the ubiquitous computing discipline whose objective is to em bed technology into the background of everyday life currently we are in the post pc era where smart phones and other handheld de vices are changing our environment by making it more interactive as well as informative mark weiser the forefather of ubiquitous computing ubicomp defined a smart environment 4 as the physical world that is richly and invisibly interwoven with sensors actuators displays and computational elements embedded seam lessly in the everyday objects of our lives and connected through a continuous network the creation of the internet has marked a foremost milestone towards achieving ubicomp s vision which enables individual devices to communicate with any other device in the world the inter networking reveals the potential of a seemingly endless amount of distributed computing resources and storage owned by various owners in contrast to weiser s calm computing approach rogers proposes a human centric ubicomp which makes use of human creativity in exploiting the environment and extending their capa bilities he proposes a domain specific ubicomp solution when he says in terms of who should benefit it is useful to think of how ubicomp technologies can be developed not for the sal s of the world but for particular domains that can be set up and cus tomized by an individual firm or organization such as for agricul tural production environmental restoration or retailing caceres and friday discuss the progress opportunities and challenges during the year anniversary of ubicomp they discuss the building blocks of ubicomp and the characteristics of the system to adapt to the changing world more importantly they identify two critical technologies for growing the ubicomp infrastructure cloud computing and the internet of things the advancements and convergence of micro electro mechan ical systems mems technology wireless communications and digital electronics has resulted in the development of miniature devices having the ability to sense compute and communicate wirelessly in short distances these miniature devices called nodes interconnect to form a wireless sensor networks wsn and find wide ranging applications in environmental monitoring infras tructure monitoring traffic monitoring retail etc this has the ability to provide a ubiquitous sensing capability which is critical in realizing the overall vision of ubicomp as outlined by weiser 4 for the realization of a complete iot vision efficient secure scal able and market oriented computing and storage resourcing is es sential cloud computing is the most recent paradigm to emerge which promises reliable services delivered through next genera tion data centers that are based on virtualized storage technolo gies this platform acts as a receiver of data from the ubiquitous sensors as a computer to analyze and interpret the data as well as providing the user with easy to understand web based visual ization the ubiquitous sensing and processing works in the back ground hidden from the user this novel integrated sensor actuator internet framework shall form the core technology around which a smart environment will be shaped information generated will be shared across di verse platforms and applications to develop a common operating picture cop of an environment where control of certain unre stricted things is made possible as we move from www static pages web to social networking web to ubiquitous computing web the need for data on demand using sophisticated intuitive queries increases to take full advantage of the available internet technology there is a need to deploy large scale platform independent wireless sensor network infrastructure that includes data management and processing actuation and analytics cloud computing promises high reliability scalability and autonomy to provide ubiquitous access dynamic resource discovery and com posability required for the next generation internet of things ap plications consumers will be able to choose the service level by changing the quality of service parameters definitions trends and elements definitions as identified by atzori et al internet of things can be re alized in three paradigms internet oriented middleware things oriented sensors and semantic oriented knowledge although this type of delineation is required due to the interdisciplinary na ture of the subject the usefulness of iot can be unleashed only in an application domain where the three paradigms intersect the rfid group defines the internet of things as fig internet of things schematic showing the end users and application areas based on data the worldwide network of interconnected objects uniquely addressable based on standard communication protocols according to cluster of european research projects on the internet of things things are active participants in business information and social processes where they are enabled to interact and com municate among themselves and with the environment by ex changing data and information sensed about the environment while reacting autonomously to the real physical world events and influencing it by running processes that trigger actions and create services with or without direct human intervention according to forrester a smart environment uses information and communications technologies to make the critical infrastructure components and services of a city s administration education healthcare public safety real estate transportation and utilities more aware interactive and efficient in our definition we make the definition more user centric and do not restrict it to any standard communication protocol this will allow long lasting applications to be developed and deployed using the available state of the art protocols at any given point in time our definition of the internet of things for smart environments is interconnection of sensing and actuating devices providing the ability to share information across platforms through a uni fied framework developing a common operating picture for enabling innovative applications this is achieved by seamless ubiquitous sensing data analytics and information representa tion with cloud computing as the unifying framework trends internet of things has been identified as one of the emerging technologies in it as noted in gartner s it hype cycle see fig a hype cycle is a way to represent the emergence adoption maturity and impact on applications of specific technologies it has been forecasted that iot will take years for market adoption the popularity of different paradigms varies with time the web search popularity as measured by the google search trends during the last years for the terms internet of things wireless sensor networks and ubiquitous computing are shown in fig 11 as it can be seen since iot has come into existence search volume is consistently increasing with the falling trend for wireless sensor networks as per google s search forecast dotted line in fig this trend is likely to continue as other enabling technologies converge to form a genuine internet of things iot elements we present a taxonomy that will aid in defining the compo nents required for the internet of things from a high level per spective specific taxonomies of each component can be found elsewhere 12 14 there are three iot components which enables seamless ubicomp a hardware made up of sensors actuators and embedded communication hardware b middleware on de mand storage and computing tools for data analytics and c presentation novel easy to understand visualization and interpre tation tools which can be widely accessed on different platforms and which can be designed for different applications in this sec tion we discuss a few enabling technologies in these categories which will make up the three components stated above j gubbi et al future generation computer systems 1660 j gubbi et al future generation computer systems 1660 fig google search trends since for terms internet of things wireless sensor networks ubiquitous computing radio frequency identification rfid rfid technology is a major breakthrough in the embedded com munication paradigm which enables design of microchips for wire less data communication they help in the automatic identification of anything they are attached to acting as an electronic barcode 15 the passive rfid tags are not battery powered and they use the power of the reader s interrogation signal to communicate the id to the rfid reader this has resulted in many applications par ticularly in retail and supply chain management the applications can be found in transportation replacement of tickets registra tion stickers and access control applications as well the passive tags are currently being used in many bank cards and road toll tags which are among the first global deployments active rfid readers have their own battery supply and can instantiate the communi cation of the several applications the main application of active rfid tags is in port containers for monitoring cargo 2 wireless sensor networks wsn recent technological advances in low power integrated circuits and wireless communications have made available efficient low cost low power miniature devices for use in remote sensing ap plications the combination of these factors has improved the vi ability of utilizing a sensor network consisting of a large number of intelligent sensors enabling the collection processing analysis and dissemination of valuable information gathered in a variety of environments active rfid is nearly the same as the lower end wsn nodes with limited processing capability and storage the scientific challenges that must be overcome in order to realize the enormous potential of wsns are substantial and multidisciplinary in nature sensor data are shared among sensor nodes and sent to a distributed or centralized system for analytics the compo nents that make up the wsn monitoring network include a wsn hardware typically a node wsn core hardware con tains sensor interfaces processing units transceiver units and power supply almost always they comprise of multiple a d converters for sensor interfacing and more modern sensor nodes have the ability to communicate using one frequency band making them more versatile b wsn communication stack the nodes are expected to be de ployed in an ad hoc manner for most applications designing fig 2 gartner hype cycle of emerging technologies source gartner inc 10 j gubbi et al future generation computer systems 1660 an appropriate topology routing and mac layer is critical for the scalability and longevity of the deployed network nodes in a wsn need to communicate among themselves to transmit data in single or multi hop to a base station node drop outs and consequent degraded network lifetimes are frequent the communication stack at the sink node should be able to inter act with the outside world through the internet to act as a gate way to the wsn subnet and the internet c wsn middleware a mechanism to combine cyber infrastruc ture with a service oriented architecture soa and sensor net works to provide access to heterogeneous sensor resources in a deployment independent manner this is based on the idea of isolating resources that can be used by several appli cations a platform independent middleware for developing sensor applications is required such as an open sensor web architecture oswa oswa is built upon a uniform set of operations and standard data representations as defined in the sensor web enablement method swe by the open geospatial consortium ogc d secure data aggregation an efficient and secure data aggre gation method is required for extending the lifetime of the network as well as ensuring reliable data collected from sen sors node failures are a common characteristic of wsns the network topology should have the capability to heal it self ensuring security is critical as the system is automatically linked to actuators and protecting the systems from intruders becomes very important 3 addressing schemes the ability to uniquely identify things is critical for the success of iot this will not only allow us to uniquely identify billions of devices but also to control remote devices through the internet the few most critical features of creating a unique address are uniqueness reliability persistence and scalability every element that is already connected and those that are go ing to be connected must be identified by their unique identifica tion location and functionalities the current may support to an extent where a group of cohabiting sensor devices can be identi fied geographically but not individually the internet mobility at tributes in the may alleviate some of the device identification problems however the heterogeneous nature of wireless nodes variable data types concurrent operations and confluence of data from devices exacerbates the problem further persistent network functioning to channel the data traffic ubiquitously and relentlessly is another aspect of iot although the tcp ip takes care of this mechanism by routing in a more reliable and efficient way from source to destination the iot faces a bottleneck at the interface between the gateway and wireless sensor devices furthermore the scalability of the device address of the existing network must be sustainable the addition of networks and devices must not hamper the performance of the network the functioning of the devices the reliability of the data over the network or the effective use of the devices from the user interface to address these issues the uniform resource name urn sys tem is considered fundamental for the development of iot urn creates replicas of the resources that can be accessed through the url with large amounts of spatial data being gathered it is of ten quite important to take advantage of the benefits of metadata for transferring the information from a database to the user via the internet also gives a very good option to access the resources uniquely and remotely another critical development in addressing is the development of a lightweight that will en able addressing home appliances uniquely wireless sensor networks considering them as building blocks of iot which run on a different stack compared to the internet cannot possess stack to address individually and hence a subnet with a gateway having a urn will be required with this in mind we then need a layer for addressing sensor devices by the relevant gateway at the subnet level the urn for the sensor devices could be the unique ids rather than human friendly names as in the www and a lookup table at the gateway to address this device further at the node level each sensor will have a urn as numbers for sensors to be addressed by the gateway the entire network now forms a web of connectivity from users high level to sensors low level that is addressable through urn accessible through url and controllable through urc 3 3 4 data storage and analytics one of the most important outcomes of this emerging field is the creation of an unprecedented amount of data storage owner ship and expiry of the data become critical issues the internet con sumes up to of the total energy generated today and with these types of demands it is sure to go up even further hence data cen ters that run on harvested energy and are centralized will ensure energy efficiency as well as reliability the data have to be stored and used intelligently for smart monitoring and actuation it is im portant to develop artificial intelligence algorithms which could be centralized or distributed based on the need novel fusion algo rithms need to be developed to make sense of the data collected state of the art non linear temporal machine learning methods based on evolutionary algorithms genetic algorithms neural net works and other artificial intelligence techniques are necessary to achieve automated decision making these systems show charac teristics such as interoperability integration and adaptive commu nications they also have a modular architecture both in terms of hardware system design as well as software development and are usually very well suited for iot applications more importantly a centralized infrastructure to support storage and analytics is re quired this forms the iot middleware layer and there are numer ous challenges involved which are discussed in future sections as of cloud based storage solutions are becoming increasingly popular and in the years ahead cloud based analytics and visual ization platforms are foreseen 3 3 visualization visualization is critical for an iot application as this allows the interaction of the user with the environment with recent advances in touch screen technologies use of smart tablets and phones has become very intuitive for a lay person to fully benefit from the iot revolution attractive and easy to understand visualization has to be created as we move from to screens more information can be provided in meaningful ways for consumers this will also enable policy makers to convert data into knowledge which is crit ical in fast decision making extraction of meaningful information from raw data is non trivial this encompasses both event detec tion and visualization of the associated raw and modeled data with information represented according to the needs of the end user 4 applications there are several application domains which will be impacted by the emerging internet of things the applications can be classi fied based on the type of network availability coverage scale het erogeneity repeatability user involvement and impact we categorize the applications into four application domains per sonal and home 2 enterprize 3 utilities and 4 mobile this is depicted in fig 1 which represents personal and home iot at the scale of an individual or home enterprize iot at the scale of a community utility iot at a national or regional scale and mo bile iot which is usually spread across other domains mainly due to the nature of connectivity and scale there is a huge crossover 1650 j gubbi et al future generation computer systems 1660 in applications and the use of data between domains for instance the personal and home iot produces electricity usage data in the house and makes it available to the electricity utility company which can in turn optimize the supply and demand in the utility iot the internet enables sharing of data between different service providers in a seamless manner creating multiple business oppor tunities a few typical applications in each domain are given 4 1 personal and home the sensor information collected is used only by the individuals who directly own the network usually wifi is used as the back bone enabling higher bandwidth data video transfer as well as higher sampling rates sound ubiquitous healthcare has been envisioned for the past two decades iot gives a perfect platform to realize this vision using body area sensors and iot back end to upload the data to servers for instance a smartphone can be used for communication along with several interfaces like bluetooth for interfacing sensors mea suring physiological parameters so far there are several applica tions available for apple ios google android and windows phone operating systems that measure various parameters however it is yet to be centralized in the cloud for general physicians to access the same an extension of the personal body area network is creating a home monitoring system for elderly care which allows the doctor to monitor patients and the elderly in their homes thereby reducing hospitalization costs through early intervention and treatment 23 control of home equipment such as air conditioners refriger ators washing machines etc will allow better home and energy management this will see consumers become involved in the iot revolution in the same manner as the internet revolution itself 25 social networking is set to undergo another transforma tion with billions of interconnected objects 27 an interesting development will be using a twitter like concept where individual things in the house can periodically tweet the readings which can be easily followed from anywhere creating a tweetot although this provides a common framework using cloud for information ac cess a new security paradigm will be required for this to be fully realized 4 2 enterprize we refer to the network of things within a work environment as an enterprize based application information collected from such networks are used only by the owners and the data may be released selectively environmental monitoring is the first common application which is implemented to keep track of the number of occupants and manage the utilities within the building e g hvac lighting sensors have always been an integral part of the factory setup for security automation climate control etc this will eventually be replaced by a wireless system giving the flexibility to make changes to the setup whenever required this is nothing but an iot subnet dedicated to factory maintenance one of the major iot application areas that is already draw ing attention is smart environment iot 28 there are several testbeds being implemented and many more planned in the com ing years smart environment includes subsystems as shown in ta ble 1 and the characteristics from a technological perspective are listed briefly it should be noted that each of the sub domains cover many focus groups and the data will be shared the applications or use cases within the urban environment that can benefit from the realization of a smart city wsn capability are shown in table 2 these applications are grouped according to their impact areas this includes the effect on citizens considering health and well be ing issues transport in light of its impact on mobility productiv ity pollution and services in terms of critical community services managed and provided by local government to city inhabitants 4 3 utilities the information from the networks in this application domain is usually for service optimization rather than consumer consump tion it is already being used by utility companies smart meter by electricity supply companies for resource management in order to optimize cost vs profit these are made up of very extensive net works usually laid out by large organization on a regional and na tional scale for monitoring critical utilities and efficient resource management the backbone network used can vary between cel lular wifi and satellite communication smart grid and smart metering is another potential iot applica tion which is being implemented around the world efficient energy consumption can be achieved by continuously monitoring every electricity point within a house and using this information to modify the way electricity is consumed this information at the city scale is used for maintaining the load balance within the grid ensuring high quality of service video based iot which integrates image processing com puter vision and networking frameworks will help develop a new challenging scientific research area at the intersection of video infrared microphone and network technologies surveillance the most widely used camera network applications helps track tar gets identify suspicious activities detect left luggage and monitor unauthorized access automatic behavior analysis and event detec tion as part of sophisticated video analytics is in its infancy and breakthroughs are expected in the next decade as pointed out in the gartner chart refer fig 2 water network monitoring and quality assurance of drinking water is another critical application that is being addressed using iot sensors measuring critical water parameters are installed at important locations in order to ensure high supply quality this avoids accidental contamination among storm water drains drinking water and sewage disposal the same network can be extended to monitor irrigation in agricultural land the network is also extended for monitoring soil parameters which allows informed decision making concerning agriculture 4 4 mobile smart transportation and smart logistics are placed in a sepa rate domain due to the nature of data sharing and backbone im plementation required urban traffic is the main contributor to traffic noise pollution and a major contributor to urban air qual ity degradation and greenhouse gas emissions traffic congestion directly imposes significant costs on economic and social activities in most cities supply chain efficiencies and productivity includ ing just in time operations are severely impacted by this conges tion causing freight delays and delivery schedule failures dynamic traffic information will affect freight movement allow better plan ning and improved scheduling the transport iot will enable the use of large scale wsns for online monitoring of travel times ori gin destination o d route choice behavior queue lengths and air pollutant and noise emissions the iot is likely to replace the traffic information provided by the existing sensor networks of inductive loop vehicle detectors employed at the intersections of existing traffic control systems they will also underpin the devel opment of scenario based models for the planning and design of mitigation and alleviation plans as well as improved algorithms for urban traffic control including multi objective control systems combined with information gathered from the urban traffic control j gubbi et al future generation computer systems 1660 table 1 smart environment application domains smart home office smart retail smart city smart agriculture forest smart water smart transportation network size small small medium medium large large large users very few fam ily members few community level many policy makers general public few landowners policy makers few government large general public energy rechargeable battery rechargeable battery rechargeable battery energy harvesting energy harvesting energy harvesting rechargeable battery energy harvesting internet connectivity wifi lte backbone wifi lte backbone wifi lte backbone wifi satellite communication satellite communication microwave links wifi satellite communication data management local server local server shared server local server shared server shared server shared server iot devices rfid wsn rfid wsn rfid wsn wsn single sensors rfid wsn single sensors bandwidth requirement small small large medium medium medium large example testbeds aware home sap future retail center smart santander citysense sisvia gbroos 34 semat a few trial implementations 36 table 2 potential iot applications identified by different focus groups of the city of melbourne citizens healthcare triage patient monitoring personnel monitoring disease spread modeling and containment real time health status and predictive information to assist practitioners in the field or policy decisions in pandemic scenarios emergency services defense remote personnel monitoring health location resource management and distribution response planning sensors built into building infrastructure to guide first responders in emergencies or disaster scenarios crowd monitoring crowd flow monitoring for emergency management efficient use of public and retail spaces workflow in commercial environments transport traffic management intelligent transportation through real time traffic information and path optimization infrastructure monitoring sensors built into infrastructure to monitor structural fatigue and other maintenance accident monitoring for incident management and emergency response coordination services water water quality leakage usage distribution waste management building management temperature humidity control activity monitoring for energy usage management d heating ventilation and air conditioning hvac environment air pollution noise monitoring waterways industry monitoring system valid and relevant information on traffic conditions can be presented to travelers the prevalence of bluetooth technology bt devices reflects the current iot penetration in a number of digital products such as mo bile phones car hands free sets navigation systems etc bt devices emit signals with a unique media access identification mac id number that can be read by bt sensors within the coverage area readers placed at different locations can be used to identify the movement of the devices complemented by other data sources such as traffic signals or bus gps research problems that can be addressed include vehicle travel time on motorways and arterial streets dynamic time dependent o d matrices on the network identification of critical intersections and accurate and reliable real time transport network state information there are many privacy concerns by such usages and digital forgetting is an emerg ing domain of research in iot where privacy is a concern another important application in mobile iot domain is efficient logistics management this includes monitoring the items being transported as well as efficient transportation planning the monitoring of items is carried out more locally say within a truck replicating enterprize domain but transport planning is carried out using a large scale iot network 5 cloud centric internet of things the vision of iot can be seen from two perspectives internet centric and thing centric the internet centric architecture will involve internet services being the main focus while data is contributed by the objects in the object centric architecture the smart objects take the center stage in our work we develop an internet centric approach a conceptual framework integrating the ubiquitous sensing devices and the applications is shown in fig 4 in order to realize the full potential of cloud computing as well as ubiquitous sensing a combined framework with a cloud at the center seems to be most viable this not only gives the flexibility of dividing associated costs in the most logical manner but is also highly scalable sensing service providers can join the network and offer their data using a storage cloud analytic tool developers can provide their software tools artificial intelligence experts can provide their data mining and machine learning tools useful in converting information to knowledge and finally computer graphics designers can offer a variety of visualization tools cloud computing can offer these services as infrastructures platforms or software where the full potential of human creativity can be tapped using them as services this in some sense agrees with the ubicomp vision of weiser as well as rogers human centric approach the data generated tools used and the visualization created disappears into the background tapping the full potential of the internet of things in various application domains as can be seen from fig 4 the cloud integrates all ends of ubicomp by providing scalable storage computation time and other tools to build new businesses in this section we describe the cloud platform using manjrasoft aneka and microsoft azure platforms to demonstrate how cloud integrates storage computation and visualization paradigms furthermore we introduce an important realm of interaction between clouds which is useful for combining public and private clouds using aneka this interaction is critical for application developers in order to bring sensed information analytics algorithms and visualization under one single seamless framework however developing iot applications using low level cloud programming models and interfaces such as thread and mapre 1652 j gubbi et al future generation computer systems 1660 fig 5 a model of end to end interaction between various stakeholders in cloud centric iot framework duce models is complex to overcome this we need a iot applica tion specific framework for rapid creation of applications and their deployment on cloud infrastructures this is achieved by mapping the proposed framework to cloud apis offered by platforms such as aneka therefore the new iot application specific framework should be able to provide support for 1 reading data streams ei ther from senors directly or fetch the data from databases 2 easy expression of data analysis logic as functions operators that pro cess data streams in a transparent and scalable manner on cloud infrastructures and 3 if any events of interest are detected out comes should be passed to output streams which are connected to a visualization program using such a framework the developer of iot applications will able to harness the power of cloud com puting without knowing low level details of creating reliable and scale applications a model for the realization of such an environ ment for iot applications is shown in fig 5 thus reducing the time and cost involved in engineering iot applications 5 1 aneka cloud computing platform aneka is a net based application development platform as a service paas which can utilize storage and compute resources of both public and private clouds it offers a runtime envi ronment and a set of apis that enable developers to build cus fig 4 conceptual iot framework with cloud computing at the center fig overview of aneka within internet of things architecture tomized applications by using multiple programming models such as task programming thread programming and mapreduce pro gramming aneka provides a number of services that allow users to control auto scale reserve monitor and bill users for the resources used by their applications in the context of smart environment application aneka paas has another important characteristic of supporting the provisioning of resources on public clouds such as microsoft azure amazon and gogrid while also harnessing private cloud resources ranging from desktops and clusters to vir tual data centers an overview of aneka paas is shown in fig for the application developer the cloud service as well as ubiq uitous sensor data is hidden and they are provided as services at a cost by the aneka provisioning tool automatic management of clouds for hosting and delivering iot services as saas software as a service applications will be the integrating platform of the future internet there is a need to create data and service sharing infrastructure which can be used for addressing several applica tion scenarios for example anomaly detection in sensed data car ried out at the application layer is a service which can be shared between several applications existing new applications deployed as a hosted service and accessed over the internet are referred to as saas to manage saas applications on a large scale the platform as a service paas layer needs to coordinate the cloud resource provisioning and application scheduling without im pacting the quality of service qos requirements of any appli cation the autonomic management components are to be put in place to schedule and provision resources with a higher level of accuracy to support iot applications this coordination requires the paas layer to support autonomic management capabilities required to handle the scheduling of applications and resource provisioning such that the user qos requirements are satisfied the autonomic management components are thus put in place to schedule and provision resources with a higher level of accuracy to support iot applications the autonomic management system will tightly integrate the following services with the aneka framework accounting monitoring and profiling scheduling and dynamic provisioning accounting monitoring and profiling will feed the sensors of the autonomic manager while the managers effectors will control scheduling and dynamic provisioning from a logical point of view the two components that will mostly take advantage of the introduction of autonomic features in aneka are the appli cation scheduler and the dynamic resource provisioning 5 2 application scheduler and dynamic resource provisioning in aneka for iot applications the aneka scheduler is responsible for assigning each resource to a task in an application for execution based on user qos parame ters and the overall cost for the service provider depending on the computation and data requirements of each sensor application it directs the dynamic resource provisioning component to instanti ate or terminate a specified number of computing storage and net work resources while maintaining a queue of tasks to be scheduled this logic is embedded as multi objective application scheduling algorithms the scheduler is able to mange resource failures by re allocating those tasks to other suitable cloud resources the dynamic resource provisioning component implements the logic for provisioning and managing virtualized resources in the private and public cloud computing environments based on the resource requirements as directed by the application scheduler this is achieved by dynamically negotiating with the cloud infrastructure as a service iaas providers for the right kind of resource for a certain time and cost by taking into account the past execution history of applications and budget availability this decision is made at runtime when saas applications continuously send requests to the aneka cloud platform j gubbi et al future generation computer systems 1660 j gubbi et al future generation computer systems 1660 table 3 microsoft azure components microsoft azure on demand compute services storage services sql azure supports transact sql and support for the synchronization of relational data across sql azure and on premises sql server appfabric interconnecting cloud and on premise applications accessed through the http rest api azure marketplace online service for making transactions on apps and data iot sensor data analytics saas using aneka and microsoft azure microsoft azure is a cloud platform offered by microsoft in cludes four components as summarized in table 3 there are several advantages for integrating azure and aneka aneka can launch any number of instances on the azure cloud to run their applications essentially it provides the provisioning infrastruc ture similarly aneka provides advanced paas features as shown in fig 6 it provides multiple programming models task thread mapreduce runtime execution services workload management services dynamic provisioning qos based scheduling and flexible billing as discussed earlier to realize the ubicomp vision tools and data need to be shared between application developers to create new apps there are two major hurdles in such an implementation firstly interaction between clouds becomes critical which is addressed by aneka in the intercloud model aneka support for the intercloud model enables the creation of a hybrid cloud computing environment that combines the resources of private and public clouds that is whenever a private cloud is unable to meet application qos requirements aneka leases extra capability from a public cloud to ensure that the application is able to execute within a specified deadline in a seamless manner secondly data analytics and artificial intelligence tools are computationally demanding which requires huge resources for data analytics and artificial intelligence tools the aneka task programming model provides the ability of expressing applications as a collection of independent tasks each task can perform different operations or the same operation on different data and can be executed in any order by the runtime environment in order to demonstrate this we have used a scenario where there are multiple analytics algorithms and multiple data sources a schematic of the interaction between aneka and azure is given in fig where aneka worker containers are deployed as instances of azure worker role the aneka master container will be deployed in the on premises private cloud while aneka worker containers will be run as instances of microsoft azure worker role as shown in fig there are two types of microsoft azure worker roles used these are the aneka worker role and message proxy role in this case one instance of the message proxy role and at least one instance of the aneka worker role are deployed the maximum number of instances of the aneka worker role that can be launched is limited by the subscription offer of microsoft azure service that a user selects in this deployment scenario when a user submits an application to the aneka master the job units will be scheduled by the aneka master by leveraging on premises aneka workers if they exist and aneka worker instances on microsoft azure simultaneously when aneka workers finish the execution of aneka work units they will send the results back to aneka master and then aneka master will send the result back to the user application there are many interoperability issues when scaling across multiple clouds aneka overcomes this problem by providing a framework which enables the creation of adaptors for different cloud infrastructures as there is currently no interoperability standard these standards are currently under development by many forums and when such standards become real a new adaptor for aneka will be developed this will ensure that the iot applications making use of aneka can seamlessly benefit from either private public or hybrid clouds another important feature required for a seamless indepen dent iot working architecture is saas to be updated by the de velopers dynamically in this example analytics tools usually in the form of dlls have to be updated and used by several clients due to administrative privileges provided by azure this becomes a non trivial task management extensibility framework mef pro vides a simple solution to the problem the mef is a composition layer for net that improves the flexibility maintainability and testability of large applications mef can be used for third party plugins or it can bring the benefits of a loosely coupled plugin like architecture for regular applications it is a library for creating lightweight extensible applications it allows application develop ers to discover and use extensions with no configuration required it also lets extension developers easily encapsulate code and avoid fragile hard dependencies mef not only allows extensions to be reused within applications but across applications as well mef provides a standard way for the host application to expose itself and consume external extensions extensions by their nature can be reused amongst different applications however an extension could still be implemented in a way that is application specific the extensions themselves can depend on one another and mef will make sure they are wired together in the correct order one of the key design goals of an iot web application is that it would be extensible and mef provides this solution with mef we can use different algorithms as and when it becomes available for iot data analytics e g drop an analytics assembly into a folder and it instantly becomes available to the application the system context diagram of the developed data analytics is given in fig 47 open challenges and future directions the proposed cloud centric vision comprises a flexible and open architecture that is user centric and enables different players to interact in the iot framework it allows interaction in a manner suitable for their own requirements rather than the iot being thrust upon them in this way the framework includes provisions to meet different requirements for data ownership security privacy and sharing of information some open challenges are discussed based on the iot elements presented earlier the challenges include iot specific challenges such as privacy participatory sensing data analytics gis based visualization and cloud computing apart from the standard wsn challenges including architecture energy efficiency security protocols and quality of service the end goal is to have plug n play smart objects which can be deployed in any environment with an interoperable backbone allowing them to blend with other smart objects around them standardization of frequency bands and protocols plays a pivotal role in accomplishing this goal a roadmap of key developments in iot research in the context of pervasive applications is shown in fig which includes the technology drivers and key application outcomes expected in the next decade the section ends with a few international initiatives in the domain which could play a vital role in the success of this rapidly emerging technology fig system context diagram 1 architecture overall architecture followed at the initial stages of iot re search will have a severe bearing on the field itself and needs to be investigated most of the works relating to iot architecture have been from the wireless sensor networks perspective european union projects of sensei and internet of things architecture iot a have been addressing the challenges par ticularly from the wsn perspective and have been very successful in defining the architecture for different applications we are refer ring architecture to overall iot where the user is at the center and will enable the use of data and infrastructure to develop new ap plications an architecture based on cloud computing at the center has been proposed in this paper however this may not be the best option for every application domain particularly for defense where human intelligence is relied upon although we see cloud centric architecture to be the best where cost based services are required other architectures should be investigated for different application domains 2 energy efficient sensing efficient heterogeneous sensing of the urban environment needs to simultaneously meet competing demands of multiple sensing modalities this has implications on network traffic data storage and energy utilization importantly this encompasses both fixed and mobile sensing infrastructure as well as contin uous and random sampling a generalized framework is required for data collection and modeling that effectively exploits spatial and temporal characteristics of the data both in the sensing do main as well as the associated transform domains for example urban noise mapping needs an uninterrupted collection of noise fig schematic of aneka azure interaction for data analytics application j gubbi et al future generation computer systems 1660 j gubbi et al future generation computer systems 1660 fig roadmap of key technological developments in the context of iot application domains envisioned levels using battery powered nodes using fixed infrastructure and participatory sensing as a key component for health and qual ity of life services for its inhabitants compressive sensing enables reduced signal measurements without impacting accurate reconstruction of the signal a signal sparse in one basis may be recovered from a small number of pro jections onto a second basis that is incoherent with the first the problem reduces to finding sparse solutions through smallest norm coefficient vector that agrees with the measurements in the ubiquitous sensing context this has implications for data com pression network traffic and the distribution of sensors compres sive wireless sensing cws utilizes synchronous communication to reduce the transmission power of each sensor transmitting noisy projections of data samples to a central location for aggrega tion 3 secure reprogrammable networks and privacy security will be a major concern wherever networks are de ployed at large scale there can be many ways the system could be attacked disabling the network availability pushing erroneous data into the network accessing personal information etc the three physical components of iot rfid wsn and cloud are vul nerable to such attacks security is critical to any network and the first line of defense against data corruption is cryptogra phy of the three rfid particularly passive seems to be the most vulnerable as it allows person tracking as well as the objects and no high level intelligence can be enabled on these devices 16 these complex problems however have solutions that can be provided using cryptographic methods and deserve more research before they are widely accepted against outsider attackers encryption ensures data confiden tiality whereas message authentication codes ensure data in tegrity and authenticity encryption however does not protect against insider malicious attacks to address which non cryptographic means are needed particularly in wsns also periodically new sensor applications need to be installed or existing ones need to be updated this is done by remote wire less reprogramming of all nodes in the network traditional network reprogramming consists solely of a data dissemination protocol that distributes code to all the nodes in the network with out authentication which is a security threat a secure reprogram ming protocol allows the nodes to authenticate every code update and prevent malicious installation most such protocols e g are based on the benchmark protocol deluge we need cryp tographic add ons to deluge which lays the foundation for more sophisticated algorithms to be developed security in the cloud is another important area of research which will need more attention along with the presence of the data and tools cloud also handles economics of iot which will make it a bigger threat from attackers security and identity protection becomes critical in hybrid clouds where private as well as public clouds will be used by businesses 56 remembering forever in the context of iot raises many privacy issues as the data collected can be used in positive for advertise ment services and negative ways for defamation digital forget ting could emerge as one of the key areas of research to address the concerns and the development of an appropriate framework to protect personal data j gubbi et al future generation computer systems 2013 1660 4 quality of service heterogeneous networks are by default multi service provid ing more than one distinct application or service this implies not only multiple traffic types within the network but also the ability of a single network to support all applications without qos com promise there are two application classes throughput and delay tolerant elastic traffic of e g monitoring weather parame ters at low sampling rates and the bandwidth and delay sensi tive inelastic real time traffic e g noise or traffic monitoring which can be further discriminated by data related applications e g high vs low resolution videos with different qos require ments therefore a controlled optimal approach to serve differ ent network traffics each with its own application qos needs is required it is not easy to provide qos guarantees in wireless networks as segments often constitute gaps in resource guaran tee due to resource allocation and management ability constraints in shared wireless media quality of service in cloud computing is another major research area which will require more and more at tention as the data and tools become available on clouds dynamic scheduling and resource allocation algorithms based on particle swarm optimization are being developed for high capacity appli cations and as iot grows this could become a bottleneck 5 new protocols the protocols at the sensing end of iot will play a key role in complete realization they form the backbone for the data tunnel between sensors and the outer world for the system to work effi ciently an energy efficient mac protocol and appropriate routing protocol are critical several mac protocols have been proposed for various domains with tdma collision free csma low traffic ef ficiency and fdma collision free but requires additional circuitry in nodes schemes available to the user none of them are ac cepted as a standard and with more things available this scenario is going to get more cluttered which requires further research an individual sensor can drop out for a number of reasons so the network must be self adapting and allow for multi path routing multi hop routing protocols are used in mobile ad hoc networks and terrestrial wsns they are mainly divided into three categories data centric location based and hierarchical again based on different application domains energy is the main consideration for the existing routing protocols in the case of iot it should be noted that a backbone will be available and the number of hops in the multi hop scenario will be limited in such a scenario the existing routing protocols should suffice in practical implementation with minor modifications 6 participatory sensing a number of projects have begun to address the development of people centric or participatory sensing platforms as noted earlier people centric sensing offers the possibility of low cost sensing of the environment localized to the user it can there fore give the closest indication of environmental parameters ex perienced by the user it has been noted that environmental data collected by the user forms a social currency this results in more timely data being generated compared to the data available through a fixed infrastructure sensor network most importantly it is the opportunity for the user to provide feedback on their ex perience of a given environmental parameter that offers valuable information in the form of context associated with a given event the limitations of people centric sensing place a new signifi cance on the reference data role provided by a fixed infrastruc ture iot as a backbone the problem of missing samples is a fundamental limitation of people centric sensing relying on users volunteering data and on the inconsistent gathering of samples ob tained across varying times and varying locations based on a user s desired participation and given location or travel path limits the ability to produce meaningful data for any applications and policy decisions only in addressing issues and implications of data own ership privacy and appropriate participation incentives can such a platform achieve genuine end user engagement further sensing modalities can be obtained through the addition of sensor modules attached to the phone for application specific sensing such as air quality sensors or biometric sensors in such scenarios smart phones become critical iot nodes which are connected to the cloud on one end and several sensors at the other end data mining extracting useful information from a complex sensing environ ment at different spatial and temporal resolutions is a challenging research problem in artificial intelligence current state of the art methods use shallow learning methods where pre defined events and data anomalies are extracted using supervised and unsuper vised learning the next level of learning involves inferring local activities by using temporal information of events extracted from shallow learning the ultimate vision will be to detect com plex events based on larger spatial and longer temporal scales based on the two levels before the fundamental research problem that arises in complex sensing environments of this nature is how to simultaneously learn representations of events and activities at multiple levels of complexity i e events local activities and com plex activities an emerging focus in machine learning research has been the field of deep learning which aims to learn mul tiple layers of abstraction that can be used to interpret given data furthermore the resource constraints in sensor networks create novel challenges for deep learning in terms of the need for adap tive distributed and incremental learning techniques gis based visualization as new display technologies emerge creative visualization will be enabled the evolution from crt to plasma lcd led and amoled displays has given rise to highly efficient data representa tion using touch interface with the user being able to navigate the data better than ever before with emerging displays this area is certain to have more research and development opportunities however the data that comes out of ubiquitous computing is not always ready for direct consumption using visualization platforms and requires further processing the scenario becomes very com plex for heterogeneous spatio temporal data new visualiza tion schemes for the representation of heterogeneous sensors in a landscape that varies temporally have to be developed an other challenge of visualizing data collected within iot is that they are geo related and are sparsely distributed to cope with such a challenge a framework based on internet gis is required 7 9 cloud computing integrated iot and cloud computing applications enabling the creation of smart environments such as smart cities need to be able to a combine services offered by multiple stakeholders and b scale to support a large number of users in a reliable and decentralized manner they need to be able operate in both wired and wireless network environments and deal with constraints such as access devices or data sources with limited power and unreliable connectivity the cloud application platforms need to be enhanced to support a the rapid creation of applications by providing domain specific programming tools and environments and b seamless execution of applications harnessing capabilities ________________ 1658 j gubbi et al future generation computer systems 2013 1660 of multiple dynamic and heterogeneous resources to meet quality of service requirements of diverse users the cloud resource management and scheduling system should be able to dynamically prioritize requests and provision resources such that critical requests are served in real time to deliver results in a reliable manner the scheduler needs to be augmented with task duplication algorithms for failure management specifically the cloud application scheduling algorithms need to exhibit the following capability 1 multi objective optimization the scheduling algorithms should be able to deal with qos parameters such as response time cost of service usage maximum number of resources available per unit price and penalties for service degradation 2 task duplication based fault tolerance critical tasks of an application will be transparently replicated and executed on different resources so that if one resource fails to complete the task the replicated version can be used this logic is crucial in real time tasks that need to be processed to deliver services in a timely manner 7 10 international activities internet of things activities are gathering momentum around the world with numerous initiatives underway across industry academia and various levels of government as key stakeholders seek to map a way forward for the coordinated realization of this technological evolution in europe substantial effort is underway to consolidate the cross domain activities of research groups and organizations spanning wsn and rfid into a unified iot framework supported by the european commission frame work program eu this includes the internet of things euro pean research cluster ierc encompassing a number of eu projects its objectives are to establish a cooperation platform and research vision for iot activities in europe and become a contact point for iot research around the world it includes projects such as a consortium of international partners from europe the usa china japan and korea exploring issues surrounding rfid and its role in realizing the internet of things also ierc includes the internet of things architecture iot a project established to determine an architectural reference model for the interoperability of internet of things systems and key building blocks to achieve this at the same time the iot initiative iot i is a coordinated ac tion established to support the development of the european iot community the iot i project brings together a consortium of part ners to create a joint strategic and technical vision for the iot in eu rope that encompasses the currently fragmented sectors of the iot domain holistically simultaneously the smart santander project is developing a city scale iot testbed for research and service pro vision deployed across the city of santander spain as well as sites located in the uk germany serbia and australia at the same time large scale initiatives are underway in japan korea the usa and australia where industry associated organi zations and government departments are collaborating on vari ous programs advancing related capabilities towards an iot this includes smart city initiatives smart grid programs incorporating smart metering technologies and roll out of high speed broadband infrastructure a continuing development of rfid related technolo gies by industry and consortiums such as the auto id lab founded at mit and now with satellite labs at leading universities in south korea china japan united kingdom australia and switzerland dedicated to creating the internet of things using rfid and wire less sensor networks are being pursued significantly the need for consensus around iot technical issues has seen the establishment of the internet protocol for smart objects ipso alliance now with more than member companies from leading technology com munications and energy companies working with standards bod ies such as ietf ieee and itu to specify new ip based technologies and promote industry consensus for assembling the parts for the internet of things substantial iot development activity is also un derway in china with its five year plan specify ing iot investment and development to be focused on smart grid intelligent transportation smart logistics smart home environ ment and safety testing industrial control and automation health care fine agriculture finance and service military defense this is being aided by the establishment of an internet of things center in shanghai with a total investment over us million to study technologies and industrial standards an industry fund for the in ternet of things and an internet of things union sensing china has been founded in wuxi initiated by more than telecom op erators institutes and companies who are the primary drivers of the industry 8 summary and conclusions the proliferation of devices with communicating actuating capabilities is bringing closer the vision of an internet of things where the sensing and actuation functions seamlessly blend into the background and new capabilities are made possible through access of rich new information sources the evolution of the next generation mobile system will depend on the creativity of the users in designing new applications iot is an ideal emerging technology to influence this domain by providing new evolving data and the required computational resources for creating revolutionary apps presented here is a user centric cloud based model for ap proaching this goal through the interaction of private and public clouds in this manner the needs of the end user are brought to the fore allowing for the necessary flexibility to meet the diverse and sometimes competing needs of different sectors we propose a framework enabled by a scalable cloud to provide the capacity to utilize the iot the framework allows networking computation storage and visualization themes separate thereby allowing inde pendent growth in every sector but complementing each other in a shared environment the standardization which is underway in each of these themes will not be adversely affected with cloud at its center in proposing the new framework associated challenges have been highlighted ranging from appropriate interpretation and visualization of the vast amounts of data through to the privacy security and data management issues that must underpin such a platform in order for it to be genuinely viable the consolidation of international initiatives is quite clearly accelerating progress to wards an iot providing an overarching view for the integration and functional elements that can deliver an operational iot datacenter workloads demand high computational capabili ties flexibility power efficiency and low cost it is challenging to improve all of these factors simultaneously to advance dat acenter capabilities beyond what commodity server designs can provide we have designed and built a composable recon figurable fabric to accelerate portions of large scale software services each instantiation of the fabric consists of a d torus of high end stratix v fpgas embedded into a half rack of machines one fpga is placed into each server acces sible through pcie and wired directly to other fpgas with pairs of gb sas cables in this paper we describe a medium scale deployment of this fabric on a bed of servers and measure its efficacy in accelerating the bing web search engine we describe the requirements and architecture of the system detail the critical engineering challenges and solutions needed to make the system robust in the presence of failures and measure the performance power and resilience of the system when ranking candidate documents under high load the large scale reconfigurable fabric improves the ranking throughput of each server by a factor of for a fixed latency distribution or while maintaining equivalent throughput reduces the tail latency by introduction the rate at which server performance improves has slowed considerably this slowdown due largely to power limitations has severe implications for datacenter operators who have traditionally relied on consistent performance and efficiency improvements in servers to make improved services economi cally viable while specialization of servers for specific scale workloads can provide efficiency gains it is problematic for two reasons first homogeneity in the datacenter is highly and university of texas at austin web services university institute of technology and university of washington inc polytechnique fédérale de lausanne epfl all authors contributed to this work while employed by microsoft desirable to reduce management issues and to provide a consis tent platform that applications can rely on second datacenter services evolve extremely rapidly making non programmable hardware features impractical thus datacenter providers are faced with a conundrum they need continued improve ments in performance and efficiency but cannot obtain those improvements from general purpose systems reconfigurable chips such as field programmable gate arrays fpgas offer the potential for flexible acceleration of many workloads however as of this writing fpgas have not been widely deployed as compute accelerators in either datacenter infrastructure or in client devices one challenge traditionally associated with fpgas is the need to fit the ac celerated function into the available reconfigurable area one could virtualize the fpga by reconfiguring it at run time to support more functions than could fit into a single device however current reconfiguration times for standard fpgas are too slow to make this approach practical multiple fpgas provide scalable area but cost more consume more power and are wasteful when unneeded on the other hand using a single small fpga per server restricts the workloads that may be accelerated and may make the associated gains too small to justify the cost this paper describes a reconfigurable fabric that we call catapult for brevity designed to balance these competing concerns the catapult fabric is embedded into each half rack of servers in the form of a small board with a medium sized fpga and local dram attached to each server fpgas are directly wired to each other in a two dimensional torus allowing services to allocate groups of fpgas to provide the necessary area to implement the desired functionality we evaluate the catapult fabric by offloading a significant fraction of microsoft bing ranking stack onto groups of eight fpgas to support each instance of this service when a server wishes to score rank a document it performs the software portion of the scoring converts the document into a format suitable for fpga evaluation and then injects the document to its local fpga the document is routed on the inter fpga network to the fpga at the head of the ranking pipeline after running the document through the eight fpga pipeline the computed score is routed back to the requesting server although we designed the fabric for general purpose service c ieee acceleration we used web search to drive its requirements due to both the economic importance of search and its size and complexity we set a performance target that would be a significant boost over software throughput in the number of documents ranked per second per server including portions of ranking which are not offloaded to the fpga one of the challenges of maintaining such a fabric in the datacenter is resilience the fabric must stay substantially available in the presence of errors failing hardware reboots and updates to the ranking algorithm fpgas can potentially corrupt their neighbors or crash the hosting servers during bitstream reconfiguration we incorporated a failure handling protocol that can reconfigure groups of fpgas or remap ser vices robustly recover from failures by remapping fpgas and report a vector of errors to the management software to diagnose problems we tested the reconfigurable fabric search workload and failure handling service on a bed of servers equipped with fpgas the experiments show that large gains in search throughput and latency are achievable using the large scale reconfigurable fabric compared to a pure software imple mentation the catapult fabric achieves a improvement in throughput at each ranking server with an equivalent latency distribution or at the same throughput reduces tail latency by the system is able to run stably for long periods with a failure handling service quickly reconfiguring the fabric upon errors or machine failures the rest of this paper describes the catapult architecture and our measurements in more detail catapult hardware the acceleration of datacenter services imposes several strin gent requirements on the design of a large scale reconfigurable fabric first since datacenter services are typically large and complex a large amount of reconfigurable logic is necessary second the fpgas must fit within the datacenter architecture and cost constraints while reliability is important the scale of the datacenter permits sufficient redundancy that a small rate of faults and failures is tolerable to achieve the required capacity for a large scale reconfig urable fabric one option is to incorporate multiple fpgas onto a daughtercard and house such a card along with a subset of the servers we initially built a prototype in this fashion with six xilinx virtex fpgas connected in a mesh network through the fpga general purpose i os although straightforward to implement this solution has four problems first it is inelastic if more fpgas are needed than there are on the daughtercard the desired service cannot be mapped second if fewer fpgas are needed there is stranded capac ity third the power and physical space for the board cannot be accommodated in conventional ultra dense servers requir ing either heterogeneous servers in each rack or a complete redesign of the servers racks network and power distribu tion finally the large board is a single point of failure whose failure would result in taking down the entire subset of servers figure the logical mapping of the torus network and the physical wiring on a pod of x servers the alternative approach we took places a small daughter card in each server with a single high end fpga and connects the cards directly together with a secondary network provided that the latency on the inter fpga network is sufficiently low and that the bandwidth is sufficiently high services requiring more than one fpga can be mapped across fpgas residing in multiple servers this elasticity permits efficient utilization of the reconfigurable logic and keeps the added acceleration hardware within the power thermal and space limits of dense datacenter servers to balance the expected per server per formance gains versus the necessary increase in total cost of ownership tco including both increased capital costs and operating expenses we set aggressive power and cost goals given the sensitivity of cost numbers on elements such as pro duction servers we cannot give exact dollar figures however adding the catapult card and network to the servers did not exceed our limit of an increase in tco of including a limit of for total server power board design to minimize disruption to the motherboard we chose to in terface the board to the host cpu over pcie while a tighter coupling of the fpga to the cpu would provide benefits in a b c figure a a block diagram of the fpga board b a picture of the manufactured board c a diagram of the u half width server that hosts the fpga board the air flows from the left to the right leaving the fpga in the exhaust of both cpus dram fpga w ecc jtag qspi flash terms of latency direct access to system memory and poten tially coherence the selection of pcie minimized disruption to this generation of the server design since the fpga resides in i o space the board needed working memory to accommodate certain services we chose to add local dram as sram qdr arrays were too expensive to achieve sufficient capacity gb of dram was sufficient to map the services we had planned and fit within our power and cost envelopes figure shows a logical diagram of the fpga board along with a picture of the manufactured board and the server it installs into we chose a high end altera stratix v fpga which has considerable reconfigurable logic on chip memory blocks and dsp units the gb of dram consists of two dual rank so dimms which can operate at speeds with the full gb capacity or trade capacity for additional bandwidth by running as gb single rank dimms at speeds the pcie and inter fpga network traces are routed to a mezzanine connec tor on the bottom of the daughtercard which plugs directly into a socket on the motherboard other components on the board include a programmable oscillator and mb of quad spi flash to hold fpga configurations because of the limited physical size of the board and the number of signals that must be routed we used a layer board design our target appli cations would benefit from increased memory bandwidth but there was insufficient physical space to add additional dram channels we chose to use dimms with ecc to add resilience as dram failures are commonplace at datacenter scales figure c shows the position of the board in one of the datacenter servers we used the mezzanine connector at the back of the server so that heat from the fpga did not disrupt the existing system components since the fpga is subject to the air being heated by the host cpus which can reach c we used an industrial grade fpga part rated for higher temperature operation up to c it was also necessary to add emi shielding to the board to protect other server components from interference from the large number of high speed signals on the board one requirement for serviceability was that no jumper cables should be attached to the board e g power or signaling by limiting the power draw of the daughtercard to under w the pcie bus alone provided all necessary power by keeping the power draw to under w during normal operation we met our thermal requirements and our limit for added power network design the requirements for the inter fpga network were low la tency and high bandwidth to meet the performance targets low component costs plus only marginal operational expense when servicing machines the rack configuration we target is organized into two half racks called pods each pod has its own power distribution unit and top of rack switch the pods are organized in a u arrangement of half width u servers two servers fit into each u tray based on our rack configuration we selected a two dimensional torus for the network topology this arrange ment balanced routability and cabling complexity figure shows how the torus is mapped onto a pod of machines the server motherboard routes eight high speed traces from the mezzanine connector to the back of the server chassis where the connections plug into a passive backplane the traces are exposed on the backplane as two sff sas ports we built custom cable assemblies shells of eight and six cables that plugged into each sas port and routed two high speed signals between each pair of connected fpgas at gb sig naling rates each inter fpga network link supports gb of peak bidirectional bandwidth at sub microsecond latency with no additional networking costs such as nics or switches since the server sleds are plugged into a passive backplane and the torus cabling also attaches to the backplane a server can be serviced by pulling it out of the backplane without unplugging any cables thus the cable assemblies can be installed at rack integration time tested for topological cor rectness and delivered to the datacenter with correct wiring and low probability of errors when servers are repaired datacenter deployment to test this architecture on a number of datacenter services at scale we manufactured and deployed the fabric in a production datacenter the deployment consisted of populated pods of machines in racks for a total of machines each server uses an intel xeon socket ep motherboard core sandy bridge cpus gb of dram and two ssds in addition to four hdds the machines have a gb network card connected to a port top of rack switch which in turn connects to a set of level two switches the daughtercards and cable assemblies were both tested at manufacture and again at system integration at deployment we discovered that cards had a hardware failure and that one of the links in the cable assemblies was defective since then over several months of operation we have seen no additional hardware failures infrastructure and platform architecture supporting an at scale deployment of reconfigurable hardware requires a robust software stack capable of detecting failures while providing a simple and accessible interface to software applications if developers have to worry about low level fpga details including drivers and system functions e g pcie the platform will be difficult to use and rendered in compatible with future hardware generations there are three categories of infrastructure that must be carefully designed to enable productive use of the fpga apis for interfac ing software with the fpga interfaces between fpga application logic and board level functions and support for resilience and debugging software interface applications targeting the catapult fabric share a common driver and user level interface the communication interface between the cpu and fpga must satisfy two key design goals the interface must incur low latency taking fewer than μs for transfers of kb or less and the interface must be safe for multithreading to achieve these goals we developed a custom pcie interface with dma support in our pcie implementation low latency is achieved by avoiding system calls we allocate one input and one output buffer in non paged user level memory and supply the fpga with a base pointer to the buffers physical memory addresses thread safety is achieved by dividing the buffer into slots where each slot is of the buffer and by statically assign ing each thread exclusive access to one or more slots in the case study in section we use slots of kb each each slot has a set of status bits indicating whether the slot is full to send data to the fpga a thread fills its slot with data then sets the appropriate full bit for that slot the fpga monitors the full bits and fairly selects a candidate slot for dma ing into one of two staging buffers on the fpga clearing the full bit once the data has been transferred fairness is achieved by taking periodic snapshots of the full bits and dma ing all full slots before taking another snapshot of the full bits when the fpga produces results for readback it checks to make sure that the output slot is empty and then dmas the results into the output buffer once the dma is complete the fpga sets the full bit for the output buffer and generates an interrupt to wake and notify the consumer thread to configure the fabric with a desired function user level services may initiate fpga reconfigurations through calls to a low level software library when a service is deployed each server is designated to run a specific application on its local fpga the server then invokes the reconfiguration function passing in the desired bitstream as a parameter shell architecture in typical fpga programming environments the user is of ten responsible for developing not only the application itself but also building and integrating system functions required for data marshaling host to fpga communication and inter chip fpga communication if available system integration places a significant burden on the user and can often exceed the effort needed to develop the application itself this devel opment effort is often not portable to other boards making it difficult for applications to work on future platforms motivated by the need for user productivity and design re usability when targeting the catapult fabric we logically divide all programmable logic into two partitions the shell and the role the shell is a reusable portion of programmable logic common across applications while the role is the application logic itself restricted to a large fixed region of the chip gb gb ecc so dimm ecc so dimm host cpu role inter fpga router seu figure components of the shell architecture role designers access convenient and well defined inter faces and capabilities in the shell e g pcie dram routing etc without concern for managing system correctness the shell consumes of each fpga although extra capacity can be obtained by discarding unused functions in the future partial reconfiguration would allow for dynamic switching between roles while the shell remains active even routing inter fpga traffic while a reconfiguration is taking place figure shows a block level diagram of the shell architec ture consisting of the following components two dram controllers which can be operated indepen dently or as a unified interface on the stratix v our dual rank dimms operate at mhz single rank dimms or only using one of the two ranks of a dual rank dimm can operate at mhz four high speed serial links running seriallite iii a lightweight protocol for communicating with neighboring fpgas it supports fifo semantics xon xoff flow control and ecc router logic to manage traffic arriving from pcie the role or the cores reconfiguration logic based on a modified remote status update rsu unit to read write the configuration flash the pcie core with the extensions to support dma single event upset seu logic which periodically scrubs the fpga configuration state to reduce system or applica tion errors caused by soft errors the router is a standard crossbar that connects the four inter fpga network ports the pcie controller and the ap plication role the routing decisions are made by a static software configured routing table that supports different rout ing policies the transport protocol is virtual cut through with no retransmission or source buffering shell core core config flash rsu jtag leds temp sensors xcvr reconfig mb qspi config flash pcie core dma engine application north south east west sliii sliii sliii sliii since uncorrected bit errors can cause high level disruptions requiring intervention from global management software we employ double bit error detection and single bit error correc tion on our dram controllers and links the use of ecc on our links incurs a reduction in peak bandwidth ecc on the links is performed on individual flits with cor rection for single bit errors and detection of double bit errors flits with three or more bit errors may proceed undetected through the pipeline but are likely to be detected at the end of packet transmission with a crc check double bit errors and crc failures result in the packet being dropped and not returned to the host in the event of a dropped packet the host will time out and divert the request to a higher level failure handling protocol the seu scrubber runs continuously to scrub configura tion errors if the error rates can be brought sufficiently low with conservative signaling speeds and correction the rare errors can be handled by the higher levels of software without resorting to expensive approaches such as source based re transmission or store and forward protocols the speed of the fpgas and the ingestion rate of requests is high enough that store and forward would be too expensive for the applications that we have implemented software infrastructure the system software both at the datacenter level and in each individual server required several changes to accommodate the unique aspects of the reconfigurable fabric these changes fall into three categories ensuring correct operation failure detection and recovery and debugging two new services are introduced to implement this sup port the first called the mapping manager is responsible for configuring fpgas with the correct application images when starting up a given datacenter service the second called the health monitor is invoked when there is a suspected failure in one or more systems these services run on servers within the pod and communicate through the ethernet network correct operation the primary challenge we found to ensuring correct operation was the potential for instability in the system introduced by fpgas reconfiguring while the system was otherwise up and stable these problems manifested along three dimensions first a reconfiguring fpga can appear as a failed pcie device to the host raising a non maskable interrupt that may desta bilize the system second a failing or reconfiguring fpga may corrupt the state of its neighbors across the links by randomly sending traffic that may appear valid third re configuration cannot be counted on to occur synchronously across servers so fpgas must remain robust to traffic from neighbors with incorrect or incompatible configurations e g old data from fpgas that have not yet been reconfigured the solution to a reconfiguring pcie device is that the driver that sits behind the fpga reconfiguration call must first dis able non maskable interrupts for the specific pcie device the fpga during reconfiguration the solution to the corruption of a neighboring fpga dur ing reconfiguration is more complex when remote fpgas are reconfigured they may send garbage data to prevent this data from corrupting neighboring fpgas the fpga being reconfigured sends a tx halt message indicating that the neighbors should ignore all further traffic until the link is re established in addition messages are delayed a few clock cycles so that in case of an unexpected link failure it can be detected and the message can be suppressed similarly when an fpga comes out of reconfiguration it cannot trust that its neighbors are not sending garbage data to handle this each fpga comes up with rx halt enabled automatically throwing away any message coming in on the links the mapping manager tells each server to release rx halt once all fpgas in a pipeline have been configured failure detection and recovery when a datacenter application hangs for any reason a machine at a higher level in the service hierarchy such as a machine that aggregates results will notice that a set of servers are unresponsive at that point the health monitor is invoked the health monitor queries each machine to find its status if a server is unresponsive it is put through a sequence of soft reboot hard reboot and then flagged for manual service and possible replacement until the machine starts working correctly if the server is operating correctly it responds to the health monitor with information about the health of its local fpga and associated links the health monitor returns a vector with error flags for inter fpga connections dram status bit errors and calibration failures errors in the fpga application pll lock issues pcie errors and the occurrence of a temperature shutdown this call also returns the machine ids of the north south east and west neighbors of an fpga to test whether the neighboring fpgas in the torus are acces sible and that they are the machines that the system expects in case the cables are miswired or unplugged based on this information the health monitor may update a failed machine list including the failure type updating the machine list will invoke the mapping manager which will determine based on the failure location and type where to re locate various application roles on the fabric it is possible that relocation is unnecessary such as when the failure occurred on a spare node or when simply reconfiguring the fpga in place is sufficient to resolve the hang the mapping manager then goes through its reconfiguration process for every fpga involved in that service clearing out any corrupted state and mapping out any hardware failure or a recurring failure with an unknown cause in the current fabric running accelerated search failures have been exceedingly rare we observed no hangs due to data corruption the failures that we have seen have been due to transient phenomena primarily machine reboots due to maintenance or other unresponsive services debugging support in a large scale datacenter deployment hardware bugs or faults inevitably occur at scale that escape testing and functional validation diagnosing these scenarios often requires visibility into the state of the hardware leading up to the point of failure the use of traditional interactive fpga debugging tools at scale e g altera signaltap xilinx chipscope is limited by finite buffering capacity the need to automatically recover the failed service and the impracticality of putting usb jtag units into each machine to overcome these issues we embed a lightweight always on flight data recorder that captures only salient information about the fpga during run time into on chip memory that can be streamed out via pcie at any time during the health status check the information kept in the fdr allows us to verify at scale that fpgas power on sequences were correct e g links locked properly plls and resets correctly sequenced etc and that there were no intermittent errors in addition the fdr maintains a circular buffer that records the most recent head and tail flits of all packets entering and ex iting the fpga through the router this information includes a trace id that corresponds to a specific compressed docu ment that can be replayed in a test environment the size of the transaction the direction of travel e g north to south link and other miscellaneous states about the system e g non zero queue lengths although the fdr can only capture a limited window recent events it was surprisingly effective during late stage deployment and enabled us to diagnose and resolve problems that only manifested at scale such as rare deadlock events on an stage fpga pipeline untested inputs that resulted in hangs in the stage logic intermittent server reboots and unreliable links in the future we plan to extend the fdr to perform compression of log information and to opportunistically buffer into dram for extended histories application case study to drive the requirements of our hardware platform we ported a significant fraction of bing ranking engine onto the cata pult fabric we programmed the fpga portion of the ranking engine by hand in verilog and partitioned it across seven fpgas plus one spare for redundancy thus the engine maps to rings of eight fpgas on one dimension of the torus our implementation produces results that are identical to software even reproducing known bugs with the exception of uncontrollable incompatibilities such as floating point round ing artifacts caused by out of order operations although there were opportunities for further fpga specific optimizations we decided against implementing them in favor of maintaining consistency with software bing search has a number of stages many outside the scope of our accelerated ranking service as search queries arrive at the datacenter they are checked to see if they hit in a front end cache service if a request misses in the cache it is routed to a top level aggregator tla that coordinates the processing of the query and aggregates the final result the tla sends the same query through mid level aggregators to a large number of machines performing a selection service that finds documents web pages that match the query and narrows them down to a relatively small number per machine each of those high priority documents and its query are sent to a separate machine running the ranking service the portion that we accelerate with fpgas that assigns each document a score relative to the query the scores and document ids are returned to the tla that sorts them generates the captions and returns the results the ranking service is performed as follows when a query document arrives at a ranking service server the server retrieves the document and its metadata which together is called a metastream from the local solid state drive the document is processed into several sections creating several metastreams a hit vector which describes the locations of query words in each metastream is computed a tuple is created for each word in the metastream that matches a query term each tuple describes the relative offset from the previ ous tuple or start of stream the matching query term and a number of other properties many features such as the number of times each query word occurs in the document are then computed synthetic features called free form expressions ffes are computed by arithmetically combining computed features all the features are sent to a machine learned model that generates a score that score determines the document position in the overall ranked list of documents returned to the user we implemented most of the feature computations all of the free form expressions and all of the machine learned model on fpgas what remains in software is the ssd lookup the hit vector computation and a small number of software computed features software interface while the ranking service processes document query tu ples transmitting only a compressed form of the document saves considerable bandwidth each encoded document query request sent to the fabric contains three sections i a header with basic request parameters ii the set of software computed features and iii the hit vector of query match locations for each document metastreams the header contains a number of necessary additional fields including the location and length of the hit vector the software computed features document length and number of query terms the software computed features section contains one or more pairs of feature id feature value tuples for features which are either not yet implemented on the fpga or do not make sense to implement in hardware such as document features which are independent of the query and are stored within the document figure cumulative distribution of compressed document sizes nearly all compressed documents are kb or less to save bandwidth software computed features and hit vector tuples are encoded in three different sizes using two four or six bytes depending on the query term these streams and tuples are processed by the feature extraction stage to produce the dynamic features these combined with the precomputed software features are forwarded to subsequent pipeline stages due to our slot based dma interface and given that the latency of feature extraction is proportional to tuple count we truncate compressed documents to kb this represents the only unusual deviation of the accelerated ranker from the pure software implementation but the effect on search relevance is extremely small figure shows a cdf of all document sizes in a kdoc sample collected from real world traces as shown nearly all of the compressed documents are under kb only require truncation on average documents are kb with the percentile at kb for each request the pipeline produces a single score a byte float representing how relevant the document is to the query the score travels back up the pipeline through the dedi cated network to the fpga that injected the request a pcie dma transfer moves the score query id and performance counters back to the host macropipeline the processing pipeline is divided into macropipeline stages with the goal of each macropipeline stage not exceeding μs and a target frequency of mhz per stage this means that each stage has fpga clock cycles or less to complete processing figure shows how we allocate functions to fp gas in the eight node group one fpga for feature extraction two for free form expressions one for a compression stage that increases the efficiency of the scoring engines and three to hold the machine learned scoring models the eighth fpga is a spare which allows the service manager to rotate the ring upon a machine failure and keep the ranking pipeline alive queue manager and model reload so far the pipeline descriptions assumed a single set of features free form expressions and machine learned scorer in practice however there are many different sets of features free forms cpu cpu cpu cpu figure mapping of ranking roles to fpgas on the reconfig urable fabric and scorers we call these different sets models different models are selected based on each query and can vary for language e g spanish english chinese query type or for trying out experimental models when a ranking request comes in it specifies which model should be used to score the query the query and document are forwarded to the head of the processing pipeline and placed in a queue in dram which contains all queries using that model the queue manager qm takes documents from each queue and sends them down the processing pipeline when the queue is empty or when a timeout is reached qm will switch to the next queue when a new queue i e queries that use a different model is selected qm sends a model reload command down the pipeline which will cause each stage to load the instructions and data needed to evaluate the query with the specified model model reload is a relatively expensive operation in the worst case it requires all of the embedded rams to be reloaded with new contents from dram on each board fpga there are ram blocks each with kb capacity using the high capacity dram configuration at speeds model reload can take up to μs this is an order of magnitude slower than processing a sin gle document so the queue manager role in minimizing model reloads among queries is crucial to achieving high per formance however while model reload is slow relative to document processing it is fast relative to fpga configuration or partial reconfiguration which ranges from milliseconds to seconds for the fpga actual reload times vary both by stage and by model in practice model reload takes much less than μs because not all embedded memories in the design need to be reloaded and not all models utilize all of the processing blocks on the fpgas cpu queue manager fe spare ffe scoring requests responses scoring ffe scoring compress scoring cpu cpu cpu figure the first stage of the ranking pipeline a com pressed document is streamed into the stream processing fsm split into control and data tokens and issued in paral lel to the unique feature state machines generated feature and value pairs are collected by the feature gathering net work and forward on to the next pipeline stage feature extraction the first stage of the scoring acceleration pipeline feature extraction fe calculates numeric scores for a variety of fea tures based on the query and document combination there are potentially thousands of unique features calculated for each document as each feature calculation produces a result for every stream in the request furthermore some features produce a result per query term as well our fpga accelerator offers a significant advantage over software because each of the feature extraction engines can run in parallel working on the same input stream this is effectively a form of multiple instruction single data misd computation we currently implement unique feature extraction state machines with up to features calculated and used by downstream ffe stages in the pipeline each state machine reads the stream of tuples one at a time and performs a local calculation for some features that have similar computations a single state machine is responsible for calculating values for multiple features as an example the numberofoccurences feature simply counts up how many times each term in the query appears in each stream in the document at the end of a stream the state machine outputs all non zero feature values for numberofoccurences this could be up to the number of terms in the query to support a large collection of state machines working in parallel on the same input data at a high clock rate we organize the blocks into a tree like hierarchy and replicate the input stream several times figure shows the logical organization of the fe hierarchy input data the hit vector is fed into a stream processing state machine which produces a series of control and data messages that the various feature state machines process each state machine processes the stream a rate of clock cycles per token when a state machine finishes its computation it emits one or more feature index and values that are fed into the feature gathering network figure ffe placed and routed on fpga that coalesces the results from the state machines into a single output stream for the downstream ffe stages inputs to fe are double buffered to increase throughput free form expressions free form expressions ffes are mathematical combinations of the features extracted during the feature extraction stage ffes give developers a way to create hybrid features that are not conveniently specified as feature extraction state machines there are typically thousands of ffes ranging from very simple such as adding two features to large and complex thousands of operations including conditional execution and complex floating point operators such as ln pow and divide ffes vary greatly across different models so it is impractical to synthesize customized datapaths for each expression one potential solution is to tile many off the shelf soft pro cessor cores e g nios ii but these single threaded cores are not efficient at processing thousands of threads with long latency floating point operations in the desired amount of time per macropipeline stage μs instead we developed a custom multicore processor with massive multithreading and long latency operations in mind the result is the ffe proces sor shown in figure as we will describe in more detail the ffe microarchitecture is highly area efficient allowing us to instantiate cores on a single fpga there are three key characteristics of the custom ffe pro cessor that makes it capable of executing all of the expressions within the required deadline first each core supports si multaneous threads that arbitrate for functional units on a cycle by cycle basis while one thread is stalled on a long op eration such as fpdivide or ln other threads continue to make progress all functional units are fully pipelined so any unit can accept a new operation on each cycle second rather than fair thread scheduling threads are stati cally prioritized using a priority encoder the assembler maps the expressions with the longest expected latency to thread slot on all cores then fills in slot on all cores and so forth once all cores have one thread in each thread slot the remain ing threads are appended to the end of previously mapped threads starting again at thread slot third the longest latency expressions are split across mul tiple fpgas an upstream ffe unit can perform part of the computation and produce an intermediate result called a metafeature these metafeatures are sent to the downstream cluster core core fst core core complex core core per stage injection throughput thread pcie thread thread pcie thread fe comp spare figure the maximum compressed document injection rate single and multi threaded for each individual fpga stage when operating in pcie only and loopback results are normalized to single threaded pcie throughput ffes like any other feature effectively replacing that part of the expressions with a simple feature read because complex floating point instructions consume a large amount of fpga area multiple cores typically are clustered together to share a single complex block arbitration for the block is fair with round robin priority the complex block consists of units for ln fpdiv exp and float to int pow integer divide and mod are all translated into multiple in structions by the compiler to eliminate the need for expensive dedicated units in addition the complex block contains the feature storage tile fst the fst is double buffered allow ing one document to be loaded while another is processed document scoring the last stage of the pipeline is a machine learned model evaluator which takes the features and free form expressions as inputs and produces single floating point score this score is sent back to the search software and all of the resulting scores for the query are sorted and returned to the user in sorted order as the sorted search results evaluation we evaluate the catapult fabric by deploying and measuring the bing ranking engine described in section on a bed of servers with fpgas our investigation focuses on node ring and system level experiments to understand the impact of hardware acceleration on latency and throughput we also report fpga area utilization and power efficiency node level experiments we measure each stage of the pipeline on a single fpga and inject scoring requests col lected from real world traces figure reports the average throughput of each pipeline stage normalized to the slowest stage in two loopback modes requests and responses sent over pcie and requests and responses routed through a loopback sas cable to measure the impact of link latency and throughput on performance overall the results show a significant variation in throughput across all stages although the stages devoted to scoring achieve very high processing rates the pipeline is limited by the throughput of fe throughput vs cpu threads injecting no other nodes injecting cpu threads injecting figure overall pipeline throughput increases from to threads normalized to a single thread beyond threads the throughput is limited by the slowest stage in the pipeline latency vs cpu threads injecting no other nodes injecting cpu threads injecting figure latency in the pipeline increases with the number of threads due to queuing ring level experiments single node injector in our ring level experiments we perform injection tests on a full pipeline with eight fpgas figure shows the normalized pipeline throughput when a single node in this case fe injects documents with a varying number of cpu threads as shown in figure we achieve full pipeline saturation at around cpu threads a level consistent with our node level through put experiments for the same set of conditions figure plots the normalized latency for the user level software i e between the time the ranking application injects a document and when the response is received as thread count increases hardware pipeline latency vs compressed document size figure this experiment plots the end to end hardware pipeline latency normalized to the smallest measured value against the input compressed document size n i m o t d e z i l a m r o n y c n e t a l compressed document size b aggregate throughput vs nodes injecting nodes injecting figure aggregate throughput increases almost linearly with the number of injecting nodes in this experiment only thread injects from each node node latency vs nodes injecting feature extraction spare nodes injecting figure as the number of nodes injecting thread each increases from to request latency increases slightly due to increased contention for network bandwidth between nodes figure shows the unloaded latency of the scoring pipeline versus the size of a compressed document the results show a minimum latency incurred that is proportional to the document size i e the buffering and streaming of control and data tokens along with a variable computation time needed to process the input documents ring level experiments multi node injectors we next evaluate the effect on latency and throughput when multiple servers are allowed to inject documents into a shared ranking pipeline figure shows the aggregate pipeline throughput as we increase the total number of injecting nodes when all eight servers are injecting the peak pipeline saturation is reached equal to the rate at which fe can process scoring requests under the same conditions figure shows the latencies observed by two different nodes injecting requests from the head fe and tail spare of the pipeline because the spare fpga must forward its requests along a channel shared with responses it perceives a slightly higher but negligible latency increase over fe at maximum throughput production software measurements in this section we compare the average and tail latency distributions of bing production level ranker running with and without fpgas on a bed of servers of which run the ranking service for a range of representative injection rates per server used in production figure illustrates how the fpga accelerated ranker substantially reduces the end to end scoring latency relative to software for example given a target injection rate e r a w t f o s a g p f y c n e t a l e v i t a l e average r document injection rate normalized figure the fpga ranker achieves lower average and tail latencies relative to software as the injection rate increases percentile latency vs throughput fpga software gain figure the points on the x axis at show the maximum sustained throughputs on both the fpga and software while satisfying bing target for latency at the percentile of per server the fpga reduces the worst case latency by in the percentile distribution the improvement in fpga scoring latency increases further at higher injection rates because the variability of software latency increases at higher loads due to contention in the cpu memory hierar chy while the fpga performance remains stable figure shows the measured improvement in scoring throughput while bounding the latency at the percentile distribution for the points labeled on the x axis at which represent the maximum latency tolerated by bing at the percentile the fpga achieves a gain in scoring through put relative to software given that fpgas can be used to improve both latency and throughput bing could reap the benefits in two ways for equivalent ranking capacity fewer servers can be purchased in the target above by nearly a factor of two or new capabili ties and features can be added to the software and or hardware stack without exceeding the maximum allowed latency fpga area power and frequency table shows the fpga area consumption and clock frequencies for all of the stages devoted to ranking despite the modest area consump tion and operating at clock frequencies much lower than con ventional processors the use of fpgas significantly improves throughput and latency in the long term there is substan d e z i l a m r o n t u p h g u o r h t latency normalized to percentile target fe comp spare logic ram dsp clock mhz 166 table fpga area usage and clock frequencies for each of the ranking stages tial headroom to improve both the fpga clock rate and area efficiency of our current pipeline to measure the maximum power overhead of introducing fpgas to our servers we ran a power virus bitstream on one of our fpgas i e maxing out the area and activity factor and measured a modest power consumption of w related work many other groups have worked on incorporating fpgas into cpu systems to accelerate workloads in large scale systems one challenge to developing a hybrid computing system is the integration of server class cpus with fpgas one approach is to plug the fpga directly onto the native sys tem bus for example in systems using amd hypertrans port or intel front side bus and quickpath interconnect qpi while integrating the fpga directly onto the processor bus would reduce dma latency this la tency is not the bottleneck in our application and would not significantly improve overall performance in addition attach ing the fpga to qpi would require replacing one cpu with an fpga severely impacting the overall utility of the server for applications which cannot use the fpga ibm coherence attach processor interface capi and convey hybrid core memory interconnect hcmi both enable advanced memory sharing with coherence be tween the fpga and cpu since our ranking application only requires simple memory sharing these mechanisms are not yet necessary but may be valuable for future applications instead of incorporating fpgas into the server several groups have created network attached fpga appliances that operate over ethernet or infiniband the convey hc maxeler mpc series beecube and src map station are all examples of commercial fpga acceleration appliances while the appliance model appears to be an easy way to integrate fpgas into the datacenter it breaks homo geneity and reduces overall datacenter flexibility in addition many to one network communication can result in dropped packets making the bounds on latencies much harder to guar antee finally the appliance creates a single point of failure that can disable many servers thus reducing overall reliability for these reasons we distribute fpgas across servers several large systems have also been built with distributed fpgas including the cray xd novo g and qp these systems integrate the fpga with the cpu but the fpga to fpga communication must be routed through the cpu maxwell is the most similar to our design as it di rectly connects fpgas in a d torus using infiniband cables although the fpgas do not implement routing logic these systems are targeted to hpc rather than datacenter workloads but they show the viability of fpga acceleration in large sys tems however datacenters require greater flexibility within tighter cost power and failure tolerance constraints than spe cialized hpc machines so many of the design decisions made for these systems do not apply directly to the catapult fabric fpgas have been used to implement and accelerate impor tant datacenter applications such as memcached com pression decompression k means clustering and web search pinaka and vanderbauwhede et al used fpgas to accelerate search but focused primarily on the selection stage of web search which selects which doc uments should be ranked our application focuses on the ranking stage which takes candidate documents chosen in the selection stage as the input the ffe stage is a soft processor core one of many avail able for fpgas including microblaze and nios ii unlike other soft cores ffe is designed to run a large number of threads interleaved on a cycle by cycle basis the shell role design is aimed at abstracting away the board level details from the application developer several other projects have explored similar directions including vir tualrc coram borph and leap conclusions fpgas show promise for accelerating many computational tasks but they have not yet become mainstream in commodity systems unlike gpus their traditional applications rapid asic prototyping and line rate switching are unneeded in high volume client devices and servers however fpgas are now powerful computing devices in their own right suitable for use as fine grained accelerators this paper described a large scale reconfigurable fabric intended for accelerating dat acenter services our goal in building the catapult fabric was to understand what problems must be solved to operate fpgas at scale and whether significant performance improvements are achievable for large scale production workloads when we first began this investigation we considered both fpgas and gpus as possible alternatives both classes of devices can support copious parallelism as both have hundreds to thousands of arithmetic units available on each chip we decided not to incorporate gpus because the current power requirements of high end gpus are too high for conventional datacenter servers but also because it was unclear that some latency sensitive ranking stages such as feature extraction would map well to gpus our study has shown that fpgas can indeed be used to accelerate large scale services robustly in the datacenter we have demonstrated that a significant portion of a complex datacenter service can be efficiently mapped to fpgas by using a low latency interconnect to support computations that must span multiple fpgas special care must be taken when reconfiguring fpgas or rebooting machines so that they do not crash the host server or corrupt their neighbors we described and tested a high level protocol for ensuring safety when reconfiguring one or more chips with this protocol and the appropriate fault handling mechanisms we showed that a medium scale deployment of fpgas can increase ranking throughput in a production search infrastructure by at comparable latency to a software only solution the added fpga compute boards only increased power consumption by and did not exceed our limit in the total cost of ownership of an individual server yielding a significant overall improvement in system efficiency we conclude that distributed reconfigurable fabrics are a viable path forward as increases in server performance level off and will be crucial at the end of moore law for contin ued cost and capability improvements reconfigurability is a critical means by which hardware acceleration can keep pace with the rapid rate of change in datacenter services a major challenge in the long term is programmability fpga development still requires extensive hand coding in rtl and manual tuning yet we believe that incorporating domain specific languages such as scala or opencl fpga targeted c to gates tools such as autoesl or impulse c and libraries of reusable components and design patterns will be sufficient to permit high value services to be productively targeted to fpgas for now longer term more integrated de velopment tools will be necessary to increase the programma bility of these fabrics beyond teams of specialists working with large scale service developers within ten to fifteen years well past the end of moore law compilation to a combination of hardware and software will be commonplace reconfigurable systems such as the catapult fabric presented here will be necessary to support these hybrid computation models the emergence of openflow capable switches enables exciting new network functionality at the risk of pro gramming errors that make communication less reliable the centralized programming model where a single con troller program manages the network seems to reduce the likelihood of bugs however the system is inherently distributed and asynchronous with events happening at different switches and end hosts and inevitable delays affecting communication with the controller in this pa per we present efficient systematic techniques for test ing unmodified controller programs our nice tool ap plies model checking to explore the state space of the en tire system the controller the switches and the hosts scalability is the main challenge given the diversity of data packets the large system state and the many possi ble event orderings to address this we propose a novel way to augment model checking with symbolic execu tion of event handlers to identify representative pack ets that exercise code paths on the controller we also present a simplified openflow switch model to reduce the state space and effective strategies for generating event interleavings likely to uncover bugs our proto type tests python applications on the popular nox plat form in testing three real applications a mac learning switch in network server load balancing and energy efficient traffic engineering we uncover eleven bugs introduction while lowering the barrier for introducing new func tionality into the network software defined networking sdn also raises the risks of software faults or bugs even today networking software written and exten sively tested by equipment vendors and constrained at least somewhat by the protocol standardization process can have bugs that trigger internet wide out ages in contrast programmable networks will of fer a much wider range of functionality through software created by a diverse collection of network operators and third party developers the ultimate success of sdn and enabling technologies like openflow depends on having effective ways to test applications in pursuit of achieving high reliability in this paper we present nice a tool that efficiently uncovers bugs in openflow programs through a combination of model checking and symbolic execution building on our position paper that argues for automating the testing of openflow ap plications we introduce several new contributions sum marized in section bugs in openflow applications an openflow network consists of a distributed collec tion of switches managed by a program running on a logically centralized controller as illustrated in figure each switch has a flow table that stores a list of rules for processing packets each rule consists of a pattern matching on packet header fields and actions such as forwarding dropping flooding or modifying the pack ets or sending them to the controller a pattern can re quire an exact match on all relevant header fields i e a microflow rule or have don t care bits in some fields i e a wildcard rule for each rule the switch main tains traffic counters that measure the bytes and packets processed so far when a packet arrives a switch selects the highest priority matching rule updates the counters and performs the specified action if no rule matches the switch sends the packet header to the controller and awaits a response on what actions to take switches also send event messages such as a join upon joining the network or port change when links go up or down the openflow controller un installs rules in the switches reads traffic statistics and responds to events for each event the controller program defines a handler which may install rules or issue requests for traffic statis tics many openflow are written on the nox controller platform which offers an openflow this paper we use the terms openflow application and con troller program interchangeably controller install rule packet openflow program install rule delayed host a host b switch switch figure an example of openflow network traversed by a packet in a plausible scenario due to delays between controller and switches the packet does not encounter an installed rule in the second switch api for python and c applications these programs can perform arbitrary computation and maintain arbitrary state a growing collection of controller applications support new network functionality over open flow switches available from several different vendors our goal is to create an efficient tool for systematically testing these applications more precisely we seek to discover violations of network wide correctness prop erties due to bugs in the controller programs on the surface the centralized programming model should reduce the likelihood of bugs yet the system is inherently distributed and asynchronous with events happening at multiple switches and inevitable delays af fecting communication with the controller to reduce overhead and delay applications push as much packet handling functionality to the switches as possible a common programming idiom is to respond to a packet arrival by installing a rule for handling subsequent pack ets in the data plane yet a race condition can arise if additional packets arrive while installing the rule a pro gram that implicitly expects to see just one packet may behave incorrectly when multiple arrive in addition many applications install rules at multiple switches along a path since rules are not installed atomically some switches may apply new rules before others install theirs figure shows an example where a packet reaches an intermediate switch before the relevant rule is installed this can lead to unexpected behavior where an interme diate switch directs a packet to the controller as a re sult an openflow application that works correctly most of the time can misbehave under certain event orderings challenges of testing openflow apps testing openflow applications is challenging because the behavior of a program depends on the larger envi ronment the end host applications sending and receiv ing traffic and the switches handling packets installing rules and generating events all affect the program run ning on the controller the need to consider the larger en vironment leads to an extremely large state space which explodes along three dimensions large space of switch state switches run their own programs that maintain state including the many packet processing rules and associated counters and timers fur ther the set of packets that match a rule depends on the presence or absence of other rules due to the match the highest priority rule semantics as such testing open flow applications requires an effective way to capture the large state space of the switch large space of input packets applications are data plane driven i e programs must react to a huge space of possible packets the openflow specification al lows switches to match on source and destination mac addresses ip addresses and tcp udp port numbers as well as the switch input port future generations of switches will match on even more fields the controller can perform arbitrary processing based on other fields such as tcp flags or sequence numbers as such test ing openflow applications requires effective techniques to deal with large space of inputs large space of event orderings network events such as packet arrivals and topology changes can happen at any switch at any time due to communication delays the controller may not receive events in order and rules may not be installed in order across multiple switches serializing rule installation while possible would sig nificantly reduce application performance as such test ing openflow applications requires efficient strategies to explore a large space of event orderings to simplify the problem we could require program mers to use domain specific languages that prevent cer tain classes of bugs however the adoption of new lan guages is difficult in practice not surprisingly most openflow applications are written in general purpose languages like python java alternatively developers could create abstract models of their applications and use formal methods techniques to prove properties about the system however these models are time consuming to create and easily become out of sync with the real im plementation in addition existing model checking tools like spin and java pathfinder jpf cannot be directly applied because they require explicit developer inputs to resolve the data dependency issues and sophis ticated modeling techniques to leverage domain specific information they also suffer state space explosion as we show in section instead we argue that testing tools should operate directly on unmodified openflow applications and leverage domain specific knowledge to improve scalability nice research contributions to address these scalability challenges we present nice no bugs in controller execution a tool that tests un modified controller programs by automatically generat ing carefully crafted streams of packets under many pos sible event interleavings to use nice the programmer input nice output openflow controller program network topology correctness properties state space search model checking traces of property violations symbolic execution figure given an openflow program a network topol ogy and correctness properties nice performs a state space search and outputs traces of property violations supplies the controller program and the specification of a topology with switches and hosts the programmer can instruct nice to check for generic correctness properties such as no forwarding loops or no black holes and op tionally write additional application specific correctness properties i e python code snippets that make asser tions about the global system state by default nice systematically explores the space of possible system be haviors and checks them against the desired correctness properties the programmer can also configure the de sired search strategy in the end nice outputs property violations along with the traces to deterministically re produce them the programmer can also use nice as a simulator to perform manually driven step by step sys tem executions or random walks on system states our design uses explicit state software model check ing to explore the state space of the en tire system the controller program the openflow switches and the end hosts as discussed in section however applying model checking out of the box does not scale while simplified models of the switches and hosts help the main challenge is the event handlers in the controller program these handlers are data depen dent forcing model checking to explore all possible in puts which doesn t scale or a set of important in puts provided by the developer which is undesirable instead we extend model checking to symbolically ex ecute the handlers as discussed in section by symbolically executing the packet arrival handler nice identifies equivalence classes of packets ranges of header fields that determine unique paths through the code nice feeds the network a representative packet from each class by adding a state transition that injects the packet to reduce the space of event orderings we propose several domain specific search strategies that generate event interleavings that are likely to uncover bugs in the controller program as discussed in section bringing these ideas together nice combines model checking to explore system execution paths symbolic execution to reduce the space of inputs and search strategies to reduce the space of event orderings the programmer can specify correctness properties as snip pets of python code that operate on system state or se lect from a library of common properties as discussed in section our nice prototype tests unmodified appli cations written in python for the popular nox platform as discussed in section our performance evaluation in section shows that i even on small examples nice is five times faster than approaches that apply state of the art tools ii our openflow specific search strate gies reduce the state space by up to times and iii the simplified switch model brings a fold reduction on its own in section we apply nice to three real open flow applications and uncover bugs most of the bugs we found are design flaws which are inherently less nu merous than simple implementation bugs in addition at least one of these applications was tested using unit tests section discusses the trade off between testing coverage and the overhead of symbolic execution sec tion discusses related work and section concludes the paper with a discussion of future research directions model checking openflow applications the execution of a controller program depends on the un derlying switches and end hosts the controller in turn affects the behavior of these components as such test ing is not just a simple matter of exercising every path through the controller program we must consider the state of the larger system the needs to systematically explore the space of system states and check correctness in each state naturally lead us to consider model check ing techniques to apply model checking we need to identify the system states and the transitions from one state to another after a brief review of model check ing we present a strawman approach for applying model checking to openflow applications and proceed by de scribing changes that make it more tractable background on model checking modeling the state space a distributed system con sists of multiple components that communicate asyn chronously over message channels i e first in first out buffers e g see chapter of each component has a set of variables and the component state is an assign ment of values to these variables the system state is the composition of the component states to capture in flight messages the system state also includes the contents of the channels a transition represents a change from one state to another e g due to sending a message at any given state each component maintains a set of enabled transitions i e the state possible transitions for each state the enabled system transitions are the union of en abled transitions at all components a system execution corresponds to a sequence of these transitions and thus specifies a possible behavior of the system model checking process given a model of the state space performing a search is conceptually straightfor ward figure non boxed in text shows the pseudo code of the model checking loop first the model checker initializes a stack of states with the initial state of the system at each step the checker chooses one state from the stack and one of its enabled transitions after executing that transition the checker tests the correct ness properties on the newly reached state if the new state violates a correctness property the checker saves the error and the execution trace otherwise the checker adds the new state to the set of explored states unless the state was added earlier and schedules the execution of all transitions enabled in this state if any the model checker can run until the stack of states is empty or until detecting the first error transition model for openflow apps model checking relies on having a model of the system i e a description of the state space this requires us to identify the states and transitions for each component the controller program the openflow switches and the end hosts however we argue that applying existing model checking techniques imposes too much work on the developer and leads to an explosion in the state space controller program modeling the controller as a transition system seems rel atively straightforward a controller program is struc tured as a set of event handlers e g packet arrival and switch join leave for the mac learning application in figure that interact with the switches using a stan dard interface and these handlers execute atomically as such we can model the state of the program as the values of its global variables e g ctrl state in figure and treat each event handler as a transition to execute a transition the model checker can simply invoke the asso ciated event handler for example receiving a packet in message from a switch enables the packet in transi tion and the model checker can execute the transition by invoking the corresponding event handler however the behavior of event handlers is often data dependent in line of figure for instance the packet in handler assigns mactable only for uni cast source mac addresses and either installs a forward ing rule or floods a packet depending on whether or not the destination mac address is known this leads to dif ferent system executions unfortunately model check ing does not cope well with data dependent applications e g see chapter of since enumerating all pos sible inputs is intractable a brute force solution would require developers to specify a set of relevant inputs based on their knowledge of the application hence a controller transition would be modeled as a pair con sisting of an event handler and a concrete input this is clearly undesirable nice overcomes this limitation ctrl state state of the controller is a global variable a hashtable def packet in sw id inport pkt bufid handles packet arrivals mactable ctrl state sw id is bcast src pkt src is bcast dst pkt dst if not is bcast src mactable pkt src inport if not is bcast dst and mactable has key pkt dst outport mactable pkt dst if outport inport match dl src pkt src dl dst pkt dst dl type pkt type in port inport actions output outport install rule sw id match actions soft timer hard timer permanent lines optionally send packet out sw id pkt bufid combined in api return flood packet sw id pkt bufid def switch join sw id stats handles when a switch joins if not ctrl state has key sw id ctrl state sw id def switch leave sw id handles when a switch leaves if ctrl state has key sw id del ctrl state sw id figure pseudo code of a mac learning switch based on the pyswitch application the packet in handler learns the input port associated with each non broadcast source mac address if the destination mac address is known the handler installs a forwarding rule and instructs the switch to send the packet according to that rule and otherwise floods the packet the switch join leave events initialize delete a table mapping addresses to switch ports by using symbolic execution to automatically identify the relevant inputs as discussed in section openflow switches to test the controller program the system model must include the underlying switches yet switches run com plex software and this is not the code we intend to test a strawman approach for modeling the switch is to start with an existing reference openflow switch implemen tation e g define the switch state as the values of all variables and identify transitions as the portions of the code that process packets or exchange messages with the controller however the reference switch soft ware has a large amount of state e g several hundred kb not including the buffers containing packets and openflow messages awaiting service this aggravates the state space explosion problem importantly such a large program has many sources of nondeterminism and it is difficult to identify them automatically instead we create a switch model that omits inessen tial details indeed creating models of some parts of the system is common to many standard approaches for ap plying model checking further in our case this is a one time effort that does not add burden on the user follow ing the openflow specification we view a switch as a set of communication channels transitions that handle data packets and openflow messages and a flow table simple communication channels each channel is a first in first out buffer packet channels have an optionally enabled fault model that can drop duplicate or reorder packets or fail the link the channel with the controller offers reliable in order delivery of openflow messages except for optional switch failures we do not run the openflow protocol over ssl on top of tcp ip allowing us to avoid intermediate protocol encoding de coding and the substantial state in the network stack two simple transitions the switch model supports process pkt and process of transitions for pro cessing data packets and openflow messages respec tively we enable these transitions if at least one packet channel or the openflow channel is non empty re spectively a final simplification we make is in the process pkt transition here the switch dequeues the first packet from each packet channel and processes all these packets according to the flow table so multi ple packets at different channels are processed as a single transition this optimization is safe because the model checker already systematically explores the possible or derings of packet arrivals at the switch merging equivalent flow tables a flow table can eas ily have two states that appear different but are seman tically equivalent leading to a larger search space than necessary for example consider a switch with two mi croflow rules these rules do not overlap no packet would ever match both rules as such the order of these two rules is not important yet simply storing the rules as a list would cause the model checker to treat two dif ferent orderings of the rules as two distinct states in stead as often done in model checking we construct a canonical representation of the flow table that derives a unique order of rules with overlapping patterns end hosts modeling the end hosts is tricky because hosts run ar bitrary applications and protocols have large state and have behavior that depends on incoming packets we could require the developer to provide the host pro grams with a clear indication of the transitions between states instead nice provides simple programs that act as clients or servers for a variety of protocols including ethernet arp ip and tcp these models have explicit transitions and relatively little state for instance the de fault client has two basic transitions send initially en abled can execute c times where c is configurable and receive and a counter of sent packets the default server has the receive and the send reply transi tions the latter is enabled by the former a more real istic refinement of this model is the mobile host that in cludes the move transition that moves the host to a new switch port location the programmer can also cus tomize the models we provide or create new models symbolic execution of event handlers to systematically test the controller program we must explore all of its possible transitions yet the behavior of an event handler depends on the inputs e g the mac addresses of packets in figure rather than explore all possible inputs nice identifies which inputs would exercise different code paths through an event handler systematically exploring all code paths naturally leads us to consider symbolic execution se techniques after a brief review of symbolic execution we describe how we apply symbolic execution to controller programs then we explain how nice combines model checking and symbolic execution to explore the state space effectively background on symbolic execution symbolic execution runs a program with symbolic vari ables as inputs i e any values the symbolic execution engine tracks the use of symbolic variables and records the constraints on their possible values for example in line of figure the engine learns that is bcast src is pkt src at any branch the engine queries a constraint solver for two assignments of sym bolic inputs one that satisfies the branch predicate and one that satisfies its negation i e takes the else branch and logically forks the execution to follow the feasible paths for example the engine determines that to reach line of figure the source mac address must have its eighth bit set to zero unfortunately symbolic execution does not scale well because the number of code paths can grow exponen tially with the number of branches and the size of the in puts also symbolic execution does not explicitly model the state space which can cause repeated exploration of the same system in addition despite explor ing all code paths symbolic execution does not explore all system execution paths such as different event inter leavings techniques exist that can add artificial branch ing points to a program to inject faults or explore dif ferent event orderings but at the expense of extra complexity as such symbolic execution is not a sufficient solution for testing openflow applications instead nice uses model checking to explore system execution paths and detect repeated visits to the same state and symbolic execution to determine which inputs would exercise a particular state transition symbolic execution of openflow apps applying symbolic execution to the controller event han dlers is relatively straightforward with two exceptions expensive and possibly undecidable state equivalence checks are performed first to handle the diverse inputs to the packet in handler we construct symbolic packets second to min imize the size of the state space we choose a concrete rather than symbolic representation of controller state symbolic packets the main input to the packet in handler is the incoming packet to perform symbolic execution nice must identify which ranges of packet header fields determine the path through the handler rather than view a packet as a generic array of symbolic bytes we introduce symbolic packets as our symbolic data type a symbolic packet is a group of symbolic in teger variables that each represents a header field to re duce the overhead for the constraint solver we maintain each header field as a lazily initialized individual sym bolic variable e g a mac address is a byte variable which reduces the number of variables yet we still al low byte and bit level accesses to the fields we also ap ply domain knowledge to further constrain the possible values of header fields e g the mac and ip addresses used by the hosts and switches in the system model as specified by the input topology concrete controller state the execution of the event handlers also depends on the controller state for ex ample the code in figure reaches line only for uni cast destination mac addresses stored in mactable starting with an empty mactable symbolic execution cannot find an input packet that forces the execution of line yet with a non empty table certain packets could trigger line to run while others would not as such we must incorporate the global variables into the sym bolic execution we choose to represent the global vari ables in a concrete form we apply symbolic execution by using these concrete variables as the initial state and by marking as symbolic the packets and statistics argu ments to the handlers the alternative of treating the con troller state as symbolic would require a sophisticated type sensitive analysis of complex data structures e g which is computationally expensive and difficult for an untyped language like python combining se with model checking with all of nice parts in place we now describe how we combine model checking to explore system ex ecution paths and symbolic execution to reduce the space of inputs at any given controller state we want to identify the packets that each client should send specifically the set of packets that exercise all feasible code paths on the controller in that state to do so we create a special client transition called discover packets that symbolically executes the packet in handler figure shows the unfolding of controller state space graph symbolic execution of the handler starts from the initial state defined by i the concrete controller state client state client state client send pkt client discover_packets transition figure example of how nice identifies relevant packets and uses them as new enabled send packet transitions of client state controller state inport state symbolic execution of handler new relevant packets pkt enable new transitions pkt client client send pkt send pkt for clarity the circled states refer to the controller state only e g state in figure and ii a concrete con text i e the switch and input port that identify the client location for every feasible code path in the handler the symbolic execution engine finds an equiv alence class of packets that exercise it for each equiva lence class we instantiate one concrete packet referred to as the relevant packet and enable a corresponding send transition for the client while this example fo cuses on the packet in handler we apply similar tech niques to deal with traffic statistics by introducing a spe cial discover stats transition that symbolically ex ecutes the statistics handler with symbolic integers as ar guments other handlers related to topology changes operate on concrete inputs e g the switch and port ids figure shows the pseudo code of our search space algorithm which extends the basic model checking loop in two main ways initialization lines for each client the algo rithm i creates an empty map for storing the relevant packets for a given controller state and ii enables the discover packets transition checking process lines upon reaching a new state the algorithm checks for each client line whether a set of relevant packets already exists if not it enables the discover packets transition in addition it checks line if the controller has a process stats transition enabled in the newly reached state meaning that the controller is awaiting a response to a previous query for statistics if so the al gorithm enables the discover stats transition invoking the discover packets lines and discover stats lines transitions allows the system to evolve to a state where new transitions be come possible one for each path in the packet arrival or statistics handler this allows the model checker to reach new controller states allowing symbolic execution to again uncover new classes of inputs that enable addi tional transitions and so on state stack explored states errors initial state create initial state for client in initial state clients client packets client enable transition discover packets for t in initial state enabled transitions state stack push initial state t while len state stack state transition choose state stack try next state run state transition ctrl next state ctrl reference to controller in next state ctrl state state ctrl stringified controller state in next state for client in state clients if not client packets has key ctrl state client enable transition discover packets ctrl if process stats in ctrl enabled transitions ctrl enable transition discover stats state sw id check properties next state if next state not in explored states explored states add next state for t in next state enabled transitions state stack push next state t except propertyviolation as e errors append e trace def discover packets transition client ctrl sw id inport switch location of client new packets symbolicexecution ctrl packet in context sw id inport client packets state ctrl new packets for packet in client packets state ctrl client enable transition send packet def discover stats transition ctrl state sw id new stats symbolicexecution ctrl process stats context sw id for stats in new stats ctrl enable transition process stats stats figure pseudo code of the state space search algorithm used in nice for finding errors the highlighted parts in cluding the special discover transitions are our additions to the basic model checking loop by symbolically executing the controller event han dlers nice can automatically infer the test inputs for enabling model checking without developer input at the expense of some limitations in coverage of the system state space which we discuss later in section openflow specific search strategies even with our optimizations from the last two sections the model checker cannot typically explore the entire state space since this may be prohibitively large or even infinite thus we propose domain specific heuristics that substantially reduce the space of event orderings while focusing on scenarios that are likely to uncover bugs most of the strategies operate on the event inter leavings produced by model checking except for pkt seq which reduces the state space explosion due to the transitions uncovered by symbolic execution pkt seq relevant packet sequences the effect of discovering new relevant packets and using them as new enabled send transitions is that each end host gener ates a potentially unbounded tree of packet sequences to make the state space finite and smaller this heuris tic reduces the search space by bounding the possible end host transitions indirectly bounding the tree along two dimensions each of which can be fine tuned by the user the first is merely the maximum length of the se quence or in other words the depth of the tree effec tively this also places a hard limit to the issue of infi nite execution trees due to symbolic execution the sec ond is the maximum number of outstanding packets or in other words the length of a packet burst for example if client in figure is allowed only a packet burst this heuristic would disallow both send pkt in state and send pkt in state effectively this limits the level of packet concurrency within the state space to introduce this limit we assign each end host with a counter c when c the end host cannot send any more packet until the counter is replenished as we are dealing with communicating end hosts we adopt as de fault behavior to increase c by one unit for every received packet however this behavior can be modified in more complex end host models e g to mimic the tcp flow and congestion controls no delay instantaneous rule updates when us ing this simple heuristic nice treats each communi cation between a switch and the controller as a single atomic action i e not interleaved with any other transi tions in other words the global system runs in lock step this heuristic is useful during the early stages of development to find basic design errors rather than race conditions or other concurrency related problems for instance this heuristic would allow the developer to re alize that installing a rule prevents the controller from seeing other packets that are important for program cor rectness for example a mac learning application that installs forwarding rules based only on the destination mac address would prevent the controller from seeing some packets with new source mac addresses unusual uncommon delays and reorderings with this heuristic nice only explores event orderings with unusual and unexpected delays with the goal of un covering race conditions for example if an event han dler in the controller installs rules in switches and the heuristic explores transitions that reverse the order by allowing switch to install its rule first followed by switch and then switch this heuristic uncovers bugs like the example in figure flow ir flow independence reduction many openflow applications treat different groups of packets independently that is the handling of one group is not affected by the presence or absence of another in this case nice can reduce the search space by exploring only one relative ordering between the events affecting each group to use this heuristic the programmer pro vides issameflow a python function that takes two packets and the switch and input port as arguments and returns whether the packets belong to the same group for example in some scenarios different microflows are independent whereas other programs may treat packets with different destination mac addresses independently summary pkt seq is complementary to other strate gies in that it only reduces the number of send tran sitions rather than the possible kind of event orderings pkt seq is enabled by default and used in our experi ments unless otherwise noted the other heuristics can be selectively enabled specifying application correctness correctness is not an intrinsic property of a system a specification of correctness states what the system should or should not do whereas the implementation deter mines what it actually does nice allows programmers to specify correctness properties as python code snippets and provides a library of common properties e g no for warding loops or blackholes customizable correctness properties testing correctness involves asserting safety properties something bad never happens and liveness prop erties eventually something good happens defined more formally in chapter of checking for safety properties is relatively easy though sometimes writing an appropriate predicate over all state variables is te dious as a simple example a predicate could check that the collection of flow rules does not form a forward ing loop or a black hole checking for liveness proper ties is typically harder because of the need to consider a possibly infinite system execution in nice we make the inputs finite e g a finite number of packets each with a finite set of possible header values allowing us to check some liveness properties for example nice could check that once two hosts exchange at least one packet in each direction no further packets go to the con troller a property we call strictdirectpaths checking this liveness property requires knowledge not only of the system state but also which transitions have executed to check both safety and liveness properties nice al lows correctness properties to i access the system state ii register callbacks invoked by nice to observe im portant transitions in system execution and iii main tain local state in our experience these features offer enough expressiveness for specifying correctness prop erties for ease of implementation these properties are represented as snippets of python code that make as sertions about global system state nice invokes these snippets after each transition for example to check the strictdirectpaths property the code snippet would have local state variables that keep track of whether a pair of hosts has exchanged at least one packet in each direc tion and would flag a violation if a subsequent packet triggers a packet in event at the controller when a correctness check signals a violation the tool records the execution trace that recreates the problem library of correctness properties nice provides a library of correctness properties appli cable to a wide range of openflow applications a pro grammer can select properties from a list as appropriate for the application writing these correctness modules can be challenging because the definitions must be ro bust to communication delays between the switches and the controller many of the definitions must intentionally wait until a safe time to test the property to prevent natural delays from erroneously triggering a violation of the property providing these modules as part of nice can relieve the developers from the challenges of spec ifying correctness properties precisely though creating any custom modules would require similar care noforwardingloops this property asserts that pack ets do not encounter forwarding loops and is imple mented by checking that each packet goes through any given switch input port pair at most once noblackholes this property states that no packets should be dropped in the network and is implemented by checking that every packet that enters the network ul timately leaves the network or is consumed by the con troller itself for simplicity we disable optional packet drops and duplication on the channels to account for flooding the property enforces a zero balance between the packet copies and packets consumed directpaths this property checks that once a packet has successfully reached its destination future packets of the same flow do not go to the controller effectively this checks that the controller successfully establishes a direct path to the destination as part of handling the first packet of a flow this property is useful for many open flow applications though it does not apply to the mac learning switch which requires the controller to learn how to reach both hosts before it can construct unicast forwarding paths in either direction strictdirectpaths this property checks that after two hosts have successfully delivered at least one packet of a flow in each direction no successive packets reach the controller this checks that the controller has established a direct path in both directions between the two hosts noforgottenpackets this property checks that all switch buffers are empty at the end of system execu tion a program can easily violate this property by for getting to tell the switch how to handle a packet this can eventually consume all the available buffer space for packets awaiting controller instruction after a timeout the switch may discard these buffered packets a short running program may not run long enough for the queue of awaiting controller response packets to fill but the noforgottenpackets property easily detects these bugs implementation highlights we have built a prototype implementation of nice writ ten in python so as to seamlessly support openflow con troller programs for the popular nox controller platform which provides an api for python as a result of using python we face the challenge of doing symbolic execution for a dynamic untyped lan guage this task turned out to be quite challenging from an implementation perspective to avoid modifying the python interpreter we implement a derivative technique of symbolic execution called concolic execution which executes the code with concrete instead of sym bolic inputs alike symbolic execution it collects con straints along code paths and tries to explore all feasible paths another consequence of using python is that we incur a significant performance overhead which is the price for favoring usability we plan to improve perfor mance in a future release of the tool nice consists of three parts i a model checker ii a concolic execution engine and iii a collection of models including the simplified switch and several end hosts we now briefly highlight some of the implementa tion details of the first two parts the model checker and concolic engine which run as different processes model checker details to checkpoint and restore system state nice takes the approach of remembering the sequence of transitions that created the state and re stores it by replaying such sequence while leveraging the fact that the system components execute deterministi cally state matching is doing by comparing and storing hashes of the explored states the main benefit of this ap proach is that it reduces memory consumption and sec ondarily it is simpler to implement trading computa tion for memory is a common approach for other model checking tools e g to create state hashes nice serializes the state via the cpickle module and ap plies the built in hash function to the resulting string concolic execution details a key step in concolic ex ecution is tracking the constraints on symbolic variables during code execution to achieve this we first imple ment a new symbolic integer data type that tracks as signments changes and comparisons to its value while behaving like a normal integer from the program point of view we also implement arrays tuples in python ter minology of these symbolic integers second we reuse stands for concrete symbolic the python modules that naturally serve for debugging and disassembling the byte code to trace the program ex ecution through the python interpreter further before running the code symbolically we nor malize and instrument it since in python the execu tion can be traced at best with single code line granu larity specifically we convert the source code into its abstract syntax tree ast representation and then ma nipulate this tree through several recursive passes that perform the following transformations i we split com posite branch predicates into nested if statements to work around shortcut evaluation ii we move function calls before conditional expressions to ease the job for the stp constraint solver iii we instrument branches to inform the concolic engine on which branch is taken iv we substitute the built in dictionary with a special stub that exposes the constraints and v we intercept and remove sources of nondeterminism e g seeding the pseudo random number generator the ast tree is then converted back to source code for execution performance evaluation here we present an evaluation of how effectively nice copes with the large state space in openflow experimental setup we run the experiments on the simple topology of figure where the end hosts behave as follows host a sends a layer ping packet to host b which replies with a packet to a the controller runs the mac learning switch program of figure we re port the numbers of transitions and unique states and the execution time as we increase the number of concurrent pings a pair of packets we run all our experiments on a machine set up with linux that has gb of ram and a clock speed of ghz our prototype implementation does not yet make use of multiple cores benefits of simplified switch model we first perform a full search of the state space using nice as a depth first search model checker nice mc without symbolic ex ecution and compare to no switch reduction doing model checking without a canonical representa tion of the switch state effectively this prevents the model checker from recognizing that it is exploring se mantically equivalent states these results shown in table are obtained without using any of our search strategies we compute ρ a metric of state space re duction due to using the simplified switch model as unique no switch reduction unique nice mc unique no switch reduction we observe the following in both samples the number of transitions and of unique states grow roughly exponentially as expected however using the simplified switch model the unique states explored in nice mc only grow with a rate that is about half the one observed for no switch reduction nice mc no switch reduction pings transitions unique states cpu time transitions unique states cpu time ρ 474 257 391 m 979 m 853 h table dimensions of exhaustive search in nice mc vs model checking without a canonical representation of the switch state which prevents recognizing equivalent states symbolic execution is turned off in both cases no switch reduction did not finish with five pings in four days the efficiency in state space reduction ρ scales with the problem size number of pings and is substantial factor of seven for three pings heuristic based search strategies figure illustrates the contribution of no delay and flow ir in reduc ing the search space relative to the metrics reported for the full search nice mc we omit the results for un usual as they are similar the state space reduction is again significant about factor of four for three pings in summary our switch model and these heuristics result in a fold state space reduction for three pings comparison to other model checkers next we con trast nice mc with two state of the art model check ers spin and jpf we create system models in promela and java that replicate as closely as possible the system tested in nice due to space limitations we only briefly summarize the results and refer to for the details as expected by using an abstract model of the system spin performs a full search more efficiently than nice of course state space explosion still occurs e g with pings spin runs of out memory this validates our decision to maintain hashes of system states instead of keeping entire system states spin partial order reduction por decreases the growth rate of explored transitions by only this is because even the finest granularity at which por can be applied does not distinguish between independent flows taken as is jpf is already slower than nice by a factor of with pings the reason is that jpf uses java threads to represent system concurrency however jpf leads to too many possible thread interleavings to explore even in our small example even with our extra effort in rewriting the java model to explicitly expose possible transitions jpf is times slower than nice using pings these results suggest that nice in comparison to the other model checkers strikes a good balance between i capturing system concurrency at the right level of granu larity ii simplifying the state space and iii allowing testing of unmodified controller programs is a well known technique for avoiding exploring unneces sary orderings of transitions e g n o i t no delay transitions c u d e r flow ir transitions no delay cpu time flow ir cpu time number of pings figure relative state space search reduction of our heuristic based search strategies vs nice mc experiences with real applications in this section we report on our experiences apply ing nice to three real applications a mac learning switch a server load balancer and energy aware traffic engineering and uncovering eleven bugs mac learning switch pyswitch our first application is the pyswitch software included in the nox distribution loc the application im plements mac learning coupled with flooding to un known destinations common in ethernet switches re alizing this functionality seems straightforward e g the pseudo code in figure yet nice automatically de tects three violations of correctness properties bug i host unreachable after moving this fairly subtle bug is triggered when a host b moves from one lo cation to another before b moves host a starts stream ing to b which causes the controller to install a forward ing rule when b moves the rule stays in the switch as long as a keeps sending traffic because the soft timeout does not expire as such the packets do not reach b new location this serious correctness bug violates the noblackholes property if the rule had a hard timeout the application would eventually flood packets and reach b at its new location then b would send return traffic that would trigger mac learning allowing future pack ets to follow a direct path to b while this bug fix pre vents persistent packet loss the network still experiences transient loss until the hard timeout expires designing a new noblackholes property that is robust to transient loss is part of our ongoing work bug ii delayed direct path the pyswitch also vi olates the strictdirectpaths property leading to subop timal performance the violation arises after a host a sends a packet to host b and b sends a response packet to a this is because pyswitch installs a forwarding rule in one direction from the sender b to the desti nation a in line of figure the controller does not install a forwarding rule for the other direction until seeing a subsequent packet from a to b for a three way packet exchange e g a tcp handshake this per formance bug directs more traffic than necessary to the controller anecdotally fixing this bug can eas ily introduce another one the naıve fix is to add an other install rule call with the addresses and ports reversed after line for forwarding packets from a to b however since the two rules are not installed atomically installing the rules in this order can allow the packet from b to reach a before the switch installs the second rule this can cause a subsequent packet from a to reach the controller unnecessarily a correct fix would install the rule for traffic from a first before al lowing the packet from b to a to traverse the switch with this fix the resulting program satisfies the strict directpaths property bug iii excess flooding when we test pyswitch on a topology that contains a cycle the program violates the noforwardingloops property this is not surprising since pyswitch does not construct a spanning tree web server load balancer data centers rely on load balancers to spread incoming requests over service replicas previous work created a load balancer application that uses wildcard rules to di vide traffic based on the client ip addresses to achieve a target load distribution the application can dy namically adjust the load distribution by installing new wildcard rules during the transition old transfers com plete at their existing servers while new requests are han dled according to the new distribution we test this ap plication with one client and two servers connected to a single switch the client opens a tcp connection to a virtual ip address corresponding to the two replicas in addition to the default correctness properties we create an application specific property flowaffinity that verifies that all packets of a single tcp connection go to the same server replica here we report on the bugs nice found in the original code loc which had already been unit tested to some extent bug iv next tcp packet always dropped after re configuration having observed a violation of the no forgottenpackets property we identified a bug where the application neglects to handle the next packet of each flow for both ongoing transfers and new requests after a change in the load balancing policy despite cor rectly installing the forwarding rule for each flow the application does not instruct the switch to forward the packet that triggered the packet in handler since the tcp sender ultimately retransmits the lost packet the program does successfully handle each web request making it hard to notice the bug the bug degrades per formance and for a long execution trace would ulti mately exhaust the switch space for buffering packets awaiting controller action bug v some tcp packets dropped after reconfig uration after fixing bug iv nice detected another noforgottenpackets violation due to a race condition in switching from one load balancing policy to another the application sends multiple updates to the switch for each existing rule i a command to remove the existing for warding rule followed by ii commands to install one or more rules one for each group of affected client ip addresses that direct packets to the controller since these commands are not executed atomically packets ar riving between the first and second step do not match either rule the openflow specification prescribes that packets that do not match any rule should go to the con troller although the packets go to the controller either way these packets arrive with a different reason code i e no match as written the packet in handler ignores such unexpected packets causing the switch to hold them until the buffer fills this appears as packet loss to the end hosts to fix this bug the program should reverse the two steps installing the new rules perhaps at a lower priority before deleting the existing ones bug vi arp packets forgotten during address res olution another noforgottenpackets violation uncov ered two bugs that are similar in spirit to the previous one the controller program handles client arp requests on behalf of the server replicas despite sending the cor rect reply the program neglects to discard the arp re quest packets from the switch buffer a similar problem occurs for server generated arp messages bug vii duplicate syn packets during transitions a flowaffinity violation detected a subtle bug that arises only when a connection experiences a duplicate e g re transmitted syn packet while the controller changes from one load balancing policy to another during the transition the controller inspects the next packet of each flow and assumes a syn packet implies the flow is new and should follow the new load balancing policy under duplicate syn packets some packets of a connec tion arriving before the duplicate syn may go to one server and the remaining packets to another leading to a broken connection the authors of acknowledged this possibility see footnote in their paper but only realized this was a problem after careful consideration energy efficient traffic engineering openflow enables a network to reduce energy consump tion by selectively powering down links and redi recting traffic to alternate paths during periods of lighter load response pre computes several routing ta bles the default is two and makes an online selection for each flow the nox implementation loc has an always on routing table that can carry all traffic un der low demand and an on demand table that serves ad ditional traffic under higher demand under high load the flows should probabilistically split evenly over the two classes of paths the application learns the link utilizations by querying the switches for port statistics upon receiving a packet of a new flow the packet in handler chooses the routing table looks up the list of switches in the path and installs a rule at each hop for testing with nice we install a network topology with three switches in a triangle one sender host at one switch and two receivers at another switch the third switch lies on the on demand path we define the fol lowing application specific correctness property usecorrectroutingtable this property checks that the controller program upon receiving a packet from an ingress switch issues the installation of rules to all and just the switches on the appropriate path for that packet as determined by the network load enforcing this prop erty is important because if it is violated the network might be configured to carry more traffic than it physi cally can degrading the performance of end host appli cations running on top of the network nice found several bugs in this application bug viii the first packet of a new flow is dropped a violation of noforgottenpackets revealed a bug that is almost identical to bug iv the packet in handler installed a rule but neglected to instruct the switch to for ward the packet that triggered the event bug ix the first few packets of a new flow can be dropped after fixing bug viii nice detected an other noforgottenpackets violation at the second switch in the path since the packet in handler installs an end to end path when the first packet of a flow enters the network the program implicitly assumes that intermedi ate switches would never direct packets to the controller however with communication delays in installing the rules the packet could reach the second switch before the rule is installed although these packets trigger packet in events the handler implicitly ignores them causing the packets to buffer at the intermediate switch this bug is hard to detect because the problem only arises under certain event orderings simply installing the rules in the reverse order from the last switch to the first is not sufficient differences in the delays for installing the rules could still cause a packet to encounter a switch that has not yet installed the rule a correct fix should ei ther handle packets arriving at intermediate switches or use barriers where available to ensure that rule instal lation completes at all intermediate hops before allowing the packet to depart the ingress switch bug x only on demand routes used under high load nice detects a correctroutingtableused vio lation that prevents on demand routes from being used properly the program updates an extra routing table in bug pkt seq only no delay flow ir unusual i ii iii 01 01 iv 66 v missed vi 07 vii 39m missed viii ix 02 02 x missed 2367 xi missed table comparison of the number of transitions running time to the first violation that uncovered each bug time is in seconds unless otherwise noted the port statistic handler when the network perceived energy state changes to either always on or on demand in an effort to let the remainder of the code simply ref erence this extra table when deciding where to route a flow unfortunately this made it impossible to split flows equally between always on and on demand routes and the code directed all new flows over on demand routes under high load a fix was to abandon the extra table and choose the routing table on per flow basis bug xi packets can be dropped when the load re duces after fixing bug ix nice detected another vi olation of the noforgottenpackets when the load re duces the program recomputes the list of switches in each always on path under delays in installing rules a switch not on these paths may send a packet to the con troller which ignores the packet because it fails to find this switch in any of those lists overhead of running nice in table we summarize how many seconds nice took and how many state transitions were explored to dis cover the first property violation that uncovered each bug under four different search strategies note the num bers are generally small because nice quickly produces simple test cases that trigger the bugs one exception bug vii is found in hour by doing a pkt seq only search but unusual can detect it in just minutes our search strategies are also generally faster than pkt seq only to trigger property violations except in one case bug iv also note that there are no false positives in our case studies every property violation is due to the manifestation of a bug and only in few cases bug v bug vii bug x and bug xi the heuristic based strategies experience false negatives expectedly no delay which does not consider rule installation delays misses race condition bugs missed bugs bug vii is missed by flow ir because the duplicate syn is treated as a new independent flow missed bugs finally the reader may find that some of the bugs we found like persistently leaving some packets in the switch buffer are relatively simple and their manifesta tions could be detected with run time checks performed by the controller platform however the programmer would not know what caused them for example a run time check that flags a no forgotten packets error due to bug iv or bug v would not tell the programmer what was special about this particular system execution that triggered the error subtle race conditions are very hard to diagnose so having a preferably small example trace like nice produces is crucial coverage vs overhead trade offs testing is inherently incomplete walking a fine line be tween good coverage and low overhead as part of our ongoing work we want to explore further how to best leverage symbolic execution in nice we here discuss some limitations of our current approach concrete execution on the switch in identifying the equivalence classes of packets the algorithm in fig ure implicitly assumes the packets reach the controller however depending on the rules already installed in the switch some packets in a class may reach the controller while others do not this leads to two limitations first if no packets in an equivalence class would go to the controller generating a representative packet from this class was unnecessary this leads to some loss in ef ficiency second if some but not all packets go to the controller we may miss an opportunity to test a code path through the handler by inadvertently generating a packet that stays in the fast path through the switches this leads to some loss in both efficiency and coverage we could overcome these limitations by extending symbolic execution to include our simplified switch model and performing symbolic packet forwarding across mul tiple switches we chose not to pursue this approach because i symbolic execution of the flow table code would lead to a path explosion problem ii including these variables would increase the overhead of the con straint solver and iii rules that modify packet headers would further complicate the symbolic analysis still we are exploring symbolic forwarding as future work concrete global controller variables in symbolically executing each event handler nice could miss com plex dependencies between handler invocations this is a byproduct of our decision to represent controller vari ables in a concrete form in some cases one call to a han dler could update the variables in a way that affects the symbolic execution of a second call to the same handler or a different one symbolic execution of the second handler would start from the concrete global variables and may miss an opportunity to recognize additional con straints on packet header fields we could overcome this limitation by running symbolic execution across multi ple handler invocations at the expense of a significant explosion in the number of code paths or we could re visit our decision to represent controller variables in a concrete form as future work we are considering ways to efficiently represent global variables symbolically infinite execution trees in symbolic execution de spite its many advantages symbolic execution can lead to infinite execution trees in our context an infi nite state space arises if each state has at least one in put that modifies the controller state this is an inher ent limitation of symbolic execution whether applied independently or in conjunction with model checking to address this limitation we explicitly bound the state space by limiting the size of the input e g a limit on the number of packets and devise openflow specific search strategies that explore the system state space efficiently these heuristics offer a tremendous improvement in effi ciency at the expense of some loss in coverage related work bug finding while model checking and sym bolic execution are automatic techniques a drawback is that they typically require a closed sys tem i e a system model together with its environ ment typically the creation of such environment is a manual process e g nice re uses the idea of model checking systematic state space exploration and combines it with the idea of symbolic execution exhaustive path coverage to avoid pushing the burden of modeling the environment on the user also nice is the first to demonstrate the applicability of these tech niques for testing the dynamic behavior of openflow networks finally nice makes a contribution in man aging state space explosion for this specific domain khurshid et al enable a model checker to per form symbolic execution both our and their work share the spirit of using symbolic variables to represent data from very large domains our approach differs in that it uses symbolic execution in a selective way for uncov ering possible transitions given a certain controller state as a result we i reduce state space explosion due to feasible code paths because not all code is symbolically executed and ii enable matching of concrete system states to further reduce the search of the state space openflow and network testing frenetic is a domain specific language for openflow that aims to eradicate a large class of programming faults using fre netic requires the network programmer to learn exten sions to python to support the higher layer abstractions ofrewind enables recording and replay of events for troubleshooting problems in production networks due to closed source network devices however it does not automate the testing of openflow controller programs mai et al use static analysis of network devices forwarding information bases to uncover problems in the data plane flowchecker applies symbolic model checking techniques on a manually constructed network model based on binary decision diagrams to detect mis configurations in openflow forwarding tables we view these works as orthogonal to ours since they both aim to analyze a snapshot of the data plane bishop et al examine the problem of testing the specification of end host protocols nice tests the net work itself in a new domain of software defined net works kothari et al use symbolic execution and developer input to identify protocol manipulation at tacks for network protocols in contrast nice combines model checking with symbolic execution to identify rel evant test inputs for injection into the model checker conclusion we built nice a tool for automating the testing of open flow applications that combines model checking and concolic execution in a novel way to quickly explore the state space of unmodified controller programs writ ten for the popular nox platform further we devised a number of new domain specific techniques for miti gating the state space explosion that plagues approaches such as ours we contrast nice with an approach that applies off the shelf model checkers to the openflow domain and demonstrate that nice is five times faster even on small examples we applied nice to imple mentations of three important applications and found bugs a release disciplined approximate programming lets programmers declare which parts of a program can be computed approximately and con sequently at a lower energy cost the compiler proves statically that all approximate computation is properly isolated from precise computation the hardware is then free to selectively apply ap proximate storage and approximate computation with no need to perform dynamic correctness checks in this paper we propose an efficient mapping of disciplined approximate programming onto hardware we describe an isa ex tension that provides approximate operations and storage which give the hardware freedom to save energy at the cost of accuracy we then propose truffle a microarchitecture design that efficiently supports the isa extensions the basis of our design is dual voltage operation with a high voltage for precise operations and a low voltage for approximate operations the key aspect of the microar chitecture is its dependence on the instruction stream to determine when to use the low voltage we evaluate the power savings poten tial of in order and out of order truffle configurations and explore the resulting quality of service degradation we evaluate several ap plications and demonstrate energy savings up to categories and subject descriptors c other architecture styles c computer systems organization hardware software interfaces general terms design performance keywords architecture disciplined approximate computation power aware computing energy introduction energy consumption is a first class concern in computer systems design potential benefits go beyond reduced power demands in servers and longer battery life in mobile devices reducing power consumption is becoming a requirement due to limits of device scaling in what is termed the dark silicon problem prior work has made significant progress in various aspects of energy efficiency hardware optimizations include power gating voltage and frequency scaling and sub threshold operation with permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee asplos march london england uk copyright c acm 03 error correction software efforts have explored managing energy as an explicit resource shutting down unnecessary hardware and energy aware compiler optimizations raising energy concerns to the programming model can enable a new space of energy savings opportunities trading off quality of service is one technique for reducing energy usage allowing computation to be approximate can lead to significant energy savings because it alleviates the correctness tax imposed by the wide safety margins on typical designs in deed prior work has investigated hardware and algorithmic tech niques for approximate execution most applica tions amenable to energy accuracy trade offs e g vision machine learning data analysis games etc have approximate components where energy savings are possible and precise components whose correctness is critical for application invariants recent work has explored language level techniques to assist programmers in identifying soft slices the parts of programs that may be safely subjected to approximate computation the hardware is free to use approximate storage and computation for the soft slices without performing dynamic safety checks broadly we advocate co designing hardware support for approximation with an associated programming model to enable new energy efficiency improvements while preserving programmability in this paper we explore how to map disciplined approximate programming models down to an approximation aware microarchi tecture our architecture proposal includes an isa extension which allows a compiler to convey what can be approximated along with microarchitectural extensions to typical in order and out of order processors that implement the isa our microarchitecture proposal relies on a dual voltage supply for sram arrays and logic a high v dd leading to accurate operation and a low v dd leading to ap proximate but lower power operation we discuss the implemen tation of the core structures using dual voltage primitives as well as dynamic instruction based control of the voltage supply we eval uate the energy consumption and quality of service degradation of our proposal using a variety of benchmarks including a game en gine a raytracer and scientific algorithms the remainder of this paper is organized as follows section describes our isa proposal and its support for an efficient microar chitecture via tight coupling with static compiler guarantees sec tion then explains microarchitecture implementation alternatives section follows up with a detailed description of our dual voltage microarchitecture and how the instruction stream can control dy namic voltage selection section is our evaluation of power sav ings and quality of service degradation sections and discuss related work and conclude an isa for disciplined approximate computation with disciplined approximate programming a program is decom posed into two components one that runs precisely with the typical semantics of conventional computers and another that runs approx imately carrying no guarantees but only an expectation of best effort computation many applications such as media processing and machine learning algorithms can operate reliably even when errors can occur in portions of them floating point data for example is by nature imprecise so many fp heavy applications have inherent tolerance to error an architecture sup porting disciplined approximation can take advantage of relaxed precision requirements to expose errors that would otherwise need to be prevented or corrected at the cost of energy by specifying the portion of the application that is tolerant to error the programmer gives the architecture permission to expose faults when running that part of the program we propose an isa that enables a compiler to communicate where approximation is allowed our isa design is defined by two guiding principles the isa should provide an abstract notion of approximation by replacing guarantees with informal expectations and the isa may be unsafe blindly trusting the programming language and compiler to enforce safety invariants statically replacing guarantees with expectations isas normally provide formal guarantees for operations e g an add instruction must produce the sum of its operands approximate operations how ever only carry an expectation that a certain operation will be car ried out correctly the result is left formally undefined for example an approximate add instruction might leave the contents of the output register formally undefined but specify an expectation that the result will approximate the sum of the operands the compiler and programmer may not rely on any particular pattern or method of approximation informally however they may expect the ap proximate addition to be useful in soft computation that requires summation the lack of strict guarantees for approximate computation is essential for abstract approximation instructions do not specify which particular energy saving techniques are used they only spec ify where approximation may be applied consequently a fully precise computer is a valid implementation of an approximation aware isa compilers for such isas can without modification take advantage of new approximation techniques as they are imple mented by providing no guarantees for approximate computation the isa permits a full range of approximation techniques by leaving the kind and degree of approximation unspecified an approximation aware isa could pose challenges for portabil ity different implementations of the isa can provide different er ror distributions for approximate operations to address this issue implementations can allow software control of implementation pa rameters such as voltage see section profiling and tuning mech anisms could then discover optimal settings for these hardware pa rameters at application deployment time this tuning would allow a single application to run at the same level of quality across widely varying approximation implementations responsibilities of the language and compiler an architecture supporting approximate computing requires collaboration from the rest of the system stack namely the architecture relegates con cerns of safety and programmability to the language and compiler in order to be usable approximate computation must be exposed to the programmer in a way that reduces the chance of catastrophic failures and other unexpected consequences these concerns while important can be relegated to the compiler programming lan guage and software engineering tools enerj is a programming language supporting disciplined approximation using a type system enerj provides a static non interference guarantee ensuring that the approximate part of a pro gram cannot affect the precise portion in effect enerj enforces separation between the error tolerant and error sensitive parts of a program identifying and isolating the parts that may be subject to relaxed execution semantics this strict separation brings safety and predictability to programming with approximation because it is static the non interference guarantee requires no runtime check ing freeing the isa and its implementation from any need for safety checks that would themselves impose overheads in perfor mance energy and design complexity in this paper we assume that a language like enerj is used to provide safety guarantees statically to the programmer the isa must only expose approximation as an option to the compiler it does not provide dynamic invariant checks error recovery or any other support for programmability the isa is thus unsafe per se if used incorrectly the isa can produce unexpected results it is tightly coupled with the compiler and trusts generated code to observe certain invariants this dependence on static enforcement is essential to the design of a simple microarchitecture that does not waste energy in providing approximation requirements for the isa an isa extension for disciplined approximate programming con sists of new instruction variants that leave certain aspects of their behavior undefined these new instructions must strike a balance between energy savings and usability they must create optimiza tion opportunities through strategic use of undefined behavior but not be so undefined that their results could be catastrophic namely approximate instructions should leave certain data values undefined but maintain predictable control flow exception handling and other bookkeeping to support a usable programming model similar to enerj an approximation aware isa should exhibit the following properties approximate computation must be controllable at an instruc tion granularity approximation is most useful when it can be interleaved with precise computation for example a loop vari able increment should likely be precise while an arithmetic op eration in the body of the loop may be approximate for this reason it must be possible to mark individual instructions as either approximate or precise the isa must support approximate storage the compiler should be able to instruct the isa to store data approximately or precisely in registers caches and main memory it must be possible to transition data between approximate and precise storage in enerj approximate to precise movement is only permitted using an explicit programmer endorsement but this annotation has no runtime effect for full flexibility the isa must permit programs to use precise data approximately and vice versa precise instructions where approximation is not explicitly en abled must carry traditional semantic guarantees the effects of approximation must be constrained to where it is requested by the compiler approximation must be confined to predictable areas for ex ample address computation for memory accesses must always be precise approximate store instructions should not be allowed to write to arbitrary memory locations approximate instruc tions must not carry semantics so relaxed that they cannot be used table isa extensions for disciplined approximate program ming these instructions are based on the alpha instruction set group approximate instruction integer load store ldx a stx a integer arithmetic add a cmpeq a cmplt a cmple a mul a sub a logical and shift and a nand a or a xnor a nor a xor cmov a sll a sra a srl a floating point load store ldf a stf a floating point operation addf a cmpf x divf a mulf a sqrtf a subf a mov a cmov a movfi a movif a isa extensions for approximation table summarizes the approximate instructions that we propose adding to a conventional architecture without loss of generality we assume an underlying alpha isa approximate operations the extended isa provides approxi mate versions of all integer arithmetic floating point arithmetic and bitwise operation instructions provided by the original isa these instructions have the same form as their precise equivalents but carry no guarantees about their output values the approxi mate instructions instead carry the informal expectation of approxi mate adherence to the original instructions behavior for example add a takes two arguments and produces one output but the isa makes no promises about what that output will be the instruction may be expected to typically perform addition but the programmer and compiler may not rely on any consistent output behavior approximate registers each register in the architecture is at any point in time in either precise mode or approximate mode when a register is in approximate mode reads are not guaranteed to obtain the exact value last written but there is an expectation that the value is likely the same the compiler does not explicitly set register modes instead the precision of a register is implicitly defined based on the precision of the last instruction that wrote to it in other words a precise op eration makes its destination register precise while an approximate operation puts its destination register into approximate mode while register precision modes are set implicitly the precision of operand accesses must be declared explicitly every instruction that takes register operands is extended to include an extra bit per operand specifying the operand precision this makes precision level information available to the microarchitecture a priori drasti cally simplifying the implementation of dual voltage registers see section it does not place a significant burden on the compiler as the compiler must statically determine registers precision any way if the correspondence between register modes and operand accesses is violated the value is undefined see below with this design data can move freely between approximate and precise registers for example a precise add instruction may use an approximate register as an operand this transition corresponds to an endorsement in the enerj language the opposite transition in which precise data is used in approximate computation is also permitted and frequently occurs in enerj programs approximate loads stores and caching the isa defines a granularity of approximation at which the architecture supports set ting the precision of cached memory in practice this granularity will likely correspond to the smallest cache line size in the proces sor cache hierarchy for example if an architecture has byte cache lines and supports varying the precision of every cache note that architects generally avoid tying cache line size to the isa however we believe that in cases of strong co design between architecture and compiler such as ours it is acceptable to do so line then it defines the approximation granularity to be bytes each region of memory aligned to the granularity of approxima tion hereafter called an approximation line for brevity is in either approximate or precise mode at any given time an approximation line precision mode is implicitly controlled by the precision of the loads and stores that access it in particular the isa guarantees reliable data storage for precise accesses if for every load from line x the preceding store to line x has the same precision that is after a precise store to x only precise loads may be issued to x until the next store to x for the compiler and memory allocator this amounts to ensuring that precise and ap proximate data never occupy the same line memory allocation and object layout must be adapted to group approximate data to line aligned boundaries statically determining each line precision is trivial for any language with sufficiently strong static properties in enerj specifically a type soundness theorem implies that the preci sion of every variable is known statically for every access the isa requires this pattern of consistent accesses in order to simplify the implementation of approximation aware caching specifically it allows the following simple cache approximation policy a line precision is set by misses and writes but is not affected by read hits during read hits the cache can assume that the precision of the line matches the precision of the access approximate loads and stores may read and write arbitrary values to memory accordingly precise stores always write data reliably but a precise load only reads data reliably when it accesses a line in precise mode however any store approximate or precise can only affect the address it is given address calculation and indexing are never approximated approximate main memory while this paper focuses on approx imation in the core main memory dram modules may also support approximate storage the refresh rate of dram cells for example may be reduced so that data is no longer stored reli ably however memory approximation is entirely decoupled from the above notion of approximate caching and load store pre cision this way an approximation aware processor can be used even with fully precise memory furthermore memory modules are likely to support a different granularity for approximation from caches dram lines for instance typically range from hundreds of bytes to several kilobytes keeping memory approximation dis tinct from cache approximation decouples the memory from spe cific architecture parameters the program controls main memory approximation explicitly using either a special instruction or a system call to manipulate the precision of memory regions loads and stores are oblivious to the memory precision the compiler is responsible for enforcing a correspondence when main memory approximation is available the operating system may store precision levels in the page table to preserve the settings across page evictions and page faults preservation of precision in several cases where the isa sup ports both precise and approximate operation it relies on the com piler to treat certain data consistently as one or the other for exam ple when a register is in approximate mode all instructions that use that register as an operand must mark that operand as approximate relying on this correspondence simplifies the implementation of approximate sram arrays section the isa does not enforce precision correspondence no excep tion is raised if it is violated instead as with many other situa tions in the isa the resulting value from any inconsistent operation is left undefined unlike other approximate operations however the informal expectation in these situations is also weaker precise reads from approximate mode registers for example should be ex pected to return random data the compiler should avoid these sit uations even when performing approximate computation write reg read execute memory back register file lsq ooo int fu d cache register file fp fu dtlb figure the data movement processing plane of the processor pipeline approximation aware structures are shaded the instruc tion control plane stages fetch and decode as well as rename issue schedule and commit in the ooo pipeline are not shown these situations constitute violations of precision correspon dence a register in approximate mode is used as a precise operand to an instruction note that a precise instruction can use an approximate operand the operand must then be declared as approximate conversely a precise mode register is used as an approximate operand an approximate load e g ldw a is issued to an address in an approximation line that is in precise mode conversely a precise load is issued to an approximate mode line in every case these are situations where undefined behavior is al ready present due to approximation so the isa formal guarantees are not affected by this choice consistency of precision only con stitutes a recommendation to the compiler that these situations be avoided these weak semantics keep the microarchitecture simple by alleviating the need for precision state checks see section design space as discussed above approximate instructions may produce arbi trary results for example add a may place any value in its des tination register however approximate instructions still have de fined semantics add a cannot modify any register other than its output register it cannot raise a divide by zero exception it can not jump to an arbitrary address these guarantees are necessary to make our approximation aware isa usable our microarchitecture must carefully distinguish between struc tures that can have relaxed correctness and those for which reliable operation is always required specifically all fetched instructions need to be decoded precisely and their target and source register indices need to be identified without error however the content of those registers may be approximate and the functional units op erating on the data may operate approximately similarly mem ory addresses must be error free but the data retrieved from the memory system can be incorrect when that data is marked as ap proximate consequently we divide the microarchitecture into two distinct planes the instruction control plane and the data move ment processing plane as depicted in figure the data move ment processing plane comprises the register file data caches load store queue functional units and bypass network the instruction control plane comprises the units that fetch decode and perform necessary bookkeeping for in flight instructions the instruction control plane is kept precise while the data movement processing plane can be approximate for approximate instructions since the data plane needs to behave precisely or approximately depending on the instruction being carried out the microarchitecture needs to do some bookkeeping to determine when a structure can behave approximately this paper explores voltage reduction as a technique for saving energy other techniques such as aggressive timing closure or reducing is data width are orthogonal each frequency associated with a minimum voltage v min and lowering level f max the voltage beyond that may cause timing errors lowering the voltage reduces energy consumption quadratically when the frequency is kept constant however we cannot lower the voltage of the whole processor as this would cause errors in structures that need to behave precisely this leads to the core question of how to provide precise behavior in the microarchitecture one way to provide precise behavior which we explore in this paper is to run critical structures at a safe voltage alternatively er ror correction mechanisms could be used for critical structures and disabled for the data movement processing plane while executing approximate instructions this way the penalty of error checking would not be incurred for approximate operations however if v dd were low enough to make approximation pay off many expensive recoveries would be required during precise operations for this reason we propose using two voltage lines one for precise oper ation and one for approximate operation below we describe two alternative designs for a dual voltage microarchitecture unchecked dual voltage design our dual voltage microarchi tecture called truffle needs to guarantee that the instruction control remains precise at all times and the data processing plane structures lower precision only when processing approximate instructions truffle has two voltages a nominal reliable level re ferred to as v dd h and a lower level called v dd l which may lead to timing errors all structures in the instruction control plane are supplied v dd h and depending on the instruction being executed the structures in the data processing plane are dynamically supplied v dd h or v dd l the detailed design of a dual voltage data process ing plane including the register file data cache functional units and bypass network is discussed in the next section checked dual voltage design the energy savings potential of truffle is limited by the proportion of power used by structures that operate precisely at v dd h therefore reducing the energy con sumption of precise operations will lead to higher relative impact of using approximation this leads to another possible design point which lowers the voltage of the instruction control plane beyond v dd h but not as aggressively as v dd l and employs error correc tion to guarantee correct behavior using an approach similar to ra zor we refer to this conservative level of voltage as v dd l high the data plane also cise instructions and operates v dd l when at v running dd l high the when approximate running the instruc pre tions since the instruction control plane operates at the reliable voltage level the components in this plane need to be checked and corrected in the case of any errors the same checking applies to the data movement processing plane while running precise instruc tions while this is an interesting design point we focus on the unchecked dual voltage design due to its lower complexity selecting at chip manufacture v dd l in the and simplest test time case however v dd l can the accuracy be set statically of ap proximate operations depends directly on this value therefore a natural option is to allow v dd l to be set dynamically depending on the qos expectations of the application fine grained voltage adjustments can be performed after fabrication using off chip volt age regulators as in the intel scc or by on chip voltage reg ulators as proposed by kim et al off chip voltage regulators have a high latency while on chip voltage regulators provide lower latency fine grained voltage scaling depending on the latency re quirement of the application and the type of regulator available v dd l can be selected at deployment time or during execution fu ture work should explore software engineering tools that assist in selecting per application voltage levels truffle a dual voltage microarchitecture for disciplined approximation this section describes truffle in detail we start with the design of a dual voltage sram structure which is an essential building block for microarchitectural components such as register files and data caches next we discuss the design of dual voltage multiplexers and level shifters we then address the design of structures in both in order and ooo truffle cores emphasizing how the voltage is selected dynamically depending on the precision level of each instruction finally we catalog the microarchitectural overheads imposed by truffle dual voltage design dual voltage sram array we propose dual voltage sram arrays or dv srams which can hold precise and approximate data simultaneously like its single voltage counterpart a dv sram array is composed of mats a mat as depicted in figure is a self contained memory structure composed of four identical subarrays and associated predecoding logic that is shared among the subarrays the data in out and address lines typically enter the mat in the middle the predecoded signals are forked off from the middle to the subarrays figure also illustrates a circuit diagram for a single subarray which is a two dimensional array of single bit sram cells arranged in rows and columns the highlighted signals correspond to a read access which determines the critical path of the array for any read or write to a dv sram array the address goes through the predecoding logic and produces the one hot rowselect signals along with the columnselect signals for the column mul tiplexers during read accesses when a row is activated by its rowselect signal the value stored in each selected sram cell is transferred over two complementary bitline wires to the sense amplifiers meanwhile the bitlines have been precharged to a certain v dd each sense amplifier senses the resulting swing on the bitlines and generates the subarray output the inputs and out puts of the sense amplifiers may be multiplexed depending on the array layout the sense amplifiers drive the dataout of the mat for a read access while datain drives the bitlines during a write access to be able to store both approximate and precise data we di vide the data array logic into two parts the indexing logic and the data storage retrieval logic to avoid potential corruption of precise data the indexing logic always needs to be precise even when manipulating approximate data the indexing logic includes the address lines to the mats the predecoding decoding logic row and column decoders and the rowselect and columnselect drivers the data storage retrieval logic in contrast needs to alter nate between precise and approximate mode data storage retrieval includes the precharge equalizing logic sram cells bitline mul tiplexers sense amplifiers dataout drivers and multiplexers and datain drivers for approximate accesses this set of logic blocks operates at v dd l in each subarray of a mat a row of bits sram cells is either at high voltage v dd h or low voltage v dd l the precision column in figure which the voltage state of is a single bit column operating at each row the precision column is v composed dd h stores of transistor cells transistor sram cells each augmented by two pmos transistors in each row the output of the precision column drives the power lines of the entire row this makes it possible to route only one power line to the data rows as well as the prechargers and routed sense to the amplifiers precision this column way in the each extra subarray v dd l power and is line distributed is only to the rows through the precision cells which significantly reduces the overhead of running two voltage lines for a read access the bitlines need to be precharged to the same voltage level as the row being accessed similarly the sense amplifiers need to operate at the same voltage level as the subarray row being accessed a precision signal is added to the address bits and routed to all the mats the precision signal presets the voltage levels of the precharge and equalizing logic and the sense amplifiers before the address is decoded and the rowselect and columnselect signals are generated this voltage presetting ensures that the sense amplifiers are ready when the selected row puts its values on the bitlines during a read furthermore during a write the value in the precision column is set based on the precision signal since the precision cell is selected at the same time as the data cells the state stored in the precision column cannot be used to set the appropriate voltage levels in the sense amplifiers and precharg ers therefore the precision information needs to be determined before the read access starts in the subarrays this is why our isa extensions section include a precision flag for each source operand these flags along with the precision levels of instructions themselves are used to set the precision signal when accessing registers and caches voltage level shifting and multiplexing several structures in the truffle microarchitecture must be able to deal with both precise and approximate data for this reason our design includes dual voltage multiplexers which can multiplex signals of differing voltage and voltage level shifters which enable data movement between the high and low voltage domains figure illustrates the transistor level design of the single bit one input dual voltage multiplexers dv mux as well as the high to low and low to high level shifters the select line of the multiplexer and its associated inverter operate at v dd h while v dd h the the input data level lines shifter can is swing a conventional from v differential to either v low to dd l or high level shifter and the level shifter is constructed from two back to back inverters one operating at v dd h and the other oper ating take an at extra v dd l input in addition precision to the input to identify signal the the level shifters voltage level of input and disengage the level shifting logic to prevent unneces sary power consumption for example in the level shifter when not require precision any level is shifting input output is precise input v or v however dd h and when does precision is the level shifter is engaged and generates the output with the required voltage level truffle microarchitectural constructs this section describes the truffle pipeline stage by stage consid ering both in order and out of order implementations the out of order design is based on the alpha and uses a tag and index register renaming approach we highlight whether a pipeline stage belongs to the instruction control plane or the data move ment processing plane importantly we discuss how the voltage is dynamically selected in the structures that support approximation fetch ooo in order instruction control plane the fetch stage belongs to the instruction control plane and is identical to a regular ooo in order fetch stage all the components of this stage including the branch predictor instruction cache and itlb are ordinary single voltage structures approximate instructions are fetched exactly the same way as precise instructions vddh vddl rowselect i columnselect j senseampclk precision column figure dual voltage mat consisting of four identical dual voltage subarrays and partial transistor level design of the subarrays and the precision column which is shaded the power lines during a read access are shown in bold dv mux level shifter vddh precision precharge vddh vddh dataout i j level shifter input vddl vddl vdd h l vddh vdd h l input output output select output vdd h l vddh vddh vdd h l vddh vddh vdd h l precision precision input vddh figure transistor level design of dual voltage multiplexer dv mux and high to low and low to high level shifters decode ooo in order instruction control plane the in struction decoding logic needs to distinguish between approximate and precise instructions the decode stage passes along one extra bit indicating the precision level of the decoded instruction in addition based on the instruction the decoder generates pre cision bits to accompany the indices for each register read or writ ten these register precision bits will be used when accessing the dual voltage register file as discussed in section the precision levels of the source registers are extracted from the operand preci sion flags while the precision of the destination register corresponds to the precision of the instruction for load and store instructions the address computation must always be performed precisely even when the data being loaded or stored is approximate for approxi mate load and store instructions the registers used for address cal culation are always precise while the data register is always approx imate recall that the microarchitecture does not check that precise registers are always used precisely and approximate registers are used approximately this correspondence is enforced by the com piler and encoded in the instructions simplifying the design and reducing the overheads of mixed precision computation rename ooo instruction control plane for the ooo de sign in which the register renaming logic generates the physical register indices tags the physical register tags are coupled with the register precision bits passed from the decode stage the rest of the register renaming logic is the same as in the base design issue ooo instruction control plane the slots in the issue logic need to store the register index precision bits as well as the physical register indices that is each physical tag entry in an issue slot is extended by one bit the issue slots also need to store the bit indicating the precision of the instruction when an approximate load instruction is issued it gets a slot in the load store queue the address entry of each slot in the load store queue is extended by one extra bit indicating whether the access is approximate or precise the precision bit is coupled with the address in order to control subarray bitline mux sense amplifier sense amplifier mux subarray output drivers write mux and drivers precharge and equalizer n m u l o c n o i i c r e v i r d e n i l d r w r e d o c e d w o r r r n r w precharge and equalizer e d o c e d w o e v i r d m u l o c e r p o e n i l d r o n o i i c e r p subarray bitline mux sense amplifier sense amplifier mux subarray output drivers write mux and drivers predecoder write mux and drivers subarray output drivers sense amplifier mux sense amplifier bitline mux p r e c i i o n c o l u m n w o r d l i n e d r i v e r r o w d e c o d e r w r o w d e c o d e r subarray o r d l i n e d r i v e r write mux and drivers subarray output drivers p r e c i i o n c o l u m n precharge and equalizer precharge and equalizer sense amplifier mux sense amplifier bitline mux subarray vddh vddh vddh vddh input vddh vddh vddh vddl the precision level of the data cache access as described above in section schedule ooo instruction control plane as will be dis cussed in the execution stage there are separate functional units for approximate and precise computation in truffle the approximate functional units act as shadows of their precise counterparts the issue width of the processor is not extended due to the extra approx imate functional units and no complexity is added to the schedul ing logic to accommodate them the coupled approximate precise functional units appear to the scheduler as a single unit for ex ample if an approximate floating point operation is scheduled to issue the precise and approximate fpus are both considered busy the bit indicating the precision level of the instruction is used to en able either the approximate or precise functional unit exclusively register read ooo in order data movement processing plane the data movement processing plane starts at the register read stage as depicted in figure the pipeline registers are divided into data two pipe sets reg starting and the at other this stage operating one at operating v dd h precise at v dd l data approx pipe reg control the approximate pipeline register holds approxi mate data the precise pipeline register contains control informa tion passed along from previous stages and precise program data the outputs of the precise and approximate pipeline registers are multiplexed through a dv mux as needed the dual voltage register file physical in the ooo design and architectural in the in order design is made up of dv sram arrays each register can be dynamically set as approximate or precise while the precision levels of the general purpose registers are determined dynamically the pipeline registers are hardwired to their respective voltages to avoid voltage level changes when running approximate and precise instructions back to back the isa allows precise instructions to use approximate registers as operands and vice versa to carry out such instructions with minimal error the voltage levels of the operands need to be changed to match the voltage level of the operation in the register read stage the level shifters illustrated in figure convert the voltage level of values read from the register file for example if is precise and used in an approximate operation its voltage level is shifted from high to low through an level shifter before being written to an approximate pipeline register the precision level of denoted by is passed to the level shifter to avoid unnecessary level shifting in the case that is already approximate similarly a low to high level shifter is used to adjust the voltage level of values written to the precise pipeline register note that only one of the approximate or precise data pipeline registers is enabled based on the precision of the instruction execute ooo in order data movement processing plane as shown in figure all the functional units are duplicated in the execution stage half of them are hardwired to v dd h while the other not distinguish half operate between at v dd l a shadow as shadows approximate that is the fu scheduler does and its precise counterpart only the instruction precision bit controls whether the results are taken from the precise or approximate fu low voltage functional units are connected to a low voltage pipeline register the outputs of the approximate fus connect to the same broadcast network as their precise counterparts through a dual voltage multi plexer driven by the approximate and precise pipeline register pair at the end of the execution stage the inputs of functional units may also be driven by the bypass network level shifters at the inputs of the functional units adjust the level of the broadcasted input data using the broadcasted preci sion bit only a single bit precision signal is added to the broadcast network because the output of each fu pair is multiplexed the extra fus do not increase the size of the broadcast network beyond adding this single bit precision line while the data bypass network alternates in the ooo between design v always dd l and works v dd at h the the high tag voltage forwarding level network since it carries necessarily precise register indexing information to avoid unnecessary dynamic power consumption in the func tional units the input to one functional unit is kept constant by not writing to its input pipeline register when its opposite precision counterpart is being used an alternative design could use dual voltage functional units and change the voltage depending on the instruction being exe cuted this would save area and static power but require a more complex scheduler design that can set the appropriate voltage level in the functional unit before delivering the operands to it since functional units can be large and draw a considerable amount of current while switching a voltage selecting transistor for a dual voltage functional unit needs to be sized such that it can provide the required drive such transistors tend to consume significant power another possibility is lowering the fu voltage based on phase pre diction when the application enters an approximate phase the voltage level of the functional unit is lowered phase prediction re quires extra power and complicates the microarchitecture design the main objective in the design of truffle is to keep the microar chitectural changes to a minimum and avoid complexity in the in struction control and bookkeeping furthermore since programs consist of a mixture of precise and approximate instructions it is not obvious that phase based voltage level adjustments can provide benefit that compensates for the phase prediction overhead addi tionally static partitioning can help tolerate process variation by using defective functional units for approximate computations memory ooo in order data movement processing plane as previously discussed the address field in each slot of the load store queue is extended by one bit that indicates whether the address location is precise or approximate the data array portion of the data cache is a dv sram array while the tag array is an ordinary single voltage sram structure the approximation gran ularity in the data cache is a cache line one extra bit is stored in the tag array which identifies the precision level of each cache line the extra bit in the load store queue is used as the precision sig nal when accessing the dv sram data array the miss buffer fill buffer prefetch buffer and write back buffers all operate at v dd h the dtlb also requires no changes the precision level of a cache line is determined by the load or store instruction that fills the line if an approximate access misses in the cache the line is fetched as approximate similarly a precise miss fills the cache line as precise subsequent write hits also affect line precision when a store instruction modifies the data in a cache line it also modifies the line precision level see section this paper focuses on the truffle core and does not present a detailed design for approximation aware lower levels of cache or main memory the cache described here could work with an unmodified fully precise cache but a number of options are available for the design of an approximation aware if the has the same line size as the then an identical strategy can be applied however lines are often larger than lines in that case one option is to control precision at a sub line granularity if the line size is n times the line size then the has n precision columns an alternative design could pair the lower level caches with main memory setting the precision of lines based on the explicitly controlled main memory precision levels these non core design decisions are orthogonal to the core truffle design and are an avenue for future work write back ooo in order data movement processing plane the write back value comes from the approximate or precise databypass level shifter level shifter n vddh precisionbypass vddh vddh l o r t n o c l o r t n o c vddh l o r t n o c g e r e p i p a t a d e i c e r p n o i i c e r p g e r vddh vddl level g e r shifter level precise fu shifter register file p level shifter a level shifter a shadow approx fu a g e r e e p i p i p a t a d e i c p a t a d e i c vddl n m u l o c n o i e r p e r p vddl vddl vddl g e r e p i p a t a d x o r p p i reg2precision c e r g e r e p i p a t g e r e p i p a t a a d d figure the register and execution stages in the truffle pipeline along with the bypass network the dv muxes and other approximation aware units are shaded the single bit precision signals in the bypass network and in each stage are dashed lines pipeline register as shown in figure the dual voltage multi plexer driving forwards the write back value to the data by pass network the precision bit accompanying the write back value is also forwarded over the bypass network precision line commit ooo instruction control plane the commit stage does not require any special changes and the reorder buffer slots do not need to store any extra information for the approximate instructions the only consideration is that during rollbacks due to mispredictions or exceptions the precision state of the registers one bit per register needs to be restored another option is to restore all the registers as precise ignoring the precision level of the registers during rollback microarchitectural overheads in truffle in this section we briefly consider the microarchitectural overheads imposed by routing two power lines to structures in truffle data movement processing plane first the data part of the pipeline registers is duplicated to store approximate data an extra level of dual voltage multiplexing is added after these pipeline registers to provide the data with the correct voltage for the next stage the dv sram arrays including the register file and data array in the data cache store one precision bit per row in addition a one bit precision signal is added to each read write port of the dv sram array and routed to each subarray along with the v dd x o r p p x o r p p queue is extended by one bit preserving the precision level of the instruction each physical tag entry in an issue slot is also extended by one precision bit similarly the slots in the load store queue are augmented by one extra bit indicating whether the access is approximate or precise the pipeline registers also need to store the precision level of the operands and instructions experimental results our goals in evaluating truffle are to determine the energy savings brought by disciplined approximation characterize where the en ergy goes and understand the qos implications for software evaluation setup we extended cacti to model the dual voltage sram struc ture we propose and incorporated the resulting models into mc pat we modeled truffle at the nm technology node in the context of both in order and out of order based on the designs table shows the detailed microarchitectural parameters disciplined approximate computation represents a trade off be tween energy consumption and application output quality there fore we evaluate each benchmark for two criteria energy savings and sensitivity to error for the former we collect statistics from the benchmarks execution as parameters to the mcpat based power l power line model for the latter we inject errors into the execution and mea the tag for each data cache line is also augmented by a bit storing sure the consequent degradation in output quality the precision state of the line however the tag array itself is an for both tasks we use a source to source transformation that ordinary single voltage sram structure to adjust the voltage level instruments the program for statistics collection and error injec of the operands accessed from the dual voltage register file one set tion statistics collected for power modeling include variable field of and one set of level shifters is added for each read and array accesses basic blocks for branches and arithmetic and port of the register file similarly in the execution stage one set of logical operators a cache simulator is used to distinguish cache each level shifter type is added per operand from the data bypass hits and misses the register file is simulated as a small fully network the data bypass network is also augmented with a single associative cache for error injection each potential injection point bit precision signal per operand in addition each entry in the issue approximate operations and approximate memory accesses is in table microarchitectural parameters parameter ooo truffle in order truffle fetch decode width issue commit width int alus fpus int mult div units approximate int alus fpus approximate int mult div units int fp issue window size rob entries int fp architectural registers int fp physical registers load store queue size itlb i cache size kbyte kbyte line width associativity dtlb d cache size kbyte kbyte line width associativity branch predictor tournament tournament table list of benchmarks application description type fft benchmark scientific kernels fp sor fp mc fp smm fp lu fp zxing bar code decoder for mobile phones fp integer jmeint jmonkeyengine game framework tri angle intersection kernel fp imagefill imagej raster image processing ap plication flood filling kernel integer raytracer image renderer fp tercepted each bit in the resulting value is flipped according to a per component probability before being returned to the program benchmarks we examine nine benchmark programs written in the enerj language which is an extension to java the ap plications are the same programs that were evaluated in exist ing java programs hand annotated with approximate type qualifiers that distinguish their approximate parts five of the benchmarks come from the suite zxing is a multi format bar code recognizer developed for android smartphones jmonkeyengine is a game development engine we examine the framework triangle intersection algorithm used for collision detection imagej is a li brary and application for raster image manipulation we examine its flood filler algorithm finally we examine a simple raytracer for each program an application specific quality of service metric is defined in order to quantify the loss in output qual ity caused by hardware approximation for most of the applica tions the metric is the root mean square error of the output vector matrix or pixel array for jmeint the jmonkeyengine triangle intersection algorithm the metric is the proportion of incorrect intersection decisions similarly for the zxing bar code recognizer it is the proportion of unsuccessful decodings of a sample qr code image unchecked truffle microarchitecture figure presents the energy savings achieved in the core and cache for the unchecked dual voltage truffle microarchitecture in both ooo and in order configurations in both designs v dd h of v v dd and h the v dd l frequency takes values is set that constant are at and mhz the truffle energy savings for ooo truffle vddl v vddl v vddl v vddl v imagefill jmeint lu mc raytracer smm sor zxing average energy savings for in order truffle vddl v vddl v vddl v vddl v imagefill jmeint lu mc raytracer smm sor zxing average figure percent energy reduction with unchecked ooo and in order truffle designs for various v dd l voltages cores include dv sram arrays and extra approximate functional units the baseline for the reported energy savings is the same core operating at the reliable voltage level of v and mhz without the extra functional units or dual voltage register files and data cache our model assumes that truffle microarchitectural additions do not prolong the critical path of the base design depending on the configuration voltage and application truf fle ranges from increasing energy by to saving for the in order configuration all voltage levels lead to energy savings the ooo design shows energy savings when v dd l is less than of v which dd h its our energy results savings suggest outweigh truffle exhibits its overheads a break even point at the difference between the energy savings in ooo and in order truffle cores stems from the fact that the instruction control plane in the ooo core accounts for a much larger portion of total energy than in the in order core recall that the instruction control plane in the ooo core includes instruction fetch and decode register renaming instruction issue and scheduling load and store queues and dtlb itlb whereas in the in order setting it includes only instruction fetch and decode and the tlbs since approximation helps reduce energy consumption in the data movement processing plane only the impact of truffle in in order cores is much higher furthermore the ooo truffle core is an aggressive four wide multiple issue processor whereas the in order truffle core is two wide anything that can reduce the energy consumption of the instruction control plane indirectly helps increase truffle impact figure depicts the energy breakdown between different mi croarchitectural components in the ooo and in order truffle cores when the benchmarks v dd l v dd imagefill h i e shows with fully precise similar benefits computation for both designs among for this benchmark and of the energy is consumed in the data movement processing plane of the ooo and in order truffle cores respectively on the other hand raytracer shows the largest difference in energy reduction between the two designs here the data movement processing plane consumes of the energy in the ooo core but just in the in order core in summary the simpler the instruction control plane in the processor the higher the potential for energy savings with truffle in addition to the design style of the truffle core the energy savings are dependent on the proportion of approximate computa tion in the execution figure shows the percentage of approxi mate dynamic instructions along with the percentage of approxi imagefill jmeint lu mc raytracer smm sor zxing d cache broadcast approx fpu fpu approx mult div mult div approx alu alu regfile itlb dtlb store q load q scheduler rename inst fetch a imagefill jmeint lu mc raytracer smm sor zxing d cache broadcast approx fpu fpu approx mult div mult div approx alu alu regfile itlb dtlb inst fetch b figure percent energy consumed by different microarchitec tural components in the a ooo and b in order truffle instruc ons reg file accesses alu accesses mult div accesses fpu accesses d cahce accesses imagefill jmeint lu mc raytracer smm sor zxing figure percentage of approximate events mate alu multiply divide and floating point operations as well as the percentage of approximate data cache accesses among the benchmarks imagefill has the lowest percentage of approximate in structions and no approximate floating point or multiplica tion operations only of its integer operations are approxi mate as a fully integer application it exhibits no opportunity for floating point approximation and its approximate integer opera tions are dwarfed by precise control flow operations imagefill also has a low ratio of approximate data cache accesses these characteristics result in low potential for truffle about energy savings for v dd l v conversely raytracer shows the high est ratio of approximate instructions in the group nearly all of its floating point operations are approximate in addition ray tracer has the highest ratio of approximate data cache accesses in the benchmark set which makes it benefit the most from truf ooo truffle core inorder truffle core imagefill jmeint lu mc raytracer smm sor zxing average figure percent energy reduction potential for checked in order and ooo truffle designs with v dd l v fle the high rate of floating point approximation is characteristic of the fp dominated benchmarks we examined for many applica tions more than of the fp operations are approximate this is commensurate with the inherently approximate nature of fp repre sentations furthermore for many benchmarks fp data constitutes the application error resilient data plane while integers dominate its error sensitive control plane these results show that as the proportion of approximate com putation increases the energy reductions from the truffle microar chitecture also increase furthermore some applications leave cer tain microarchitectural components unexercised suggesting that higher error rates may be tolerable in those components for exam ple none of the benchmarks except imagefill exercise the approxi mate integer alu and the approximate multiply divide unit is not exercised at all as a result higher error rates in those components may be tolerable the results also support the utility of application specific v dd l settings since each of the benchmarks exercise each approximate component differently overall these results show that disciplined approximation has great potential to enable low power microarchitectures also as expected the simpler the microarchitecture the higher the energy savings potential overheads we modified mcpat and cacti to model the over heads of truffle as described in section the energy across all the benchmarks increases by at most when the applications are compiled with no approximate instructions the approximate func tional units are power gated when there are no approximate instruc tions in flight the energy increase is due to the extra precision state per cache line and per register along with other microarchitectural changes cacti models show an increase of in register file area due to the precision column and a increase in the area of the level data cache the extra approximate functional units also contribute to the area overhead of truffle opportunities in a checked design as discussed above reducing energy consumption of the instruc tion control plane and the energy used in precise instructions can increase the overall impact of truffle section outlines a design that uses a sub critical voltage and error detection correction for the microarchitectural structures that need to behave precisely we now present a simple limit study of the potential of such a design fig ure presents the energy savings potential when the voltage level of the instruction control plane is reduced to v beyond the re liable results voltage show only level the of ideal v min case in which v and there v dd is l no penalty v associ the ated with error checking and correction in the precise computation as illustrated the gap in energy savings potential between the ooo r l l i f e t n e c a g j l alu t f g a i m i e m u c m r t y a r f m m r o n i x z together registers multiplier fpu cache a sensitivity to errors in all components error probability application r o r r e t u p t u o n o i t a c i l p p a fft imagefill jmeint lu mc raytracer smm sor zxing b figure application sensitivity to circuit level errors each cell in a has the same axes as b application qos degradation is related to architectural error probability on a log scale the grid a shows applications sensitivity to errors in each component in isolation the row labeled together corresponds to experiments in which the error probability for all components is the same the plot b shows these together configurations in more detail the output error is averaged over replications and in order designs is significantly reduced in one case imagefill the checked ooo truffle core shows higher potential compared to the checked in order truffle core in this benchmark the energy consumption of the instruction control plane is more dominant in the ooo design and thus lower voltage for that plane is more ef fective than in the in order design note that in an actual design energy savings will be restricted by the error rates in the instruc tion control plane and the rate at which the precise instructions fail triggering error recovery the overhead of the error checking struc tures will further limit the savings error propagation from circuits to applications we now present a study of application qos degradation as we inject errors in each of the microarchitectural structures that support ap proximate behavior the actual pattern of errors caused by voltage reduction is highly design dependent modeling the error distribu tions of approximate hardware is likely to involve guesswork the most convincing evaluation of error rates would come from exper iments with real truffle hardware for the present evaluation we thoroughly explore a space of error rates in order to characterize the range of possibilities for the impact of approximation figure shows each benchmark sensitivity to circuit level errors in each microarchitectural component some applications are significantly sensitive to error injection in most components fft for example others show very little degradation imagefill raytracer mc smm errors in some components tend to cause more application level errors than others for example errors in the integer functional units alu and multiplier only cause output degradation in the benchmarks with significant approximate integer computation imagefill and zxing the variability in application sensitivity highlights again the utility of using a tunable v dd l to customize the architecture error rate on a per application basis see section most applications exhibit a critical error rate at which the application output quality drops precipitously for example in figure b fft exhibits low output error when all components have error probability but significant degradation occurs at probability a software controllable allowable power v dd l while could maintaining allow each acceptable application output to run quality at its lowest in general the benchmarks do not exhibit drastically different sensitivities to errors in different components a given benchmark that is sensitive to errors in the register file for example is also likely to be sensitive to errors in the cache and functional units related work a significant amount of prior work has proposed hardware that compromises on execution correctness for benefits in performance energy consumption and yield ersa proposes collaboration be tween discrete reliable and unreliable cores for executing error resilient applications stochastic processors encapsulate an other proposal for variable accuracy functional units proba bilistic cmos pcmos proposes to use the probability of low voltage transistor switching as a source of randomness for special randomized algorithms finally algorithmic noise tolerance ant proposes approximation in the context of digital signal processing our proposed dual voltage design in contrast supports fine grained single core approximation that leverages language support for explicit approximation in general purpose applications it does not require manual offloading of code to co processors and permits fully precise execution on the same core as low power approximate instructions truffle extends general purpose cpus it is not a special purpose coprocessor relax is a compiler architecture system for suppressing hard ware fault recovery in certain regions of code exposing these errors to the application a truffle like architecture supports approx imation at a single instruction granularity exposes approximation in storage elements and guarantees precise control flow even when executing approximate code in addition truffle goes further and elides fault detection as well as recovery where it is not needed razor and related techniques also use voltage underscaling for energy reduction but use error recovery to hide errors from the ap plication disciplined approximate computation can enable energy savings beyond those allowed by correctness preserving op timizations broadly the key difference between truffle and prior work is that truffle was co designed with language support specifically relying on disciplined approximation with strong static guarantees offered by the compiler and language features enables an efficient and simple design static guarantees also lead to strong safety properties that significantly improve programmability the error tolerant property of certain applications is supported by a number of surveys of application level sensitivity to circuit level errors truffle is a microarchitectural technique for exploiting this application property to achieve energy savings dual voltage designs are not the only way to implement low power approximate computation fuzzy memoization and bit width reduction for example are orthogonal techniques for approximating floating point operations imprecise integer logic blocks have also been designed an approximation aware pro cessor could combine dual voltage design with these other tech niques timization previous in work fully precise has also explored computers dual v dd designs truffle for instruction power op controlled voltage changes make it fundamentally different from these previous techniques truffle resembles architectures that incorporate information flow tracking for security in that work the hardware enforces information flow invariants dynamically based on tags provided by the application or operating system with truffle the compiler provides the information flow invariant freeing the archi tecture from costly dynamic checking conclusion disciplined approximate programming is an effective and usable technique for trading off superfluous correctness guarantees for en ergy savings dual voltage microarchitectures can realize these en ergy savings by providing both approximate and precise computa tion to be controlled at a fine grain by the compiler we propose an isa that simplifies the hardware by relying on the compiler to pro vide certain invariants statically eliminating the need for checking or recovery at run time we describe a high level microarchitec ture that supports interleaved high and low voltage operations and a detailed design for a dual voltage sram array that implements approximation aware caches and registers we model the power of our proposed dual voltage microarchitecture and evaluate its en ergy consumption in the context of a variety of error tolerant bench mark applications experimental results show energy savings up to under reasonable assumptions these benchmarks exhibit low or negligible degradation in output quality emerging scale out workloads require extensive amounts of computational resources however data centers using modern server hardware face physical constraints in space and power lim iting further expansion and calling for improvements in the computational density per server and in the per operation energy continuing to improve the computational resources of the cloud while staying within physical constraints mandates optimizing server efficiency to ensure that server hardware closely matches the needs of scale out workloads in this work we introduce cloudsuite a benchmark suite of emerging scale out workloads we use performance counters on modern servers to study scale out workloads finding that today predominant processor micro architecture is inefficient for run ning these workloads we find that inefficiency comes from the mismatch between the workload needs and modern processors particularly in the organization of instruction and data memory systems and the processor core micro architecture moreover while today predominant micro architecture is inefficient when executing scale out workloads we find that continuing the current trends will further exacerbate the inefficiency in the future in this work we identify the key micro architectural needs of scale out workloads calling for a change in the trajectory of server proces sors that would lead to improved computational density and power efficiency in data centers the dominant limiting factor for data centers because their sheer size and electrical power demands cannot be met recognizing the physical constraints that stand in the way of further growth cloud providers now optimize their data centers for compute density and power consumption cloud providers have already begun building server systems specifically targeting cloud data centers improving compute density and energy efficiency by using high efficiency power supplies and removing unnecessary board level components such as audio and graphics chips although major design changes are being introduced at the board and chassis level of new cloud servers the processors used in these new servers are not designed to efficiently run scale out workloads processor vendors use the same underlying architecture for servers and for the general purpose market leading to extreme inefficiency in today data centers moreover both general pur pose e g intel and amd and traditional server processor e g sun niagara ibm designs follow a trajectory that bene fits scale up workloads a trend that was established long before the emergence of scale out workloads recognizing the space and power inefficiencies of modern processors for scale out work loads some vendors and researchers conjecture that even using processors built for the mobile space may be more efficient in this work we observe that scale out workloads share many inherent characteristics that place them into a distinct workload class from desktop parallel and traditional server workloads we categories and subject descriptors c performance of systems performance of systems design studies perform a detailed micro architectural study of a range of scale out workloads finding a large mismatch between the demands of the scale out workloads and today predominant processor micro general terms design measurement performance architecture we observe significant over provisioning of the introduction memory hierarchy and core micro architectural resources for the scale out workloads moreover continuing the current processor cloud computing is emerging as a dominant computing plat form for delivering scalable online services to a global client base today popular online services such as web search social net works and video sharing are all hosted in large data centers with the industry rapidly expanding service providers are building new data centers augmenting the existing infrastructure to meet the increasing demand however while demand for cloud infra trends will result in further widening the mismatch between the scale out workloads and server processors conversely we find that the characteristics of scale out workloads can be leveraged to gain area and energy efficiency in future servers we use performance counters to study the behavior of scale out workloads running on modern server processors we demonstrate scale out workloads suffer from high instruction cache structure continues to grow the semiconductor manufacturing industry has reached the physical limits of voltage scaling no longer able to reduce power consumption or increase power miss rates instruction caches and associated next line prefetchers found in modern processors are inadequate for scale out workloads density in new chips physical constraints have therefore become instruction and memory level parallelism in scale out workloads is low modern aggressive out of order cores are excessively complex consuming power and on chip area with permission to make digital or hard copies of all or part of this work for personal or out providing performance benefits to scale out workloads classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation data working sets of scale out workloads considerably on the first page to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission and or a fee asplos march london england uk copyright acm 03 exceed the capacity of on chip caches processor real estate and power are misspent on large last level caches that do not contribute to improved scale out workload performance on chip and off chip bandwidth requirements of scale out workloads are low scale out workloads see no benefit from fine grained coherence and high memory and core to core communication bandwidth the rest of this paper is organized as follows in section we provide an overview of the state of the art server processors and scale out workloads we provide a detailed description of our benchmarking methodology in section we present our results in section concentrating on the mismatch between the needs of scale out workloads and modern processors we summarize related work in section and conclude in section modern cores and scale out workloads today data centers are built around conventional desktop pro cessors whose architecture was designed for a broad market the dominant processor architecture closely followed the technology trends improving single thread performance with each processor generation by using the increased clock speeds and free in area and power transistors provided by progress in semiconductor manufacturing although dennard scaling has stopped with both clock frequency and transistor counts becoming lim ited by power processor architects have continued to spend resources on improving single thread performance for a broad range of applications at the expense of area and power efficiency dominant processor architectures modern processors comprise several aggressive out of order cores connected with a high bandwidth on chip interconnect to a deep three level cache hierarchy while core aggressiveness and clock frequency scaling enabled a rapid increase in computational performance off chip memory latency improvements were not as rapid the memory wall the gap between the speed at which cores could compute and the speed at which data could be deliv ered to the cores for computation mandated that the data working set must fit into the on chip caches to allow the core to compute at full speed modern processors therefore split the die area into two roughly equal parts with one part dedicated to the cores and tightly coupled private caches and the other dedicated to a large shared last level cache the emergence of multi core processors offered the possibility to run computationally intensive multi threaded applications adding the requirement of fast and high bandwidth core to core communication to allow cores to compute without incurring significant delays when operating on actively shared data to leverage the increasing number of transistors on chip for higher single thread performance cores are engineered to execute independent instructions out of order ooo allowing the proces sor to temporarily bypass instructions that stall due to a cache access while ooo execution can improve core resource utiliza tion through instruction level parallelism ilp the core complexity increases dramatically depending on the width of the pipeline and the size of the reorder window large windows require selecting and scheduling among many instructions while tracking all memory and register dependencies this functionality requires a large and power hungry scheduler reorder buffer and load store structures moreover the efficacy of growing the instruction reorder window rapidly drops off resulting in dimin ishing returns at exponentially increasing area and energy costs with every processor generation notably although wide pipelines and large reorder windows do not harm the core performance low ilp applications execute inefficiently because the area and power costs spent on ooo execution do not yield a performance benefit the first level instruction and data caches capture the primary working set of applications enabling low latency access to the most frequently referenced memory addresses to maintain low access latency the first level cache capacity must remain small due to hardware constraints whereas the last level caches can have larger capacity but with a higher access latency as the size of the last level cache llc has reached tens of megabytes in modern processors the access latency of the llc has itself created a speed gap between the first level caches and llc pushing processor designers to mitigate the gap by inserting an intermediate size sec ondary cache additionally to further mitigate the large llc latency the number of miss status handling registers mshrs is increased to allow for a large number of memory reference instructions to be performed in parallel like the core structures for supporting ilp the mechanisms to support memory level parallel ism mlp use considerable area and energy increasing parallelism in llc and off chip accesses can give a tremendous performance improvement when many independent memory accesses are available to execute but results in poor efficiency when executing workloads with low mlp to increase the core utilization when mlp is low modern pro cessors add support for simultaneous multi threading smt enabling multiple software threads to be executed simultaneously in the same core smt cores operate like single threaded cores but introduce instructions from two independent software threads into the reorder window enabling the core to find independent memory accesses and perform them in parallel even when both software threads have low mlp however introducing instructions from multiple software threads into the same pipeline causes contention for core resources limiting the performance of each thread com pared to when that thread runs alone dominant scale out workloads to find a set of applications that dominate today cloud infra structure we examined a selection of internet services based on their popularity for each popular service we analyzed the class of application software used by the major providers to offer these services either on their own cloud infrastructure or on a cloud infrastructure leased from a third party we present an over view of the applications most commonly found in today clouds along with brief descriptions of typical configuration characteris tics and dataset sizes overall we find that scale out workloads have similar characteristics all applications we examined oper ate on large data sets that are split across a large number of machines typically into memory resident shards serve large numbers of completely independent requests that do not share any state have application software designed specifically for the cloud infrastructure where unreliable machines may come and go and use inter machine connectivity only for high level task management and coordination data serving various nosql data stores have been explicitly designed to serve as the backing store for large scale web applications such as the facebook inbox google earth and google finance providing fast and scalable storage with varying and rapidly evolving storage schemas the nosql systems split hundreds of terabytes of data into shards and horizontally scale out to large cluster sizes most operations on requested objects are per formed using indexes that support fast lookup and key range scans for simplicity and scalability these systems are designed to sup port queries that can be completely executed by a single machine with all operations that require combining data from multiple shards relegated to the middleware mapreduce the explosion of accessible human generated infor mation necessitates automated analytical processing to cluster classify and filter this information the map reduce paradigm has emerged as a popular approach to large scale analysis farming out requests to a cluster of machines that first perform filtering and transformation of the data map and then aggregate the results reduce a key advantage of the map reduce paradigm is the sep aration of infrastructure and algorithms users implement algorithms using map and reduce functions and provide these functions to the map reduce infrastructure which is then responsi ble for orchestrating the work because of the generality of the infrastructure and the need to scale to thousands of independent servers communication between tasks is typically limited to read ing and writing files in a distributed file system for example map tasks produce temporary files that are subsequently read by the reduce tasks effectively rendering all map and reduce tasks archi tecturally independent whenever possible the map reduce infrastructures strive to further reduce communication by schedul ing the processing of data on the servers that store that data media streaming the availability of high bandwidth connec tions to home and mobile devices has made media streaming services such as netflix youtube and youku ubiquitous streaming services use large server clusters to gradually packetize and transmit media files ranging from megabytes to gigabytes in size pre encoded in various formats and bit rates to suit a diverse client base sharding of media content ensures that servers fre quently send the same content to multiple users enabling in memory caching of content by the servers while in memory cach ing is effective the on demand unicast nature of today streaming services practically guarantees that the streaming server will work on a different piece of the media file for each client even when concurrently streaming the same file to many clients sat solver the ability to temporarily allocate compute resources in the cloud without purchasing the infrastructure has created an opportunity for engineers and researchers to conduct large scale computations for example complex algorithms such as symbolic execution become tractable when the computation is split into smaller sub problems and distributed to the cloud where a large number of sat solver processes are hosted however unlike the traditional super computer environment with high band width low latency dedicated interconnects and balanced memory and compute resources the cloud offers dynamic and heteroge neous resources that are loosely connected over an ip network large scale computation tasks must therefore be adapted to a worker queue model with centralized load balancing that rebal ances tasks across a dynamic pool of compute resources minimizing the amount of data exchanged between the workers and load balancers and practically eliminating any communication between workers web frontend web services are hosted in the cloud to achieve high fault tolerance and dynamic scalability although many web stacks are used in the cloud the underlying service architectures are similar a load balancer distributes independent client requests across a large number of stateless web servers the web servers either directly serve static files or pass the requests to stateless middleware scripts written in high level interpreted or byte code compiled languages which then produce dynamic content all state is stored by the middleware in backend databases such as nosql data stores or traditional relational database servers web search web search engines such as those powering google and microsoft bing index terabytes of data harvested from online sources to support a large number of concurrent latency sensitive search queries against the index the data is split into memory resi dents shards with each index serving node isn responsible for processing requests to its own shard a frontend machine sends index search requests to all isns in parallel collects and sorts the responses and sends a formatted reply to the requesting client hundreds of unrelated search requests are handled by each isn every second with minimal locality shards are therefore sized to fit into the memory of the isns to avoid reducing throughput and table architectural parameters processor operating intel xeon at cmp width ooo cores core width wide issue and retire reorder buffer entries load store buffer entries reservation stations entries cache split i d cycle access latency cache cycle access per core latency llc cache cycle access latency memory channels delivering up to degrading quality of service due to disk i o for performance scal ability isns may be replicated allowing the frontend to load balance requests among multiple isns that are responsible for the same shard isns communicate only with the frontend machines and never to other isns in an isn each request is handled by a software thread without communicating with other threads methodology we conduct our study on a poweredge enclosure with two intel processors and of ram in each blade each intel processor includes six aggressive out of order processor cores with a three level cache hierarchy the and caches are private to each core while the llc is shared among all cores each core includes several simple stride and stream prefetchers labelled as adjacent line hw prefetcher and dcu streamer in the processor documentation and system bios settings the blades use high performance broadcom server nics with drivers that support multiple transmit queues and receive side scaling the nics are connected by a built in switch for bandwidth intensive benchmarks two gigabit nics are used in each blade table summarizes the key architectural parameters of the systems in this work we introduce cloudsuite a benchmark suite of emerging scale out workloads we present a characterization of the cloudsuite workloads alongside traditional benchmarks including desktop specint parallel parsec enterprise web and relational database server tpc c tpc e web backend workloads for all but the tpc e benchmark we use centos with the linux kernel for the tpc e benchmark we use microsoft windows server release measurement tools and methodology we analyze architectural behavior using intel vtune a tool that provides an interface to the processor performance coun ters for all scale out and traditional server workloads except sat solver we perform a second measurement after the workload completes the ramp up period and reaches a steady state because sat solver has no steady state behavior we achieve comparable results across runs by re using input traces we create sat solver input traces that take minutes to execute in our baseline setup and use the first minutes as warmup and the last minutes as the measurement window we measure the entire execution of the parsec and spec applications for our evaluation we limit all workload configurations to four cores tuning the workloads to achieve high utilization of the cores or hardware threads in the case of the smt experiments while maintaining the workload qos requirements to ensure that all application and operating system software runs on the cores under test we disable all unused cores using the available operating sys tem mechanisms we note that computing a breakdown of the execution time stall components of superscalar out of order processors cannot be performed precisely due to overlapped work in the pipeline we present execution time breakdown results based on the performance counters that have no overlap however we plot memory cycles side by side rather than in a stack to indicate the potential overlap with cycles during which instructions were com mitted we compute the memory cycles as a sum of the cycles when an off core request was outstanding instruction stall cycles contributed by instruction hits second level tlb miss cycles and the first level instruction tlb miss cycles because data stalls can overlap other data stalls we compute the number of cycles when statistics an off core which request measure was the outstanding number of using cycles mshr when occupancy there is at least one miss being serviced we do not include d and data cache hits in the computation because they are effectively hid den by the out of order core we perform a cache sensitivity analysis by dedicating two cores to cache polluting threads the polluter threads traverse arrays of predetermined size in a pseudo random sequence ensur ing that all accesses miss in the upper level caches and reach the llc we use performance counters to confirm that the polluter threads achieve nearly hit ratio in the llc effectively reducing the cache capacity available for the workload running on the remaining cores of the same processor to measure the frequency of read write sharing we execute the workloads on cores split across two physical processors in separate sockets when reading a recently modified block this configura tion forces accesses to actively shared read write blocks to appear as off chip accesses that hit in a remote processor cache scale out workload experimental setup data serving we benchmark the cassandra database with a yahoo cloud serving benchmark ycsb dataset server load is generated using the client that sends requests following a zipfian distribution with a read to write request ratio cassandra is configured with a java heap and new generation garbage collector space mapreduce we benchmark a node of a four node hadoop cluster running the bayesian classification algorithm from the mahout library the algorithm attempts to guess the coun try tag of each article in a set of wikipedia pages one map task is started per core and assigned a java heap media streaming we benchmark the darwin streaming server serving videos of varying duration using the faban driver to simulate the clients we limit our setup to low bit rate video streams to shift stress away from network i o sat solver we benchmark one instance per core of the klee sat solver an important component of the parallel symbolic execution engine input traces for the engine are produced by by symbolically executing the command line printf utility from the gnu coreutils using up to four byte and one byte symbolic command line arguments web frontend we benchmark a frontend machine serving olio a web web based social event calendar the frontend machine in the processors we study we measure the occupancy statistics of the super queue structure which tracks all accesses that miss in the caches runs nginx with a built in php module and apc php opcode cache we generate a backend dataset using the cloudstone benchmark and use the faban driver to simu late clients to limit disk space requirements we generate a on disk file dataset and modify the php code to always serve files from the available range web search we benchmark an index serving node isn of the distributed version of nutch lucene with an index size of and data segment size of of content crawled from the public internet we mimic real world setups by making sure that the search index fits in memory eliminating page faults and mini mizing disk activity we simulate clients using the faban driver the clients are configured to achieve the maximum search request rate while ensuring that of all search queries complete in under seconds traditional benchmark experimental setup parsec we benchmark the official applications with the native input reporting results averaged across all benchmarks we present results averaged into two groups cpu intensive cpu and memory intensive mem benchmarks spec we benchmark the official applications run ning with the first reference input reporting results averaged across all benchmarks similarly to parsec we separately pres ent cpu intensive cpu and memory intensive mem applications we benchmark the e banking workload running on the nginx web server with an external fastcgi php module and apc php opcode cache we disable connection encryption ssl when running to allow for better comparison to the web frontend workload tpc c we benchmark the tpc c workload on a commercial enterprise database management system dbms the database load is generated by clients configured with zero think time and running on a separate machine the tpc c database has ware houses the dbms is configured with a buffer pool and direct i o tpc e we benchmark the tpc e workload on a commer cial enterprise database management system dbms the client driver runs on the same machine bound to a core that is not used by the database software the tpc e database contains cus tomer records the dbms is configured with a buffer pool web backend we benchmark a machine executing the database backend of the web frontend benchmark presented above the backend machine runs the mysql database engine with a buffer pool i o infrastructure data intensive scale out workloads and traditional database server workloads exhibit a significant amount of disk i o with a large fraction of the non sequential read and write accesses scat tered throughout the storage space if the underlying i o bandwidth is limited either in raw bandwidth or in i o operation throughput the i o latency is exposed to the system resulting in an i o bound workload where the cpu is underutilized and the application performance unnecessarily suffers to isolate the cpu behavior of the applications our experi mental setup over provisions the i o subsystem to avoid an i o bottleneck to avoid bringing up disk arrays containing hundreds of disks and flash devices as it is traditionally done with large scale database installations we construct a network attached iscsi storage array by creating large ram disks in separate machines and connect the machine under test to the iscsi storage stalled os stalled application committing application committing os memory figure execution time breakdown and memory cycles of scale out workloads left and traditional benchmarks right via the high speed ethernet network this approach places the by serving static files and a small number of dynamic scripts mod entire dataset of our applications in the memory of the remote ern scalable web workloads like web frontend handle a much machines creating an illusion of a large disk cluster with higher fraction of dynamic requests leading to higher core utiliza extremely high i o bandwidth and low latency tion and less os involvement results although the behavior across scale out workloads is similar the class of scale out workloads as a whole differs significantly we begin exploring the micro architectural behavior of scale out workloads through examining the commit time execution breakdown in figure we classify each cycle of execution as committing if at least one instruction was committed during that cycle or as stalled otherwise overlapped with the execution time breakdown we show the memory cycles bar which approximates the number of cycles when the processor could not commit instruc tions due to outstanding long latency memory accesses the execution time breakdown of scale out workloads is domi nated by stalls in both application code and operating system from other workloads processor architectures optimized for desk top and parallel applications are not optimized for scale out workloads that spend the majority of their time waiting for cache misses resulting in a clear micro architectural mismatch at the same time architectures designed for workloads that perform only trivial computation and spend all of their time waiting on memory e g and tpc c also cannot cater to scale out work loads in the rest of this section we provide a detailed analysis of the inefficiencies of running scale out workloads on modern processors notably most of the stalls in scale out workloads arise due to frontend inefficiencies long latency memory accesses this behavior is in contrast to the cores idle due to high instruction cache miss rates cpu intensive desktop specint and parallel parsec bench marks which stall execution significantly less than of the cycles and experience only a fraction of the stalls due to memory accesses furthermore although the execution time breakdown of some scale out workloads e g mapreduce and sat solver appears similar to the memory intensive parsec and specint benchmarks the nature of the stalls of these workloads is different unlike the scale out workloads many parsec and specint applications frequently stall due to pipeline flushes after wrong path instructions with much of the memory access time not on the critical path of execution scale out workloads show memory system behavior that more closely matches traditional online transaction processing work loads tpc c tpc e and web backend however we observe that scale out workloads differ considerably from traditional online the time transaction stalled due processing to dependent tpc c memory which accesses spends over we find that of scale out workloads are most similar to the more recent transaction processing benchmarks tpc e and web backend that use more complex data schemas or perform more complex queries than tra ditional transaction processing we also observe that a traditional enterprise web workload behaves differently from the web frontend workload representative of modern scale out configurations while the traditional web workload is dominated caches increase average i fetch latency excessive llc capacity leads to long i fetch latency instruction fetch stalls play a critical role in system perfor mance by preventing the core from making forward progress due to a lack of instructions to execute frontend stalls serve as a fun damental source of inefficiency for both area and power as the core real estate and power consumption are entirely wasted for the cycles that the frontend spends fetching instructions figure presents the i and instruction miss rates of our workloads we find that in contrast to desktop and parallel bench marks the instruction working sets of many scale out workloads considerably exceed the i cache capacity resembling the instruction cache behavior of traditional server workloads more over the instruction working sets of most scale out workloads also exceed the cache capacity where even relatively infrequent instruction misses incur considerable performance penalties we note that the operating system behavior differs between the scale out workloads and traditional server systems although many scale out workloads actively use the operating system the functionality exercised by the scale out workloads is more restricted both i and operating system instruction cache miss rates across the scale out workloads are lower compared to traditional server workloads indicating a smaller operating system instruction working set in scale out workloads stringent access latency requirements of the i caches pre in addition to instructions and data tpc c includes request for ownership memory cycles these accesses are not on the critical path but are part of the memory cycles that appear in figure clude increasing the size of the caches to capture the instruction working set of many scale out workloads which is an order of magnitude larger than the caches found in modern processors we os i application application figure i and instruction cache miss rates for scale out workloads left and traditional benchmarks right the os bars present the instruction cache miss rates of the workloads that spend significant time executing operating system code find that modern processor architectures cannot tolerate the latency of i cache misses avoiding frontend stalls only for applications whose entire instruction working set fits into the cache fur thermore the high instruction miss rates indicate that the i capacity experiences a significant shortfall and cannot be mitigated by the addition of a modestly sized cache the disparity between the needs of the scale out workloads and the processor architecture are apparent in the instruction fetch path although exposed instruction fetch stalls serve as a key source of inefficiency under any circumstances the instruction fetch path of modern processors actually exacerbates the problem the cache experiences high instruction miss rates increasing the average fetch latency of the missing fetch requests by placing an additional intermediate lookup structure on the path to retriev ing instruction blocks from the llc moreover the entire instruction working set of any scale out workload is considerably smaller than the llc capacity however because the llc is a large cache with a large uniform access latency it contributes an unnecessarily large instruction fetch penalty cycles to access the cache implications to improve efficiency and reduce frontend stalls processors built for scale out workloads must bring instructions closer to the cores rather than relying on a deep hierarchy of caches a partitioned organization that replicates instructions and makes them available close to the requesting cores is likely to considerably reduce frontend stalls to effectively use the on chip real estate the system would need to share the partitioned instruc tion caches among multiple cores striking a balance between the die area dedicated to replicating instruction blocks and the latency of accessing these blocks from the closest cores furthermore although modern processors include next line prefetchers high instruction cache miss rates and significant fron tend stalls indicate that the prefetchers are ineffective for scale out workloads scale out workloads are written in high level lan guages use third party libraries and execute operating system code exhibiting complex non sequential access patterns that are not captured by simple next line prefetchers including instruction prefetchers that predict these complex patterns is likely to improve overall processor efficiency by eliminating wasted cycles due to frontend stalls i os core inefficiencies low ilp precludes effectively using the full core width rob and lsq are underutilized due to low mlp modern processors execute instructions out of order to enable simultaneous execution of multiple independent instructions per cycle additionally out of order execution elides stalls due to memory accesses by executing independent instructions that fol low a memory reference while the long latency cache access is in progress modern processors support up to instruction win dows with the width of the processor dictating the number of instructions that can simultaneously execute in one cycle in addition to exploiting ilp large instruction windows can exploit memory level parallelism mlp by finding independent memory accesses within the instruction window and performing the memory accesses in parallel the latency of llc hits and off chip memory accesses cannot be hidden by out of order execution achieving high mlp is therefore key to achieving high core utiliza tion by reducing the data access latency modern processors use wide cores that can decode issue execute and commit up to four instructions on each cycle how ever in practice instruction level parallelism ilp is limited by dependencies the baseline bars in figure left show the aver age number of instructions committed per cycle when running on an aggressive wide out of order core despite the abundant availability of core resources and functional units scale out work loads achieve a modest application ipc typically in the range of media streaming to web frontend although workloads that can benefit from wide cores exist with some cpu intensive parsec and specint applications reaching an ipc of indi cated by the range bars in the figure using wide processors for scale out applications does not yield significant benefit modern processors have entry load store queues enabling up to memory reference instructions in the instruction win dow however just as instruction dependencies limit ilp address dependencies limit mlp the baseline bars in figure right present the mlp ranging from web frontend to sat solver for the scale out workloads these results indicate that the memory accesses in scale out workloads are replete with complex dependencies limiting the mlp that can be found by modern aggressive processors we again note that while desktop and paral lel applications can use high mlp support with some parsec and specint applications having an mlp up to support for baseline smt baseline smt figure application instructions committed per cycle for systems with and without smt out of a maximum ipc of left and memory level parallelism for systems with and without smt right range bars indicate the minimum and maximum of the corresponding group user ipc has been shown to be proportional to application through put we verified this relationship for the scale out workloads loads we find that scale out workloads would be well suited by architectures offering multiple independent threads per core with a modest degree of superscalar out of order execution and support for several simultaneously outstanding memory accesses for example rather than implementing smt on a way core two independent way cores would consume fewer resources while achieving higher aggregate performance furthermore each nar rower core would not require a large instruction window reducing the per core area and power consumption compared to modern processors and enabling higher compute density by integrating more cores per chip data access inefficiencies large llc consumes area but does not improve performance simple data prefetchers are ineffective more than half of commodity processor die area is dedicated to the memory system modern processors feature a three level cache hierarchy where the last level cache llc is a large capacity cache shared among all cores to enable high bandwidth data fetch each core can have up to cache misses in flight the high bandwidth on chip interconnect enables cache coherent com munication between the cores to mitigate the capacity and latency gap between the caches and llc each cache is equipped with prefetchers that can issue prefetch requests into the llc and off chip memory multiple memory channels provide high bandwidth access to off chip memory the llc is the largest on chip structure its cache capacity has been increasing with each processor generation due to semicon ductor manufacturing improvements we investigate the utility of growing plot the average the llc system capacity performance for scale out of workloads scale out high mlp is not useful for scale out applications however we find that scale out workloads generally exhibit higher mlp than traditional server workloads support for wide out of order execution with a instruc tion window and up to outstanding memory requests requires multiple branch prediction numerous alus forwarding paths many port register banks large instruction schedulers highly associative reorder buffers rob and load store queues lsq and many other complex on chip structures the complexity of the cores limits core count leading to chip designs with several cores that consume half of the available on chip real estate and dissipate the vast majority of the chip dynamic power budget however our results indicate that scale out workloads exhibit low ilp and mlp deriving benefit only from a small degree of out of order execution as a result the nature of scale out workloads cannot effectively utilize the available core resources both the die area and the energy are wasted leading to data center inefficiency entire buildings are packed with aggressive cores designed to commit instructions per cycle with over outstanding memory requests but executing applications with an average ipc of and average mlp of moreover current industry trends point toward greater inefficiency in the future over the past two decades processors have gradually moved to increasingly complex cores raising the core width from way to way and increasing the window size from to instructions the inefficiency of modern cores running applications without abundant ipc and mlp has led to the addition of simultaneous multi threading smt to the processor cores to enable the core resources to be simultaneously shared by multiple software threads thereby guaranteeing that independent instructions are available to exploit parallelism we present the ipc and mlp of an in figure we workloads as a smt enabled core in figure using the bars labeled smt as function of the llc capacity normalized to a baseline system with expected the mlp found and exploited by the cores when two independent application threads run on each core concurrently is a llc unlike in the memory intensive desktop applica tions e g specint mcf we find minimal performance sensitivity nearly doubled compared to the system without smt unlike tradi to llc size above in scale out and traditional server tional database server workloads that contain many inter thread dependencies and locks the independent nature of threads in scale workloads the llc captures the instruction working sets of scale out workloads which are less than two megabytes beyond this out workloads enables them to observe considerable performance point small shared supporting structures may consume another benefits from smt with 69 improvements in ipc implications the nature of scale out workloads makes them ideal candidates to exploit multi threaded multi core architectures modern mainstream processors offer excessively complex cores one to two megabytes because scale out workloads operate on massive data sets and service a large number of concurrent requests both the dataset and the per client data are orders of mag nitude larger than the available on chip cache capacity as a result resulting in inefficiency through waste of resources at the same time our results corroborate prior work indicating that niche processors offer excessively simple e g in order cores that cannot leverage the available ilp and mlp in scale out work an llc that captures the instruction working set and minor sup porting data structures achieves nearly the same performance as an llc with double or triple the capacity our results show that the on chip resources devoted to the llc are one of the key limiters of cloud application compute density in modern processors for traditional workloads increasing the llc capacity captures the working set of a broader range of applica tions contributing to improved performance due to a reduction in average memory latency for those applications however because the llc capacity already exceeds the cloud application require ments by 3x whereas the next working set exceeds any possible sram cache capacity the majority of the die area and power cur rently dedicated to the llc is wasted moreover prior research has shown that increases in the llc capacity that do not cap ture a working set lead to an overall performance degradation llc access latency is high due to its large capacity not only wast ing on chip resources but also penalizing all cache misses by slowing down llc hits and delaying off chip accesses in addition to leveraging mlp to overlap demand requests from the processor core modern processors use prefetching to specula tively increase mlp prefetching has been shown effective at reducing cache miss rates by predicting block addresses that will be referenced in the future and bringing these blocks into the cache prior to the processor demand thereby hiding the access latency in figure we present the hit ratios of the cache when all available prefetchers are enabled baseline as well as the hit ratios after disabling the prefetchers we observe a noticeable deg radation of the hit ratios of many desktop and parallel applications when the adjacent line prefetcher and hw prefetcher are disabled in contrast only one of the scale out work loads mapreduce significantly benefits from these prefetchers with the majority of the workloads experiencing negligible changes in the cache hit rate moreover similar to traditional server workloads tpc c disabling the prefetchers results in an increase in the hit ratio for some scale out workloads media streaming sat solver finally we note that the dcu streamer not shown provides no benefit to scale out workloads in some cases marginally increasing the miss rate because it pollutes the cache with unnecessary blocks implications while modern processors grossly over provision the memory system data center efficiency can be improved by match ing the processor design to the needs of the scale out workloads whereas modern processors dedicate approximately half of the die area to the llc scale out workloads would likely benefit from a different balance a two level cache hierarchy with a modestly sized llc that makes special provision for caching instruction blocks would benefit performance the reduced llc capacity scale out server specint mcf c e n i l e a b p i r o t d e u e z i l a m r o n cache size mb figure performance sensitivity to llc capacity baseline all enabled adjacent line disabled hw prefetcher disabled figure hit ratios of a system with enabled and disabled adjacent line and hw prefetchers along with the removal of the ineffective cache would offer access latency benefits while at the same time freeing up die area and power the die area and power can be practically applied toward improving compute density and efficiency by adding more hardware contexts and more advanced prefetchers additional hardware contexts more threads per core and more cores should linearly increase application parallelism while more advanced cor relating data prefetchers could accurately prefetch complex access data patterns and increase the performance of all cores bandwidth inefficiencies lack of data sharing deprecates coherence and connectivity off chip bandwidth exceeds needs by an order of magnitude increasing core counts have brought parallel programming into the mainstream highlighting the need for fast and high bandwidth inter core communication multi threaded applications comprise a collection of threads that work in tandem to scale up the applica tion performance to enable effective scale up each subsequent generation of processors offers a larger core count and improves the on chip connectivity to support faster and higher bandwidth core to core communication we investigate the utility of the on chip interconnect for scale out workloads in figure we plot the fraction of misses that access data most recently written by another thread running on a remote core breaking down each bar into application and os components to offer insight into the source of the data sharing in general we observe limited read write sharing across the scale out workloads we find that the os level data sharing is dominated by the network subsystem multi threaded java based applications data serving and web search exhibit a small degree of sharing from the use of a parallel garbage collector that may run a collection thread on a remote core artificially inducing applica tion level communication additionally we found that the media streaming server updates global counters to track the total number of packets sent by the server reducing the amount of communica tion by keeping per thread statistics is trivial and would eliminate the mutex lock and shared object scalability bottleneck we note that the on chip application level communication in scale out workloads is distinctly different from traditional database server workloads tpc c tpc e and web backend which experience frequent interaction between threads on actively shared data struc tures that are used to service client requests the low degree of active sharing indicates that wide and low latency interconnects available in modern processors are over pro visioned for scale out workloads although the overhead with a small number of cores is limited as the number of cores on chip increases the area and energy overhead of enforcing coherence becomes significant likewise the area overheads and power con sumption of an over provisioned high bandwidth interconnect further increase processor inefficiency beyond the on chip interconnect we also find off chip band width inefficiency while the off chip memory latency has improved slowly off chip bandwidth has been improving at a rapid pace over the course of two decades the memory bus speeds have increased from to dual data rate at over raising the peak theoretical bandwidth from to per channel with the latest server processors having four independent memory channels in figure we plot the per core off chip bandwidth utilization of our workloads as a fraction of the available per core off chip bandwidth scale out workloads expe rience non negligible off chip miss rates however the mlp of the applications is low due to the complex data structure dependencies leading to low aggregate off chip bandwidth utilization even when all cores have outstanding off chip memory accesses among the scale out workloads we examine media streaming is the only application that uses up to of the available off chip band width however we note that our applications are configured to stress the processor demonstrating the worst case behavior over all we find that modern processors significantly over provision off chip bandwidth for scale out workloads implications the on chip interconnect and off chip memory buses can be scaled back to improve processor efficiency because the scale out workloads perform only infrequent communication via the network there is typically no read write sharing in the applications processors can therefore be designed as a collection of core islands using a low bandwidth interconnect that does not enforce coherence between the islands eliminating the power associated with the high bandwidth interconnect as well as the power and area overheads of fine grained coherence tracking off chip memory buses can be optimized for scale out workloads by scaling back unnecessary bandwidth memory controllers consume a large fraction of the chip area and memory buses are responsible for a large fraction of the system power reducing the number of memory channels and the power draw of the memory buses should improve scale out workload efficiency without affecting applica tion performance related work previous research has characterized the micro architectural behavior of traditional commercial server applications when run ning on modern hardware using real machines and simulation environments we include traditional application os figure percentage of llc data references accessing cache blocks modified by a thread running on a remote core application os figure average off chip memory bandwidth utilization as a percentage of available off chip bandwidth server applications in our study to compare them with scale out workloads and to validate our evaluation framework the research community uses the parsec benchmark suite to perform experiments with chip multiprocessors bienia et al characterize the parsec suite working sets and the communica tion patterns among threads in our work we examine the execution time breakdown of parsec unlike the scale out and traditional server applications parsec has a negligible instruc tion working set and exhibits a high degree of memory level parallelism displaying distinctly different micro architectural behavior compared to scale out workloads previous research analyzed various performance or power inef ficiencies of modern processors running traditional commercial applications tuck and tullsen showed that simultaneous multithreading can improve performance of sci entific and engineering applications by on a processor our results show similar trends for scale out work loads kgil et al show that for a particular class of throughput oriented web workloads modern processors are extremely power inefficient arguing that the chip area should be used for processing cores rather than caches a similar observation has been made in the gpu domain our results corroborate these findings showing that for scale out workloads the time spent accessing the large and slow last level caches accounts for more than half of the data stalls calling for resizing and reor ganizing the cache hierarchy we draw similar conclusions to davis et al and hardavellas et al who showed that heav ily multithreaded in order cores are more efficient for throughput oriented workloads compared to aggressive out of order cores ranganathan and jouppi motivate the need for integrated analysis of micro architectural efficiency and application service level agreements in their survey of enterprise information technology trends to address the power inefficiencies of current general purpose processors specialization at various hardware levels has been proposed 46 as cloud computing has become ubiquitous there has been sig nificant research activity on characterizing particular scale out workloads either micro architecturally or at the system level to the best of our knowledge our study is the first work to systematically characterize the micro architectural behavior of a wide range of cloud services kozyrakis et al presented a system wide characterization of large scale online ser vices provided by microsoft and showed the implications of such workloads on data center server design reddi et al character ized the bing search engine showing that the computational intensity of search tasks is increasing as a result of adding more machine learning features to the engine these findings are consis tent with our results which show that web search has the highest ipc among the studied scale out workloads much work focused on benchmarking the cloud and datacenter infrastructure the yahoo cloud serving benchmark ycsb is a framework to benchmark large scale distributed data serving systems we include results for the ycsb benchmark and provide its micro architectural characterization running on cassandra a popular cloud database fan et al discuss web mail web search and map reduce as three representative workloads present in the google datacenter lim et al extend this set of benchmarks and add an additional media streaming workload they further analyze the energy efficiency of a variety of systems when running these applications our benchmark suite similarly includes work loads from these categories cloudcmp is a framework to compare cloud providers using a systematic approach to benchmarking various components of the infrastructure huang et al analyzed performance and power characteristics of hadoop clusters using hibench a bench mark that specifically targets the hadoop map reduce framework our mapreduce benchmark uses the same infrastructure how ever we provide the micro architectural rather than a system wide characterization of the map reduce workload for bench marking modern web technologies we use cloudstone an open source benchmark that simulates activities related to social events conclusions cloud computing has emerged as a dominant computing plat form to provide hundreds of millions of users with online services to support the growing popularity and continue expanding their services cloud providers must work to overcome the physical space and power constraints limiting data center growth while strides have been made to improve data center efficiency at the rack and chassis levels we observe that the predominant processor micro architecture of today data centers is inherently inefficient for running scale out workloads resulting in low compute density and poor trade offs between performance and energy in this work we used performance counters to analyze the micro architectural behavior of a wide range of scale out work loads we analyzed and identified the key sources of area and power inefficiency in the instruction fetch core micro architecture and memory system organization we then identified the specific needs of scale out workloads and suggested the architectural mod ifications that can lead to dense and power efficient data center processor designs in the future specifically our analysis showed that efficiently executing scale out workloads requires optimizing the instruction fetch path for multi megabyte instruction working sets reducing the core aggressiveness and last level cache capacity to free area and power resources in favor of more cores each with more hardware threads and scaling back the over provisioned on chip and off chip bandwidth general purpose gpus gpgpus are becoming prevalent in mainstream computing and performance per watt has emerged as a more crucial evaluation metric than peak per formance as such gpu architects require robust tools that will enable them to quickly explore new ways to optimize gpgpus for energy efficiency we propose a new gpgpu power model that is configurable capable of cycle level cal culations and carefully validated against real hardware mea surements to achieve configurability we use a bottom up methodology and abstract parameters from the microarchi tectural components as the model inputs we developed a rigorous suite of microbenchmarks that we use to bound any modeling uncertainties and inaccuracies the power model is comprehensively validated against measurements of two commercially available gpus and the measured error is within and for the two target gpus gtx and quadro the model also accurately tracks the power consumption trend over time we integrated the power model with the cycle level simulator gpgpu sim and demonstrate the energy savings by utilizing dynamic voltage and frequency scaling dvfs and clock gating traditional dvfs reduces gpu energy consumption by by lever aging within kernel runtime variations more finer grained sm cluster level dvfs improves the energy savings from to for those benchmarks that show clustered ex ecution behavior we also show that clock gating inactive lanes during divergence reduces dynamic power by categories and subject descriptors c processor architectures parallel architectures c performance of systems modeling techniques general terms experimentation measurement power performance keywords energy cuda gpu architecture power estimation gpuwattch is named after the original cpu power modeling framework wattch which enabled widespread architecture level power analysis and optimizations however our approach is independent of wattch permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee isca tel aviv israel copyright acm 2079 06 introduction from datacenters to power constrained mobile devices performance per watt has emerged as an indispensable met ric for evaluating the efficiency of a gpu architecture 18 although gpu performance models such as gpgpu sim multi2sim and macsim have enabled per formance oriented research on branch divergence memory bandwidth pressure and so forth similar efforts to investigate and optimize gpu energy efficiency problems have been difficult owing to the lack of a suit able power modeling infrastructure researchers using these tools may inadvertently be optimizing for performance while penalizing performance per watt to avoid such pitfalls and develop energy efficient gpu architectures we require a ro bust power model a robust power model must satisfy three requirements to be useful for computer architecture research it must be configurable cycle level and strongly vali dated against existing processor architectures using a rigor ous methodology as shown in table wattch and mc pat are robust cpu power models that satisfy all three requirements and as such have enabled new research areas in energy efficient cpu design no such power model exists for gpu architecture research hong and kim were the first to propose an integrated power and performance model for gpus however their power model is not configurable to different architectural parameters moreover it is inca pable of providing cycle level power estimates to evaluate fine grained power saving techniques such as clock gating in this paper we introduce gpuwattch a new power model that addresses all of the aforementioned requirements we follow the rigorous process shown in figure a to de velop the robust power model we use a bottom up method ology to build the initial model then we compare our sim ulated power with the measured hardware power to identify any modeling inaccuracies we resolve these inaccuracies us ing a special suite of microbenchmarks that are designed to create a system of linear equations that correspond to the total power consumption by solving for the unknowns in the system we progressively eliminate the inaccuracies we work gpu configurable cycle level validated wattch mcpat no yes yes yes hong and kim yes no no yes gpuwattch yes yes yes yes table robust power modeling requirements for a gpu config micro architectural component parameters gpgpu sim performance counters leakage power a microbenchmark yes stress components rf caches exe units access patterns hit miss ratio coalesce pattern validation profiling tools power measurement start basic structures wire logic sram initial modeling stage a good refinement coverage accuracy achieved no yes no a microbenchmark refinement adjust hardware assumptions add more microbenchmarks power model power saving techniques p c states clock gating power model bottom up modeling power model refinement identify modeling unknowns adjust component parameters scale estimated power component rf caches exe units stop refinement stage dynamic validation power stage b figure a steps to build a robust power model the various stages indicate our systematic and rigorous methodology to iteratively identify and refine inaccuracies in the model b our integrated power and performance modeling framework validated the simulated power model average and runtime estimates against measured results using a comprehensive set of real world kernels that were not used to originally develop the power model the power model achieves an av erage accuracy that is within error of the measured results for the gtx and for the quadro moreover it accurately tracks the trace of relative power consumption over time a salient feature of our power model is its longevity power consumption is often sensitive to the details of the spe cific architecture implementation choices and technology parameters all of which continuously evolve from one gen eration to another therefore the power model must be con tinuously adapted refined and validated for evolving gpu architectures and their manufacturing technology a key distinguishing feature of our power model as compared to other architecture level power models is that it is equipped with the suite of microbenchmarks and a refinement and validation methodology that can support future gpus we integrate the power model with gpgpu sim for cycle level power calculation as shown in figure b to es tablish a complete framework for exploring energy efficient gpu architectures and power management techniques on the basis of this framework we demonstrate that tradi tional processor level dynamic voltage and frequency scaling dvfs achieves energy savings in gpus by leverag ing the phase behavior within kernels average performance loss is within furthermore we identify new opportuni ties for cluster level dvfs i e grouping streaming multi processors sms in the gpu to perform dvfs separately cluster level dvfs can achieve an additional energy savings over conventional dvfs for benchmarks that ex hibit such grouped behavior we also evaluate fine grained lane level clock gating in order to reduce processor power consumption when simd lanes go inactive due to branch divergence on average lane gating reduces dynamic power consumption by approximately in summary our work makes the following contributions we propose a power model based on the bottom up methodology for gpu architecture research that can enable performance per watt energy efficiency studies we describe a systematic and rigorous methodology to develop and validate the power model using a large variety of microbenchmarks that stress test the mi croarchitecture we demonstrate the opportunities for improving the energy efficiency of gpus using both traditional tech niques dvfs and new techniques lane gating we begin with an overview of our power modeling approach in section we explain how we model the various impor tant microarchitectural structures to assess and fix any discrepancies in the initial model we describe our exten sive microbenchmarking methodology in section next we validate the power model comprehensively using hard ware measurements in section we then demonstrate the benefits of two hardware optimizations to save gpu power in section we compare our results with related work and conclude the paper in section and section respectively gpu power modeling we model gpu architectures similar to nvidia gpus the main components shown in table include streaming multiprocessors sms memory controllers the interconnec tion network and dram figure shows the architecture of an sm in a gpu the sm major difference from a tra ditional cpu core is that it has a simd execution unit and also contains a texture cache constant cache and shared memory gpus also adopt gddr instead of ddr memory for higher bandwidth note that our initial modeling may differ from the real hardware implementation there fore during the refinement stage of the modeling methodol ogy we update the initial model estimates to be more precise as shown in figure a equation captures at a very high level all aspects of gpu power that we model which consists of the leakage idle sm and all components n in total dynamic power each component dynamic power is calculated as the ac tivity factor α i multiplied by the component peak power p max i power n α i p max i idle sm power leakage 488 warp scheduler instruc on cache fetch and decode register file banks execu on units shared memory tex constant figure streaming multiprocessor sm overview infrastructure we use publicly available resources that describe the gpu microarchitecture and rely on mcpat to model most mi croarchitectural blocks refer to table throughout we stay consistent with the process of abstracting the microar chitectural parameters of each component and using them to model each component circuit implementation in order to ensure configurability although mcpat includes detailed models for several mi croarchitectural blocks many components are either not present or are considerably different for gpus as compared to general purpose cpus therefore we added or adapted several important blocks in mcpat to more accurately rep resent the underlying gpu microarchitecture we model sram array structures using cacti register file owing to the lack of resources for the nvidia gpu reg ister file architecture we adopt the architecture used in gpgpu sim version see figure which is inspired by nvidia patents each sm has a large and unified register file that is shared across warps executing in the same sm gpus adopt a multibanked register file architecture to avoid the area and power overhead of a multiported register file crossbar networks and operand collectors are used to transport operands from different banks to the appropriate simd execution lane the register file bandwidth and size determines the num ber and size of banks fermi gpus can perform a fused description microarchitectural components basic structures sm pipeline pipeline in order multithreaded pipeline with simd datapath register files register file banks bit wide entry sram operand collectors bit wide sram collection network bit wide x crossbar shared memory shared shared memory memory banks network architecture dependent size x crossbar network caches texture and constant caches architecture dependent size memory coalescing logic synthesis based power model execution unit integer alus floating point units synthesis based power model special function units main memory empirical model memory controller network on chip mcpat model table main gpu components in the fermi that we model as best we can due to limited public information rf register file bank bank bank bank bank reg2 bits write operand collectors port figure register file structure in the fermi architecture multiply add fma operation per cycle which needs three input operands and one output operand from the regis ter file each sm contains bit registers which are shared by all threads executed on the same sm we model the register file as dual ported one read and one write port banks each of which provides 048 logical bit reg isters physically these registers are accessed as bit registers thus each operand read from a register file bank provides operands for simt threads we estimate the power consumption for these memory arrays using cacti a crossbar interconnection network is used to transfer operands from register file banks to operand collectors fur thermore in the case of bank conflicts the arbiter shown in figure is responsible for serializing the register file ac cess and the operand collectors are used to buffer the data already read from the register file we include the crossbar interconnect model in our register file power model the crossbar network parameters are determined by the number of register file banks as well as the number of operand collector units for the fermi archi tecture we model a input output crossbar network as shown in figure each input and output of the crossbar is 024 bits wide consequently the operands read from a register file bank are routed to the proper operand collector we model each operand collector as an sram array bank shared memory nvidia gpus contain shared memory per sm it can be used for interthread communication for threads in a thread block because simd lanes can access any address concur rently shared memory is multibanked and contains a cross bar interconnect to improve performance that is shared memory has a very similar structure to the register files sec tion in the case of the fermi architecture the number of banks is and the crossbar is input output execution units we model the fp pipeline in the fermi architecture us ing floating point fma units two lanes of fma units can combine to execute a double precision dp operation the sfu units are also modeled as dp fma units because most instructions executed on the sfu units are transcendental operations that employ iterative algorithms to compute the result these units power consumption depends on the la tency and throughput of instructions area and power modeling of execution units are initially estimated based upon synthesis of their verilog descriptions the execution units are synthesized using a nm standard cell library the synthesized netlists are annotated with physical register ids x crossbar network arbiter read port sfus alus fpus sfus alus fpus execution units memory requests tid request size base address offset address pending request table memory address coalescing pending request count entry entry entry entry thread thread thread mask mask mask thread masks figure main components of memory address coalescing switching activity for random inputs we use the synopsys power compiler r to estimate the power consumption of the designs after the annotation of switching activity all designs assume an operating nominal v dd of v we per form technology and voltage scaling according to itrs pro jections to estimate the dynamic power consumption at the given voltage and technology node memory coalescing logic mcl gpus load store units are responsible for coalescing mem ory accesses to reduce the bandwidth usage figure depicts mcl main structures each mcl contains a pending re quest table prt with multiple entries each prt entry stores the thread index the base and offset addresses for the memory access and the request sizes for all of the concur rently issued memory requests by a warp these entries are written to the prt whenever a memory request is issued to determine the number of coalesced memory requests that a warp must issue mcl compares the base address of the first thread request with the base addresses of all the remaining requests in the prt entry the memory request mask of all the simt threads with the same base address is set to zero the process is repeated until masks have been generated for all requests in the prt entry these masks are stored in a separate thread mask array memory requests are generated for all addresses whose mask bit are set for each prt entry a pending request count prc is maintained and is incremented when a request is sent to memory and decremented when a response is received when the prc becomes zero requests for the warp are satisfied we model the prt as an sram array structure using cacti idle sms due to load imbalance it is possible to have sms that are idling during execution in our experiments we observe that an idle sm still consumes a noticeable amount of power that affects the accuracy of the power model some benchmarks such as hrtwl and mgst suffer from this load imbalance issue where nearly a quarter of their sms becomes idle after finishing their work for the idle sm activity factor α i in is zero but the sm still consumes power therefore it is important and necessary to model the idle power of individual sms correctly we determine the dynamic power consumption of an idle sm using a microbenchmarking methodology first we use a microbenchmark in which each sm can run at most one thread block concurrently then we vary the number of active sms to perform a linear regression fit to estimate the power used when all sms are idle we subtract this power from the constant power component of our gpu card discussed in section to determine the dynamic power component of an idle sm main memory our paper focuses on modeling gpu chip power we found no public information on power modeling and while flexible dram power models could be in corporated into our effort they require additional reverse engineering of the gddr implementation which is beyond the scope of our processor microarchitecture centric work hence we use an empirical approach based on prior work to compute the effect of dram on our studies the ap proach considers only dram dynamic power modeling equation states the empirical dram dynamic power model we used we consider its dynamic power to be com posed of precharge power row buffer activation power read power and write power the precharge power is calcu lated as the per operation energy e pre times the number of precharge operations c pre divided by the execution time read write and activation powers are calculated similarly p dram e pre c pre e act c act e wr c wr e rd c rd execution time the above dram power model can be lumped into equa tion resulting in a complete linear system with all gpu components access rates we treat the access energy for each dram operation as an unknown variable in the next section we explain the process of solving for the unknowns microbenchmarking the uncer tainities microbenchmarking plays a critical role in the refinement and validation of our initial power model we rely on mi crobenchmarks to address the power modeling uncertainties that arise for various reasons such as misguided assumptions about undocumented features the microbenchmarks also help us isolate the power consumption of key components in the gpu so that we can validate and refine the power model component level power breakdown they also help us achieve good test coverage of the various components in the processor real benchmarks do not stress all aspects of the processor microarchitecture as the microbenchmarks lse problem formation a major source of inaccuracies in the initial power model arises from uncertainties due to undocumented design de cisions in the target architecture undocumented aspects of the gpus we model include unknown sizes or configu rations of components our first step is to ascertain these inaccuracies we use an iterative process to continuously refine the power model based on inaccuracies that we observe between the power model and the actual hardware power measure ments as equation shows we model the dynamic power consumption as a linear combination of access rate α i ac cess counts divided by time of each microarchitectural com ponent multiplied by the modeling inaccuracy its peak power for a particular p max microarchitectural i we consider component i as an unknown variable x i in equation 490 void kernel unsigned a unsigned c unsigned tid offset sum tid thread id offset tid m tid k offset sum a offset manually unroll sum a offset operator creates dependency figure 5 a generic microbenchmark stressing the cache thus if there are n access rates for the different microar chitectural components each microbenchmark will yield a microarchitectural component access rate vector that con stitutes one linear equation with an arbitrary number of microbenchmarks say m this will result in a m n linear estimation problem as shown in equation 5 with a suf ficiently large number of equations i e microbenchmarks and hardware power measurements p m we can effectively solve the modeling inaccuracies using the least squares esti mation lse p dynamic n α i p max i p max i p modeled max i x i a m n x n p m 5 we iteratively refine the power model on the basis of the sources of the various inaccuracies that lse identifies for instance in our infrastructure i e mcpat the power esti mation for certain components is biased toward cpu imple mentations we narrow the resulting inaccuracy gap for the gpu power model by fixing our initial assumptions about the implementation and then applying the scaling factors that are obtained from lse microbenchmarking design methodology to aid the lse solving process we use a systematic mi crobenchmarking methodology we describe the three im portant microbenchmark characteristics that we identified as requirements then we show that our microbenchmarks satisfy these requirements component stress to solve the lse problem formed by all microbenchmarks effectively the performance counter vector of each microbenchmark should have as low a correla tion as possible with the others we achieve this by de signing some microbenchmarks to stress individual microar chitectural components for example we limit the number of variables in the execution unit stressing microbenchmark so that all necessary values reside in the register files with minimal access to caches or dram similarly when design ing cache microbenchmarks we disable the cache both in the simulator and hardware for power isolation we fol low a similarly strict design methodology for all of the other microbenchmarks this step is also important for enabling component level model validation discussed in section access patterns it is important to design microbench marks to exercise the same microarchitectural component in different ways because switching activity impacts dy namic power consumption thus we created additional microbenchmarks with different access patterns to create more equations for lse estimation for example cache mi crobenchmarks exercise different memory spaces hit ratios and coalescing patterns using reads or writes the kernel shown in figure 5 is a parametrized microbenchmark de color key alue microbenchmarks and histogram histogram m correlation b color key and histogram figure correlation heat map of all microbenchmarks signed for the cache that can be configured to achieve different hit ratios with different coalescing patterns the cross thread instruction locality within each thread is highly amortized by massive multithreading hence the hit ratio is mostly determined by the interthread locality which can be engineered based on the k m and offset parameters test coverage real benchmarks only exercise a small portion of the processor for example few benchmarks use the texture or constant caches to ensure that our refine ment and validation tests all components of the processor we rely on our microbenchmarks to achieve good test cov erage as shown in table we designed four sets of mi crobenchmarks functional microbenchmarks are designed to exercise executional units such as integer and floating point memory microbenchmarks access the various caches and shared memory intensively dram exercises the main memory in addition we also create the mix microbench marks that have more complex access patterns such as ac cessing multiple components simultaneously these efforts result in unique microbenchmarks name t n u x10 k r a m h c o c n e b o r c i a heat map k r a m h c n e b o r ic m m a r g o t i h 5 5 2 value components exercised counts name components exercised counts func integer floating mem texture point and special constant caches function unit and shared memory dram dram mix mix of functional dram and memory 22 table refinement and validation microbenchmarks microbenchmark correlation figure shows a ma trix for the correlation between any two microbenchmarks performance counter vectors for our microbenchmark figure shows its color key the hotter the color is the higher correlation two microbenchmarks have and the his togram for correlation among all microbenchmarks fig ure indicates that most microbenchmarks have a low cor relation around which is good because it shows that these microbenchmarks are independent and stress compo nents differently the only microbenchmarks that have a high correlation are the dram microbenchmarks that are used to develop dram power model we expect this high correlation because the activity differences among these mi crobenchmarks are limited to dram reads and writes power model validation we begin this section with a detailed description of our power measurement setup for real gpu cards we select two nvidia gpu cards with different architectures to show our power model configurability for validation table sum marizes the differences between the architectures we focus on comprehensive validation of the leakage power average dynamic power and dynamic power trace of kernels against measurement results for both the gpu card configurations category gtx quadro architecture fermi cuda cores frequency ghz 2 ghz register file size kb kb shared memory kb kb cache kb n a cache kb kb texture cache kb 5 kb constant cache kb kb technology node nm nm memory type memory bandwidth gb gb memory controllers modeled leakage power 3w table used two gpu cards configurations experimental setup we validate our power model with a reference hardware platform containing both the cards we provide details here power measurement setup we use both nvidia gtx and quadro for validation both the cards are connected to the pcie slot through a pcie riser card and an atx power supply the pcie riser card and the atx power supply have power pins that deliver power to the gpu for each power supply source we measure the instantaneous current and voltage to compute power we sense the current draw by measuring the voltage drop across a current sensing resistor we use a ni daq to sample the voltage drop at a rate of 2 ms figure shows a detailed schematic of our setup the diagram illustrates the peripheral components that we are measuring as part of the total gpu power these periph erals include the gpu processor dram modules voltage regulator module vrm and other auxiliary support cir cuitry as shown in equation our power model is built to model both static leakage and dynamic power and there fore in our experiments we separate these two parts and validate them individually p measured p proc leak p mem leak p v rm p peripherals p const independent of frequency p proc dynamic p mem dynamic p dyn 2 simulator setup the software power model builds on mcpat it is integrated with gpgpu sim version 2 we configure the simulator to match the two gpu cards separately we use gpgpu sim ptxplus mode to simulate the native instruction set isa on the quadro gpu sass we also use the nvidia compute profiler to ensure that the mi crobenchmarks performance matches the target hardware 2 constant power component the measurement setup captures both dynamic and con stant power equation shows that each power source p const scales with frequency and p dyn consists of different components the first part p const includes processor leakage power main modeled power figure gpu power measurement schematic memory leakage power vrm power and all peripheral cir cuits power p const dram dram measured power m r v v gpu gpu chip v mem resistor sense supply power atx dram dram gpu card peripherals probe 3v 3v ni probe daq pcie is independent of processor memory frequency the other part p dyn comprises the dynamic power of both the processor and the main memory p dyn scales linearly with processor memory frequency therefore if memory frequency scales the same as the processor fre quency we can rewrite the measured power in equation as a linear function of only the processor frequency f this is shown below in equation p measured k f p const k constant f processor frequency using equation with varying frequencies f we can determine the constant power component with the aid of overclocking tools for the nvidia gpu card we scale the processor and dram frequencies separately and measure power with the simplified linear model in the equation we performed a linear fit to get the constant power component subtracting the constant power numbers from the mea sured total power gives us the dynamic power component of the gpu processor and its main memory we perform the above procedure on both the test cards figure shows the experiment results for gtx we use two different work loads the first workload exercises the integer unit heav ily while the second workload exercises the floating point unit we choose these two computation intensive workloads to minimize interaction with the dram memory subsys tem figure shows the constant power component for the gtx card which is approximately w the constant power component for the quadro card is w another reason for determining the constant power com ponent is to get the static leakage power of both gpu cards our model reports w and w leakage power for the gtx and quadro respectively however we cannot validate these numbers owing to the lack of publicly available resources on gpu leakage power numbers nev ertheless according to equation the linearly fitted con stant power serves as an upper bound which the results re ported by our power model do not violate we do not have leakage power estimates for the memory because we adopt an empirical approach to estimate its power dynamic power validation we validate the power model using microbenchmarks and real programs from public benchmark suites we compare the average power and the dynamic power behavior of sev eral kernels against the measured hardware results the measurement process has inherent limitations that affect the measured functional measured fitted w r e 50 memory dram mix workload workload 2 w o p d e t a l u im s fitted constant power 0 0 0 2 6 50 200 processor frequency ghz measured power w figure linear regression figure microbenchmark fit to get constant power power comparison benchmark selection and dynamic power trace comparisons and this is not to be confused with simulation inaccuracies 1 hardware measurement issues in our measurements we find that any large changes in power draw are accompanied by an exponential increasing or decaying curve i e rlc effect as well as an oscilla tion as shown in figure a the behavior is related to the power delivery network pdn component characteris tics and interactions i e voltage regulator module and de coupling capacitance which effectively form an rlc cir cuit these effects are impossible to isolate from our power measurements because they are board level components we verified that the exponential power trace is not caused by workload or architectural policies such as thread scheduling by constructing microbenchmarks figure b shows a snapshot of the measured and simu lated power trace for mgst in which we can mimic the rlc behavior in the measured trace by transforming our simulated power trace with a derived rlc filter this rlc filter is derived based on figure a which is effectively the filter step response analyzing figure a the fil ter can be approximated as a second order system with rise time 0 18 μs and maximum overshoot in our experiment we must filter out gpu kernels with execution less than μs because the measured power trace of this type of kernel is always in the exponential increase stage due to the rlc effect this type of kernel could have a much higher power consumption than the measured value thus we only trust kernels with long enough execution times μs for validation purposes for each benchmark we use the largest available input sizes and modify parameters to increase the execution time without changing functional ity however this execution time limitation still filters out unique kernels in the ispass and rodinia benchmarks suites to the kernels discussed in section in addi tion we study another four kernels from prior work by hong and kim so in all we have long running kernels to avoid the rlc effect our microbenchmarks were designed with long enough typically several seconds execution time 2 microbenchmark based validation following the rigorous design methodology discussed in section we designed microbenchmarks that are used to iteratively refine our power model we present the final power model accuracy for these microbenchmarks and vali date the component level power isolation microbenchmarks by showing their simulation power distribution average power test coverage is one purpose of our mi constant b quadro bar chart left is sorted in order of accuracy figure estimated average power comparison and breakdown for real benchmarks under the microbenchmarking tests is 2 this gpu has a different implementation of the memory coalescing logic 30 from the simulator which causes our memory mi crobenchmarks that were designed for the fermi to perform poorly thus resulting in the slightly larger modeling error component level validation it is impossible for us to measure the component level power consumption we achieve the component level modeling validation by isolating the component power when designing the microbenchmarks as explained in section 2 if the targeted component con sumes most of the total power the error in the total power consumption is a good indicator of that component mod eling error in the ideal case that the component consumes of the total power this component modeling error is equivalent to the total power error we present our microbenchmarks power isolation effect in heat map figure which shows estimated per component dynamic power distribution for a representative set of the component level power isolation microbenchmarks by stress ing a certain component each colored grid corresponds to the percentage of estimated dynamic power for that partic ular microarchitectural component x axis in the designed microbenchmark y axis for example the execution unit for the functional microbenchmark in figure consumes 6 of the total dynamic power with significantly less power consumed by the register file reg and pipeline pipe achieving component power isolation for the execution units and dram is easier these two components consume 6 and of the total power separately in their stress testing microbenchmarks it is almost impossible to isolate cache power from noc power because each cache access involves an noc access we also found that caches data constant and texture are hard to isolate for instance the data cache d consumes 29 1 of the total power whereas register file and execution units still consume and power separately this extra power overhead is caused to some extent by the data de pendencies figure 5 that we intentionally insert to prevent the compiler from optimizing away the microbenchmarking code i in summary the functional units and dram mi crobenchmarks achieve better power isolation on targeted components thus total power error better indicates compo nent modeling error however this does not mean other components have higher modeling errors the reason is two fold firstly we designed more complex patterns to exercise these components secondly these components are majorly sram based arrays which have less modeling uncertainty as compared to functional units and dram 3 3 benchmark based validation we evaluate the power model on a set of benchmarks that are not used in the refinement process we use 25 kernels from the different benchmarks presented in figure there are 18 kernels from the rodinia suite and 3 from the ispass suite we also include four microbenchmarks presented in prior work mgst kernels are not shown for the quadro because they fail to run on the card average power figure shows the our power model accuracy compared to measured power along with component level breakdown of power consumption for both the gpu cards the results are sorted in order of modeling accu racy for gtx the power model achieves an average error within 9 9 memory intensive kernels such as the first kernel in kmn kmn and cfd show more er ror due to the mismatch in memory hierarchies between our performance model and the real hardware for the quadro card the model obtains an aver age error of the results are weaker because gpgpu sim performance correlation in terms of execution time is for this card and for the gtx moreover the number of evaluated benchmarks for quadro is less than that used for gtx because this card does not support the computation ability required by mgst comparing the performance per watt we deduce that the gtx is 3 better than quadro gtx con sumes 1 33 the total power of the latter but it is faster in performance the contributing factors include sev eral changes in the gtx for energy efficiency and per formance such as the presence of more functional units in creased simd width and the introduction of data caches the only kernels that show a decrease in the overall perfor mance per watt are kmn mum and lib they do not have enough data locality to utilize the caches effec tively and thus retrieve data more often from dram the pie charts in figure show the power breakdown of the modeled dynamic power consumption in the two cards the chart is derived by averaging and normalizing all ker nels power breakdowns shared memory texture constant and data caches contribute much less than other components to the average dynamic power and are thus grouped into the other label in the pie chart the lower power consumption for these components are caused by few kernels using these structures the increased number of functional units and simd width cause execution units to increase from in quadro to 1 in gtx pipeline power in creases from to because of the increased pipeline depth in gtx gtx has added both data caches which reduces the dram and memory controller mc power consumption from 8 and 8 3 to 8 and 4 8 separately note that although gtx has a larger register file rf size the percentage of rf power consumption actually increases because the total power con sumption of gtx increases more than the rf does runtime trace we also validate the dynamic behav ior of our power model against the hardware measurements this is an important step because a steady power trace can have the same average power as a trace that is oscillating around the steady state power in figure we show the trace of five kernels from benchmarks lib cfd and mgst these kernels are run separately and thus the time in the horizontal axis is not continuous moreover we only show 0 8 1 2 0 5 1 0 execution time ms each kernel runs separately so the x axis is not continuous figure measured and modeled power trace comparison 1 ms of kernel execution time due to space constraints the start time for all kernels but one mgst discussed later is 0 4 ms because data before this time includes the expo nential increase due to the rlc effect see section 4 3 1 the dynamic power profile in gpgpu applications can be categorized into two types the first type has a steady power consumption as seen in the first four kernels from lib to mgst in figure the second type is more like mgst which has phases in the power trace the power model tracks the steady state power consump tion trend accurately both across kernels from within a benchmark as well as across kernels from different bench marks for example the cfd kernel from the cfd benchmark has higher measured power w than the cfd kernel w lib and mgst have the lowest power consumption both below w another thing to notice is that the simulator typically appears more noisy compared to the measured trace because the hard ware rlc circuit forms a low pass filter that smooths out the measured power trace the simulator is in fact effec tively capturing subtle variations in workload activity in mgst the dynamic power consumption drops past 0 5 ms due to load imbalance cores are idling in the sec ond half of the kernel execution as shown in figure mgst has a relatively large portion of idle sm power and the model accurately tracks this behavior in the tem poral perspective note that this kernel starting time is not 0 4 ms as for other kernels because it has only 1 ms execution so the whole execution time is shown here the exponential decay pointed out by the arrow is again caused by the rlc effect rather than any inaccuracies in our model 4 4 power model extensions on the basis of the methodology in figure 1 a the power model we have described thus far can adapt to newer archi tectures depending on the extent of the differences in the target architecture from the existing power model full or partial iterations of the refinement and validation loop may be required in this process components may need to be added or removed from the power and performance simula tors activity counters may need to be added that capture the behavior of the modified architecture and additional microbenchmarks may also be necessary to stress and refine the new components consider the example of adapting the power model from the quadro to the gtx architecture this ex tension requires the addition of new components such as and data caches adding these components requires designers to systematically go through the process of ini tial modeling microbenchmarking and iteratively refining and validating the power model however differences in the implementation of the execution units between the quadro and gtx 28 can easily be captured by the refinement and validation stages using the microbenchmarks targeted at the functional units 5 energy optimizations we find that a number of kernels have strong phase behav ior thus we demonstrate benefits using dvfs exploiting runtime memory and compute boundedness furthermore we quantify the potential for fine grained lane level clock gating during branch divergence both techniques are based on the gtx configuration in the gpgpu sim b clustered phase behavior in hrtwl figure two types of phase behavior 5 1 exploiting phase behavior we studied the opportunity for processor level dvfs i e single voltage and frequency value for the entire processor within a kernel by tracking the average stall cycles in ev ery sm caused by memory operations we conducted the analysis at the individual sm level which indicates that in most programs all the sms are in global synchronous be havior this is to be expected because programs suitable for gpu architectures are inherently optimized for through put behavior however an interesting aspect of the global synchronous behavior is that the gpu kernels exhibit phase behavior we also investigated the opportunity for cluster level dvfs optimization in which groups of sms that be have similarly are clustered together so that a few voltage domains can allow for fine grained control figure a shows the averaged stall cycles during the execution of stmcl although the stall cycles shown in the plot are averaged across all sms each sm s behavior is almost identical due to the global synchronous behav ior the benchmark repeatedly shifts between compute and memory bounded phases every 000 cycles processor level dvfs can leverage these memory bounded phases in this case however in some gpgpu programs sms are not in global synchronous behavior for example benchmark hrtwl suffers from load imbalance in this benchmark all the sms are assigned thread blocks to execute at the start of program execution after the sms finish executing the assigned thread blocks there are no more blocks for further assignment to sms 3 5 and 6 figure b these sms remain idle but they consume non negligible power we implemented a dvfs algorithm by monitoring the average stall cycles caused by memory operations which is used to decide whether the kernel is memory bounded in the case of global synchronized behavior it is sufficient to monitor either a single sm or averaged metric across all sms we assume that the gpgpu has p states ranging from a peak of mhz to a minimum of mhz with step of mhz these settings align with gtx s existing dvfs settings see section 4 we use the nm predictive technology models 2 to scale the voltage with frequency from 1 v to 0 55 v the baseline architecture is always running at the highest frequency we quantify dvfs bene fits under two scenarios first we assume a fast responding on chip regulator that can make p state transitions quickly within cycles which are similar to those used by kim et al in prior work for cpus 19 second we consider cycles cycles figure energy savings and performance loss for fine grained and coarse grained processor level dvfs conventional off chip regulator with coarser granularity of 000 cycles transition time or roughly μs figure shows the results of the processor level dvfs algorithm for both settings benchmark stmcl from fig ure achieves 8 energy savings with only 0 loss in performance assuming fast on chip regulators even in the case of slow off chip regulation the energy savings is 3 because the duration of each computation memory bound phase is typically long enough that the advantage of fast responding diminishes the fine grained dvfs scheme saves about more energy than coarse grained dvfs in benchmarks kmn ss and srad these programs have short phases that only on chip regulation can effectively leverage for benchmarks that are memory bounded during the entire execution such as bfs and mum both the dvfs schemes achieve more than energy savings purely compute bound benchmarks such as lud and hotsp do not ben efit from dvfs the worst case performance loss is cfd in the fine grained dvfs scheme because it has some rapid phase changes which our simple prediction technique fails to capture overall the algorithm achieves 4 and 2 savings in energy under both fine grained and coarse grained dvfs with 2 9 and 1 3 performance loss respectively we also evaluate the benefit of cluster level dvfs for benchmark hrtwl in figure b with the cluster level dvfs we set the frequency of the idle sms to the lowest p state point i e mhz and 0 55 v the idle sm leak age power reduces to approximately 10 2 cluster level dvfs achieves 6 energy savings while processor level dvfs achieves only 6 6 to put the benefits of cluster level dvfs in context we compare it against ideal power gating which achieves a energy reduction the cluster level dvfs scheme does nearly as well as ideal power gat ing however there are several factors that designers must consider such as modeling all of the additional overheads careful trade off analysis for this optimization is beyond the scope of this paper 5 2 harnessing branch divergence gpgpu programs suffer from branch divergence fig ure shows the amount of branch divergence related lane inactivity the data is constructed by calculating the frac tion of time when none that is idle one to two or three to four out of total simd lanes are active similar to for example benchmark nn has only one or two lanes active during execution whenever branch divergence oc curs inactive execution lanes are idle but they still consume clocking and toggling logic dynamic power we propose to minimize the unnecessary toggling dynamic active lanes no divergence figure time distribution of active lanes power by applying clock gating to the unused idle execution lanes this technique is well suited to exploit the short du rations of branch divergence we design microbenchmarks with different numbers of active threads to verify that nei ther card implements clock gating to the best of our knowl edge there is no published literature that explores power re duction using fine grained lane level clock gating for branch divergence in gpgpu architectures we implemented fine grained clock gating in the power model to ensure that only simd lanes with active threads are active in addition rf reads writes and operand collec tors are also clock gated at a fine grained granularity it is important to note that clock gating incurs power overhead owing to additional logic in the clock tree we model these overheads using the approach proposed by li et al we assume an overall pipeline depth of for the gpu architec ture execution units operand collectors and the crossbar network are clock gated in each simd lane level to pre vent unnecessary switching activity while the register files are gated in the register file bank level for each of the simd lanes and the pipeline stages per lane fine grained clock gating requires an additional and gate to gate the clock and an additional flip flop to store the clock gating control bit the control bit ensures that the decision to clock gate a stage is available before the start of the cycle the gpu maintains active lane masks for all warps which can be used to determine which lanes should be clock gated we utilize the nm tsmc technology library to estimate the power consumption of the additional flip flops and logic gates by our estimation lane level clock gating increases the gpu power consumption by 0 3 w this power is con sumed regardless of the lane level activity but as we will see next this overhead is negligible relative to the net benefits in our experiment we found the power saving of idle lane clock gating is proportional to the time of control divergence figure shows the dynamic power saving of applying this technique note that we only count the dynamic proces sor i e without leakage and dram power benchmark nn benefits the most 6 savings from the clock gating because it has only 1 or 2 lanes active for more than ex ecution time as shown in figure although benchmarks hotsp and lps both suffer severely from divergence they only show 4 and 6 dynamic power savings respec tively because they have more active lanes during diver gence other benchmarks such as kmn sto and srad have little or no divergence behavior and thus show no im provement overall the fine grained technique achieves a geometric averaged 2 dynamic power savings 6 related work there have been prior works regarding building a gpgpu power model 8 we compare our model against these works from the perspective of the power mod baseline with clock gating figure dynamic power saving after clock gating eling methodology and the necessary validation effort modeling methodology zhang et al showed that gpu power consumption increases linearly with the program s computation intensity and they use this to pre dict the power consumption similar to maruyama et al use hardware performance counters to predict power hong and kim propose a power and performance pre diction model for gpus that predicts the gpu s execution time and power consumption using kernel and architecture characteristics these modeling methodologies are good for program level power prediction but they lack our frame work s configurability for modeling the power consumption of different gpu architectures moreover although these previous works can only predict the gpu s average power consumption our proposed model can provide cycle level power trace of gpu programs which lets us capture differ ent execution phases within a kernel validation effort hong and kim measure the power of the whole system for validation whereas we iso late and measure the power that is consumed only by the processor additionally by conducting the voltage and fre quency scaling experiment mentioned in section 4 we isolate the constant power component from our measured power to validate the static leakage power and dynamic power sep arately we validated the dynamic power with a suite of microbenchmarks that contains kernels that exercise dif ferent microarchitectural components moreover with the 2 m s sampling rate we measured the power consumption of real benchmarks the execution time for these bench marks is typically around several milliseconds we show the measured traces of real benchmarks and use those traces to validate the runtime power trace of our power model prior work 37 leveraged memory compute phase be havior for cpu energy optimizations we adopt a simi lar methodology to conduct phase analysis at the individ ual sm level and identify both global and clustered syn chronous behavior across sms and use that to demonstrate the benefits of processor level dvfs lee et al per formed the oracle study for processor level dvfs in gpus but they tried to maximize throughput given a power con straint while we try to minimize the energy consumption moreover we study dvfs opportunity within kernel execu tion while their study emphasizes behavior across kernels the impact of fine grained clock gating on power con sumption has been studied for cpus and some simd processors 31 we exploit this technique to solve a gpu specific problem i e branch divergence 7 conclusion the community requires a robust gpu power model we demonstrate a configurable cycle level and validated power model for gpgpus that can be used for architecture and software energy efficiency studies the robustness of the power model is proven against the measured power of two commercial gpus using a complete suite of both microbench marks and real benchmarks the power model achieves the averaged absolute error within 9 9 for gtx card with 4 for quadro respectively for our evaluated benchmark suite from rodinia and ispass we developed this model on the basis of a strong power modeling method ology that will enable us to extend the existing power model systematically to support future gpu architectures using gpuwattch we show that fine grained and coarse grained dvfs are useful for reducing dynamic power con sumption in gpgpu workloads because they exhibit phase behavior on average coarse grained dvfs achieves 13 2 energy savings while fine grained dvfs achieves 4 en ergy savings both with less than 3 performance loss for kernels with short lived phases fine grained dvfs achieves 7 more energy savings than coarse grained dvfs in addi tion we explore the opportunity for sm cluster level dvfs to address the load imbalance problem in some gpgpu workloads this technique reduces energy consumption fur ther by 7 for benchmark hrtwl additionally we eval uated lane gating by clock gating individual simd lanes during branch divergence we show that in some programs with high branch divergence such as nn this technique re duces dynamic power consumption by more than while the quality of the current cmap additive force ﬁeld for proteins has been demonstrated in a large number of applications limitations in the model with respect to the equilibrium between the sampling of helical and extended conformations in folding simulations have been noted to overcome this as well as make other improvements in the model we present a combination of reﬁnements that should result in enhanced accuracy in simulations of proteins the common non gly pro backbone cmap potential has been reﬁned against experimental solution nmr data for weakly structured peptides resulting in a rebalancing of the energies of the α helix and extended regions of the ramachandran map correcting the α helical bias of cmap the gly and pro cmaps have been reﬁtted to more accurate quantum mechanical energy surfaces side chain torsion parameters have been optimized by ﬁtting to backbone dependent quantum mechanical energy surfaces followed by additional empirical optimization targeting nmr scalar couplings for unfolded proteins a comprehensive validation of the revised force ﬁeld was then performed against a collection of experimental data i comparison of simulations of eight proteins in their crystal environments with crystal structures ii comparison with backbone scalar couplings for weakly structured peptides iii comparison with nmr residual dipolar couplings and scalar couplings for both backbone and side chains in folded proteins iv equilibrium folding of mini proteins the results indicate that the revised charmm parameters represent an improved model for modeling and simulation studies of proteins including studies of protein folding assembly and functionally relevant conformational changes and amber ildn ildn q and charmm from the perspective of protein folding it has been shown that in long simulations with the additive force ﬁeld certain fast folding proteins will reach the native state starting from a completely unfolded conﬁguration e g villin headpiece subdomain however there are some indications of signiﬁcant deﬁciencies examples include misfolding encountered in long simulations of the pin ww domain starting from an unfolded conﬁguration and diﬀerences of the villin folding mechanism from that inferred experimentally in the case of the ww domain free energy calculations subsequently showed that the misfolded states were in fact lower in free energy than the folded state conﬁrming that the energy function was at fault a number of studies have suggested that such discrepancies may be largely due to relatively small inaccuracies in the potential for the introduction accurate force ﬁeld parameters are essential for the application of empirical potential energy functions to study protein structure stability folding and function as a result of many years of careful reﬁnement current additive protein energy functions are of suﬃcient quality that they may be used predictively for studying protein dynamics and protein protein interactions and in pharmacological applications it is clear that the next major step in advancing protein force ﬁeld accuracy requires a diﬀerent representation of the molecular energy surface in particular the eﬀects of charge polarization must be included as ﬁelds induced by ions solvent other macromolecules and the protein itself will aﬀect the charge distribution however since it may be some time before such energy functions are widely available and computationally accessible for mainstream use it is important to ensure the highest possible accuracy of current additive force ﬁelds indeed over the past few years more rigorous evaluation of force ﬁelds in the context of conformational sampling of polypeptides and protein folding have led to new variants of the amber and force ﬁelds including amber american chemical society received may published july dx doi org j chem theory comput journal of chemical theory and computation article backbone resulting in a net bias toward either structure which may be corrected by means of a minor adjustment to the backbone potential here we report a revised set of all atom protein force ﬁeld parameters that represents a signiﬁcant improvement in representing the potential energy surface of proteins within the context of the current functional form the changes described in the present work are i a new backbone cmap potential reﬁned against a range of data for dipeptides as well as experimental data on small peptides such as hairpins and helices and ii new side chain dihedral potentials optimized against quantum mechanical energies from dipeptides and nmr data from unfolded proteins other diﬀerences from the previous cmap protein force ﬁeld include previously published revised lennard jones lj parameters for aliphatic hydrogens improved treatment of the internal parameters for the guanidinium ion and new parameters for tryptophan the backbone and side chain improvements have been undertaken in parallel such that the new force ﬁeld is balanced with respect to the contribution of these moieties to protein structure and dynamics validation of the force ﬁeld against a wide range of test systems including comparison with other state of the art protein force ﬁelds indicates improvements in the quality of the force ﬁeld in reproducing a number of experimental observables with a å cubic box for ns at k using the same nonbonded treatment thermostat and barostat as those for ac aaqaa below ii replica exchange simulations of ac aaqaa the n terminally acetylated and c terminally amidated peptide initially in a helical conformation was solvated in a truncated octahedron simulation cell with a distance between nearest faces of å using gromacs the resulting system contained water molecules it was found that a larger box size was necessary than was used in earlier work with amber force ﬁelds as the coil state was more expanded a ps simulation was run at a constant pressure of bar to obtain equilibrium box dimensions at k the peptide was then unfolded using a ns constant volume simulation at k starting from the ﬁnal conﬁguration from this simulation in which all peptide bonds were trans constant volume replica exchange md was run using gromacs with replicas spanning a temperature range from to k and exchange attempts every ps for a total of ns per replica electrostatic interactions were computed with pme using a real space cutoﬀ of å and a å grid spacing and lennardjones interactions were switched oﬀ smoothly between and å a langevin thermostat with a friction coeﬃcient of ps was used with a time step of fs to integrate the equations of motion a residue was deﬁned as being helical if it lay within a stretch of at least three residues in the α helical region of the ramachandran map deﬁned as ϕ and ψ to deﬁne residues that lay within the wider α minimum but not necessarily part of a helix a larger range of angles denoted α was used i e ϕ and ψ iii solute tempering simulation of unfolded proteins in urea simulations of unfolded proteins and ubiquitin in urea were performed at a constant pressure of bar using a truncated octahedron cell with a distance between nearest faces of å using gromacs the protein was solvated in an m solution of urea in water with the model being used for urea based on the favorable results obtained in peptide simulations using this model four potassium ions were added to the simulation to neutralize the overall charge simulations of ns duration of the unfolded proteins were found to be insuﬃcient to get a representative sampling of side chain rotamer distributions particularly for the bulky aromatic side chains since for systems of this size atoms standard replica exchange would require a very large number of replicas we instead used the solute tempering replica exchange proposed by liu et al in this scheme the pairwise forces are split into protein protein protein water and water water interactions and the energy of the water water interactions is scaled in such a way that the water water energy vanishes from the replica exchange acceptance criterion speciﬁcally the potential energy of replica m is given by methods simulation methods several simulation codes were used to perform the calculations in this work reﬂecting the packages in which the charmm force ﬁeld is currently implemented itself and gromacs all molecular dynamics md simulations were performed in explicit water with periodic boundary conditions and with long range electrostatics treated using particle mesh ewald pme nonbonded pair interactions were treated similarly in all packages as described below the water model used was for simulations with charmm and a modiﬁed including lennard jones parameters for was used while for gromacs the standard was used comparison of results with the two variants for the and ac aaqaa test cases discussed below showed negligible diﬀerences i simulations of and other short peptides and were simulated in the npt ensemble at k and atm pressure under periodic boundary conditions all peptides were unblocked and had protonated ctermini experimental ph is initial box sizes were and for tri penta and heptapeptides respectively pme was used to calculate the electrostatic interactions with a real space cutoﬀ set to å and a å grid spacing while the lj interactions were treated with a switching function from to å the equations of motion were integrated with a fs time step while shake was used to constrain covalent bonds involving nonwater hydrogen bonds and was used to maintain rigid water geometries all of the peptides were simulated for ns each with the new force ﬁeld in addition was also simulated for ns with the previous cmap force ﬁeld amber amber opls aa and gromos all of the simulations were carried out with namd version the equilibration protocol for all of the simulations consisted of initial minimization followed by stepwise heating to k simulations of zwitterionic gpgg were run using gromacs kbtm with kb being boltzmann constant and tm the temperature of replica m replica is the replica for which the correct equilibrium distribution is desired sampling was done via a general hamiltonian replica exchange utility implemented in a modiﬁed version of gromacs simulation parameters were the same as for ac aaqaa with a ps interval between replica exchange attempts iv replica exchange simulations of remd simulations were also performed for the unblocked peptide derived from hen egg white lysozyme with sequence kvfgrc sme elaaamkrhgldn the structure and parameters for the s methylated cys were adapted from those for methionine and are given in the supporting information figure and table respectively both termini as well as all acidic side chains were protonated corresponding to the experimental conditions of ph the peptide was solvated in a truncated octahedron simulation cell with a å distance between nearest faces and equilibrated at a constant pressure of bar for ps at k constant volume remd was run with replicas spanning the temperature range k for ns of which the ﬁrst ns were discarded in the analysis all other simulation parameters were the same as for ac aaqaa v crystal structure simulations simulations of the proteins in their crystal environments table which were used previously during optimization of the cmap force ﬁeld were performed using charmm on full unit cells with added waters and counterions to ﬁll the vacuum space once the full unit cell was constructed on the basis of the coordinates in the protein databank a box of water with dimensions that encompassed the full unit cell was overlaid onto the crystal coordinates while preserving crystal waters ions and ligands water molecules with oxygen within å of any of the crystallographic non hydrogen atoms were removed as described below as well as those occupying space beyond the full unit cell to neutralize the total charge of each system sodium or chloride ions were added to the system at random locations at least å from any crystallographic non hydrogen atom or previously added ions and å from any water oxygen final selection of the water molecule deletion distance was performed by initially applying a å criteria to all systems followed by system equilibration and an npt production run of ns following which the lattice parameters were analyzed the deletion distances were then increased and the equilibration and ns production npt simulation were repeated until the ﬁnal lattice parameters were in satisfactory agreement with experimental data the ﬁnal water deletion distances and unit cell parameters from the full ns production simulations are presented in table of the supporting information for the minimization and md simulations electrostatic interactions were treated with pme using a real space cutoﬀ of å the lj interactions were included with force switching from å to å while the list of nonbonded atoms was kept for interatomic distances of up to å and updated heuristically each crystal system was ﬁrst minimized with steps of steepest decent sd with nonwater non ion crystallographic atoms held ﬁxed followed by steps of sd with harmonic positional restraints of kcal mol on solute non hydrogen atoms the minimized system was then subject to an equilibration phase consisting of ps of nvt in the presence of harmonic positional restraints followed by ns ps for l and of fully relaxed nvt simulation with a time step of fs during the simulations all covalent bonds involving hydrogens were constrained using shake production phase simulations were conducted for ns in the isothermal and isobaric npt ensemble the only symmetry enforced was translational i e periodic boundaries reference temperatures were set to match the crystallographic conditions table supporting information and maintained by the nose hoover thermostat with a thermal piston mass of kcal mol while a pressure mass of amu was used with the langevin piston the ﬁrst ns of the production simulations were considered as equilibration and therefore discarded from analysis which was performed on coordinate sets saved every ps the boundaries for α helices and β strands were obtained from a consensus of author annotations and structural assignments calculated by and from the crystal structures vi simulation of a dimeric coiled coil in solution simulation of a dimeric coiled coil in solution was initiated from model of the nmr structures in system preparation using charmm with inputs initially generated using the charmm gui involved solvating the dimer with an aqueous solution of approximately mm kcl all nonhydrogen atoms in the peptides were then harmonically restrained kcal mol and the system subjected to a step sd minimization followed by a ps npt simulation at k in the presence of the restraints the ﬁnal coordinate set was used to initiate the production simulation using namd with the coordinates saved every ps for analysis simulations involved a fs time step treatment of electrostatics via pme with a real space cutoﬀ of å with smoothing of the lj interactions via a switching function over to å vii simulations of folded proteins in solution simulations using gromacs were carried out for the following four folded globular proteins bovine pancreatic trypsin inhibitor bpti ubiquitin and hen lysozyme starting from the published experimental structures pdb entries respectively each protein was solvated in a truncated octahedron simulation cell ﬁlled with water with nearest distance between images dx doi org j chem theory comput journal of chemical theory and computation article the total number of data the error estimate was based on the uncertainty in the prediction of the experimental observables from the simulation that is the estimated error in the predictions from the karplus equation and the sparta algorithm the motivation for this choice is that should be when the average deviation from experiment is comparable to the error in the prediction although it diﬀers from the standard deﬁnition of the statistic in most cases the error of the experimental measurement and the statistical error from the simulation are small relative to this prediction error typical values for the statistical uncertainty of backbone j couplings from simulations are hz e g see table compared with hz for the error in the karplus equation prediction and for chemical shifts ppm statistical error compared with ppm prediction error values of σ used for each type of scalar coupling and chemical shift are given in table of the supporting information for side chain jcouplings the sampling errors can be more signiﬁcant as a result of the slow transitions about χ torsion angles for these we have propagated the statistical error in the j couplings to report an error on the estimate of for gly and pro cmaps based on the respective cbs aug cc pvdz energy surfaces were used without additional modiﬁcation side chain and dihedral parameter optimization dipeptides corresponding to each residue excluding gly pro and ala were generated with the program charmm the dipeptides contain n acetylated and n methylamidated termini and are representative of the amino acid side chains in the local environment of peptide backbone for the initial ﬁtting of χ parameters we used a published set of energy surfaces derived from qm as target data the selected qm level cc pvtz g for the asp and glu dipeptides represents a compromise between accuracy and computational accessability that has been shown to yield conformational energies comparable to ccsd t complete basis set calculations on complex molecules including those containing anomeric oxygens charmm potential energy surfaces were generated in a manner consistent with the qm target data which consisted of full scans of the and dihedral angles in increments for each dipeptide constrained at each of the following backbone conformations α β or αl initial coordinates were obtained from the qm optimized geometries structures were minimized for steps of conjugate gradient conj and steps of adopted basis newton raphson abnr with a convergence criteria of the root mean square rms gradient kcal mol å with harmonic dihedral restraints of kcal mol rad on the respective dihedrals the resulting energy surfaces were oﬀset relative to the respective global minimum for each residue type a monte carlo simulated annealing mcsa automated ﬁtting was used for the optimization of side chain dihedral parameters this program allows simultaneous ﬁtting of multiple dihedrals for an arbitrary number of dipeptides as required for ﬁtting parameters for dihedrals common to more than one side chain during optimization of the dihedral parameters the multiplicities n were restricted to and the force constant k was limited to k and the phase δ limited to either or energy surfaces from all three backbone conformations were considered together as target data the objective function of the ﬁtting program shown in eq of å for all proteins except for lysozyme for which the distance was å sodium and chloride ions were added as needed to yield a ﬁnal salt concentrations of mm with adjustments to ensure charge neutrality for each protein a step sd energy minimization of the whole system was performed followed by ps of md at a constant pressure of bar and temperature of k in which harmonic positional restraints of kcal mol were applied to each cartesian component of each protein non hydrogen atom using the minimized structure as a reference each protein was then simulated at a constant pressure of bar and a temperature of k for ns pressure was regulated by a parinello rahman with a coupling time of ps otherwise all details were as described for ac aaqaa viii replica exchange folding simulation of β hairpins remd simulations were performed on two well studied β hairpins the hairpin residues of protein g and using gromacs completely unfolded structures obtained from high temperature vacuum simulations were solvated in a å truncated octahedron box with water molecules the remd simulations spanned a temperature range k utilizing replicas suﬃcient to achieve an exchange probability around with a frequency of attempted exchanges of ps an initial sampling of ns per replica was obtained with a preliminary version of the force ﬁeld with a further ns per replica obtained with the ﬁnal parameter set of which the ﬁrst ns was discarded as initial equilibration all other details were as described for ac aaqaa ix quantum mechanical calculations quantum mechanical calculations of the gly and pro dipeptides were performed to obtain two dimensional ramachandran ϕ ψ potential energy surfaces with ϕ and ψ constrained in increments the dipeptides were optimized at the aug ccpvdz level of theory using gaussian the optimized structures were then subjected to single point energy determinations at the cc pvtz and cc pvqz levels of theory using qchem these energies were then used to extrapolate to the complete basis set cbs limit using the method of halkier et al for the pro dipeptide the surface was limited to ϕ to as the remainder of the surface is energetically highly unfavorable the resulting gly and pro cbs aug cc pvdz cmaps were then calculated from the qm surfaces as previously described optimization methods backbone optimization optimization of backbone parameters was done via the cmap potential vcmap φ ψ for which separate parameters are used for each of i non gly pro residues ii gly and iii pro for non gly pro residues the starting point was the cmap potential this cmap was further optimized as described further below to obtain an acceptable match to nmr data for both a short peptide and a longer helix forming peptide ac aaqaa the target functions for optimization were the deviations from the experimental observables that is scalar couplings for and carbonyl chemical shifts for ac aaqaa the target function optimized was deﬁned as is the rms energy diﬀerence rmsd where eqm and emm are the relative qm and mm energies respectively for conformation i c is an oﬀset to minimize the rmsd and w is a weighting factor for conformation i in practice the rmsd is calculated with the force constants on the dihedral parameters to be optimized set to zero such that during each iteration of the mcsa only the dihedral energies need to be evaluated and the change in the rmsd calculated in a computationally expedient fashion during ﬁtting we applied weighting factors of w on energy surfaces from α and β backbone conformations and w for data from αl each set of side chain parameters was ﬁtted with a minimum of mcsa steps and an initial temperature of k the initial parameters for condensed phase simulation testing were obtained as the arithmetic mean of ﬁve individual mcsa runs that diﬀered by the initial random number seed convergence of each run was veriﬁed by monitoring the rmsd throughout the optimization optimized parameters from the automated ﬁtting program were subsequently subjected to manual adjustments focused on the relative energy of the local minima in the qm surfaces to improve the reproduction of experimental relative rotamer populations in md simulations for comparison of and from the crystal simulations with experimental data crystal distributions for and were obtained from a survey of the protein data with the data converted to probability distributions using a bin size of the analogous probability distributions were calculated from the protein crystal simulations to measure the agreement between the crystallographic and simulated χ distributions the one dimensional overlap coeﬃcients oc for the two probability distributions were calculated over the sampled grid points as previously described using eq oc pm pn pm pn results parameter optimization the main aim of the optimization was to improve the parameter set for torsion angles in the force ﬁeld this was motivated in the case of the backbone by the results of folding simulations that showed the backbone parameters for cmap to have an excessive helical bias in addition since the torsion parameters had never been explicitly optimized in a side chain speciﬁc fashion it was felt this was an area in which improvements could be made the backbone and side chain optimization were initially undertaken independently and the results were then combined it was found that there was some coupling between the backbone and side chain parameters as described below the ﬁnal optimized parameters may be obtained from the mackerell laboratory web page at http mackerell umaryland edu html backbone optimization the charmm additive protein force ﬁeld contains in addition to the usual fourier terms for the backbone torsion angles ϕ and ψ a twodimensional cubic spline potential or cmap vcmap φ ψ the spline is speciﬁed by energies determined on a lattice with a grid spacing for internal residues we only modify the cmap terms for the backbone since these eﬀectively include any features of the energy surface that could be captured by the backbone torsion terms some independent optimization of the backbone torsion angles for terminal gly residues was carried out as discussed below the previous cmap optimized for use with the all atom protein force ﬁeld was based on the adiabatic qm vacuum energy surface of alanine dipeptide with empirical modiﬁcations to correct for systematic deviations of protein crystal structure simulations from the x ray diﬀraction structure we had initially aimed to base the new cmap on the qm dipeptide energy surface once more however it was found that the previous empirical corrections captured additional cooperativity that was lacking in the dipeptide based maps as described in a separate publication we therefore retained the original cmap as a starting point for further optimization for completeness we summarize here the strategy used to derive the cmap the cmap was initially ﬁtted to minimize the diﬀerence between the mm energy and that obtained from the qm surface calculated with cc pvqz g although this brought about some improvements it was found that simulations of proteins in their crystal environment exhibited systematic deviations from the backbone φ ψ angles in the experimental structures a probable cause of this deviation is that the additive energy function cannot capture many body eﬀects important in folded proteins as a compromise small manual adjustments of the cmap were made in order to match the minima for regions of canonical secondary structure more closely to those in the pdb with these changes the average backbone torsion angles from simulations of folded proteins in their crystal environment now matched those in the experimental structures in the present work the grid energies were initially optimized using an automated monte carlo based protocol to improve agreement with based nmr scalar coupling data iterative manual adjustments were then made in order to further improve the match with the experimental nmr data for peptides in solution speciﬁcally the following data sets i an extensive set of bond scalar coupling data for published by the group of and ii a set of chemical shift data reﬂecting helix formation in the residue peptide j couplings calculated with set of karplus and chemical shifts with sparta at k statistical errors from a block error are given in brackets a aaqaa at k the purpose of the ﬁrst data set is to capture intrinsic propensity for populating the diﬀerent minima in the ramachandran map in the absence of any signiﬁcant secondary structure while the second data set is for a peptide with a signiﬁcant helical population in each round of optimization a perturbation was used to estimate the correction needed to better match experimental data following which further sampling was performed to test the new parameter set an important principle that emerged was that one should not apply more than the minimal changes needed to match the experimental data to within the estimated error in calculating the experimental observables from simulation since overﬁtting to one data set may result in deleterious eﬀects for example it is possible to match the data for more closely by making the polyproline ii region of the map even lower in energy but we found that this has a very destabilizing eﬀect on β hairpins even after formation of interstrand hydrogen bonds the backbone remains in the polyproline ii region of the ramachandran map we summarize the ﬁnal results of the optimization for the two peptides and ac aaqaa in table together with the analogous statistics for the cmap the amber and the modiﬁed amber force ﬁelds we see that the overall agreement with scalar couplings for is much improved relative to cmap and also relative to and the individual residues sample more of the polyproline ii helix region of the ramachandran map than any of the other force ﬁelds and less of the enlarged α α helical region deﬁned as ϕ and ψ for the longer helix forming peptide ac aaqaa we ﬁnd that the overall fraction helix at k is much more reasonable than that obtained with the previous cmap and it is more in line with the experimental estimate of obtained by shalongo and stellwagen from nmr chemical shifts in and the similar estimate of which can be interpolated from the circular dichroism data of scholtz et al on the related peptide ac aaqaa y in water this agreement can also be seen at the level of individual residues as we show in figure since the fraction helix from experiment is only inferred by ﬁtting a thermodynamic model we have also calculated carbonyl chemical shifts using the sparta chemical shift prediction program sparta uses a neural network trained on sets of known folded protein structures and corresponding chemical shifts in order to predict shifts from structure the figure helix formation in ac aaqaa a fraction helix per residue estimated from at k black lines compared with average fraction helix calculated from remd simulations with solid blue symbols solid red symbols amber open blue symbols and amber open red symbols using the replica closest in temperature to k b average carbonyl carbon chemical shifts calculated with sparta from the simulations in a compared with experimental shifts color scheme as in a estimated error in carbonyl shift prediction is approximately ppm although the training set does not include weakly structured peptides such as we consider it does include loops in folded proteins which are also not in canonical secondary structures in figure we compare the predicted shifts with the carbonyl chemical shifts measured in the original experiments conﬁrming the improvement aﬀorded by the backbone optimization a ﬁnal noteworthy feature of the data for the helix forming peptide is that the overall fraction helix is slightly higher than that in vs yet the overall population of the α region of the ramachandran map is lower vs this suggests additional cooperativity as mentioned above as described in a separate publication dx doi org j chem theory comput journal of chemical theory and computation article figure rms diﬀerences in relative energies between mm and qm potential energy surfaces rms diﬀerences were calculated for relative energies less than kcal mol above the global minimum for the three or surfaces higher energy cutoﬀs of kcal mol for arg and lys or kcal mol for asp glu and hsp were used for the charged amino acids table average and rms diﬀerences rmsd in kcal mol between mm and qm calculated energies for all rotamers over all the amino acids average diﬀerence cmap mcsa ﬁt of kcal mol deg for the force constant k the latter constraint reduces the likelihood of overﬁtting where individual parameters with large k values cancel each other thereby maintaining parameter transferability we also limited the phase δ to or such that the resulting parameters are not speciﬁc to chirality in addition during ﬁtting only those conformations with relative energies less than kcal mol were included in the calculation of the rmsd eq to avoid ﬁtting the high energy regions at the expense of the low energy regions that are accessible during md simulations higherenergy cutoﬀs kcal mol for arg and lys or kcal mol for asp glu and hsp were used for the charged side chains as favorable electrostatic interactions with the backbone in selected conformations led to deep minima in the energy surface such that other local minima are kcal mol above the global minima in addition to the criteria in the preceding paragraph and parameters were grouped based on sidechain type the groupings are shown in table of the supporting information for example initially lys arg gln glu and met have the same parameters fitting tests were undertaken in which selected atom types were changed to make the parameters for one or more of the side chains unique not shown based on this analysis unique atom types were introduced at the cβ atom of glu asp and protonated his hsp thereby allowing for improvements in the quality of the ﬁts of those side chains as well as of the remaining side chains in the respective groups gly and pro cmaps were determined by taking the diﬀerence between the target qm and force ﬁeld energies for gly and pro dipeptides respectively the new cmaps were already found to result in good agreement with nmr data for and gpgg with a signiﬁcant improvement for gly and were therefore not further adjusted since the gly termini are not aﬀected by the cmap we have performed a simple optimization on the n terminal ψ and c terminal ϕ torsion parameters in each case only a single parameter was optimized namely a dihedral force constant in such a way as to minimize the between simulated and experimental jcouplings for gly each terminus was independently optimized the old and new parameters are listed in table in the supporting information side chain optimization dihedral parameters that modulate side chain and conformational energies were initially optimized using qm energy surfaces as target data using a mcsa based ﬁtting followed by subsequent empirical optimization targeting the rotamer populations obtained from both solution and crystal simulations as described below we note that an alternative method to mcsa for optimizing torsion angle coeﬃcients is linear leastsquares ﬁtting we have opted for mcsa as it is more generally applicable and due to the greater ﬂexibility aﬀorded such as the inclusion of constraints on parameters as was used in the present study dihedral parameter multiplicities of up to the third term are used in conjunction with an upper boundary values calculated using eq combined values for ubiquitin and are obtained as an average over the two proteins weighted by the number of data points for each overall are obtained as an unweighted average of the for the diﬀerent types of residue statistical uncertainty from a block error analysis is given in brackets next to each number bn a indicates that no amino acids of that type are available in the respective proteins for asp and hsp this is due to poor reproduction of the local minima required to better reproduce the high energy regions a compromise that is due to inherent limitations in the form of the potential energy function that limits the ability to match the entire energy surfaces analysis of the rmsd for low relative energy regions i e kcal mol above the energy minima versus high relative energy regions i e kcal mol above the minima show the overall improvement for the low energy regions to be minimal with the largest improvements in the higher energy regions table supporting information such changes indicate that the new model may have a larger impact on side chain dynamics rather than on sampling of the diﬀerent rotamer populations see below while alternate ﬁtting protocols may alleviate the lack of signiﬁcant improvements in the reproduction of the qm data in the low energy regions as has been shown previously rigorous reproduction of qm gas phase data does not always ensure satisfactory reproduction of condensed phase properties target data for further optimization of the parameters following the initial mcsa ﬁtting was nmr j coupling data for ubiquitin and in m urea simulations of the two unfolded proteins were undertaken with the results showing that the initial mcsa based parameter set did not correctly reproduce the experimental χ rotamer distributions as judged by the reproduction of nmr data table therefore empirical adjustments of the χ dihedral parameters were undertaken to optimize the relative energy of the local minima in order to obtain improved side chain sampling in the unfolded proteins while maintaining overall ﬁt to qm data this process primarily involved adjustments of the and dihedral terms to shift the relative energies of the minima without signiﬁcantly impacting the higher energy regions of the surfaces following several iterations of parameter adjustments and simulations the ﬁnal parameter set was rms diﬀerences between mm and qm energy proﬁles for those conformations below the relative cutoﬀ energies are shown in figure as is evident signiﬁcant improvement was made in the overall agreement following the initial mcsa ﬁt with respect to cmap this is expected as the cmap parameters were typically assigned values associated with alkanes and not explicitly optimized targeting qm or other data in table the average and rms diﬀerences in relative energies of the expected minima or rotamers between mm and qm levels of theory are presented the individual relative energies and deﬁnitions of the minima for the studied amino acids are shown in tables and of the supporting information the results indicate that although the mcsa ﬁt brought the energies closer to qm as compared to empirical ﬁtting based on reproduction of the rotamer distributions from protein simulations narrowed the gap further for the α and αl backbone conformations the rmsd values increased with the β conformation due to compromises made to better satisfy the agreement of the other secondary structures and better reproduce sampling in the protein simulations for most side chains the relative energies of the various rotamers are satisfactorily reproduced although the quality of the agreement varied signiﬁcantly between the diﬀerent residues tables and supporting information a signiﬁcant exception occurred with glu which has the highest rmsd associated in part with the charged nature of the residue while mcsa ﬁtting lead to improvement over the cmap rmsd these parameters yielded poor agreement between crystallographic rotamer distribitutions and results from crystal md simulations accordingly additional optimization was performed for the torsion of glu via adjustments to the fold torsion term to improve agreement with crystallographic survey distributions poor agreement figure rmsd kcal mol is also seen dx doi org j chem theory comput journal of chemical theory and computation article figure sampling of backbone ϕ ψ torsion angles in the central residues of and colors indicate relative free energies according to color bar given on the right figure sampling of ϕ ψ torsion angles in the central residues of with diﬀerent force ﬁelds the new force ﬁeld the previous cmap force ﬁeld amber opls aa gromos and amber supporting information as well as of the relative sampling of the χ rotamers based on reproduction of the nmr data obtained the agreement with the nmr target data on the unfolded proteins is signiﬁcantly improved table while the rmsd for the ﬁnal set of parameters with respect to the qm data was generally only slightly worse than that based on the mcsa ﬁt parameters figure the one exception is glu where signiﬁcant disagreement with crystal data was observed requiring signiﬁcant adjustment of the torsion parameters the ﬁnal set of χ dihedral parameters is anticipated to treat the higher energy regions of the side chain conformations more accurately based on better reproduction of the qm data table results validation of new parameters results for short peptides the ϕ ψ sampling for and is shown in figure in alanine and valine based peptides the dominant minimum lies at ppii but additional minima at and αr are only slightly higher in energy additional minima at αl and are about kcal mol higher than the ppii conformation there is only a small diﬀerence between and but the sampling for all j coupling values in hz bexperimental data from graf et al cdata from ref ddata from present work the set of karplus parameters denoted in ref was used values calculated using eq statistical errors on individual j couplings are given in brackets next to the values based on block error analysis probing the ψ backbone torsion angle however the main reason for the improvement over other force ﬁelds e g is due to the j couplings probing the φ torsion angle in fact most of the couplings are probing this torsion angle this improvement largely reﬂects the balance between the β and ppii regions of the ramachandran map which are those contributing the most toward the ensemble averaged jcouplings in most force ﬁelds although the value for val is slightly worse it should be noted that substituent eﬀects make a signiﬁcant contribution to the j coupling and the use of an alternative set of karplus equation parameters table supporting information reduces many of the deviations from experiment note that although data was used to derive gly parameters for the termini table supporting information the internal glycine j couplings have not been used as target data in the parametrization with the qm surface used directly to create the gly ϕ ψ cmap term in the force ﬁeld as an independent test of terminal glycine parameters and of the proline parameters which were also based directly on the ϕ ψ qm surface we have compared our results with data for the peptide gpgg obtaining much improved agreement over cmap particularly for the cterminal gly in gpgg table supporting information residue disordered fragment of hen lysozyme in addition to tests on structure forming peptides it is important to also consider peptides that form little helix or sheet structure as a test for such a largely disordered peptide we chose a residue fragment of hen lysozyme for which scalar couplings have been measured by graf et al extensive sampling of this peptide was obtained by means of replica exchange molecular dynamics from which the degree of structure formed was assessed by means of backbone scalar couplings in table we report scalar couplings for the central is signiﬁcantly diﬀerent because of the presence of the bulkier hydrophobic side chain exhibits symmetric sampling with similar minima at αr αl and ppii in figure the ϕ ψ map with the new parameters is compared against the cmap map and maps from other popular force ﬁelds it can be seen that the changes with respect to cmap are relatively subtle with the main diﬀerences consisting of a deeper ppii minimum a higher and more focused αr minimum and less sampling of the αl state the changes go in the direction of the qm based map where the ppii minimum is even deeper among the other force ﬁelds the amber and are closest to the new charmm force ﬁeld but signiﬁcant diﬀerences exist is qualitatively diﬀerent with a pronounced minimum at and the gromos force has two similarly populated minima in the αr basin and a low energy transition region between αr and that is not present in any of the other force ﬁelds the ϕ ψ sampling was further compared with experimental j coupling constants that are available from nmr experiments table summarizes the results with details given in tables and of the supporting information the agreement is very good for the alanine based peptides and for and reasonable for val3 for we also compared with other force ﬁelds amber opls aa gromos the new force ﬁeld has lower values than all of the other force ﬁelds and there is in particular signiﬁcant improvement over the previous cmap force ﬁeld this improvement is largely due to decreased sampling of αr conformations and increased sampling of ppii figures and the main reason for the signiﬁcant improvement over cmap is the correction of the known bias toward αr in that force ﬁeld the improvement is reﬂected in the j couplings dx doi org j chem theory comput journal of chemical theory and computation article the rmsd for that helix as well as the second helix figure supporting information the rmsd of the backbone atoms of the self aligned helices are all å indicating that the helical structure of the individual peptides was maintained an important outcome as the developed ff was designed to yield a lower population of helices and an additional indication that the balance of the ff between sampling of extended and helical conformations is satisfactory also shown is the rmsd of the second nonaligned helix with the diﬀerence being in the range å with the presence of larger ﬂuctuations showing that the helices are moving relative to each other in the simulation indicating that the force ﬁeld is not overly restraining the dimer in a conformation close to that in the experimental structure overall these results indicate that the ff adequately reproduces the structure of the coiled coil dimer and thus appropriately models hydrophobic interactions and the helical character of individual helices crystal simulations to assess the quality of the new parameters on full proteins simulations were performed on eight proteins of diﬀerent morphology and size table in their crystal environments by performing these calculations on the full unit cells the sampling was signiﬁcantly enhanced as the unit cells contain two to four monomers of each protein the analysis emphasizes shifts in the ϕ ψ population that may indicate systematic problems in the backbone parameters in addition analysis of the distributions was undertaken average diﬀerences in ϕ ψ obtained from the ns simulations are shown in table average diﬀerences were calculated to identify local systematic deviations in the backbone conformations the overall agreement is good with the md simulations resulting in minimal deviation from the crystallographic backbone geometries table while larger deviations occurred in selected ϕ ψ torsions of β strand regions of erabutoxin b pdb id and crambin pdb id the overall agreement of is similar to that of cmap the larger deviations in were not unexpected as the original cmap was speciﬁcally optimized to reproduce the local ϕ ψ sampling building upon the previous cmap in the present study appears to lead to the small yet acceptable degradation in the local ϕ ψ sampling indicating that optimization did not lead to a bias in the sampling of ϕ and ψ while yielding signiﬁcant improvements in the treatment of the smaller polypeptides a more detailed analysis of the experimental and calculated backbone ϕ ψ distributions is presented in figure diagrams were constructed by overlaying crystallographic ϕ ψ values for each of the simulated proteins with the respective probability distributions calculated from the md simulations overall ϕ ψ sampling yields population distributions centered on crystallographic ϕ ψ values worth noting is the reproduction of backbone conformations in loop regions as depicted by data points falling outside of the classic ramachandran secondary structure regions these results demonstrate the robustness of in maintaining crystal backbone conformations to analyze the conformational properties of the side chains in the crystal simulations and distributions for each residue type were extracted from the ns portion of the protein crystal simulations and compared to distributions obtained from a crystallographic survey the level of agreement was quantiﬁed by calculating the overlap coeﬃcients eq between simulation and protein database survey results simulation results for the three protonation states of histidine alanine residue full data set in table of the supporting information overall the agreement with the couplings is much better than the previous cmap force ﬁeld and slightly better than the previously optimized amber force ﬁeld in particular the agreement with scalar couplings reﬂecting the ψ torsion angle was the best out of the force ﬁelds considered stability of dimeric coiled coil the developed parameters were also tested for their ability to reproduce the structure of a dimeric coiled coil protein these structures include two or more individual helices that interact primarily via hydrophobic amino acids protruding along one side of the helices this class of peptides has been subject to a number of md simulations where in some cases obtaining stable native state simulations has been challenging one such example is a heterodimeric parallel coiled coil pdb identiﬁer which was found to give poor results with a number of force ﬁelds the closest rms diﬀerence from the native being approximately å accordingly the dimeric peptide represents a good test of the force ﬁeld in the light of the previous simulation results as well as the overall structure being dominated by hydrophobic interactions between the two helices in the dimer figure shows the rmsd as a function figure a rmsd from experimental structure and b interhelical angle of the dimeric coiled coil as a function of simulation time rms diﬀerences are for the backbone n cα c o or non hydrogen atoms of both helices following least squares alignment of the backbone atoms in both helices the n and c terminal residues were excluded from the analysis for the interhelical angles vectors deﬁning the helical axes were calculated using the nonterminal residue cα atoms using the approach of chothia et al horizontal lines represent the interhelical angles from the models generated in the nmr study of simulation time for a ns simulation initiated with the ﬁrst model from the collection of nmr structures the backbone rmsd being in the vicinity of to å with the c and nterminal residues excluded indicates the overall structure to be stable representing a signiﬁcant improvement over previously reported results the ability of the model to reproduce the experimental structure is further judged by comparison of the interhelical angles between the two helices from the simulation with those obtained from the nmr models figure the overlap is excellent indicating to satisfactorily model the interactions between the two peptides additional analysis involved alignment of one of the two helices and calculation of table we present the values including the data for all four folded proteins split by residue type we also include results for the unfolded proteins used to guide the side chain optimization however we note that the side chain j couplings cannot be considered as a true validation as they were used in the optimization the values for with respect to this data would therefore be artiﬁcially lowered relative to other force ﬁelds ideally a validation would be done for unfolded proteins not included in the present study which provide a sensitive test of torsion angle distributions however the only other data set we are aware of is that reported for hen lysozyme by schwalbe and co workers and the size of this system is not amenable to current computational resources we ﬁnd that the revised side chain parameters represent an overall improvement over cmap with an overall of and for unfolded and folded proteins respectively compared with and for the original cmap the results for are also comparable with the of and for unfolded and folded proteins obtained with amber ildn the second type of data considered were residual dipolar couplings rdcs for the folded proteins which are very sensitive to small structural deviations the agreement between experiment and simulation has been quantiﬁed in this case by the q parameter conventionally used in structure reﬁnement with rdcs deﬁned as hsd hse and hsp were summed and normalized together as appropriate for comparisons with the his distribution from the database survey the results from this comparison are presented in table overall the crystal simulations with yield excellent agreement with database distributions and demonstrate a small overall improvement over that with cmap the torsion in glu and that in lys were among the torsions that have improved signiﬁcantly with glu improvement of sampling required sacriﬁcing the quality of agreement with the qm data figure as discussed for asp beneﬁtted appreciably from the new torsion parameters however in a number of cases there are slight degradations in relative to cmap such that the overall level of agreement for the distributions is similar for the two models with both yielding satisfactory agreement with the crystallographic survey data comparison with nmr data for folded proteins nmr data forms a valuable complement to crystal structure data for validating backbone and side chain parameters since it is very sensitive to the full distribution of dihedral angles particularly the population of multiple rotamers accordingly ns md simulations were performed on a set of four proteins for which extensive nmr data are available bovine pancreatic trypsin inhibitor bpti hen lysozyme and ubiquitin a similar data set was used by lindorﬀ larsen et al in their tests of their modiﬁed amber force ﬁeld ildn in figure we show the rmsd of each protein from the experimental reference in all cases the backbone rmsd obtained with is comparable or lower than that with cmap with deviations generally being å or less the larger jumps observed for bpti have been seen in other force ﬁelds and arise from transitions between multiple substates of the native state two types of nmr data were used for validation the ﬁrst were scalar couplings reporting on the side chain torsion angle in folded proteins this data set was collated from published and unpublished data by lindorﬀ larsen et al in where di obs and di calc represent respectively the ith residual dipolar coupling from experiment and as back calculated from the simulation the results for various alignment media and folded proteins are given in table although rdcs have also been measured for the unfolded proteins because of uncertainties in calculating the alignment tensor in this case data for his is based on arithmetic average over the oc for hsd and hse figure ramanchandran plot for backbone sampling crystallographic ϕ ψ values triangles are overlaid onto probability distributions obtained from the md simulation of full unit cells probabilities calculated with snapshots from to ns at ps intervals we have not attempted to compute these data from the unfolded state simulations we ﬁnd that the backbone rdcs obtained with are generally slightly better than or comparable to those obtained with cmap or amber relative to cmap the improvement in side chain rdcs is clear for and hewl and the results for ubiquitin are comparable the results for side chains are a signiﬁcant improvement over amber however these runs did not include the ildn side chain correction we have also used the backbone rdcs to assess whether the manual adjustments to the cmap in order to reduce systematic deviations from canonical secondary structure have any deleterious eﬀects on residues not in elements of canonical secondary structure for each of the proteins ubiquitin and hen lysozyme we have classiﬁed the φ ψ angles of each residue the ramachandran map into narrowly deﬁned α β polyproline ii αl and coil the deﬁnitions given in the legend of table are chosen to be mutually exclusive and exhaustive with coil comprising angles not within one of the other regions for each of these regions we have calculated a finally we note that the qrms for the coil region is not appreciably worse than for amber suggesting that the tweaks did not lead to detrimental eﬀects on residues outside canonical regions of the ramachandran map equilibrium folding simulations of β hairpins since most of the backbone optimizations focused on helix forming peptides we have also determined the folding equilibrium of two β hairpins using temperature replica exchange the two hairpins are the original native c terminal hairpin residues of protein g and the variant which has been optimized for greater stability replica exchange runs of both hairpins resulted in well folded structures deﬁning the folded state as being within å backbone rmsd of the hairpin in the protein g crystal structure a folded fraction of for and for at k is we describe ptraj and its successor cpptraj two complementary portable and freely available computer programs for the analysis and processing of time series of three dimensional atomic positions i e coordinate trajectories and the data therein derived common tools include the ability to manipulate the data to convert among trajectory formats process groups of trajectories generated with ensemble methods e g replica exchange molecular dynamics image with periodic boundary conditions create average structures strip subsets of the system and perform calculations such as rms ﬁtting measuring distances b factors radii of gyration radial distribution functions and time correlations among other actions and analyses both the ptraj and cpptraj programs and source code are freely available under the gnu general public license version and are currently distributed within the ambertools suite of support programs that make up part of the amber package of computer programs see http ambermd org this overview describes the general design features and history of these two programs as well as algorithmic improvements and new features available in cpptraj introduction biomolecular simulation methods have proven useful in a wide variety of applications ranging from protein folding and computer aided drug design to the characterization of materials properties broadly deﬁned such methods include not only the molecular dynamics md and monte carlo approaches but also molecular docking geometry optimization free energy and or path sampling approaches any of these methods may generate a time series or an ensemble of three dimensional atomic positions of a model or set of models i e a coordinate trajectory in practice a key enabler of any robust biomolecular simulation method is not only providing the ability to generate conformational ensembles that describe the processes of interest but also facilitating the means to analyze the ensemble data beyond simply understanding the static structure derived from experiment or the starting and end points of a simulation much more information can be gleaned by characterizing the full ensemble or time dependent evolution of the sets of atomic or model coordinates this trajectory analysis refers to analyzing a potentially large set or time series of positions and their derived properties compared to the gigabyte or smaller trajectories that could be generated in the mid today we can run ns replicate md trajectories of a solvated dna mer on gpu resources such as xsede keeneland in months to generate over terabytes tb of data representing an aggregate μs of simulation data the data explosion is only getting worse with access to resources like blue waters at ncsa which has over gpus available given that these data sets are american chemical society becoming larger and are generated more quickly it is critical that the data generated can be analyzed not only rapidly and eﬃciently but in a manner that is both ﬂexible and easy to use and ideally within a generalizable and extensible framework ptraj short for process trajectory has served as the main analysis program of the amber software since the early and oﬀers a wide range of functionality including simple geometric analysis of coordinates conversion between diﬀerent coordinate formats advanced vector and matrix analysis and so on the overall goal of ptraj was to provide a uniﬁed interface to commonly needed analysis tools as well as provide reusable routines and an extensible framework for the easy development and incorporation of new analysis tools although ptraj has been an integral part of amber for decades this is the ﬁrst publication providing an in depth description of the code outside the information provided by the amber manuals and tutorials several other software packages for the analysis of coordinate trajectories exist including vmd mmtsb mdanalysis pteros loos pyloos and himach the mdanalysis pteros loos pyloos and himach packages provide libraries of functions essentially analysis oriented programming language extensions that can be used to construct analysis programs while this approach provides a great deal of ﬂexibility the low level interface can be challenging for some users the mmtsb package provides a series of perl scripts received april published june dx doi org j chem theory comput journal of chemical theory and computation article important was simplicity functionality generality and ease of implementation when the software was originally written the rate determining step of biomolecular simulation was md trajectory generation not analysis and the simulations tended to be relatively short up to thousands of frames in ns length simulations that took months to generate ptraj was also initially designed with the idea of only processing single sets of input trajectory data with a single output trajectory ﬁle using a single parameter topology ﬁle for example it is not readily possible to calculate the root mean square deviation rmsd of a coordinate frame to a reference structure with a diﬀerent topology e g a mutant protein structure or a related receptor with a diﬀerent ligand bound moreover ptraj tends to output derived data as raw text ﬁles necessitating further postprocessing of the data with graphing programs to display the results although ptraj has served the computational chemistry community well for about two decades there are several factors that drove a complete overhaul of the code this overhaul was the development of cpptraj a rewrite of ptraj in c although certain portions of the ptraj code are used in cpptraj often initially cut and paste and then recoded and optimized and elements of the ptraj design philosophy have been retained the code base has been rebuilt from the ground up with an eye toward improving calculation speed and making future additions to the code as simple as possible as cpptraj was developed the intent was to be fully backward compatible with ptraj commands and input ﬁles although there are some diﬀerences most notably in output formats to allow direct visualization of results and to facilitate further postprocessing of the derived data cpptraj can read multiple topology ﬁles and reference structures write multiple output trajectories for which speciﬁc frames to be written can be speciﬁed and strip topology ﬁles in addition the results from separate commands can be directed to the same data ﬁle e g the results from two dihedral angle time series calculations like protein phi and psi angles can be written to one ﬁle and there is native support for compressed ﬁles along with many other improvements overall cpptraj shows a signiﬁcant speedup compared to ptraj particularly when processing amber netcdf trajectories in addition several commands have been parallelized with to take advantage of multicore machines for even more speedup each of which can perform a diﬀerent type of analysis however combining diﬀerent types of analysis into a single run is not straightforward vmd provides a convenient graphical interface for various kinds of coordinate analysis while vmd does provide a tcl text interface that can be used for scripting it is not typically used for batch processing various types of analyses which is particularly useful when analyzing data at remote sites in contrast to these software packages ptraj provides a variety of higher level analysis commands via a uniﬁed interface that is still amenable to batch processing and performing multiple types on analysis in one run the initial incarnation of ptraj evolved out of the program rdparm read parameters which was designed to read and display parameter information from a single amber topology ﬁle at the time rdparm was developed in the kollman lab at ucsf the formatted amber topology data ﬁles could be generated via multiple mechanisms including the soon after retired prep link edit parm plep programs and its replacement program still used in amber today leap the topology ﬁles were diﬃcult to read not only due to the formatting but also since the ordering and speciﬁcation of information could be diﬀerent yet still lead to equivalent results for example via diﬀerent bond orderings or equivalent but opposite torsion atom deﬁnitions x c c y vs y c c x because of this one could not simply diﬀ the ﬁles to expose diﬀerences between plep and leap topologies rdparm emerged when many of the members of the kollman lab realized after months of simulations with new leap topologies that there were inconsistencies in the simulation results a tool was needed to read the topology ﬁles and allow easy display of the parameters to see what was wrong in the ﬁles and rdparm was created a summary of the functionality and output is presented in the supporting information section at this time many members of the amber community had a myriad of scripts and programs to do basic md trajectory processing and analysis written in various languages with signiﬁcant code duplication this set of tools included both home grown scripts and those distributed with amber while experimenting with adding analysis capabilities to rdparm ptraj was designed as an extensible tool with the intention of it becoming the central place to aggregate amber md trajectory analysis tools to facilitate md trajectory analysis ptraj was built within rdparm the two source codes are merged with diﬀerent run time behaviors determined by the name of the executable promoting reuse of common functionality and providing the underlying analysis code with a single interface to the various trajectory and topology formats the code is fairly well documented and attempts to be modular and extensible by others in fact the code for ptraj has been modiﬁed and extended by several diﬀerent authors over the years leading to new functionality time correlation functions matrix analyses etc moreover ptraj grew to support other md trajectory formats beyond pdb and amber including psf topology and dcd md trajectory formats among others despite the attempts to abstract the functionality extending the code is not particularly easy since the rdparm and ptraj codes were written primarily in c leading to an overly complicated code base c was chosen as the primary language since c was not yet a reliable standard when the rdparm and ptraj programs were initially written the complicated code base led to code fragmentation and duplication and memory leaks are still present moreover ptraj was not written with speed or eﬃciency in mind more general overview although ptraj and cpptraj have some limited interactive capability they are mainly designed to be run as a batch job with predeﬁned input outlining a series of commands to process and analyze the data an example run might look like ptraj gaac topo commands in where in this case the ﬁle gaac topo deﬁnes the topology of an mer dna duplex in solution and the input ﬁle commands in is as follows dx doi org j chem theory comput journal of chemical theory and computation article figure overall program ﬂow of ptraj left and cpptraj right orange boxes represent data blue boxes represent phases of the program certain actions will not function properly if the total number of frames cannot be determined as is the case for corrupted or certain compressed trajectories as the input ﬁle is read in the various commands to be processed are put into a stack of actions which will be performed sequentially on each coordinate frame at this stage necessary memory allocation and setup is performed after the initialization phase comes the actions phase where coordinate frames are read one by one from the input trajectories and processed by each action actions can generate data at this point as well as change the state of the system for example if a strip command is speciﬁed to remove coordinates all subsequent actions after the strip will use the new and truncated set of coordinates in order to retain generality there is little checking on the actions to see if they make sense this can lead to issues if care is not taken to understand the implications for example trying to periodically image coordinates after an rms ﬁt rotates the periodic cell will alter the coordinates incorrectly by imaging with respect to the unrotated unit cell on the other hand by not imposing strict rules there is more ﬂexibility for example atomic positional ﬂuctuations or b factors can be calculated either with free coordinates including molecule rotation or by preﬁtting to a common reference frame in other words the atomicﬂuct command does not require a particular ﬁt to a reference frame if an output trajectory was speciﬁed output coordinates are written during this phase once all actions have processed the current trajectory frame once coordinate processing is complete any accumulated data can be used to perform various types of analysis in the analysis phase last any data or information not written during the actions phase is printed cpptraj extends this ﬂow into ﬁve distinct phases initialization setup actions analysis and data formatting during initialization everything including speciﬁcation of topologies can be prepared based on user input again either from an input ﬁle or via standard input all topology information and reference structures are read coordinate trajectories are prepared for reading and actions analyses to be performed are instantiated the next two phases setup and actions comprise coordinate reading and data accumulation trajin gaac cdf trajin gaac cdf trajout gaac strip crd nobox center mass origin image origin center familiar byres center mass origin image origin center familiar rms first mass out rms time average avgpdb avg pdb pdb strip wat the above input ﬁle reads in two trajectory ﬁles via trajin performs centering and imaging of the coordinates via center and image with respect to a particular atom selection e g refers to residues see supporting information section for an extended description of ptraj cpptraj mask syntax performs an rms ﬁt to the ﬁrst frame outputs an average structure strips removes residues named wat in amber this is the standard name for water molecules and outputs the resulting coordinates to another trajectory via trajout note that the placement of the trajout command does not matter coordinates produced via trajout are always output after all actions have processed a frame to better understand the trajectory processing and analysis it is useful to describe the code ﬂow which is shown in figure and is similar for both ptraj and cpptraj ptraj has three distinct phases initialization actions and analysis in the initialization phase the topology ﬁle and an input ﬁle of commands are read in and parsed the format of the topology ﬁle is automatically detected and is used to deﬁne the size of the system the atom and residue names and numbers for atom selections solvent and other properties if present e g atomic masses charges etc next input commands are parsed either from a ﬁle or from standard input this may include reading and checking reference coordinates or input trajectories as with topology ﬁles the format of coordinate trajectory ﬁles is automatically detected one current limitation of ptraj is that for many actions it is necessary to ﬁrst predetermine how many frames will be processed for memory allocation meaning that dx doi org j chem theory comput journal of chemical theory and computation article are taken from the name speciﬁed with the action default names are assigned if not speciﬁed the format of the output can be changed simply by changing the ﬁlename extension for example to output in xmgrace format the user can simply change the name of the output ﬁle to standard agr or to obtain gnuplot map output the name can be changed to standard gnu the results of this are shown in figure the ﬁgures shown are exactly what is rendered by during the setup phase all topology dependent aspects of each action e g atom mask parsing are processed when a trajectory with a diﬀerent topology needs to be loaded all actions are set up again for the new topology before continuing during the actions phase input trajectories are read in one frame at a time and processed by each action as with ptraj actions are performed in sequence on each coordinate frame that is read in and data may be generated at this point it should be noted that although some actions do output data in the actions phase the majority do not for reasons of eﬃciency see e g the secstruct command in the discussion in contrast to ptraj output coordinates can be written out during action processing as opposed to only after all actions are processed this allows output of coordinates before modiﬁcation from e g an rms ﬁt cpptraj also has the ability to write out amber topology ﬁles either during the initialization or setup phases which is particularly useful when actions modify topologies as with ptraj once coordinate processing is complete any accumulated data can be used to perform analysis during the analysis phase during the last phase data formatting any data slated for output is formatted and then written to disk overall the procedure followed by cpptraj is similar to ptraj with three notable exceptions trajectories can have diﬀerent topologies thus actions require separate initialization and setup phases the number of frames does not need to be known for memory allocation in actions as data set memory can be allocated dynamically there is now a distinct output phase after trajectory processing and analysis in which data can be formatted this latter phase is intended to give the user more output format options as well as to eliminate the need for some common postprocessing steps most data generated from actions can be output in one of three formats essentially any output speciﬁed with the out keyword standard data data in whitespace delimited columns xmgrace a freely available graphing program for x windows http plasma gate weizmann ac il grace and gnuplot contour map another freely available graphing program http www gnuplot info the output ﬁle type is detected from the ﬁlename extension data from separate actions can be output to single or multiple ﬁles in any combination desired by the user for example consider the following cpptraj input figure xmgrace top and gnuplot map bottom output of cpptraj showing how data from separate actions in this case a distance and radius of gyration calculation may be combined the plots were created using xmgrace and gnuplot on the data output from cpptraj and were not modiﬁed in any way parm dpdp trajin dpdp nc distance out standard dat radgyr rog nomax out standard dat xmgrace and gnuplot directly from the cpptraj output i e no alterations were made and no massaging of the ﬁles was required to get axis labels and legends overview of file processing both ptraj and cpptraj handle a wide variety of topology and trajectory ﬁle formats shown in table detection of the ﬁle type is automatic and proceeds via magic numbers i e hex signature for binary ﬁles and via other characteristics for ascii ﬁles e g pdb keywords etc in ptraj trajectory ﬁles can be read in compressed zip gzip format via external program calls in cpptraj both topology and trajectory ﬁles can be read and written in compressed gzip format via internal calls to the freely available zlib libraries respectively in addition output data ﬁles can be written compressed topology information can be read from amber topology pdb charmm psf and cpptraj only ﬁles additionally in cpptraj atom bonding information can be this input reads in a single trajectory and performs two actions a distance calculation named and a radius of gyration calculation named rog with both actions performing output to the ﬁle standard dat in ptraj this would result in the radius of gyration action overwriting the results of the distance action however in cpptraj the results of both actions are written to the same ﬁle in separate columns where n is the number of atoms ri denotes atomic position and rm denotes the mean position of all atoms five membered ring pucker can be calculated using the method of altona and or the method of cremer and pople with the analysis utilities averages and standard deviations can be reported with proper cyclic averages being calculated for periodic values e g torsions both ptraj and cpptraj employ a version of kabsch for calculating the best ﬁt rmsd of a structure to a reference structure although there are several small diﬀerences in the implementation between the two programs speciﬁcally cpptraj allows the speciﬁcation of separate masks for the target and reference allowing some more ﬂexibility cpptraj also contains an additional mode that automatically calculates the no ﬁt rmsd of speciﬁed residues after an overall rms ﬁt has been performed enabled with the perres keyword which gives an idea of the motion of individual residues within the overall reference frame ptraj and cpptraj have the ability to calculate protein secondary structure using the method of kabsch and also known as the dssp method both programs can output both secondary structure for each residue per frame as well as the average secondary structure of each residue over all frames in addition cpptraj can output these results in native xmgrace and or gnuplot format shown in figure ptraj and cpptraj also have several commands for modifying coordinates and or topology the strip command removes user speciﬁed atoms from coordinates and topology which is useful when for example removing solvent molecules from a trajectory in addition to this cpptraj can output a corresponding stripped amber topology ﬁle that matches the stripped coordinates similar to strip the closest or closestwaters command can be used to keep only a userspeciﬁed number of waters near a speciﬁc area of solute e g near a bound ligand and solvent can be redeﬁned using the solvent command for the generality of keeping nearby molecules as with strip cpptraj can also be used to output a corresponding amber topology cpptraj also has an additional command unstrip which can be used to restore a topology to its original state which can be used for example to separate a complex into separate receptor and ligand trajectories in one pass indeed cpptraj is used by the mm pbsa method in amber mmpbsa py python script for this very purpose for trajectories with periodic boundary conditions ptraj and cpptraj can perform imaging in order to bring molecules outside the primary unit cell back into the primary unit cell this is usually done by centering molecules of interest prior to imaging imaging can be performed for both orthorhombic and nonorthorhombic cells nonorthorhombic cell types include all of the general triclinic unit cells and imaging can be performed to the triclinic unit cell shape which often looks like a slanted rectangle see figure with the triclinic keyword or to the more familiar or spherical shape that displays the molecules such that the single periodic image closest to the center of the unit cell is displayed with the familiar keyword ptraj defaults to the triclinic for historical reasons whereas parm topology ﬁle coord coordinates trajectory generated via the bondsearch keyword for topology ﬁles that do not already have such information for use in actions which require it e g the linear combination of pairwise overlaps surface area cpptraj also has the ability to write amber topology ﬁles which can be particularly useful when the topology has been modiﬁed such as after a strip command see the commands overview section for more details ptraj and cpptraj can read and write in several trajectory formats scripps binpos ptraj only amber coordinates ascii amber restart amber netcdf trajectory charmm dcd pdb and cpptraj only amber netcdf restart and ﬁles trajectory ﬁles can be read in either for processing or for use as reference frames for certain actions e g rmsd trajectory ﬁles can be read in using user speciﬁed start stop and oﬀset frame numbers in addition users can choose to process frames at a speciﬁc temperature from an ensemble of trajectories generated from amber replica exchange molecular dynamics remd via the remdtraj remdtrajtemp keywords replica trajectories are automatically searched for one given replica ﬁlename based on the assumption of a numeric ﬁle suﬃx output trajectories are written after coordinate processing multiple input trajectories can be written to a single output trajectory existing trajectories can also be appended cpptraj also has some additional trajectory processing functionality worth mentioning for processing replica trajectories users can explicitly specify trajectory ﬁle names in addition to the automatic ﬁle name search a given replica ensemble can also be converted to a temperature ensemble in one pass using the remdout keyword when reading in trajectories the last keyword can be used if the stop frame is not known in advance and the lastframe keyword can be used to explicitly choose the ﬁnal frame of a trajectory when writing out trajectories users can specify which input frames to be output also multiple output trajectories can be speciﬁed and trajectories can be written out during action processing as opposed to only after in ptraj with the outtraj command examples are shown in the supporting information section ptraj cpptraj commands overview both ptraj and cpptraj can calculate various geometric quantities including distance angle dihedral radius of gyration and pucker a complete list of commands is provided in the supporting information section the distance calculations can perform imaging for both orthorhombic and nonorthorhombic cells by default the shortest imaged distance is dx doi org j chem theory comput journal of chemical theory and computation article speciﬁed oﬀsets to create larger unit cells or to better understand packing for example to output coordinates one unit cell over in each direction the command would be image xoﬀset yoﬀset zoﬀset both ptraj and cpptraj have the ability to unwrap coordinates i e perform the reverse of imaging which is necessary if one is interested in diﬀusive properties cpptraj also has an action called autoimage which performs centering imaging with no additional input from the user and will not separate molecular complexes such as dna or other multimers which was occasionally an issue with ptraj imaging vector matrix analysis ptraj and cpptraj have several commands which allow users to track and analyze vectors and matrices generated from input coordinates in addition to storing simple atom coordinate vectors e g between a nitrogen atom bonded to a hydrogen atom both principal axis vectors and dipole vectors can be stored as well vectors can also be stored for use with the isotropic reorganizational eigenmode dynamics ired approach of prompers and bru schweiler vectors are stored as sets of cartesian coordinates magnitude and origin auto or cross time correlation functions can be calculated for vectors using spherical harmonics addition theorem and a fast fourier transform fft approach via cross correlation wiener khinchin theorem several types of matrices can be calculated including distance covariance mass weighted covariance correlation distancecovariance ired and isotropically distributed ensemble analysis distance matrices record the average distance between atom pairs mass weighted covariance matrices record the coordinate covariance correlation records the correlation between atom vectors and distance covariance records the covariance between atom pair distances ired matrices use previously deﬁned ired vectors to generate an ired matrix using the analyze matrix command symmetric matrices can be diagonalized and the eigenvectors and eigenvalues calculated and output if all eigenvalues eigenvectors are desired the dspev routine from the lapack math is used for the calculation if desired only the eigenvalues may be computed if a subset of the eigenvectors eigenvalues is desired the dsaupd dseupd routines from the arpack math are used if the matrix being analyzed is an ired matrix ired order parameters can be calculated and output if the matrix being analyzed is mass weighted covariance quasiharmonic analysis entropy heat capacity and internal energy can be performed using the standard statistical mechanical formula for an ideal gas eigenvectors from covariance matrices may also be reduced according to the procedure of abseher and nilges which can be useful for comparing eigenvectors from cartesian space to those in distance space eigenmodes generated from analyze matrix can be further analyzed to calculate rms ﬂuctuations displacements of cartesian coordinates along mode directions or dipole dipole correlation functions the modes can also be read back in and coordinate frames projected onto them using the project command to calculate how much that frame contributes to each mode principal component or quasi harmonic analysis is a common use of the matrix analysis facilities a typical workﬂow would involve calculating the covariance matrix mass weighted for quasi harmonic analysis diagonalizing it to get the eigenmodes then projecting frames onto those figure cpptraj native xmgrace top and gnuplot bottom output for the secstruct dssp command for a short trajectory of a model β sheet peptide the plots represent exactly what is output from cpptraj and were not modiﬁed in any way in the xmgrace output the average secondary structure content y axis for each residue x axis is shown in the gnuplot output each residue y axis is assigned a secondary structure type no structure parallel beta antiparallel beta helix α helix pi helix turn for each frame x axis figure an rna tetraloop yellow with surrounding water solvent blue only showing the oxygen atoms and ions na in red and cl in green showing equivalent periodic unit cells imaged either to the triclinic unit cell left or more spherical or familiar imaging right image created with vmd cpptraj defaults to familiar which is the common imaging mode in amber for truncated octahedral unit cells in addition to imaging by molecule ptraj can image by atom by residue or according to a user speciﬁed mask expression and can build nearby unit cells through dx doi org j chem theory comput journal of chemical theory and computation article using either the connolly or the lcpo with the molsurf and surf commands respectively although both ptraj and cpptraj can use distance based masks that use reference coordinates cpptraj can also make use of a distance based mask that updates each frame via the mask command which can be used for example to write separate pdb ﬁles of all water molecules within a certain distance of a ligand as opposed to the closest command which only outputs a ﬁxed number of molecules cpptraj can also calculate j coupling values from dihedral angles according to the procedure of chou et al or perez et al cpptraj comes with many test cases that also serve as examples of how to run various commands these test cases are part of the larger ambertools distribution when ambertools is installed they are located in the directory amberhome ambertools test cpptraj automatic imaging imaging a trajectory could prove to be challenging with ptraj for systems with more than one nonsolvent molecule e g dna duplexes receptor ligand complexes protein tetramers etc the autoimage command in cpptraj was designed to perform centering and imaging in one step with minimal input from the user the command functions by dividing all molecules into three regions anchor mobile and ﬁxed the command will pick defaults for each region although any of the regions can be chosen by the user via mask expressions the anchor region is made up of a single molecule that will be centered either to the box center or the coordinate origin all other molecules are imaged with respect to the anchor molecule by default the ﬁrst molecule is chosen as the anchor the mobile region is made up of molecules that can be imaged freely by default all solvents and ions are chosen to be mobile the ﬁxed region is made up of all remaining molecules molecules in this region are imaged only if the imaged position is closer to the anchor molecule an example of this command is shown in the supporting information section new hydrogen bonding facility like ptraj cpptraj has the ability to keep track of hydrogen bonds formed over the course of a trajectory however there are several important diﬀerences from the ptraj implementation unlike ptraj where hydrogen bond donors and acceptors were speciﬁed with separate commands in cpptraj hydrogen bond donors and acceptors are speciﬁed with the hbond command as masks note that unlike ptraj the deﬁnitions of hydrogen bond donor and acceptor follow standard conventions hydrogen bond donors consist of a heavy atom and hydrogen hydrogen bond acceptors consist of a single atom hydrogen bond donors and acceptors can be searched for automatically following the simplistic criterion that donors are considered to be n o and f atoms bonded to hydrogen while acceptors are considered to be n o and f atoms with no bonded hydrogen atoms donors and or acceptors can also be explicitly speciﬁed with donormask and acceptormask keywords respectively as with ptraj a distance and angle criterion is used to determine when a hydrogen bond is present and the average occupancy distance and angle of each hydrogen bond found is output unlike ptraj however only solute solute hydrogen bonds are searched for and there is no option to record the time series and hence get lifetimes of hydrogen bonds although these features are planned for future releases of cpptraj an example of this command is shown in the supporting information section eigenmodes to obtain the values of the principal components at each frame examples are shown in the supporting information section ptraj specific commands advanced cluster analysis while cpptraj has the ability to perform cluster analysis on input coordinates via a hierarchical agglomerative approach with single average or complete linkage ptraj is able to perform clustering using a much greater variety of clustering algorithms including topdown splitting bayesian and cobweb among others in addition in ptraj clustering one has the ability to sieve input trajectories in order to reduce total computation time during a cluster sieve clustering is initially performed on a smaller subset of the input coordinates after which the remaining input coordinates are added to resulting clusters based on their closeness to the cluster centroid due to the complexity of the clustering code in ptraj it has not yet been fully ported to cpptraj although this is planned for future versions of cpptraj hydrogen bond facility although both ptraj and cpptraj both have commands to track hydrogen bonds the ptraj hydrogen bonding facility diﬀers signiﬁcantly from cpptraj in ptraj hydrogen bond donors and acceptors are deﬁned prior to an hbond command with the donor and acceptor keywords note that in ptraj the deﬁnition of hydrogen bond donor and acceptor are reversed with respect to standard conventions the electron pair acceptor is bonded to hydrogen and the donor is the atom to which the hydrogen bond is formed i e in ptraj a donor can be thought of as donating electrons to the hydrogen atom this has not been changed in order to preserve backward compatibility solute donors can either be speciﬁed using a mask or by giving a residue and atom name or by specifying a mask solute acceptors can be speciﬁed by giving residue heavy atom and hydrogen atom names or by giving heavy atom and hydrogen atom masks this information is then used by a subsequent hbond command solute solute solute solvent and solvent solvent hydrogen bonds can all be tracked a hydrogen bond is considered formed when distance and angle cutoﬀ criteria are met if desired the user can disable the angle criterion ptraj can also be used to track particle particle interactions by specifying the same selection for the acceptor speciﬁcation twice for example to track ion interactions with a typical donor in addition ptraj allows deﬁnition of solventdonor and solventacceptor hydrogen bonds for example to track generic solvent interactions e g if any solvent interacts in contrast to each speciﬁc solvent molecule and the deﬁnition of solvent is general an example script is shown in the supporting information section at the end of coordinate processing a summary of the average occupancy distance and angle of each hydrogen bond found is output note that in ptraj hydrogen bond angles are reported as deviations from linear i e as angle the time series of each hydrogen bond can optionally be saved from which details on hydrogen bond lifetimes can be calculated ptraj will also report when solute residues are bridged by a single water molecule cpptraj specific commands cpptraj has several actions not available in ptraj the solvent accessible surface area of a molecule can be calculated dx doi org j chem theory comput journal of chemical theory and computation article nucleic acid structure analysis cpptraj has the ability to calculate basic nucleic acid structure parameters base pair parameters shear stretch stagger buckle propeller twist and opening base pair step parameters shift slide rise tilt roll and twist and helical parameters x displacement ydisplacement rise inclination tip and twist are calculated using the same procedure employed by using reference frame coordinates from olson et al base pairing is determined on a per frame basis first each base is identiﬁed and assigned a standard reference frame next for each base potential base pairing partners are determined based on how close their reference frames are and whether they are hydrogenbonded currently only base pairs with standard watson crick hydrogen bonding patterns are recognized cpptraj has some ability to recognize nonstandard modiﬁed nucleic acid bases through a user speciﬁed argument which attempts to map the nonstandard base to one of the ﬁve standard bases currently this works as long as the modiﬁed base contains at least the same heavy atoms as the standard base to which it is being mapped an example of this command is shown in the supporting information section rotational diﬀusion tensor the rotational diﬀusion properties of a molecule can be calculated with cpptraj using the rotdif command which determines the diﬀusion tensor with both small and full anisotropy the procedure followed is brieﬂy described here for further details refer to the original implementation by wong and case first a user speciﬁed number of random vectors are generated and rotated using rotation matrices obtained from rms ﬁtting the trajectory of the target molecule with a suitable reference typically the average structure the correlation function cl for each vector is then determined using a legendre polynomial pl of order l or l speciﬁed by the user default is the deff equation can be solved for q and therefore d using singular value decomposition in cpptraj this is done via an external call to a lapack routine dgesvd diagonalization of the diﬀusion tensor to yield principal components and axes is also accomplished with a lapack routine dsyev the diﬀusion tensor in the full anisotropic limit is then found using a downhill simplex minimization scheme that uses the diﬀusion tensor in the small anisotropic limit as initial input an example of this command is shown in the supporting information section rmsd autocorrelation time correlation functions are useful tools that allow discernment of patterns that may be hidden in a given set of data a property that one may be interested in calculating the time correlation function for is rmsd in order to determine the similarity of structures in a trajectory over diﬀerent time scales and to assess convergence to the average structure however one cannot directly calculate the time correlation function of a series of rmsd values and obtain this as the diﬀerence between two rmsd values may not be indicative of how similar the structures are for example suppose one has a trajectory of a small peptide which starts in a helical conformation then adopts both hairpin and extended conformations with respect to the starting structure the hairpin and extended conformations may have similar rmsd values although they are obviously diﬀerent structures to calculate the similarity of structures over diﬀerent time scales we have developed the rmsd average correlation method or rmsavgcorr this method calculates the autocorrelation of rmsd as the average rmsd of coordinate frames which have been averaged over sliding time windows of a certain size n rac τ i j cl τ pl nj nj i τ l l tl n once the local eﬀective diﬀusion constants for each vector have been determined they can be used to determine a tensor q by solving deff at q where deff is a column vector composed of the local diﬀusion constants and at is a by n matrix the rows of which are composed of n vector components q is related to the diﬀusion tensor d by q n τ where τ is the window size ranging from to n n is the total number of frames avgcrd t t τ is the average coordinates from frames t to t τ and rmsd represents the calculation of best ﬁt rmsd of the given frame to reference avgcrd τ this means that rac is just the average rmsd to the ﬁrst frame and rac n is always atom mapping in some cases two structures of the same molecule may exist with diﬀerent atom ordering this can occur for example when generating a ligand with two diﬀerent programs hydrogen atoms may be placed at the end of one ﬁle whereas they may be placed close to their bonded heavy atom in another the two structures then become impossible to compare using e g the rmsd algorithm in ptraj cpptraj since it is expected that the atom ordering between the target and reference structure matches to address this issue the atommap command was created for cpptraj the atommap command employs a very simple algorithm which attempts to map a target structure onto a reference structure i e solve in an approximate way the maximum common subgraph isomorphism problem the central idea of the algorithm is to assign each atom in the target and reference a unique id based on its chemical environment and from these ids attempt to create a map between the target and reference to create the unique id ﬁrst each atom is assigned a single character based on its atomic element so carbon becomes c hydrogen becomes h etc the next step is to assign each atom what is called an atomid which is made up of the single character plus the characters of n i where τ denotes the maximum lag time n represents the rotated vector i and j denote diﬀerent times and n is the total number of frames since trajectories are of ﬁnite size τ should be somewhat less than n in order to limit statistical errors the average or eﬀective correlation time for a vector tl n can be determined from the integral over the calculated correlation function in cpptraj this is accomplished by ﬁrst creating a cubic spline mesh then performing simple integration via the trapezoid rule this can then be used to calculate the local eﬀective diﬀusion constant dloc n l for that vector using an iterative procedure dloc n l t rmsd avgcrd t t τ i d dx doi org j chem theory comput journal of chemical theory and computation article the atoms it is bonded to so for example the α carbon in an alanine side chain would receive an atomid of ccchn since it is bonded to a carbonyl carbon a beta carbon an alpha hydrogen and an amide n the atomid is then combined with the atomid of all bonded atoms and sorted in alphabetical order to form the unique id in the case of alanine the atomids of the previously mentioned atoms bonded to the α carbon are ccoo cchhh hc and nchh so the total unique id for the α carbon is ccccccccchhhhhhhnnoo the unique id of an atom therefore reﬂects the local chemical environment of the atom because the chemical environment of an atom may not truly be unique which is the case with symmetric atoms for example each unique id is checked to see how many times it is repeated in the molecule if it is not repeated it is marked as truly unique the atom mapping procedure is illustrated in figure once the truly unique atoms have been found in the target and stage are usually symmetric in some way if atoms are mapped in this way the algorithm then returns to the ﬁrst step figure green atoms otherwise mapping is complete when the molecule is highly symmetric it is possible that no unique atoms can be found to make an initial map when this occurs the algorithm attempts to guess an initial mapping based on atoms whose unique ids are duplicated only once preferring chiral atoms the algorithm performs mapping using each potential pair as an initial guess and ranks each guess based on rmsd of mapped atoms the mapping which produces the lowest rmsd is then used after mapping is complete several options are available the map can be used to reorder an input trajectory based on the target so that it matches the reference or only the map itself or the rmsd of mapped atoms can be printed if desired in cases where not all atoms can be mapped either due to diﬀerent numbers of atoms in the target and reference or incomplete mapping structures can be modiﬁed to only include mapped atoms despite the relative simplicity of the algorithm it can be quite eﬀective at least for small molecules in a recent docking study in which the atom ordering of crystal poses did not match that of the starting poses the cpptraj atommap algorithm was able to map all ligands including structures where the protonation state changed and highly symmetric structures an example of this command is shown in the supporting information section adding new functionality both ptraj and cpptraj were developed with the idea that the addition of new functionality actions analyses trajectory formats should be easy in ptraj all actions were located in the actions c ﬁle and all analyses in the analysis c ﬁle the addition of new actions analyses was done via the addition of a function to the appropriate ﬁle adding a pointer to that function to the appropriate array in the dispatch c ﬁle and then adding an entry to the ptrajsetup routine in the ptraj c ﬁle the addition of new trajectory ﬁle formats was slightly more complicated requiring the modiﬁcation of several routines in the ptraj c ﬁle as well as potentially adding new functions to the trajectory c ﬁle although comments in the code guide potential developers on how to do this a goal of cpptraj was to make the addition of new functionality more straightforward to this end the addition of new actions analyses and trajectory formats requires implementing a new class that inherits from the appropriate base class type action analysis and trajectoryio respectively then adding an entry for the new type in the appropriate container actionlist analysislist and trajectoryfile respectively the advantage of this approach is that the code for diﬀerent actions analyses trajectory formats is compartmentalized enhancing code readability and making future code maintenance easier the cpptraj codebase is itself extensively documented in doxygen format http www doxygen org and the code is distributed with a developers guide that has step by step instructions for adding new actions among other information figure illustration of atommap stages blue atoms initial unique assignment red atom only atom bonded to a mapped atom orange atoms mapped based on chirality yellow atoms assignment based on element and bonding to mapped atoms a return to step atoms can now be mapped as they are the only atoms bonded to previously mapped atoms image created with vmd reference molecules these atoms are mapped to each other figure blue atoms this constitutes an initial guess which the rest of the algorithm uses to map the structures once the initial unique atoms have been mapped the algorithm proceeds and attempts to map the remaining unmapped atoms the ﬁrst step is to identify atoms that are the only ones of their kind bonded to a mapped atom e g a lone hydrogen bonded to a mapped carbon atom figure red atom to ensure that chirality in molecules is preserved the second step is to identify atoms that are bonded to partially mapped chiral centers i e the chiral atom and two atoms it is bonded to are mapped these atoms are then mapped by matching dihedral angles formed between them and the three mapped atoms figure orange atoms if any atoms are mapped in this way the algorithm returns to the ﬁrst step the ﬁnal step is to map atoms based on their element and bonding to previously mapped atoms figure yellow atoms the atoms in this discussion of ptraj cpptraj performance there are several diﬀerences between ptraj and cpptraj both in the implementation of underlying algorithms and general code layout which are worth noting as they contribute to the generally better performance of cpptraj vs ptraj dx doi org j chem theory comput 3095 journal of chemical theory and computation article using an openmp pragma in front of a key loop since the input trajectory is never split up there is no postprocessing to recombine data from actions on diﬀerent threads in the correct order benchmarks benchmarks of several commonly used actions are shown for ptraj and cpptraj in table one major diﬀerence is that by design cpptraj attempts to exploit the locality of reference as much as possible i e to make data access as eﬃcient as possible this tends to be a feature when programming with an object oriented language such as c using modern compilers as classes tend to keep both code and the target data together in cpptraj this strategy was also employed for the storage of coordinates whereas in ptraj x y and z coordinates are stored in three separate arrays in cpptraj x y and z coordinates are stored sequentially in a single array this results in signiﬁcantly improved performance when processing amber netcdf trajectories in particular since such trajectories have their coordinates stored in a similar fashion another diﬀerence is in the implementation of atom masks in ptraj atom masks are stored as an integer array of size natom where natom is the total number of atoms in the system set to if the atom is selected and if not this means that for example a routine calculating the center of mass of atoms in a mask would have a loop that always executes natom times with a conditional inside the loop determining whether an atom is selected or not although it should be noted there are some actions in ptraj which do have special cases where only one atom is selected in contrast cpptraj employs two mask types the ﬁrst is similar to ptraj masks where there is a character array of size natom set to t if the atom is selected and f if the atom is not selected this is useful when one needs to know both selected and unselected atoms however it is far more common to only be interested in selected atoms and so the second mask type in cpptraj is an integer array of size nselected where nselected is the total number of selected atoms containing only the atom numbers of selected atoms typically nselected is much smaller than natom so to use the previous example of a center of mass calculation instead of having to execute natom times the loop only needs to be executed nselected times with no conditional inside the loop this also saves memory as only selected atoms are stored versus all atoms for a ptraj mask although both ptraj and cpptraj have parallelized code the strategies employed in each case are radically diﬀerent for ptraj the strategy was to parallelize trajectory reads i e divide each trajectory among the number of threads being used within an framework so given a frame trajectory and two threads thread would read frames while thread would read frames while this did at times result in a signiﬁcant speedup and scaled reasonably well there were several problems with this approach the ﬁrst was that as implemented the number of input frames was required to be a multiple of the number of threads so for example it was not possible to read frames with two threads another is that the performance has proved to be extremely dependent on the underlying ﬁle system for some standard nonparallel ﬁle systems it was possible to see no speedup or even a slowdown when using multiple threads finally this approach also requires that the trajectory be seekable i e the ﬁle can be opened at a speciﬁc frame as such its use is currently restricted to amber formatted and netcdf trajectories to avoid these issues it was decided that for cpptraj only certain time consuming actions would be parallelized using openmp as it is now quite commonplace for even desktop machines to have two or more cores the simplicity of parallelizing time consuming actions within a shared memory framework makes openmp an attractive parallelization solution in many cases speedup can be achieved by simply table time necessary to complete common actions for frames unless otherwise noted in ambertools cpptraj and ptraj and speedup of cpptraj vs ptraj both programs were compiled with gnu compilers version in ambertools ptraj is currently built without explicit compiler optimizations turned on for historical reasons so timings are reported for ptraj with compiler optimizations turned on denoted ptraj opt turning on compiler optimizations for ptraj results in a speedup versus unoptimized ptraj of on average the trajectory processed was an amber netcdf trajectory 115 atoms all actions processed frames except for the closest and radial actions which only processed one frame due to their time consuming nature the cpu used was an amd athlon ghz for all commands cpptraj is faster than ptraj with an overall average speedup of for relatively simple calculations such as angle dihedral pucker radgyr radius of gyration strip and center the average speed up for cpptraj is much of this is a result of the better handling of netcdf ﬁles and improved locality of reference as mentioned in the previous section both image actions show slightly better speedups of and this is due to faster handling of atom masks in cpptraj as well as vectorization of the nonorthogonal imaging code originally used in ptraj the rms action shows a much larger speedup of this is largely the result of two changes to the rmsd calculation from ptraj copies of the target and reference coordinate frames are made prior to the calculation based on atom masks so that the rmsd calculation itself is only ever on the atoms of interest eliminating the need for conditionals to check for selected atoms and or mass information inside the rmsd calculation loop and the reference frame is set up and precentered once instead of each time the rmsd calculation is performed the secstruct action dssp analysis has a huge speedup of much of this is due to the fact that in ptraj secstruct data are output during trajectory processing whereas in cpptraj data are output after trajectory processing this dx doi org j chem theory comput 3095 journal of chemical theory and computation article necessitated a code rewrite cpptraj has been created to initially complement but eventually replace ptraj as the main analysis engine of the amber software package cpptraj has signiﬁcantly improved performance compared to ptraj and the reorganization of the code should make future additions to the code easier although cpptraj supports most of the functionality and syntax from ptraj the code is not yet fully compatible one of the main goals for further development of cpptraj is to enhance and increase the ﬂexibility of data set handling for example although cpptraj allows the creation of several types of matrices users may want to create a custom matrix composed of data from various diﬀerent sources e g distances angles dihedrals etc and perform various operations on that matrix such as principal component analysis additional future aims for cpptraj are to become fully backwardcompatible with ptraj add novel analysis capabilities add more parallelization and improve upon existing parallelization and increase the number of recognized topology and coordinate ﬁle formats illustrates how optimizing disk access can dramatically improve performance although it does so at the expense of using more memory openmp benchmarks benchmarks for several generally time consuming actions which have been parallelized using openmp in cpptraj are shown in table cpptraj was table cpptraj openmp benchmarks for certain parallelized actions the rapidly developing field of molecular magnetism relies on quantum chemical calculations to explain and quantify experimental observations recently a shift in focus from compounds containing magnetically isotropic spin only ions to those containing anisotropic spin orbit coupled ions has rendered commonly employed methods somewhat obsolete ab initio calculations utilizing the complete active space method have gained popularity recently however due to the computational complexity of such calculations they are only suitable for mononuclear species or mononuclear fragments therefore to obtain a quantitative measure of the exchange interaction in polynuclear complexes empirical fits making use of phenomenological hamiltonians are required given the current interest in novel exchange coupled molecules of the and variety displaying a wide range of interesting phenomena it is evident that there is a need for an easy to use yet highly efficient software package designed for the simulation and fitting of polynuclear compounds containing orbitally degenerate ions that will also support ab initio approaches while there are software packages currently available to perform calculations involving anisotropic ions for example condon the implementation uses large bases to accurately model complete electron configurations thus making the treatment of large polynuclear clusters difficult if not prohibitive due to large memory and time requirements on the other hand complementary software such as magpack is focused on the treatment of large spin only systems and therefore does not provide facilities for the treatment of unquenched orbital moments or crystal field cf effects a niche between the two vastly different codes was therefore identified for a new program that would unite the benefits of each while improving upon existing techniques therefore providing a much more general and powerful tool thus a software package specifically designed for the analysis of journal of computational chemistry multiple data or gpu paradigms using a threaded mpi or gpu model respectively for a given problem phi is been shown to be almost times faster than the well known program c wiley magpack limited only by available hardware v periodicals inc doi jcc exchange coupled clusters of mixed ions as well as pure or pure systems was envisaged this new program would be designed with computational efficiency in mind using high level parallelism and gpu capabilities to take advantage of modern and increasingly parallel computing infrastructures in addition to theoretical and computational advantages provided by such an approach the new program would be designed to be easily adopted by non experts with a simple and user friendly interface herein the concept capabilities theory and structure of the new program phi is presented along with some worked examples to demonstrate its use we show how using phi results from mononuclear ab initio calculations may be translated into the familiar framework of crystal field theory cft example yielding further insight into the magnetic anisotropy and providing a framework within which the exchange interactions of a polynuclear complex may be easily determined design and capabilities the foundation of phi is the ability to perform calculations involving first order unquenched orbital moments by using electronic terms as the basis states for each ion this method a n f chilton and k s murray school of chemistry monash university clayton victoria australia e mail nfchilton gmail com keith murray monash edu b r p anderson and l d turner school of physics monash university clayton victoria australia c a soncini school of chemistry university of melbourne parkville victoria australia contract grant sponsor australian research council arc discovery grant australia india aisrf grant to ksm phi is available freely for download as pre compiled binaries or as source code at http www monash edu science about schools chemistry mmm phi html c wiley periodicals inc v www chemistryviews com software news and updates www c chem org is well suited to situations where the ground term is well isolated from excited terms and becomes less applicable when this separation becomes smaller for ions the use of the ground free ion term is well justified in most cases and provides increased accuracy over calculations employing the ground multiplet only for ions the situation is more complicated due to much stronger cf effects compared to the lanthanides of course the free ion term may be used to examine the effects of weak cfs however as intermediate to strong cfs are much more common experimentally the ground term of the strong field limit is usually taken as the basis in cubic symmetry and terms which have no orbital contribution in first order can usually be well modeled as terms where the g tensor is used to correct for the presence of excited states cubic terms on the other hand do possess a first order orbital moment and can be well modeled as terms through the t p equivalence where an extra parameter is required to account for the orbital reduction and covalency operationally phi has the ability to simulate perform fits and parameter surveys for the following types of calculations and concurrently combinations thereof wavefunctions energy levels g tensors magnitudes and directions magnetization susceptibility and zeeman splitting where thermodynamic calculations can be performed simultaneously for arbitrary combinations of temperatures and magnetic fields the ability to fit arbitrary combinations of experimental data from different sources vastly increases the certainty with which model parameters can be determined compared with fits using a single data set phi is much simpler to use than other packages as it does not need to be compiled for each execution uses easily understood plain text input files and is available for most operating systems in addition to this the program can be operated through a graphical user interface gui which provides facilities for real time data visualization and serves as a convenient alternative to command line input the gui is a very good tool for teaching demonstrating how experimental observations are effected by different components of common magnetic hamiltonians through the use of high level parallelism either under the symmetric multiprocessing smp or the single process multiple data spmd paradigms phi is many times faster than magpack for similar calculations in addition to the added benefits of fitting and no limit to the number of magnetic centers through the simple input and the gui we feel that phi is much more approachable than condon and is easily applied to large polynuclear systems theory hamiltonian for a given problem the hilbert space is constructed from n sites with angular momentum basis states of either si msii ji mjii or li mli si msi where i n note only a single term is used to describe each ion that is si ji or li and si are fixed the total uncoupled basis of the system is the direct product of all the individual basis states in which the hamiltonian matrix eq is constructed h h so þ h ex þ h cf þ h zee the first component of the hamiltonian the spin orbit so hamiltonian h so follows the parameterization of karayianis eq where the power series is used to correct deviations from the land e interval rule in the splitting of the ground term due to so mixing with excited terms the sum extends to order where si is the total spin of the term in question the coefficients and were tabulated for the tripositive lanthanides by karayianis however we have optimized these and included higher orders where required see supporting information as the so coupling is much weaker in ions compared to ions the land e interval rule is well obeyed and therefore it is most common to employ only the first order contribution the second component of the hamiltonian the exchange interaction h ex is parameterized solely with the isotropic heisenberg dirac vanvleck effective hamiltonian eq while such an approach is common place in spin only clusters the subject of magnetic exchange between orbitally degenerate ions is nontrivial and a number of attempts have been made to determine an effective operator for such cases currently in phi the exchange interaction for orbitally degenerate ions follows the treatment of lines which includes only the isotropic spin spin exchange between the true spins and can be applied in the ji mjii or li mli si msii bases indeed the isotropic exchange model is an oversimplification in some cases however the determination of the complete exchange tensor including antisymmetric and asymmetric anisotropic parameters requires a much more indepth analysis than is commonplace therefore in an attempt to avoid over parameterization this simple model has been adopted the third component of the hamiltonian the cf hamiltonian h cf eq follows the operator equivalent technique outlined by stevens et al where hki is the operator equivalent factor for rank k and the bkqi are the crystal field parameters cfps equivalent to akqihrkii in stevens notation the o qk operators are based on the tesseral harmonics and this implementation includes all orders k q k where the negative q operators are equivalent to the sine operators in hutchings notation the cf hamiltonian may be applied to ions in the ji mjii or li mli si msii bases the required operator equivalent factors for the lanthanides in the ji mjii basis have been tabulated and while the second and fourth rank factors for the li mli si msii basis can be determined from a pair of formulae reference to the sixth rank factors could not be found by the present authors therefore all operator equivalent journal of computational chemistry noncollinear magnetic systems polynuclear metal complexes with low symmetry and strongly axial local anisotropy can display a noncollinear arrangement of these local anisotropy axes this complicates considerably any calculations compared to a scenario where all axes are parallel as no good axial quantum number can be identified due to reduced angular momentum symmetry the noncollinear scenario is in fact very common and can lead to new magnetic quantum states such as toroidal magnetic moments recently identified via ab initio calculations in a triangle it has been shown that the low lying magnetic states in noncollinear polynuclear complexes can be represented in an especially clear and simple manner by means of noncollinear spin basis states made up of the direct products of local spin functions quantized along the local axes here in the program phi this technique has been generalized to include orbital angular momentum and allows users to rotate the individual reference frames of the magnetic centers to allow description of each center in an optimal local frame the two sources of magnetic anisotropy in phi are the anisotropic g tensor and the cf hamiltonian rotation of a principal valued g tensor through the euler angles a b and c is trivial however the rotation of the cfps is more complicated and is performed according to mulak and mulak convention magnetic properties once the hamiltonian matrix is constructed it can be diagonalized see program structure to yield the wavefunctions and energies eigenvectors and eigenvalues for the system through the inclusion of the zeeman hamiltonian the nonperturbative field dependent magnetic properties may be calculated through the fundamental thermodynamic expressions eqs and where a b x y z dim is the dimension of the hilbert space and z is the partition function these expressions calculate vm and m in the units commonly employed by chemists of mol and lb mol respectively the finite difference method is used determine the energy journal of computational chemistry tives with respect to the magnetic field equation reduces to the traditional vanvleck formula in the limit of zero magnetic field however the numerical method employed here is capable of accurately determining the susceptibility in the presence of non zero fields as used in experimental measurements care must be taken at level crossings to correctly determine the derivatives as the eigenvalues provided by most numerical libraries are ordered by value and therefore change index at a crossing it is of paramount importance to identify the correct sets of eigenvalues at such crossings in order to obtain correct magnetic properties for isotropic spin only systems the hamiltonian is spherically symmetrical and thus all directions in space are equivalent and only one needs to be evaluated to determine the powder properties for anisotropic systems a large number of orientations of the field with respect to the system are in general required to correctly account for the powder behavior to accomplish this the zaremba conroywolfsberg powder integration scheme using nonrandom directions has been implemented according to levitt et al for calculations involving low symmetry anisotropic single ions with an odd number of unpaired electrons half integer spin kramers ions g tensors may be calculated within the basis of each kramers doublet which is equivalent to treating each doublet as a pseudospin s state whose magnetic anisotropy is given by the g tensor high symmetry cases where the state multiplicity is greater than two require thirdand higher rank g tensors to correctly account for the magnetic behavior which can have observable consequences in nmr spectroscopy tensors of rank greater than two have not yet been implemented in phi however it is expected that even where high symmetry environments are identified active jahn teller modes will reduce the symmetry such that the spectrum splits into kramers doublets so that our approach is virtually exhaustive for the study of molecular nanomagnets for calculations involving single lanthanide ions in the li mli si msii basis the wavefunction is expressed also in the ji mjii basis providing a means of investigating the extent of j mixing by the cf while the general method for the calculation of the magnetic properties of arbitrary systems has been given above a useful simplification of the method is possible when considering magnetically isotropic spin only compounds taking advantage of the spherical symmetry of the hamiltonian in conjunction with first order approximation methods leads to a substantial reduction in the computational demands of the problem while the uncoupled basis is most useful for anisotropic systems easily allowing formulation of the so and cf www chemistryviews com software news and updates www c chem org figure operational schematic of phi note the boxes are representative only of the flow of the program and do not necessarily correspond to individual subroutines or functions hamiltonians isotropic systems requiring only the exchange hamiltonian are block diagonal in a total spin basis in this case the problem can be solved by considering each block independently greatly reducing the dimension of the problem and speeding up the calculation the matrix elements can be calculated using irreducible tensor operators itos and the wigner ekhart theorem and while the literature is well established the necessary equations and procedures are presented in the supporting information to clarify frequent typographical errors and to present a consistent notation once the matrix elements have been calculated the matrix is diagonalized and the magnetic properties calculated by considering the first order zeeman perturbation to the ms states through an averaged g factor program structure phi is written entirely in fortran with an optional gui written in python using the pygtk and matplotlib libraries and designed with glade phi relies on blas and lapack libraries of any flavor mkl atlas etc and optionally cuda cula plasma and mpi the program can be well understood by means of an operational schematic figure phi has been written to take advantage of multiple processor cores now common in consumer and specialized machines there are three models of parallelism currently supported by phi gpu hardware smp and spmd which use fundamentally different ideas to perform tasks more efficiently compared to a sequential model gpu support is provided by the cuda and cula libraries smp processing is achieved through multithreading using openmp and the plasma library and spmd execution uses the mpi standard nearly all components of phi have been parallelized however the parallel speedup depends greatly on the type of calculation to be performed see performance section multidimensional nonlinear optimization is a difficult problem often requiring in depth parameter space analysis to determine the global minimum for a given model for this reason phi contains two internal fitting algorithms powell method and the simplex method which have been implemented based on those described in numerical recipes for fortran the simplex method is well suited to optimizing nearby minima while powell method is often useful in situations where a good initial guess is not available phi contains several functions from the fortran version of stevenson anglib library modified versions of the functions cleb sixj binom and angdelta are contained within the source in the block diagonal perturbation method the computation time is largely dominated by the evaluation of the matrix elements which in turn is largely dominated by the evaluation of the wigner symbols of equations to speed up these calculations the symbols are uniquely stored in an array and retrieved to avoid repeated calculations of the same values this becomes inefficient however when using a smp parallel implementation due to data synchronizations and therefore this storage mechanism is only utilized in sequential or spmd calculations performance the parallel performance of phi scales extremely well for anisotropic calculations due to the implementation of high level parallelism a model calculation of the magnetic susceptibility for an anisotropic dimer such as that presented below in example has a hilbert space dimension of and in this case we have used directions for the powder averaging procedure the theoretical and observed speedups with respect to a sequential calculation are shown in figure where the smp calculations were performed on a system with four eight core intel xenon nehalem cpus ghz and the spmd calculations were performed on a series of nodes each with two quad core intel xenon nehalem cpus ghz linked by a quad data rate infiniband network it can be seen that the code shows only minor deviations from the journal of computational chemistry 1175 software news and updates www c chem org enhanced by plasma with multiple threads to diagonalize a single matrix leading to heterogeneous nested parallel computation an elaborate binning procedure based on table dimensions of the sub spaces an empirical timing heu for the associated spins within the block ristic is used to assign the diagonal form of the hdvv iihamiltonian iii available cores to the in a coupled basis for a mn problem most appropriate submatrices to minimize the total spin dimension total computational time phi has been bench marked by solving the problem on a smp system with four eight core intel xenon neha lem cpus ghz and is compared to 3829 magpack oc tober which solves the problem in approxi mately minutes using a single core of the machine the scaling of this method figure is not nearly as ideal as that observed for the anisotropic calculations due to the inherent heterogeneity of the problem however there is no theoretical ceiling to the obtainable speedup which is only limited by the available hardware figure theoretical and observed parallel speedup for an anisotropic calculation of magnetic susceptibility for a dimer utilizing directions parallel scaling is shown for the smp top and the spmd bottom models expected behavior as the number of cores increases when using a maximum of cores in this example the average performance of the code was roughly tflop floating point operations per second as expected performance degradation is observed if the number of threads or processes is greater than the number of physical cpu cores to illustrate the benefits of parallelization in conjunction with the ito or block diagonal approach the magnetic properties for a model cluster have been calculated the dimension of the hilbert space for a set of four mnii s and four mniii s ions is and a square matrix of such dimension which would consume roughly tb of memory in a computer not counting the auxiliary storage required to perform computations with it currently this is prohibitively large for most common machines however by applying the techniques described in the supporting information the requirement can be greatly reduced by examining the matrix in a coupled basis and removing the superfluous ms basis elements the problem is reduced to the block diagonal submatrices of the independent spins listed in table using multiple threads the submatrices can be diagonalized simultaneously thus reducing the total computational time to that for the largest submatrix provided a sufficient number of cores are available computational efficiency can be further journal of computational chemistry 1175 figure the observed parallel speedup for the calculation of the magnetic properties of the problem on a smp system with cores compared to magpack note that for one core the performance is slightly less than magpack due to less compile time optimizations examples a number of examples utilizing phi can be found in the literature however three examples are provided here briefly to show sample applications of the code www chemistryviews com www c chem org figure metal core of o teah dea iso ome f phen 3meoh where triethanolamine diethanolamine isoh isonicotinic acid and phen phenanthroline mniii yellow mnii green coiii purple n blue o red f orange example o teah dea iso ome f phen 3meoh this pentanuclear coordination cluster recently reported by some of us can be considered magnetically as a trinuclear cluster as the two coiii ions are low spin and therefore diamagnetic there are two mnii ions and one mniii ion and while all the ions are unique the local environments for both mnii ions are very similar and therefore they will be modeled with the same parameters the exchange scheme for the cluster is therefore a simple isosceles triangle with two exchange pathways and two g factors figure our aim is to use the magnetic susceptibility and magnetization data to determine the magnetic properties of this spin cluster first we used the survey feature of phi to calculate the residual error compared to the experimental data when varying the value of the two exchange parameters with g fixed at software news and updates for all ions figure left note that the residual error is the product of the residuals for the magnetization and susceptibility calculations thus ensuring one error surface does not dominate it can be seen that the error is extremely large red region for note logarithmic scale and that there is a valley where good fits are achieved blue region when is strongly antiferromagnetic and dominant the origin of this valley is due to frustration effects where the sign and magnitude of is almost inconsequential as long as is dominating upon examining this valley more closely a subtle fine structure can be identified figure right and a minimum is located initiating a fit from cm and cm the simplex minimization routine in phi finds that cm and cm with a residual error of r next the isotropic g factors are allowed to vary from along with the exchange parameters and the minimization routine finds cm cm g mniii and g mnii with r this fit however does not visually reproduce the low temperature plateau of in the vmt data and therefore a number of separate fits to the experimental data were conducted using the biasing features of phi in order to force better agreement to for example the low temperature susceptibility data upon considering all the separate fits visually the average of the parameters was taken giving the final set cm g mniii and g mnii while this parameter set is not numerically the best fit with r it is the most reasonable set which reproduces the features of the data qualitatively and quantitatively figure example o cmnm cl meoh pzh this tetranuclear coordination cluster recently reported by razali et al displays the common butterfly motif with the figure contour plot of the residual error vs and residual error is a logarithmic scale red dark blue left and a linear scale white red blue right lower residuals signify better quality fits to the experimental data journal of computational chemistry 1175 software news and updates www c chem org exact magnitude as the subtle splitting of the low lying states is not overly affected and therefore the low temperature data does not provide much information about performing multiple fits to the susceptibility and magnetization data individually and simultaneously with various areas of focus low temperature or high field etc allowed the determination of an average parameter set which best reflects the data cm cm and g figure multiple g factors for the different manganese ions were not employed to avoid over parameterization example acac tea mn acac figure dc magnetic susceptibility top and magnetization bottom for o teah dea iso ome f phen 3meoh the red lines are calculated using the parameters in the text less common oxidation state distribution where the central body ions are mniii and the outer wingtip ions are mnii similarly to example our aim is to determine the magnitudes and signs of the exchange coupling pathways and g factors by fitting the magnetic data using phi the cluster has a center of inversion thereby rendering both pairs of mnii and mniii crystallographically identical this leads to a three j coupling scheme figure with one body body interaction and two distinct wing body interactions and the two wing body exchange pathways are isomorphic meaning that they can be interchanged in the model without altering the magnetic properties only magneto structural correlations can unambiguously assign the nature of the pair however difficult this may prove to be initially we note that reasonable correspondence with the data can only be obtained when is strongly antiferromagnetic roughly cm the well isolated low lying levels are dictated by and and therefore a survey of these two parameters was performed with g fixed at for all ions and cm the error surface for the two exchange parameters was calculated and shows a plane of reflection along the line due to the isomorphism figure left a local minima is observed in the region however the global minimum is in the region due to the very strong nature of it is difficult to determine its journal of computational chemistry 1175 this hetero bi metallic tetranuclear star cluster was recently reported by chilton et al along with a rare example of a cluster exhibiting ferromagnetic interactions the analysis of the magnetic exchange in acac tea mn acac was only examined via visual simulations of the vmt data with magpack as the software lacks data fitting capabilities therefore we have subsequently reanalyzed the magnetic data now including both vmt vs t and m vs h data with a more complex hamiltonian taking into account the zero field splitting zfs and noncollinearity of the mniii ions the star complex has threefold symmetry around the central gdiii ion each arm bridged to an mn acac unit by two alkoxo bridges of two triethanolamine groups which lie above and below the plane of the three metals fig there is a single mn acac molecule in the lattice for every star cluster which interacts only via van der waals forces with other molecules and is therefore considered to be magnetically noninteracting initially the compound is modeled as in ref with two isotropic exchange interactions for the nearest neighbor and next nearest neighbor pairs with g fixed where smn and sgd fitting both sets of magnetic data simultaneously led to the parameter set cm 03 cm which is significantly different from that originally reported of cm cm and g while the new parameters model the vmt vs t adequately it is clear from the m vs h data that there is significant anisotropy present which needs to be taken into account as the figure butterfly motif of o cmnm cl meoh pzh cmnm cyano imino methoxy methyl nitrosomethanide mn light blue n dark blue o red central mn ions body ions are trivalent outer wing ions are divalent www chemistryviews com www c chem org software news and updates figure contour plot of the residual error vs and residual error is a logarithmic scale red dark blue left and a linear scale white red dark blue right lower residuals signify better quality fits to the experimental data exchange coupling values are without doubt very small in magnitude the excited states are very close in energy to the ground state which precludes use of the giant spin approxi figure dc magnetic susceptibility top and magnetization bottom for o cmnm cl meoh pzh the red lines are calculated using the parameters in the text mation to determine the zfs of the ground state we therefore opted for another approach using the local anisotropy of the mniii ions while neglecting any zfs of the gdiii ion as this is expected to be orders of magnitude smaller the zfs parameters for mn acac are well characterized as d cm and e cm so the vmt vs t and m vs h data were simulated for an isolated mn acac molecule and subtracted from the observed data assuming a negligible interaction with the star cluster the x ray crystal structure of acac tea mn acac shows that the jahn teller elongated axes for the mniii ions in the star cluster are canted by a torsion angle of with respect to the vertical threefold rotation axis and by in the plane of the four metals with respect to each other therefore the noncollinearity of the axial directions for each figure acac tea star motif mn acac not shown for clarity where acach acetylacetone and triethanolamine jahnteller elongated axes for the peripheral mniii ions are shown in gold mniii light blue gdiii green c grey n dark blue o red h atoms omitted for clarity journal of computational chemistry 1175 software news and updates www c chem org the energy spectrum of the star resembles a regular staircase with steps consisting of four kramers doublets at intervals of approximately cm up to approximately cm above which the spectrum becomes almost continuous table as the zfs on the mniii ions is much larger table twelve lowest lying doublets than and much for the acac tea star much larger than gxy gz doublet energy cm the properties of the molecule can be 131 understood by iii ering the mn ions as simple dipoles the ground doublet is magnetic in the xy plane of the star due to the mutual tion of the magnetic 300 moments of the mniii ions when they are all canted the same way around the star which also gives rise to the non zero gz in the first excited state the orientation of one of the mniii ions has flipped from the ground state giving a small gz and a significant gxy subsequent pairs of excited doublets are conceptually similar to the lowest two however they are sequentially mixed with smaller projections of the gdiii spin due to the antiferromagnetic interaction and therefore the magnetic moments increase with energy figure dc magnetic susceptibility top and magnetization bottom for the acac tea star the red lines are calculated using the parameters in the text mniii ion can be easily described by the sets of euler angles a b c respectively these rotations were fixed along with the exchange coupling values given above and g while only the axial zfs parameter d for the mniii ions in the star was varied eq note the zfs operators in eq are primed as they are translated into the local frame of each mniii ion not the global frame as all other operators h ðs þ s þ s þ s gd þ ðs s d þ s s þ s s þ þ ðo þ o þ o þ þ lb ðgmn ðs þ s þ s þ þ ggd s gd þ ba initial variation of d showed a minimum around d cm however the jahn teller elongation of the mniii ions restricts the sign of d to d and we can assume that the single ion anisotropy will be reduced from that of mn acac therefore d cm was taken as a starting point for a fit of and d for both the magnetization and susceptibility data this led to the final optimized parameter set fig cm cm and d cm with g 1172 journal of computational chemistry 34 1175 example a model complex and a dyiii dimer to extract information on the low lying magnetic wavefunctions of lanthanide ions in crystalline environments the current computational standard involves post hartree fock ab initio calculations utilizing the casscf treatment 56 as mentioned in the introduction we explain a method whereby ab initio results may be translated into the framework of cft where polynuclear calculations are much more feasible calculations have been performed using molcas 59 with the casscf rassi modules and ano rcc basis sets on a series of model complexes ions of varying square antiprismatic sap geometries one with perfect sap geometry one with axially compressed sap geometry and one with axially elongated sap geometry figure the ano rcc vqzp and the ano rcc vtzp bases were used for dy and o respectively in the rasscf module only the space was occupied and contained active electrons in orbitals thus representing a in complete active space see supporting information table for the number of roots and states used for dyiii in the casscf and rassi procedures the anisotropic g tensors for the lowest spin orbit and crystal field kramers doublets were calculated based on the pseudospin s formalism care was taken to identify the kramers doublets and corresponding g tensors belonging to the ground term as the multiplet often falls between the and the www chemistryviews com www c chem org software news and updates figure visualization of the three model square antiprismatic complexes a compressed b perfect and c elongated multiplets and must be excluded for simulations based on solely the ground term the energy levels and g tensors resulting from the ab initio calculation were then fitted using phi with the so and cf hamiltonians appropriate to the point symmetry of the ions eq for these calculations the so parameters were initially fixed supporting information table and the cfps were allowed to vary until the unique global minimum was found the so parameters were then allowed to vary with the cfps to determine the optimal so splitting for the model the so parameters were then fixed at their optimized values and the cfps reminimized with respect to the g tensors and only the lowest energy levels of the multiplet leading to the final set of parameters given in table the resultant cfps yield energy splittings and g tensors which agree almost perfectly with those determined by molcas across the ground term the low lying energy spectra of the three situations are shown in figure þ o þ o þ h o x lines approach for coupling constants j both positive and negative recent work by feng et al has shown that changing the bridging ligands from oh to f was able to modulate the exchange interaction between a pair of dyiii ions in a centrosymmetric dimer from antiferromagnetic to ferromagnetic this system would be a good candidate for analysis by the method proposed here example magnetic moments of d block t terms through the pioneering work of figgis lewis gerloch mabbs and webb magnetochemists have a robust methodology for approximating the magnetic properties of orbitally degenerate d block metal ions as discussed t terms can be well modeled using a fictional orbital angular momentum of l through the t p equivalence using phi it is easy to reproduce identical results to the simulations of mabbs et al and kj ð l s þj j¼1 while the wavefunctions of an axial cf are j jzi j mji eigenstates this is not the case for lower symmetry cfs and such a translation shows the composition of the wavefunctions with clarity not possible in ab initio results once the cfps are known for a single ion its individual magnetic properties can be calculated and thus interactions with other ions may be easily modeled using phi the theoretical magnetic susceptibility curves for an exchange coupled dimer perfect sap elongated sap have been calculated figure using the table cf and so parameters determined from ab initio calculations on three square anti prisms parameter b04 k3 compressed cm perfect cm elongated cm 69 375 106 000135 374 00 124 0000440 figure cf splittings of the multiplet showing the jz eigenstates for the three studied square antiprisms journal of computational chemistry 34 1175 software news and updates www c chem org figure calculated magnetic susceptibility curves for a dimer with one perfect and one elongated square antiprism coupling constants expressed in cm figgis et al for the average magnetic moments of various octahedral ions with and without tetragonal distortions respectively figure in ref which presents the average magnetic moment for a tetragonally distorted octahedral ion with a ground term is reproduced by phi in figure where r m ¼ d k and d is the magnitude of the tetragonal distortion figure in ref presents the average magnetic moment of and terms of octahedral transition metal ion for both positive and negative so coupling such a theoretical exploration is easily handled by phi figure where r ¼ conclusion we have presented a powerful new tool phi capable of calculating the magnetic properties of simple or more complex molecular systems in a very efficient manner we have shown its applicability to simple spin only complexes and to aniso figure average magnetic moment for octahedral and terms for both positive and negative so coupling k note the different scale for the plots compared to ref tropic exchange coupled systems the performance of the package has been shown to exceed that of currently available software and display excellent parallel scaling in the future we hope to add electronic paramagnetic resonance and inelastic neutron scattering capabilities to the program zinc is a free public resource for ligand discovery the database contains over twenty million commercially available molecules in biologically relevant representations that may be downloaded in popular ready to dock formats and subsets the web site also enables searches by structure biological activity physical property vendor catalog number name and cas number small custom subsets may be created edited shared docked downloaded and conveyed to a vendor for purchase the database is maintained and curated for a high purchasing success rate and is freely available at zinc docking org introduction zinc is a research tool for investigators seeking chemical matter for their biological targets it incorporates purchasable compounds from over one hundred vendors and annotated compounds from over twenty databases since it ﬁrst appeared eight years ago zinc has grown over fold in size the quality of its molecular representations have been improved and new features have been added to its interface whereas it retains its original focus on biologically relevant forms for molecular docking zinc now supports other chemoinformatic techniques to be useful for research a database for ligand discovery should be big the compounds purchasable the molecules relevant and in biologically applicable forms i e representations that actually bind to proteins it should be available in useful subsets easy to search and download and ready to use without additional processing to maximize its coverage of chemical space the database should include as many catalogs of biologically relevant molecules as possible hypothesis testing of computationally predicted ligands for proteins is fastest if the compounds are purchasable and thus current information about expected delivery is crucial the user should have some say in how long he is willing to wait for delivery to minimize screening artifacts it is common practice to ﬁlter out compounds containing problematic functional groups but reasonable people disagree about exactly which rules should be applied ideally the user should have some choice about whether to be strict or permissive about including molecules that are only sometimes problematic using the relevant protonated and tautomeric molecular form for molecular docking is important as exempliﬁed by recent studies a suitable database should include all relevant protonated and tautomeric forms and as few irrelevant ones as possible some forms such as deprotonated sulfonamides thiols and aromatic alcohols are only relevant at high ph for instance in american chemical society the binding site of a zinc metalloenzyme on the other hand some protein sites bind protonated anilines but these same forms would be irrelevant decoys for most binding sites therefore ph dependent representations of the screening library are required current opinion favors general purpose screening libraries that are ﬁltered by physicochemical properties particularly for molecules that have low complexity two are particularly popular lead like for assays and techniques where binding is not observed directly and requires higher aﬃnities and therefore more mass and fragment like for techniques such as xray crystallography nuclear magnetic resonance and surface plasmon resonance where high concentrations and weak aﬃnities can be detected for compatibility with historical studies in the ﬁeld and to be generally useful the library should also be available as a drug like subset the database should allow known compounds to be looked up by the target they bind experimentally known compounds can be used as experimental controls chemical probes as starting points for hit to lead optimization or to calibrate docking calculations whereas compound bioactivity data are available in the literature actually assembling sets of purchasable bioactives has remained labor intensive database searching should be fast and easy for nonexperts while oﬀering ﬂexibility and power for experts it should be possible to also search by molecular similarity substructure physical properties delivery time and even name and cas number libraries of purchasable natural products metabolites drugs and experimental compounds would be useful for research because it would allow known bioactives to be docked and then acquired for testing for instance docking metabolites can be used for protein function received march published may dx doi org j chem inf model 1768 journal of chemical information and modeling article prediction 13 and docking drugs might be useful for predicting oﬀ target eﬀects or repurposing zinc has a unique focus but inevitably overlaps to some extent with other databases chembl pubchem drugbank bindingdb and tcm taiwan for instance all contain biological activity data but lack zinc focus on docking and purchasability chemspider www chemspider com combines both biological activity data and purchasing information but does not have zinc focus on biologically relevant representation for docking or its organization into discovery oriented subsets procurement agents and compound vendors oﬀer purchasability and increasingly e commerce sites and downloadable screening libraries including target focused libraries for compounds in their collection but they do not share zinc focus on relevant forms and subsets for docking in an eﬀort to improve ligand discovery and virtual screening we describe here an improved public resource of purchasable molecules that are relevant for medicinal chemistry and chemical biology the salient criteria for zinc are as follows compounds should be purchasable for rapid testing of docking hypotheses subsets of molecules should be easy to create the database should contain biologically relevant molecules represented in biologically relevant protonated and tautomeric forms and be organized into subsets that are ready for screening the database should be quick to search and download and it should be straightforward to obtain regular updates in many cases it should be possible to simply look up the biological activities of molecules or to look up compounds that are active against a particular target and are listed in text and graphical at ﬁltering docking org see the supporting information molecule preparation protocol catalogs are obtained as sdf ﬁles and converted to isomeric smiles using openeye oechem software we generate up to four stereoisomers for stereochemically ambiguous molecules a trial structure is ﬁrst generated using molecular networks corina to generate a single canonical conformation with the best ring puckering if applicable arguments are d neu wh rc mc canon molecules are generated in four ph ranges using schrodinger epik version as follows at ph of 05 a single best conﬁguration is generated using the arguments ph 05 ms for the range ph of i e additional protonated and tautomeric forms are generated such that they have a relative population of at least within that ph range using the arguments ph pht tp similarly for high ph of i e and low ph of i e 75 the arguments are ph 75 pht 75 tp and ph pht 75 tp respectively for ﬂexibase ﬁles used by dock 24 conformations are calculated using openeye omega with the following settings warts true fromct false fixmaxmatch enumnitrogen false enumring false energywindow maxconfgen maxconfs rmsthreshold atomic charges and desolvation are calculated using using a protocol we have reported previously the zinc processing pipeline continues to evolve and is described online in more detail at http wiki bkslab org index php zpp calculations of physical properties we use molinspiration mib software www molinspiration com to calculate logp polar surface area psa molecular weight number of hydrogen bond donors and acceptors and number of rotatable bonds we use to calculate polar and apolar desolvation energies using the protocol of wei graphical user interface software the zinc user interface has been completely rewritten in php and javascript ajax this ﬂexible architecture allows for easy implementation of new chemical tools which will further facilitate development of zinc the interface uses jquery and webme www molinspiration com this software is available for free from our web site and will be published separately clustering and library diversity we assess the chemical diversity of a subset by clustering the molecules first we sort ligands by increasing molecular weight then we use the subset to progressively select compounds that diﬀer from those previously selected by at least the tanimoto cutoﬀ using chemaxon budapest hungary axonpath ﬁngerprints in jchem the resulting representatives have two interesting properties first each representative diﬀers from all the others by at least the tanitmoto cutoﬀ second all the molecules in the subset are within the tanimoto cutoﬀ of at least one representative thus the representatives can be said to cover the chemical space of the subset at a given tanimoto level natural product like library we took all natural products known to us via public sources and fragmented them with mib www molinspiration com using the ﬂag we accepted only fragments that had ten or more non hydrogen atoms we then compared them to zinc accepting all molecules that had a tanimoto coeﬃcient of or higher to at least one natural product or its fragments using chemaxon axonpath ﬁngerprints in jchembase budapest hungary clustering chembl annotations we wanted to combine annotations for highly related species e g and also to recognize that methods compound sources and filters zinc was loaded from commercial supplier catalogs and annotated catalogs table if a salt the largest organic component is taken and table summary of zinc contents by a catalogs and b moleculesa a catalogs in zinc commercial screening compounds commercial building blocks commercial make on demand procurement agents boutique expensive for screening total purchasable catalogs annotated for bioactivity lab use total number of catalogs b molecules in zinc commercially available annotated for bioactivity lead like rule of fragment like rule of drug like rule of total unique molecules additional detailed information is provided in the supporting information and online at zinc docking org catalogs bnb numbers are approximate because zinc contents change frequently molecules containing an atom other than h c n o f s p si cl br or i are removed a limitation due to our use of the merck forceﬁeld only molecules passing the primary ﬁltering rules are loaded filtering rules are implemented in openeye functional group clean subsets or delivery time now subsets results in subsets that while smaller retain similar property distributions figure twenty catalogs of compounds annotated for biological activity such as have been added and new special subsets of natural products metabolites drugs and building blocks have been created enabling new research the user interface and the software behind zinc have been completely rewritten to enable new queries such as searches by biological target and delivery time an authentication and shopping cart system allow the user to organize research in progress the zinc database is updated continuously property subsets quarterly or better the last update of each catalog is shown on its catalog detail page zinc aims to follow the rule of the molecules in zinc are veriﬁed as being for sale within days we begin with new and improved features followed by examples of how zinc may be used for research new catalog types purchasability is critical to rapid hypothesis testing and thus is a central focus of zinc in the previous version there was only one kind of catalog purchasable screening compounds now there are eight catalog types corresponding to variation in cost delivery time and quantity for sale allowing subsets and search results to better match user requirements building blocks are available in gram quantities for synthesis and are often also available in milligram quantities for screening screening compounds are available in milligram quantities at an average price of up to per sample procurement agents incorporate many examples of these two catalog types and can simplify the logistics of compound acquisition these three classes are all treated as in stock for delivery within two weeks with a purchase success rate in our experience around make on demand catalogs of both building blocks and screening annotated ligands can be from very diﬀerent chemical series to do this we ﬁrst used the table in to group annotations of or better into six organism classes archaea bacteria eukaryotes fungi viruses and other unclassiﬁed for each annotation with a swissprot code we looked up the uniprot code and clustered annotations by uniprot preﬁx swissprot codes for which we could not ﬁnd a uniprot code were left unchanged for instance and were grouped into a single annotation e e for eukaryotic whereas and were in two diﬀerent groupings dyr e and dyr b respectively for each grouped annotation we used single level sphere exclusion clustering sphex with a minimum separation between cluster centroids of 85 as implemented in the jklustor program version chemaxon budapest hungary this resulted in typically one to three clusters for most grouped targets a few targets required as many as ten clusters because of the chemical diversity among the ligands results a new version of zinc that is substantially enlarged and improved since the previous is now available the number of catalogs of purchasable compounds has grown from to over table each molecule is now represented in multiple biologically relevant forms organized into four ph ranges to better model the appropriate molecular species the database is organized into subsets that better reﬂect current opinion in the ﬁeld the molecules in each subset have physical properties suitable for drug discovery figure there are now more subset choices by properties by delivery time and by whether potentially reactive groups are present filtering by dx doi org j chem inf model 1768 journal of chemical information and modeling article figure physical properties of lead like subsets color scheme lead like blue clean leads containing only benign chemical functionality red leads now in stock for two week delivery green nine properties a molecular weight daltons b calculated logp c heavy atom count d rotatable bonds e polar surface area f hydrogen bond donors g hydrogen bond acceptors h polar desolvation energy kcal mol i apolar desolvation energy kcal mol table property subsets of zinca these are general purpose screening libraries representing current opinions in the ﬁeld standard subsets are the biggest and have the most chemical diversity with week delivery times clean subsets have only molecules with benign functionality all molecules with potentially problematic functionality such as thiols aldehydes and michael acceptors have been removed now subsets are in stock only for rapid week delivery times see figure a we assess vendors by their responsiveness and ability to supply compounds in a timely fashion to our own requests and requests of our colleagues unresponsive vendors are rare but when we have evidence they are removed from zinc improved functional group filtering filtering out compounds with potentially problematic functional groups is common practice but reasonable people disagree about exactly which rules should be applied in the previous version a single set of ﬁltering rules prevented the most reactive compounds such as triﬂates alkyl halides and percholorates from being loaded this ﬁltering remains in eﬀect in the current version but what about compounds with more nuanced reactivity such as michael acceptors aldehydes and thiols our approach is to load these compounds have delivery times of weeks and in our experience a purchase success rate or better natural product catalogs contain compounds that the vendor attests are from natural sources collabocules a portmanteau of collaborate and molecules denotes molecules that are not for sale but may be available via collaboration such as brazilian natural products or compounds made by investigators in the enzyme function initiative annotated catalogs are databases not vendors in which many compounds have a biological measurement or annotation often available via a url boutique catalogs contain compounds that do not ﬁt into any of these categories often because they are building blocks that are not sold in small pack sizes and therefore are thought to be too expensive for screening dx doi org j chem inf model 1768 journal of chemical information and modeling article compounds and then ﬂag them as not having benign functionality now zinc users have a choice standard subsets which exclude the most reactive and problematic compounds and clean subsets which contain only molecules having benign functional groups table we ourselves often choose a standard subset because they contain a broader diversity of chemistry and nonbenign compounds are not necessarily a problem for instance in a recent experimental screen of compounds against ampc beta lactamase none of the nonbenign compounds were hits in a recent study against dopamine several hits were classiﬁed nonbenign and would have thus been missed if the clean subset had been screened zinc gives the user a choice improved biologically relevant molecular models recent studies continue to demonstrate the importance of using the relevant and molecular form for docking in the previous version we calculated the most relevant forms for docking without regard to ph as a consequence deprotonated sulfonamides and thiols necessary for docking to metalloenzymes were also included in the docking library when no metal was present in this context they were at best biologically irrelevant distractions and at worst decoys that could be the cause of much wasted eﬀort now molecular representations are enumerated and classiﬁed into four ph ranges see methods allowing ph dependent subsets of zinc most proteins are screened near physiological ph requiring representations between ph and some metalloenzymes require higher ph representations between and where most thiols and sulfonamides and some aromatic alcohols can deprotonate at the other end of physiological ph targets such as the mutant of cytochrome c peroxidase binds protonated aniline requiring that representations between ph and be included in a docking figure every zinc chembl bindingdb and pubchem for each annotated catalog zinc oﬀers both the full library and a subset containing only those that may be purchased which we call purchasable bioactive compounds pbcs pbcs are of particular interest because they are both biologically active and commercially available and thus may be immediately useful as controls probes or chemical starting points for optimization improved user interface the prior interface to zinc had numerous weaknesses slow unreliable queries and a single inﬂexible result format for instance some basic queries were either arcane or impossible to formulate and some functions were simply unreliable to address these problems and enable new research the web interface was redesigned the new modular interface makes interesting questions easy to construct for both beginners and experts figure queries may be composed using the search menu the quick search bar or by composing a url directly see the supporting information results may be formatted in over ten ways to suit a range of needs and tastes premade subsets by physical properties and catalog are available for immediate download small subsets of up to molecules by biological target combinations of searches or arbitrary criteria may be created edited shared prioritized and downloaded quickly and easily the user compounds may be uploaded and processed using the standard zinc molecule processing pipeline even for compounds that are not already in zinc provided they pass the basic zinc ﬁlters see methods physically matched decoys for docking benchmarks may be generated new tools to simplify compound acquisition are available a growing number of vendors now support direct transfer of the zinc shopping cart to their e commerce sites in all seven new and four improved search types are available which we now take up in turn new and improved search in the ﬁrst version substructure and similarity searches were unreliable and could take minutes it was not diﬃcult to create searches that never ﬁnished now zinc uses chemaxon jchembase for similarity and substructure search as a result users can now expect to search twenty seven million molecules in a few seconds in most cases the software that translates what is entered on the web page into an executable query runs it and presents the results has been completely redesigned new and improved structure query composition to support more users and more devices we now use the webme drawing tool a java free ajax technology that runs in all browsers and on all devices we have tried smiles are now presented in a text window as the structure is drawn and both the smiles and its depiction are alternately editable multiple smiles may be searched at once each at its own similarity level each smiles may be viewed and edited separately and up to one hundred smiles may be pasted from the clipboard and searched in a single transaction improved physical property speciﬁcation in the prior version limiting values were typed as text into clumsy text ﬁelds now the physical property range speciﬁcation tool allows maximum and minimum values to be speciﬁed either by typing or by sliding graphical controls presets such as lead like f ragment like and drug like simplify the most popular choices improved combination search in the original version all searches were always combined on a single page now each search type has its own page and a combination page allows complex queries when required new text queries in the previous version it was not possible to search zinc by just typing text now a quick search bar enables many queries with google like simplicity figure ph dependent representation of molecules in zinc illustrated by examples a protonation of histidine b tautomerization of folate c deprotontation of sulfonamide for metals and d protonation of aniline at low ph subset is now available in ph dependent subsets and every search result and shopping cart may be downloaded in representations ﬁltered by ph range using the representation selector in the results control bar figures and new bioactive compounds annotated compounds can be important controls for docking calculations and thus have an important role to play in zinc these include catalogs of metabolites such the human metabolome and metacyc natural products such as tcm taiwan approved drugs and clinical compounds such as and as w ell as actives from the medicinal chemistry literature such as dx doi org j chem inf model 1768 journal of chemical information and modeling article figure zinc combination search page the drawing panel is to the left and the smiles ﬁeld displays and allows editing of the smiles of the current molecule the redraw button updates the current smiles in the drawing panel the similarity level selector may be used to select the tanimoto cutoﬀ to be used or substructure the button adds a new smiles the report control bar controls how the results will be ﬁltered and formatted it contains page size speciﬁcation format selector representations selector purchasability selector and a run query button to the right is shown the property selection control bar including the preset selector top right allowing easy selection of popular choices figure zinc search results as tiles the default report format molecules may be added to the active cart by clicking on them making them blue hovering the mouse over a molecule reveals a popup with additional detail about purchasability physical properties and other similar molecules the report control bar allows additional report formats and changes to purchasability and ph it contains page number next page button page size speciﬁcation format selector representations selector purchasability selector add all to add all molecules to the active cart and ref resh to apply any changes that are made contains a catalog selection tool allowing ﬁne control over which catalogs will be searched catalogs may be selected individually or the purchasability may be speciﬁed by catalog purchasability type the vendor page also allows searching by original catalog code providing a list of partial matches on the ﬂy using ajax pattern matching is supported up to catalog codes may be uploaded by pasting from the clipboard and searched in a single transaction new search by target in the prior version there was no biological activity data in zinc now using chembl in the prior version zinc did not keep track of cas numbers and drug names in this version names synonyms and cas numbers as supplied by vendors and annotated catalogs have been loaded and may be searched using the quick search bar text search can also search by target by catalog by original catalog code by smiles and by zinc id pattern matching is supported using and wildcards see the supporting information new and improved catalog search in the prior version only one catalog could be speciﬁed per search the new version dx doi org j chem inf model 1768 journal of chemical information and modeling article molecules were already in zinc in the new version the upload facility has been updated and incorporated into the shopping cart facility and is only available to authenticated users having described the new and improved features of zinc we now turn to examples of how zinc can be used to discover chemical reagents for biology the ﬁrst ﬁve focus on acquisition of libraries for docking screens or chemical informatics research three address topics that were not easy before zinc but are now straightforward we conclude with some not so simple questions that can now be answered easily using zinc example acquire a general screening library for docking the original design goal of zinc was to be a source of general purpose screening libraries for docking among these lead like and fragment like most closely reﬂect current thinking in the ﬁeld we address two common concerns by oﬀering two variants of standard subsets clean subsets sacriﬁce chemical diversity for the expectation of fewer screening artifacts due to reactivity by ﬁltering out even marginally reactive compounds now subsets trade rapid sourcing from vendor stock for decreased coverage of chemical space table we also oﬀer ready to download and dock versions of every catalog in zinc for annotated catalogs there are two forms the full catalog and a subset of those that can be purchased termed purchasable bioactive compounds pbcs unique to zinc we also oﬀer special subsets of purchasable bioactive compounds drawn from many catalogs drugs metabolites natural products and natural product like compounds see methods problem the user wishes to discover ligands with new chemistry for a protein target using docking and thus requires a library of purchasable lead like compounds approach six steps are required a from the subsets menu in zinc choose properties figure b from among the kinds of subsets on the horizontal axis choose lead like c from among the types of subsets on the vertical axis choose standard d click on the word lead like in the top left corner of the table to go to the lead like subset detail page e to download the subset in sdf format on a unix or mac computer in the usual physiologically relevant forms click on the sdf link next to unix download under quick links at the bottom of the subset detail page f for ﬁner control click on the downloads tab just below the main menu where ﬁne control over ph dependent subsets is available variations i the user may also download the database in ﬂexibase or smiles formats ii the user may also select clean subsets when the tolerance for screening artifacts is very low or now subsets when compounds are needed immediately iii the user may also select fragmentlike drug like all purchasable and everything subsets example acquire a target focused library targetfocused subsets using data from chembl can be useful in silico to test whether a docking model can recapitulate known actives and in vitro as commercially available experimental controls zinc provides target focused purchasable docking libraries for over protein targets functional assays and adme t assays all active at μm or better as per chembl each bioactive refers back to the original literature via chembl problem the user wants a set of purchasable actives as controls for instance if the user is docking against a homology model of schistosoma mansoni hmg coa reductase he might wish to investigate how well approved drugs for the human form statins dock to the model approach four steps are required a from the search menu in zinc select targets b in the text ﬁeld under chembl swissprot type hmg a popup menu appears after a second or two allowing the user to compounds annotated for biological activity may be searched using the target name or uniprot or swissprot code three chembl annotation types are supported binding aﬃnity functional assay and adme t assay data only compounds reported active at or better are incorporated in zinc in addition to original chembl organism speciﬁc annotations labeled by uniprot codes we have grouped annotations by chembl organism class and clustered them by chemotype to provide a complementary way to organize compound annotations see methods improved report formats in the prior version there was a single report format listing all the information in zinc for each molecule in rows pagination was unreliable and downloading search results was limited now zinc oﬀers many report formats with several customization options to support many needs report formatting and ﬁltering may be selected using the report control bar figure the format selector oﬀers six on screen reports seven downloadable reports and ﬁve links to third party applications such as ucsf pymol schrodinger inc and instant jchem chemaxon budapest hungary the representations selector allows the ph range to be speciﬁed the purchasability selector is used to choose which catalog types to include and thus the purchasability of the compounds the report control bar is available in search results reports on the search page and in the shopping carts new scriptable interface in the prior version nearly all zinc queries required the web interface now all queries available via the graphical user interface may also be scripted a comprehensive query construction syntax is supported that covers all aspects of selection ﬁltering and formatting as a result other databases may now link to zinc directly using queries by target molecule name properties and other criteria see the supporting information the query details tab on the results page includes embeddable javascript a reusable url and zinc command language for each query new authentication and shopping cart in the previous version there was no way to combine the results of several searches or to retain results for future use in the current version a shopping cart allows the user to collect molecules from multiple searches review them and share them with colleagues compounds in a shopping cart may be prioritized by opinion using collaborative tools and an academic grading scheme of a through c the shopping cart can generate a purchasability report ranking vendors in descending order of the number of molecules in the cart they sell the contents of the cart for each vendor can be sent to that vendor by e mail or directly to the vendor e commerce site if supported users who request a free login and authenticate can access additional features such as multiple and persistent shopping carts and collaborative tools for prioritizing compounds new decoy maker a new feature allows authenticated zinc users to create physically matched decoys for the contents of a shopping cart the decoys are drawn from zinc using the dud approach for each ligand compounds with similar physical properties molecular weight logp charge h bond donors h bond acceptors but chemically distinct are selected from zinc the shopping cart is currently limited to compounds and therefore a maximum of actives are currently used for ﬁnding decoys improved upload of molecules to shopping cart in the previous version a facility was available to upload molecules to the zinc web site to prepare molecules for docking using the zinc processing pipeline regardless of whether the dx doi org j chem inf model 1768 journal of chemical information and modeling article select the ﬁrst choice c the user clicks on run query and in a few seconds approximately compounds appear as tiles d the user downloads these in the usual biologically relevant forms in format by selecting from the format selector in the results control bar and then clicking on refresh variations i the user may also choose to download in sdf or smiles format ii the user may request only a single form at ph by selecting single in the representations selector and clicking on refresh iii the user may specify all compounds not just the purchasable ones by choosing everything from the purchasability selector and clicking on refresh iv the user may type the word all into the page size ﬁeld or click the all button to specify that all available compounds should be downloaded authenticated users may download more molecules than anonymous users v a complete listing of all available purchasable annotated libraries using chembl annotations is available by choosing targets from the subsets menu vi annotations that have been clustered by organism type archaea eukaryotes bacteria fungi viruses and other as per chembl organism class and uniprot preﬁxes are also available by using the selector under clustered target instead of chembl swissprot example find purchasable analogs by catalog the user might want to explore around an initial hit by testing analogs analogs may also be used to expand a set of known actives from chembl to include possible actives for docking or testing in cases where no chembl actives are for sale for an annotation searching for purchasable analogs may yield commercially accessible controls or probes there are two ways to look for analogs overall similarity illustrated here and substructure illustrated in example below both are supported in zinc using chemaxon jchembase problem find purchasable analogs of an experimental hit for instance in a phenotypic screen the drug cetirizine was a hit the user thus wants to dock and purchase analogs of cetirizine an receptor antagonist to investigate structure activity relationships approach eight steps are required a in the quick search bar enter cetirizine and click go both enantiomers are displayed b to see full details about the compound on the right click on its zinc id number in the top left corner of the tile to display its molecule detail page c to ﬁnd close analogs click on the link under the word analogs on the right side of the page at time of writing purchasable compounds including stereoisomers are available d download them as in the previous examples e add them all to the shopping cart by clicking on add all in the top right corner of the page f switch to the shopping cart by clicking on active cart in the top right corner of the page g click on purchasability report for an analysis of how to buy them as of the time of writing toronto research chemicals trc sold more of these compounds than any other vendor h to enquire about price and availability click on email quote request if the user is logged in the e mail may be sent directly if anonymous the text may be cut and pasted to an e mail client variations i to ﬁnd more analogs select a more permissive similarity level e g ii instead of using add all put compounds in the cart individually by clicking on them compounds in the active cart are colored light blue iii authentication allows persistent carts and other features not available to anonymous users example acquire fragment and scaﬀold based libraries problem obtain a purchasable chemical library for docking for instance the user has read that molecules containing quinuclidine smiles are often active against muscarinic targets to investigate this the user wishes to download a library of fragment like compounds containing this functional group for docking approach seven steps are required a from the search menu in zinc select combination b clear any previous search by clicking on the blank sheet icon to the right of the smiling face in the webme drawing tool c in the text ﬁeld above the drawing panel enter click the update drawing icon and select substructure from the similarity level selector or draw it using the drawing tool d in the properties panel to the right select the predeﬁned set f ragment like from the preset selector in the top right corner e run the query by clicking run query f page through ﬁve pages to see the results using the next button in the results control bar at the time of writing there were purchasable compounds g to download these in smiles text format select smiles from the format selector type all in the page size ﬁeld and click ref resh variations i to download in or sdf formats make the appropriate choice from the format selector and click refresh ii to choose only compounds available for week delivery select in stock only from the purchasability selector and click refresh example acquire screening libraries of bioactive compounds problem the user is interested in discovering the function of proteins by docking and therefore wishes to only dock biogenic molecules approach four steps are required a from the subsets menu in zinc select properties b click on the special tab to view the special zinc subsets c select znp zinc natural products by clicking on the znp link d download as before either under quick links or using the downloads tab variations i other special subsets include drugs research compounds building blocks and naturalproduct like compounds ii additional libraries of purchasable bioactive compounds are available as purchasable bioactive compounds pbcs by choosing catalogs from the subsets menu and clicking on the pbcs tab example identify specialist vendors problem the user wishes to discover which vendors sell the most bioactives for a particular target for instance the user is interested in exploring the bias between beta arrestin and g coupled signaling pathways of beta adrenergic receptor ligands to study this experimentally the user wants to buy many compounds at low cost and would thus like to know which vendor sells the most known to bind at μm or better approach ten steps are required a from the subsets menu in zinc choose targets b click on the binding tab c use the web browser search feature to locate beta adrenergic note that several species are listed d next to the human annotation click on the catalogs link to perform a market analysis at the time of writing the company sigma aldrich sold compounds by far the most of any single vendor not counting chemonaut and molport which are purchasing agents e to view these compounds click on overview f to add these to the shopping cart click on add all g there is more than one page so click next to go to page and click on add all again h click on active cart in the top right of the page to view the shopping cart i click on purchasability report to see vendors sorted in decreasing order of the number of compounds they sell j click on e mail quote request to compose a message to the vendor authenticated users may modify and send this email directly to the vendor whereas anonymous users must copy and paste the text into their own e mail program to prevent spam variations i over targets are available to choose based on chembl dx doi org j chem inf model 1768 journal of chemical information and modeling article whereas peers can only see their own scores when ﬁnished click on update users to ﬁnalize changes e add molecules to the cart f teammates may view the cart by choosing carts from the subsets menu then rating the molecules using an a to c scoring scheme g the user may see teammates ratings by viewing the cart and clicking on hit picking party above the report control bar zinc can answer many other questions that have been challenging in the past a few brief examples illustrate some possibilities what targets does a vendor library hit before purchasing a library the user wishes to learn what targets it hits approach from the subsets menu in zinc choose catalogs click on the targets or clustered targets tabs to see a targetfocused analysis of the vendor library click on the catalogs link of any target to see a ranked list of vendors selling compounds for that target two complementary kinds of annotation are available native species speciﬁc chembl annotations and annotations grouped by organism class and then clustered by chemotype which approved drugs can be purchased as pure compounds approach from the subsets menu in zinc choose properties and then choose the special tab for drugs click on zdd for the zinc drug database this subset is ready to download or click on sample molecules to browse all the molecules in all special subsets are purchasable the user can also see which drugs are not for sale by clicking on sample molecules to browse and then choosing not for sale from the purchasability selector in the results control bar these are drugs that according to zinc are not for sale as pure substances if it is available we would appreciate being informed of where it can be purchased so that we can update zinc results can be obtained for natural products and metabolites also which library is most diverse library diversity is a huge topic with many nuances but for many users a simplistic analysis from zinc may be good enough for some applications approach in the subsets menu in zinc choose catalogs and then click the diversity tab this table displays the clustered diversity of every subset the user can sort by the diversity at calculated at four levels for instance clicking on sorts by the number of clustered representatives at the tanimoto level see methods the results in zinc show that there is far more chemical diversity represented in annotated catalogs such as chembl and bindingdb with sixteen and twelve thousand representatives at the tanimoto level respectively than is present in even the more diverse single vendor libraries such as vitas m and chembridge with about eight thousand each how can i dock and purchase vitamin k approach in the quick search bar type vitamin k and click go the molecule including purchasing information and ready to dock ﬁles is available drug names are loaded from vendor and annotation catalogs wildcards are supported thus c lexa will match celexa and methotr will match methotrexate and methotrimeprazine the pattern azepam will ﬁnd purchasable drugs that hit gaba and pride will ﬁnd drugs that hit dopamine as well as some other compounds some names will not hit this could be because the compound is not in zinc or we do not yet know about the name used cas numbers may be used also with wildcards our list of cas numbers is drawn from vendor catalogs and as such may be incomplete the protocols and instructions described here will evolve as zinc develops for the current versions of these protocols bioactivity data ii the user may combine the ligands from several annotations e g rat mouse and human by adding them each to the shopping cart and individual compounds in the cart may be removed by clicking on them example decoys for docking zinc can generate dud style decoy molecules with similar physical properties chemically dissimilar to the actives as a bias controlled docking benchmark decoys are useful for testing that docking enrichment is not artiﬁcial and therefore misleadingly good due simply to gross physical diﬀerences between the actives and the decoys problem the user wants to benchmark docking using actives and physically matched decoys at a high level the approach involves logging in putting the actives or their representatives into a shopping cart clicking on generate decoys conﬁrming waiting a few minutes and then downloading from a new cart approach ten steps are required a log in by clicking on sign in in the top right of the page b create a new cart select it and put the actives into it there are many ways to supply actives using chembl annotations uploading zinc ids or smiles while creating a cart or hand picking compounds from several queries in this example we will generate decoys for the glutamate receptor ka swissprot code c click on manage my carts must be logged in d in section for cart name type and click submit note that is now the active cart e from the search menu in zinc choose targets f from the annotation type selector choose binding in the text ﬁeld enter and click run query g click on add all to add these to the cart h go to the cart by clicking on active cart i in the cart click on generate decoys on the conﬁrmation page that follows click generate only the ﬁrst compounds in the cart will be used for creating decoys a new cart called decoys is created and is initially empty the decoy procedure runs asynchronously in the background and loads the decoys into the decoys cart when it has ﬁnished which typically takes a few minutes and depends on system load j when ready the decoys may be downloaded in smiles or sdf all other features work in the usual way example prioritize hits for purchase problem the user has identiﬁed molecules she likes by docking or chemical informatics before purchasing she wants to enlist the help of her colleagues to help prioritize them for purchase we call this a hit picking party a standard procedure in our lab buying compounds can be expensive and a hit picking party can help avoid blunders and solicit experience learned from other projects hit picking parties can also be didactically useful because they foster discussion about the biophysics and trade oﬀs involved in protein ligand binding approach a log in to zinc by clicking on sign in in the top right of the page b go to cart management by clicking on manage my carts top right c create a new cart named mygreatproject by using option typing mygreatproject in the text ﬁeld and clicking on submit optionally the user may populate the cart by uploading a ﬁle of zinc ids or smiles one per line in the large text area provide a description of the project the new cart is now the active cart d in section of the cart management page add teammates to the cart by typing part of their user name or part of their email address in the text ﬁeld and clicking on the plus button each teammate requires a docking org id which is free on request for subsequent projects the cart can inherit the teammates from a previous project by clicking on the buttons displayed below there are two kinds of teammates peers and pis pis can see all teammates scores dx doi org j chem inf model 1768 journal of chemical information and modeling article when looking for ligands for a target the simplest place to start is often the literature zinc incorporates a subset of chembl the medicinal chemistry database to provide a simple way to ﬁnd purchasable compounds for over targets and biological activities for hundreds of thousands of compounds compounds with experimentally known aﬃnities can be useful as experimental controls chemical probes as starting points for hitto lead optimization as well as controls for a docking calculation until recently assembling sets of purchasable bioactives for a target could take weeks of combing the literature now with zinc and chembl compounds are available in ready to dock formats in seconds making it easier to test docking models whenever ligands are available in the literature zinc also provides the ability to generate physically matched yet chemically distinct decoys which can help avoid library bias in docking calculations building zinc is prone to numerous pitfalls perhaps the biggest of which is irrelevant molecules among these incorrect protonation states and tautomeric forms are decoys that waste an investigator time compounds that are no longer for sale are irrelevant wasting time with false hopes of acquisition keeping these distracting molecules out of zinc is an almost sisyphean task and there is more to drive the perfectionist to tears catalogs that are not yet in zinc biological activity errors in the literature data transcription errors and coding errors in the interface whenever a problem is brought to our attention we aim to investigate it and ﬁx it promptly if we can zinc by its very nature is imperfect as each improvement allows new problems to surface notwithstanding these caveats zinc has emerged as a pragmatic and useful tool it supports molecular docking and chemoinformatics and is a general purpose source of relevant molecules for other techniques increasingly biological activity for compounds can be simply looked up or purchasable bioactives for a given target can be identiﬁed we hope zinc will help investigators to ﬁnd reagents for their biological targets and to discover the mechanism of action of their bioactive compounds please visit our web site http wiki bkslab org index php discussion three main results emerge from this work first zinc represents a much larger database of commercially available compounds for ligand discovery and virtual screening than before the database is actively maintained and is freely available online http zinc docking org second zinc molecules are represented in improved biologically relevant forms and organized into discovery relevant subsets such as lead like and fragment like that are ready for downloading and docking third zinc can now be used to discover the biological targets for a compound by simple mouse click or to ﬁnd purchasable compounds for a given target based on literature annotations we take up each of these results in turn at over million purchasable molecules zinc is the largest database of commercially available compounds for virtual screening the over vendor catalogs used to create zinc provide a diverse range of chemistry including drugs such as ketoconazole metabolites such as thebaine natural products such as harmaline and synthetic compounds such as dihydroxyphenyl ethylamine zinc catalogs are categorized by type and delivery time to allow the user to select molecules that match research goals for projects with tight deadlines subsets consisting only of compounds from stock oﬀer rapid delivery and high acquisition success rates whereas projects that can aﬀord to wait can beneﬁt from on average twice as many molecules albeit with lower acquisition success rates when a target screen cannot tolerate reactive groups subsets containing only benign chemistry can be used whereas other projects can beneﬁt from subsets with signiﬁcantly more compounds but also a higher risk of artifacts docking is often challenging frequently because of all the things that can go wrong with a docked molecule possible problems include incorrect protonation state irrelevant tautomeric form wrong conformation and commercial unavailability zinc improves docking compared to previous versions by providing better representations of biologically relevant molecules organized into reﬁned subsets that are relevant for discovery a primary focus of zinc is on the protonation state and tautomeric form of a molecule in our own docking projects we are constantly examining molecular forms that are automatically generated by our pipeline of programs when we ﬁnd errors we ﬁx the pipeline or if necessary individual molecules themselves another common problem is wrong conformations often associated with less common aliphatic rings or functional group combinations when we identify these problems often during a hit picking party we compare conformations against the cambridge structural and amend the molecule our protocol or both as needed we have found docking to be an outstanding way to ﬁnd errors in a library and in our molecule processing pipeline because incorrect molecules often rise to the top of the docking hit list demanding scrutiny even if the molecule is ideal from the chemical and physical perspective if it is unavailable to purchase too expensive or takes too long to arrive it is also irrelevant zinc tackles these concerns by updating and reclassifying catalogs continuously a static database would rapidly become less useful since at least one million new compounds become available every year and more than half as many become unavailable database curation for relevance is an ongoing unavoidable process if we are to have discovery relevant docking libraries associated content s supporting information tables summarizing the contents of commercial catalogs and annotated databases in zinc ﬁltering rules a guide to the zinc query construction syntax including some sample query scripts a quick search bar guide how to link to zinc and a description of the zinc processing pipeline this material is available free of charge via the internet at http pubs acs org author information corresponding author e mail jji cgl ucsf edu notes the authors declare no competing ﬁnancial interest acknowledgments we thank nigms for ﬁnancial support to b shoichet and j j i and the national research service award kirschstein fellowship to r g c we are grateful to the commercial software suppliers who support zinc chemaxon budapest hungary for the use of jchem marvin instant jchem and cartridge openeye scientiﬁc software santa fe nm for the use of oechem omega ogham and quacpac molecular networks gmbh for the use molecular mechanics force ﬁelds are widely used in computer aided drug design for the study of drug candidates interacting with biological systems in these simulations the biological part is typically represented by a specialized biomolecular force ﬁeld while the drug is represented by a matching general organic force ﬁeld in order to apply these general force ﬁelds to an arbitrary druglike molecule functionality for assignment of atom types parameters and partial atomic charges is required in the present article algorithms for the assignment of parameters and charges for the charmm general force field cgenff are presented these algorithms rely on the existing parameters and charges that were determined as part of the parametrization of the force ﬁeld bonded parameters are assigned based on the similarity between the atom types that deﬁne said parameters while charges are determined using an extended bond charge increment scheme charge increments were optimized to reproduce the charges on model compounds that were part of the parametrization of the force ﬁeld a penalty score is returned for every bonded parameter and charge allowing the user to quickly and conveniently assess the quality of the force ﬁeld representation of diﬀerent parts of the compound of interest case studies are presented to clarify the functioning of the algorithms and the signiﬁcance of their output data introduction intramolecular internal bonded terms part i of this described the charmm general force field cgenff atom typer which deterministically assigns bonds atom types to an arbitrary organic molecule using a program kb b k θ θ angles kϕ cos nϕ δ mable decision tree in the present paper a set of algorithms is presented to assign bonded parameters and charges to such a improper dihedrals dihedrals kφ φ kub urey bradley molecule as discussed in part i an essential component of a intermolecular external nonbonded terms molecular mechanics force ﬁeld is the parameter set including the bonded parameters associated with combinations of atom types as well as lennard jones lj and charge parameters nonbonded required to treat nonbonded interactions speciﬁcally in the case r min ij r min ij εij rij rij qiqj of class i additive force ﬁelds eq such as charmm for as the number of atom types for which bonded parameter are required increases it becomes increasingly unrealistic to optimize parameters for every possible combination of atom types while cgenff has a good coverage of individual chemical groups which continues to grow organic molecules consist of virtually limitless combinations of chemical groups consequently a typical drug like compound will contain combinations of atom types that do not match existing bonded parameters in the force ﬁeld in the context of the program charmm these every covalently bound pair of atom types a reference distance and a force constant are deﬁned and kb in eq similarly valence angle parameters kθ and are deﬁned for every covalently bonded combination of three atom types and dihedral parameters kϕ n and δ for every covalently bonded combination of four atom types also where necessary urey bradley terms kub and and improper dihedral parameters kφ and are deﬁned for select combinations of three and four atom types received august published november respectively american chemical society dx doi org j chem inf model 52 journal of chemical information and modeling article cases are called missing parameters because bonded parameters are required for every combination of atom types in the molecule as for the nonbonded parameters combining are commonly used to derive the lennard jones radius rij and well depth εij see eq for each i j atom pair in a chemical system from the per atom quantities ri rj εi and εj which in turn are associated with individual atom types therefore the atom typing functionality described in part i automatically takes care of the lennard jones parameters this leaves the charges qi to be assigned in addition to the bonded parameters the problem of assigning bonded parameters and charges to an organic molecule is far from new and a wide variety of approaches have been applied in organic force ﬁelds such as the tripos force ﬁeld cvff uff cff93 9 allinger mm3 13 and force ﬁelds and momany and rone commercial charmm force not to be confused with the academic charmm force ﬁeld note the diﬀerent capitalization of particular note are the general amber force ﬁeld gaff and its associated antechamber toolkit which marked the ﬁrst extension of a specialized biomolecular force ﬁeld i e amber to organic molecules and which provided inspiration for the present charge assignment scheme accordingly the present charge assignment scheme is empirical as are most of the aforementioned charge assignment approaches notable exceptions are amber and opls where charge assignment schemes based on a quantum mechanics qm electron density typically or have been proposed for small organic molecules while the resulting charges capture polarization and quantum eﬀects and therefore are typically signiﬁcantly more accurate than charges generated by more empirical assignment schemes they are also computationally signiﬁcantly more expensive which may become a bottleneck in high throughput applications involving millions of molecules substituting a very specialized atom type with a more generic variant typically has a lower penalty score than vice versa because it carries a lower risk of catastrophically inconsistent behavior before discussing the construction of the penalty matrix we give a brief overview of the computational methods for assigning parameters by analogy which provides a foundation for assignment of the penalty scores total penalty score tps calculation the total penalty score between two parameters ie a missing parameter and an existing parameter can be calculated as the sum of the penalties for substituting the atom types deﬁning the one parameter with the atom types in the second parameter thus for every missing parameter the program for assignment of parameters by analogy is tasked with ﬁnding the existing parameter in the parameter ﬁle or parameter database with the lowest tps with respect to the missing parameter this is accomplished by simply iterating through the parameter database calculating the tps for every parameter and returning the parameter with the lowest penalty this process is sped up by the fact that both the penalty matrix and the data structure containing the parameters are collated by atom type reducing the number and computational expense of lookup operations in the penalty matrix the speed gain achieved by collating the parameters is a result of the fact that a parameter in a majority of cases will only diﬀer from the previous parameter in the collated list by its last atom type thus in the example of an angle parameter deﬁned by atom types a b c the algorithm can be made to keep track of the penalty of the ﬁrst atom type a and the sum of the penalties of the ﬁrst two atom types a b if it is determined that only the last atom type is diﬀerent c only this atom type needs to be looked up in the penalty matrix and its penalty can be added to the latter sum similarly if the two last atom types are diﬀerent b c only those two atom types must be looked up and their sum added to the previously determined penalty for a thus substantially improving performance finally for every existing parameter the lowest of two tpss should be used the tps between the ordered missing parameter a b c and the existing parameter and the same tps for the physically equivalent reverse ordered missing parameter c b a as the former is not guaranteed to be always lower than the latter all resulting parameters are stored in a binary search tree which makes it easy to determine whether a missing parameter has been encountered and assigned before penalty matrix deﬁnition rule file the penalty scores for atom substitutions ideally should reﬂect the chemical characteristics on which the deﬁnitions of the atom types were based however as the number of atom types in cgenff is approximately and is likely to increase with the parametrization of new chemical groups directly populating the penalty matrix manually is not feasible or would at the very least be extremely error prone instead the full matrix is constructed from a smaller number of submatrices and oﬀsets which are deﬁned in a second section of the rule ﬁle used to assign atom types described in the preceding article this parameter assignment section has a formally similar layout as the atom typing rules although it is processed quite diﬀerently atom types are divided in a tree like structure with categories and subcategories delineated by a cat keyword deﬁning the name of the category and an end keyword every entry in a category can either be an atom type typ or a reference to a subcategory sub the name of which is followed by a colon the diﬀerence with the atom typing rules lies in the keywords after the colon the ﬁrst of which is pri short for priority followed by the approach and algorithms to obtain missing parameters in cgenff existing parameters that were determined as part of the parametrization of cgenff are identiﬁed by analogy based on the similarity between the atom types that deﬁne the parameters charges are assigned using a bond charge increment scheme that is conceptually similar to that implemented in the charge increments which function as parameters in the charge assignment algorithm are optimized to reproduce the charges on a training set of model compounds that were part of the parametrization of the force ﬁeld a noteworthy innovation in the presented charge assignment scheme is the fact that apart from the single charge increment associated with each possible bond as deﬁned by two atom types there are two charge increments associated with each possible angle and three charge increments with each possible dihedral angle as deﬁned by three and four atom types respectively the combined functionality in the cgenff atom and parameter assignment scheme makes it possible to apply the force ﬁeld to an arbitrary molecule in an automated fashion notably as a measure of the accuracy of the approximation a penalty score is returned to the user for every bonded parameter and charge which can be used to guide selective optimization of parameters and charges assignment of parameters by analogy the assignment of missing parameters is based on penalty scores quantifying the dissimilarity between atom types the penalty scores are based on an n n penalty matrix where n is the number of atom types it should be noted that this matrix is not necessarily symmetric dx doi org j chem inf model 52 journal of chemical information and modeling article table extract of the parameter assignment rule file that handles nitrogen atom types atom types and the penalty scores in this category were machinegenerated based on reference bond lengths in the cgenff parameter ﬁle the second largest and largest manually generated category contains oxygen atom types most of the penalties in the rule ﬁle are chosen manually it is a relative rare occurrence that an objective measure for similarity could be found that performed satisfactory for the purpose of assigning parameters by analogy during the process of programming the penalty matrix certain constraints were respected for instance values at a certain level in the tree are all in the same order of magnitude this is illustrated in table the category has an up penalty of for both sub rules while both of its subcategories have up penalties of for all their atom types and penalties within each category are lower than this up value also the penalty for substituting atom a for atom b was usually kept similar to the penalty for substituting atom b for atom a however these rules were not always followed rigidly for example in cases where a very speciﬁc type is changed into a type that is clearly more general the penalty was deliberately set lower than for the reverse substitution ie changing a general type into a very speciﬁc one as mentioned above in the end what mattered more than any constraint in the construction of the penalty matrix was its ability to assign sensible parameters by analogy and a large part of the eﬀort was focused on validating this ability this validation also gave rise to the below recommendations to validate and reoptimize parameters with tps values above certain thresholds most of this work was based on chemical intuition as it is not straightforward to do this in a systematic or quantitative way indeed cgenff internal parameters are validated against target data which is relatively expensive to compute and it is not trivial to devise a measure to objectively compare their performance when applied on diﬀerent molecules the present paper contains two case studies a more systematic validation study is the subject of future work the whole rule tree is read into a temporary data structure and checked for consistency by a tree walking algorithm that also replaces all atom type and rule name strings with pointers then a diﬀerent tree walking algorithm is started at the ﬁrst typ rule and exhaustively visits all other rules in the tree in a similar fashion as outlined in the above example ﬁlling in one line of the penalty matrix the aforementioned pointers help keep the penalty oﬀset added to the atom type substitution total penalty score when entering the subcategory through a sub keyword in a hierarchically higher category this will be clariﬁed by an example in the next paragraph this is followed by one or more alt keywords detailing alternative atom types or subcategories within the same category the number of alt entries is equal to the number of rules in the current category minus one every alt keyword is followed by the name of another typ or sub rule in the same category and the penalty score for substituting the current type with the named alternative type finally the rule ends with an up keyword followed by the penalty oﬀset for going one level up in the hierarchy as an example table contains a short extract of the rule ﬁle that handles nitrogen atom types in the category two subcategories are deﬁned and for neutral and positively charged nitrogen atom types respectively consider for example the protonated primary ammonium type substituting this atom type with secondary ammonium tertiary ammonium and quaternary ammonium types respectively incurs penalties of and if these substitutions fail the penalty for going up in the hierarchy is doing so brings us to the sub rule in the category which speciﬁes that substituting an atom in the subcategory with an atom in the subcategory carries a penalty of the penalties are added so the total penalty score so far is after entering the category the rule with the lowest pri penalty is picked which by convention is the ﬁrst rule in the category in this case with a penalty of thus the ﬁnal penalty score for substituting with is similarly the penalty score for substituting with is and so forth this scheme allows ﬁlling the full penalty matrix using a rule ﬁle of manageable size it is left to the writer of the rule ﬁle to make a trade oﬀ between specifying more speciﬁc and thus potentially more accurate values by making fewer larger categories on the one hand and having a smaller rule ﬁle and thus less potential for programming error by creating more smaller categories on the other hand however as the number of penalties in a category goes with the square of the number of rules in that category making large categories quickly becomes unwieldy thus the largest category in the parameter section of the current version of the cgenff rule ﬁle contains hydrogen dx doi org j chem inf model 52 journal of chemical information and modeling article figure chemical structures of compounds discussed in this article including atom naming negatively charged proline used as an example for illustrating the diﬀerent ways of calculating per charge penalties see table dimethylcarbamoyl indole case study n benzylacetamide case study computational cost of this process well within acceptable limits the same procedure is repeated for all typ rules to complete the penalty matrix the rows and columns of the penalty matrix are then sorted by atom type diﬀerential treatment of inner and outer atoms to improve the algorithm capability to pick good analogies two extra features were added to the basic algorithm described in the preceding paragraphs the penalties for substituting the central atom in an angle or improper dihedral and the two inner atoms in a dihedral are multiplied by because changing these inner atoms has a much higher impact than changing the outer atoms the same multiplication factor is applied to the penalties for substituting atoms in bond parameters because the impact of changing atoms in a bond parameter is typically comparable to changing the inner atoms in an angle or dihedral the penalty matrix used for these inner atoms including the atoms in a bond parameter will henceforward be called the bonded matrix in contrast to the nonbonded matrix that is used for the two outer atoms in angle and dihedral parameters speciﬁcally the bonded matrix focuses more on bonded properties valence and hybridization state while the nonbonded matrix is more aimed at describing discrepancies in nonbonded properties electrostatic and van der waals for example in the deﬁnition of the bonded matrix in the rule ﬁle the highest category in the hierarchy diﬀerentiates subcategories by hybridization state which are subsequently divided by period i e row in the periodic table while in the nonbonded matrix the period has precedence over the hybridization state the use of these two diﬀerent matrices can be rationalized by considering that the potential associated with a dihedral deﬁned by atoms is typically the result of a combination of nonbonded interactions on the one hand and the bond order of the bond and the hybridization states of atoms and which are bonded properties on the other hand a similar observation can be made about angle potentials however this reasoning does not apply to improper dihedrals where better results were obtained by using the bonded matrix for all four atoms although the aforementioned scaling factor only applies to the central atom problem strained rings the algorithm as described above was made available for public beta testing as part of versions 9 and 9 of the cgenff program during subsequent testing an important shortcoming was identiﬁed see the dimethylcarbamoyl indole compound in figure case study below consider a planar membered ring as depicted in figure geometry dictates that the sum of the ring inner angles figure generic planar membered ring α must be and that the sum of the angles α β β around each of the trigonal planar centers must be indeed in practice pyrrole and other membered aromatic heterocycles have inner angles α that are relatively close to the ideal and corresponding outer angles β close to in certain cases where the parameter assignment algorithm is given a substituted membered ring in which one of the outer angles corresponds to a missing parameter such as the angle in compound it will use an in ring parameter as a source of analogy or vice versa which leads to an error of in the reference angle and an unacceptable deformation in the geometry indeed in compound is an carbon which exhibits signiﬁcant similarity to an carbon in a membered ring from a nonbonded perspective therefore the parameter assignment algorithm used the angle parameter deﬁned by atom types as a source of analogy for and for the parameter on the angle giving rise to the deviation in table and to a pronounced deviation from planarity φ although the penalty score for this particular case was which indicates that validation is required similar cases were found with penalties lower than the recommended validation threshold as planar membered rings are ubiquitous in drug like molecules and this phenomenon can be shown to be even more pronounced in and rings it seriously jeopardized the applicability of the parameter assignment algorithm we ﬁrst attempted to remedy this shortcoming by increasing the penalties for substitutions between atoms types for rings with diﬀerent sizes and noncyclic atom types but it was found that a very large increase in the penalties was required in order to eliminate the problem and that this large increase considerably aﬀected the quality of other parameters by analogy for ring substitutions for example before increasing the penalties parameters with a membered ring atom type for were used as a source of analogy for the and angles and were given a justiﬁably low penalty penalties that are suﬃciently high to cure the problem discussed above would make this impossible solution bond groups as changing the penalty scores did not lead to an acceptable solution an extra bond group term was introduced in the penalty function starting from version 9 of the cgenff program table shows the deﬁnitions of these bond groups as they occur in the rule ﬁle here a bond group consists of the keyword bgrp followed by a penalty and a list of one or more atom types for the purpose of assigning bond group penalties bond angle and dihedral parameters are considered to contain one two and three virtual covalent bonds respectively for example the angle parameter discussed above contains the two virtual bonds and for each virtual covalent bond in a missing parameter the bond is member of a given bond group if both atom types are members of that bond group this way a virtual bond can be member of zero one two or more bond groups at the same time in the above example the virtual bond is member of both the bond group for planar membered rings and the bond group for all membered rings in table because both atom types are members of both bond groups conversely the virtual bond cg2r51 is not member of any bond groups because does not occur in any of these groups when searching the parameter database for candidate parameters with optimal analogy to this missing parameter bond group membership is compared pairwise between the virtual bonds in the missing and the candidate parameter if the virtual bond under consideration in the missing parameter is a member of a bond group and the corresponding virtual bond in the candidate parameter is not a member of the same group or vice versa then that bond group penalty is added to the tps such an approach severely penalizes for example the substitution of an endocyclic parameter for an exocylic term and vice versa as the bond group for a ring of a certain size includes all the endocyclic atom types that can occur in a ring of this size and no other exocyclic atom types two extra features were added to the basic bond group functionality described in this paragraph similar to the above the bond group penalties for substituting the central bond in a dihedral parameter or any bond in an angle or bond parameter are multiplied by because changing these bonds has a much higher impact than changing the outer bonds in a dihedral or the bonds in an improper the two ﬁrst bond groups in the rule ﬁle are considered equivalent for the purpose of determining diﬀerences in bond group membership between two virtual bonds this exception makes it possible to use the bond group feature to improve treatment of conjugated double bonds as discussed below dx doi org j chem inf model 52 journal of chemical information and modeling article may require optimization this speaks of the limited transferability of empirical force ﬁeld parameters and the need for the user to make decisions on the level of accuracy they need for system under study conversely cases have been encountered with large penalties yet satisfactory accuracy which can be understood by interpreting the penalty as a measure of the statistical imprecision or spread of the parameter assignment charge assignment extended bond charge increment scheme charges are assigned using a bond charge increment scheme from an atom centric point of view this can be formalized as qi jβij where qi is the ﬁnal partial charge on atom i is the previously assigned formal and βij is the bond charge increment for the bond between atoms i and j with βji βij from a bond centric point of view this translates to the following pseudocode it can now be seen how the bond group feature remedies the shortcoming in the treatment of strained rings as mentioned above the cg2r51 cg2r51 virtual bond is member of both ring bond groups in table while the corresponding cg2r51 virtual bond in the parameter originally used as a source of analogy is member of neither adding a bond group contribution of 20 20 to the tps and thereby virtually making sure that a more appropriate parameter will be chosen as a source of analogy at the same time the usage of a ring atom type for as a source of analogy for the and angles does not incur any additional penalty because none of the virtual bonds involved are members of any bond group additional applications of bond groups in addition to ﬁxing the original problem with strained rings the bond group feature allows for a better treatment of biphenyls and conjugated double bonds speciﬁcally as described in the preceding manuscript part i there are two sets of atom types for conjugated double bonds cg251o and cg252o which are applied such that in a chain of conjugated double bonds double bonded atoms are given atom types from the same set and single bonded atoms get atom types from a diﬀerent set initially in versions 9 and 9 it was necessary to apply very high penalties to substitutions involving these atom types in order to prevent single bond parameters being assigned to double bonds and vice versa as a side eﬀect of these high penalties favorable substitutions involving conjugated double bond atom types were also suppressed for example a missing parameter containing the virtual nonconjugated double bond could never be substituted with its conjugated equivalents and because penalties low enough to allow this substitution would make it equally likely to substitute the original virtual double bond with the virtual single bond cg2dc2 the introduction of the ﬁrst two bond groups in table which as mentioned above are considered equivalent for the purpose of determining diﬀerences in bond group membership eﬀectively gives the parameter assignment algorithm explicit knowledge of the bond orders consequently the penalties for substituting conjugated double bond atom types could be set very low instead relying on the bond groups to penalize incorrect substitutions analogous observations were made for biphenyls advantages and limitations finally it should be noted that compared to the wildcards used in a number of force ﬁelds the current scheme oﬀers a more exhaustive eﬀort to identify the best parameter to be used when an exact match for a parameter is not present it also oﬀers the ability to generate penalty scores that may alert the user to possible limitations in the assigned parameters nevertheless it should be emphasized that these penalty scores are based purely on the level of chemical similarity of the atom types in a parameter consequently even in cases where there is a penalty of zero i e when the exact parameter is already available in cgenff the quality of that parameter for determining the physical properties of the molecule may be poor for example all the parameters for a given type of linker between two rings might be available in cgenff however if the nature of the rings changes signiﬁcantly without a corresponding change in the atom types across the ring e g the c h carbon atoms in both pyridone and the pyridinium ion carry the type while the former is neutral and the latter is charged then the conformational properties of that linker are likely to change such that the associated dihedral parameters with penalties of zero we extended this scheme to angle charge increments α and dihedral charge increments δ this is implemented by associating one charge increment βij with every bond in the parameter ﬁle two charge increments αij and αjk with every angle and three charge increments δij δjk and δjk with every dihedral for every bond angle or dihedral in the molecules the charge increments associated with the corresponding parameter in the parameter ﬁle are used if there exists no such parameter charge increments from an analogous parameter are applied using the same algorithm for ﬁnding the best analogy as described above except that the nonbonded matrix is used for all atoms as the algorithm for assigning parameters by analogy has the ability to match an existing parameter in which the atom types are in reverse order care was taken to reverse the order and sign of the charge increments whenever such reversal occurs it should be noted that the present algorithm only assigns correct symmetric charges to delocalized cases like amidinium guanidinium and carboxylate when given symmetric formal charges therefore it was necessary to include formal charge assignment in the atom typing rules as discussed in the preceding manuscript part i finally when applying this charge assignment algorithm to a test set of molecules that were not part of the cgenff parametrization it was found that charge increments associated with high penalty dihedrals lead to a degradation in the quality of the predicted charges indeed it is a known problem in force ﬁeld development dx doi org j chem inf model 52 journal of chemical information and modeling article select dihedrals were constrained a priori to see table supporting information indeed as this is the ﬁnal optimization there are no higher order charge increments that can be used to compensate for the nonperfect ﬁt caused by zeroing charge increments a posteriori this left degrees of freedom to be ﬁt resulting in an rmsd of 0081 e after rounding the charge increments to the third digit after the decimal point and further rounding charge increments smaller than 0025 e that involve a hydrogen to zero the rmsd increased by e consequently the rmsd calculated using the ﬁnal charge increments was 0082 e all numbers presented in this paragraph are based on release 9 of the cgenff program force ﬁeld version as the charge increment ﬁtting is based on explicitly parametrized compounds for which all parameters are available the algorithmic improvements in newer versions discussed in this section do not inﬂuence the results the release of version of the force ﬁeld does but not to a signiﬁcant extent fitting considerations and results the number of degrees of freedom in the ﬁnal ﬁt is lower than the number of target data points so that the ﬁtting problem is formally not underdeﬁned however there are many redundancies i e chemical groups that occur in more than one molecule in the target data additionally bond charge increments in a ring are only deﬁned plus or minus a constant for example in a ring a b c adding a constant fractional charge simultaneously to the increments βab βbc and βca does not change the resulting charge distribution to counteract the resulting linear dependence in the ﬁtting problem all bond charge increments were subject to a restraining bias toward in other words the merit function of the least squares ﬁt is given by that transferability of parameters rapidly degrades with increasing number of atoms in the parameter deﬁnition and becomes problematic for dihedral parameters 32 accordingly this nontransferabilty is now shown to also apply to the associated charge increments this problem was solved by ignoring charge increments associated with dihedral parameters with a penalty score higher than an arbitrary limit which is currently set to charge increment optimization the presence of charge increments associated with angles and dihedrals has the advantage of allowing charge eﬀects to propagate over up to three bonds which opens the possibility to capture inductive and to a limited extent mesomeric or resonance eﬀects also the presence of charge increments explicitly associated with transferred dihedral parameters may improve compatibility between these dihedral parameters and the electrostatic interactions which always has been one of the main factors limiting dihedral parameter transferability in general force ﬁelds the disadvantage is that this leaves a very large number of charge increments to be optimized as target data for this optimization we used the charges on the full model compounds currently in cgenff since all partial charges in a molecule must sum up to the total charge of said molecule this corresponds to independent data points for the actual optimization three least squares ﬁts are performed first all bond charge increments for bonds between diﬀerent atom types are optimized the charge increment for a bond between two identical atom types is zero by deﬁnition in total degrees of freedom were ﬁt in this stage and the ﬁnal rms charge deviation was 0394 e the resulting bond charge increments are rounded to the third digit after the decimal point and a number of bond charge increments involving hydrogen atoms are set to charmm standard values in order to make the hydrogen charges consistent with the charmm charge assignment rules speciﬁcally aliphatic hydrogen atoms are always assigned a charge of 09 e except when they are located on a membered ring aliphatic carbon atom directly adjacent to a positively charged nitrogen atom where they are assigned a standard charge of e similarly hydrogen atoms bound to aromatic methylene and methine ch carbon atoms are given charges of 115 21 and e respectively see table supporting information it should be noted that these changes amount to further rounding of the aﬀected bond charge increments since a vast majority of the molecules in the target data comply with the charmm charge assignment rules the optimized bond charge increments were already at or very close to their ideal values before this adjustment as testiﬁed by the e increase in rmsd after adjustment next charge increments for angles are optimized as with the bonds charge increments on symmetric angles are constrained at zero the total number of degrees of freedom at this stage was and the ﬁnal rmsd was 0174 e again the charge increments are rounded to the third digit after the decimal point charge increments smaller than 0025 e that involve a hydrogen are further rounded to zero as well as the charge increments on a number of select angles see table supporting information in order to enforce the aforementioned charmm hydrogen charge assignment rules this raised the rmsd by e finally the optimization of the dihedral charge increments is performed again constraining symmetric dihedrals to zero additionally instead of setting a number of selected charge increments at a posteriori as was done for the angles seven atoms i qi qitarget k increments n where qi is calculated from the charge increments βn as described above and k is a factor determining the strength of the restraining bias which we set to this bias has a minimal eﬀect on properly determined charge increments but eﬀectively restrains in ring increments and other poorly determined degrees of freedom to their lowest possible value in this sense the procedure is somewhat analogous to bayly et al resp model 23 the restraining factor also eﬀectively prevents overﬁtting of the aforementioned poorly determined degrees of freedom and this preventive eﬀect is further ampliﬁed by the incremental ﬁtting and rounding scheme discussed in the subsection charge increment optimization above indeed of the degrees of freedom ﬁtted to absolute values of 002 e or lower which usually do not have a signiﬁcant eﬀect on the charges thus it can be argued that if the poorly determined degrees of freedom that were kept near zero by the restraining bias are not counted well deﬁned degrees of freedom were ﬁt to nonzero values using data points this still does not fully eliminate the risk of overﬁtting in the sense that even a well deﬁned degree of freedom may reﬂect nonrepresentative structures and charges in the target data this risk is hard to quantify using the current data set because it is chemically very and contains too little redundancy for proper cross validation thus the only way to identify and cure possible cases of well deﬁned overﬁtting is to increase the size of the target data set which is an ongoing eﬀort indeed as the charmm charge assignment methodology is laborious to perform a systematic validation study is a research project in its own right and is the subject of future work dx doi org 1021 j chem inf model 52 journal of chemical information and modeling article bond charge increments into account green squares unlike the hydrogen atoms this chain disappears when including higherorder increments because no constraint was applied to the charges on non hydrogen atoms this suggests that our charge assignment scheme would indeed be able to capture mesomeric charge eﬀects in aromatic rings if given an appropriate training set another horizontal line at y 197 consists of aromatic hydrogen atoms in indole like rings indeed the cgenff collection of model compound includes three diﬀerent charge sets for indole and a number of indole like heterocycles that were all optimized independently and have diﬀerent charges as a consequense of the common excess of degrees of freedom in the charge optimization for these kind of compounds the 197 charge as well as other charges on the indole ring system is a least squares compromise between the aforementioned model compounds as testiﬁed by the chain of data points being horizontally centered around the diagonal and by the acceptable reproduction of qm water interaction energies demonstrated in the dimethylcarbamoyl indole case study below just like for the regular atomatic rings there is a less clearly resolved cluster of outliers representing the corresponding carbon atoms at the negative side of the plot per charge penalties finally a method was needed to derive a per charge penalty σi from the per parameter penalties of the parameters that contributed charge increments to a select atom the ﬁrst attempt was to use the maximum of all these contributions this had the undesirable eﬀect of producing high penalties for atoms that receive a small charge contribution from a high penalty parameter see for instance the m penalty for and in table making it clear that the magnitude of the contributing charge increments should inﬂuence the percharge penalties next we considered using averages of the perparameter penalties σn weighted by the absolute values of the corresponing charge increments βn eq with p this approach was initially chosen in favor of sums see below because we wanted the magnitude and interpretation of the percharge penalties to be on the same scale as the per parameter penalties however it was observed that a large number of small penalties with large weights could strongly pull down the average this problem which would give rise to unintuitive results such as the two atoms in a highly polarized bond having very diﬀerent penalties compare for instance for and in table could be somewhat alleviated by using averages that are sensitive to high numbers such as the quadratic cubic quartic and eighthpower means i e power means with exponent p and 8 respectively see eq but doing so comes at the cost of a decreased sensitivity to the weights as can be rationalized by considering that in the limit for p the power mean becomes the maximum special treatment was required for conjugated double bond atom types as mentioned under the subsection additional applications of bond groups above there are two sets of atom types for conjugated double bonds which alternate in a chain of conjugated double bonds this implies that every parameter involving these conjugated double bonds has a counterpart in which the atom types are switched for the purpose of optimizing charge increments extra constraints were implemented so that the same increments are applied to both counterparts in order to prevent charge assignment from arbitrarily depending on the assignment of the double bonded atom types similarly charge assignments associated with parameters that remain identical except for reversal of the order of the atom types after swapping double bonded atom types were constrained to zero figure shows the agreement between the manually assigned and optimized partial charges in cgenff and their figure results of charge increment ﬁtting by highest term the green squares represent the bond charge increments only the red dots bonds and angles and the blue diamonds bonds angles and dihedrals the solid diagonal lines are deviations of e from identity counterparts that were determined using the ﬁtted charge increments when using only the bond charge increments green squares the data exhibits a relatively large spread especially considering that a e deviation solid diagonal lines in the partial charge of an atom that participates in a hydrogen bond typically results in a kcal mol deviation in the strength of that hydrogen bond the spread becomes gradually less as the angle and dihedral charge increments are included in line with the halving of the rmsd for every additional term as shown above our ﬁnal approach including bonds angles and dihedrals blue diamonds is in excellent agreement with the existing charges a few minor outliers are visible in figure in the form of horizontal chains of data points that cross the diagonal the chain at y 115 extending to the right of the diagonal is caused by the fact that the aromatic hydrogen charges on diﬂuorobenzene diﬂuorotoluene and benzylphosphonate were manually optimized and hence deviate from the standard charmm charges to which the charge increment optimization was constrained a similar chain representing the corresponding carbon atoms can be seen at y 115 extending to the left when only taking the σi m p σn p increments βn σnp increments n βn in the end no acceptable trade oﬀ between sensitivity to high penalties and sensitivity to the weights was found moreover the very concept of averaging is not consistent with the cumulative nature of penalties for a constant per parameter penalty applying a larger number of charge increments or a charge increment that is larger in absolute value should yield a larger per charge penalty in this sense the nearly identical penalties for n and in table are undesirable therefore we settled on using charge increment weighted sums of the penalties to retain the desired sensitivity to high penalties we the atom names are explained in figure the power means m and as deﬁned in eq are respectively equivalent to the maximum max arithmetic mean a and root mean square rms use the root of the sum of the squares thus eﬀectively treating the per parameter penalties as uncorrelated relative errors of the charge increments that are summed to obtain partial charges the resulting ﬁle contained 662 structures out of which 429 could be successfully assigned atom types the remaining compounds contained chemical groups that are not yet supported by the cgenff force ﬁeld at the time of writing most importantly membered rings atom typing and assignment of charges and parameter by analogy on these 429 structures took on a single core of a mhz amd athlon ii 255 processor which can be considered a below average desktop processor at the time of writing this is equivalent to a rate of 8 molecules per second demonstrating the suitability of the current method for high throughput applications weighting this sum by the charge increments yielded results that were deemed too sensitive to the weights compare for instance and in table while should have a higher penalty because it involves more high value high penalty increments the factor diﬀerence in the q column is too high to decrease this sensitivity we instead tried weighting by the square root q and the cube root q of the weight acceptable sensitivity was obtained with both options but the cube root had the additional advantage of bringing the ﬁnal sum in an acceptable range eliminating the need to apply an arbitrary scaling factor to compensate for the fact that the sum of the absolute values of the charge increments is much lower than in a vast majority of cases finally the absolute values of all charge increments were increased by δ 05 8 before being used as weight factors as can be seen in the ﬁnal expression for σi the per charge penalty on atom i this is equivalent to assigning a weight factor to the per parameter penalties σn associated with increments βn eq doing so is important in the presence of dihedrals with a high penalty for which the charge increment is set to and the penalty to the maximum allowable value for dihedrals currently as mentioned earlier although these increments do not contribute to the partial charge they should have some eﬀect on the penalty to reﬂect the fact that a contribution to the partial charge was ignored without the bond group feature were given inappropriately small tps values manual parameter optimization based on geometric measurements of the qm and cgenff minimized conformations readily allowed the and angles to be brought within of the qm values last columns in table further properties reported in this paragraph include this correction so that the angular deviation does not interfere with the evaluation of the dihedrals as for the interaction energies with water which were calculated using the geometry table and are signiﬁcantly outside the kcal mol diﬀerence criterion for cgenff and the interaction distance for is longer than expected in line with its interaction energy being too weak this can be explained by the fact that the current training set for the charge increments is based on the assumption that substituted aromatic systems have the same charges as unsubstituted ones thus negating the algorithm ability to represent mesomeric eﬀects however while allowing mesomeric substituent eﬀects to inﬂuence the charges in the aromatic rings in the training set may improve it will not necessarily improve because the current implementation can only capture these electronic eﬀects up to a distance of three bonds fortunately the hydrogen bond donated by is one of the weaker interactions in the system and most relevant intermolecular properties are dominated by the stronger hydrogen bonds in this sense it is comforting that the hydrogen bond accepted by is reproduced signiﬁcantly better indeed its interaction energy is only slightly outside of the cgenff target range which is an encouraging result given that is involved in the formation of a novel bond as indicated by its high penalty finally although penalty is in line with its deviation from the qm target data the example of illustrates that even a penalty of does not guarantee perfect agreement as discussed above the penalty is a measure of the degree of analogy between chemical groups in the training set and the user molecule and neither factors in the quality of the training set nor more importantly the fundamental appropriateness of applying charges and parameters by analogy for the given molecule the directions of the hf and dipole moments diﬀer by a relatively large the cgenff dipole moment is in better agreement with the hf dipole moment table which is a commonly observed consequence of the fact that the charges are primarily ﬁtted to reproduce hf interaction energies however the agreement with both the hf and dipole moment is acceptable given the fact that the magnitude of the cgenff dipole moment should be overestimated by 20 the potential energy scan pes around the rotatable bond is shown in figure the local minimum at is kcal mol too low and shifted by 15 in cgenff while the barriers at and are 84 kcal mol too high and 61 kcal mol too low respectively while the agreement is not as good as in the cgenff results following parameter optimization these pes are expected to yield qualitatively similar results in md simulations which is very good considering that the penalties of the four dihedrals terms around this bond are 87 88 and and our guidelines mandate extensive validation and or reoptimization for any parameters with penalties higher than case study n benzylacetamide a second case study is n benzylacetamide compound in figure table of the supporting information in this case one angle with penalty was assigned by analogy as well as three dihedrals with penalties and the highest penalty of the charges is 4 a comparison between cgenff charges and both minimized in vacuum measurements of relevant bond lengths and angles are compared in table all values are within acceptable limits 03 å for the bonds and 3 for the angles from their qm values except the bond which has a correspondingly high penalty and the and c10 angles which exhibits serious deviations along with the c10 improper dihedral speciﬁcally without the bond group feature the parameter for this angle was transferred from an in ring parameter for 5 membered rings with a reference value near the ideal while the out of ring angles for these kind of rings should be near an ideal see assignment of parameters by analogy above and figure 2 as the sum of the three angles around is only 9 a catastrophic out of plane deviation of 2 arises in the c10 improper dihedral although the penalty score for these angles is 5 and basic validation is recommended for penalties higher than it would be more ideal if a penalty score higher than would have been assigned which would mandate extensive validation optimization furthermore similar cases were identiﬁed in a signiﬁcant percentage of user supplied molecules some of which featured penalties lower than the frequency and magnitude of this problem taken together with the fact that it might potentially go undetected by a quick glance at the penalties prompted us to implement the bond group feature the inclusion of this extra contribution to the penalty function caused diﬀerent angle parameters to be chosen by the assignment algorithm which eliminated the out of plane deviation it should be noted that although the sum of the angles is now close to there is still a large in plane deviation this is due to an unexpectedly large physical deviation from the idealized geometry indeed the values for c10 and c3 c10 are far from such that the minimized geometry figure 4 is visibly diﬀerent from the idealized figure 4 2 dimethylcarbamoyl indole 2 minimized conformation geometry for a 5 membered ring figure 2 we speculate that this is caused by a combination of favorable electrostatic interactions between the and atoms and steric repulsion between the dimethylamide group and the ring system which cannot be relieved by tilting the amide group out of the plane of the ring because the system is kept planar by π orbital overlap eﬀects in addition to the electrostatic interaction mentioned above as the force ﬁeld lj parameters were optimized targeting bulk phase properties it appears that the steric repulsion seen in the vacuum qm calculation is not adequately reproduced yielding a signiﬁcantly diﬀerent angular geometry however the new penalties for these angles are and respectively providing the user a clear and unambiguous indication that their values should be veriﬁed and or optimized note that although the bond group feature increased the penalties the parameters by analogy became better as the inappropriate parameters obtained cgenff data were calculated using the geometry for 2 hf 31g d interaction energies are scaled by a factor 1 16 hf interaction distances are not scaled however empirical hydrogen bond minimum distances i e involving model compound hydrogen bond donors and acceptors should be approximately 2 å shorter than hf 31g d values as required to yield adequate bulk phase properties the meaning of the symbols describing the interaction geometries as well as the scaling factor and oﬀset are explained in ref 2 results include average deviation ad root mean square deviation rmsd and absolute average deviation aad the values in the column penalty are the penalties of the atoms that directly interact with the water probe while the penalty parent values are associated with the atom to which the latter is bound the parameters by analogy for produce good agreement with the qm from to but the pess diverge by 2 kcal mol from to on the basis of the geometry at figure we speculate that this is because the intramolecular electrostatic interaction of the amide nh group with the benzene ring is substantially diﬀerent from its interaction with an ester or protonated carboxylic acid in a cterminal amino acid which is the origin of the parameter by analogy for this dihedral manual ﬁtting of this dihedral leads to the introduction of a dihedral term with n 1 kϕ 1 and δ and a second dihedral term with n 3 kϕ 1 7 and δ the former term s relatively high kϕ might indicate that the intramolecular nonbonded interactions are not perfectly balanced for the purpose of reproducing this vacuum pes and figure 5 potential energy scan around the c2 c10 bond in 2 dimethylcarbamoyl indole 2 standard charmm charges is given in table 7 this table shows a fair correlation between the deviation from the standard charmm charges and the penalty the three highest penalties correspond to the three charges that have signiﬁcant though less than 05 e deviations given the lack of high penalty bond and angle parameters it is not surprising that all minimized bond lengths and angles are in excellent agreement see table of the supporting information figure depicts the relaxed potential dx doi org 10 1021 j chem inf model 52 3168 journal of chemical information and modeling article figure potential energy scans around the and bonds in n benzylacetamide 3 parameter the aforementioned sum is returned to the user as a measure for the accuracy of the approximation charges are assigned using a variation on the classical bond charge increment scheme which essentially associates a bond charge increment with each bond parameter we extended this scheme by associating two charge increments with each angle parameter and three charge increments with each dihedral parameter to apply this bond charge increment scheme to a novel molecule the aforementioned algorithm for assigning parameters by analogy is used albeit using slightly diﬀerent criteria for analogy this allows for per charge penalty scores to be returned which give the user an idea of the quality of the charges charge increment values are optimized to reproduce the charges on model compounds that were part of the parametrization of the force ﬁeld as well as possible using a restrained least squares ﬁtting algorithm a ﬁnal charge rmsd of 0 0082 e was obtained the validity of the ﬁnal parameters and charges and the relevance of the associated penalty scores were illustrated by two case studies a more thorough validation study will be the subject of a future paper as the methods discussed in the preceding and present paper are entirely empirical they are excellently suited for high throughput applications as testiﬁed by the fact that we processed a representative database of 18 429 drug like structures at a rate of 8 compounds per second on a single core of a below average desktop processor finally it should be reiterated that the penalties are indicative of the analogy of the generated parameters or charges with available terms in cgenff rather than the intrinsic ﬁtness of these terms for application in the molecule of interest consequently large penalties may yield satisfactory accuracy while signiﬁcant deviations from acceptable accuracy may occur when small or zero penalties are reported indeed given the sensitivity of chemical properties to environment in cases where parameters are available in cgenff for a new molecule there may still be signiﬁcant limitations in the accuracy of the resulting model it is important that the user is aware of such limitations and perform the appropriate validation and optimization as required use of the cgenff program for automatic atom typing and assignment of parameters and charges by analogy is provided at https www paramchem org figure 7 n benzylacetamide 3 conformation for ϕ 90 and ϕ c9 we speculate that the ﬁtted parameters might be poorly transferable to other molecules this is in line with the observation that the transferability of dihedral parameters in general is often problematic see section charge assignment above although the penalty of 28 5 associated with this parameter mandates validation it is possible in some cases to obtain a similarly mediocre agreement for parameters with even lower penalties therefore if accuracy is of the utmost importance in the molecule s under study validation is recommended regardless of the penalties concerning the scan the data points with poor agreement between cgenff and are caused by the c7 c9 dihedral which is relaxed at every scan point scan being close to indeed the value for the c7 parameter did not change signiﬁcantly during manual optimization and the improvement in agreement is purely due to the optimization of the c7 n8 c9 dihedral this demonstrates that a higher penalty is not a guarantee for poor agreement rather the penalty scores should be thought of as measure for the approximation error on the parameter molecular mechanics force ﬁelds are widely used in computer aided drug design for the study of drug candidates interacting with biological systems in these simulations the biological part is typically represented by a specialized biomolecular force ﬁeld while the drug is represented by a matching general organic force ﬁeld in order to apply these general force ﬁelds to an arbitrary druglike molecule functionality for assignment of atom types parameters and partial atomic charges is required in the present article algorithms for the assignment of parameters and charges for the charmm general force field cgenff are presented these algorithms rely on the existing parameters and charges that were determined as part of the parametrization of the force ﬁeld bonded parameters are assigned based on the similarity between the atom types that deﬁne said parameters while charges are determined using an extended bond charge increment scheme charge increments were optimized to reproduce the charges on model compounds that were part of the parametrization of the force ﬁeld a penalty score is returned for every bonded parameter and charge allowing the user to quickly and conveniently assess the quality of the force ﬁeld representation of diﬀerent parts of the compound of interest case studies are presented to clarify the functioning of the algorithms and the signiﬁcance of their output data introduction intramolecular internal bonded terms part i of this described the charmm general force field cgenff atom typer which deterministically assigns bonds atom types to an arbitrary organic molecule using a program kb b k θ θ angles kϕ cos nϕ δ mable decision tree in the present paper a set of algorithms is presented to assign bonded parameters and charges to such a improper dihedrals dihedrals kφ φ kub urey bradley molecule as discussed in part i an essential component of a intermolecular external nonbonded terms molecular mechanics force ﬁeld is the parameter set including the bonded parameters associated with combinations of atom of class i additive force ﬁelds eq such as charmm for as the number of atom types for which bonded parameter are required increases it becomes increasingly unrealistic to optimize parameters for every possible combination of atom types while cgenff has a good coverage of individual chemical groups which continues to grow organic molecules consist of virtually limitless combinations of chemical groups consequently a typical drug like compound will contain combinations of atom types that do not match existing bonded parameters in the force ﬁeld in the context of the program charmm these every covalently bound pair of atom types a reference distance and a force constant are deﬁned and kb in eq similarly valence angle parameters kθ and are deﬁned for every covalently bonded combination of three atom types and dihedral parameters kϕ n and δ for every covalently bonded combination of four atom types also where necessary urey bradley terms kub and and improper dihedral parameters kφ and are deﬁned for select combinations of three and four atom types received august published november respectively american chemical society dx doi org j chem inf model journal of chemical information and modeling article cases are called missing parameters because bonded parameters are required for every combination of atom types in the molecule as for the nonbonded parameters combining are commonly used to derive the lennard jones radius rij and well depth εij see eq for each i j atom pair in a chemical system from the per atom quantities ri rj εi and εj which in turn are associated with individual atom types therefore the atom typing functionality described in part i automatically takes care of the lennard jones parameters this leaves the charges qi to be assigned in addition to the bonded parameters the problem of assigning bonded parameters and charges to an organic molecule is far from new and a wide variety of approaches have been applied in organic force ﬁelds such as the tripos force ﬁeld cvff uff allinger mm3 and force ﬁelds and momany and rone commercial charmm force not to be confused with the academic charmm force ﬁeld note the diﬀerent capitalization of particular note are the general amber force ﬁeld gaff and its associated antechamber toolkit which marked the ﬁrst extension of a specialized biomolecular force ﬁeld i e amber to organic molecules and which provided inspiration for the present charge assignment scheme accordingly the present charge assignment scheme is empirical as are most of the aforementioned charge assignment approaches notable exceptions are amber and opls where charge assignment schemes based on a quantum mechanics qm electron density typically or have been proposed for small organic molecules while the resulting charges capture polarization and quantum eﬀects and therefore are typically signiﬁcantly more accurate than charges generated by more empirical assignment schemes they are also computationally signiﬁcantly more expensive which may become a bottleneck in high throughput applications involving millions of molecules substituting a very specialized atom type with a more generic variant typically has a lower penalty score than vice versa because it carries a lower risk of catastrophically inconsistent behavior before discussing the construction of the penalty matrix we give a brief overview of the computational methods for assigning parameters by analogy which provides a foundation for assignment of the penalty scores total penalty score tps calculation the total penalty score between two parameters ie a missing parameter and an existing parameter can be calculated as the sum of the penalties for substituting the atom types deﬁning the one parameter with the atom types in the second parameter thus for every missing parameter the program for assignment of parameters by analogy is tasked with ﬁnding the existing parameter in the parameter ﬁle or parameter database with the lowest tps with respect to the missing parameter this is accomplished by simply iterating through the parameter database calculating the tps for every parameter and returning the parameter with the lowest penalty this process is sped up by the fact that both the penalty matrix and the data structure containing the parameters are collated by atom type reducing the number and computational expense of lookup operations in the penalty matrix the speed gain achieved by collating the parameters is a result of the fact that a parameter in a majority of cases will only diﬀer from the previous parameter in the collated list by its last atom type thus in the example of an angle parameter deﬁned by atom types a b c the algorithm can be made to keep track of the penalty of the ﬁrst atom type a and the sum of the penalties of the ﬁrst two atom types a b if it is determined that only the last atom type is diﬀerent c only this atom type needs to be looked up in the penalty matrix and its penalty can be added to the latter sum similarly if the two last atom types are diﬀerent b c only those two atom types must be looked up and their sum added to the previously determined penalty for a thus substantially improving performance finally for every existing parameter the lowest of two tpss should be used the tps between the ordered missing parameter a b c and the existing parameter and the same tps for the physically equivalent reverse ordered missing parameter c b a as the former is not guaranteed to be always lower than the latter all resulting parameters are stored in a binary search tree which makes it easy to determine whether a missing parameter has been encountered and assigned before penalty matrix deﬁnition rule file the penalty scores for atom substitutions ideally should reﬂect the chemical characteristics on which the deﬁnitions of the atom types were based however as the number of atom types in cgenff is approximately and is likely to increase with the parametrization of new chemical groups directly populating the penalty matrix manually is not feasible or would at the very least be extremely error prone instead the full matrix is constructed from a smaller number of submatrices and oﬀsets which are deﬁned in a second section of the rule ﬁle used to assign atom types described in the preceding article this parameter assignment section has a formally similar layout as the atom typing rules although it is processed quite diﬀerently atom types are divided in a tree like structure with categories and subcategories delineated by a cat keyword deﬁning the name of the category and an end keyword every entry in a category can either be an atom type typ or a reference to a subcategory sub the name of which is followed by a colon the diﬀerence with the atom typing rules lies in the keywords after the colon the ﬁrst of which is pri short for priority followed by the approach and algorithms to obtain missing parameters in cgenff existing parameters that were determined as part of the parametrization of cgenff are identiﬁed by analogy based on the similarity between the atom types that deﬁne the parameters charges are assigned using a bond charge increment scheme that is conceptually similar to that implemented in the charge increments which function as parameters in the charge assignment algorithm are optimized to reproduce the charges on a training set of model compounds that were part of the parametrization of the force ﬁeld a noteworthy innovation in the presented charge assignment scheme is the fact that apart from the single charge increment associated with each possible bond as deﬁned by two atom types there are two charge increments associated with each possible angle and three charge increments with each possible dihedral angle as deﬁned by three and four atom types respectively the combined functionality in the cgenff atom and parameter assignment scheme makes it possible to apply the force ﬁeld to an arbitrary molecule in an automated fashion notably as a measure of the accuracy of the approximation a penalty score is returned to the user for every bonded parameter and charge which can be used to guide selective optimization of parameters and charges assignment of parameters by analogy the assignment of missing parameters is based on penalty scores quantifying the dissimilarity between atom types the penalty scores are based on an n n penalty matrix where n is the number of atom types it should be noted that this matrix is not necessarily symmetric dx doi org j chem inf model journal of chemical information and modeling article table extract of the parameter assignment rule file that handles nitrogen atom types atom types and the penalty scores in this category were machinegenerated based on reference bond lengths in the cgenff parameter ﬁle the second largest and largest manually generated category contains oxygen atom types most of the penalties in the rule ﬁle are chosen manually it is a relative rare occurrence that an objective measure for similarity could be found that performed satisfactory for the purpose of assigning parameters by analogy during the process of programming the penalty matrix certain constraints were respected for instance values at a certain level in the tree are all in the same order of magnitude this is illustrated in table the category has an up penalty of for both sub rules while both of its subcategories have up penalties of for all their atom types and penalties within each category are lower than this up value also the penalty for substituting atom a for atom b was usually kept similar to the penalty for substituting atom b for atom a however these rules were not always followed rigidly for example in cases where a very speciﬁc type is changed into a type that is clearly more general the penalty was deliberately set lower than for the reverse substitution ie changing a general type into a very speciﬁc one as mentioned above in the end what mattered more than any constraint in the construction of the penalty matrix was its ability to assign sensible parameters by analogy and a large part of the eﬀort was focused on validating this ability this validation also gave rise to the below recommendations to validate and reoptimize parameters with tps values above certain thresholds most of this work was based on chemical intuition as it is not straightforward to do this in a systematic or quantitative way indeed cgenff internal parameters are validated against target data which is relatively expensive to compute and it is not trivial to devise a measure to objectively compare their performance when applied on diﬀerent molecules the present paper contains two case studies a more systematic validation study is the subject of future work the whole rule tree is read into a temporary data structure and checked for consistency by a tree walking algorithm that also replaces all atom type and rule name strings with pointers then a diﬀerent tree walking algorithm is started at the ﬁrst typ rule and exhaustively visits all other rules in the tree in a similar fashion as outlined in the above example ﬁlling in one line of the penalty matrix the aforementioned pointers help keep the penalty oﬀset added to the atom type substitution total penalty score when entering the subcategory through a sub keyword in a hierarchically higher category this will be clariﬁed by an example in the next paragraph this is followed by one or more alt keywords detailing alternative atom types or subcategories within the same category the number of alt entries is equal to the number of rules in the current category minus one every alt keyword is followed by the name of another typ or sub rule in the same category and the penalty score for substituting the current type with the named alternative type finally the rule ends with an up keyword followed by the penalty oﬀset for going one level up in the hierarchy as an example table contains a short extract of the rule ﬁle that handles nitrogen atom types in the category two subcategories are deﬁned and for neutral and positively charged nitrogen atom types respectively consider for example the protonated primary ammonium type substituting this atom type with secondary ammonium tertiary ammonium and quaternary ammonium types respectively incurs penalties of and if these substitutions fail the penalty for going up in the hierarchy is doing so brings us to the sub rule in the category which speciﬁes that substituting an atom in the subcategory with an atom in the subcategory carries a penalty of the penalties are added so the total penalty score so far is after entering the category the rule with the lowest pri penalty is picked which by convention is the ﬁrst rule in the category in this case with a penalty of thus the ﬁnal penalty score for substituting with is similarly the penalty score for substituting with is and so forth this scheme allows ﬁlling the full penalty matrix using a rule ﬁle of manageable size it is left to the writer of the rule ﬁle to make a trade oﬀ between specifying more speciﬁc and thus potentially more accurate values by making fewer larger categories on the one hand and having a smaller rule ﬁle and thus less potential for programming error by creating more smaller categories on the other hand however as the number of penalties in a category goes with the square of the number of rules in that category making large categories quickly becomes unwieldy thus the largest category in the parameter section of the current version of the cgenff rule ﬁle contains hydrogen dx doi org j chem inf model 3168 journal of chemical information and modeling article figure chemical structures of compounds discussed in this article including atom naming negatively charged proline used as an example for illustrating the diﬀerent ways of calculating per charge penalties see table dimethylcarbamoyl indole case study n benzylacetamide case study computational cost of this process well within acceptable limits the same procedure is repeated for all typ rules to complete the penalty matrix the rows and columns of the penalty matrix are then sorted by atom type diﬀerential treatment of inner and outer atoms to improve the algorithm capability to pick good analogies two extra features were added to the basic algorithm described in the preceding paragraphs the penalties for substituting the central atom in an angle or improper dihedral and the two inner atoms in a dihedral are multiplied by because changing these inner atoms has a much higher impact than changing the outer atoms the same multiplication factor is applied to the penalties for substituting atoms in bond parameters because the impact of changing atoms in a bond parameter is typically comparable to changing the inner atoms in an angle or dihedral the penalty matrix used for these inner atoms including the atoms in a bond parameter will henceforward be called the bonded matrix in contrast to the nonbonded matrix that is used for the two outer atoms in angle and dihedral parameters speciﬁcally the bonded matrix focuses more on bonded properties valence and hybridization state while the nonbonded matrix is more aimed at describing discrepancies in nonbonded properties electrostatic and van der waals for example in the deﬁnition of the bonded matrix in the rule ﬁle the highest category in the hierarchy diﬀerentiates subcategories by hybridization state which are subsequently divided by period i e row in the periodic table while in the nonbonded matrix the period has precedence over the hybridization state the use of these two diﬀerent matrices can be rationalized by considering that the potential associated with a dihedral deﬁned by atoms is typically the result of a combination of nonbonded interactions on the one hand and the bond order of the bond and the hybridization states of atoms and which are bonded properties on the other hand a similar observation can be made about angle potentials however this reasoning does not apply to improper dihedrals where better results were obtained by using the bonded matrix for all four atoms although the aforementioned scaling factor only applies to the central atom problem strained rings the algorithm as described above was made available for public beta testing as part of versions and of the cgenff program during subsequent testing an important shortcoming was identiﬁed see the dimethylcarbamoyl indole compound in figure case study below consider a planar membered ring as depicted in figure geometry dictates that the sum of the ring inner angles figure generic planar membered ring α must be and that the sum of the angles α β β around each of the trigonal planar centers must be indeed in practice pyrrole and other membered aromatic heterocycles have inner angles α that are relatively close to the ideal and corresponding outer angles β close to in certain cases where the parameter assignment algorithm is given a substituted membered ring in which one of the outer angles corresponds to a missing parameter such as the angle in compound it will use an in ring parameter as a source of analogy or vice versa which leads to an error of in the reference angle and an unacceptable deformation in the geometry indeed in compound is an carbon which exhibits signiﬁcant similarity to an carbon in a membered ring from a nonbonded perspective therefore the parameter assignment algorithm used the angle parameter deﬁned by atom types as a source of analogy for and for the parameter on the angle giving rise to the deviation in table and to a pronounced deviation from planarity φ although the penalty score for this particular case was which indicates that validation is required similar cases were found with penalties lower than the recommended validation threshold as planar membered rings are ubiquitous in drug like molecules and this phenomenon can be shown to be even more pronounced in and rings it seriously jeopardized the applicability of the parameter assignment algorithm we ﬁrst attempted to remedy this shortcoming by increasing the penalties for substitutions between atoms types for rings with diﬀerent sizes and noncyclic atom types but it was found that a very large increase in the penalties was required in order to eliminate the problem and that this large increase considerably aﬀected the quality of other parameters by analogy for ring substitutions for example before increasing the penalties parameters with a membered ring atom type for were used as a source of analogy for the and angles and were given a justiﬁably low penalty penalties that are suﬃciently high to cure the problem discussed above would make this impossible table bond groups in version of the parameter assignment rule file solution bond groups as changing the penalty scores did not lead to an acceptable solution an extra bond group term was introduced in the penalty function starting from version of the cgenff program table shows the deﬁnitions of these bond groups as they occur in the rule ﬁle here a bond group consists of the keyword bgrp followed by a penalty and a list of one or more atom types for the purpose of assigning bond group penalties bond angle and dihedral parameters are considered to contain one two and three virtual covalent bonds respectively for example the angle parameter discussed above contains the two virtual bonds and for each virtual covalent bond in a missing parameter the bond is member of a given bond group if both atom types are members of that bond group this way a virtual bond can be member of zero one two or more bond groups at the same time in the above example the virtual bond is member of both the bond group for planar membered rings and the bond group for all membered rings in table because both atom types are members of both bond groups conversely the virtual bond cg2r51 is not member of any bond groups because does not occur in any of these groups when searching the parameter database for candidate parameters with optimal analogy to this missing parameter bond group membership is compared pairwise between the virtual bonds in the missing and the candidate parameter if the virtual bond under consideration in the missing parameter is a member of a bond group and the corresponding virtual bond in the candidate parameter is not a member of the same group or vice versa then that bond group penalty is added to the tps such an approach severely penalizes for example the substitution of an endocyclic parameter for an exocylic term and vice versa as the bond group for a ring of a certain size includes all the endocyclic atom types that can occur in a ring of this size and no other exocyclic atom types two extra features were added to the basic bond group functionality described in this paragraph similar to the above the bond group penalties for substituting the central bond in a dihedral parameter or any bond in an angle or bond parameter are multiplied by because changing these bonds has a much higher impact than changing the outer bonds in a dihedral or the bonds in an improper the two ﬁrst bond groups in the rule ﬁle are considered equivalent for the purpose of determining diﬀerences in bond group membership between two virtual bonds this exception makes it possible to use the bond group feature to improve treatment of conjugated double bonds as discussed below dx doi org j chem inf model 3168 journal of chemical information and modeling article may require optimization this speaks of the limited transferability of empirical force ﬁeld parameters and the need for the user to make decisions on the level of accuracy they need for system under study conversely cases have been encountered with large penalties yet satisfactory accuracy which can be understood by interpreting the penalty as a measure of the statistical imprecision or spread of the parameter assignment charge assignment extended bond charge increment scheme charges are assigned using a bond charge increment scheme from an atom centric point of view this can be formalized as qi jβij where qi is the ﬁnal partial charge on atom i is the previously assigned formal and βij is the bond charge increment for the bond between atoms i and j with βji βij from a bond centric point of view this translates to the following pseudocode it can now be seen how the bond group feature remedies the shortcoming in the treatment of strained rings as mentioned above the cg2r51 cg2r51 virtual bond is member of both ring bond groups in table while the corresponding cg2r51 virtual bond in the parameter originally used as a source of analogy is member of neither adding a bond group contribution of to the tps and thereby virtually making sure that a more appropriate parameter will be chosen as a source of analogy at the same time the usage of a ring atom type for as a source of analogy for the and angles does not incur any additional penalty because none of the virtual bonds involved are members of any bond group additional applications of bond groups in addition to ﬁxing the original problem with strained rings the bond group feature allows for a better treatment of biphenyls and conjugated double bonds speciﬁcally as described in the preceding manuscript part i there are two sets of atom types for conjugated double bonds cg251o and cg252o which are applied such that in a chain of conjugated double bonds double bonded atoms are given atom types from the same set and single bonded atoms get atom types from a diﬀerent set initially in versions and it was necessary to apply very high penalties to substitutions involving these atom types in order to prevent single bond parameters being assigned to double bonds and vice versa as a side eﬀect of these high penalties favorable substitutions involving conjugated double bond atom types were also suppressed for example a missing parameter containing the virtual nonconjugated double bond could never be substituted with its conjugated equivalents and because penalties low enough to allow this substitution would make it equally likely to substitute the original virtual double bond with the virtual single bond cg2dc2 the introduction of the ﬁrst two bond groups in table which as mentioned above are considered equivalent for the purpose of determining diﬀerences in bond group membership eﬀectively gives the parameter assignment algorithm explicit knowledge of the bond orders consequently the penalties for substituting conjugated double bond atom types could be set very low instead relying on the bond groups to penalize incorrect substitutions analogous observations were made for biphenyls advantages and limitations finally it should be noted that compared to the wildcards used in a number of force ﬁelds the current scheme oﬀers a more exhaustive eﬀort to identify the best parameter to be used when an exact match for a parameter is not present it also oﬀers the ability to generate penalty scores that may alert the user to possible limitations in the assigned parameters nevertheless it should be emphasized that these penalty scores are based purely on the level of chemical similarity of the atom types in a parameter consequently even in cases where there is a penalty of zero i e when the exact parameter is already available in cgenff the quality of that parameter for determining the physical properties of the molecule may be poor for example all the parameters for a given type of linker between two rings might be available in cgenff however if the nature of the rings changes signiﬁcantly without a corresponding change in the atom types across the ring e g the c h carbon atoms in both pyridone and the pyridinium ion carry the type while the former is neutral and the latter is charged then the conformational properties of that linker are likely to change such that the associated dihedral parameters with penalties of zero we extended this scheme to angle charge increments α and dihedral charge increments δ this is implemented by associating one charge increment βij with every bond in the parameter ﬁle two charge increments αij and αjk with every angle and three charge increments δij δjk and δjk with every dihedral for every bond angle or dihedral in the molecules the charge increments associated with the corresponding parameter in the parameter ﬁle are used if there exists no such parameter charge increments from an analogous parameter are applied using the same algorithm for ﬁnding the best analogy as described above except that the nonbonded matrix is used for all atoms as the algorithm for assigning parameters by analogy has the ability to match an existing parameter in which the atom types are in reverse order care was taken to reverse the order and sign of the charge increments whenever such reversal occurs it should be noted that the present algorithm only assigns correct symmetric charges to delocalized cases like amidinium guanidinium and carboxylate when given symmetric formal charges therefore it was necessary to include formal charge assignment in the atom typing rules as discussed in the preceding manuscript part i finally when applying this charge assignment algorithm to a test set of molecules that were not part of the cgenff parametrization it was found that charge increments associated with high penalty dihedrals lead to a degradation in the quality of the predicted charges indeed it is a known problem in force ﬁeld development dx doi org j chem inf model 3168 journal of chemical information and modeling article select dihedrals were constrained a priori to see table supporting information indeed as this is the ﬁnal optimization there are no higher order charge increments that can be used to compensate for the nonperfect ﬁt caused by zeroing charge increments a posteriori this left degrees of freedom to be ﬁt resulting in an rmsd of e after rounding the charge increments to the third digit after the decimal point and further rounding charge increments smaller than e that involve a hydrogen to zero the rmsd increased by e consequently the rmsd calculated using the ﬁnal charge increments was e all numbers presented in this paragraph are based on release of the cgenff program force ﬁeld version as the charge increment ﬁtting is based on explicitly parametrized compounds for which all parameters are available the algorithmic improvements in newer versions discussed in this section do not inﬂuence the results the release of version of the force ﬁeld does but not to a signiﬁcant extent fitting considerations and results the number of degrees of freedom in the ﬁnal ﬁt is lower than the number of target data points so that the ﬁtting problem is formally not underdeﬁned however there are many redundancies i e chemical groups that occur in more than one molecule in the target data additionally bond charge increments in a ring are only deﬁned plus or minus a constant for example in a ring a b c adding a constant fractional charge simultaneously to the increments βab βbc and βca does not change the resulting charge distribution to counteract the resulting linear dependence in the ﬁtting problem all bond charge increments were subject to a restraining bias toward in other words the merit function of the least squares ﬁt is given by that transferability of parameters rapidly degrades with increasing number of atoms in the parameter deﬁnition and becomes problematic for dihedral parameters accordingly this nontransferabilty is now shown to also apply to the associated charge increments this problem was solved by ignoring charge increments associated with dihedral parameters with a penalty score higher than an arbitrary limit which is currently set to charge increment optimization the presence of charge increments associated with angles and dihedrals has the advantage of allowing charge eﬀects to propagate over up to three bonds which opens the possibility to capture inductive and to a limited extent mesomeric or resonance eﬀects also the presence of charge increments explicitly associated with transferred dihedral parameters may improve compatibility between these dihedral parameters and the electrostatic interactions which always has been one of the main factors limiting dihedral parameter transferability in general force ﬁelds the disadvantage is that this leaves a very large number of charge increments to be optimized as target data for this optimization we used the charges on the full model compounds currently in cgenff since all partial charges in a molecule must sum up to the total charge of said molecule this corresponds to independent data points for the actual optimization three least squares ﬁts are performed first all bond charge increments for bonds between diﬀerent atom types are optimized the charge increment for a bond between two identical atom types is zero by deﬁnition in total degrees of freedom were ﬁt in this stage and the ﬁnal rms charge deviation was e the resulting bond charge increments are rounded to the third digit after the decimal point and a number of bond charge increments involving hydrogen atoms are set to charmm standard values in order to make the hydrogen charges consistent with the charmm charge assignment rules speciﬁcally aliphatic hydrogen atoms are always assigned a charge of e except when they are located on a membered ring aliphatic carbon atom directly adjacent to a positively charged nitrogen atom where they are assigned a standard charge of e similarly hydrogen atoms bound to aromatic methylene and methine ch carbon atoms are given charges of and e respectively see table supporting information it should be noted that these changes amount to further rounding of the aﬀected bond charge increments since a vast majority of the molecules in the target data comply with the charmm charge assignment rules the optimized bond charge increments were already at or very close to their ideal values before this adjustment as testiﬁed by the e increase in rmsd after adjustment next charge increments for angles are optimized as with the bonds charge increments on symmetric angles are constrained at zero the total number of degrees of freedom at this stage was and the ﬁnal rmsd was e again the charge increments are rounded to the third digit after the decimal point charge increments smaller than e that involve a hydrogen are further rounded to zero as well as the charge increments on a number of select angles see table supporting information in order to enforce the aforementioned charmm hydrogen charge assignment rules this raised the rmsd by e finally the optimization of the dihedral charge increments is performed again constraining symmetric dihedrals to zero additionally instead of setting a number of selected charge increments at a posteriori as was done for the angles seven where qi is calculated from the charge increments βn as described above and k is a factor determining the strength of the restraining bias which we set to this bias has a minimal eﬀect on properly determined charge increments but eﬀectively restrains in ring increments and other poorly determined degrees of freedom to their lowest possible value in this sense the procedure is somewhat analogous to bayly et al resp model the restraining factor also eﬀectively prevents overﬁtting of the aforementioned poorly determined degrees of freedom and this preventive eﬀect is further ampliﬁed by the incremental ﬁtting and rounding scheme discussed in the subsection charge increment optimization above indeed of the degrees of freedom ﬁtted to absolute values of e or lower which usually do not have a signiﬁcant eﬀect on the charges thus it can be argued that if the poorly determined degrees of freedom that were kept near zero by the restraining bias are not counted well deﬁned degrees of freedom were ﬁt to nonzero values using data points this still does not fully eliminate the risk of overﬁtting in the sense that even a well deﬁned degree of freedom may reﬂect nonrepresentative structures and charges in the target data this risk is hard to quantify using the current data set because it is chemically very and contains too little redundancy for proper cross validation thus the only way to identify and cure possible cases of well deﬁned overﬁtting is to increase the size of the target data set which is an ongoing eﬀort indeed as the charmm charge assignment methodology is laborious to perform a systematic validation study is a research project in its own right and is the subject of future work dx doi org j chem inf model 3168 journal of chemical information and modeling article bond charge increments into account green squares unlike the hydrogen atoms this chain disappears when including higherorder increments because no constraint was applied to the charges on non hydrogen atoms this suggests that our charge assignment scheme would indeed be able to capture mesomeric charge eﬀects in aromatic rings if given an appropriate training set another horizontal line at y consists of aromatic hydrogen atoms in indole like rings indeed the cgenff collection of model compound includes three diﬀerent charge sets for indole and a number of indole like heterocycles that were all optimized independently and have diﬀerent charges as a consequense of the common excess of degrees of freedom in the charge optimization for these kind of compounds the charge as well as other charges on the indole ring system is a least squares compromise between the aforementioned model compounds as testiﬁed by the chain of data points being horizontally centered around the diagonal and by the acceptable reproduction of qm water interaction energies demonstrated in the dimethylcarbamoyl indole case study below just like for the regular atomatic rings there is a less clearly resolved cluster of outliers representing the corresponding carbon atoms at the negative side of the plot per charge penalties finally a method was needed to derive a per charge penalty σi from the per parameter penalties of the parameters that contributed charge increments to a select atom the ﬁrst attempt was to use the maximum of all these contributions this had the undesirable eﬀect of producing high penalties for atoms that receive a small charge contribution from a high penalty parameter see for instance the m penalty for and in table making it clear that the magnitude of the contributing charge increments should inﬂuence the percharge penalties next we considered using averages of the perparameter penalties σn weighted by the absolute values of the corresponing charge increments βn eq with p this approach was initially chosen in favor of sums see below because we wanted the magnitude and interpretation of the percharge penalties to be on the same scale as the per parameter penalties however it was observed that a large number of small penalties with large weights could strongly pull down the average this problem which would give rise to unintuitive results such as the two atoms in a highly polarized bond having very diﬀerent penalties compare for instance for and in table could be somewhat alleviated by using averages that are sensitive to high numbers such as the quadratic cubic quartic and eighthpower means i e power means with exponent p and respectively see eq but doing so comes at the cost of a decreased sensitivity to the weights as can be rationalized by considering that in the limit for p the power mean becomes the maximum special treatment was required for conjugated double bond atom types as mentioned under the subsection additional applications of bond groups above there are two sets of atom types for conjugated double bonds which alternate in a chain of conjugated double bonds this implies that every parameter involving these conjugated double bonds has a counterpart in which the atom types are switched for the purpose of optimizing charge increments extra constraints were implemented so that the same increments are applied to both counterparts in order to prevent charge assignment from arbitrarily depending on the assignment of the double bonded atom types similarly charge assignments associated with parameters that remain identical except for reversal of the order of the atom types after swapping double bonded atom types were constrained to zero figure shows the agreement between the manually assigned and optimized partial charges in cgenff and their figure results of charge increment ﬁtting by highest term the green squares represent the bond charge increments only the red dots bonds and angles and the blue diamonds bonds angles and dihedrals the solid diagonal lines are deviations of e from identity counterparts that were determined using the ﬁtted charge increments when using only the bond charge increments green squares the data exhibits a relatively large spread especially considering that a e deviation solid diagonal lines in the partial charge of an atom that participates in a hydrogen bond typically results in a kcal mol deviation in the strength of that hydrogen bond the spread becomes gradually less as the angle and dihedral charge increments are included in line with the halving of the rmsd for every additional term as shown above our ﬁnal approach including bonds angles and dihedrals blue diamonds is in excellent agreement with the existing charges a few minor outliers are visible in figure in the form of horizontal chains of data points that cross the diagonal the chain at y extending to the right of the diagonal is caused by the fact that the aromatic hydrogen charges on diﬂuorobenzene diﬂuorotoluene and benzylphosphonate were manually optimized and hence deviate from the standard charmm charges to which the charge increment optimization was constrained a similar chain representing the corresponding carbon atoms can be seen at y extending to the left when only taking the in the end no acceptable trade oﬀ between sensitivity to high penalties and sensitivity to the weights was found moreover the very concept of averaging is not consistent with the cumulative nature of penalties for a constant per parameter penalty applying a larger number of charge increments or a charge increment that is larger in absolute value should yield a larger per charge penalty in this sense the nearly identical penalties for n and in table are undesirable therefore we settled on using charge increment weighted sums of the penalties to retain the desired sensitivity to high penalties we the atom names are explained in figure the power means m and as deﬁned in eq are respectively equivalent to the maximum max arithmetic mean a and root mean square rms use the root of the sum of the squares thus eﬀectively treating the per parameter penalties as uncorrelated relative errors of the charge increments that are summed to obtain partial charges eq with q σi βn q σn increments n http zinc docking org catalogs maybhit the resulting ﬁle contained structures out of which could be successfully assigned atom types the remaining compounds contained chemical groups that are not yet supported by the cgenff force ﬁeld at the time of writing most importantly membered rings atom typing and assignment of charges and parameter by analogy on these structures took on a single core of a mhz amd athlon ii processor which can be considered a below average desktop processor at the time of writing this is equivalent to a rate of molecules per second demonstrating the suitability of the current method for high throughput applications weighting this sum by the charge increments yielded results that were deemed too sensitive to the weights compare for instance and in table while should have a higher penalty because it involves more high value high penalty increments the factor diﬀerence in the q column is too high to decrease this sensitivity we instead tried weighting by the square root q and the cube root q of the weight acceptable sensitivity was obtained with both options but the cube root had the additional advantage of bringing the ﬁnal sum in an acceptable range eliminating the need to apply an arbitrary scaling factor to compensate for the fact that the sum of the absolute values of the charge increments is much lower than in a vast majority of cases finally the absolute values of all charge increments were increased by δ before being used as weight factors as can be seen in the ﬁnal expression for σi the per charge penalty on atom i this is equivalent to assigning a weight factor to the per parameter penalties σn associated with increments βn eq doing so is important in the presence of dihedrals with a high penalty for which the charge increment is set to and the penalty to the maximum allowable value for dihedrals currently as mentioned earlier although these increments do not contribute to the partial charge they should have some eﬀect on the penalty to reﬂect the fact that a contribution to the partial charge was ignored σi increments n βn δ σn case studies case study dimethylcarbamoyl indole as a case study we ﬁrst consider dimethylcarbamoyl indole compound in figure the presented algorithm assigns the following bonded parameters by analogy see tables and in the supporting information for the original charmm toppar stream ﬁles generated without and with the bond group feature respectively bond penalty both with and without the bond group feature discussed above under the heading assignment of parameters by analogy the assignment without and with the bond group feature was respectively performed with version and of the cgenff program angles maximum penalty without bond groups or with bond groups dihedrals maximum penalty without bond groups or with bond groups improper dihedral penalty all these parameters contain the bond which is deﬁned by a combination of atom types that was not part of the cgenff parametrization the highest penalty of the charges is without bond groups or with bond groups the parameters and charges were validated using the original cgenff parametrization procedure the details of which are described elsewhere for the bonds and angles the conformation was compared with the cgenff conformation benchmarking as a simple benchmark for the performance of the methods described in the preceding and present paper we downloaded a ready to dock version reference ph of the maybridge hitfinder collection from the zinc dx doi org j chem inf model 3168 journal of chemical information and modeling article without the bond group feature were given inappropriately small tps values manual parameter optimization based on geometric measurements of the qm and cgenff minimized conformations readily allowed the and angles to be brought within of the qm values last columns in table further properties reported in this paragraph include this correction so that the angular deviation does not interfere with the evaluation of the dihedrals as for the interaction energies with water which were calculated using the geometry table and are signiﬁcantly outside the kcal mol diﬀerence criterion for cgenff and the interaction distance for is longer than expected in line with its interaction energy being too weak this can be explained by the fact that the current training set for the charge increments is based on the assumption that substituted aromatic systems have the same charges as unsubstituted ones thus negating the algorithm ability to represent mesomeric eﬀects however while allowing mesomeric substituent eﬀects to inﬂuence the charges in the aromatic rings in the training set may improve it will not necessarily improve because the current implementation can only capture these electronic eﬀects up to a distance of three bonds fortunately the hydrogen bond donated by is one of the weaker interactions in the system and most relevant intermolecular properties are dominated by the stronger hydrogen bonds in this sense it is comforting that the hydrogen bond accepted by is reproduced signiﬁcantly better indeed its interaction energy is only slightly outside of the cgenff target range which is an encouraging result given that is involved in the formation of a novel bond as indicated by its high penalty finally although penalty is in line with its deviation from the qm target data the example of illustrates that even a penalty of does not guarantee perfect agreement as discussed above the penalty is a measure of the degree of analogy between chemical groups in the training set and the user molecule and neither factors in the quality of the training set nor more importantly the fundamental appropriateness of applying charges and parameters by analogy for the given molecule the directions of the hf and dipole moments diﬀer by a relatively large the cgenff dipole moment is in better agreement with the hf dipole moment table which is a commonly observed consequence of the fact that the charges are primarily ﬁtted to reproduce hf interaction energies however the agreement with both the hf and dipole moment is acceptable given the fact that the magnitude of the cgenff dipole moment should be overestimated by the potential energy scan pes around the rotatable bond is shown in figure the local minimum at is kcal mol too low and shifted by in cgenff while the barriers at and are kcal mol too high and kcal mol too low respectively while the agreement is not as good as in the cgenff results following parameter optimization these pes are expected to yield qualitatively similar results in md simulations which is very good considering that the penalties of the four dihedrals terms around this bond are and and our guidelines mandate extensive validation and or reoptimization for any parameters with penalties higher than case study n benzylacetamide a second case study is n benzylacetamide compound in figure table of the supporting information in this case one angle with penalty was assigned by analogy as well as three dihedrals with penalties and the highest penalty of the charges is a comparison between cgenff charges and both minimized in vacuum measurements of relevant bond lengths and angles are compared in table all values are within acceptable limits å for the bonds and for the angles from their qm values except the bond which has a correspondingly high penalty and the and c10 angles which exhibits serious deviations along with the c10 improper dihedral speciﬁcally without the bond group feature the parameter for this angle was transferred from an in ring parameter for membered rings with a reference value near the ideal while the out of ring angles for these kind of rings should be near an ideal see assignment of parameters by analogy above and figure as the sum of the three angles around is only a catastrophic out of plane deviation of arises in the c10 improper dihedral although the penalty score for these angles is and basic validation is recommended for penalties higher than it would be more ideal if a penalty score higher than would have been assigned which would mandate extensive validation optimization furthermore similar cases were identiﬁed in a signiﬁcant percentage of user supplied molecules some of which featured penalties lower than the frequency and magnitude of this problem taken together with the fact that it might potentially go undetected by a quick glance at the penalties prompted us to implement the bond group feature the inclusion of this extra contribution to the penalty function caused diﬀerent angle parameters to be chosen by the assignment algorithm which eliminated the out of plane deviation it should be noted that although the sum of the angles is now close to there is still a large in plane deviation this is due to an unexpectedly large physical deviation from the idealized geometry indeed the values for c10 and c3 c10 are far from such that the minimized geometry figure is visibly diﬀerent from the idealized figure dimethylcarbamoyl indole minimized conformation geometry for a membered ring figure we speculate that this is caused by a combination of favorable electrostatic interactions between the and atoms and steric repulsion between the dimethylamide group and the ring system which cannot be relieved by tilting the amide group out of the plane of the ring because the system is kept planar by π orbital overlap eﬀects in addition to the electrostatic interaction mentioned above as the force ﬁeld lj parameters were optimized targeting bulk phase properties it appears that the steric repulsion seen in the vacuum qm calculation is not adequately reproduced yielding a signiﬁcantly diﬀerent angular geometry however the new penalties for these angles are and respectively providing the user a clear and unambiguous indication that their values should be veriﬁed and or optimized note that although the bond group feature increased the penalties the parameters by analogy became better as the inappropriate parameters obtained dx doi org j chem inf model 3168 journal of chemical information and modeling article table interaction energies kcal mol and distances å of complexes of dimethylcarbamoyl indole with water in diﬀerent orientationsa interaction geometry cgenff data were calculated using the geometry for hf d interaction energies are scaled by a factor hf interaction distances are not scaled however empirical hydrogen bond minimum distances i e involving model compound hydrogen bond donors and acceptors should be approximately å shorter than hf d values as required to yield adequate bulk phase properties the meaning of the symbols describing the interaction geometries as well as the scaling factor and oﬀset are explained in ref results include average deviation ad root mean square deviation rmsd and absolute average deviation aad the values in the column penalty are the penalties of the atoms that directly interact with the water probe while the penalty parent values are associated with the atom to which the latter is bound table components of the dipole moment of dimethylcarbamoyl indole calculated with cgenff and compared to hf and energy surfaces around the and dihedrals the parameters by analogy for produce good agreement with the qm from to but the pess diverge by kcal mol from to on the basis of the geometry at figure we speculate that this is because the intramolecular electrostatic interaction of the amide nh group with the benzene ring is substantially diﬀerent from its interaction with an ester or protonated carboxylic acid in a cterminal amino acid which is the origin of the parameter by analogy for this dihedral manual ﬁtting of this dihedral leads to the introduction of a dihedral term with n kϕ and δ and a second dihedral term with n kϕ and δ the former term relatively high kϕ might indicate that the intramolecular nonbonded interactions are not perfectly balanced for the purpose of reproducing this vacuum pes and figure potential energy scan around the c2 c10 bond in dimethylcarbamoyl indole standard charmm charges is given in table this table shows a fair correlation between the deviation from the standard charmm charges and the penalty the three highest penalties correspond to the three charges that have signiﬁcant though less than e deviations given the lack of high penalty bond and angle parameters it is not surprising that all minimized bond lengths and angles are in excellent agreement see table of the supporting information figure depicts the relaxed potential dx doi org j chem inf model 3168 journal of chemical information and modeling article figure potential energy scans around the and bonds in n benzylacetamide parameter the aforementioned sum is returned to the user as a measure for the accuracy of the approximation charges are assigned using a variation on the classical bond charge increment scheme which essentially associates a bond charge increment with each bond parameter we extended this scheme by associating two charge increments with each angle parameter and three charge increments with each dihedral parameter to apply this bond charge increment scheme to a novel molecule the aforementioned algorithm for assigning parameters by analogy is used albeit using slightly diﬀerent criteria for analogy this allows for per charge penalty scores to be returned which give the user an idea of the quality of the charges charge increment values are optimized to reproduce the charges on model compounds that were part of the parametrization of the force ﬁeld as well as possible using a restrained least squares ﬁtting algorithm a ﬁnal charge rmsd of e was obtained the validity of the ﬁnal parameters and charges and the relevance of the associated penalty scores were illustrated by two case studies a more thorough validation study will be the subject of a future paper as the methods discussed in the preceding and present paper are entirely empirical they are excellently suited for high throughput applications as testiﬁed by the fact that we processed a representative database of drug like structures at a rate of compounds per second on a single core of a below average desktop processor finally it should be reiterated that the penalties are indicative of the analogy of the generated parameters or charges with available terms in cgenff rather than the intrinsic ﬁtness of these terms for application in the molecule of interest consequently large penalties may yield satisfactory accuracy while signiﬁcant deviations from acceptable accuracy may occur when small or zero penalties are reported indeed given the sensitivity of chemical properties to environment in cases where parameters are available in cgenff for a new molecule there may still be signiﬁcant limitations in the accuracy of the resulting model it is important that the user is aware of such limitations and perform the appropriate validation and optimization as required use of the cgenff program for automatic atom typing and assignment of parameters and charges by analogy is provided at https www paramchem org figure n benzylacetamide conformation for ϕ and ϕ we speculate that the ﬁtted parameters might be poorly transferable to other molecules this is in line with the observation that the transferability of dihedral parameters in general is often problematic see section charge assignment above although the penalty of associated with this parameter mandates validation it is possible in some cases to obtain a similarly mediocre agreement for parameters with even lower penalties therefore if accuracy is of the utmost importance in the molecule under study validation is recommended regardless of the penalties concerning the c7 scan the data points with poor agreement between cgenff and are caused by the c7 c9 dihedral which is relaxed at every scan point scan being close to indeed the value for the c7 parameter did not change signiﬁcantly during manual optimization and the improvement in agreement is purely due to the optimization of the c7 n8 c9 dihedral this demonstrates that a higher penalty is not a guarantee for poor agreement rather the penalty scores should be thought of as measure for the approximation error on the parameter summary the present paper describes the algorithms for automatic assignment of bonded parameters and charges in cgenff thereby allowing for automatic application of cgenff to arbitrary molecules bonded parameters are assigned by substituting atom types in the deﬁnition of the desired parameter the assignment of bonded parameters also forms the basis for the charge assignment a penalty is associated with every substitution and the existing parameter with the lowest sum of penalties is chosen as an approximation for the desired a list of bond angle and dihedral charge increments that were constrained during the charge increment optimization toppar stream ﬁles for the case studies as generated by the cgenff program a table with minimized bond length and angles for nbenzylacetamide analogous to table this material is available free of charge via the internet at http pubs acs org multiwfn is a multifunctional program for wavefunction analysis its main functions are calculating and visualizing real space function such as electrostatic potential and electron localization function at point in a line in a plane or in a spatial scope population analysis bond order analysis orbital composition analysis plot density of states and spectrum topology analysis for electron density some other useful utilities involved in quantum chemistry studies are also provided the built in graph module enables the results of wavefunction analysis to be plotted directly or exported to high quality graphic file the program interface is very user friendly and suitable for both research and teaching purpose the code of multiwfn is substantially optimized and parallelized its efficiency is demonstrated to be significantly higher than related programs with the same functions five practical examples involving a wide variety of systems and analysis methods are given to illustrate the usefulness of multiwfn the program is free of charge and open source its precompiled file and source codes are available from http c wiley periodicals inc multiwfn codeplex com v introduction theoretical backgrounds according to quantum theory a system wavefunction contains all information needed however it is often a rather complicated function and may be viewed as a black box in common quantum chemistry calculations only energy is usually extracted evidently this is insufficient to fully interpret intrinsic characteristics of electron structures to obtain more information of chemical interest wavefunction analysis technologies must be used for further disclosing the black box unfortunately only minimal wavefunction analysis methods are available in most of mainstream quantum chemistry packages although there are plenty of standalone wavefunction analysis programs some are limited to certain analysis techniques and some are inconvenient to use or have low efficiency we feel that it is quite meaningful to develop a new program that integrated most of important wavefunction analysis methods meanwhile and is very easy to use and have high efficiency multiwfn is such a wavefunction analyzer running on windows and linux platform it is especially good at visual study of real space functions such as electrostatic potential esp and electron localization function elf multiwfn accepts wavefunctions in proaim format which has been supported by many quantum chemistry packages some other file types such as gaussian formatted checkpoint file and nbo plot files are also supported multiwfn is free and open source its size is very small about the latest version can be downloaded at http multiwfn codeplex com the rest of this article is organized as follows section theoretical backgrounds briefs involved theoretical backgrounds section program introduces various aspects of multiwfn in section efficiency efficiency comparison between multiwfn and related programs is given and section practical applications presents five practical applications to illustrate the usefulness of multiwfn real space functions journal of computational chemistry doi jcc visualization of real space functions is an important and convenient way of studying electron structure multiwfn supports almost all valuable real space functions and they will be sketched in turn the functions involved from sections electron density to sign q are purely based on electron density and thus they can be obtained not only by quantum chemistry calculations but also by experimental techniques such as x ray diffraction the functions involved from sections value of orbital wavefunction to fermi hole and fermi correlation factor function are directly based on wavefunctions and they are useful but unobservable experimentally three special real space functions presented in multiwfn are briefed in sections esp from nuclear or atomic charges and promolecular and deformation properties electron density qðrþ x i x x gi jui ðrþj gi cl i vl ðrþ i l where u and g correspond to the natural orbital and its occupation number respectively v denotes basis function and c is coefficient matrix a t lu f chen department of chemistry and chemical engineer school of chemical and biological engineering university of science and technology beijing beijing people republic of china e mail sobereva sina com contract grant sponsor national natural science foundation of china contract grant number c wiley periodicals inc v www chemistryviews com software news and updates www c chem org gradient norm of electron density table the magnitude of electron density its gradient norm and reduced density gradient in different area sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ qðrþ qðrþ qðrþ þ jrqðrþj þ x y z laplacian of electron density qðrþ qðrþ qðrþ qðrþ þ þ x y z the positive and negative values correspond to electron density locally depleted and concentrated respectively the relationships between q and valence shell electron pair repulsion model chemical bond type electron localization and chemical reactivity have been built electron spin density and spin polarization parameter spin den q r q r rdg r around nuclei around chemical bond weak interaction region edge of molecule large large medium minor medium minor small small medium very small small small medium very large larity the formula of shannon information entropy for normalized and continuous probability function is as follows z pðxþ ln pðxþdx for chemical system if p x is replaced by q r n then the integrand may be called local information entropy of electrons sity is defined as follows sðrþ ðqðrþ nþ lnðqðrþ nþ qs ðrþ qa ðrþ qb ðrþ a similar function named spin polarization parameter expressed as follows reduced density gradient reduced density gradient rdg is the absolute value of f going from zero to unity corresponds to local region going from unpolarized case to completely polarized case electrostatic potential x a za jr ra j is defined as constant coefficient is ignored follows rdgðrþ qa ðrþ qb ðrþ fðrþ a q ðrþ þ qb ðrþ vtot ðrþ vnu ðrþ þ vele ðrþ z þ dr jr j function measures the electrostatic interaction between a unit point charge placed at r and the system of interest a positive negative value implies that current position is dominated by nuclear electronic charges molecular esp has been widely used for prediction of nucleophilic and electrophilic sites for a long time it is also valuable in studying hydrogen bonds halogen bonds molecular recognitions and the intermolecular interaction of aromatics moreover on the basis of statistical analysis murray et al found a set of functions called gipf which connects esp in molecular surface and macroscopic properties the links between esp and covalent radii electronegativity and system energy were reviewed in ref local information entropy information entropy is a quantifi cation of information and this theory was proposed by shannon in his study of information transmission in noise channel nowadays its application has been largely widened to other areas including theoretical chemistry for example aslangul et al attempted to decompose diatomic and triatomic molecules into mutually exclusive space by minimizing information entropy parr et al discussed the relationship between information entropy and atom partition as well as molecular simi jrqðrþj ðrþ the original intention for introducing rdg is to convert density gradient namely eq to a dimensionless quantity recently johnson et al found that the isosurface of rdg is a very valuable tool for revealing weak interaction regions the mechanism is easy to be explained by table from table it is clear that different area can be distinguished according to different magnitude of real space functions in the low electron density area the rdg isosurfaces with small isovalue reveal a weak interaction region sign q in bader qtaim theory also see below type critical point cp usually appears in chemical bond path or between atom pairs that have weak attractive interaction whereas generally appears in the center of ring system and displays steric effect the criterion for distinguishing and is the sign of that is the second largest eigenvalue of hessian matrix of electron density besides the strength of weak interaction has positive correlation with q r hence a new function can be defined as follows kðrþ ðrþþqðrþ where the sign is the operator used to extract the sign of variable if the value of k r is represented by colors and mapped onto rdg isosurfaces then not only the region of weak interaction but also the type and strength are revealed visually we will illustrate the power of this visual analysis method by practical examples in section weak interaction analysis of urea crystal weak interaction has significant influence on conformation of macromolecules and binding mode of proteins and ligands unfortunately reproduction of electron density by ab initio and grid data calculation of rdg and sign q for such huge systems are always too time consuming however electron journal of computational chemistry software news and updates www c chem org density can be approximately constructed by superposing electron densities of free state atoms which are the so called promolecular approximation johnson et al found that weak interaction analysis under this approximate density is still reasonable the fitted densities of free state atoms by slatertype orbitals are already prebuilt in multiwfn value of orbital wavefunction ui ðrþ x cl i vl ðrþ l hamiltonian kinetic energy density the kinetic energy density is not uniquely defined because the expected value of kinetic energy operator w w can be recovered by integrating kinetic energy density from alternative definitions one of commonly used definition is as follows kðrþ g u ui ðrþ i i i lagrangian kinetic energy density the shortcoming of hamiltonian kinetic energy density is that the value is not always positive and real another definition that has more clear physical meaning is as follows gðrþ g jrui i i is in a region the more likely the electron motion is confined within it if electrons are completely localized then they can be distinguished from the ones outside bader and stephens found that the regions that have large electron localization must have large magnitudes of fermi hole integration however the fermi hole is a six dimensional function and thus difficult to be studied visually becke and edgecombe noted that spherically averaged like spin conditional pair probability has direct correlation with the fermi hole and then suggested the following elf þ ðdðrþ where occ jrqa rqb ðrþ þ dðrþ jrui j i qa ðrþ qb ðrþ h i ðrþ qa þ qb ui denotes spin orbital r is the counterpart of d r for noninteraction homogenous electron gas note that above expression is a generalization of elf for spin polarized problems it reduces to its original definition for closed shell system when qa r is equal to qb r savin et al have reinterpreted elf in the view of kinetic energy which made elf also meaningful for kohn sham dft wavefunction they indicated that d r reveals the excess kinetic energy density caused by pauli repulsion whereas r can be considered as thomas fermi kinetic energy occ jrui 72þ qðrþ i with eq elf is completely independent of the wavefunction of system of interest and then can be used conveniently to analyze electron density from x ray diffraction data localized orbital locator localized orbital locator lol pro posed by schmider and becke is another function for characterizing electron localization lolðrþ electron localization function the larger the electron localization elfðrþ density because r is introduced into elf as a reference what the elf reveals is actually a relative localization elf is within the range of a large elf value means that electrons are greatly localized indicating that there will be a covalent bond a lone pair or inner shells of an atom involved elf has been widely used for a wide variety of systems such as organic and inorganic small molecules atomic crystals coordination compounds clusters and for different problems such as the atomic shell structure classifications of chemical bonding modes and verifications of charge shift bond an approximate version of elf has been put forward by tsirelson and stash in which the actual kinetic energy term is replaced by kirzhnits type second order gradient expansion journal of computational chemistry sðrþ þ sðrþ the r used in multiwfn is a generalized form which can be written as follows sðrþ slsda þ ðrþ þ qb ðrþ occ exact p jrui i when only alpha or beta electrons presents the above formula returns to the original form lol has similar expression compared with elf actually the chemically significant regions that highlighted by lol and elf are generally qualitative comparable whereas jacobsen pointed out that lol conveys more decisive and clearer picture than elf obviously lol can be interpreted in kinetic energy way as for elf however lol can also be interpreted in view of localized orbital small large lol value usually appears in boundary inner region of localized orbitals because the gradient of orbital wavefunction is large small in this area the value range of lol is the same as elf namely like what they did for elf tsirelson and stash substituted sexact in eq with the density expansion in eq and then obtained an approximate version of lol average local ionization energy average local ionization energy is written as follows p iðrþ qi ðrþjei j i qðrþ where qi r and ei are the electron density function and orbital energy of the ith molecular orbital respectively this function www chemistryviews com software news and updates www c chem org has many uses for example reproducing atomic shell structure measuring electronegativity quantifying local polarizability and hardness and predicting sites for electrophilic or radical attack there are many potential uses of i r waiting for further investigation an excellent review of i r has been given by politzer et al fermi hole and fermi correlation factor function fermi hole function reveals the decrease of probability of finding another likespin electron at owing to pauli repulsion when a electron present at the function for r spin electrons is defined as follows hrr þ qrr þ qr þ qr þ the value range is from qr to zero for single determinant wavefunctions this function can be explicitly written as follows hrr þ occ x occ occ x x ui þu j þu i þuj þ juk j2r types and which correspond to three two one and none of eigenvalues of hessian matrix of q are negative respectively as mentioned earlier generally appears between attractive atom pairs and hence called bond cp bcp the value of real space functions at bcp has great significance for example the value of q and the sign of q at bcp are closely related to bonding strength and bond type respectively the potential energy density namely q r r at bcp has been shown to be highly correlated with hydrogen bond energies noorizadeh and shakerzadeh found that local information entropy at bcp is a good indicator of aromaticity the maximal gradient path linking bcp and associated two local maxima of density that is is termed bond path which reveals atomic interaction path for all kinds of bonding the collection of bond paths is known as molecular graph which provides an unambiguous definition of molecular structure population fermi correlation factor is a function closely related to fermi hole f rr þ hrr r2 þ qa þ esp from nuclear or atomic charges vnu ðrþ x a za jr ra j where ra and za denote position vector and nuclear charge of atom a respectively when chg file see section wavefunction types and file formats is used as input z will stand for the atomic charge recorded in the file at this time eq is useful for analyzing the difference between exact esp and the esp reproduced by atomic charges promolecular and deformation properties promolecular real space function distribution or say promolecular property reflects the state after molecule formation but before electron relaxation whereas deformation property displays the variation during electron relaxation and they are defined respectively as follows ppro ðrþ x pfree a ðr ra þ pdef ðrþ pmol ðrþ ppro ðrþ a where pmol r and pfree r stand for any real space function mentioned above of present system and atom in free state respectively if real space function is chosen as electron density then the pdef r is often referred as deformation density which is very useful in bonding nature analysis the topology of electron density cps of electron density are the positions at which q vanishes except at infinity and they can be classified into four the main use of population analysis is to derive atomic charge which is almost the simplest model describing charge distribution atomic charges are unobservable experimentally so strictly speaking they do not have rigorous physical basis however the concept of atomic charge is very useful it helps to intuitively understand basic properties of molecules and it also has important applications in the field of quantum chemistry molecular modeling and chemical informatics hirshfeld population hirshfeld population is a very popular method based on deformation density partition and hirshfeld charge is defined as follows z qa wahirsh ðrþqdef ðrþdr where qdef ðrþ qðrþ qpro ðrþ x qfree qpro ðrþ a ðr ra þ a pro ðrþ wahirsh ðrþ qfree a ðr ra þ q qfree r is spherically averaged electron density of atom in free state hirshfeld charge represents the charge variation in corresponding atomic space whirsh during molecule formation hirshfeld charges are qualitative consistency with general chemical concepts and insensibility to the quality of wavefunction atomic dipole moment corrected hirshfeld population hirshfeld charges are always too small and have poor reproducibility of observable quantity the main reason may be that atomic dipole moments are completely neglected in atomic dipole moment corrected hirshfeld method adch hirshfeld charges are corrected by expanding atomic dipole moments to correction charges placed at neighbor atoms adch atomic charges are very reasonable in chemical sense molecular dipole moment is exactly reproduced and the reproducibility journal of computational chemistry software news and updates www c chem org of esp is also close to the atomic charges from fitting esp compared with another correction method for hirshfeld charges namely hirshfeld i the computational cost of adch correction is almost zero reproducibility of molecular dipole moment is better however lo wdin population does not possess stronger physical meaning than mpa because the orthogonalization procedure itself is more or less arbitrary voronoi deformation density population the only difference modified mpa some people had proposed various partition methods for cross term to improve mpa we collectively refer them as modified mpa mmpa in the method proposed by ros and schuit eq is replaced by between voronoi deformation density vdd population and hirshfeld population is the partition manner of molecular space in vdd the voronoi cell like partition is used the results of vdd are close to hirshfeld population in general because the magnitude of deformation density is relatively small so that different weighting function definitions would not cause too much difference in atomic charges becke population in ref becke proposed a weighting function for decomposing whole space dft integral to multiple single center spherical integrals although this weighting function is not intended for population analysis we still make an attempt to utilize it to evaluate atomic charges the becke charge can be defined as below and the details of weighting function are given in original article z qa za wabecke ðrþqðrþdr mulliken population mulliken population mpa is the oldest population method inserting linear combination equation into orthonormality condition of spin orbital yields x ca i xx a a ca i cb i sa b b a where s is a overlap matrix among basis functions the first term in eq is a local term denoting the net population of basis function in orbital i the second term is a cross term denoting the shared electrons between basis function pairs mulliken defined the composition of basis function a in spin orbital i as follows hi a ca i þ x ca i cb i sa b hi a ca i x cb i b because only square of coefficients present in the formula this method is also called c squared population analysis scpa evidently this definition ensures h positivity the h defined by stout and politzer is as follows þ hi a ca i x wa b i cb i sa b where ðca i þ cb i þ wa b ca i this definition has more consideration on the unbalanced nature of the cross term the mmpa proposed by bickelhaupt et al can be equivalently rewritten to the form of eq but weighting function is different p wa b p gi ca i i gk ca k p þ gj cb j j orbital composition it is clear that in mpa each cross term icb isa b is equally partitioned to corresponding basis functions mulliken atomic charge can be written as follows qa za x i gi x hi a mpa is deprecated for practical application because of serious shortcomings poor reproducibility of observable properties the equal partition of cross term has no strict physical meaning very high basis set dependence especially when diffusion function is used and occasionally meaningless result occurs population number is negative lo wdin population in lo wdin population lo wdin ortho gonalization is performed first to eliminate cross terms in eq and then corresponding local terms are just population number the advantage of lo wdin population relative to mpa is that the negative population number never occurs and journal of computational chemistry 592 orbital composition analysis is helpful to understand orbital intrinsic characteristics studying reaction sites and identifying mode of charge transfer transition those h mentioned above can be utilized in orbital composition analysis and the quantity hi a is just the composition of basis function a in orbital i hirshfeld weighting function defined in eq can also be used for decomposing orbital to atoms and the composition of atom a in orbital i is z ðrþwahirsh ðrþdr we found that this partition has greater basis set stability and is always more reliable than use of h to evaluate atom composition bond order bond order is a useful tool for characterizing bond type and measuring bond strength and this is an artificial index www chemistryviews com software news and updates www c chem org however some bond order definitions can be intimately connected with the experimentally measurable quantity bond energies the relationship is referred as bebo in literature wiberg bond order wiberg bond order is defined in the foot note of ref bab xx where p is single particle density matrix this definition is only suitable for the wavefunction represented by orthogonal basis sets this constraint conflicts with most ab initio wavefunctions hence multiwfn automatically performs lo wdin orthogonalization before the analysis mayer two center multicenter bond order and valence mayer bond order is a natural extension of wiberg bond order for nonorthogonal basis mayer bond order between atom a and b is defined as bab baab þ bbab xx sþab ðpa sþba þ ðpb sþab ðpb sþba in general mayer bond order is in good agreement with empirical bond order for the classical single double and triple bonds the value is close to and respectively mayer bond order had been generalized to multicenter for arbitrary number of centers the bond order can be expressed as follows bab z xx b2b x ðpsþab ðpsþbc ðpsþza x ðpsþaa x xx ðpsþab ðpsþba ðps sþab ðps sþba bab xx b2a where ps is spin density matrix this quantity displays remaining capacity of forming new bonds by sharing electron pairs mulliken bond order mulliken bond order is the total cross term between corresponding two atoms bab x i biab tdosðeþ x dðe ei þ i where e stands for eigenvalue of single particle hamiltonian and d is dirac delta function if d is replaced by a broadening function f x continuous tdos yields partial density of states pdos is defined as follows x pdosa ðeþ ni a fðe ei þ i where ni a is composition of fragment a in orbital i and it can be evaluated by summing up corresponding h terms which has been discussed above the overlap density of states opdos between fragment a and b is defined as follows opdosab ðeþ x xiab fðe ei þ i where xiab denotes the composition of total cross term between fragment a and b in orbital i free valence is defined as follows fa va density of states dos is an important concept in solid physics it denotes the number of states in unit energy interval in isolated systems the energy levels are discrete and the concept of dos is thus somewhat questionable however if the discrete energy levels are broadened to curves artificially dos graph becomes a valuable tool for visually characterizing orbital compositions total dos tdos of isolated system can be written as follows total valence measures atomic bonding capacity and the expression given by mayer is as follows va density of states of isolated systems x i gi xx i cb i sa b b2b this bond order definition is deprecated for quantifying bonding strength for which mayer bond order generally performs better however mulliken bond order is a good qualitative indicator for bonding positive value and antibonding negative value program special features several special features of multiwfn are discussed as follows very user friendly quantum chemistry softwares for academic purpose are often inconvenient to use and users need to write complex input file with consulting lengthy manual to tackle this problem multiwfn is designed as an interactive program prompts shown in each step clearly instructs users what need to do next multiwfn also never print obscure messages and hence there is no any barrier even for beginners besides there are more than practical examples in the manual which would be very helpful for new users no third part plotting softwares are required a high level graphical library dislin is invoked internally and automatically by multiwfn for visualizing results and most of the plotting parameters are adjustable by users in an interactive interface thus the procedure of wavefunction analysis is remarkably simplified especially for studying distribution of real space function multiwfn is a semi gui program that is the majority of operations are based on command line mode while graphical interfaces appear when necessary the advantage over full gui journal of computational chemistry 592 software news and updates design is that multiwfn can run in silent and batch mode easily by redirection and shell scripts high efficiency the code of multiwfn is substantially optimized most parts are parallelized by openmp technology for time consuming tasks the efficiency of multiwfn exceeds analogous programs significantly meanwhile the memory requirement is very low wavefunction types and file formats wavefunction type supported by multiwfn includes restricted unrestricted single determinant wavefunction restricted openshell wavefunction and post hf wavefunction in natural orbital formalism for basis function cartesian or spherical harmonic gauss functions with angular moment up to g are supported input file format supported by multiwfn includes proaim wavefunction file wfn the most popular format for exchanging wavefunction information supported by gaussian gamess us uk firefly and q chem aim extended wavefunction files wfx a newly introduced format by since b revision as an extension of wfn format gaussian formatted checkpoint file fch for the functions relied on basis function expansion this file type is mandatory plot file of nbo program containing various orbital informations generated by nbo program such as natural atomic orbitals and natural bond orbitals protein data bank format pdb this file type can be used as structure input for the functions that only require atom coordinates for example evaluation of rdg and sign q under promolecular approximation charge files chg a private format only atom coordinates and atomic charges present the main use of this format is for analysis of esp reproduced by atomic charges graphs plotted by multiwfn can be outputted to graphic file in publication quality abundant graphical formats are supported such as postscript ps portable document format pdf tiff graphics interchange format gif and portable network graphics png main functions calculating and plotting real space functions all real space functions mentioned in section real space functions can be calculated at a point in a line in a plane or in a spatial scope multiwfn provides very flexible ways to define the spatial region determining by program automatically to accommodate system size assign a center and then extend specified lengths in each direction to generate the region the center can be assigned by inputting coordinate or defined as middle point of two atoms the latter manner is very convenient for study intermolecular interaction journal of computational chemistry 592 www c chem org input original point and translation vectors the number of grid points in each dimension is controlled by users all points are evenly distributed in the calculation region once the calculation is finished users can select to visualize the data directly for and cases curve line graph and isosurface graph can be plotted respectively for cases five graph types are supported color filled map contour line map relief map shaded surface map with or without projection and gradient line map with or without contour lines the generated grid data can also be written to gaussian cube file which is a standard volumetric data format recognized by the majority of molecular graphics programs for flexibility consideration multiwfn preserves a blank function this design enables the real space functions supported by multiwfn to be arbitrarily and easily extended by users themselves assuming one wants to analyze weizsa cker kinetic energy density what is needed to do is simply filling the code x y z t fdens x y z into userfunc routine and recompile the program multiwfn is able to calculate and plot promolecular and deformation property for any supported real space function the required wavefunctions for free state atoms can be generated by multiwfn automatically through invoking gaussian program user can also choose to use the atom wavefunctions provided by themselves or use the set of prebuilt atom wavefunctions under basis set notice that multiwfn sphericalizes atom wavefunctions internally to avoid orientation dependency before evaluation of promolecular and deformation properties multiwfn permits operation between real space function from multiple wavefunctions this function makes the studies related to multiple molecules or one molecule in different states very easy for example the analysis of fukui function and interfragment density transition population analysis and bond order analysis all population methods mentioned in section population are supported for mpa the atomic population number can be further decomposed to basis functions shells and orbital contribution multiwfn supports bond order definitions discussed in section bond order the maximum number of centers of mayer multicenter bond order analysis is six orbital contribution for mulliken bond order in each orbital is printable for analyzing orbitals that are favorable or unfavorable for specific bonding orbital composition analysis in multiwfn orbitals can be decomposed to basis functions shells and atoms composition by mpa scpa stout politzer and hirshfeld methods fragments can be defined expediently in an interactive interface composition of intrafragment and interfragment in each orbital can be analyzed in detail plotting dos and spectrum tdos pdos and opdos can be plotted by multiwfn plain text file recorded energies levels gaussian output file wfn wfx and fch format are acceptable as input the broadening functions supported include gaussian lorentzian and pseudo voigt function multiwfn is able to plot ir raman and uv vis spectrum too topology analysis of electron density multiwfn is able to search cps of electron density by newton method and several modes are provided for setting up starting point input www chemistryviews com software news and updates www c chem org wavefunction examination and modification there are figure the interface for showing topology analysis result index of bond paths are labeled by red texts magenta orange and yellow spheres correspond to and critical points respectively color figure can be viewed in the online issue which is available at wileyonlinelibrary com coordinate directly use nuclear positions which are usually very close to type cps use geometry center of two three four nuclear positions and these modes are suitable for searching þ3 type cps respectively and set sphere center and radius and then specified number of starting points are randomly distributed in the sphere this mode is appropriate for searching the cps that are difficult to be located by other modes the search of cps is very fast once it is finished one can choose to generate the gradient paths that linked cps the results can be visualized conveniently figure shows the cps and gradient paths of imidazole magnesium porphyrin complex the value of all supported real space functions with hessian matrix at a given cp can be printed out cps and paths are allowed to be added deleted or modified and their detail informations can be exported out or imported in via formatted text file utilities numerous utilities are provided in multiwfn for facilitating wavefunction analysis and other quantum chemistry calculations some of them will be mentioned below viewing molecular structure and orbitals a convenient graphical interface is integrated in multiwfn for examining molecular structure and orbitals before wavefunction analysis see supporting information for screenshot plenty of options used for wavefunction examination and modification before analysis they can be summarized into five categories printing various informations of wavefunction in readable format discarding or only reserving wavefunction information of specified atoms or orbitals in which their contributions for real space functions can be eliminated or evaluated assigning and swapping various informations of specified gauss functions translating and duplicating wavefunction which is a very useful function for extending primitive cell wavefunction outputted by gaussian to periodic wavefunction saving modified wavefunction to new file in wfn format analysis between two real space functions this function enables grid data of two real space functions to be generated and outputted at the same time with sharing grid setting and then the scatter graph between them can be drawn which is very useful for correlation analysis moreover there are options used for setting the value of one real space function where the value of another real space function is within or without of certain range this design particularly facilitates the visual research of weak interaction by rdg and sign q and one can screen the strong interaction regions where q is large integrating a real space function in whole space this is a powerful function used to evaluate integral of selected real space function in whole space by properly modifying the code of userfunc routine mentioned above that is the integrand by users the quantity such as electric multipole moment and overlap between the norms of two orbitals can be easily obtained the numerical method is based on the scheme proposed by becke for integrating dft functional monitoring self consistent field convergence the progress of self consistent field convergence of gaussian program can be monitored the variations of energy density matrix etc in specified step range are summarized and plotted in terms of curve map which helps users to analyze the behavior of convergence and find solutions in ill convergence cases combining wavefunction of fragments multiwfn can generate gaussian input file with initial guess that combined from multiple fragment wavefunctions there are three primary purposes generating high quality initial guess wavefunction for weakly interacting system performing simple energy decomposition journal of computational chemistry 592 software news and updates www c chem org table calculation speed of multiwfn dgrid and checkden laplacian of q elf runtime checkden dgrid multiwfn n multiwfn n multiwfn n multiwfn n b c c c c speed a runtime speed a 00 a the speed of multiwfn in serial mode is considered as reference b cutoff for evaluating exponential functions is disabled for other cases this treatment is enabled c n denotes the number of threads in parallel implementation similarly hereinafter the energy difference between the first and the last round of selfconsistent field iteration can be regarded as a cause of polarization of electron density and charge transfer between fragments studying symmetry broken singlet state efficiency grid data calculation is very computationally expensive there are already some programs aiming at generating grid data for various properties for example dgrid and checkden notice that none of them supports parallel implementation to demonstrate the high efficiency of multiwfn we compared their calculation speed of grid data of elf and q for acetic acid with basis set used points are evenly distributed in the molecular region in consistency with public version multiwfn is compiled by intel visual fortran compose xe only with low level optimization option for compatibility consideration other programs are precompiled version the cpu we used is intel quad notice that to improve calculation speed multiwfn checks if the exponent is smaller than a cutoff value before exactly evaluating an exponential function if it is true then the function will be ignored this treatment is safe enough to ensure that loss of accuracy cannot be detected in any quantitative study of real space functions from table it can be seen that in serial mode multiwfn is one order of magnitude faster than checkden and several times faster than dgrid even if the cutoff treatment is disabled multiwfn is still at least one time faster than dgrid for q the speedup of multiwfn from parallel implementation is significant when four cpu cores are available more than calculation time in serial mode could be reduced we also tested the performance of generating grid data for rdg and sign q in multiwfn see supporting information the time consuming by multiwfn in serial mode is comparable to or less than nciplot which is a program proposed specifically for generating grid data for these two properties notice that nciplot has not been parallelized yet ometry optimizations and wavefunction calculations are performed by unless otherwise specified hybrid functional is chosen as theoretical method elf and fermi hole function of ecause fermi hole of ks dft wavefunction is not well defined in this example hartree fock wavefunction is used instead in combination with basis set figure depicts elf and fermi hole function in the line of cabr bond of molecule the internal shells of the two atoms are clearly shown between carbon and bromine there is a wide and high electron localization region which reveals that the bonding nature is covalent as elf is an abstract reflection of fermi hole it is interesting to check fermi hole distribution when reference point is located at maximum of the bonding elf domain from the lower part of figure it can be seen that fermi hole function has relatively high magnitude in the cabr bond meanwhile around the two atomic nuclei there are several narrow regions with high magnitude too because electron can go where its hole goes the electrons shared by the two atoms are localized in the covalent bonding region to some extent yet they are not completely confined within that region molecular esp of dihalogens halogen bonding is the noncovalent interaction that occurs between a halogen atom lewis acid and a negative site lewis base it can be expressed as follows d x b where the donor atom d could be an organic group or another halogen atom it seems weird that the halogen atom x which usually carries a negative charge could act as lewis practical applications in this section several practical examples are given to illustrate the capabilities of multiwfn all graphs except color filled isosurface map are directly generated by multiwfn only trivial modifications have been made for clearer expression all journal of computational chemistry 592 figure electron localization function positive part and fermi hole function negative part of along cabr bond the dashed line indicates reference position of fermi hole function color figure can be viewed in the online issue which is available at wileyonlinelibrary com www chemistryviews com www c chem org software news and updates figure contour map of esp of a brcl and b icl blue lines denote the position where electron density equals to which is generally regarded as molecular surface solid and dashed black lines are positive and negative isopotential lines respectively the isovalues are labeled on the graph unit is in a u color figure can be viewed in the online issue which is available at wileyonlinelibrary com acid it has been found that the reason can be clearly explained by esp here we consider dihalogen molecule brcl and icl in which chlorine is seen as x relativistic effective core potential lanl combining with valence basis set is utilized for iodine atom for chlorine and bromine is adopted the isopotential lines are shown in figure and the areas enclosed by blue lines are internal region of the molecule and will not be concerned because chlorine has larger electronegativity than bromine and iodine most negative region of esp appears in the vicinity of chlorine atom there is an evident difference between brcl and icl in esp distribution in figure the positive region dominated by nuclear charge of chlorine atom bulged out significantly owing to the diminution of electron density in corresponding region meanwhile negative region is completely separated this is a typical nature of the halogen bonding that is the attractive interaction caused by electrostatic effect between the positive potential site commonly referred as r hole of halogen atom and the electron rich site can make system energy decreased because more electrons can be pulled from iodine than bromine by chlorine atom the r hole in figure is largely contracted and chlorine atom is fully encompassed by dashed lines therefore it can be expected that the halogen bonding is much harder to be formed in this situation electron localization of and cluster the global minimum structure of cluster is planar with symmetry figure depicts its electron structure characteristic by elf under basis set it can be seen that very high value occurs between each boundary boron atom although electrons also localized between the central atoms as well as in some regions between central and boundary atoms e g but the extend is evidently weaker this observation suggests that only the bonding between boundary atoms is strong by examining mayer bond order this conclusion can be further verified all two center bond orders of adjoining atoms in boundary exceed unity whereas bond orders between any other adjoining atoms are less than note that electron localization in some areas is very low for example between and and around the center of corresponding two center and three center bond orders are very small and respectively thus these bonding can be expected to be very weak analysis of q gives similar conclusions as discussed above see supporting information planar type structure has been found to be a local but not a global minimum on potential energy surface of cluster figure shows its lol distribution under basis set it is clear that electrons are mainly localized within the three outside triangles these regions have been known as three center two electron bond mayer three center bond order analysis shows that the value of outside triangle is much larger than the one of inner triangle namely it is worth noting that the red color in figure somewhat intrudes into the interstitial space between boundary atoms and bond order of boundary liali bond is markedly larger than the inner one moreover high accurate calculation shows that the former bond length is slightly smaller than the latter one hence boundary liali bonds turn out to be stronger than the inner ones journal of computational chemistry 592 software news and updates www c chem org ones whereas the electron density at the bcp corresponding to the vertical one is larger than the parallel one there is a small brown surface between each two adjacent ureas as pointed out by orange arrow this region implies the steric effect caused by close contact which in turn is the consequence of the two hydrogen bonds around it the surfaces crossed by magenta dashed lines that is the small green ellipsoids and the crooked slices clearly exhibit the van der waals interactions between adjacent ureas which are easily overlooked in common interaction analysis another example using rdg and sign q function namely the analysis of weak interaction between glycophorin a dimer is given in supporting information dos map of ferrocene figure color filled map of electron localization function of planar cluster weak interaction analysis of urea crystal here we use rdg and sign q function to reveal weak interaction in urea crystal the structure is taken from ref and the electron density is reproduced by calculation vmd is used to plot the graph the isosurfaces in figure depict where weak interactions occur the blue red and green or earth green colors indicate the strong attractive strong repulsive and van der waals interaction respectively as shown by blue dashed lines each urea molecule in crystal environment forms eight hydrogen bonds in total oxygen atom in the urea participates in four of them careful inspection shows that among the four hydrogen bonds those vertical to urea plane are slightly stronger than those parallel to the plane because the central area of the rdg surfaces corresponding to former are more blue actually the length of the vertical hydrogen bonds is shorter than the parallel figure relief map with projection of localized orbital locator of planar cluster journal of computational chemistry 592 the last example is analysis of tdos pdos and opdos map of ferrocene see figure basis set combining lanl pseudopotential is used for iron whereas for other elements notice that only relative height rather than absolute height of curves is meaningful z axis is perpendicular to cyclopentadienyl groups the graph clearly exhibits orbital characteristics in different energy ranges it is obvious that the major contribution from s px and py basis functions of carbon magenta curve is due to low lying mos instead of frontier mos the major composition of mos around a u is pz orbital of carbon blue curve and iron atom red curve inspection of the green opdos curve which expresses the bonding between carbon pz and iron atom suggests that carbon pz figure reduced density gradient isosurface map with isovalue of of urea crystal the value of sign q in surfaces is represented by filling color according to the color bar in upper right corner for clarity the atom pairs mainly involved in hydrogen bonds and van der waals interactions between central urea and neighbor ureas are marked by blue and magenta dashed line respectively orange arrow points at repulsive interaction region www chemistryviews com www c chem org software news and updates figure total black partial red blue and magenta and overlap green between fe and carbon pz basis functions density of states map of ferrocene discrete lines represent original data the curves broadened from which have been scaled by factor of left axis is for total and partial density ofstates right axis is for overlap density of states isovalue of mo isosurfaces is the plane of cyclopentadienyl groups is perpendicular to z axis orbitals are very important for stabilization of ferrocene because opdos has large positive value in these ranges homo is almost purely contributed by iron orbitals however its slight overlap with carbon pz is still beneficial to bonding for all virtual mos opdos curve is in negative region and shows antibonding characteristic this is due to the unfavorable overlapping in orbital phase as can been seen from lumo isosurface conclusions in this article theory backgrounds features and functions of multiwfn were described the high efficiency of multiwfn was demonstrated by comparison with related programs and the five practical applications referring to a wide variety of systems and analysis methods were given to illustrate the capacities and usefulness of multiwfn further our program is also very easy to use for beginners multiwfn will be updated continuously in the future many new functions are already planned for succeeding versions such as charge decomposition analysis plotting orbital interaction diagram basin integral and analysis of molecular surface properties besides several elfs intended for correlation wavefunction will be supported and topology analysis module will be available for other real space functions soon orca is a general purpose quantum chemistry program package that features virtually all modern electronic structure methods density functional theory many body perturbation and coupled cluster theories and multireference and semiempirical methods it is designed with the aim of generality extendibility efficiency and user friendliness its main field of application is larger molecules transition metal complexes and their spectroscopic properties orca uses standard gaussian basis functions and is fully parallelized the article provides an overview of its current possibilities and documents its efficiency c john wiley sons ltd how to cite this article wires comput mol sci doi wcms program design o rca is a flexible electronic structure project that was initiated in executables but not source code can be obtained free of charge for academics from http www thch uni bonn de tc orca highlights of the program are user friendliness flexibility and efficiency it has been written from scratch in a straightforward way simplicity and readability being given preference over elegance in programming using c and is completely parallelized using the message passing interface mpi platform or thirdparty library dependence was carefully avoided the only exceptions are the basic linear algebra subroutines blas the message passing interface mpi and a very small amount of the linear algebra package lapack orca compiles out of the box on any platform with a standard c compiler self consistent field methods orca uses contracted gaussian basis functions in conventional semidirect and direct integral handling models it is most efficient with segmented contracted bases where it is probably comparable in speed to the most efficient alternative programs a special integral branch exists for dealing with generally contracted atomic natural orbital basis sets basis sets are internally stored in a fairly comprehensive library but can be read from a file changed for atom types or correspondence to neese thch uni bonn de lehrstuhl fu r theoretische chemie universita t bonn institut fu r physikalische und theoretische chemie bonn germany doi wcms volume january february for individual atoms effective core potentials ecps and associated basis sets are available for all of the periodic table closed shell rhf spin unrestricted uhf and restricted open shell rohf self consistent field scf calculations on the basis of various density functional theory dft methods as well as hartree fock hf theory are available the program recognizes and optionally maintains molecular symmetry and subgroups but so far makes little use of it in speeding up calculations available dft functionals include generalized gradient approximation gga meta gga hybrid dft hdft and double hybrid dft dhdft functionals table grimme semiempirical van der waals corrections to dft are implemented semiempirical methods in orca include zerner intermediate neglect of differential overlap zindo modified neglect of differential overlap mndo and initial guesses are flexible and include transferring wavefunctions from one geometry to another or from one basis set to another broken symmetry scf solutions can be searched for with specialized techniques a variety of options exist for speeding up convergence including a quadratically convergent newton raphson procedure population analysis according to the lo wdin mulliken and mayer procedures is available throughout orca an interface to the natural bond order gennbo program is provided http www chem wisc edu electrostatic potential derived charges charges from electrostatic potentials using a grid based method chelpg can be generated as well c john wiley sons ltd software focus wires wiley com wcms t a b l e exchange and correlation functionals built into orca exchange correlation slater pbe rpbe mpw tpss revtpss optx pwlda pbe op tpss revtpss for functionals with no hf exchange the resolution of the identity ri approximation is exploited and greatly speeds up the calculation of the coulomb term ri j exchange correlation terms are handled by numerical integration using standard techniques the nonlocal hf exchange term can be approximated in an efficient way asymptotically linear scaling chain of spheres cosx algorithm similar to pseudospectral techniques and combined with ri j to give the popular rijcosx approximation relativistic options scalar relativistic all electron calculations are available the latter can be done using the zeroth order regular approximation zora or the infinite order regular approximation iora alternatively the second order douglas kroll hess dkh hamiltonian is available in the standard implementation two component methods are not supported analytic derivatives are available for all of these methods geometry optimizations using scalar relativistic hamiltonians use the one center approximation a set of segmented all electron relativistically contracted sarc basis sets were developed for third row transition metals lanthanides and actinides in zora and versions for the remaining elements the all electron basis sets of weigend et al were recontracted all electron and small core ecp calculations are usually of comparable efficiency note a comprehensive study of the performance of scalar relativistic and ecp based dft for transition metal geometries excited state calculations excited states can be calculated with all multireference methods described below for large molecules it is more convenient to resort to easier albeit much less general methods such methods are implemented in the orca cis program configuration interaction ci with single excitations cis relative to an rhf or uhf reference determinant and the associated time dependent dft td dft methods are available cis is known to have major deficiencies and rarely if ever provides balanced predictions for electronic spectra in cases where it is reasonable starting point a double correc tion improves results cis d method the rijcosx approximation leads to dramatic speedups at no loss of accuracy for hdft the tamm dancoff approximation tda is presently obligatory in conjunction with the d correction excited states can be calculated for dhdfs intensities for electronic transitions are computed within the dipole length and dipole velocity formalisms quadrupole and magnetic dipole intensities are optionally available the latter become important for the computation of x ray absorption spectra for which orca cis is widely used analytic gradients are available for cis and td dft tda and form the basis for the efficient calculation of resonance raman spectra and absorption and fluorescence bandshapes all of which are available orca asa module many body perturbation theory and dhdft orca features an efficient implementation of second order mo ller plesset perturbation theory and its spin component scaled variant for energies and analytic gradients on the basis of rhf uhf or rohf wavefunctions whereas the canonical implementation is fairly efficient most calculations are done within the ri approximation analytic gradients for dhdfs in ri and non ri variants are implemented despite the unfavorable o scaling the step is usually much faster than the preceding scf calculation even for systems with more than basis functions the combination of with the rijcosx approximation is particularly powerful this is also available for analytic gradients the orbital optimized is available and improves results for open shell molecues it has been argued to be similar to the popular method coupled pair and coupled cluster methods canonical methods orca features an efficient and parallel module to calculate closed shell and spin unrestricted coupled cluster energies including single and double excitations ccsd as well as perturbative triple excitations c john wiley sons ltd volume january february wires computational molecular science the orca program system ccsd t the closely related quadratic ci qci method is available a comprehensive set of the recently revived coupled electron pair cepa methods is implemented for all methods implementations exist that involve either a full four index transformation or only a partial transformation involving integrals with up to two external labels whereas the remaining terms are directly formed in the atomicorbital basis if triples are computed all integrals with up to three external labels must be stored on disk which limits the applicability orbital optimization natural orbital iterations and generation of bru ckner orbitals are supported spin component scaled thirdorder perturbation theory scs can be performed analytic gradients and use of symmetry has not been implemented arbitrary order coupled cluster energies but not gradients or densities can be obtained via an interface to mihail kally mrcc program http fkt bme hu local correlation methods the local pair natural orbital lpno approach has been recently developed the lpno cepa and lpno ccsd methods have been shown to faithfully within a few tenth of a kilocalorie per mole reproduce the results of canonical ccsd and cepa calculations while leading to speedups of up to and exceeding three orders of magnitude the lpno approximation is particularly effective in conjunction with large basis sets calculations involving basis functions can be routinely done with the code in an entirely black box fashion typically the correlation calculation takes only two to four times longer than the preceding scf step thus these methods are well suited for large scale computational chemistry multireference methods complete active space scf a flexible and efficient program for the generation of complete active space scf casscf wavefunctions is part orca the program allows for the averaging of an arbitrary number of roots of varying spatial symmetry and multiplicity the program generally cycles between ci and orbital optimization steps a variety of first and second order convergence accelerating methods is available use of the ri approximation can be made in the integral transformation steps the calculation of fock like matrices supports the rijcosx and ri jk approximations large orbital up to basis functions and active spaces active orbitals can be treated analytic gradients are available volume january february multireference perturbation theory a highly efficient way to obtain second order perturbation energies even for large orbital and active spaces is provided by n electron valence perturbation theory that is available in a particularly efficient implementation is similar in scope to the popular method but in our opinion features a number of advantages it is intruder state free due to the choice of the zeroth order hamiltonian dyall hamiltonian it is diagonal such that no linear equation systems need to be solved and no large sets of coefficients need to be stored and it is strongly contracted such that no linear dependencies and nonorthogonal configuration state functions csfs are ever generated the challenging steps in an calculation are the integral transformation with integrals up to two external labels and the generation of the up to fourth order density matrix efficient approximations for both steps are available and in general the calculation of the second order perturbation energies for all roots involved in the casscf is usually much faster than the casscf step itself multireference ci and related methods a flexible module for the generation of multireference ci mr ci and related wavefunctions is available it is using spin and space symmetry adapted single configuration csfs to expand the wavefunction and is hence limited to reasonably small reference spaces any set of reference csfs can be employed and an arbitrary number of blocks of varying spin symmetry and space symmetry can be treated at the same time individual selection and reference space selection is optionally performed four index integrals can be generated on the fly from ri generated three index integrals the program also features mraverage coupled pair functional acpf and mraverage quadratic coupled cluster aqcc as well as a variety of difference dedicated ci methods that can be extended to larger molecules atoms or basis functions the spectroscopy oriented ci is original to orca incorporation of relativistic effects by quasi degenerate perturbation theory the casscf and mr ci modules feature analogous options which allow the rediagonalization of a relativistic hamiltonian which includes spin orbit coupling soc spin spin coupling ssc and linear external magnetic fields in a basis of roots of the born oppenheimer hamiltonian quasi degenerate perturbation theory qdpt all magnetic sublevels c john wiley sons ltd software focus wires wiley com wcms m s s s are generated for each root with total spin s and the matrix elements over the relativistic many particle hamiltonian is calculated using wigner eckart algebra any number of multiplicities and spatial symmetries can be included in these calculations in the case of the matrix elements are computed over state averaged casscf wavefunctions whereas the diagonal energies of the qdpt matrix carry the dynamic correlation correction brought in by a number of spectroscopic parameters including absorption and circular dichroism cd spectra magnetic circular dichroism mcd spectra g values and zero field splittings zfss can be calculated in this way combined quantum mechanical molecular mechanical and related methods estimates are supported calculated hessians e g from a lower level of theory can be fed into the program the popular broyden fletcher goldfarb shanno bfgs hessian update is generally used transition states are located using eigenvector following the initial hessian does not need to have a single negative frequency the program will use usersupplied information in order to follow the mode of interest a partial hessian involving only those atoms that are implicated most strongly in the transition state vector can be calculated to initiate the search this can save large amounts of computer time the bofill update is used by default a special application is a method for the location of minimum energy crossing points between two potential surfaces it is implemented in orca for all methods that deliver analytic gradients frequencies orca can be easily interfaced with existing combined quantum mechanical molecular mechanical qm mm package and supports the incorporation of large sets of external point charges into the quantum hamiltonian as well as the calculation of all necessary analytic gradients orca is successfully used together with chemshell an interface is supported by gromacs and pdynamo orca only supports the calculations of numerical harmonic frequencies from the differentiation of analytic gradients single or double sided numerical frequencies can be calculated this part of the program is parallelized for many cores using a master slave strategy a similar parallel implementation is available for numerical gradients of methods for which no analytic gradients have been coded e g ccsd t solvation spectroscopic properties orca supports the conductor like screening cosmo polarizable continuum model throughout all its modules and also in all implemented analytic gradients the direct cosmo rs is also available electron paramagnetic resonance and nuclear magnetic resonance geometry optimization geometry optimization is most conveniently performed using a set of program generated redundant internal coordinates these coordinates can be user modified special provision is made for disjoint molecular fragments any number of constraints can be imposed on the positions of individual atoms bonds angles or dihedrals if desired the program only optimizes the positions of hydrogen atoms or can invert all constraints relaxed surface scans can be generated constraints do not need to be satisfied in the starting geometries the program will automatically reassign coordinates if the set of redundant internal coordinates becomes invalid e g because an angle is approaching linearity several initial hessian the spin hamiltonian parameters that enter electron paramagnetic resonance spectra have always been a focal point in the development of orca most commonly dft linear response methods are used these methods are available for the g tensor the ssc and soc contributions to the zfs tensor all three parts of the hyperfine coupling tensor as well as for electric field gradient tensors scalar relativistic corrections including picture change effects are available for all of these parameters using the dkh and zora hamiltonians the spin orbit mean field approximation to the soc operator is used in an efficient implementation in the presence of near orbital degeneracy all of these parameters can be computed on the basis of casscf mr ci or calculations using qdpt methods as described above mo ssbauer spectroscopy the calculation of mo ssbauer parameters is a minor addition that can be achieved with the orca eprnmr c john wiley sons ltd volume january february wires computational molecular science the orca program system module following a simple calibration procedure such calculations on the basis of dft have seen many successful applications infrared and raman spectroscopy thermodynamic properties infrared and raman spectra are computed on the basis of harmonic frequencies that are presently computed using hf dft or wavefunctions likewise thermodynamic corrections to the calculated electronic energies are obtained using standard ideal gas equations a nonstandard application is the calculation of nuclear resonance vibrational spectra for which orca contains a specifically advanced module vibrational modes can be animated and conveniently analyzed according to the contributions of individual bond stretches absorption and cd spectroscopy absorption and cd spectra can be computed on the basis of cis cis d td dft casscf or mr ci calculations as described above magnetic cd spectroscopy orca features a fairly advanced implementation on the basis of qdpt for mr ci and multireference approaches absorption and fluorescence bandshapes and resonance raman spectroscopy orca features a highly efficient implementation based on analytic gradient techniques in conjunction with heller time dependent semiclassical theory the orca asa module that carries out these calculations can also be operated in a stand alone mode that allows for the fitting and analysis of experimental spectra x ray absorption and emission spectroscopy x ray absorption spectra can be fairly successfully calculated on the basis of td dft approaches as described above x ray emission spectra are successfully predicted with an even simple one electron approach importantly quadrupole contributions to the absorption and emission cross sections must be taken into account for calculations on metal kedges standard protocols for the calculation of these spectra have been proposed conclusion it has hopefully become plausible that orca is a powerful efficient and flexible multipurpose research tool that can be used for a wide range of computational chemistry applications it is user friendly and free of charge for academic researchers hans joachim werner peter j knowles gerald knizia frederick r and martin schu molpro available at http www molpro net is a general purpose quantum chemical program the original focus was on high accuracy wave function calculations for small molecules but using local approximations combined with explicit correlation treatments highly accurate coupled cluster calculations are now possible for molecules with up to approximately atoms recently multireference correlation treatments were also made applicable to larger molecules furthermore an efficient implementation of density functional theory is available c john wiley sons ltd how to cite this article wires comput mol sci doi wcms introduction m olpro was founded by wilfried meyer and peter pulay in the late at that time pulay developed the first analytical gradient code for hartree fock hf and meyer his pno cepa pseudo natural orbital coupled electron pair approximation methods orbital optimization for multireference cases became possible in through a new state averaged quadratically convergent mcscf multiconfiguration self consistent field program by werner and meyer in werner and reinsch developed the first internally contracted multireference configuration interaction icmrci program a new generation of programs was founded in when werner and knowles started to collaborate on a new casscf complete active space scf program they combined fast orbital optimization with determinant based full ci codes and additional more general unitary group configuration interaction ci codes this resulted in a very quickly converging mcscf casscf code called multi which is still in use today subsequently knowles and werner developed a new more efficient ic mrci method extensions for accurate treatments of excited states using ic mrci followed somewhat later for brevity henceforth ic mrci will be denoted as mrci these mcscf and mrci programs formed the basis of the modern molpro in the following years coupled cluster dft and many other programs were added these will be summarized in the following sections many people have contributed to this and a full author list can be found at http www molpro net due to restrictions on length not all references to original work can be included and we refer to our cited earlier papers for more information quantum chemical methods correspondence to werner theochem uni stuttgart de institut fu r theoretische chemie universita t stuttgart stuttgart germany school of chemistry cardiff university cardiff uk school of chemistry university of bristol cantocks close bristol uk institut fu r physikalische und theoretische chemie universita t regensburg regensburg germany doi 1002 wcms hartree fock molpro includes closed shell and open shell spinrestricted rhf rohf hartree fock programs as well as spin unrestricted hartree fock uhf all programs can run with precomputed integrals on disk or in direct mode in which the integrals are computed on the fly for both cases parallel implementations are available the most efficient way to c john wiley sons ltd volume march april wires computational molecular science molpro t a b l e elapsed times in seconds on a single node with two quad core xeon cpus for density fitting hartree fock df hf df and density fitting spin restricted df ks calculations for pregnanediol using the cc pvtz orbital basis set obs cgtos and cc pvtz jkfit gtos for hf or tzvpp jfit gtos for blyp df hf df df ks blyp df ks cores energy gradient energy energy gradient energy gradient 462 1555 54 830 113 in all df hf and df ks calculations iterations were needed carry out hf calculations for large molecules is to use density fitting df this is available for rhf and rohf and very efficiently implemented using the new aic adaptive integral core integral written by one of us gk example timings are given in table optionally local density 16 can be used to speed up the treatment of exchange using localized orbitals and local subsets of fitting functions for each orbital product in large molecules this can significantly increase efficiency without much affecting accuracy for example the elapsed time for the calculation in table is roughly halved with submicrohartree errors density functional theory density functional theory dft is available for spinrestricted ks and spin unrestricted uks cases either using precomputed integrals direct integrals or density fitting many different functionals with or without exact exchange are available the functionals are expressed naturally in their mathematical form within the syntax of the symbolic algebra system which is used to generate both executable fortran code and documentation recent improvements of the df ks code include faster integral evaluation integral caching and faster evaluation of the exchange correlation potential ongoing efforts also include the development of special code for graphical processing units gpus optionally using poisson density fitting table contains a comparison of elapsed times for df hf and df ks calculations and the corresponding gradients unfortunately for pure functionals the speedup with more than four processors is small because nonparallelized parts such as diagonalization and direct inversion of the iterative subspace diis start to dominate however because the whole dft calculation takes less than a minute for a atomic molecule with triple zeta basis this is of little relevance volume march april standard single reference correlation methods most standard single reference correlation methods are available in molpro for closed shells ccsd t qcisd t bccd t cisd and cepa various versions are available in addition singles configuration interaction cis and equation of motion coupled cluster eom ccsd can be used to compute excitation energies for open shells based on uhf orbitals and based on rohf semicanonical are implemented the standard spin orbital based rhf uccsd t method as well as the partially spinrestricted rhf rccsd t are implemented all these methods are based on efficient matrix tensor algorithms that extensively use matrix multiplications and therefore run at highest possible speed on modern computers and are parallelized cf section parallel execution for the integrals can be computed most efficiently using density fitting approximations timing examples for df and ccsd t can be found in tables and respectively the mrcc program of ka is interfaced to molpro making it possible to carry out coupled cluster calculations with higher excitations ccsdt q ccsdtq and many other variants mcscf casscf and full ci full ci calculations are possible using the determinant based full ci program of knowles and handy the mcscf casscf program can use either this algorithm default for casscf or configuration state functions for general mcscf calculations the optimization includes higher than quadratic terms in the orbital rotation parameters which makes convergence very fast the determinantbased casscf code allows state averaged calculations even for states of different symmetry and spin dipole and transition moments as well as many other c john wiley sons ltd software focus wires wiley com wcms properties can be computed cf section electrical and vibrational properties the final orbitals can be transformed to natural pseudo canonical or localized form either using state averaged or state specific density matrices several different orbital sets can be stored for further use or visualization the valence bond program casvb of cooper et al is also interfaced to multi t a b l e root mean square errors of various properties computed with vtz relative to extrapolated basis set limits multireference perturbation theory single state and rayleigh schro dinger perturbation theory is available in two different implementations the older is based on the mrci code of werner and cf section multireference configuration interaction this program also allows and multistate ms calculations level shifts or the ipea can be used to avoid intruder state problems in the program developed by celani and werner the inactive orbitals are treated much more efficiently than before and density matrices and coupling coefficients are computed only for the active orbital space furthermore most classes of internal and singly external configurations are internally contracted the resulting speedups make it possible to treat much larger molecules recently explicitly correlated terms have been added to the program which strongly reduces basis set truncation errors cf section explicit correlation methods as an alternative to or the program of cimiraglia et al is available in molpro multireference configuration interaction the mrci program of werner and uses internally contracted configurations their number depends only on the number of correlated orbitals not on the number of reference configurations so very large reference spaces can be used excited states can be computed using two in the first case all states are treated together using the union of the contracted configurations from all reference states this is the most accurate method and allows the proper description of avoided crossings and conical intersections but the computational effort increases almost quadratically with the number of optimized states this bottleneck can be avoided using a projection method in which each state is optimized in turn each with only one reference state size consistency errors can be reduced either by using the davidson or pople corrections or by using the averaged coupledpair functional acpf or one of its variants e g aqcc property ccsd t ccsd t atomization energies kj mol reaction energies kj mol electron mev ionization potentials mev equilibrium distances pm vibrational frequencies cm 21 05 using the aug cc pvtz basis set an entirely new mrci program cic was recently developed by shamasundar et al using the same contraction scheme as the program the resulting method is highly complex but has been implemented using automated techniques described below cf section the integrated tensor framework despite its complexity the cic program is much more efficient than the old mrci the savings are greatest for large molecules with many inactive orbitals as typically encountered in transition metal clusters for example a calculation for the complex cu2 basis functions reference configuration state functions csfs correlated electrons took min cpu time per iteration such calculations cannot currently be performed with any other program cic is still limited to single state calculations but an extension to the multistate case is under development another important recent extension is the addition of explicitly correlated terms to mrci section explicit correlation methods explicit correlation methods one problem with conventional wave function methods is slow convergence of electron correlation energies with the size of the basis this can be circumvented using wave functions that depend explicitly on enormous progress has been made in recent years to develop so called methods molpro now contains a very wide range of methods that are very efficient robust and easy to use currently closed shell high spin open shell ccsd t and open and ms as well as mrci for and states are available local will be described in the section local correlation methods for ground states the additional effort for the corrections is small and becomes negligible for larger molecules table summarizes the typical improvements for various properties and table the part is computed using density fitting without symmetry column obs ri shows the sizes of the orbital basis set obs and the auxiliary basis set used for the resolution of the identity ri the casscf reference energies for example this reduces the difference of the mrci vdz and mrci vqz ground state dissociation energies from kcal mol to kcal mol if state averaged orbitals are used as in figure the effect is even larger the mrci methods will be made available for public use soon shows typical elapsed times the pure user cpu times are shorter as the part currently does not use symmetry while abelian symmetry can be used in the ccsd t calculation the fraction of time spent for the is larger in cases with high symmetry cf benzene than without symmetry e g ethanol cf table figure shows one dimensional cuts through the mrci potential energy surface pes of ozone as a function of one bond distance with fixed at and the bond angle at 49 figure a shows conventional mrci calculations for several basis sets and figure b shows the corresponding calculations while the conventional curves are significantly affected by the basis set in particular the dissociation energies the curves obtained with mrci and the vdz and basis sets can hardly be distinguished part of the improvement is due to a cabs singles correction of local correlation methods for ground states the steep scaling of the computational cost with molecular size is mainly due to the delocalized character of the canonical hf orbitals that are used in most conventional calculations the short range character of electron correlation can be exploited using local occupied and virtual orbitals in molpro such methods have been developed over the last years 56 it is possible to achieve linear scaling i a possible step in the synthesis of biotin ii reaction of testosterone reaction i vtz reaction ii aug cc pvtz reaction iii aug cc pvtz h cc pvtz the cbs values have been obtained by extrapolating qz results even for lccsd t 70 the local variant of a given method is denoted by prepending an l the most efficient variants of the local methods use density fitting for evaluating and transforming all twoelectron integrals currently local single reference methods are available for closed shell df and df lccsd t as well as high spin open shell df df luccsd t the open shell methods will be made available soon analytical energy gradients are available for df df and df calculations are easily possible with more than basis functions lccsd t calculations have been done with more than basis functions using augmented triple zeta basis sets a noniterative denoted as is normally used for the perturbative triples correction iterative variants are also available 71 the cpu time for df lccsd calculations is often comparable to the time needed for the preceding df hf calculation these developments have made it possible to study enzyme reactions using lccsd in qm mm calculations for a long time a major problem of local correlation methods was the error caused by the domain approximation the domain errors can be reduced by extending the domains but the cpu time increases with the fourth power of the average domain size recently it has been shown that by adding suitably defined explicitly correlated terms to or lccsd wave functions the domain errors can be eliminated almost completely 55 benchmarks for over reactions have shown that the accuracy of ccsd t and lccsd t is virtually the same the local approximations strongly reduce the scaling of computational effort with molecular size and rather large molecules can be treated approximately atoms using and atoms iii reaction step to synthesize androstendione f i g u r e three reactions studied using lccsd t with lccsd t as examples we present in table results for the three reactions shown in figure comparison of df cbs with df using triple zeta basis sets first two rows in the table shows that there are significant basis set effects for example kj mol for reaction iii furthermore comparing df with df rows and shows that there is a large domain error kj mol for reaction iii which partly cancels with the basis set error this may be due to the reduction of basis set superposition effects bsse in the local methods in contrast the and results agree with each other and also with the extrapolated cbs limit within approximately kj mol of course there is no way to compare the dflccsd calculations with canonical results but basis set and domain errors in and lccsd calculations are known to be very similar so it can be expected that the lccsd results are accurate to approximately kj mol local correlation methods for excited states singlet and triplet excited states of extended molecules can be calculated within the framework of timedependent local coupled cluster td lcc response theory at the level of the second order model density fitting is used to decompose the electron repulsion integrals due to the importance of the singles amplitudes for td cc response theory local approximations are applied to the doubles amplitudes only multistate calculations with c john wiley sons ltd volume march april wires computational molecular science molpro ω 74 ev a u f a u ω 01 ev a u f i g u r e density difference of the lowest singlet and triplet states of the molecule relative to the ground state the yellow bright and dark gray iso surfaces represent a value of and respectively the excitation energies the norms of the dipole difference vectors and the oscillator strength length gauge singlet state only are also given state specific adaptive local approximations for the individual states are possible by virtue of the laplace transform trick 79 the deviations between the local and canonical excitation energies are well below ev transition strength tensors for singlet excited states are calculated as the residues of the linear response function excited state first order properties dipole moments etc are available for singlet and triplet excited states as an alternative to the td response method also a local correlation variant of the secondorder algebraic diagrammatic construction scheme adc 80 is available even though this method has an entirely different background the working equations are closely related to those of td response however the matrix to be diagonalized for the excitation energies is hermitian in contrast to the jacobian this has advantages near conical intersections where the excitation energies become complex and for calculating properties for example calculations have been carried out on the organic solar cell sensitizer shown in figure the molecule has atoms correlated electrons and basis functions in the cc pvdz basis figure displays the calculated density difference plots for the lowest singlet and triplet excited states the corresponding excitation energies dipole moment changes and oscillator strengths are also given in agreement with experiment 83 a singlet charge transfer state with large oscillator strength is calculated as the lowest excited singlet state the computed excitation energy of 74 ev is in very good agreement with the experimental value of 71 ev a pilot im volume march april plementation of local eom ccsd is also available gradients and geometry optimization analytical energy gradients are currently available for hartree fock and dft rhf rohf uhf ks rks uks in all cases with or without density fitting single reference correlation methods df df 68 qcisd t lqcisd t ccsd as well as multiconfiguration methods mcscf casscf including state averaged mcscf ms for any other methods gradients can be computed numerically using finite differences in order to minimize the number of displacements in numerical gradient calculations symmetrical displacement coordinates are used automatic geometry optimization for minima transition states and reaction is possible for any method the optimization can be carried out using cartesian coordinates z matrix coordinates or redundant internal coordinates various optimization algorithms and hessian update schemes are implemented for details see ref optionally exact or approximate hessians can be computed at intermediate optimization steps there are facilities to linearly combine gradients so that for example counterpoise corrected geometry optimizations are possible electrical and vibrational properties many one electron properties can be computed using molpro for example multipole moments electric c john wiley sons ltd software focus wires wiley com wcms fields electric field gradients diamagnetic shielding tensors angular momenta velocity operators darwin and cowan griffin relativistic corrections and spin orbit couplings second order properties such as dipole polarizabilities can be computed analytically for hf and for other methods the finite field approach can be used for magnetic properties see section magnetic properties analytical hessians are available for closedshell hf and mcscf casscf in the latter case only without symmetry and without state averaging in all other cases hessians can be constructed automatically using finite difference methods using analytical gradients or energies harmonic vibrational frequencies and thermodynamic data are computed in the standard way anharmonic vibrational spectra can be computed using vibrational scf vscf vci or vmcscf procedures developed by rauhut and coworkers the necessary many dimensional potential energy surfaces around the equilibrium structures are computed automatically so called many mode approximations in which coupling potentials are computed at lower lever than the diagonal potentials along the normal coordinates can be used to save computation time without much loss of accuracy the pes calculation is perfectly parallelized using the molpro mppx scheme cf section parallel execution where independent processes compute the energies for different geometries magnetic properties nuclear magnetic shielding tensors can presently be calculated at the level of df hf the gaugeorigin problem is circumvented by the use of gaugeincluding atomic orbitals giaos giaos cannot be used as fitting functions since the explicit gauge origin dependence then cannot cancel there is no corresponding bra to the ket function density fitting based on a set of ordinary gaussians however works remarkably well the dependence of the chemical shifts on the size of the fitting basis set is virtually negligible and the agreement with results from conventional calculations without density fitting is typically within ppm or less the program is quite efficient and molecules with over atoms can be treated a program for correlated nuclear magnetic shielding tensors at the level of density fitted local is presently under development special relativity molpro is a schro dinger only code that deals only with ansa tze based on one component nonrelativis tic wave functions however it is possible to include some relativistic effects approximately associated with the library of basis sets there is a corresponding library of effective core potentials ecps some of which embed scalar relativistic effects as well as effective one electron potentials there is also the possibility to introduce outer product two electron potentials with the specific example of core polarization potentials for modeling core valence correlation effects it is also possible to calculate the spin orbit part of the matrix element of the breit pauli operator between casscf or mrci wave functions and these calculations are placed in an automatic framework that calculates all nonzero matrix elements arising in a given set of states and then performs degenerate perturbation theory to obtain the spin orbit shifts of the schro dinger states as an alternative to the use of ecps approximate scalar relativistic effects can be included in all electron calculations through perturbation theory or using the douglas kroll hess dkh hamiltonian density functional theory symmetry adapted perturbation theory a density functional theory symmetry adapted perturbation theory dft sapt program is also available in molpro dft sapt directly calculates the individual components of the intermolecular interaction energy between two closed shell systems that is first order electrostatic and exchange second order induction and dispersion it provides very accurate interaction energies in quality close to ccsd t the intramonomer correlation is treated at the level of dft the program employs density fitting and is very efficient some very recent applications comprise the and destabilization of dna watson crick pairs in photodamaged dna technical features input script language molpro execution is controlled by an input file that contains a script written in a custom designed language each statement of the script consists of a command followed by one or more qualifying options some of the commands run program modules that implement a particular quantum chemistry method e g rhf ccsd t mrci etc and in most cases the name of the command is identical to the standard acronym c john wiley sons ltd volume march april wires computational molecular science molpro for the method syntax checking of the whole script is performed at the start of execution other commands are built in to the language and offer features such as branching loops and data manipulation the language also supports symbolic variables that are one dimensional arrays of integer floating point logical or string type and scalars are implemented as a special case of these vectors variables can be assigned to arbitrary arithmetical expressions containing constants and other variables that are evaluated element by element down the list a typical use of variables is to store the results of a number of calculations with possible postprocessing followed by presentation tabular and graphical and export of these results built in utilities molpro contains a number of utilities to support analysis and manipulation of results molecular orbitals densities etc these include extrapolate which automatically extrapolates the last energy calculation to the basis set limit according to one of the standard formulae the merge and matrop utilities perform matrix manipulations of objects holding molecular orbitals densities and properties additionally there are utilities for performing orbital localization and mulliken natural bond orbital and distributed multipole analyses visualization and interface to other programs molpro can produce several standard format representations of geometry molecular orbital and electron density data built in commands implement the writing of data for the molden viewer which constructs pictures of orbitals and densities from the orbital basis set obs and coefficients and the writing of cube files that tabulate values of orbitals on a cuboidal grid suitable for manipulation by programs such as gopenmol an interface to the jmol interactive three dimensional widget supports the construction of interactive web pages directly from molpro simple two dimensional tables can be constructed by specifying one or more variables that will form its columns these tables can be exported to external files in a number of standard formats tables can also be presented graphically through full integration with the grace plotting system producing pdf files directly from the molpro run a recent development is a graphical application for preparing running and viewing molpro jobs volume march april this graphical user interface gui is built on the qt system making extensive use of molecular model display and other features in the avogadro library and runs on macintosh windows and linux systems the gui integrates a simple editor for input a viewer for the output stream and a visual object display for molecular models and other graphical objects symmetry all methods make use of point group symmetry up to the highest abelian subgroup except in the case of local methods and where density fitting is used parallel execution molpro is parallelized using a multiple process model at the start of a job a specified number of identical instances of the executable is started and in many parts of the program work is distributed between the processors with appropriate communication to share data and consolidate results the support for these communication functions is provided by a software layer parallel programming interface for distributed data ppidd that itself can be built to sit on top of the global arrays ga toolkit or any standard mpi library ppidd supports global data structures that can be accessed by any process without message exchange or other reference to other processes normally designated mpp parallelization is fine grained for example splitting the work for constructing a single fock matrix all functionality of the code works correctly in parallel and reasonable performance on up to tens of processors is achieved for example in dft ccsd ccsd t and mrci calculations alternatively designated mppx the program can be built so that certain large tasks for example entire energy calculations at displaced geometries for finite difference forces are each run in serial but concurrently molpro makes extensive use of storage of temporary quantities outside the memory space of a process so that calculations need not be limited by constraints in that space the abstraction layer that serves this data staging can be configured to choose from a number of different mechanisms in some computing environments it uses files that are completely shared between all processes either through the use of a diskbased shared file system such as lustre or gpfs or more efficiently in the combined global memory of all nodes and under these circumstances the appropriate functionality provided by ga or mpi is used on systems where scratch disks are local to individual nodes molpro can take advantage of the large c john wiley sons ltd software focus wires wiley com wcms aggregate i o bandwidth available by managing its own data placement on node local files with the option to either replicate and maintain synchronized items of data on all nodes or to distribute data across nodes these features are important in achieving optimum performance on a variety of computer architectures the integrated tensor framework novel hardware platforms are increasingly difficult to program efficiently and electronic structure methods are becoming more and more complex to address this situation an abstraction layer between the quantum chemistry algorithms and the hardware platforms is being developed the integrated tensor framework itf itf is a framework for handling networks of tensor contractions in an efficient manner such networks can execute the core operations of many electronic structure methods for example we implemented multireference methods coupled cluster methods ccsd analytic gradients and methods on top of it each time reaching performance rivalling the hand optimized implementation when using itf the equations are not directly programmed in a native programming language but rather in terms of an expressive high level language which is interpreted at runtime and which only encodes the very essential aspects of an evaluation algorithm implementation details are handled by the framework this approach simplifies the programming of equation systems and allows for automatic adoption of different computer platforms abstract a positive π hole is a region of positive electrostatic potential that is perpendicular to a portion of a molecular framework it is the counterpart of a σ hole which is along the extension of a covalent bond to an atom both σ holes and π holes become more positive a in going from the lighter to the heavier atoms in a given group of the periodic table and b as the remainder of the molecule is more electronwithdrawing positive σ and π holes can interact in a highly directional manner with negative sites e g the lone pairs of lewis bases in this work the complexes of π holecontaining molecules with the nitrogen lone pairs of hcn and have been characterized computationally using the and procedures while the electroj s murray p politzer clevetheocomp w street suite cleveland oh usa e mail jsmurray uno edu p lane department of chemistry university of new orleans new orleans la usa t clark computer chemie centrum and interdisciplinary center for molecular materials friedrich alexander universität erlangen nürnberg nägelsbachstraße 91052 erlangen germany t clark centre for molecular design university of portsmouth mercantile house portsmouth uk k e riley department of chemistry university of puerto rico p o box rio piedras pr usa static interaction is a major driving force in π hole bonding a gradation is found from weakly noncovalent to considerably stronger with possible indications of some degree of coordinate covalency keywords electrostatic potentials interaction energies π holes σ holes σ holes the term σ hole was introduced by clark et al to describe the regions of positive electrostatic potential that are present on the outer surfaces of many covalentlybonded halogens these positive regions discovered by brinck et al in can interact electrostatically with negative sites on the same or more often other molecules e g lone pairs and π electrons giving rise to noncovalent halogen bonding a σ hole is formed when a halogen atom participates in a covalent sigma bond the accompanying rearrangement of the atom electronic density typically leaves a region of diminished negative charge on its outer non involved side this region which is along the extension of the bond is called a σ hole such charge anisotropy has indeed long been recognized 12 when it is sufficient a region of positive electrostatic potential results a positive σ hole although the σ hole concept was originally used to help explain the seeming anomaly of an electronegative halogen interacting attractively with a negative site it has since been found to be applicable as well to covalentlybonded atoms in groups vi v and iv interactions between these regions of positive electrostatic potential positive σ holes and negative sites on the same or another molecular system are labeled σ hole bonding halogen bonding is a subset of this σ holes are normally concentrated along the extensions of the covalent bonds to an atom this can be seen in fig for it has two positive σ holes on the selenium and one on each chlorine on the extensions of the cl se and se cl bonds respectively accordingly the resulting interactions tend to be highly directional for a σ hole bonded complex r y z where z is the negative site the angle r y z is usually near barring secondary interactions within any one of the groups iv vii σ holes become more positive and their interactions stronger a in going from the lighter to the heavier elements as polarizability increases and electronegativity decreases and also b as the remainder of the molecule becomes more electron withdrawing 5 16 thus the σ holes on the selenium in are more positive than those on the sulfur in but less positive than the selenium ones in for a given negative site σ hole bonding interaction energies have been shown to correlate well with the magnitudes of the positive σ hole electrostatic potentials it should be noted however that σ holes need not always be positive if the charge anisotropy is not sufficient a negative σ hole can result this is found for example on the fluorine in f for more extensive discussions of σ holes and their interactions see politzer et al 5 and murray et al j mol model hydrolysis other candidates for positive π holes might include and if the π bonding electrons are drawn sufficiently toward the oxygens so that there are positive potentials above the sulfur and selenium indeed fig shows that there is a positive electrostatic potential a positive π hole above and below the selenium in will this interact attractively with a negative site our objective in this work has been to test for the presence of π holes in a series of molecules of different types and to investigate their interactions with the nitrogen lone pairs in hcn and the first step is to compute the electrostatic potential on an appropriate outer surface of each molecule of interest the electrostatic potential the nuclei and electrons of a molecule create an electrostatic potential v r in the surrounding space given by eq z x za vðrþ ð1þ j r a rj j rj a the suggestion has been made earlier that some molecules may exhibit positive π holes 5 regions of low electronic density that are perpendicular to portions of a molecular framework instead of being along the extensions of bonds as are σ holes it was shown already some time ago that there are positive electrostatic potentials above the acyl carbons in c o f and c o 21 which correlate with their relative tendencies to undergo za is the charge on nucleus a located at ra and ρ r is the electronic density v r is a physical observable which can be determined experimentally by diffraction methods as well as computationally the sign of v r in any region depends upon whether the positive effect of the nuclei or the negative one of the electrons is dominant there the electrostatic potential is an effective means for analyzing and predicting noncovalent interactions which are largely electrostatically driven for this purpose v r is commonly computed on an appropriate outer surface of the molecule since this is what will be seen by other approaching species following the suggestion of bader et al the au elec fig computed electrostatic potential on the au molecular surface of two views are shown left the selenium is in the foreground right the chlorines are pointing to the left and right color ranges in kcal mol are red greater than yellow from to green from to blue less than negative the positions of the most positive electrostatic potentials associated with the σ holes on the selenium and the chlorines are shown as black hemispheres vs max se 5 kcal mol vs max cl kcal mol π holes j mol model results electrostatic potentials fig computed electrostatic potential on the au molecular surface of the selenium is in the middle color ranges in kcal mol are red greater than yellow from to green from to blue less than negative the position of the most positive electrostatic potential associated with the π hole above and below the selenium is indicated by a black hemisphere vs max kcal mol trons contour of the molecule s electronic density is frequently taken to be the surface this has the advantage of being specific to the particular molecule and thus reflecting features such as lone pairs π electrons and strained bonds the au contour normally lies beyond the van der waals radii of the main group atoms except for hydrogen which makes it suitable for noncovalent interactions for each of the molecules in table except for regions of positive electrostatic potential with vs max are found above and below the central atom these are what we label positive π holes in and the phosphorus is considered to be the central atom the πholes of and are displayed in figs and the vs max associated with the π holes are given in table their magnitudes are comparable to and in some cases considerably exceed those of σ holes 5 16 as with the latter π hole vs max become more positive in going from the lighter to the heavier atoms in a given column of the periodic table compare and note that does not even have a π hole whereas has quite a strong one the vs max also become more positive as the remainder of the molecule is more electron withdrawing compare and the sizable difference between the π hole vs max of and is probably due to overlapping of the positive π hole potential and the positive potentials of the nearby methyl hydrogens in complexes with hcn and procedure we have computed the electrostatic potentials on the au surfaces of the molecules listed in table the g d p procedure was used to allow comparisons with earlier calculated σ hole potentials 5 16 18 28 the wavefunctions were obtained with gaussian 29 and the surface potentials labeled vs r with the wave function analysis surface analysis suite the latter code gives the magnitudes and positions of the locally most positive and most negative values of vs r designated vs max and vs min there can be more than one of each on any given molecular surface the geometries and interaction energies δe of the complexes formed between the molecules in table and the lewis bases hcn and were determined by three different computational techniques and in conjunction with the aug cc pvdz basis set the interaction energies were obtained from the molecular energy minima at k with eq δe ¼ eðcomplexþ eðp hole moleculeþ eðlewis baseþ in view of the large basis set being used basis set superposition error should be minimal and was accordingly not considered in tables and are some key properties of the complexes formed by the molecules in table except with the nitrogen lone pairs of hcn and as computed by the different procedures mentioned above the and the table computed electrostatic potential maxima vs max on au molecular surfaces a above and below indicated atom π holes molecule atom with π hole vs max kcal mol h2sio c c c si si n p p p b 8 5 58 seo2 central o s se 32 9 2 2 a b g d p the surface above and below the carbon is positive but there is no vs max present fig computed electrostatic potential on the au molecular surface of the phosphorus is in the middle color ranges in kcal mol are red greater than yellow from to green from to blue less than negative the position of the most positive electrostatic potential associated with the π hole on the side of the phosphorus that is shown is indicated by a black hemisphere vs max 5 kcal mol usually predict somewhat stronger binding more negative δe than does the the method is known to give reasonably accurate binding energies and interaction geometries for a wide variety of complex types when used along with a medium sized basis set such as cc pvtz or aug cc pvdz 35 thus results obtained using the aug cc pvdz method should be seen as the reference values in the current study although these data cannot be said to be of benchmark quality most dft methods including are considered to not properly describe dispersion interactions within molecular complexes the 2x functional however does account for dispersion at least semiquantitatively and has been found to be effective for studies involving noncovalent interactions although with a tendency for overbinding in some instances 37 the most likely explanation for the underbinding seen for the aug cc pvdz method in tables 2 and as well as table 4 is its inadequate treatment of dispersion which certainly plays some role in stabilizing these complexes even though the primary driving forces for these interactions as will be demonstrated are electrostatic the interaction of a given molecule with is invariably more stabilizing than with hcn this is evident in the more negative δe and the usually shorter separations observed with one reason for this is that the electric field of the nitrogen lone pair in is considerably stronger and more polarizing than that in hcn which enhances the electrostatic interactions of the former comparison of the vs max in table with the δe in tables 2 and shows that within groups iv v and vi taken separately the interaction energies with both hcn and become generally more negative more stabilizing j mol model 18 as the π hole vs max increases this reflects the importance of the electrostatic factor in π hole interactions the δe in tables 2 and cover a remarkably large range while many are between and 12 kcal mol as is expected for noncovalent interactions some especially among the complexes with are considerably more negative with values of to kcal mol these surprisingly strong interactions are reflected in the ratios of the separations to the sums of the van der waals radii of the respective atoms which are as low as 5 to it seems evident that some of these complexes are beyond what is expected of noncovalent interactions this conclusion is buttressed by their relatively large interaction angles tables 2 and since the molecules in table are all planar except for the methyl hydrogens in the π hole concept implies that these angles should be about and this is indeed true of some of them deviations of a few degrees can be attributed to secondary interactions for example the o c n angle of about in o reflects some degree of repulsion between the oxygen and the nitrogen lone pair analogous explanations can be given in other instances of positive deviations from in nch and the and m06 2x interaction angles are significantly less than this can be rationalized by noting that the o n separations predicted by these methods are small enough to allow the outer oxygens in to interact attractively with the carbon in hcn and one of the hydrogens in the o n separations are much greater and the o c and o h interactions consequently appear to not be important while secondary interactions can explain many of the observed angular deviations from the larger ones may reflect an additional factor a number of the complexes have interaction angles in the immediate vicinity of or even larger as much as these tend to be the same systems that have the most negative δe and the low values of the ratio of separation to sum of van der waals radii mentioned above in each instance the atom having the π hole is a silicon or a phosphorus these complexes may have some degree of coordinate covalent character with the hcn or nitrogen sharing its lone pair to some extent the atom that has the π hole changes from a trigonal to a quasi tetrahedral configuration a coordinate covalent contribution to the interaction would be favored for over hcn the average local ionization energy of the nitrogen lone pair in is 7 ev at the 6 g d p level compared to 10 7 ev for hcn accordingly the lone pair electrons are more readily available thus the stronger interactions found for can perhaps be explained in terms of both electrostatic see above and coordinate covalent considerations gradations in π hole and σ hole bonding what we see in tables 2 and is that there is a gradation in the interactions between molecules having π holes and lewis bases they can range from quite weak with separations near the sums of the respective van der waals radii and little or no effect upon the π hole molecule s geometry to relatively strong with small separations and significant distortion what determines where a given complex will fit into this spectrum of interactions the results in tables 2 and indicate that the stronger interactions are more likely when the π hole is on a relatively large atom e g silicon or phosphorus such atoms are more polarizable and less electronegative both of which lead to a larger initial vs max high polarizability also increases the response of the atom to the electric field of the lewis base in addition a secondrow or larger atom can better accommodate a close approach of the base in a possible increase in coordination in orbital terminology the atom can become hypervalent finally the chance of some degree of coordinate covalency is enhanced by the lone pair electrons of the base having low ionization energies and by the π hole molecule having the capacity to partially share them which is related to its overall polarizability analogous considerations are applicable to interactions with σ holes as well as with π holes we have seen examples in earlier work involving σ holes on covalentlybonded silicon for instance in the formation of complexes between and amines 41 and in an intramolecular rearrangement of a silicon nitrate ester note also some of the silyl halide and silyl halide systems studied by ignatyev and schaefer recently del bene et al have found a very marked gradation of halogen σ hole bond strengths in a series of systems of the type fcl cnx where x represents various atoms or groups selected to give cnx a range of basicities an interesting example of gradations in bonding is provided by the complexes of and with hcn and nh bf and bcl clearly have positive π holes corresponding to the unoccupied boron orbitals the 6 g d p vs max are 8 kcal mol for and kcal mol for the difference reflects the greater electron withdrawing power of the fluorines table 4 shows that there is quite a contrast between the complexes that and form with hcn and those with with hcn the interactions are fairly weak that of is stronger with δe about 7 kcal mol vs 4 kcal mol for due to the much more positive vs max of with on the other hand the complexes of both and are as strongly bound as some of those involving silicon and phosphorus derivatives in table they have δe between and 31 kcal mol short separations and interaction angles of 105 this may seem inconsistent with the earlier statement that strong interactions are more likely with larger more polarizable atoms but it should be noted that boron is near the beginning of the first row and thus significantly larger and more polarizable than the atoms that follow it the polarizability of boron 03 is closer to that of phosphorus 63 than to that of carbon 76 the greater stability of the complexes compared to the hcn can be explained by the stronger electric field and lower ionization energy of the lone pair electrons what is notable however is that δe is slightly more negative for than for since has a much more positive vs max than does it appears that some additional factor is involved brinck et al suggested that this other factor is the higher charge capacity of by virtue of its larger polarizability 9 38 vs 3 31 is better able to accept some share of the lone pair conclusions the concept of σ holes has been extended by demonstrating the existence of π holes and showing that they have analogous properties the interactions of a series of molecules having π holes with the nitrogen lone pairs of hcn and have been analyzed and described computationally a major driving force in both σ hole and π hole bonding is the electrostatic interaction between the positive σ or π hole and the negative site however both types of bonds can show a significant gradation ranging from weak noncovalent and largely electrostatic to considerably stronger with evidence of some coordinate covalent character acknowledgments tc gratefully acknowledges the generous support of the deutsche forschungsgemeinschaft as part of sonderforschungsbereich redox active metal complexes control of reactivity in molecular architecture and ker the nsf national science foundation epscor experimental program to stimulate competitive research program grant number and the nsf prem partnership for research education in materials program grant number dmr 