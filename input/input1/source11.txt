process control introduction we now turn to the process control provided by the unix system this includes the creation of new processes program execution and process termination we also look at the various ids that are the property of the process real effective and saved user and group ids and how they re affected by the process control primitives interpreter files and the system function are also covered we conclude the chapter by looking at the process accounting provided by most unix systems this lets us look at the process control functions from a different perspective process identifiers every process has a unique process id a non negative integer because the process id is the only well known identifier of a process that is always unique it is often used as a piece of other identifiers to guarantee uniqueness for example applications sometimes include the process id as part of a filename in an attempt to generate unique filenames although unique process ids are reused as processes terminate their ids become candidates for reuse most unix systems implement algorithms to delay reuse however so that newly created processes are assigned ids different from those used by processes that terminated recently this prevents a new process from being mistaken for the previous process to have used the same id there are some special processes but the details differ from implementation to implementation process id is usually the scheduler process and is often known as the swapper no program on disk corresponds to this process which is part of the kernel and is known as a system process process id is usually the init process and is invoked by the kernel at the end of the bootstrap procedure the program file for this process was etc init in older versions of the unix system and is sbin init in newer versions this process is responsible for bringing up a unix system after the kernel has been bootstrapped init usually reads the system dependent initialization files the etc rc files or etc inittab and the files in etc init d and brings the system to a certain state such as multiuser the init process never dies it is a normal user process not a system process within the kernel like the swapper although it does run with superuser privileges later in this chapter we ll see how init becomes the parent process of any orphaned child process in mac os x the init process was replaced with the launchd process which performs the same set of tasks as init but has expanded functionality see section in singh for a discussion of how launchd operates each unix system implementation has its own set of kernel processes that provide operating system services for example on some virtual memory implementations of the unix system process id is the pagedaemon this process is responsible for supporting the paging of the virtual memory system in addition to the process id there are other identifiers for every process the following functions return these identifiers note that none of these functions has an error return we ll return to the parent process id in the next section when we discuss the fork function the real and effective user and group ids were discussed in section fork function an existing process can create a new one by calling the fork function the new process created by fork is called the child process this function is called once but returns twice the only difference in the returns is that the return value in the child is whereas the return value in the parent is the process id of the new child the reason the child process id is returned to the parent is that a process can have more than one child and there is no function that allows a process to obtain the process ids of its children the reason fork returns to the child is that a process can have only a single parent and the child can always call getppid to obtain the process id of its parent process id is reserved for use by the kernel so it not possible for to be the process id of a child both the child and the parent continue executing with the instruction that follows the call to fork the child is a copy of the parent for example the child gets a copy of the parent data space heap and stack note that this is a copy for the child the parent and the child do not share these portions of memory the parent and the child do share the text segment however section modern implementations don t perform a complete copy of the parent data stack and heap since a fork is often followed by an exec instead a technique called copy on write cow is used these regions are shared by the parent and the child and have their protection changed by the kernel to read only if either process tries to modify these regions the kernel then makes a copy of that piece of memory only typically a page in a virtual memory system section of bach and sections and of mckusick et al provide more detail on this feature variations of the fork function are provided by some platforms all four platforms discussed in this book support the vfork variant discussed in the next section linux also provides new process creation through the clone system call this is a generalized form of fork that allows the caller to control what is shared between parent and child freebsd provides the rfork system call which is similar to the linux clone system call the rfork call is derived from the plan operating system pike et al solaris provides two threads libraries one for posix threads pthreads and one for solaris threads in previous releases the behavior of fork differed between the two thread libraries for posix threads fork created a process containing only the calling thread but for solaris threads fork created a process containing copies of all threads from the process of the calling thread in solaris this behavior has changed fork creates a child containing a copy of the calling thread only regardless of which thread library is used solaris also provides the function which can be used to create a process that duplicates only the calling thread and the forkall function which can be used to create a process that duplicates all the threads in the process threads are discussed in detail in chapters and in general we never know whether the child starts executing before the parent or vice versa the order depends on the scheduling algorithm used by the kernel if it required that the child and parent synchronize their actions some form of interprocess communication is required in the program shown in figure we simply have the parent put itself to sleep for seconds to let the child execute there is no guarantee that the length of this delay is adequate and we talk about this and other types of synchronization in section when we discuss race conditions in section we show how to use signals to synchronize a parent and a child after a fork when we write to standard output we subtract from the size of buf to avoid writing the terminating null byte although strlen will calculate the length of a string not including the terminating null byte sizeof calculates the size of the buffer which does include the terminating null byte another difference is that using strlen requires a function call whereas sizeof calculates the buffer length at compile time as the buffer is initialized with a known string and its size is fixed note the interaction of fork with the i o functions in the program in figure recall from chapter that the write function is not buffered because write is called before the fork its data is written once to standard output the standard i o library however is buffered recall from section that standard output is line buffered if it connected to a terminal device otherwise it fully buffered when we run the program interactively we get only a single copy of the first printf line because the standard output buffer is flushed by the newline when we redirect standard output to a file however we get two copies of the printf line in this second case the printf before the fork is called once but the line remains in the buffer when fork is called this buffer is then copied into the child when the parent data space is copied to the child both the parent and the child now have a standard i o buffer with this line in it the second printf right before the exit just appends its data to the existing buffer when each process terminates its copy of the buffer is finally flushed file sharing when we redirect the standard output of the parent from the program in figure the child standard output is also redirected indeed one characteristic of fork is that all file descriptors that are open in the parent are duplicated in the child we say duplicated because it as if the dup function had been called for each descriptor the parent and the child share a file table entry for every open descriptor recall figure consider a process that has three different files opened for standard input standard output and standard error on return from fork we have the arrangement shown in figure it is important that the parent and the child share the same file offset consider a process that forks a child then waits for the child to complete assume that both processes write to standard output as part of their normal processing if the parent has its standard output redirected by a shell perhaps it is essential that the parent file offset be updated by the child when the child writes to standard output in this case the child can write to standard output while the parent is waiting for it on completion of the child the parent can continue writing to standard output knowing that its output will be appended to whatever the child wrote if the parent and the child did not share the same file offset this type of interaction would be more difficult to accomplish and would require explicit actions by the parent parent process table entry file table v node table figure sharing of open files between parent and child after fork if both parent and child write to the same descriptor without any form of synchronization such as having the parent wait for the child their output will be intermixed assuming it a descriptor that was open before the fork although this is possible we saw it in figure it not the normal mode of operation there are two normal cases for handling the descriptors after a fork the parent waits for the child to complete in this case the parent does not need to do anything with its descriptors when the child terminates any of the shared descriptors that the child read from or wrote to will have their file offsets updated accordingly both the parent and the child go their own ways here after the fork the parent closes the descriptors that it doesn t need and the child does the same thing this way neither interferes with the other open descriptors this scenario is often found with network servers besides the open files numerous other properties of the parent are inherited by the child real user id real group id effective user id and effective group id supplementary group ids process group id session id controlling terminal the set user id and set group id flags current working directory root directory file mode creation mask signal mask and dispositions the close on exec flag for any open file descriptors environment attached shared memory segments memory mappings resource limits the differences between the parent and child are the return values from fork are different the process ids are different the two processes have different parent process ids the parent process id of the child is the parent the parent process id of the parent doesn t change the child and values are set to these times are discussed in section file locks set by the parent are not inherited by the child pending alarms are cleared for the child the set of pending signals for the child is set to the empty set many of these features haven t been discussed yet we ll cover them in later chapters the two main reasons for fork to fail are a if too many processes are already in the system which usually means that something else is wrong or b if the total number of processes for this real user id exceeds the system limit recall from figure that specifies the maximum number of simultaneous processes per real user id there are two uses for fork when a process wants to duplicate itself so that the parent and the child can each execute different sections of code at the same time this is common for network servers the parent waits for a service request from a client when the request arrives the parent calls fork and lets the child handle the request the parent goes back to waiting for the next service request to arrive when a process wants to execute a different program this is common for shells in this case the child does an exec which we describe in section right after it returns from the fork some operating systems combine the operations from step a fork followed by an exec into a single operation called a spawn the unix system separates the two as there are numerous cases where it is useful to fork without doing an exec also separating the two operations allows the child to change the per process attributes between the fork and the exec such as i o redirection user id signal disposition and so on we ll see numerous examples of this in chapter the single unix specification does include spawn interfaces in the advanced real time option group these interfaces are not intended to be replacements for fork and exec however they are intended to support systems that have difficulty implementing fork efficiently especially systems without hardware support for memory management vfork function the function vfork has the same calling sequence and same return values as fork but the semantics of the two functions differ the vfork function originated with some consider the function a blemish but all the platforms covered in this book support it in fact the bsd developers removed it from the release but all the open source bsd distributions that derive from added support for it back into their own releases the vfork function was marked as an obsolescent interface in version of the single unix specification and was removed entirely in version we include it here for historical reasons only portable applications should not use it the vfork function was intended to create a new process for the purpose of executing a new program step at the end of the previous section similar to the method used by the bare bones shell from figure the vfork function creates the new process just like fork without copying the address space of the parent into the child as the child won t reference that address space the child simply calls exec or exit right after the vfork instead the child runs in the address space of the parent until it calls either exec or exit this optimization is more efficient on some implementations of the unix system but leads to undefined results if the child modifies any data except the variable used to hold the return value from vfork makes function calls or returns without calling exec or exit as we mentioned in the previous section implementations use copy on write to improve the efficiency of a fork followed by an exec but no copying is still faster than some copying another difference between the two functions is that vfork guarantees that the child runs first until the child calls exec or exit when the child calls either of these functions the parent resumes this can lead to deadlock if the child depends on further actions of the parent before calling either of these two functions example the program in figure is a modified version of the program from figure we ve replaced the call to fork with vfork and removed the write to standard output also we don t need to have the parent call sleep as we re guaranteed that it is put to sleep by the kernel until the child calls either exec or exit pid glob var here the incrementing of the variables done by the child changes the values in the parent because the child runs in the address space of the parent this doesn t surprise us this behavior however differs from the behavior of fork note in figure that we call instead of exit as we described in section does not perform any flushing of standard i o buffers if we call exit instead the results are indeterminate depending on the implementation of the standard i o library we might see no difference in the output or we might find that the output from the first printf in the parent has disappeared if the child calls exit the implementation flushes the standard i o streams if this is the only action taken by the library then we will see no difference from the output generated if the child called if the implementation also closes the standard i o streams however the memory representing the file object for the standard output will be cleared out because the child is borrowing the parent address space when the parent resumes and calls printf no output will appear and printf will return note that the parent is still valid as the child gets a copy of the parent file descriptor array refer back to figure most modern implementations of exit do not bother to close the streams because the process is about to exit the kernel will close all the file descriptors open in the process closing them in the library simply adds overhead without any benefit section of mckusick et al contains additional information on the implementation issues of fork and vfork exercises and continue the discussion of vfork exit functions as we described in section a process can terminate normally in five ways executing a return from the main function as we saw in section this is equivalent to calling exit calling the exit function this function is defined by iso c and includes the calling of all exit handlers that have been registered by calling atexit and closing all standard i o streams because iso c does not deal with file descriptors multiple processes parents and children and job control the definition of this function is incomplete for a unix system calling the or function iso c defines to provide a way for a process to terminate without running exit handlers or signal handlers whether standard i o streams are flushed depends on the implementation on unix systems and are synonymous and do not flush standard i o streams the function is called by exit and handles the unix system specific details is specified by posix in most unix system implementations exit is a function in the standard c library whereas is a system call executing a return from the start routine of the last thread in the process the return value of the thread is not used as the return value of the process however when the last thread returns from its start routine the process exits with a termination status of calling the function from the last thread in the process as with the previous case the exit status of the process in this situation is always regardless of the argument passed to we ll say more about in section the three forms of abnormal termination are as follows calling abort this is a special case of the next item as it generates the sigabrt signal when the process receives certain signals we describe signals in more detail in chapter the signal can be generated by the process itself e g by calling the abort function by some other process or by the kernel examples of signals generated by the kernel include the process referencing a memory location not within its address space or trying to divide by the last thread responds to a cancellation request by default cancellation occurs in a deferred manner one thread requests that another be canceled and sometime later the target thread terminates we discuss cancellation requests in detail in sections and regardless of how a process terminates the same code in the kernel is eventually executed this kernel code closes all the open descriptors for the process releases the memory that it was using and so on for any of the preceding cases we want the terminating process to be able to notify its parent how it terminated for the three exit functions exit and this is done by passing an exit status as the argument to the function in the case of an abnormal termination however the kernel not the process generates a termination status to indicate the reason for the abnormal termination in any case the parent of the process can obtain the termination status from either the wait or the waitpid function described in the next section note that we differentiate between the exit status which is the argument to one of the three exit functions or the return value from main and the termination status the exit status is converted into a termination status by the kernel when is finally called recall figure figure describes the various ways the parent can examine the termination status of a child if the child terminated normally the parent can obtain the exit status of the child when we described the fork function it was obvious that the child has a parent process after the call to fork now we re talking about returning a termination status to the parent but what happens if the parent terminates before the child the answer is that the init process becomes the parent process of any process whose parent terminates in such a case we say that the process has been inherited by init what normally happens is that whenever a process terminates the kernel goes through all active processes to see whether the terminating process is the parent of any process that still exists if so the parent process id of the surviving process is changed to be the process id of init this way we re guaranteed that every process has a parent another condition we have to worry about is when a child terminates before its parent if the child completely disappeared the parent wouldn t be able to fetch its termination status when and if the parent was finally ready to check if the child had terminated the kernel keeps a small amount of information for every terminating process so that the information is available when the parent of the terminating process calls wait or waitpid minimally this information consists of the process id the termination status of the process and the amount of cpu time taken by the process the kernel can discard all the memory used by the process and close its open files in unix system terminology a process that has terminated but whose parent has not yet waited for it is called a zombie the ps command prints the state of a zombie process as z if we write a long running program that forks many child processes they become zombies unless we wait for them and fetch their termination status some systems provide ways to prevent the creation of zombies as we describe in section the final condition to consider is this what happens when a process that has been inherited by init terminates does it become a zombie the answer is no because init is written so that whenever one of its children terminates init calls one of the wait functions to fetch the termination status by doing this init prevents the system from being clogged by zombies when we say one of init children we mean either a process that init generates directly such as getty which we describe in section or a process whose parent has terminated and has been subsequently inherited by init wait and waitpid functions when a process terminates either normally or abnormally the kernel notifies the parent by sending the sigchld signal to the parent because the termination of a child is an asynchronous event it can happen at any time while the parent is running this signal is the asynchronous notification from the kernel to the parent the parent can choose to ignore this signal or it can provide a function that is called when the signal occurs a signal handler the default action for this signal is to be ignored we describe these options in chapter for now we need to be aware that a process that calls wait or waitpid can block if all of its children are still running return immediately with the termination status of a child if a child has terminated and is waiting for its termination status to be fetched return immediately with an error if it doesn t have any child processes if the process is calling wait because it received the sigchld signal we expect wait to return immediately but if we call it at any random point in time it can block the differences between these two functions are as follows the wait function can block the caller until a child process terminates whereas waitpid has an option that prevents it from blocking the waitpid function doesn t wait for the child that terminates first it has a number of options that control which process it waits for if a child has already terminated and is a zombie wait returns immediately with that child status otherwise it blocks the caller until a child terminates if the caller blocks and has multiple children wait returns when one terminates we can always tell which child terminated because the process id is returned by the function for both functions the argument statloc is a pointer to an integer if this argument is not a null pointer the termination status of the terminated process is stored in the location pointed to by the argument if we don t care about the termination status we simply pass a null pointer as this argument traditionally the integer status that these two functions return has been defined by the implementation with certain bits indicating the exit status for a normal return other bits indicating the signal number for an abnormal return one bit indicating whether a core file was generated and so on posix specifies that the termination status is to be looked at using various macros that are defined in sys wait h four mutually exclusive macros tell us how the process terminated and they all begin with wif based on which of these four macros is true other macros are used to obtain the exit status signal number and the like the four mutually exclusive macros are shown in figure macro description wifexited status true if status was returned for a child that terminated normally in this case we can execute wexitstatus status to fetch the low order bits of the argument that the child passed to exit or wifsignaled status true if status was returned for a child that terminated abnormally by receipt of a signal that it didn t catch in this case we can execute wtermsig status to fetch the signal number that caused the termination additionally some implementations but not the single unix specification define the macro wcoredump status that returns true if a core file of the terminated process was generated wifstopped status true if status was returned for a child that is currently stopped in this case we can execute wstopsig status to fetch the signal number that caused the child to stop wifcontinued status true if status was returned for a child that has been continued after a job control stop xsi option waitpid only figure macros to examine the termination status returned by wait and waitpid we ll discuss how a process can be stopped in section when we discuss job control example the function in figure uses the macros from figure to print a description of the termination status we ll call this function from numerous programs in the text note that this function handles the wcoredump macro if it is defined freebsd linux mac os x and solaris all support the wcoredump macro however some platforms hide its definition if the constant is defined recall section the program shown in figure calls the function demonstrating the various values for the termination status if we run the program in figure we get a out normal termination exit status abnormal termination signal number core file generated abnormal termination signal number core file generated for now we print the signal number from wtermsig we can look at the signal h header to verify that sigabrt has a value of and that sigfpe has a value of we ll see a portable way to map a signal number to a descriptive name in section as we mentioned if we have more than one child wait returns on termination of any of the children but what if we want to wait for a specific process to terminate assuming we know which process id we want to wait for in older versions of the unix system we would have to call wait and compare the returned process id with the one we re interested in if the terminated process wasn t the one we wanted we would have to save the process id and termination status and call wait again we would need to continue doing this until the desired process terminated the next time we wanted to wait for a specific process we would go through the list of already terminated processes to see whether we had already waited for it and if not call wait include apue h include sys wait h int main void pid int status if pid fork fork error else if pid child exit if wait status pid wait for child wait error status and print its status if pid fork fork error else if pid child abort generates sigabrt if wait status pid wait for child wait error status and print its status if pid fork fork error else if pid child status divide by generates sigfpe if wait status pid wait for child wait error status and print its status exit figure demonstrate various exit statuses again what we need is a function that waits for a specific process this functionality and more is provided by the posix waitpid function the interpretation of the pid argument for waitpid depends on its value pid waits for any child process in this respect waitpid is equivalent to wait pid waits for the child whose process id equals pid pid waits for any child whose process group id equals that of the calling process we discuss process groups in section pid waits for any child whose process group id equals the absolute value of pid the waitpid function returns the process id of the child that terminated and stores the child termination status in the memory location pointed to by statloc with wait the only real error is if the calling process has no children another error return is possible in case the function call is interrupted by a signal we ll discuss this in chapter with waitpid however it also possible to get an error if the specified process or process group does not exist or is not a child of the calling process the options argument lets us further control the operation of waitpid this argument either is or is constructed from the bitwise or of the constants in figure freebsd and solaris support one additional but nonstandard option constant wnowait has the system keep the process whose termination status is returned by waitpid in a wait state so that it may be waited for again constant description wcontinued if the implementation supports job control the status of any child specified by pid that has been continued after being stopped but whose status has not yet been reported is returned xsi option wnohang the waitpid function will not block if a child specified by pid is not immediately available in this case the return value is wuntraced if the implementation supports job control the status of any child specified by pid that has stopped and whose status has not been reported since it has stopped is returned the wifstopped macro determines whether the return value corresponds to a stopped child process figure the options constants for waitpid the waitpid function provides three features that aren t provided by the wait function the waitpid function lets us wait for one particular process whereas the wait function returns the status of any terminated child we ll return to this feature when we discuss the popen function the waitpid function provides a nonblocking version of wait there are times when we want to fetch a child status but we don t want to block the waitpid function provides support for job control with the wuntraced and wcontinued options example recall our discussion in section about zombie processes if we want to write a process so that it forks a child but we don t want to wait for the child to complete and we don t want the child to become a zombie until we terminate the trick is to call fork twice the program in figure does this we call sleep in the second child to ensure that the first child terminates before printing the parent process id after a fork either the parent or the child can continue executing we never know which will resume execution first if we didn t put the second child to sleep and if it resumed execution after the fork before its parent the parent process id that it printed would be that of its parent not process id executing the program in figure gives us a out second child parent pid note that the shell prints its prompt when the original process terminates which is before the second child prints its parent process id waitid function the single unix specification includes an additional function to retrieve the exit status of a process the waitid function is similar to waitpid but provides extra flexibility like waitpid waitid allows a process to specify which children to wait for instead of encoding this information in a single argument combined with the process id or process group id two separate arguments are used the id parameter is interpreted based on the value of idtype the types supported are summarized in figure constant description wait for a particular process id contains the process id of the child to wait for wait for any child process in a particular process group id contains the process group id of the children to wait for wait for any child process id is ignored figure the idtype constants for waitid the options argument is a bitwise or of the flags shown in figure these flags indicate which state changes the caller is interested in constant description wcontinued wait for a process that has previously stopped and has been continued and whose status has not yet been reported wexited wait for processes that have exited wnohang return immediately instead of blocking if there is no child exit status available wnowait don t destroy the child exit status the child exit status can be retrieved by a subsequent call to wait waitid or waitpid wstopped wait for a process that has stopped and whose status has not yet been reported figure the options constants for waitid at least one of wcontinued wexited or wstopped must be specified in the options argument the infop argument is a pointer to a siginfo structure this structure contains detailed information about the signal generated that caused the state change in the child process the siginfo structure is discussed further in section of the four platforms covered in this book only linux mac os x and solaris provide support for waitid note however that mac os x doesn t set all the information we expect in the siginfo structure and functions most unix system implementations provide two additional functions and historically these two variants descend from the bsd branch of the unix system the only feature provided by these two functions that isn t provided by the wait waitid and waitpid functions is an additional argument that allows the kernel to return a summary of the resources used by the terminated process and all its child processes the resource information includes such statistics as the amount of user cpu time amount of system cpu time number of page faults number of signals received and the like refer to the getrusage manual page for additional details this resource information differs from the resource limits we described in section figure details the various arguments supported by the wait functions function pid options rusage posix freebsd linux mac os x solaris wait waitid waitpid figure arguments supported by wait functions on various systems the function was included in earlier versions of the single unix specification in version was moved to the legacy category was removed from the specification in version race conditions for our purposes a race condition occurs when multiple processes are trying to do something with shared data and the final outcome depends on the order in which the processes run the fork function is a lively breeding ground for race conditions if any of the logic after the fork either explicitly or implicitly depends on whether the parent or child runs first after the fork in general we cannot predict which process runs first even if we knew which process would run first what happens after that process starts running depends on the system load and the kernel scheduling algorithm we saw a potential race condition in the program in figure when the second child printed its parent process id if the second child runs before the first child then its parent process will be the first child but if the first child runs first and has enough time to exit then the parent process of the second child is init even calling sleep as we did guarantees nothing if the system was heavily loaded the second child could resume after sleep returns before the first child has a chance to run problems of this form can be difficult to debug because they tend to work most of the time a process that wants to wait for a child to terminate must call one of the wait functions if a process wants to wait for its parent to terminate as in the program from figure a loop of the following form could be used while getppid sleep the problem with this type of loop called polling is that it wastes cpu time as the caller is awakened every second to test the condition to avoid race conditions and to avoid polling some form of signaling is required between multiple processes signals can be used for this purpose and we describe one way to do this in section various forms of interprocess communication ipc can also be used we ll discuss some of these options in chapters and for a parent and child relationship we often have the following scenario after the fork both the parent and the child have something to do for example the parent could update a record in a log file with the child process id and the child might have to create a file for the parent in this example we require that each process tell the other when it has finished its initial set of operations and that each wait for the other to complete before heading off on its own the following code illustrates this scenario include apue h set things up for if pid fork fork error else if pid child child does whatever is necessary getppid tell parent we re done and wait for parent and the child continues on its way exit parent does whatever is necessary pid tell child we re done and wait for child and the parent continues on its way exit we assume that the header apue h defines whatever variables are required the five routines and can be either macros or functions we ll show various ways to implement these tell and wait routines in later chapters section shows an implementation using signals figure shows an implementation using pipes let look at an example that uses these five routines example the program in figure outputs two strings one from the child and one from the parent the program contains a race condition because the output depends on the order in which the processes are run by the kernel and the length of time for which each process runs we set the standard output unbuffered so every character output generates a write the goal in this example is to allow the kernel to switch between the two processes as often as possible to demonstrate the race condition if we didn t do this we might never see the type of output that follows not seeing the erroneous output doesn t mean that the race condition doesn t exist it simply means that we can t see it on this particular system the following actual output shows how the results can vary we mentioned in section that one use of the fork function is to create a new process the child that then causes another program to be executed by calling one of the exec functions when a process calls one of the exec functions that process is completely replaced by the new program and the new program starts executing at its main function the process id does not change across an exec because a new process is not created exec merely replaces the current process its text data heap and stack segments with a brand new program from disk there are seven different exec functions but we ll often simply refer to the exec function which means that we could use any of the seven functions these seven functions round out the unix system process control primitives with fork we can create new processes and with the exec functions we can initiate new programs the exit function and the wait functions handle termination and waiting for termination these are the only process control primitives we need we ll use these primitives in later sections to build additional functions such as popen and system the first difference in these functions is that the first four take a pathname argument the next two take a filename argument and the last one takes a file descriptor argument when a filename argument is specified if filename contains a slash it is taken as a pathname otherwise the executable file is searched for in the directories specified by the path environment variable the path variable contains a list of directories called path prefixes that are separated by colons for example the name value environment string path bin usr bin usr local bin specifies four directories to search the last path prefix specifies the current directory a zero length prefix also means the current directory it can be specified as a colon at the beginning of the value two colons in a row or a colon at the end of the value there are security reasons for never including the current directory in the search path see garfinkel et al if either execlp or execvp finds an executable file using one of the path prefixes but the file isn t a machine executable that was generated by the link editor the function assumes that the file is a shell script and tries to invoke bin sh with the filename as input to the shell with fexecve we avoid the issue of finding the correct executable file altogether and rely on the caller to do this by using a file descriptor the caller can verify the file is in fact the intended file and execute it without a race otherwise a malicious user with appropriate privileges could replace the executable file or a portion of the path to the executable file after it has been located and verified but before the caller can execute it recall the discussion of tocttou errors in section the next difference concerns the passing of the argument list l stands for list and v stands for vector the functions execl execlp and execle require each of the command line arguments to the new program to be specified as separate arguments we mark the end of the arguments with a null pointer for the other four functions execv execvp execve and fexecve we have to build an array of pointers to the arguments and the address of this array is the argument to these three functions before using iso c prototypes the normal way to show the command line arguments for the three functions execl execle and execlp was char char char argn char this syntax explicitly shows that the final command line argument is followed by a null pointer if this null pointer is specified by the constant we must cast it to a pointer if we don t it interpreted as an integer argument if the size of an integer is different from the size of a char the actual arguments to the exec function will be wrong the final difference is the passing of the environment list to the new program the three functions whose names end in an e execle execve and fexecve allow us to pass a pointer to an array of pointers to the environment strings the other four functions however use the environ variable in the calling process to copy the existing environment for the new program recall our discussion of the environment strings in section and figure we mentioned that if the system supported such functions as setenv and putenv we could change the current environment and the environment of any subsequent child processes but we couldn t affect the environment of the parent process normally a process allows its environment to be propagated to its children but in some cases a process wants to specify a certain environment for a child one example of the latter is the login program when a new login shell is initiated normally login creates a specific environment with only a few variables defined and lets us through the shell start up file add variables to the environment when we log in before using iso c prototypes the arguments to execle were shown as char pathname char char argn char char envp this syntax specifically shows that the final argument is the address of the array of character pointers to the environment strings the iso c prototype doesn t show this as all the command line arguments the null pointer and the envp pointer are shown with the ellipsis notation the arguments for these seven exec functions are difficult to remember the letters in the function names help somewhat the letter p means that the function takes a filename argument and uses the path environment variable to find the executable file the letter l means that the function takes a list of arguments and is mutually exclusive with the letter v which means that it takes an argv vector finally the letter e means that the function takes an envp array instead of using the current environment figure shows the differences among these seven functions function pathname filename fd arg list argv environ envp execl execlp execle execv execvp execve fexecve letter in name p f l v e figure differences among the seven exec functions every system has a limit on the total size of the argument list and the environment list from section and figure this limit is given by this value must be at least bytes on a posix system we sometimes encounter this limit when using the shell filename expansion feature to generate a list of filenames on some systems for example the command grep getrlimit usr share man can generate a shell error of the form argument list too long historically the limit in older system v implementations was bytes older bsd systems had a limit of bytes the limit in current systems is much higher see the output from the program in figure which is summarized in figure to get around the limitation in argument list size we can use the xargs command to break up long argument lists to look for all the occurrences of getrlimit in the man pages on our system we could use find usr share man type f print xargs grep getrlimit if the man pages on our system are compressed however we could try find usr share man type f print xargs bzgrep getrlimit we use the type f option to the find command to restrict the list so that it contains only regular files because the grep commands can t search for patterns in directories and we want to avoid unnecessary error messages we ve mentioned that the process id does not change after an exec but the new program inherits additional properties from the calling process process id and parent process id real user id and real group id supplementary group ids process group id session id controlling terminal time left until alarm clock current working directory root directory file mode creation mask file locks process signal mask pending signals resource limits nice value on xsi conformant systems see section values for and the handling of open files depends on the value of the close on exec flag for each descriptor recall from figure and our mention of the flag in section that every open descriptor in a process has a close on exec flag if this flag is set the descriptor is closed across an exec otherwise the descriptor is left open across the exec the default is to leave the descriptor open across the exec unless we specifically set the close on exec flag using fcntl posix specifically requires that open directory streams recall the opendir function from section be closed across an exec this is normally done by the opendir function calling fcntl to set the close on exec flag for the descriptor corresponding to the open directory stream note that the real user id and the real group id remain the same across the exec but the effective ids can change depending on the status of the set user id and the set group id bits for the program file that is executed if the set user id bit is set for the new program the effective user id becomes the owner id of the program file otherwise the effective user id is not changed it not set to the real user id the group id is handled in the same way in many unix system implementations only one of these seven functions execve is a system call within the kernel the other six are just library functions that eventually invoke this system call we can illustrate the relationship among these seven functions as shown in figure build argv build argv build argv try each path prefix use environ ath from self fd figure relationship of the seven exec functions in this arrangement the library functions execlp and execvp process the path environment variable looking for the first path prefix that contains an executable file named filename the fexecve library function uses proc to convert the file descriptor argument into a pathname that can be used by execve to execute the program this describes how fexecve is implemented in freebsd and linux other systems might take a different approach for example a system without proc or dev fd could implement fexecve as a system call veneer that translates the file descriptor argument into an i node pointer implement execve as a system call veneer that translates the pathname argument into an i node pointer and place all the rest of the exec code common to both execve and fexecve in a separate function to be called with an i node pointer for the file to be executed example the program in figure demonstrates the exec functions include apue h include sys wait h char user unknown path tmp null int main void pid if pid fork fork error else if pid specify pathname specify environment if execle home sar bin echoall echoall my char execle error if waitpid pid null wait error if pid fork fork error else if pid specify filename inherit environment if execlp echoall echoall only arg char execlp error exit figure example of exec functions we first call execle which requires a pathname and a specific environment the next call is to execlp which uses a filename and passes the caller environment to the new program the only reason the call to execlp works is that the directory home sar bin is one of the current path prefixes note also that we set the first argument argv in the new program to be the filename component of the pathname some shells set this argument to be the complete pathname this is a convention only we can set argv to any string we like the login command does this when it executes the shell before executing the shell login adds a dash as a prefix to argv to indicate to the shell that it is being invoked as a login shell a login shell will execute the start up profile commands whereas a nonlogin shell will not the program echoall that is executed twice in the program in figure is shown in figure it is a trivial program that echoes all its command line arguments and its entire environment list when we execute the program from figure we get a out argv echoall argv argv my user unknown path tmp argv echoall argv only arg user sar logname sar shell bin bash more lines that aren t shown home home sar note that the shell prompt appeared before the printing of argv from the second exec this occurred because the parent did not wait for this child process to finish changing user ids and group ids in the unix system privileges such as being able to change the system notion of the current date and access control such as being able to read or write a particular file are based on user and group ids when our programs need additional privileges or need to gain access to resources that they currently aren t allowed to access they need to change their user or group id to an id that has the appropriate privilege or access similarly when our programs need to lower their privileges or prevent access to certain resources they do so by changing either their user id or group id to an id without the privilege or ability access to the resource in general we try to use the least privilege model when we design our applications according to this model our programs should use the least privilege necessary to accomplish any given task this reduces the risk that security might be compromised by a malicious user trying to trick our programs into using their privileges in unintended ways we can set the real user id and effective user id with the setuid function similarly we can set the real group id and the effective group id with the setgid function include unistd h int setuid uid int setgid gid both return if ok on error there are rules for who can change the ids let consider only the user id for now everything we describe for the user id also applies to the group id if the process has superuser privileges the setuid function sets the real user id effective user id and saved set user id to uid if the process does not have superuser privileges but uid equals either the real user id or the saved set user id setuid sets only the effective user id to uid the real user id and the saved set user id are not changed if neither of these two conditions is true errno is set to eperm and is returned here we are assuming that is true if this feature isn t provided then delete all preceding references to the saved set user id the saved ids are a mandatory feature in the version of posix they were optional in older versions of posix to see whether an implementation supports this feature an application can test for the constant at compile time or call sysconf with the argument at runtime we can make a few statements about the three user ids that the kernel maintains only a superuser process can change the real user id normally the real user id is set by the login program when we log in and never changes because login is a superuser process it sets all three user ids when it calls setuid the effective user id is set by the exec functions only if the set user id bit is set for the program file if the set user id bit is not set the exec functions leave the effective user id as its current value we can call setuid at any time to set the effective user id to either the real user id or the saved set user id naturally we can t set the effective user id to any random value the saved set user id is copied from the effective user id by exec if the file set user id bit is set this copy is saved after exec stores the effective user id from the file user id figure summarizes the various ways these three user ids can be changed id exec setuid uid set user id bit off set user id bit on superuser unprivileged user real user id effective user id saved set user id unchanged unchanged copied from effective user id unchanged set from user id of program file copied from effective user id set to uid set to uid set to uid unchanged set to uid unchanged figure ways to change the three user ids note that we can obtain only the current value of the real user id and the effective user id with the functions getuid and geteuid from section we have no portable way to obtain the current value of the saved set user id freebsd and linux provide the getresuid and getresgid functions which can be used to get the saved set user id and saved set group id respectively setreuid and setregid functions historically bsd supported the swapping of the real user id and the effective user id with the setreuid function we can supply a value of for any of the arguments to indicate that the corresponding id should remain unchanged the rule is simple an unprivileged user can always swap between the real user id and the effective user id this allows a set user id program to swap to the user normal permissions and swap back again later for set user id operations when the saved set user id feature was introduced with posix the rule was enhanced to also allow an unprivileged user to set its effective user id to its saved set user id both setreuid and setregid are included in the xsi option in posix as such all unix system implementations are expected to provide support for them didn t have the saved set user id feature described earlier it used setreuid and setregid instead this allowed an unprivileged user to swap back and forth between the two values be aware however that when programs that used this feature spawned a shell they had to set the real user id to the normal user id before the exec if they didn t do this the real user id could be privileged from the swap done by setreuid and the shell process could call setreuid to swap the two and assume the permissions of the more privileged user as a defensive programming measure to solve this problem programs set both the real user id and the effective user id to the normal user id before the call to exec in the child seteuid and setegid functions posix includes the two functions seteuid and setegid these functions are similar to setuid and setgid but only the effective user id or effective group id is changed include unistd h int seteuid uid int setegid gid both return if ok on error an unprivileged user can set its effective user id to either its real user id or its saved set user id for a privileged user only the effective user id is set to uid this behavior differs from that of the setuid function which changes all three user ids figure summarizes all the functions that we ve described here that modify the three user ids superuser setreuid ruid euid superuser setuid uid superuser seteuid uid ruid unprivileged setreuid unprivileged setreuid unprivileged setuid or seteuid unprivileged setuid or seteuid figure summary of all the functions that set the various user ids group ids everything that we ve said so far in this section also applies in a similar fashion to group ids the supplementary group ids are not affected by setgid setregid or setegid example to see the utility of the saved set user id feature let examine the operation of a program that uses it we ll look at the at program which we can use to schedule commands to be run at some time in the future on linux the at program is installed set user id to user daemon on freebsd mac os x and solaris the at program is installed set user id to user root this allows the at command to write privileged files owned by the daemon that will run the commands on behalf of the user running the at command on linux the programs are run by the atd daemon on freebsd and solaris the programs are run by the cron daemon on mac os x the programs are run by the launchd daemon to prevent being tricked into running commands that we aren t allowed to run or reading or writing files that we aren t allowed to access the at command and the daemon that ultimately runs the commands on our behalf have to switch between sets of privileges ours and those of the daemon the following steps take place assuming that the at program file is owned by root and has its set user id bit set when we run it we have real user id our user id unchanged effective user id root saved set user id root the first thing the at command does is reduce its privileges so that it runs with our privileges it calls the seteuid function to set the effective user id to our real user id after this we have real user id our user id unchanged effective user id our user id saved set user id root unchanged the at program runs with our privileges until it needs to access the configuration files that control which commands are to be run and the time at which they need to run these files are owned by the daemon that will run the commands for us the at command calls seteuid to set the effective user id to root this call is allowed because the argument to seteuid equals the saved set user id this is why we need the saved set user id after this we have real user id our user id unchanged effective user id root saved set user id root unchanged because the effective user id is root file access is allowed after the files are modified to record the commands to be run and the time at which they are to be run the at command lowers its privileges by calling seteuid to set its effective user id to our user id this prevents any accidental misuse of privilege at this point we have real user id our user id unchanged effective user id our user id saved set user id root unchanged the daemon starts out running with root privileges to run commands on our behalf the daemon calls fork and the child calls setuid to change its user id to our user id because the child is running with root privileges this changes all of the ids we have real user id our user id effective user id our user id saved set user id our user id now the daemon can safely execute commands on our behalf because it can access only the files to which we normally have access we have no additional permissions by using the saved set user id in this fashion we can use the extra privileges granted to us by the set user id of the program file only when we need elevated privileges any other time however the process runs with our normal permissions if we weren t able to switch back to the saved set user id at the end we might be tempted to retain the extra permissions the whole time we were running which is asking for trouble interpreter files all contemporary unix systems support interpreter files these files are text files that begin with a line of the form pathname optional argument the space between the exclamation point and the pathname is optional the most common of these interpreter files begin with the line bin sh the pathname is normally an absolute pathname since no special operations are performed on it i e path is not used the recognition of these files is done within the kernel as part of processing the exec system call the actual file that gets executed by the kernel is not the interpreter file but rather the file specified by the pathname on the first line of the interpreter file be sure to differentiate between the interpreter file a text file that begins with and the interpreter which is specified by the pathname on the first line of the interpreter file be aware that systems place a size limit on the first line of an interpreter file this limit includes the the pathname the optional argument the terminating newline and any spaces on freebsd this limit is bytes on linux the limit is bytes mac os x supports a limit of bytes whereas solaris places the limit at bytes example let look at an example to see what the kernel does with the arguments to the exec function when the file being executed is an interpreter file and the optional argument on the first line of the interpreter file the program in figure execs an interpreter file include apue h include sys wait h int main void pid if pid fork fork error else if pid child if execl home sar bin testinterp testinterp my char execl error if waitpid pid null parent waitpid error exit figure a program that execs an interpreter file the following shows the contents of the one line interpreter file that is executed and the result from running the program in figure cat home sar bin testinterp home sar bin echoarg foo a out argv home sar bin echoarg argv foo argv home sar bin testinterp argv argv my the program echoarg the interpreter just echoes each of its command line arguments this is the program from figure note that when the kernel execs the interpreter home sar bin echoarg argv is the pathname of the interpreter argv is the optional argument from the interpreter file and the remaining arguments are the pathname home sar bin testinterp and the second and third arguments from the call to execl in the program shown in figure and my both argv and argv from the call to execl have been shifted right two positions note that the kernel takes the pathname from the execl call instead of the first argument testinterp on the assumption that the pathname might contain more information than the first argument example a common use for the optional argument following the interpreter pathname is to specify the f option for programs that support this option for example an awk program can be executed as awk f myfile which tells awk to read the awk program from the file myfile systems derived from unix system v often include two versions of the awk language on these systems awk is often called old awk and corresponds to the original version distributed with version in contrast nawk new awk contains numerous enhancements and corresponds to the language described in aho kernighan and weinberger this newer version provides access to the command line arguments which we need for the example that follows solaris provides both versions the awk program is one of the utilities included by posix in its standard which is now part of the base posix specification in the single unix specification this utility is also based on the language described in aho kernighan and weinberger the version of awk in mac os x is based on the bell laboratories version which has been placed in the public domain freebsd and some linux distributions ship with gnu awk called gawk which is linked to the name awk gawk conforms to the posix standard but also includes other extensions because they are more up to date gawk and the version of awk from bell laboratories are preferred to either nawk or old awk the bell labs version of awk is available at http cm bell labs com cm cs awkbook index html using the f option with an interpreter file lets us write bin awk f awk program follows in the interpreter file for example figure shows usr local bin awkexample an interpreter file usr bin awk f note on solaris use nawk instead begin for i i argc i printf argv d n i argv i exit figure an awk program as an interpreter file if one of the path prefixes is usr local bin we can execute the program in figure assuming that we ve turned on the execute bit for the file as awkexample argv awk argv argv argv when bin awk is executed its command line arguments are bin awk f usr local bin awkexample the pathname of the interpreter file usr local bin awkexample is passed to the interpreter the filename portion of this pathname what we typed to the shell isn t adequate because the interpreter bin awk in this example can t be expected to use the path variable to locate files when it reads the interpreter file awk ignores the first line since the pound sign is awk comment character we can verify these command line arguments with the following commands bin su become superuser password enter superuser password mv usr bin awk usr bin awk save save the original program cp home sar bin echoarg usr bin awk and replace it temporarily suspend suspend the superuser shell stopped bin su using job control awkexample argv bin awk argv f argv usr local bin awkexample argv argv argv fg resume superuser shell using job control bin su mv usr bin awk save usr bin awk restore the original program exit and exit the superuser shell in this example the f option for the interpreter is required as we said this tells awk where to look for the awk program if we remove the f option from the interpreter file an error message usually results when we try to run it the exact text of the message varies depending on where the interpreter file is stored and whether the remaining arguments represent existing files this is because the command line arguments in this case are bin awk usr local bin awkexample filename2 and awk is trying to interpret the string usr local bin awkexample as an awk program if we couldn t pass at least a single optional argument to the interpreter f in this case these interpreter files would be usable only with the shells are interpreter files required not really they provide an efficiency gain for the user at some expense in the kernel since it the kernel that recognizes these files interpreter files are useful for the following reasons they hide that certain programs are scripts in some other language for example to execute the program in figure we just say awkexample optional arguments instead of needing to know that the program is really an awk script that we would otherwise have to execute as awk f awkexample optional arguments interpreter scripts provide an efficiency gain consider the previous example again we could still hide that the program is an awk script by wrapping it in a shell script awk begin for i i argc i printf argv d n i argv i exit the problem with this solution is that more work is required first the shell reads the command and tries to execlp the filename because the shell script is an executable file but isn t a machine executable an error is returned and execlp assumes that the file is a shell script which it is then bin sh is executed with the pathname of the shell script as its argument the shell correctly runs our script but to run the awk program the shell does a fork exec and wait thus there is more overhead involved in replacing an interpreter script with a shell script interpreter scripts let us write shell scripts using shells other than bin sh when it finds an executable file that isn t a machine executable execlp has to choose a shell to invoke and it always uses bin sh using an interpreter script however we can simply write bin csh c shell script follows in the interpreter file again we could wrap all of this in a bin sh script that invokes the c shell as we described earlier but more overhead is required none of this would work as we ve shown here if the three shells and awk didn t use the pound sign as their comment character system function it is convenient to execute a command string from within a program for example assume that we want to put a time and date stamp into a certain file we could use the functions described in section to do this call time to get the current calendar time then call localtime to convert it to a broken down time then call strftime to format the result and finally write the result to the file it is much easier however to say system date file iso c defines the system function but its operation is strongly system dependent posix includes the system interface expanding on the iso c definition to describe its behavior in a posix environment if cmdstring is a null pointer system returns nonzero only if a command processor is available this feature determines whether the system function is supported on a given operating system under the unix system system is always available because system is implemented by calling fork exec and waitpid there are three types of return values if either the fork fails or waitpid returns an error other than eintr system returns with errno set to indicate the error if the exec fails implying that the shell can t be executed the return value is as if the shell had executed exit otherwise all three functions fork exec and waitpid succeed and the return value from system is the termination status of the shell in the format specified for waitpid some older implementations of system returned an error eintr if waitpid was interrupted by a caught signal because there is no strategy that an application can use to recover from this type of error the process id of the child is hidden from the caller posix later added the requirement that system not return an error in this case we discuss interrupted system calls in section figure shows an implementation of the system function the one feature that it doesn t handle is signals we ll update this function with signal handling in section the shell c option tells it to take the next command line argument cmdstring in this case as its command input instead of reading from standard input or from a given file the shell parses this null terminated c string and breaks it up into separate command line arguments for the command the actual command string that is passed to the shell can contain any valid shell commands for example input and output redirection using and can be used if we didn t use the shell to execute the command but tried to execute the command ourself it would be more difficult first we would want to call execlp instead of execl to use the path variable like the shell we would also have to break up the null terminated c string into separate command line arguments for the call to execlp finally we wouldn t be able to use any of the shell metacharacters note that we call instead of exit we do this to prevent any standard i o buffers which would have been copied from the parent to the child across the fork from being flushed in the child we can test this version of system with the program shown in figure the function was defined in figure running the program in figure gives us a out sat feb est normal termination exit status for date sh nosuchcommand command not found normal termination exit status for nosuchcommand sar console jan sar feb sar jan sar jan sar jan normal termination exit status for exit the advantage in using system instead of using fork and exec directly is that system does all the required error handling and in our next version of this function in section all the required signal handling earlier systems including and didn t have the waitpid function available instead the parent waited for the child using a statement such as while lastpid wait status pid lastpid a problem occurs if the process that calls system has spawned its own children before calling system because the while statement above keeps looping until the child that was generated by system terminates if any children of the process terminate before the process identified by pid then the process id and termination status of these other children are discarded by the while statement indeed this inability to wait for a specific child is one of the reasons given in the posix rationale for including the waitpid function we ll see in section that the same problem occurs with the popen and pclose functions if the system doesn t provide a waitpid function set user id programs what happens if we call system from a set user id program doing so creates a security hole and should never be attempted figure shows a simple program that just calls system for its command line argument we ll compile this program into the executable file printuids running both programs gives us the following tsys printuids normal execution no special privileges real uid effective uid normal termination exit status su become superuser password enter superuser password chown root tsys change owner chmod u tsys make set user id ls l tsys verify file permissions and owner rwsrwxr x root feb tsys exit leave superuser shell tsys printuids real uid effective uid oops this is a security hole normal termination exit status the superuser permissions that we gave the tsys program are retained across the fork and exec that are done by system some implementations have closed this security hole by changing bin sh to reset the effective user id to the real user id when they don t match on these systems the previous example doesn t work as shown instead the same effective user id will be printed regardless of the status of the set user id bit on the program calling system if it is running with special permissions either set user id or set group id and wants to spawn another process a process should use fork and exec directly being certain to change back to normal permissions after the fork before calling exec the system function should never be used from a set user id or a set group id program one reason for this admonition is that system invokes the shell to parse the command string and the shell uses its ifs variable as the input field separator older versions of the shell didn t reset this variable to a normal set of characters when invoked as a result a malicious user could set ifs before system was called causing system to execute a different program process accounting most unix systems provide an option to do process accounting when enabled the kernel writes an accounting record each time a process terminates these accounting records typically contain a small amount of binary data with the name of the command the amount of cpu time used the user id and group id the starting time and so on we ll take a closer look at these accounting records in this section as it gives us a chance to look at processes again and to use the fread function from section process accounting is not specified by any of the standards thus all the implementations have annoying differences for example the i o counts maintained on solaris are in units of bytes whereas freebsd and mac os x maintain units of blocks although there is no distinction between different block sizes making the counter effectively useless linux on the other hand doesn t try to maintain i o statistics at all each implementation also has its own set of administrative commands to process raw accounting data for example solaris provides runacct and acctcom whereas freebsd provides the sa command to process and summarize the raw accounting data a function we haven t described acct enables and disables process accounting the only use of this function is from the accton command which happens to be one of the few similarities among platforms a superuser executes accton with a pathname argument to enable accounting the accounting records are written to the specified file which is usually var account acct on freebsd and mac os x var log account pacct on linux and var adm pacct on solaris accounting is turned off by executing accton without any arguments the structure of the accounting records is defined in the header sys acct h although the implementation of each system differs the accounting records look something like typedef bit base exponent bit fraction struct acct char flag see figure char termination status signal core flag only solaris only real user id real group id controlling terminal starting calendar time user cpu time system cpu time elapsed time average memory usage bytes transferred by read and write blocks on bsd systems ac_rw blocks read or written not present on bsd systems char command name for solaris for mac os x for freebsd and for linux times are recorded in units of clock ticks on most platforms but freebsd stores microseconds instead the member records certain events during the execution of the process these events are described in figure the data required for the accounting record such as cpu times and number of characters transferred is kept by the kernel in the process table and initialized whenever a new process is created as in the child after a fork each accounting record is written when the process terminates this has two consequences first we don t get accounting records for processes that never terminate processes like init that run for the lifetime of the system don t generate accounting records this also applies to kernel daemons which normally don t exit second the order of the records in the accounting file corresponds to the termination order of the processes not the order in which they were started to know the starting order we would have to go through the accounting file and sort by the starting calendar time but this isn t perfect since calendar times are in units of seconds section and it possible for many processes to be started in any given second alternatively the elapsed time is given in clock ticks which are usually between and ticks per second but we don t know the ending time of a process all we know is its starting time and ending order thus even though the elapsed time is more accurate than the starting time we still can t reconstruct the exact starting order of various processes given the data in the accounting file the accounting records correspond to processes not programs a new record is initialized by the kernel for the child after a fork not when a new program is executed although exec doesn t create a new accounting record the command name changes and the afork flag is cleared this means that if we have a chain of three programs a description freebsd linux mac os x solaris afork process is the result of fork but never called exec asu process used superuser privileges acore process dumped core axsig process was killed by a signal aexpnd expanded accounting entry anver new record format figure values for from accounting record execs b then b execs c and c exits only a single accounting record is written the command name in the record corresponds to program c but the cpu times for example are the sum for programs a b and c example to have some accounting data to examine we ll create a test program to implement the diagram shown in figure parent first child third child fourth child execl bin dd figure process structure for accounting example the source for the test program is shown in figure it calls fork four times each child does something different and then terminates we ll run the test program on solaris and then use the program in figure to print out selected fields from the accounting records bsd derived platforms don t support the member so we define the constant on the platforms that do support this member basing the defined symbol on the feature instead of on the platform makes the code read better and allows us to modify the program simply by adding the new definition to our compilation command the alternative would be to use if defined bsd defined macos which becomes unwieldy as we port our application to additional platforms we define similar constants to determine whether the platform supports the acore and axsig accounting flags we can t use the flag symbols themselves because on linux they are defined as enum values which we can t use in a ifdef expression to perform our test we do the following become superuser and enable accounting with the accton command note that when this command terminates accounting should be on therefore the first record in the accounting file should be from this command exit the superuser shell and run the program in figure this should append six records to the accounting file one for the superuser shell one for the test parent and one for each of the four test children a new process is not created by the execl in the second child there is only a single accounting record for the second child become superuser and turn accounting off since accounting is off when this accton command terminates it should not appear in the accounting file run the program in figure to print the selected fields from the accounting file the output from step follows we have appended the description of the process in italics to selected lines for the discussion later accton e chars stat s sh e chars stat s dd e chars stat second child a out e chars stat parent a out e chars stat f first child a out e chars stat f fourth child a out e chars stat f third child for this system the elapsed time values are measured in units of clock ticks figure shows that this system generates clock ticks per second for example the sleep in the parent corresponds to the elapsed time of clock ticks for the first child the sleep becomes clock ticks note that the amount of time a process sleeps is not exact we ll return to the sleep function in chapter also the calls to fork and exit take some amount of time note that the member is not the true termination status of the process but rather corresponds to a portion of the termination status that we discussed in section the only information in this byte is a core flag bit usually the high order bit and the signal number usually the seven low order bits if the process terminated abnormally if the process terminated normally we are not able to obtain the exit status from the accounting file for the first child this value is the is the core flag bit and happens to be the value on this system for sigabrt which is generated by the call to abort the value for the fourth child corresponds to the value of sigkill we can t tell from the accounting data that the parent argument to exit was and that the third child argument to exit was the size of the file etc passwd that the dd process copies in the second child is bytes the number of characters of i o is just over twice this value it is twice the value as bytes are read in then bytes are written out even though the output goes to the null device the bytes are still accounted for the additional bytes come from the dd command reporting the summary of bytes read and written which it prints to stdout the values are are what we would expect the f flag is set for all the child processes except the second child which does the execl the f flag is not set for the parent because the interactive shell that executed the parent did a fork and then an exec of the a out file the first child process calls abort which generates a sigabrt signal to generate the core dump note that neither the x flag nor the d flag is on as they are not supported on solaris the information they represent can be derived from the field the fourth child also terminates because of a signal but the sigkill signal does not generate a core dump it just terminates the process as a final note the first child has a count for the number of characters of i o yet this process generated a core file it appears that the i o required to write the core file is not charged to the process user identification any process can find out its real and effective user id and group id sometimes however we want to find out the login name of the user who running the program we could call getpwuid getuid but what if a single user has multiple login names each with the same user id a person might have multiple entries in the password file with the same user id to have a different login shell for each entry the system normally keeps track of the name we log in under section and the getlogin function provides a way to fetch that login name this function can fail if the process is not attached to a terminal that a user logged in to we normally call these processes daemons we discuss them in chapter given the login name we can then use it to look up the user in the password file to determine the login shell for example using getpwnam to find the login name unix systems have historically called the ttyname function section and then tried to find a matching entry in the utmp file section freebsd and mac os x store the login name in the session structure associated with the process table entry and provide system calls to fetch and store this name system v provided the cuserid function to return the login name this function called getlogin and if that failed did a getpwuid getuid the ieee standard specified cuserid but it called for the effective user id to be used instead of the real user id the version of posix dropped the cuserid function the environment variable logname is usually initialized with the user login name by login and inherited by the login shell realize however that a user can modify an environment variable so we shouldn t use logname to validate the user in any way instead we should use getlogin process scheduling historically the unix system provided processes with only coarse control over their scheduling priority the scheduling policy and priority were determined by the kernel a process could choose to run with lower priority by adjusting its nice value thus a process could be nice and reduce its share of the cpu by adjusting its nice value only a privileged process was allowed to increase its scheduling priority the real time extensions in posix added interfaces to select among multiple scheduling classes and fine tune their behavior we discuss only the interfaces used to adjust the nice value here they are part of the xsi option in posix refer to gallmeister for more information on the real time scheduling extensions in the single unix specification nice values range from to nzero although some implementations support a range from to nzero lower nice values have higher scheduling priority although this might seem backward it actually makes sense the more nice you are the lower your scheduling priority is nzero is the default nice value of the system be aware that the header file defining nzero differs among systems in addition to the header file linux makes the value of nzero accessible through a nonstandard sysconf argument a process can retrieve and change its nice value with the nice function with this function a process can affect only its own nice value it can t affect the nice value of any other process the incr argument is added to the nice value of the calling process if incr is too large the system silently reduces it to the maximum legal value similarly if incr is too small the system silently increases it to the minimum legal value because is a legal successful return value we need to clear errno before calling nice and check its value if nice returns if the call to nice succeeds and the return value is then errno will still be zero if errno is nonzero it means that the call to nice failed the getpriority function can be used to get the nice value for a process just like the nice function however getpriority can also get the nice value for a group of related processes the which argument can take on one of three values to indicate a process to indicate a process group and to indicate a user id the which argument controls how the who argument is interpreted and the who argument selects the process or processes of interest if the who argument is then it indicates the calling process process group or user depending on the value of the which argument when which is set to and who is the real user id of the calling process is used when the which argument applies to more than one process the highest priority lowest value of all the applicable processes is returned the setpriority function can be used to set the priority of a process a process group or all the processes belonging to a particular user id the which and who arguments are the same as in the getpriority function the value is added to nzero and this becomes the new nice value the nice system call originated with an early pdp version of the research unix system the getpriority and setpriority functions originated with the single unix specification leaves it up to the implementation whether the nice value is inherited by a child process after a fork however xsi compliant systems are required to preserve the nice value across a call to exec a child process inherits the nice value from its parent process in freebsd linux mac os x and solaris example the program in figure measures the effect of adjusting the nice value of a process two processes run in parallel each incrementing its own counter the parent runs with the default nice value and the child runs with an adjusted nice value as specified by the optional command argument after running for seconds both processes print the value of their counter and exit by comparing the counter values for different nice values we can get an idea how the nice value affects process scheduling figure evaluate the effect of changing the nice value we run the program twice once with the default nice value and once with the highest valid nice value the lowest scheduling priority we run this on a uniprocessor linux system to show how the scheduler shares the cpu among processes with different nice values with an otherwise idle system a multiprocessor system or a multicore cpu would allow both processes to run without the need to share a cpu and we wouldn t see much difference between two processes with different nice values a out nzero current nice value in parent is current nice value in child is adjusting by now child nice value is child count parent count a out nzero current nice value in parent is current nice value in child is adjusting by now child nice value is parent count child count when both processes have the same nice value the parent process gets of the cpu and the child gets of the cpu note that the two processes are effectively treated equally the percentages aren t exactly equal because process scheduling isn t exact and because the child and parent perform different amounts of processing between the time that the end time is calculated and the time that the processing loop begins in contrast when the child has the highest possible nice value the lowest priority we see that the parent gets of the cpu while the child gets only of the cpu these values will vary based on how the process scheduler uses the nice value so a different unix system will produce different ratios process times in section we described three times that we can measure wall clock time user cpu time and system cpu time any process can call the times function to obtain these values for itself and any terminated children this function fills in the tms structure pointed to by buf struct tms user cpu time tms_stime system cpu time user cpu time terminated children system cpu time terminated children note that the structure does not contain any measurement for the wall clock time instead the function returns the wall clock time as the value of the function each time it called this value is measured from some arbitrary point in the past so we can t use its absolute value instead we use its relative value for example we call times and save the return value at some later time we call times again and subtract the earlier return value from the new return value the difference is the wall clock time it is possible though unlikely for a long running process to overflow the wall clock time see exercise the two structure fields for child processes contain values only for children that we have waited for with one of the wait functions discussed earlier in this chapter all the values returned by this function are converted to seconds using the number of clock ticks per second the value returned by sysconf section most implementations provide the getrusage function this function returns the cpu times and other values indicating resource usage historically this function originated with the bsd operating system so bsd derived implementations generally support more of the fields than do other implementations example the program in figure executes each command line argument as a shell command string timing the command and printing the values from the tms structure if we run this program we get a out sleep date man bash dev null normal termination exit status command date sun feb est real user sys child user child sys normal termination exit status command man bash dev null real user sys child user child sys normal termination exit status in the first two commands execution is fast enough to avoid registering any cpu time at the reported resolution in the third command however we run a command that takes enough processing time to note that all the cpu time appears in the child process which is where the shell and the command execute summary a thorough understanding of the unix system process control is essential for advanced programming there are only a few functions to master fork the exec family wait and waitpid these primitives are used in many applications the fork function also gave us an opportunity to look at race conditions our examination of the system function and process accounting gave us another look at all these process control functions we also looked at another variation of the exec functions interpreter files and how they operate an understanding of the various user ids and group ids that are provided real effective and saved is critical to writing safe set user id programs given an understanding of a single process and its children in the next chapter we examine the relationship of a process to other processes sessions and job control we then complete our discussion of processes in chapter when we describe signals exercises in figure we said that replacing the call to with a call to exit might cause the standard output to be closed and printf to return modify the program to check whether your implementation behaves this way if it does not how can you simulate this behavior recall the typical arrangement of memory in figure because the stack frames corresponding to each function call are usually stored in the stack and because after a vfork the child runs in the address space of the parent what happens if the call to vfork is from a function other than main and the child does a return from this function after the vfork write a test program to verify this and draw a picture of what happening rewrite the program in figure to use waitid instead of wait instead of calling determine the equivalent information from the siginfo structure when we execute the program in figure one time as in a out the output is correct but if we execute the program multiple times one right after the other as in a out a out a out output from parent ooutput from parent ouotuptut from child put from parent output from child utput from child the output is not correct what happening how can we correct this can this problem happen if we let the child write its output first in the program shown in figure we call execl specifying the pathname of the interpreter file if we called execlp instead specifying a filename of testinterp and if the directory home sar bin was a path prefix what would be printed as argv when the program is run write a program that creates a zombie and then call system to execute the ps command to verify that the process is a zombie we mentioned in section that posix requires open directory streams to be closed across an exec verify this as follows call opendir for the root directory peek at your system implementation of the dir structure and print the close on exec flag then open the same directory for reading and print the close on exec flag this page intentionally left blank process relationships introduction we learned in the previous chapter that there are relationships between processes first every process has a parent process the initial kernel level process is usually its own parent the parent is notified when the child terminates and the parent can obtain the child exit status we also mentioned process groups when we described the waitpid function section and explained how we can wait for any process in a process group to terminate in this chapter we ll look at process groups in more detail and the concept of sessions that was introduced by posix we ll also look at the relationship between the login shell that is invoked for us when we log in and all the processes that we start from our login shell it is impossible to describe these relationships without talking about signals and to talk about signals we need many of the concepts in this chapter if you are unfamiliar with the unix system signal mechanism you may want to skim through chapter at this point terminal logins let start by looking at the programs that are executed when we log in to a unix system in early unix systems such as version users logged in using dumb terminals that were connected to the host with hard wired connections the terminals were either local directly connected or remote connected through a modem in either case these logins came through a terminal device driver in the kernel for example the common devices on pdp were dh and dz a host had a fixed number of these terminal devices so there was a known upper limit on the number of simultaneous logins as bitmapped graphical terminals became available windowing systems were developed to provide users with new ways to interact with host computers applications were developed to create terminal windows to emulate character based terminals allowing users to interact with hosts in familiar ways i e via the shell command line today some platforms allow you to start a windowing system after logging in whereas other platforms automatically start the windowing system for you in the latter case you might still have to log in depending on how the windowing system is configured some windowing systems can be configured to log you in automatically the procedure that we now describe is used to log in to a unix system using a terminal the procedure is similar regardless of the type of terminal we use it could be a character based terminal a graphical terminal emulating a simple character based terminal or a graphical terminal running a windowing system bsd terminal logins the bsd terminal login procedure has not changed much over the past years the system administrator creates a file usually etc ttys that has one line per terminal device each line specifies the name of the device and other parameters that are passed to the getty program one parameter is the baud rate of the terminal for example when the system is bootstrapped the kernel creates process id the init process and it is init that brings the system up in multiuser mode the init process reads the file etc ttys and for every terminal device that allows a login does a fork followed by an exec of the program getty this gives us the processes shown in figure process id k forks once per terminal c each child execs getty figure processes invoked by init to allow terminal logins all the processes shown in figure have a real user id of and an effective user id of i e they all have superuser privileges the init process also execs the getty program with an empty environment it is getty that calls open for the terminal device the terminal is opened for reading and writing if the device is a modem the open may delay inside the device driver until the modem is dialed and the call is answered once the device is open file descriptors and are set to the device then getty outputs something like login and waits for us to enter our user name if the terminal supports multiple speeds getty can detect special characters that tell it to change the terminal speed baud rate consult your unix system manuals for additional details on the getty program and the data files gettytab that can drive its actions when we enter our user name getty job is complete and it then invokes the login program similar to execle bin login login p username char envp there can be options in the gettytab file to have it invoke other programs but the default is the login program init invokes getty with an empty environment getty creates an environment for login the envp argument with the name of the terminal something like term foo where the type of terminal foo is taken from the gettytab file and any environment strings that are specified in the gettytab the p flag to login tells it to preserve the environment that it is passed and to add to that environment not replace it figure shows the state of these processes right after login has been invoked process id init fork init exec reads etc ttys forks once per terminal creates empty environment opens terminal device file descriptors reads user name initial environment set figure state of processes after login has been invoked all the processes shown in figure have superuser privileges since the original init process has superuser privileges the process id of the bottom three processes in figure is the same since the process id does not change across an exec also all the processes other than the original init process have a parent process id of the login program does many things since it has our user name it can call getpwnam to fetch our password file entry then login calls getpass to display the prompt password and read our password with echoing disabled of course it calls crypt to encrypt the password that we entered and compares the encrypted result to the field from our shadow password file entry if the login attempt fails because of an invalid password after a few tries login calls exit with an argument of this termination will be noticed by the parent init and it will do another fork followed by an exec of getty starting the procedure over again for this terminal this is the traditional authentication procedure used on unix systems modern unix systems however have evolved to support multiple authentication procedures for example freebsd linux mac os x and solaris all support a more flexible scheme known as pam pluggable authentication modules pam allows an administrator to configure the authentication methods to be used to access services that are written to use the pam library if our application needs to verify that a user has the appropriate permission to perform a task we can either hard code the authentication mechanism in the application or use the pam library to give us the equivalent functionality the advantage to using pam is that administrators can configure different ways to authenticate users for different tasks based on the local site policies if we log in correctly login will change to our home directory chdir change the ownership of our terminal device chown so we own it change the access permissions for our terminal device so we have permission to read from and write to it set our group ids by calling setgid and initgroups initialize the environment with all the information that login has our home directory home shell shell user name user and logname and a default path path change to our user id setuid and invoke our login shell as in execl bin sh sh char the minus sign as the first character of argv is a flag to all the shells that indicates they are being invoked as a login shell the shells can look at this character and modify their start up accordingly the login program really does more than we ve described here it optionally prints the message of the day file checks for new mail and performs other tasks in this chapter we re interested only in the features that we ve described recall from our discussion of the setuid function in section that since it is called by a superuser process setuid changes all three user ids the real user id effective user id and saved set user id the call to setgid that was done earlier by login has the same effect on all three group ids at this point our login shell is running its parent process id is the original init process process id so when our login shell terminates init is notified it is sent a sigchld signal and it starts the whole procedure over again for this terminal file descriptors and for our login shell are set to the terminal device figure shows this arrangement process id through getty and login fd hard wired connection figure arrangement of processes after everything is set for a terminal login our login shell now reads its start up files profile for the bourne shell and korn shell or profile for the gnu bourne again shell and cshrc and login for the c shell these start up files usually change some of the environment variables and add many other variables to the environment for example most users set their own path and often prompt for the actual terminal type term when the start up files are done we finally get the shell prompt and can enter commands mac os x terminal logins on mac os x the terminal login process follows essentially the same steps as in the bsd login process since mac os x is based in part on freebsd with mac os x however there are some differences the work of init is performed by launchd we are presented with a graphical based login screen from the start linux terminal logins the linux login procedure is very similar to the bsd procedure indeed the linux login command is derived from the login command the main difference between the bsd login procedure and the linux login procedure is in the way the terminal configuration is specified some linux distributions ship with a version of the init program that uses administrative files patterned after system v init file formats on these systems etc inittab contains the configuration information specifying the terminal devices for which init should start a getty process other linux distributions such as recent ubuntu distributions ship with a version of init that is known as upstart it uses configuration files named conf that are stored in the etc init directory for example the specifications for running getty on dev might be found in the file etc init conf depending on the version of getty in use the terminal characteristics are specified either on the command line as with agetty or in the file etc gettydefs as with mgetty solaris terminal logins solaris supports two forms of terminal logins a getty style as described previously for bsd and b ttymon logins a feature introduced with normally getty is used for the console and ttymon is used for other terminal logins the ttymon command is part of a larger facility termed saf the service access facility the goal of the saf was to provide a consistent way to administer services that provide access to a system see chapter of rago for more details for our purposes we end up with the same picture as in figure with a different set of steps between init and the login shell init is the parent of sac the service access controller which does a fork and exec of the ttymon program when the system enters multiuser state the ttymon program monitors all the terminal ports listed in its configuration file and does a fork when we enter our login name this child of ttymon does an exec of login and login prompts us for our password once this is done login execs our login shell and we re at the position shown in figure one difference is that the parent of our login shell is now ttymon whereas the parent of the login shell from a getty login is init network logins the main physical difference between logging in to a system through a serial terminal and logging in to a system through a network is that the connection between the terminal and the computer isn t point to point in this case login is simply a service available just like any other network service such as ftp or smtp with the terminal logins that we described in the previous section init knows which terminal devices are enabled for logins and spawns a getty process for each device in the case of network logins however all the logins come through the kernel network interface drivers e g the ethernet driver and we don t know ahead of time how many of these will occur instead of having a process waiting for each possible login we now have to wait for a network connection request to arrive to allow the same software to process logins over both terminal logins and network logins a software driver called a pseudo terminal is used to emulate the behavior of a serial terminal and map terminal operations to network operations and vice versa in chapter we ll talk about pseudo terminals in detail bsd network logins in bsd a single process waits for most network connections the inetd process sometimes called the internet superserver in this section we ll look at the sequence of processes involved in network logins for a bsd system we are not interested in the detailed network programming aspects of these processes refer to stevens fenner and rudoff for all the details as part of the system start up init invokes a shell that executes the shell script etc rc one of the daemons that is started by this shell script is inetd once the shell script terminates the parent process of inetd becomes init inetd waits for tcp ip connection requests to arrive at the host when a connection request arrives for it to handle inetd does a fork and exec of the appropriate program let assume that a tcp connection request arrives for the telnet server telnet is a remote login application that uses the tcp protocol a user on another host that is connected to the server host through a network of some form or on the same host initiates the login by starting the telnet client telnet hostname the client opens a tcp connection to hostname and the program that started on hostname is called the telnet server the client and the server then exchange data across the tcp connection using the telnet application protocol what has happened is that the user who started the client program is now logged in to the server host this assumes of course that the user has a valid account on the server host figure shows the sequence of processes involved in executing the telnet server called telnetd process id tcp connection request from telnet client fork fork exec of bin sh which executes shell script etc rc when system comes up multiuser when connection request arrives from telnet client inetd exec figure sequence of processes involved in executing telnet server the telnetd process then opens a pseudo terminal device and splits into two processes using fork the parent handles the communication across the network connection and the child does an exec of the login program the parent and the child are connected through the pseudo terminal before doing the exec the child sets up file descriptors and to the pseudo terminal if we log in correctly login performs the same steps we described in section it changes to our home directory and sets our group ids user id and our initial environment then login replaces itself with our login shell by calling exec figure shows the arrangement of the processes at this point process id fd through inetd telnetd and login network connection through telnetd server and telnet client figure arrangement of processes after everything is set for a network login obviously a lot is going on between the pseudo terminal device driver and the actual user at the terminal we ll show all the processes involved in this type of arrangement in chapter when we talk about pseudo terminals in more detail the important thing to understand is that whether we log in through a terminal figure or a network figure we have a login shell with its standard input standard output and standard error connected to either a terminal device or a pseudo terminal device we ll see in the coming sections that this login shell is the start of a posix session and that the terminal or pseudo terminal is the controlling terminal for the session mac os x network logins logging in to a mac os x system over a network is identical to logging in to a bsd system because mac os x is based partially on freebsd however on mac os x the telnet daemon is run from launchd by default the telnet daemon is disabled on mac os x although it can be enabled with the launchctl command the preferred way to perform a network login on mac os x is with ssh the secure shell command linux network logins network logins under linux are the same as under bsd except that some distributions use an alternative inetd process called the extended internet services daemon xinetd the xinetd process provides a finer level of control over services it starts compared to inetd solaris network logins the scenario for network logins under solaris is almost identical to the steps under bsd and linux an inetd server is used that is similar in concept to the bsd version except that the solaris version runs as a restarter in the service management facility smf a restarter is a daemon that has the responsibility to start and monitor other daemon processes and restart them if they fail although the inetd server is started by the master restarter in the smf the master restarter is started by init and we end up with the same overall picture as in figure the solaris service management facility is a framework that manages and monitors system services and provides a way to recover from failures affecting system services for more details on the service management facility see adams and the solaris manual pages smf and inetd process groups in addition to having a process id each process belongs to a process group we ll encounter process groups again when we discuss signals in chapter a process group is a collection of one or more processes usually associated with the same job job control is discussed in section that can receive signals from the same terminal each process group has a unique process group id process group ids are similar to process ids they are positive integers and can be stored in a data type the function getpgrp returns the process group id of the calling process in older bsd derived systems the getpgrp function took a pid argument and returned the process group for that process the single unix specification defines the getpgid function that mimics this behavior include unistd h getpgid pid returns process group id if ok on error if pid is the process group id of the calling process is returned thus getpgid is equivalent to getpgrp each process group can have a process group leader the leader is identified by its process group id being equal to its process id it is possible for a process group leader to create a process group create processes in the group and then terminate the process group still exists as long as at least one process is in the group regardless of whether the group leader terminates this is called the process group lifetime the period of time that begins when the group is created and ends when the last remaining process leaves the group the last remaining process in the process group can either terminate or enter some other process group a process joins an existing process group or creates a new process group by calling setpgid in the next section we ll see that setsid also creates a new process group this function sets the process group id to pgid in the process whose process id equals pid if the two arguments are equal the process specified by pid becomes a process group leader if pid is the process id of the caller is used also if pgid is the process id specified by pid is used as the process group id a process can set the process group id of only itself or any of its children furthermore it can t change the process group id of one of its children after that child has called one of the exec functions in most job control shells this function is called after a fork to have the parent set the process group id of the child and to have the child set its own process group id one of these calls is redundant but by doing both we are guaranteed that the child is placed into its own process group before either process assumes that this has happened if we didn t do this we would have a race condition since the child process group membership would depend on which process executes first when we discuss signals we ll see how we can send a signal to either a single process identified by its process id or a process group identified by its process group id similarly the waitpid function from section lets us wait for either a single process or one process from a specified process group sessions a session is a collection of one or more process groups for example we could have the arrangement shown in figure here we have three process groups in a single session process group process group process group session figure arrangement of processes into process groups and sessions the processes in a process group are usually placed there by a shell pipeline for example the arrangement shown in figure could have been generated by shell commands of the form proc4 a process establishes a new session by calling the setsid function if the calling process is not a process group leader this function creates a new session three things happen the process becomes the session leader of this new session a session leader is the process that creates a session the process is the only process in this new session the process becomes the process group leader of a new process group the new process group id is the process id of the calling process the process has no controlling terminal we ll discuss controlling terminals in the next section if the process had a controlling terminal before calling setsid that association is broken this function returns an error if the caller is already a process group leader to ensure this is not the case the usual practice is to call fork and have the parent terminate and the child continue we are guaranteed that the child is not a process group leader because the process group id of the parent is inherited by the child but the child gets a new process id hence it is impossible for the child process id to equal its inherited process group id the single unix specification talks only about a session leader there is no session id similar to a process id or a process group id obviously a session leader is a single process that has a unique process id so we could talk about a session id that is the process id of the session leader this concept of a session id was introduced in historically bsd based systems didn t support this notion but have since been updated to include it the getsid function returns the process group id of a process session leader some implementations such as solaris join with the single unix specification in the practice of avoiding the use of the phrase session id opting instead to refer to this as the process group id of the session leader the two are equivalent since the session leader is always the leader of a process group if pid is getsid returns the process group id of the calling process session leader for security reasons some implementations may restrict the calling process from obtaining the process group id of the session leader if pid doesn t belong to the same session as the caller controlling terminal sessions and process groups have a few other characteristics a session can have a single controlling terminal this is usually the terminal device in the case of a terminal login or pseudo terminal device in the case of a network login on which we log in the session leader that establishes the connection to the controlling terminal is called the controlling process the process groups within a session can be divided into a single foreground process group and one or more background process groups if a session has a controlling terminal it has a single foreground process group and all other process groups in the session are background process groups whenever we press the terminal interrupt key often delete or control c the interrupt signal is sent to all processes in the foreground process group whenever we press the terminal quit key often control backslash the quit signal is sent to all processes in the foreground process group if a modem or network disconnect is detected by the terminal interface the hang up signal is sent to the controlling process the session leader these characteristics are shown in figure session background process group session leader controlling process background process group foreground process group controlling terminal figure process groups and sessions showing controlling terminal usually we don t have to worry about the controlling terminal it is established automatically when we log in posix leaves the choice of the mechanism used to allocate a controlling terminal up to each individual implementation we ll show the actual steps in section systems derived from unix system v allocate the controlling terminal for a session when the session leader opens the first terminal device that is not already associated with a session as long as the call to open does not specify the flag section bsd based systems allocate the controlling terminal for a session when the session leader calls ioctl with a request argument of tiocsctty the third argument is a null pointer the session cannot already have a controlling terminal for this call to succeed normally this call to ioctl follows a call to setsid which guarantees that the process is a session leader without a controlling terminal the posix flag to open is not used by bsd based systems except in compatibility mode support for other systems figure summarizes the way each platform discussed in this book allocates a controlling terminal note that although mac os x is derived from bsd it behaves like system v when allocating a controlling terminal method freebsd linux mac os x solaris open without tiocsctty ioctl command figure how various implementations allocate controlling terminals there are times when a program wants to talk to the controlling terminal regardless of whether the standard input or standard output is redirected the way a program guarantees that it is talking to the controlling terminal is to open the file dev tty this special file is a synonym within the kernel for the controlling terminal naturally if the program doesn t have a controlling terminal the open of this device will fail the classic example is the getpass function which reads a password with terminal echoing turned off of course this function is called by the crypt program and can be used in a pipeline for example crypt salaries lpr decrypts the file salaries and pipes the output to the print spooler because crypt reads its input file on its standard input the standard input can t be used to enter the password also crypt is designed so that we have to enter the encryption password each time we run the program to prevent us from saving the password in a file which could be a security hole there are known ways to break the encoding used by the crypt program see garfinkel et al for more details on encrypting files tcgetpgrp tcsetpgrp and tcgetsid functions we need a way to tell the kernel which process group is the foreground process group so that the terminal device driver knows where to send the terminal input and the terminal generated signals figure the function tcgetpgrp returns the process group id of the foreground process group associated with the terminal open on fd if the process has a controlling terminal the process can call tcsetpgrp to set the foreground process group id to pgrpid the value of pgrpid must be the process group id of a process group in the same session and fd must refer to the controlling terminal of the session most applications don t call these two functions directly instead the functions are normally called by job control shells the tcgetsid function allows an application to obtain the process group id for the session leader given a file descriptor for the controlling tty applications that need to manage controlling terminals can use tcgetsid to identify the session id of the controlling terminal session leader which is equivalent to the session leader process group id job control job control is a feature that was added to bsd around this feature allows us to start multiple jobs groups of processes from a single terminal and to control which jobs can access the terminal and which jobs are run in the background job control requires three forms of support a shell that supports job control the terminal driver in the kernel must support job control the kernel must support certain job control signals provided a different form of job control called shell layers the bsd form of job control however was selected by posix and is what we describe here in earlier versions of the standard job control support was optional but posix now requires platforms to support it from our perspective when using job control from a shell we can start a job in either the foreground or the background a job is simply a collection of processes often a pipeline of processes for example vi main c starts a job consisting of one process in the foreground the commands pr c lpr make all start two jobs in the background all the processes invoked by these background jobs are in the background as we said to use the features provided by job control we need to use a shell that supports job control with older systems it was simple to say which shells supported job control and which didn t the c shell supported job control the bourne shell didn t and it was an option with the korn shell depending on whether the host supported job control but the c shell has been ported to systems e g earlier versions of system v that don t support job control and the bourne shell when invoked by the name jsh instead of sh supports job control the korn shell continues to support job control if the host does the bourne again shell also supports job control we ll just talk generically about a shell that supports job control versus one that doesn t when the difference between the various shells doesn t matter when we start a background job the shell assigns it a job identifier and prints one or more of the process ids the following script shows how the korn shell handles this make all make out pr c lpr just press return done pr c lpr done make all make out the make is job number and the starting process id is the next pipeline is job number and the process id of the first process is when the jobs are done and we press return the shell tells us that the jobs are complete the reason we have to press return is to have the shell print its prompt the shell doesn t print the changed status of background jobs at any random time only right before it prints its prompt to let us enter a new command line if the shell didn t do this it could produce output while we were entering an input line the interaction with the terminal driver arises because a special terminal character affects the foreground job the suspend key typically control z entering this character causes the terminal driver to send the sigtstp signal to all processes in the foreground process group the jobs in any background process groups aren t affected the terminal driver looks for three special characters which generate signals to the foreground process group the interrupt character typically delete or control c generates sigint the quit character typically control backslash generates sigquit the suspend character typically control z generates sigtstp in chapter we ll see how we can change these three characters to be any characters we choose and how we can disable the terminal driver processing of these special characters another job control condition can arise that must be handled by the terminal driver since we can have a foreground job and one or more background jobs which of these receives the characters that we enter at the terminal only the foreground job receives terminal input it is not an error for a background job to try to read from the terminal but the terminal driver detects this and sends a special signal to the background job sigttin this signal normally stops the background job by using the shell we are notified of this event and can bring the job into the foreground so that it can read from the terminal the following example demonstrates this cat temp foo start in background but it ll read from standard input we press return stopped sigttin cat temp foo fg bring job number into the foreground cat temp foo the shell tells us which job is now in the foreground hello world enter one line ˆd type the end of file character cat temp foo check that the one line was put into the file hello world note that this example doesn t work on mac os x when we try to bring the cat command into the foreground the read fails with errno set to eintr since mac os x is based on freebsd and freebsd works as expected this must be a bug in mac os x the shell starts the cat process in the background but when cat tries to read its standard input the controlling terminal the terminal driver knowing that it is a background job sends the sigttin signal to the background job the shell detects this change in status of its child recall our discussion of the wait and waitpid function in section and tells us that the job has been stopped we then move the stopped job into the foreground with the shell fg command refer to the manual page for the shell that you are using for all the details on its job control commands such as fg and bg and the various ways to identify the different jobs doing this causes the shell to place the job into the foreground process group tcsetpgrp and send the continue signal sigcont to the process group since it is now in the foreground process group the job can read from the controlling terminal what happens if a background job sends its output to the controlling terminal this is an option that we can allow or disallow normally we use the stty command to change this option we ll see in chapter how we can change this option from a program the following example shows how this works cat temp foo execute in background hello world the output from the background job appears after the prompt we press return done cat temp foo stty tostop disable ability of background jobs to output to controlling terminal cat temp foo try it again in the background we press return and find the job is stopped stopped sigttou cat temp foo fg resume stopped job in the foreground cat temp foo the shell tells us which job is now in the foreground hello world and here is its output when we disallow background jobs from writing to the controlling terminal cat will block when it tries to write to its standard output because the terminal driver identifies the write as coming from a background process and sends the job the sigttou signal as with the previous example when we use the shell fg command to bring the job into the foreground the job completes figure summarizes some of the features of job control that we ve been describing the solid lines through the terminal driver box mean that the terminal i o and the terminal generated signals are always connected from the foreground process init inetd or launchd exec after setsid then establishing controlling terminal login exec session figure summary of job control features with foreground and background jobs and terminal driver group to the actual terminal the dashed line corresponding to the sigttou signal means that whether the output from a process in the background process group appears on the terminal is an option is job control necessary or desirable job control was originally designed and implemented before windowing terminals were widespread some people claim that a well designed windowing system removes any need for job control some complain that the implementation of job control requiring support from the kernel the terminal driver the shell and some applications is a hack some use job control with a windowing system claiming a need for both regardless of your opinion job control is a required feature of posix shell execution of programs let examine how the shells execute programs and how this relates to the concepts of process groups controlling terminals and sessions to do this we ll use the ps command again first we ll use a shell that doesn t support job control the classic bourne shell running on solaris if we execute ps o pid ppid pgid sid comm the output is pid ppid pgid sid command sh ps the parent of the ps command is the shell which we would expect both the shell and the ps command are in the same session and foreground process group we say that is the foreground process group because that is what you get when you execute a command with a shell that doesn t support job control some platforms support an option to have the ps command print the process group id associated with the session controlling terminal this value would be shown under the tpgid column unfortunately the output of the ps command often differs among versions of the unix system for example solaris doesn t support this option under freebsd linux and mac os x the command ps o pid ppid pgid sid tpgid comm prints exactly the information we want note that it is misleading to associate a process with a terminal process group id the tpgid column a process does not have a terminal process control group a process belongs to a process group and the process group belongs to a session the session may or may not have a controlling terminal if the session does have a controlling terminal then the terminal device knows the process group id of the foreground process this value can be set in the terminal driver with the tcsetpgrp function as we show in figure the foreground process group id is an attribute of the terminal not the process this value from the terminal device driver is what ps prints as the tpgid if it finds that the session doesn t have a controlling terminal ps prints either or depending on the platform if we execute the command in the background ps o pid ppid pgid sid comm the only value that changes is the process id of the command pid ppid pgid sid command sh ps this shell doesn t know about job control so the background job is not put into its own process group and the controlling terminal isn t taken away from the background job now let look at how the bourne shell handles a pipeline when we execute ps o pid ppid pgid sid comm the output is pid ppid pgid sid command sh ps the program is just a copy of the standard cat program with a different name we have another copy of cat with the name which we ll use later in this section when we have two copies of cat in a pipeline the different names let us differentiate between the two programs note that the last process in the pipeline is the child of the shell and that the first process in the pipeline is a child of the last process it appears that the shell forks a copy of itself and that this copy then forks to make each of the previous processes in the pipeline if we execute the pipeline in the background ps o pid ppid pgid sid comm only the process ids change since the shell doesn t handle job control the process group id of the background processes remains as does the process group id of the session what happens in this case if a background process tries to read from its controlling terminal for example suppose that we execute cat temp foo with job control this is handled by placing the background job into a background process group which causes the signal sigttin to be generated if the background job tries to read from the controlling terminal the way this is handled without job control is that the shell automatically redirects the standard input of a background process to dev null if the process doesn t redirect standard input itself a read from dev null generates an end of file this means that our background cat process immediately reads an end of file and terminates normally the previous paragraph adequately handles the case of a background process accessing the controlling terminal through its standard input but what happens if a background process specifically opens dev tty and reads from the controlling terminal the answer is it depends but the result is probably not what we want for example crypt salaries lpr is such a pipeline we run it in the background but the crypt program opens dev tty changes the terminal characteristics to disable echoing reads from the device and resets the terminal characteristics when we execute this background pipeline the prompt password from crypt is printed on the terminal but what we enter the encryption password is read by the shell which tries to execute a command of that name the next line we enter to the shell is taken as the password and the file is not encrypted correctly sending junk to the printer here we have two processes trying to read from the same device at the same time and the result depends on the system job control as we described earlier handles this multiplexing of a single terminal between multiple processes in a better fashion returning to our bourne shell example if we execute three processes in the pipeline we can examine the process control used by this shell ps o pid ppid pgid sid comm this pipeline generates the following output pid ppid pgid sid command sh ps don t be alarmed if the output on your system doesn t show the proper command names sometimes you might get results such as pid ppid pgid sid command 947 sh 949 949 sh 949 949 ps 949 949 sh what happening here is that the ps process is racing with the shell which is forking and executing the cat commands in this case the shell hasn t yet completed the call to exec when ps has obtained the list of processes to print again the last process in the pipeline is the child of the shell and all previous processes in the pipeline are children of the last process figure shows what is happening fork c exec figure processes in the pipeline ps when invoked by bourne shell since the last process in the pipeline is the child of the login shell the shell is notified when that process terminates now let examine the same examples using a job control shell running on linux this shows the way these shells handle background jobs we ll use the bourne again shell in this example the results with other job control shells are almost identical ps o pid ppid pgid sid tpgid comm gives us pid ppid pgid sid tpgid command bash ps starting with this example we show the foreground process group in a bolder font we immediately see a difference from our bourne shell example the bourne again shell places the foreground job ps into its own process group the ps command is the process group leader and the only process in this process group furthermore this process group is the foreground process group since it has the controlling terminal our login shell is a background process group while the ps command executes note however that both process groups and are members of the same session indeed we ll see that the session never changes through our examples in this section executing this process in the background ps o pid ppid pgid sid tpgid comm gives us pid ppid pgid sid tpgid command bash ps again the ps command is placed into its own process group but this time the process group is no longer the foreground process group it is a background process group the tpgid of indicates that the foreground process group is our login shell executing two processes in a pipeline as in ps o pid ppid pgid sid tpgid comm gives us pid ppid pgid sid tpgid command bash ps both processes ps and are placed into a new process group and this is the foreground process group we can also see another difference between this example and the similar bourne shell example the bourne shell created the last process in the pipeline first and this final process was the parent of the first process here the bourne again shell is the parent of both processes if we execute this pipeline in the background ps o pid ppid pgid sid tpgid comm the results are similar but now ps and are placed in the same background process group pid ppid pgid sid tpgid command 2818 bash ps 2837 cat1 note that the order in which a shell creates processes can differ depending on the particular shell in use orphaned process groups we ve mentioned that a process whose parent terminates is called an orphan and is inherited by the init process we now look at entire process groups that can be orphaned and see how posix handles this situation example consider a process that forks a child and then terminates although this is nothing abnormal it happens all the time what happens if the child is stopped using job control when the parent terminates how will the child ever be continued and does the child know that it has been orphaned figure shows this situation the parent process has forked a child that stops and the parent is about to exit session figure example of a process group about to be orphaned the program that creates this situation is shown in figure this program has some new features here we are assuming a job control shell recall from the previous section that the shell places the foreground process into its own process group in include apue h include errno h static void int signo printf sighup received pid ld n long getpid static void char name int printf pid ld ppid ld pgrp ld tpgrp ld n name long getpid long getppid long getpgrp long tcgetpgrp fflush stdout main void char c pid parent if pid fork fork error else if pid parent sleep sleep to let child stop itself else child child signal sighup establish signal handler kill getpid sigtstp stop ourself child prints only if we re continued if read c printf read error d on controlling tty n errno exit figure creating an orphaned process group this example and that the shell stays in its own process group 2837 the child inherits the process group of its parent after the fork the parent sleeps for seconds this is our imperfect way of letting the child execute before the parent terminates the child establishes a signal handler for the hang up signal sighup so we can see whether it is sent to the child we discuss signal handlers in chapter the child sends itself the stop signal sigtstp with the kill function this stops the child similar to our stopping a foreground job with our terminal suspend character control z when the parent terminates the child is orphaned so the child parent process id becomes which is the init process id at this point the child is now a member of an orphaned process group the posix definition of an orphaned process group is one in which the parent of every member is either itself a member of the group or is not a member of the group session another way of saying this is that the process group is not orphaned as long as a process in the group has a parent in a different process group but in the same session if the process group is not orphaned there is a chance that one of those parents in a different process group but in the same session will restart a stopped process in the process group that is not orphaned here the parent of every process in the group e g process is the parent of process belongs to another session since the process group is orphaned when the parent terminates and the process group contains a stopped process posix requires that every process in the newly orphaned process group be sent the hang up signal sighup followed by the continue signal sigcont this causes the child to be continued after processing the hang up signal the default action for the hang up signal is to terminate the process so we have to provide a signal handler to catch the signal we therefore expect the printf in the function to appear before the printf in the function here is the output from the program shown in figure a out parent pid ppid 2837 pgrp tpgrp child pid ppid pgrp tpgrp sighup received pid child pid ppid pgrp tpgrp 2837 read error on controlling tty note that our shell prompt appears with the output from the child since two processes our login shell and the child are writing to the terminal as we expect the parent process id of the child has become after calling in the child the program tries to read from standard input as we saw earlier in this chapter when a process in a background process group tries to read from its controlling terminal sigttin is generated for the background process group but here we have an orphaned process group if the kernel were to stop it with this signal the processes in the process group would probably never be continued posix specifies that the read is to return an error with errno set to eio whose value is on this system in this situation finally note that our child was placed in a background process group when the parent terminated since the parent was executed as a foreground job by the shell we ll see another example of orphaned process groups in section with the pty program freebsd implementation having talked about the various attributes of a process process group session and controlling terminal it worth looking at how all this can be implemented we ll look briefly at the implementation used by freebsd some details of the implementation of these features can be found in williams figure shows the various data structures used by freebsd tty structure linked list of process group members pgrp structure session structure vnode structure proc structure proc structure proc structure figure freebsd implementation of sessions and process groups let look at all the fields that we ve labeled starting with the session structure one of these structures is allocated for each session e g each time setsid is called is the number of process groups in the session when this counter is decremented to the structure can be freed is a pointer to the proc structure of the session leader is a pointer to the vnode structure of the controlling terminal is a pointer to the tty structure of the controlling terminal is the session id recall that the concept of a session id is not part of the single unix specification when setsid is called a new session structure is allocated within the kernel now is set to is set to point to the proc structure of the calling process is set to the process id and and are set to null pointers since the new session doesn t have a controlling terminal let move to the tty structure the kernel contains one of these structures for each terminal device and each pseudo terminal device we talk more about pseudo terminals in chapter t_session points to the session structure that has this terminal as its controlling terminal note that the tty structure points to the session structure and vice versa this pointer is used by the terminal to send a hang up signal to the session leader if the terminal loses carrier figure points to the pgrp structure of the foreground process group this field is used by the terminal driver to send signals to the foreground process group the three signals generated by entering special characters interrupt quit and suspend are sent to the foreground process group is a structure containing all the special characters and related information for this terminal such as baud rate whether echo is enabled and so on we ll return to this structure in chapter is a winsize structure that contains the current size of the terminal window when the size of the terminal window changes the sigwinch signal is sent to the foreground process group we show how to set and fetch the terminal current window size in section to find the foreground process group of a particular session the kernel has to start with the session structure follow to get to the controlling terminal tty structure and then follow to get to the foreground process group pgrp structure the pgrp structure contains the information for a particular process group is the process group id points to the session structure for the session to which this process group belongs is a pointer to the list of proc structures that are members of this process group the structure in that proc structure is a doubly linked list entry that points to both the next process and the previous process in the group and so on until a null pointer is encountered in the proc structure of the last process in the group the proc structure contains all the information for a single process contains the process id is a pointer to the proc structure of the parent process points to the pgrp structure of the process group to which this process belongs is a structure containing pointers to the next and previous processes in the process group as we mentioned earlier finally we have the vnode structure this structure is allocated when the controlling terminal device is opened all references to dev tty in a process go through this vnode structure summary this chapter has described the relationships between groups of processes sessions which are made up of process groups job control is a feature supported by most unix systems today and we ve described how it implemented by a shell that supports job control the controlling terminal for a process dev tty is also involved in these process relationships we ve made numerous references to the signals that are used in all these process relationships the next chapter continues the discussion of signals looking at all the unix system signals in detail exercises refer back to our discussion of the utmp and wtmp files in section why are the logout records written by the init process is this handled the same way for a network login write a small program that calls fork and has the child create a new session verify that the child becomes a process group leader and that the child no longer has a controlling terminal signals introduction signals are software interrupts most nontrivial application programs need to deal with signals signals provide a way of handling asynchronous events for example a user at a terminal typing the interrupt key to stop a program or the next program in a pipeline terminating prematurely signals have been provided since the early versions of the unix system but the signal model provided with systems such as version was not reliable signals could get lost and it was difficult for a process to turn off selected signals when executing critical regions of code both and made changes to the signal model adding what are called reliable signals but the changes made by berkeley and at t were incompatible fortunately posix standardized the reliable signal routines and that is what we describe here in this chapter we start with an overview of signals and a description of what each signal is normally used for then we look at the problems with earlier implementations it is often important to understand what is wrong with an implementation before seeing how to do things correctly this chapter contains numerous examples that are not entirely correct and a discussion of the defects signal concepts first every signal has a name these names all begin with the three characters sig for example sigabrt is the abort signal that is generated when a process calls the abort function sigalrm is the alarm signal that is generated when the timer set by the alarm function goes off version had different signals and both had different signals freebsd supports different signals mac os x and linux each support different signals whereas solaris supports different signals freebsd linux and solaris however support additional application defined signals introduced to support real time applications although the posix real time extensions aren t covered in this book refer to gallmeister for more information as of the real time signal interfaces have moved to the base specification signal names are all defined by positive integer constants the signal number in the header signal h implementations actually define the individual signals in a different header file but this header file is included by signal h it is considered bad form for the kernel to include header files meant for user level applications so if the applications and the kernel both need the same definitions the information is placed in a kernel header file that is then included by the user level header file thus both freebsd and mac os x define the signals in sys signal h linux defines the signals in bits signum h and solaris defines them in sys iso h no signal has a signal number of we ll see in section that the kill function uses the signal number of for a special case posix calls this value the null signal numerous conditions can generate a signal the terminal generated signals occur when users press certain terminal keys pressing the delete key on the terminal or control c on many systems normally causes the interrupt signal sigint to be generated this is how to stop a runaway program we ll see in chapter how this signal can be mapped to any character on the terminal hardware exceptions generate signals divide by invalid memory reference and the like these conditions are usually detected by the hardware and the kernel is notified the kernel then generates the appropriate signal for the process that was running at the time the condition occurred for example sigsegv is generated for a process that executes an invalid memory reference the kill function allows a process to send any signal to another process or process group naturally there are limitations we have to be the owner of the process that we re sending the signal to or we have to be the superuser the kill command allows us to send signals to other processes this program is just an interface to the kill function this command is often used to terminate a runaway background process software conditions can generate signals when a process should be notified of various events these aren t hardware generated conditions as is the divide by condition but software conditions examples are sigurg generated when out of band data arrives over a network connection sigpipe generated when a process writes to a pipe that has no reader and sigalrm generated when an alarm clock set by the process expires signals are classic examples of asynchronous events they occur at what appear to be random times to the process the process can t simply test a variable such as errno to see whether a signal has occurred instead the process has to tell the kernel if and when this signal occurs do the following we can tell the kernel to do one of three things when a signal occurs we call this the disposition of the signal or the action associated with a signal ignore the signal this works for most signals but two signals can never be ignored sigkill and sigstop the reason these two signals can t be ignored is to provide the kernel and the superuser with a surefire way of either killing or stopping any process also if we ignore some of the signals that are generated by a hardware exception such as illegal memory reference or divide by the behavior of the process is undefined catch the signal to do this we tell the kernel to call a function of ours whenever the signal occurs in our function we can do whatever we want to handle the condition if we re writing a command interpreter for example when the user generates the interrupt signal at the keyboard we probably want to return to the main loop of the program terminating whatever command we were executing for the user if the sigchld signal is caught it means that a child process has terminated so the signal catching function can call waitpid to fetch the child process id and termination status as another example if the process has created temporary files we may want to write a signal catching function for the sigterm signal the termination signal that is the default signal sent by the kill command to clean up the temporary files note that the two signals sigkill and sigstop can t be caught let the default action apply every signal has a default action shown in figure note that the default action for most signals is to terminate the process figure lists the names of all the signals an indication of which systems support the signal and the default action for the signal the sus column contains if the signal is defined as part of the base posix specification and xsi if it is defined as part of the xsi option when the default action is labeled terminate core it means that a memory image of the process is left in the file named core of the current working directory of the process because the file is named core it shows how long this feature has been part of the unix system this file can be used with most unix system debuggers to examine the state of the process at the time it terminated the generation of the core file is an implementation feature of most versions of the unix system although this feature is not part of posix it is mentioned as a potential implementation specific action in the single unix specification xsi option the name of the core file varies among implementations on freebsd for example the core file is named cmdname core where cmdname is the name of the command corresponding to the process that received the signal on mac os x the core file is named core pid where pid is the id of the process that received the signal these systems allow the core filename to be configured via a sysctl parameter on linux the name is configured through proc sys kernel most implementations leave the core file in the current working directory of the corresponding process mac os x places all core files in cores instead name description iso c sus freebsd linux mac os x solaris default action sigabrt abnormal termination abort terminate core sigalrm timer expired alarm terminate sigbus hardware fault terminate core sigcancel threads library internal use ignore sigchld change in status of child ignore sigcont continue stopped process continue ignore sigemt hardware fault terminate core sigfpe arithmetic exception terminate core sigfreeze checkpoint freeze ignore sighup hangup terminate sigill illegal instruction terminate core siginfo status request from keyboard ignore sigint terminal interrupt character terminate sigio asynchronous i o terminate ignore sigiot hardware fault terminate core java virtual machine internal use ignore java virtual machine internal use ignore sigkill termination terminate siglost resource lost terminate siglwp threads library internal use terminate ignore sigpipe write to pipe with no readers terminate sigpoll pollable event poll terminate sigprof profiling time alarm setitimer terminate sigpwr power fail restart terminate ignore sigquit terminal quit character terminate core sigsegv invalid memory reference terminate core sigstkflt coprocessor stack fault terminate sigstop stop stop process sigsys invalid system call xsi terminate core sigterm termination terminate sigthaw checkpoint thaw ignore sigthr threads library internal use terminate sigtrap hardware fault xsi terminate core sigtstp terminal stop character stop process sigttin background read from control tty stop process sigttou background write to control tty stop process sigurg urgent condition sockets ignore user defined signal terminate user defined signal terminate sigvtalrm virtual time alarm setitimer xsi terminate sigwaiting threads library internal use ignore sigwinch terminal window size change ignore sigxcpu cpu limit exceeded setrlimit xsi terminate or terminate core sigxfsz file size limit exceeded setrlimit xsi terminate or terminate core sigxres resource control exceeded ignore figure unix system signals the core file will not be generated if a the process was set user id and the current user is not the owner of the program file b the process was set group id and the current user is not the group owner of the file c the user does not have permission to write in the current working directory d the file already exists and the user does not have permission to write to it or e the file is too big recall the limit in section the permissions of the core file assuming that the file doesn t already exist are usually user read and user write although mac os x sets only user read in figure the signals with a description of hardware fault correspond to implementation defined hardware faults many of these names are taken from the original pdp implementation of the unix system check your system manuals to determine exactly which type of error these signals correspond to we now describe each of these signals in more detail sigabrt this signal is generated by calling the abort function section the process terminates abnormally sigalrm this signal is generated when a timer set with the alarm function expires see section for more details this signal is also generated when an interval timer set by the setitimer function expires sigbus this signal indicates an implementation defined hardware fault implementations usually generate this signal on certain types of memory faults as we describe in section sigcancel this signal is used internally by the solaris threads library it is not meant for general use sigchld whenever a process terminates or stops the sigchld signal is sent to the parent by default this signal is ignored so the parent must catch this signal if it wants to be notified whenever a child status changes the normal action in the signal catching function is to call one of the wait functions to fetch the child process id and termination status earlier releases of system v had a similar signal named sigcld without the h the semantics of this signal were different from those of other signals and as far back as the manual page strongly discouraged its use in new programs strangely enough this warning disappeared in the and versions of the manual page applications should use the standard sigchld signal but be aware that many systems define sigcld to be the same as sigchld for backward compatibility if you maintain software that uses sigcld you need to check your system manual page to see which semantics it follows we discuss these two signals in section sigcont this job control signal is sent to a stopped process when it is continued the default action is to continue a stopped process but to ignore the signal if the process wasn t stopped a full screen editor for example might catch this signal and use the signal handler to make a note to redraw the terminal screen see section for additional details sigemt this indicates an implementation defined hardware fault the name emt comes from the pdp emulator trap instruction not all platforms support this signal on linux for example sigemt is supported only for selected architectures such as sparc mips and pa risc sigfpe this signals an arithmetic exception such as divide by floating point overflow and so on sigfreeze this signal is defined only by solaris it is used to notify processes that need to take special action before freezing the system state such as might happen when a system goes into hibernation or suspended mode sighup this signal is sent to the controlling process session leader associated with a controlling terminal if a disconnect is detected by the terminal interface referring to figure we see that the signal is sent to the process pointed to by the field in the session structure this signal is generated for this condition only if the terminal clocal flag is not set the clocal flag for a terminal is set if the attached terminal is local the flag tells the terminal driver to ignore all modem status lines we describe how to set this flag in chapter note that the session leader that receives this signal may be in the background see figure for an example this differs from the normal terminal generated signals interrupt quit and suspend which are always delivered to the foreground process group this signal is also generated if the session leader terminates in this case the signal is sent to each process in the foreground process group this signal is commonly used to notify daemon processes chapter to reread their configuration files the reason sighup is chosen for this task is that a daemon should not have a controlling terminal and would normally never receive this signal sigill this signal indicates that the process has executed an illegal hardware instruction generated this signal from the abort function sigabrt is now used for this purpose siginfo this bsd signal is generated by the terminal driver when we type the status key often control t this signal is sent to all processes in the foreground process group refer to figure this signal normally causes status information on processes in the foreground process group to be displayed on the terminal linux doesn t provide support for siginfo although the symbol is defined to be the same value as sigpwr on the alpha platform this is most likely to provide some level of compatibility with software developed for osf sigint this signal is generated by the terminal driver when we press the interrupt key often delete or control c this signal is sent to all processes in the foreground process group refer to figure this signal is often used to terminate a runaway program especially when it generating a lot of unwanted output on the screen sigio this signal indicates an asynchronous i o event we discuss it in section in figure we labeled the default action for sigio as either terminate or ignore unfortunately the default depends on the system under system v sigio is identical to sigpoll so its default action is to terminate the process under bsd the default is to ignore the signal linux and solaris define sigio to be the same value as sigpoll so the default behavior is to terminate the process on freebsd and mac os x the default is to ignore the signal sigiot this indicates an implementation defined hardware fault the name iot comes from the pdp mnemonic for the input output trap instruction earlier versions of system v generated this signal from the abort function sigabrt is now used for this purpose on freebsd linux mac os x and solaris sigiot is defined to be the same value as sigabrt a signal reserved for use by the java virtual machine on solaris another signal reserved for use by the java virtual machine on solaris sigkill this signal is one of the two that can t be caught or ignored it provides the system administrator with a sure way to kill any process siglost this signal is used to notify a process running on a solaris client system that a lock could not be reacquired during recovery siglwp this signal is used internally by the solaris threads library it is not available for general use on freebsd siglwp is defined to be an alias for sigthr sigpipe if we write to a pipeline but the reader has terminated sigpipe is generated we describe pipes in section this signal is also generated when a process writes to a socket of type that is no longer connected we describe sockets in chapter sigpoll this signal is marked obsolescent in so it might be removed in a future version of the standard it can be generated when a specific event occurs on a pollable device we describe this signal with the poll function in section sigpoll originated with and loosely corresponds to the bsd sigio and sigurg signals on linux and solaris sigpoll is defined to have the same value as sigio sigprof this signal is marked obsolescent in so it might be removed in a future version of the standard this signal is generated when a profiling interval timer set by the setitimer function expires sigpwr this signal is system dependent its main use is on a system that has an uninterruptible power supply ups if power fails the ups takes over and the software can usually be notified nothing needs to be done at this point as the system continues running on battery power but if the battery gets low for example if the power is off for an extended period the software is usually notified again at this point it behooves the system to shut everything down this is when sigpwr should be sent on most systems the process that is notified of the low battery condition sends the sigpwr signal to the init process and init handles the system shutdown solaris and some linux distributions have entries in the inittab file for this purpose powerfail and powerwait or powerokwait in figure we labeled the default action for sigpwr as either terminate or ignore unfortunately the default depends on the system the default on linux is to terminate the process on solaris the signal is ignored by default sigquit this signal is generated by the terminal driver when we press the terminal quit key often control backslash this signal is sent to all processes in the foreground process group refer to figure this signal not only terminates the foreground process group as does sigint but also generates a core file sigsegv this signal indicates that the process has made an invalid memory reference which is usually a sign that the program has a bug such as dereferencing an uninitialized pointer the name segv stands for segmentation violation sigstkflt this signal is defined only by linux it showed up in the earliest versions of linux where it was intended to be used for stack faults taken by the math coprocessor this signal is not generated by the kernel but remains for backward compatibility sigstop this job control signal stops a process it is similar to the interactive stop signal sigtstp but sigstop cannot be caught or ignored sigsys this signals an invalid system call somehow the process executed a machine instruction that the kernel thought was a system call but the parameter with the instruction that indicates the type of system call was invalid this might happen if you build a program that uses a new system call and you then try to run the same binary on an older version of the operating system where the system call doesn t exist sigterm this is the termination signal sent by the kill command by default because it can be caught by applications using sigterm gives programs a chance to terminate gracefully by cleaning up before exiting in contrast to sigkill which can t be caught or ignored sigthaw this signal is defined only by solaris and is used to notify processes that need to take special action when the system resumes operation after being suspended sigthr this is a signal reserved for use by the thread library on freebsd it is defined to have the same value as siglwp sigtrap this signal indicates an implementation defined hardware fault the signal name comes from the pdp trap instruction implementations often use this signal to transfer control to a debugger when a breakpoint instruction is executed sigtstp this interactive stop signal is generated by the terminal driver when we press the terminal suspend key often control z this signal is sent to all processes in the foreground process group refer to figure unfortunately the term stop has different meanings when discussing job control and signals we talk about stopping and continuing jobs the terminal driver however has historically used the term stop to refer to stopping and starting the terminal output using the control s and control q characters therefore the terminal driver calls the character that generates the interactive stop signal the suspend character not the stop character sigttin this signal is generated by the terminal driver when a process in a background process group tries to read from its controlling terminal refer to the discussion of this topic in section as special cases if either a the reading process is ignoring or blocking this signal or b the process group of the reading process is orphaned then the signal is not generated instead the read operation fails with errno set to eio sigttou this signal is generated by the terminal driver when a process in a background process group tries to write to its controlling terminal this is discussed in section unlike the case with background reads a process can choose to allow background writes to the controlling terminal we describe how to modify this option in chapter if background writes are not allowed then like the sigttin signal there are two special cases if either a the writing process is ignoring or blocking this signal or b the process group of the writing process is orphaned then the signal is not generated instead the write operation returns an error with errno set to eio regardless of whether background writes are allowed certain terminal operations other than writing can also generate the sigttou signal these include tcsetattr tcsendbreak tcdrain tcflush tcflow and tcsetpgrp we describe these terminal operations in chapter sigurg this signal notifies the process that an urgent condition has occurred it is optionally generated when out of band data is received on a network connection this is a user defined signal for use in application programs this is another user defined signal similar to for use in application programs sigvtalrm this signal is generated when a virtual interval timer set by the setitimer function expires sigwaiting this signal is used internally by the solaris threads library and is not available for general use sigwinch the kernel maintains the size of the window associated with each terminal and pseudo terminal a process can get and set the window size with the ioctl function which we describe in section if a process changes the window size from its previous value using the ioctl set window size command the kernel generates the sigwinch signal for the foreground process group sigxcpu the single unix specification supports the concept of resource limits as part of the xsi option refer to section if the process exceeds its soft cpu time limit the sigxcpu signal is generated in figure we labeled the default action for sigxcpu as either terminate or terminate with a core file the default depends on the operating system linux and solaris support a default action of terminate with a core file whereas freebsd and mac os x support a default action of terminate without generating a core file the single unix specification requires that the default action be to terminate the process abnormally whether a core file is generated is left up to the implementation sigxfsz this signal is generated if the process exceeds its soft file size limit refer to section just as with sigxcpu the default action taken with sigxfsz depends on the operating system on linux and solaris the default is to terminate the process and create a core file on freebsd and mac os x the default is to terminate the process without generating a core file the single unix specification requires that the default action be to terminate the process abnormally whether a core file is generated is left up to the implementation sigxres this signal is defined only by solaris it is optionally used to notify processes that have exceeded a preconfigured resource value the solaris resource control mechanism is a general facility for controlling the use of shared resources among independent application sets signal function the simplest interface to the signal features of the unix system is the signal function the signal function is defined by iso c which doesn t involve multiple processes process groups terminal i o and the like therefore its definition of signals is vague enough to be almost useless for unix systems implementations derived from unix system v support the signal function but it provides the old unreliable signal semantics we describe these older semantics in section the signal function provides backward compatibility for applications that require the older semantics new applications should not use these unreliable signals also provides the signal function but it is defined in terms of the sigaction function which we describe in section so using it under provides the newer reliable signal semantics most current systems follow this strategy but solaris follows the system v semantics for the signal function because the semantics of signal differ among implementations we must use the sigaction function instead we provide an implementation of signal that uses sigaction in section all the examples in this text use the signal function from figure to give us consistent semantics regardless of which particular platform we use the signo argument is just the name of the signal from figure the value of func is a the constant b the constant or c the address of a function to be called when the signal occurs if we specify we are telling the system to ignore the signal remember that we cannot ignore the two signals sigkill and sigstop when we specify we are setting the action associated with the signal to its default value see the final column in figure when we specify the address of a function to be called when the signal occurs we are arranging to catch the signal we call the function either the signal handler or the signal catching function the prototype for the signal function states that the function requires two arguments and returns a pointer to a function that returns nothing void the signal function first argument signo is an integer the second argument is a pointer to a function that takes a single integer argument and returns nothing the function whose address is returned as the value of signal takes a single integer argument the final int in plain english this declaration says that the signal handler is passed a single integer argument the signal number and that it returns nothing when we call signal to establish the signal handler the second argument is a pointer to the function the return value from signal is the pointer to the previous signal handler many systems call the signal handler with additional implementation dependent arguments we discuss this further in section the perplexing signal function prototype shown at the beginning of this section can be made much simpler through the use of the following typedef plauger typedef void sigfunc int then the prototype becomes sigfunc signal int sigfunc we ve included this typedef in apue h appendix b and use it with the functions in this chapter if we examine the system header signal h we will probably find declarations of the form define void define void define void these constants can be used in place of the pointer to a function that takes an integer argument and returns nothing the second argument to signal and the return value from signal the three values used for these constants need not be and they must be three values that can never be the address of any declarable function most unix systems use the values shown example figure shows a simple signal handler that catches either of the two user defined signals and prints the signal number in section we describe the pause function which simply suspends the calling process until a signal is received include apue h static void int one handler for both signals int main void if signal can t catch if signal can t catch for pause static void int signo argument is signal number if signo printf received n else if signo printf received n else received signal d n signo figure simple program to catch and we invoke the program in the background and use the kill command to send it signals note that the term kill in the unix system is a misnomer the kill command and the kill function just send a signal to a process or process group whether that signal terminates the process depends on which signal is sent and whether the process has arranged to catch the signal a out start process in background job control shell prints job number and process id kill send it received kill send it received kill now send it sigterm terminated a out when we send the sigterm signal the process is terminated since it doesn t catch the signal and the default action for the signal is termination program start up when a program is executed the status of all signals is either default or ignore normally all signals are set to their default action unless the process that calls exec is ignoring the signal specifically the exec functions change the disposition of any signals being caught to their default action and leave the status of all other signals alone naturally a signal that is being caught by a process that calls exec cannot be caught by the same function in the new program since the address of the signal catching function in the caller probably has no meaning in the new program file that is executed one specific example of this signal status behavior is how an interactive shell treats the interrupt and quit signals for a background process with a shell that doesn t support job control when we execute a process in the background as in cc main c the shell automatically sets the disposition of the interrupt and quit signals in the background process to be ignored this is done so that if we type the interrupt character it doesn t affect the background process if this weren t done and we typed the interrupt character it would terminate not only the foreground process but also all the background processes many interactive programs that catch these two signals have code that looks like void int int if signal sigint signal sigint if signal sigquit signal sigquit following this approach the process catches the signal only if the signal is not currently being ignored these two calls to signal also show a limitation of the signal function we are not able to determine the current disposition of a signal without changing the disposition we ll see later in this chapter how the sigaction function allows us to determine a signal disposition without changing it process creation when a process calls fork the child inherits the parent signal dispositions here since the child starts off with a copy of the parent memory image the address of a signal catching function has meaning in the child unreliable signals in earlier versions of the unix system such as version signals were unreliable by this we mean that signals could get lost a signal could occur and the process would never know about it also a process had little control over a signal a process could catch the signal or ignore it sometimes we would like to tell the kernel to block a signal don t ignore it just remember if it occurs and tell us later when we re ready changes were made with to provide what are called reliable signals a different set of changes was then made in to provide reliable signals under system v posix chose the bsd model to standardize one problem with these early versions was that the action for a signal was reset to its default each time the signal occurred in the previous example when we ran the program in figure we avoided this detail by catching each signal only once the classic example from programming books that described these earlier systems concerns how to handle the interrupt signal the code that was described usually looked like int my signal handling function signal sigint establish handler signal sigint reestablish handler for next time process the signal the reason the signal handler is declared as returning an integer is that these early systems didn t support the iso c void data type the problem with this code fragment is that there is a window of time after the signal has occurred but before the call to signal in the signal handler when the interrupt signal could occur another time this second signal would cause the default action to occur which for this signal terminates the process this is one of those conditions that works correctly most of the time causing us to think that it is correct when it isn t another problem with these earlier systems was that the process was unable to turn a signal off when it didn t want the signal to occur all the process could do was ignore the signal there are times when we would like to tell the system prevent the following signals from interrupting me but remember if they do occur the classic example that demonstrates this flaw is shown by a piece of code that catches a signal and sets a flag for the process that indicates that the signal occurred int my signal handling function int set nonzero when signal occurs main signal sigint establish handler while pause go to sleep waiting for signal signal sigint reestablish handler for next time set flag for main loop to examine here the process is calling the pause function to put it to sleep until a signal is caught when the signal is caught the signal handler just sets the flag to a nonzero value the process is automatically awakened by the kernel after the signal handler returns notices that the flag is nonzero and does whatever it needs to do but there is a window of time when things can go wrong if the signal occurs after the test of but before the call to pause the process could go to sleep forever assuming that the signal is never generated again this occurrence of the signal is lost this is another example of some code that isn t right yet it works most of the time debugging this type of problem can be difficult interrupted system calls a characteristic of earlier unix systems was that if a process caught a signal while the process was blocked in a slow system call the system call was interrupted the system call returned an error and errno was set to eintr this was done under the assumption that since a signal occurred and the process caught it there is a good chance that something has happened that should wake up the blocked system call here we have to differentiate between a system call and a function it is a system call within the kernel that is interrupted when a signal is caught to support this feature the system calls are divided into two categories the slow system calls and all the others the slow system calls are those that can block forever included in this category are reads that can block the caller forever if data isn t present with certain file types pipes terminal devices and network devices writes that can block the caller forever if the data can t be accepted immediately by these same file types opens on certain file types that block the caller until some condition occurs such as a terminal device open waiting until an attached modem answers the phone the pause function which by definition puts the calling process to sleep until a signal is caught and the wait function certain ioctl operations some of the interprocess communication functions chapter the notable exception to these slow system calls is anything related to disk i o although a read or a write of a disk file can block the caller temporarily while the disk driver queues the request and then the request is executed unless a hardware error occurs the i o operation always returns and unblocks the caller quickly one condition that is handled by interrupted system calls for example is when a process initiates a read from a terminal device and the user at the terminal walks away from the terminal for an extended period in this example the process could be blocked for hours or days and would remain so unless the system was taken down posix semantics for interrupted reads and writes changed with the version of the standard earlier versions gave implementations a choice of how to deal with reads and writes that have processed partial amounts of data if read has received and transferred data to an application buffer but has not yet received all that the application requested and is then interrupted the operating system could either fail the system call with errno set to eintr or allow the system call to succeed returning the partial amount of data received similarly if write is interrupted after transferring some of the data in an application buffer the operation system could either fail the system call with errno set to eintr or allow the system call to succeed returning the partial amount of data written historically implementations derived from system v fail the system call whereas bsd derived implementations return partial success with the version of the posix standard the bsd style semantics are required the problem with interrupted system calls is that we now have to handle the error return explicitly the typical code sequence assuming a read operation and assuming that we want to restart the read even if it interrupted would be again if n read fd buf buffsize if errno eintr goto again just an interrupted system call handle other errors to prevent applications from having to handle interrupted system calls introduced the automatic restarting of certain interrupted system calls the system calls that were automatically restarted are ioctl read readv write writev wait and waitpid as we ve mentioned the first five of these functions are interrupted by a signal only if they are operating on a slow device wait and waitpid are always interrupted when a signal is caught since this caused a problem for some applications that didn t want the operation restarted if it was interrupted allowed the process to disable this feature on a per signal basis posix requires an implementation to restart system calls only when the flag is in effect for the interrupting signal as we will see in section this flag is used with the sigaction function to allow applications to request that interrupted system calls be restarted historically when using the signal function to establish a signal handler implementations varied with respect to how interrupted system calls were handled system v never restarted system calls by default bsd in contrast restarted them if the calls were interrupted by signals on freebsd linux and mac os x when signal handlers are installed with the signal function interrupted system calls will be restarted the default on solaris however is to return an error eintr instead when system calls are interrupted by signal handlers installed with the signal function by using our own implementation of the signal function shown in figure we avoid having to deal with these differences one of the reasons introduced the automatic restart feature is that sometimes we don t know that the input or output device is a slow device if the program we write can be used interactively then it might be reading or writing a slow device since terminals fall into this category if we catch signals in this program and if the system doesn t provide the restart capability then we have to test every read or write for the interrupted error return and reissue the read or write figure summarizes the signal functions and their semantics provided by the various implementations functions system signal handler remains installed ability to block signals automatic restart of interrupted system calls signal iso c posix unspecified unspecified unspecified never solaris never always 4bsd freebsd linux mac os x default sigaction posix 4bsd freebsd linux mac os x solaris optional figure features provided by various signal implementations be aware that unix systems from other vendors can have values different from those shown in this figure for example sigaction under sunos restarts an interrupted system call by default unlike the platforms listed in figure in figure we provide our own version of the signal function that automatically tries to restart interrupted system calls other than for the sigalrm signal in figure we provide another function that tries to never do the restart we talk more about interrupted system calls in section with regard to the select and poll functions reentrant functions when a signal that is being caught is handled by a process the normal sequence of instructions being executed by the process is temporarily interrupted by the signal handler the process then continues executing but the instructions in the signal handler are now executed if the signal handler returns instead of calling exit or longjmp for example then the normal sequence of instructions that the process was executing when the signal was caught continues executing this is similar to what happens when a hardware interrupt occurs but in the signal handler we can t tell where the process was executing when the signal was caught what if the process was in the middle of allocating additional memory on its heap using malloc and we call malloc from the signal handler or what if the process was in the middle of a call to a function such as getpwnam section that stores its result in a static location and we call the same function from the signal handler in the malloc example havoc can result for the process since malloc usually maintains a linked list of all its allocated areas and it may have been in the middle of changing this list in the case of getpwnam the information returned to the normal caller can get overwritten with the information returned to the signal handler the single unix specification specifies the functions that are guaranteed to be safe to call from within a signal handler these functions are reentrant and are called async signal safe by the single unix specification besides being reentrant they block any signals during operation if delivery of a signal might cause inconsistencies figure lists these async signal safe functions most of the functions that are not included in figure are missing because a they are known to use static data structures b they call malloc or free or c they are part of the standard i o library most implementations of the standard i o library use global data structures in a nonreentrant way note that even though we call printf from signal handlers in some of our examples it is not guaranteed to produce the expected results since the signal handler can interrupt a call to printf from our main program be aware that even if we call a function listed in figure from a signal handler there is only one errno variable per thread recall the discussion of errno and threads in section and we might potentially modify its value consider a signal handler that is invoked right after main has set errno if the signal handler calls read for example this call can change the value of errno wiping out the value that was just abort faccessat linkat select socketpair accept fchmod listen stat access fchmodat lseek send symlink fchown lstat sendmsg symlinkat fchownat mkdir sendto tcdrain fcntl mkdirat setgid tcflow alarm fdatasync mkfifo setpgid tcflush bind fexecve mkfifoat setsid tcgetattr cfgetispeed fork mknod setsockopt tcgetpgrp cfgetospeed fstat mknodat setuid tcsendbreak cfsetispeed fstatat open shutdown tcsetattr cfsetospeed fsync openat sigaction tcsetpgrp chdir ftruncate pause sigaddset time chmod futimens pipe sigdelset chown getegid poll sigemptyset geteuid sigfillset close getgid pselect sigismember times connect getgroups raise signal umask creat getpeername read sigpause uname dup getpgrp readlink sigpending unlink getpid readlinkat sigprocmask unlinkat execl getppid recv sigqueue utime execle getsockname recvfrom sigset utimensat execv getsockopt recvmsg sigsuspend utimes execve getuid rename sleep wait kill renameat sockatmark waitpid link rmdir socket write figure reentrant functions that may be called from a signal handler stored in main therefore as a general rule when calling the functions listed in figure from a signal handler we should save and restore errno be aware that a commonly caught signal is sigchld and its signal handler usually calls one of the wait functions all the wait functions can change errno note that longjmp section and siglongjmp section are missing from figure because the signal may have occurred while the main routine was updating a data structure in a nonreentrant way this data structure could be left half updated if we call siglongjmp instead of returning from the signal handler if it is going to do such things as update global data structures as we describe here while catching signals that cause sigsetjmp to be executed an application needs to block the signals while updating the data structures example figure shows a program that calls the nonreentrant function getpwnam from a signal handler that is called every second we describe the alarm function in section we use it here to generate a sigalrm signal every second include apue h include pwd h static void int signo int struct passwd rootptr printf in signal handler n if rootptr getpwnam root null getpwnam root error alarm main void struct passwd ptr signal sigalrm alarm for if ptr getpwnam sar null getpwnam error if strcmp ptr sar printf return value corrupted n ptr figure call a nonreentrant function from a signal handler when this program was run the results were random usually the program would be terminated by a sigsegv signal when the signal handler returned after several iterations an examination of the core file showed that the main function had called getpwnam but that when getpwnam called free the signal handler interrupted it and called getpwnam which in turn called free the data structures maintained by malloc and free had been corrupted when the signal handler indirectly called free while the main function was also calling free occasionally the program would run for several seconds before crashing with a sigsegv error when the main function did run correctly after the signal had been caught the return value was sometimes corrupted and sometimes fine as shown by this example if we call a nonreentrant function from a signal handler the results are unpredictable sigcld semantics two signals that continually generate confusion are sigcld and sigchld the name sigcld without the h is from system v and this signal has different semantics from the bsd signal named sigchld the posix signal is also named sigchld the semantics of the bsd sigchld signal are normal in the sense that its semantics are similar to those of all other signals when the signal occurs the status of a child has changed and we need to call one of the wait functions to determine what has happened system v however has traditionally handled the sigcld signal differently from other signals based systems continue this questionable tradition i e compatibility constraint if we set its disposition using either signal or sigset the older compatible functions to set the disposition of a signal this older handling of sigcld consists of the following behavior if the process specifically sets its disposition to children of the calling process will not generate zombie processes note that this is different from its default action which from figure is to be ignored instead on termination the status of these child processes is discarded if it subsequently calls one of the wait functions the calling process will block until all its children have terminated and then wait returns with errno set to echild the default disposition of this signal is to be ignored but this default will not cause the preceding semantics to occur instead we specifically have to set its disposition to posix does not specify what happens when sigchld is ignored so this behavior is allowed the xsi option requires this behavior to be supported for sigchld 4bsd always generates zombies if sigchld is ignored if we want to avoid zombies we have to wait for our children with if either signal or sigset is called to set the disposition of sigchld to be ignored zombies are never generated all four platforms described in this book follow in this behavior with sigaction we can set the flag figure to avoid zombies this action is also supported on all four platforms if we set the disposition of sigcld to be caught the kernel immediately checks whether any child processes are ready to be waited for and if so calls the sigcld handler item changes the way we have to write a signal handler for this signal as illustrated in the following example example recall from section that the first thing to do on entry to a signal handler is to call signal again to reestablish the handler this action is intended to minimize the window of time when the signal is reset back to its default and could get lost we show this in figure this program doesn t work on traditional system v platforms the output is a continual string of sigcld received lines eventually the process runs out of stack space and terminates abnormally include apue h include sys wait h static void int int main pid if signal sigcld perror signal error if pid fork perror fork error else if pid child sleep pause parent exit static void int signo interrupts pause pid int status printf sigcld received n if signal sigcld reestablish handler perror signal error if pid wait status fetch child status perror wait error printf pid d n pid figure system v sigcld handler that doesn t work freebsd and mac os x don t exhibit this problem because bsd based systems generally don t support historical system v semantics for sigcld linux also doesn t exhibit this problem because it doesn t call the sigchld signal handler when a process arranges to catch sigchld and child processes are ready to be waited for even though sigcld and sigchld are defined to be the same value solaris on the other hand does call the signal handler in this situation but includes extra code in the kernel to avoid this problem although the four platforms described in this book solve this problem realize that platforms such as aix still exist that haven t addressed it the problem with this program is that the call to signal at the beginning of the signal handler invokes item from the preceding discussion the kernel checks whether a child needs to be waited for which is the case since we re processing a sigcld signal so it generates another call to the signal handler the signal handler calls signal and the whole process starts over again to fix this program we have to move the call to signal after the call to wait by doing this we call signal after fetching the child termination status the signal is generated again by the kernel only if some other child has since terminated posix states that when we establish a signal handler for sigchld and there exists a terminated child we have not yet waited for it is unspecified whether the signal is generated this allows the behavior described previously but since posix does not reset a signal disposition to its default when the signal occurs assuming that we re using the posix sigaction function to set its disposition there is no need for us to ever establish a signal handler for sigchld within that handler be cognizant of the sigchld semantics for your implementation be especially aware of some systems that define sigchld to be sigcld or vice versa changing the name may allow you to compile a program that was written for another system but if that program depends on the other semantics it may not work of the four platforms described in this text only linux and solaris define sigcld on these platforms sigcld is equivalent to sigchld reliable signal terminology and semantics we need to define some of the terms used throughout our discussion of signals first a signal is generated for a process or sent to a process when the event that causes the signal occurs the event could be a hardware exception e g divide by a software condition e g an alarm timer expiring a terminal generated signal or a call to the kill function when the signal is generated the kernel usually sets a flag of some form in the process table we say that a signal is delivered to a process when the action for a signal is taken during the time between the generation of a signal and its delivery the signal is said to be pending a process has the option of blocking the delivery of a signal if a signal that is blocked is generated for a process and if the action for that signal is either the default action or to catch the signal then the signal remains pending for the process until the process either a unblocks the signal or b changes the action to ignore the signal the system determines what to do with a blocked signal when the signal is delivered not when it generated this allows the process to change the action for the signal before it delivered the sigpending function section can be called by a process to determine which signals are blocked and pending what happens if a blocked signal is generated more than once before the process unblocks the signal posix allows the system to deliver the signal either once or more than once if the system delivers the signal more than once we say that the signals are queued most unix systems however do not queue signals unless they support the real time extensions to posix instead the unix kernel simply delivers the signal once with the real time signal functionality moved from the real time extensions to the base specification as time goes on more systems will support queueing signals even if they don t support the real time extensions we discuss queueing signals further in section the manual pages for claimed that the sigcld signal was queued while the process was executing its sigcld signal handler although this might have been true on a conceptual level the actual implementation was different instead the signal was regenerated by the kernel as we described in section in the manual was changed to indicate that the sigcld signal was ignored while the process was executing its signal handler for sigcld the manual removed any mention of what happens to sigcld signals that are generated while a process is executing its sigcld signal handler the sigaction manual page in at t claims that the flag figure causes signals to be reliably queued this is wrong apparently this feature was partially implemented within the kernel but it is not enabled in curiously the svid didn t make the same claims of reliable queuing what happens if more than one signal is ready to be delivered to a process posix does not specify the order in which the signals are delivered to the process the rationale for posix does suggest however that signals related to the current state of the process be delivered before other signals sigsegv is one such signal each process has a signal mask that defines the set of signals currently blocked from delivery to that process we can think of this mask as having one bit for each possible signal if the bit is on for a given signal that signal is currently blocked a process can examine and change its current signal mask by calling sigprocmask which we describe in section since it is possible for the number of signals to exceed the number of bits in an integer posix defines a data type called that holds a signal set the signal mask for example is stored in one of these signal sets we describe five functions that operate on signal sets in section kill and raise functions the kill function sends a signal to a process or a group of processes the raise function allows a process to send a signal to itself the raise function was originally defined by iso c posix includes it to align itself with the iso c standard but posix extends the specification of raise to deal with threads we discuss how threads interact with signals in section since iso c does not deal with multiple processes it could not define a function such as kill that requires a process id argument include signal h int kill pid int signo int raise int signo the call raise signo is equivalent to the call kill getpid signo both return if ok on error there are four different conditions for the pid argument to kill pid the signal is sent to the process whose process id is pid pid the signal is sent to all processes whose process group id equals the process group id of the sender and for which the sender has permission to send the signal note that the term all processes excludes an implementation defined set of system processes for most unix systems this set of system processes includes the kernel processes and init pid pid the signal is sent to all processes whose process group id equals the absolute value of pid and for which the sender has permission to send the signal again the set of all processes excludes certain system processes as described earlier pid the signal is sent to all processes on the system for which the sender has permission to send the signal as before the set of processes excludes certain system processes as we ve mentioned a process needs permission to send a signal to another process the superuser can send a signal to any process for other users the basic rule is that the real or effective user id of the sender has to equal the real or effective user id of the receiver if the implementation supports as posix now requires the saved set user id of the receiver is checked instead of its effective user id one special case for the permission testing also exists if the signal being sent is sigcont a process can send it to any other process in the same session posix defines signal number as the null signal if the signo argument is then the normal error checking is performed by kill but no signal is sent this technique is often used to determine if a specific process still exists if we send the process the null signal and it doesn t exist kill returns and errno is set to esrch be aware however that unix systems recycle process ids after some amount of time so the existence of a process with a given process id does not necessarily mean that it the process that you think it is also understand that the test for process existence is not atomic by the time that kill returns the answer to the caller the process in question might have exited so the answer is of limited value if the call to kill causes the signal to be generated for the calling process and if the signal is not blocked either signo or some other pending unblocked signal is delivered to the process before kill returns additional conditions occur with threads see section for more information alarm and pause functions the alarm function allows us to set a timer that will expire at a specified time in the future when the timer expires the sigalrm signal is generated if we ignore or don t catch this signal its default action is to terminate the process the seconds value is the number of clock seconds in the future when the signal should be generated when that time occurs the signal is generated by the kernel although additional time could elapse before the process gets control to handle the signal because of processor scheduling delays earlier unix system implementations warned that the signal could also be sent up to second early posix does not allow this behavior there is only one of these alarm clocks per process if when we call alarm a previously registered alarm clock for the process has not yet expired the number of seconds left for that alarm clock is returned as the value of this function that previously registered alarm clock is replaced by the new value if a previously registered alarm clock for the process has not yet expired and if the seconds value is the previous alarm clock is canceled the number of seconds left for that previous alarm clock is still returned as the value of the function although the default action for sigalrm is to terminate the process most processes that use an alarm clock catch this signal if the process then wants to terminate it can perform whatever cleanup is required before terminating if we intend to catch sigalrm we need to be careful to install its signal handler before calling alarm if we call alarm first and are sent sigalrm before we can install the signal handler our process will terminate the pause function suspends the calling process until a signal is caught include unistd h int pause void returns with errno set to eintr the only time pause returns is if a signal handler is executed and that handler returns in that case pause returns with errno set to eintr example using alarm and pause we can put a process to sleep for a specified amount of time the function in figure appears to do this but it has problems as we shall see shortly include signal h include unistd h static void int signo nothing to do just return to wake up the pause unsigned int unsigned int seconds if signal sigalrm return seconds alarm seconds start the timer pause next caught signal wakes us up return alarm turn off timer return unslept time figure simple incomplete implementation of sleep this function looks like the sleep function which we describe in section but this simple implementation has three problems if the caller already has an alarm set that alarm is erased by the first call to alarm we can correct this by looking at alarm return value if the number of seconds until some previously set alarm is less than the argument then we should wait only until the existing alarm expires if the previously set alarm will go off after ours then before returning we should reset this alarm to occur at its designated time in the future we have modified the disposition for sigalrm if we re writing a function for others to call we should save the disposition when our function is called and restore it when we re done we can correct this by saving the return value from signal and resetting the disposition before our function returns there is a race condition between the first call to alarm and the call to pause on a busy system it possible for the alarm to go off and the signal handler to be called before we call pause if that happens the caller is suspended forever in the call to pause assuming that some other signal isn t caught earlier implementations of sleep looked like our program with problems and corrected as described there are two ways to correct problem the first uses setjmp which we show in the next example the other uses sigprocmask and sigsuspend and we describe it in section example the implementation of sleep used setjmp and longjmp section to avoid the race condition described in problem of the previous example a simple version of this function called is shown in figure to reduce the size of this example we don t handle problems and described earlier include setjmp h include signal h include unistd h static static void int signo longjmp unsigned int unsigned int seconds if signal sigalrm return seconds if setjmp alarm seconds start the timer pause next caught signal wakes us up return alarm turn off timer return unslept time figure another imperfect implementation of sleep the function avoids the race condition from figure even if the pause is never executed the function returns when the sigalrm occurs there is however another subtle problem with the function that involves its interaction with other signals if the sigalrm interrupts some other signal handler then when we call longjmp we abort the other signal handler figure shows this scenario the loop in the sigint handler was written so that it executes for longer than seconds on one of the systems used by the author we simply want it to execute longer than the argument to the integer k is declared as volatile to prevent an optimizing compiler from discarding the loop include apue h unsigned int unsigned int static void int int main void unsigned int unslept if signal sigint signal sigint error unslept printf returned u n unslept exit static void int signo int i j volatile int k tune these loops to run for more than seconds on whatever system this test program is run printf starting n for i i i for j j j k i j printf finished n figure calling from a program that catches other signals when we execute the program shown in figure and interrupt the sleep by typing the interrupt character we get the following output a out ˆc we type the interrupt character starting returned we can see that the longjmp from the function aborted the other signal handler even though it wasn t finished this is what you ll encounter if you mix the sleep function with other signal handling see exercise the purpose of the and examples is to show the pitfalls in dealing naively with signals the following sections will show ways around all these problems so we can handle signals reliably without interfering with other pieces of code example a common use for alarm in addition to implementing the sleep function is to put an upper time limit on operations that can block for example if we have a read operation on a device that can block a slow device as described in section we might want the read to time out after some amount of time the program in figure does this reading one line from standard input and writing it to standard output include apue h static void int int main void int n char line maxline if signal sigalrm signal sigalrm error alarm if n read line maxline read error alarm write line n exit static void int signo nothing to do just return to interrupt the read figure calling read with a timeout this sequence of code is common in unix applications but this program has two problems the program in figure has one of the same flaws that we described in figure a race condition between the first call to alarm and the call to read if the kernel blocks the process between these two function calls for longer than the alarm period the read could block forever most operations of this type use a long alarm period such as a minute or more making this unlikely nevertheless it is a race condition if system calls are automatically restarted the read is not interrupted when the sigalrm signal handler returns in this case the timeout does nothing here we specifically want a slow system call to be interrupted we ll see a portable way to do this in section example let redo the preceding example using longjmp this way we don t need to worry about whether a slow system call is interrupted include apue h include setjmp h static void int static int main void int n char line maxline if signal sigalrm signal sigalrm error if setjmp read timeout alarm if n read line maxline read error alarm write line n exit static void int signo longjmp figure calling read with a timeout using longjmp this version works as expected regardless of whether the system restarts interrupted system calls realize however that we still have the problem of interactions with other signal handlers as in figure if we want to set a time limit on an i o operation we need to use longjmp as shown previously while recognizing its possible interaction with other signal handlers another option is to use the select or poll functions described in sections and signal sets we need a data type to represent multiple signals a signal set we ll use this data type with such functions as sigprocmask in the next section to tell the kernel not to allow any of the signals in the set to occur as we mentioned earlier the number of different signals can exceed the number of bits in an integer so in general we can t use an integer to represent the set with one bit per signal posix defines the data type to contain a signal set and the following five functions to manipulate signal sets the function sigemptyset initializes the signal set pointed to by set so that all signals are excluded the function sigfillset initializes the signal set so that all signals are included all applications have to call either sigemptyset or sigfillset once for each signal set before using the signal set because we cannot assume that the c initialization for external and static variables corresponds to the implementation of signal sets on a given system once we have initialized a signal set we can add and delete specific signals in the set the function sigaddset adds a single signal to an existing set and sigdelset removes a single signal from a set in all the functions that take a signal set as an argument we always pass the address of the signal set as the argument implementation if the implementation has fewer signals than bits in an integer a signal set can be implemented using one bit per signal for the remainder of this section assume that an implementation has signals and bit integers the sigemptyset function zeros the integer and the sigfillset function turns on all the bits in the integer these two functions can be implemented as macros in the signal h header define sigemptyset ptr ptr define sigfillset ptr ptr note that sigfillset must return in addition to setting all the bits on in the signal set so we use c comma operator which returns the value after the comma as the value of the expression using this implementation sigaddset turns on a single bit and sigdelset turns off a single bit sigismember tests a certain bit since no signal is ever numbered we subtract from the signal number to obtain the bit to manipulate figure shows implementations of these functions include signal h include errno h signal h usually defines nsig to include signal number define sigbad signo signo signo nsig int sigaddset set int signo if sigbad signo errno einval return int set signo turn bit on return sigdelset set int signo if sigbad signo errno einval return int set signo turn bit off return sigismember const set int signo if sigbad signo errno einval return return set signo figure an implementation of sigaddset sigdelset and sigismember we might be tempted to implement these three functions as one line macros in the signal h header but posix requires us to check the signal number argument for validity and to set errno if it is invalid this is more difficult to do in a macro than in a function sigprocmask function recall from section that the signal mask of a process is the set of signals currently blocked from delivery to that process a process can examine its signal mask change its signal mask or perform both operations in one step by calling the following function first if oset is a non null pointer the current signal mask for the process is returned through oset second if set is a non null pointer the how argument indicates how the current signal mask is modified figure describes the possible values for how is an inclusive or operation whereas is an assignment note that sigkill and sigstop can t be blocked how description the new signal mask for the process is the union of its current signal mask and the signal set pointed to by set that is set contains the additional signals that we want to block the new signal mask for the process is the intersection of its current signal mask and the complement of the signal set pointed to by set that is set contains the signals that we want to unblock the new signal mask for the process is replaced by the value of the signal set pointed to by set figure ways to change the current signal mask using sigprocmask if set is a null pointer the signal mask of the process is not changed and how is ignored after calling sigprocmask if any unblocked signals are pending at least one of these signals is delivered to the process before sigprocmask returns the sigprocmask function is defined only for single threaded processes a separate function is provided to manipulate a thread signal mask in a multithreaded process we ll discuss this in section example figure shows a function that prints the names of the signals in the signal mask of the calling process we call this function from the programs shown in figure and figure include apue h include errno h void const char str sigset int errno we can be called by signal handlers if sigprocmask null sigset sigprocmask error else printf str if sigismember sigset sigint printf sigint if sigismember sigset sigquit printf sigquit if sigismember sigset printf if sigismember sigset sigalrm printf sigalrm remaining signals can go here printf n errno restore errno figure print the signal mask for the process to save space we don t test the signal mask for every signal that we listed in figure see exercise sigpending function the sigpending function returns the set of signals that are blocked from delivery and currently pending for the calling process the set of signals is returned through the set argument example figure shows many of the signal features that we ve been describing include apue h static void int int main void newmask oldmask pendmask if signal sigquit can t catch sigquit block sigquit and save current signal mask sigemptyset newmask sigaddset newmask sigquit if sigprocmask newmask oldmask error sleep sigquit here will remain pending if sigpending pendmask sigpending error if sigismember pendmask sigquit printf nsigquit pending n restore signal mask which unblocks sigquit if sigprocmask oldmask null error printf sigquit unblocked n sleep sigquit here will terminate with core file exit static void int signo printf caught sigquit n if signal sigquit can t reset sigquit figure example of signal sets and sigprocmask the process blocks sigquit saving its current signal mask to restore later and then goes to sleep for seconds any occurrence of the quit signal during this period is blocked and won t be delivered until the signal is unblocked at the end of the second sleep we check whether the signal is pending and unblock the signal note that we saved the old mask when we blocked the signal to unblock the signal we did a of the old mask alternatively we could only the signal that we had blocked be aware however if we write a function that can be called by others and if we need to block a signal in our function we can t use to unblock the signal in this case we have to use and restore the signal mask to its prior value because it possible that the caller had specifically blocked this signal before calling our function we ll see an example of this in the system function in section if we generate the quit signal during this sleep period the signal is now pending and unblocked so it is delivered before sigprocmask returns we ll see this occur because the printf in the signal handler is output before the printf that follows the call to sigprocmask the process then goes to sleep for another seconds if we generate the quit signal during this sleep period the signal should terminate the process since we reset the handling of the signal to its default when we caught it in the following output the terminal prints ˆ when we input control backslash the terminal quit character a out ˆ generate signal once seconds are up sigquit pending after return from sleep caught sigquit in signal handler sigquit unblocked after return from sigprocmask ˆ quit coredump generate signal again a out ˆ ˆ ˆ ˆ ˆ ˆ ˆ ˆ ˆ ˆ generate signal times seconds are up sigquit pending caught sigquit signal is generated only once sigquit unblocked ˆ quit coredump generate signal again the message quit coredump is printed by the shell when it sees that its child terminated abnormally note that when we run the program the second time we generate the quit signal ten times while the process is asleep yet the signal is delivered only once to the process when it unblocked this demonstrates that signals are not queued on this system sigaction function the sigaction function allows us to examine or modify or both the action associated with a particular signal this function supersedes the signal function from earlier releases of the unix system indeed at the end of this section we show an implementation of signal using sigaction the argument signo is the signal number whose action we are examining or modifying if the act pointer is non null we are modifying the action if the oact pointer is non null the system returns the previous action for the signal through the oact pointer this function uses the following structure struct sigaction void int addr of signal handler or or additional signals to block int signal options figure alternate handler void int void when changing the action for a signal if the field contains the address of a signal catching function as opposed to either of the constants or then the field specifies a set of signals that are added to the signal mask of the process before the signal catching function is called if and when the signal catching function returns the signal mask of the process is reset to its previous value this way we are able to block certain signals whenever a signal handler is invoked the operating system includes the signal being delivered in the signal mask when the handler is invoked hence we are guaranteed that whenever we are processing a given signal another occurrence of that same signal is blocked until we re finished processing the first occurrence recall from section that additional occurrences of the same signal are usually not queued if the signal occurs five times while it is blocked when we unblock the signal the signal handling function for that signal will usually be invoked only one time this characteristic was illustrated in the previous example once we install an action for a given signal that action remains installed until we explicitly change it by calling sigaction unlike earlier systems with their unreliable signals posix requires that a signal handler remain installed until explicitly changed the field of the act structure specifies various options for the handling of this signal figure details the meaning of these options when set the sus column contains if the flag is defined as part of the base posix specification and xsi if it is defined as part of the xsi option the field is an alternative signal handler used when the flag is used with sigaction implementations might use the same storage for both the field and the field so applications can use only one of these fields at a time option sus freebsd linux mac os x solaris description system calls interrupted by this signal are not automatically restarted the xsi default for sigaction see section for more information if signo is sigchld do not generate this signal when a child process stops job control this signal is still generated of course when a child terminates but see the option below when the xsi option is supported sigchld won t be sent when a stopped child continues if this flag is set if signo is sigchld this option prevents the system from creating zombie processes when children of the calling process terminate if it subsequently calls wait the calling process blocks until all its child processes have terminated and then returns with errno set to echild recall section when this signal is caught the signal is not automatically blocked by the system while the signal catching function executes unless the signal is also included in note that this type of operation corresponds to the earlier unreliable signals xsi if an alternative stack has been declared with sigaltstack this signal is delivered to the process on the alternative stack the disposition for this signal is reset to and the flag is cleared on entry to the signal catching function note that this type of operation corresponds to the earlier unreliable signals the disposition for the two signals sigill and sigtrap can t be reset automatically however setting this flag can optionally cause sigaction to behave as if is also set system calls interrupted by this signal are automatically restarted refer to section this option provides additional information to a signal handler a pointer to a siginfo structure and a pointer to an identifier for the process context figure option flags for the handling of each signal normally the signal handler is called as void handler int signo but if the flag is set the signal handler is called as void handler int signo info void context the siginfo structure contains information about why the signal was generated an example of what it might look like is shown below all posix compliant implementations must include at least the and members additionally implementations that are xsi compliant contain at least the following fields struct siginfo int signal number int if nonzero errno value from errno h int additional info depends on signal sending process id sending process real user id void address that caused the fault int exit value or signal number union sigval application specific value possibly other fields also the sigval union contains the following fields int void applications pass an integer value in or pass a pointer value in when delivering signals figure shows values of for various signals as defined by the single unix specification note that implementations may define additional code values if the signal is sigchld then the si_status and fields will be set if the signal is sigbus sigill sigfpe or sigsegv then the contains the address responsible for the fault although the address might not be accurate the field contains the error number corresponding to the condition that caused the signal to be generated although its use is implementation defined the context argument to the signal handler is a typeless pointer that can be cast to a structure identifying the process context at the time of signal delivery this structure contains at least the following fields pointer to context resumed when this context returns signals blocked when this context is active stack used by this context machine specific representation of saved context the field describes the stack used by the current context it contains at least the following members void stack base or pointer stack size int flags when an implementation supports the real time signal extensions signal handlers established with the flag will result in signals being queued reliably a separate range of reserved signal numbers is available for real time application use applications can pass information along with the signal by using the sigqueue function section signal code reason sigill ill_illtrp ill_badstk illegal opcode illegal operand illegal addressing mode illegal trap privileged opcode privileged register coprocessor error internal stack error sigfpe fpe_fltovf fpe_fltsub integer divide by zero integer overflow floating point divide by zero floating point overflow floating point underflow floating point inexact result invalid floating point operation subscript out of range sigsegv address not mapped to object invalid permissions for mapped object sigbus invalid address alignment nonexistent physical address object specific hardware error sigtrap process breakpoint trap process trace trap sigchld cld_trapped child has exited child has terminated abnormally no core child has terminated abnormally with core traced child has trapped child has stopped stopped child has continued any si_asyncio signal sent by kill signal sent by sigqueue expiration of a timer set by completion of asynchronous i o request arrival of a message on a message queue real time extension figure code values example signal function let now implement the signal function using sigaction this is what many platforms do and what a note in the posix rationale states was the intent of posix systems with binary compatibility constraints on the other hand might provide a signal function that supports the older unreliable signal semantics unless you specifically require these older unreliable semantics for backward compatibility you should use the following implementation of signal or call sigaction directly as you might guess an implementation of signal with the old semantics could call sigaction specifying and all the examples in this text that call signal call the function shown in figure include apue h reliable version of signal using posix sigaction sigfunc signal int signo sigfunc func struct sigaction act oact act func sigemptyset act act if signo sigalrm ifdef act endif else act if sigaction signo act oact return return oact figure an implementation of signal using sigaction note that we must use sigemptyset to initialize the member of the structure we re not guaranteed that act does the same thing we intentionally set the flag for all signals other than sigalrm so that any system call interrupted by these other signals will be automatically restarted the reason we don t want sigalrm restarted is to allow us to set a timeout for i o operations recall the discussion of figure some older systems such as sunos define the flag these systems restart interrupted system calls by default so specifying this flag causes system calls to be interrupted linux defines the flag for compatibility with applications that use it but by default does not restart system calls when the signal handler is installed with sigaction the single unix specification specifies that the sigaction function not restart interrupted system calls unless the flag is specified example function figure shows a version of the signal function that tries to prevent any interrupted system calls from being restarted include apue h sigfunc int signo sigfunc func struct sigaction act oact act func sigemptyset act act ifdef act endif if sigaction signo act oact return return oact figure the function for improved portability we specify the flag if defined by the system to prevent interrupted system calls from being restarted sigsetjmp and siglongjmp functions in section we described the setjmp and longjmp functions which can be used for nonlocal branching the longjmp function is often called from a signal handler to return to the main loop of a program instead of returning from the handler we saw this approach in figures and there is a problem in calling longjmp however when a signal is caught the signal catching function is entered with the current signal automatically being added to the signal mask of the process this prevents subsequent occurrences of that signal from interrupting the signal handler if we longjmp out of the signal handler what happens to the signal mask for the process under freebsd and mac os x setjmp and longjmp save and restore the signal mask linux and solaris however do not do this although linux supports an option to provide bsd behavior freebsd and mac os x provide the functions and which do not save and restore the signal mask to allow either form of behavior posix does not specify the effect of setjmp and longjmp on signal masks instead two new functions sigsetjmp and siglongjmp are defined by posix these two functions should always be used when branching from a signal handler the only difference between these functions and the setjmp and longjmp functions is that sigsetjmp has an additional argument if savemask is nonzero then sigsetjmp also saves the current signal mask of the process in env when siglongjmp is called if the env argument was saved by a call to sigsetjmp with a nonzero savemask then siglongjmp restores the saved signal mask example the program in figure demonstrates how the signal mask that is installed by the system when a signal handler is invoked automatically includes the signal being caught this program also illustrates the use of the sigsetjmp and siglongjmp functions include apue h include setjmp h include time h static void int static void int static jmpbuf static volatile canjump int main void if signal signal error if signal sigalrm signal sigalrm error starting main figure if sigsetjmp jmpbuf ending main exit canjump now sigsetjmp is ok for pause static void int signo starttime if canjump return unexpected signal ignore starting alarm sigalrm in seconds starttime time null for busy wait for seconds if time null starttime break finishing canjump siglongjmp jmpbuf jump back to main don t return static void int signo in figure example of signal masks sigsetjmp and siglongjmp this program demonstrates another technique that should be used whenever siglongjmp is called from a signal handler we set the variable canjump to a nonzero value only after we ve called sigsetjmp this variable is examined in the signal handler and siglongjmp is called only if the flag canjump is nonzero this technique provides protection against the signal handler being called at some earlier or later time when the jump buffer hasn t been initialized by sigsetjmp in this trivial program we terminate quickly after the siglongjmp but in larger programs the signal handler may remain installed long after the siglongjmp providing this type of protection usually isn t required with longjmp in normal c code as opposed to a signal handler since a signal can occur at any time however we need the added protection in a signal handler here we use the data type which is defined by the iso c standard to be the type of variable that can be written without being interrupted by this we mean that a variable of this type should not extend across page boundaries on a system with virtual memory and can be accessed with a single machine instruction for example we always include the iso type qualifier volatile for these data types as well since the variable is being accessed by two different threads of control the main function and the asynchronously executing signal handler figure shows a timeline for this program we can divide figure into three parts the left part corresponding to main the center part and the right part while the process is executing in the left part its signal mask is no signals are blocked while executing in the center part its signal mask is while executing in the right part its signal mask is sigalrm signal signal sigsetjmp pause delivered alarm time time time sigalrm delivered return sigsetjmp exit siglongjmp figure timeline for example program handling two signals let examine the output when the program in figure is executed a out start process in background starting main the job control shell prints its process id kill send the process starting in sigalrm finishing ending main just press return done a out the output is what we expect when a signal handler is invoked the signal being caught is added to the current signal mask of the process the original mask is restored when the signal handler returns also siglongjmp restores the signal mask that was saved by sigsetjmp if we change the program in figure so that the calls to sigsetjmp and siglongjmp are replaced with calls to setjmp and longjmp on linux or and on freebsd the final line of output becomes ending main this means that the main function is executing with the signal blocked after the call to setjmp this probably isn t what we want 16 sigsuspend function we have seen how we can change the signal mask for a process to block and unblock selected signals we can use this technique to protect critical regions of code that we don t want interrupted by a signal but what if we want to unblock a signal and then pause waiting for the previously blocked signal to occur assuming that the signal is sigint the incorrect way to do this is newmask oldmask sigemptyset newmask sigaddset newmask sigint block sigint and save current signal mask if sigprocmask newmask oldmask error critical region of code restore signal mask which unblocks sigint if sigprocmask oldmask null error window is open pause wait for signal to occur continue processing if the signal is sent to the process while it is blocked the signal delivery will be deferred until the signal is unblocked to the application this can look as if the signal occurs between the unblocking and the pause depending on how the kernel implements signals if this happens or if the signal does occur between the unblocking and the pause we have a problem any occurrence of the signal in this window of time is lost in the sense that we might not see the signal again in which case the pause will block indefinitely this is another problem with the earlier unreliable signals to correct this problem we need a way to both restore the signal mask and put the process to sleep in a single atomic operation this feature is provided by the sigsuspend function the signal mask of the process is set to the value pointed to by sigmask then the process is suspended until a signal is caught or until a signal occurs that terminates the process if a signal is caught and if the signal handler returns then sigsuspend returns and the signal mask of the process is set to its value before the call to sigsuspend note that there is no successful return from this function if it returns to the caller it always returns with errno set to eintr indicating an interrupted system call example figure shows the correct way to protect a critical region of code from a specific signal include apue h static void int int main void newmask oldmask waitmask program start if signal sigint signal sigint error sigemptyset waitmask sigaddset waitmask sigemptyset newmask sigaddset newmask sigint block sigint and save current signal mask if sigprocmask newmask oldmask error critical region of code in critical region pause allowing all signals except if sigsuspend waitmask sigsuspend error after return from sigsuspend reset signal mask which unblocks sigint if sigprocmask oldmask null error and continue processing program exit exit static void int signo nin figure protecting a critical region from a signal when sigsuspend returns it sets the signal mask to its value before the call in this example the sigint signal will be blocked so we restore the signal mask to the value that we saved earlier oldmask running the program from figure produces the following output a out program start in critical region sigint ˆc type the interrupt character in sigint after return from sigsuspend sigint program exit we added to the mask installed when we called sigsuspend so that when the signal handler ran we could tell that the mask had actually changed we can see that when sigsuspend returns it restores the signal mask to its value before the call example another use of sigsuspend is to wait for a signal handler to set a global variable in the program shown in figure we catch both the interrupt signal and the quit signal but want to wake up the main routine only when the quit signal is caught include apue h volatile quitflag set nonzero by signal handler static void int signo one signal handler for sigint and sigquit int if signo sigint printf ninterrupt n else if signo sigquit quitflag set flag for main loop main void newmask oldmask zeromask if signal sigint signal sigint error if signal sigquit signal sigquit error sigemptyset zeromask sigemptyset newmask sigaddset newmask sigquit block sigquit and save current signal mask if sigprocmask newmask oldmask error while quitflag sigsuspend zeromask sigquit has been caught and is now blocked do whatever quitflag reset signal mask which unblocks sigquit if sigprocmask oldmask null error exit figure using sigsuspend to wait for a global variable to be set sample output from this program is a out ˆc type the interrupt character interrupt ˆc type the interrupt character again interrupt ˆc and again interrupt ˆ now terminate with the quit character for portability between non posix systems that support iso c and posix systems the only thing we should do within a signal handler is assign a value to a variable of type nothing else posix goes further and specifies a list of functions that are safe to call from within a signal handler figure but if we do this our code may not run correctly on non posix systems example as another example of signals we show how signals can be used to synchronize a parent and child figure shows implementations of the five routines and from section include apue h static volatile sigflag set nonzero by sig handler static newmask oldmask zeromask static void int signo one signal handler for and sigflag void void if signal err_sys signal error if signal err_sys signal error sigemptyset zeromask sigemptyset newmask sigaddset newmask sigaddset newmask block and and save current signal mask if sigprocmask newmask oldmask err_sys error void pid kill pid tell parent we re done void wait_parent void while sigflag sigsuspend zeromask and wait for parent sigflag reset signal mask to original value if sigprocmask oldmask null err_sys error void pid kill pid tell child we re done void void while sigflag sigsuspend zeromask and wait for child sigflag reset signal mask to original value if sigprocmask oldmask null err_sys error figure routines to allow a parent and child to synchronize we use the two user defined signals is sent by the parent to the child and is sent by the child to the parent in figure we show another implementation of these five functions using pipes the sigsuspend function is fine if we want to go to sleep while we re waiting for a signal to occur as we ve shown in the previous two examples but what if we want to call other system functions while we re waiting unfortunately this problem has no bulletproof solution unless we use multiple threads and dedicate a separate thread to handling signals as we discuss in section without using threads the best we can do is to set a global variable in the signal handler when the signal occurs for example if we catch both sigint and sigalrm and install the signal handlers using the function the signals will interrupt any slow system call that is blocked the signals are most likely to occur when we re blocked in a call to the read function waiting for input from a slow device this is especially true for sigalrm since we set the alarm clock to prevent us from waiting forever for input the code to handle this looks similar to the following if flag set by our sigint handler if flag set by our sigalrm handler signals occurring in here are lost while read if errno eintr if else if else some other error else if n end of file else process input we test each of the global flags before calling read and again if read returns an interrupted system call error the problem occurs if either signal is caught between the first two if statements and the subsequent call to read signals occurring in here are lost as indicated by the code comment the signal handlers are called and they set the appropriate global variable but the read never returns unless some data is ready to be read what we would like to be able to do is the following sequence of steps in order block sigint and sigalrm test the two global variables to see whether either signal has occurred and if so handle the condition call read or any other system function and unblock the two signals as an atomic operation the sigsuspend function helps us only if step is a pause operation abort function we mentioned earlier that the abort function causes abnormal program termination include stdlib h void abort void this function never returns this function sends the sigabrt signal to the caller processes should not ignore this signal iso c states that calling abort will deliver an unsuccessful termination notification to the host environment by calling raise sigabrt iso c requires that if the signal is caught and the signal handler returns abort still doesn t return to its caller if this signal is caught the only way the signal handler can t return is if it calls exit longjmp or siglongjmp section discusses the differences between longjmp and siglongjmp posix also specifies that abort overrides the blocking or ignoring of the signal by the process the intent of letting the process catch the sigabrt is to allow it to perform any cleanup that it wants to do before the process terminates if the process doesn t terminate itself from this signal handler posix states that when the signal handler returns abort terminates the process the iso c specification of this function leaves it up to the implementation as to whether output streams are flushed and whether temporary files section are deleted posix goes further and allows an implementation to call fclose on open standard i o streams before terminating if the call to abort terminates the process earlier versions of system v generated the sigiot signal from the abort function furthermore it was possible for a process to ignore this signal or to catch it and return from the signal handler in which case abort returned to its caller 3bsd generated the sigill signal before doing this the 3bsd function unblocked the signal and reset its disposition to terminate with core file this prevented a process from either ignoring the signal or catching it historically implementations of abort have differed in how they deal with standard i o streams for defensive programming and improved portability if we want standard i o streams to be flushed we specifically do it before calling abort we do this in the function appendix b since most unix system implementations of tmpfile call unlink immediately after creating the file the iso c warning about temporary files does not usually concern us example figure shows an implementation of the abort function as specified by posix include signal h include stdio h include stdlib h include unistd h void abort void posix style abort function mask struct sigaction action caller can t ignore sigabrt if so reset to default sigaction sigabrt null action if action action sigaction sigabrt action null if action fflush null flush all open stdio streams caller can t block sigabrt make sure it unblocked sigfillset mask sigdelset mask sigabrt mask has only sigabrt turned off sigprocmask mask null kill getpid sigabrt send the signal if we re here process caught sigabrt and returned fflush null flush all open stdio streams action sigaction sigabrt action null reset to default sigprocmask mask null just in case kill getpid sigabrt and one more time exit this should never be executed figure implementation of posix abort we first see whether the default action will occur if so we flush all the standard i o streams this is not equivalent to calling fclose on all the open streams since it just flushes them and doesn t close them but when the process terminates the system closes all open files if the process catches the signal and returns we flush all the streams again since the process could have generated more output the only condition we don t handle is the case where the process catches the signal and calls or in this case any unflushed standard i o buffers in memory are discarded we assume that a caller that does this doesn t want the buffers flushed recall from section that if calling kill causes the signal to be generated for the caller and if the signal is not blocked which we guarantee in figure then the signal or some other pending unlocked signal is delivered to the process before kill returns we block all signals except sigabrt so we know that if the call to kill returns the process caught the signal and the signal handler returned system function in section we showed an implementation of the system function that version however did not do any signal handling posix requires that system ignore sigint and sigquit and block sigchld before showing a version that handles these signals correctly let see why we need to worry about signal handling example the program shown in figure uses the version of system from section to invoke the ed editor this editor has been part of unix systems for a long time we use it here because it is an interactive program that catches the interrupt and quit signals if we invoke ed from a shell and type the interrupt character it catches the interrupt signal and prints a question mark the ed program also sets the disposition of the quit signal so that it is ignored the program in figure catches both sigint and sigchld if we invoke the program we get a out a append text to the editor buffer here is one line of text period on a line by itself stops append mode p print first through last lines of buffer to see what there here is one line of text w temp foo write the buffer to a file editor says it wrote bytes q and leave the editor caught sigchld when the editor terminates the system sends the sigchld signal to the parent the a out process we catch it and return from the signal handler but if it is catching the sigchld signal the parent should be doing so because it has created its own children so that it knows when its children have terminated the delivery of this signal in the include apue h static void int signo printf caught sigint n static void int signo printf caught sigchld n int main void if signal sigint sig_err err_sys signal sigint error if signal sigchld sig_err err_sys signal sigchld error if system bin ed err_sys system error exit figure using system to invoke the ed editor parent should be blocked while the system function is executing indeed this is what posix specifies otherwise when the child created by system terminates it would fool the caller of system into thinking that one of its own children terminated the caller would then use one of the wait functions to get the termination status of the child thereby preventing the system function from being able to obtain the child termination status for its return value if we run the program again this time sending the editor an interrupt signal we get a out a append text to the editor buffer hello world period on a line by itself stops append mode p print first through last lines to see what there hello world w temp foo write the buffer to a file editor says it wrote bytes ˆc type the interrupt character editor catches signal prints question mark caught sigint and so does the parent process q leave editor caught sigchld recall from section that typing the interrupt character causes the interrupt signal to be sent to all the processes in the foreground process group figure shows the arrangement of the processes when the editor is running fork exec fork exec background process group foreground process group figure foreground and background process groups for figure in this example sigint is sent to all three foreground processes the shell ignores it as we can see from the output both the a out process and the editor catch the signal but when we re running another program with the system function we shouldn t have both the parent and the child catching the two terminal generated signals interrupt and quit instead these two signals should be sent to the program that is running the child since the command that is executed by system can be an interactive command as is the ed program in this example and since the caller of system gives up control while the program executes waiting for it to finish the caller of system should not be receiving these two terminal generated signals for this reason posix specifies that the system function should ignore these two signals while waiting for the command to complete example figure shows an implementation of the system function with the required signal handling include sys wait h include errno h include signal h include unistd h int system const char cmdstring with appropriate signal handling pid int status struct sigaction ignore saveintr savequit chldmask savemask if cmdstring null return always a command processor with unix ignore ignore sigint and sigquit sigemptyset ignore ignore if sigaction sigint ignore saveintr return if sigaction sigquit ignore savequit return sigemptyset chldmask now block sigchld sigaddset chldmask sigchld if sigprocmask chldmask savemask return if pid fork status probably out of processes else if pid child restore previous signal actions reset signal mask sigaction sigint saveintr null sigaction sigquit savequit null sigprocmask savemask null execl bin sh sh c cmdstring char exec error else parent while waitpid pid status if errno eintr status error other than eintr from waitpid break restore previous signal actions reset signal mask if sigaction sigint saveintr null return if sigaction sigquit savequit null return if sigprocmask savemask null return return status figure correct posix implementation of system function if we link the program in figure with this implementation of the system function the resulting binary differs from the last flawed one in the following ways no signal is sent to the calling process when we type the interrupt or quit character when the ed command exits sigchld is not sent to the calling process instead it is blocked until we unblock it in the last call to sigprocmask after the system function retrieves the child termination status by calling waitpid posix states that if wait or waitpid returns the status of a child process while sigchld is pending then sigchld should not be delivered to the process unless the status of another child process is also available freebsd mac os x and solaris all implement this semantic linux however doesn t sigchld remains pending after the system function calls waitpid when the signal is unblocked it is delivered to the caller if we called wait in the function in figure a linux system would return with errno set to echild since the system function already retrieved the termination status of the child many older texts show the ignoring of the interrupt and quit signals as follows if pid fork err_sys fork error else if pid child execl parent signal sigint signal sigquit waitpid pid status signal sigint signal sigquit the problem with this sequence of code is that we have no guarantee after the fork regarding whether the parent or child runs first if the child runs first and the parent doesn t run for some time after an interrupt signal might be generated before the parent is able to change its disposition to be ignored for this reason in figure we change the disposition of the signals before the fork note that we have to reset the dispositions of these two signals in the child before the call to execl this allows execl to change their dispositions to the default based on the caller dispositions as we described in section return value from system the return value from system is the termination status of the shell which isn t always the termination status of the command string we saw some examples in figure 23 and the results were as we expected if we execute a simple command such as date the termination status is executing the shell command exit gave us a termination status of what happens with signals let run the program in figure 24 and send some signals to the command that executing tsys sleep ˆcnormal termination exit status we press the interrupt key tsys sleep ˆ sh quit we press the quit key normal termination exit status when we terminate the sleep call with the interrupt signal the function figure thinks that it terminated normally the same thing happens when we kill the sleep call with the quit key as this example demonstrates the bourne shell has a poorly documented feature in which its termination status is plus the signal number when the command it was executing is terminated by a signal we can see this with the shell interactively sh make sure we re running the bourne shell sh c sleep ˆc press the interrupt key echo print termination status of last command sh c sleep ˆ sh quit core dumped press the quit key echo print termination status of last command exit leave bourne shell on the system being used sigint has a value of and sigquit has a value of giving us the shell termination statuses of and let try a similar example but this time we ll send a signal directly to the shell and see what is returned by system tsys sleep start it in background this time ps f look at the process ids uid pid ppid tty time cmd sar 949 pts 00 ps f sar pts 00 sh c sleep sar 949 947 pts bin sh sar 949 pts 00 tsys sleep sar pts 00 sleep kill kill kill the shell itself abnormal termination signal number here we can see that the return value from system reports an abnormal termination only when the shell itself terminates abnormally other shells behave differently when handling terminal generated signals such as sigint and sigquit with bash and dash for example pressing the interrupt or quit key will result in an exit status indicating abnormal termination with the corresponding signal number however if we find our process executing sleep and send it a signal directly so that the signal goes only to the individual process instead of the entire foreground process group we will find that these shells behave like the bourne shell and exit with a normal termination status of plus the signal number when writing programs that use the system function be sure to interpret the return value correctly if you call fork exec and wait yourself the termination status is not the same as if you call system sleep nanosleep and functions we ve used the sleep function in numerous examples throughout the text and we showed two flawed implementations of it in figures and this function causes the calling process to be suspended until either the amount of wall clock time specified by seconds has elapsed a signal is caught by the process and the signal handler returns as with an alarm signal the actual return may occur at a time later than requested because of other system activity in case the return value is when sleep returns early because of some signal being caught case the return value is the number of unslept seconds the requested time minus the actual time slept although sleep can be implemented with the alarm function section this isn t required if alarm is used however there can be interactions between the two functions the posix standard leaves all these interactions unspecified for example if we do an alarm and wall clock seconds later do a sleep what happens the sleep will return in seconds assuming that some other signal isn t caught in the interim but will another sigalrm be generated seconds later these details depend on the implementation freebsd linux mac os x and solaris implement sleep using the nanosleep function which allows the implementation to be independent of signals and the alarm timer for portability you shouldn t make any assumptions about the implementation of sleep but if you have any intentions of mixing calls to sleep with any other timing functions you need to be aware of possible interactions example figure shows an implementation of the posix sleep function this function is a modification of figure which handles signals reliably avoiding the race condition in the earlier implementation we still do not handle any interactions with previously set alarms as we mentioned these interactions are explicitly undefined by posix include apue h static void int signo nothing to do just returning wakes up sigsuspend unsigned int sleep unsigned int seconds struct sigaction newact oldact newmask oldmask suspmask unsigned int unslept set our handler save previous information newact sigemptyset newact newact sigaction sigalrm newact oldact block sigalrm and save current signal mask sigemptyset newmask sigaddset newmask sigalrm sigprocmask newmask oldmask alarm seconds suspmask oldmask make sure sigalrm isn t blocked sigdelset suspmask sigalrm wait for any signal to be caught sigsuspend suspmask some signal has been caught sigalrm is now blocked unslept alarm reset previous action sigaction sigalrm oldact null reset signal mask which unblocks sigalrm sigprocmask oldmask null return unslept figure reliable implementation of sleep it takes more code to write this reliable implementation than what is shown in figure we don t use any form of nonlocal branching as we did in figure to avoid the race condition between alarm and pause so there is no effect on other signal handlers that may be executing when the sigalrm is handled the nanosleep function is similar to the sleep function but provides nanosecond level granularity this function suspends the calling process until either the requested time has elapsed or the function is interrupted by a signal the reqtp parameter specifies the amount of time to sleep in seconds and nanoseconds if the sleep interval is interrupted by a signal and the process doesn t terminate the timespec structure pointed to by the remtp parameter will be set to the amount of time left in the sleep interval we can set this parameter to null if we are uninterested in the time unslept if the system doesn t support nanosecond granularity the requested time is rounded up because the nanosleep function doesn t involve the generation of any signals we can use it without worrying about interactions with other functions the nanosleep function used to belong to the timers option in the single unix specification but was moved to the base in with the introduction of multiple system clocks recall section we need a way to suspend the calling thread using a delay time relative to a particular clock the function provides us with this capability the argument specifies the clock against which the time delay is evaluated identifiers for clocks are listed in figure the flags argument is used to control whether the delay is absolute or relative when flags is set to the sleep time is relative i e how long we want to sleep when it is set to the sleep time is absolute i e we want to sleep until the clock reaches the specified time the other arguments reqtp and remtp are the same as in the nanosleep function however when we use an absolute time the remtp argument is unused because it isn t needed we can reuse the same value for the reqtp argument for additional calls to until the clock reaches the specified absolute time value note that except for error returns the call reqtp remtp has the same effect as the call nanosleep reqtp remtp the problem with using a relative sleep is that some applications require precision with how long they sleep and a relative sleep time can lead to sleeping longer than desired for example if an application wants to perform a task at regular intervals it would have to get the current time calculate the amount of time until the next time to execute the task and then call nanosleep between the time that the current time is obtained and the call to nanosleep is made processor scheduling and preemption can result in the relative sleep time extending past the desired interval using an absolute time improves the precision even though a time sharing process scheduler makes no guarantee that our task will execute immediately after our sleep time has ended in older versions of the single unix specification the function belonged to the clock selection option in it was moved to the base sigqueue function in section we said that most unix systems don t queue signals with the real time extensions to posix some systems began adding support for queueing signals with the queued signal functionality has moved from the real time extensions to the base specification generally a signal carries one bit of information the signal itself in addition to queueing signals these extensions allow applications to pass more information along with the delivery recall section this information is embedded in a siginfo structure along with system provided information applications can pass an integer or a pointer to a buffer containing more information to the signal handler to use queued signals we have to do the following specify the flag when we install a signal handler using the sigaction function if we don t specify this flag the signal will be posted but it is left up to the implementation whether the signal is queued provide a signal handler in the member of the sigaction structure instead of using the usual field implementations might allow us to use the field but we won t be able to obtain the extra information sent with the sigqueue function use the sigqueue function to send signals the sigqueue function is similar to the kill function except that we can only direct signals to a single process with sigqueue and we can use the value argument to transmit either an integer or a pointer value to the signal handler signals can t be queued infinitely recall the limit from figure 9 and figure when this limit is reached sigqueue can fail with errno set to eagain with the real time signal enhancements a separate set of signals was introduced for application use these are the signal numbers between sigrtmin and sigrtmax inclusive be aware that the default action for these signals is to terminate the process figure summarizes the way queued signals differ in behavior among the implementations covered in this text behavior sus freebsd linux mac os x solaris supports sigqueue queues other signals besides sigrtmin to sigrtmax optional queues signals even if the caller doesn t use the flag optional figure behavior of queued signals on various platforms mac os x doesn t support sigqueue or real time signals on solaris sigqueue is in the real time library librt job control signals of the signals shown in figure posix considers six to be job control signals sigchld child process has stopped or terminated sigcont continue process if stopped sigstop stop signal can t be caught or ignored sigtstp interactive stop signal sigttin read from controlling terminal by background process group member sigttou write to controlling terminal by a background process group member except for sigchld most application programs don t handle these signals interactive shells usually do all the work required to handle them when we type the suspend character usually control z sigtstp is sent to all processes in the foreground process group when we tell the shell to resume a job in the foreground or background the shell sends all the processes in the job the sigcont signal similarly if sigttin or sigttou is delivered to a process the process is stopped by default and the job control shell recognizes this and notifies us an exception is a process that is managing the terminal the vi editor for example it needs to know when the user wants to suspend it so that it can restore the terminal state to the way it was when vi was started also when it resumes in the foreground the vi editor needs to set the terminal state back to the way it wants it and it needs to redraw the terminal screen we see how a program such as vi handles this in the example that follows there are some interactions between the job control signals when any of the four stop signals sigtstp sigstop sigttin or sigttou is generated for a process any pending sigcont signal for that process is discarded similarly when the sigcont signal is generated for a process any pending stop signals for that same process are discarded note that the default action for sigcont is to continue the process if it is stopped otherwise the signal is ignored normally we don t have to do anything with this signal when sigcont is generated for a process that is stopped the process is continued even if the signal is blocked or ignored example the program in figure demonstrates the normal sequence of code used when a program handles job control this program simply copies its standard input to its standard output but comments are given in the signal handler for typical actions performed by a program that manages a screen include apue h define buffsize static void int signo signal handler for sigtstp int mask move cursor to lower left corner reset tty mode unblock sigtstp since it blocked while we re handling it sigemptyset mask sigaddset mask sigtstp sigprocmask mask null signal sigtstp reset disposition to default kill getpid sigtstp and send the signal to ourself we won t return from the kill until we re continued signal sigtstp reestablish signal handler reset tty mode redraw screen main void int n char buf buffsize only catch sigtstp if we re running with a job control shell if signal sigtstp signal sigtstp while n read buf buffsize if write buf n n err_sys write error if n err_sys read error exit figure how to handle sigtstp when the program in figure starts it arranges to catch the sigtstp signal only if the signal disposition is the reason is that when the program is started by a shell that doesn t support job control bin sh for example the signal disposition should be set to in fact the shell doesn t explicitly ignore this signal init sets the disposition of the three job control signals sigtstp sigttin and sigttou to this disposition is then inherited by all login shells only a job control shell should reset the disposition of these three signals to sig_dfl when we type the suspend character the process receives the sigtstp signal and the signal handler is invoked at this point we would do any terminal related processing move the cursor to the lower left corner restore the terminal mode and so on we then send ourself the same signal sigtstp after resetting its disposition to its default stop the process and unblocking the signal we have to unblock it since we re currently handling that same signal and the system blocks it automatically while it being caught at this point the system stops the process it is continued only when it receives usually from the job control shell in response to an interactive fg command a sigcont signal we don t catch sigcont its default disposition is to continue the stopped process when this happens the program continues as though it returned from the kill function when the program is continued we reset the disposition for the sigtstp signal and do whatever terminal processing we want we could redraw the screen for example signal names and numbers in this section we describe how to map between signal numbers and names some systems provide the array extern char the array index is the signal number giving a pointer to the character string name of the signal freebsd linux and mac os x all provide this array of signal names solaris does too but it uses the name instead to print the character string corresponding to a signal number in a portable manner we can use the psignal function the string msg which normally includes the name of the program is output to the standard error followed by a colon and a space followed by a description of the signal followed by a newline if msg is null then only the description is written to the standard error this function is similar to perror section if you have a siginfo structure from an alternative sigaction signal handler you can print the signal information with the psiginfo function it operates in a similar manner to the psignal function although this function has access to more information than just the signal number platforms vary in exactly what additional information is printed if you only need the string description of the signal and don t necessarily want to write it to standard error you might want to write it to a log file for example you can use the strsignal function this function is similar to strerror also described in section given a signal number strsignal will return a string that describes the signal this string can be used by applications to print error messages about signals received all the platforms discussed in this book provide the psignal and strsignal functions but differences do occur on solaris strsignal will return a null pointer if the signal number is invalid whereas freebsd linux and mac os x return a string indicating that the signal number is unrecognized only linux and solaris support the psiginfo function solaris provides a couple of functions to map a signal number to a signal name and vice versa these functions are useful when writing interactive programs that need to accept and print signal names and numbers the function translates the given signal number into a string and stores the result in the memory pointed to by str the caller must ensure that the memory is large enough to hold the longest string including the terminating null byte solaris provides the constant in signal h to define the maximum string length the string consists of the signal name without the sig prefix for example translating sigkill would result in the string kill being stored in the str memory buffer the function translates the given name into a signal number the signal number is stored in the integer pointed to by signop the name can be either the signal name without the sig prefix or a string representation of the decimal signal number i e 9 note that and depart from common practice and don t set errno when they fail 23 summary signals are used in most nontrivial applications an understanding of the hows and whys of signal handling is essential to advanced unix system programming this chapter has taken a long and thorough look at unix system signals we started by looking at the warts in previous implementations of signals and how they manifest themselves we then proceeded to the posix reliable signal concept and all the related functions once we covered all these details we were able to provide implementations of the posix abort system and sleep functions we finished with a look at the job control signals and the ways that we can convert between signal names and signal numbers exercises in figure remove the for statement what happens and why implement the function described in section draw pictures of the stack frames when we run the program from figure 9 in figure we showed a technique that often used to set a timeout on an i o operation using setjmp and longjmp the following code has also been seen signal sigalrm alarm if setjmp handle timeout what else is wrong with this sequence of code using only a single timer either alarm or the higher precision setitimer provide a set of functions that allows a process to set any number of timers write the following program to test the parent child synchronization functions in figure 24 the process creates a file and writes the integer to the file the process then calls fork and the parent and child alternate incrementing the counter in the file each time the counter is incremented print which process parent or child is doing the increment in the function shown in figure 25 if the caller catches sigabrt and returns from the signal handler why do we go to the trouble of resetting the disposition to its default and call kill the second time instead of simply calling why do you think the siginfo structure section includes the real user id instead of the effective user id in the field 9 rewrite the function in figure to handle all the signals from figure the function should consist of a single loop that iterates once for every signal in the current signal mask not once for every possible signal write a program that calls sleep in an infinite loop every five times through the loop every minutes fetch the current time of day and print the field run the program overnight and explain the results how would a program such as the cron daemon which runs every minute on the minute handle this situation modify figure as follows a change buffsize to b catch the sigxfsz signal using the function printing a message when it caught and returning from the signal handler and c print the return value from write if the requested number of bytes wasn t written modify the soft resource limit section to bytes and run your new program copying a file that is larger than bytes try to set the soft resource limit from your shell if you can t do this from your shell call setrlimit directly from the program run this program on the different systems that you have access to what happens and why write a program that calls fwrite with a large buffer about one gigabyte before calling fwrite call alarm to schedule a signal in second in your signal handler print that the signal was caught and return does the call to fwrite complete what happening threads introduction we discussed processes in earlier chapters we learned about the environment of a unix process the relationships between processes and ways to control processes we saw that a limited amount of sharing can occur between related processes in this chapter we ll look inside a process further to see how we can use multiple threads of control or simply threads to perform multiple tasks within the environment of a single process all threads within a single process have access to the same process components such as file descriptors and memory anytime you try to share a single resource among multiple users you have to deal with consistency we ll conclude this chapter with a look at the synchronization mechanisms available to prevent multiple threads from viewing inconsistencies in their shared resources thread concepts a typical unix process can be thought of as having a single thread of control each process is doing only one thing at a time with multiple threads of control we can design our programs to do more than one thing at a time within a single process with each thread handling a separate task this approach can have several benefits we can simplify code that deals with asynchronous events by assigning a separate thread to handle each event type each thread can then handle its event using a synchronous programming model a synchronous programming model is much simpler than an asynchronous one multiple processes have to use complex mechanisms provided by the operating system to share memory and file descriptors as we will see in chapters 383 and threads in contrast automatically have access to the same memory address space and file descriptors some problems can be partitioned so that overall program throughput can be improved a single threaded process with multiple tasks to perform implicitly serializes those tasks because there is only one thread of control with multiple threads of control the processing of independent tasks can be interleaved by assigning a separate thread per task two tasks can be interleaved only if they don t depend on the processing performed by each other similarly interactive programs can realize improved response time by using multiple threads to separate the portions of the program that deal with user input and output from the other parts of the program some people associate multithreaded programming with multiprocessor or multicore systems the benefits of a multithreaded programming model can be realized even if your program is running on a uniprocessor a program can be simplified using threads regardless of the number of processors because the number of processors doesn t affect the program structure furthermore as long as your program has to block when serializing tasks you can still see improvements in response time and throughput when running on a uniprocessor because some threads might be able to run while others are blocked a thread consists of the information necessary to represent an execution context within a process this includes a thread id that identifies the thread within a process a set of register values a stack a scheduling priority and policy a signal mask an errno variable recall section and thread specific data section everything within a process is sharable among the threads in a process including the text of the executable program the program global and heap memory the stacks and the file descriptors the threads interfaces we re about to see are from posix the threads interfaces also known as pthreads for posix threads originally were optional in posix but moved them to the base the feature test macro for posix threads is applications can either use this in an ifdef test to determine at compile time whether threads are supported or call sysconf with the constant to determine this at runtime systems conforming to define the symbol to have the value thread identification just as every process has a process id every thread has a thread id unlike the process id which is unique in the system the thread id has significance only within the context of the process to which it belongs recall that a process id represented by the data type is a non negative integer a thread id is represented by the data type implementations are allowed to use a structure to represent the data type so portable implementations can t treat them as integers therefore a function must be used to compare two thread ids linux uses an unsigned long integer for the data type solaris represents the data type as an unsigned integer freebsd and mac os x use a pointer to the pthread structure for the data type a consequence of allowing the data type to be a structure is that there is no portable way to print its value sometimes it is useful to print thread ids during program debugging but there is usually no need to do so otherwise at worst this results in nonportable debug code so it is not much of a limitation a thread can obtain its own thread id by calling the function this function can be used with when a thread needs to identify data structures that are tagged with its thread id for example a master thread might place work assignments on a queue and use the thread id to control which jobs go to each worker thread this situation is illustrated in figure a single master thread places new jobs on a work queue a pool of three worker threads removes jobs from the queue instead of allowing each thread to process whichever job is at the head of the queue the master thread controls job assignment by placing the id of the thread that should process the job in each job structure each worker thread then removes only jobs that are tagged with its own thread id thread creation the traditional unix process model supports only one thread of control per process conceptually this is the same as a threads based model whereby each process is made up of only one thread with pthreads when a program runs it also starts out as a single process with a single thread of control as the program runs its behavior should be indistinguishable from the traditional process until it creates more threads of control additional threads can be created by calling the function figure work queue example the memory location pointed to by tidp is set to the thread id of the newly created thread when returns successfully the attr argument is used to customize various thread attributes we ll cover thread attributes in section but for now we ll set this to null to create a thread with the default attributes the newly created thread starts running at the address of the function this function takes a single argument arg which is a typeless pointer if you need to pass more than one argument to the function then you need to store them in a structure and pass the address of the structure in arg when a thread is created there is no guarantee which will run first the newly created thread or the calling thread the newly created thread has access to the process address space and inherits the calling thread floating point environment and signal mask however the set of pending signals for the thread is cleared note that the pthread functions usually return an error code when they fail they don t set errno like the other posix functions the per thread copy of errno is provided only for compatibility with existing functions that use it with threads it is cleaner to return the error code from the function thereby restricting the scope of the error to the function that caused it instead of relying on some global state that is changed as a side effect of the function example although there is no portable way to print the thread id we can write a small test program that does to gain some insight into how threads work the program in figure creates one thread and prints the process and thread ids of the new thread and the initial thread include apue h include pthread h ntid void printids const char pid tid pid getpid tid printf pid lu tid lu lx n unsigned long pid unsigned long tid unsigned long tid void void arg printids new thread return void int main void int err err ntid null null if err err can t create thread printids main thread sleep exit figure printing thread ids this example has two oddities which are necessary to handle races between the main thread and the new thread we ll learn better ways to deal with these conditions later in this chapter the first is the need to sleep in the main thread if it doesn t sleep the main thread might exit thereby terminating the entire process before the new thread gets a chance to run this behavior is dependent on the operating system threads implementation and scheduling algorithms the second oddity is that the new thread obtains its thread id by calling instead of reading it out of shared memory or receiving it as an argument to its thread start routine recall that will return the thread id of the newly created thread through the first parameter tidp in our example the main thread stores this id in ntid but the new thread can t safely use it if the new thread runs before the main thread returns from calling then the new thread will see the uninitialized contents of ntid instead of the thread id running the program in figure on solaris gives us a out main thread pid tid new thread pid tid 0x2 as we expect both threads have the same process id but different thread ids running the program in figure on freebsd gives us a out main thread pid tid new thread pid tid as we expect both threads have the same process id if we look at the thread ids as decimal integers the values look strange but if we look at them in hexadecimal format they make more sense as we noted earlier freebsd uses a pointer to the thread data structure for its thread id we would expect mac os x to be similar to freebsd however the thread id for the main thread is from a different address range than the thread ids for threads created with a out main thread pid tid new thread pid tid running the same program on linux gives us a out main thread pid tid new thread pid tid the linux thread ids look like pointers even though they are represented as unsigned long integers the threads implementation changed between linux and linux in linux linuxthreads implemented each thread with a separate process this made it difficult to match the behavior of posix threads in linux the linux kernel and threads library were overhauled to use a new threads implementation called the native posix thread library nptl this supported a model of multiple threads within a single process and made it easier to support posix threads semantics thread termination if any thread within a process calls exit or then the entire process terminates similarly when the default action is to terminate the process a signal sent to a thread will terminate the entire process we ll talk more about the interactions between signals and threads in section a single thread can exit in three ways thereby stopping its flow of control without terminating the entire process the thread can simply return from the start routine the return value is the thread exit code the thread can be canceled by another thread in the same process the thread can call the argument is a typeless pointer similar to the single argument passed to the start routine this pointer is available to other threads in the process by calling the function the calling thread will block until the specified thread calls returns from its start routine or is canceled if the thread simply returned from its start routine will contain the return code if the thread was canceled the memory location specified by is set to by calling we automatically place the thread with which we re joining in the detached state discussed shortly so that its resources can be recovered if the thread was already in the detached state can fail returning einval although this behavior is implementation specific if we re not interested in a thread return value we can set to null in this case calling allows us to wait for the specified thread but does not retrieve the thread termination status example figure shows how to fetch the exit code from a thread that has terminated include apue h include pthread h void void arg printf thread returning n return void void void arg printf thread exiting n void int main void int err void tret err null null if err err can t create thread err null null if err err can t create thread err tret if err err can t join with thread printf thread exit code ld n long tret err tret if err err can t join with thread printf thread exit code ld n long tret exit figure fetching the thread exit status running the program in figure gives us a out thread returning thread exiting thread exit code thread exit code as we can see when a thread exits by calling or by simply returning from the start routine the exit status can be obtained by another thread by calling the typeless pointer passed to and can be used to pass more than a single value the pointer can be used to pass the address of a structure containing more complex information be careful that the memory used for the structure is still valid when the caller has completed if the structure was allocated on the caller stack for example the memory contents might have changed by the time the structure is used if a thread allocates a structure on its stack and passes a pointer to this structure to then the stack might be destroyed and its memory reused for something else by the time the caller of tries to use it example the program in figure shows the problem with using an automatic variable allocated on the stack as the argument to include apue h include pthread h struct foo int a b c d void printfoo const char const struct foo fp printf printf structure at lx n unsigned long fp printf foo a d n fp a printf foo b d n fp b printf foo c d n fp c printf foo d d n fp d void void arg struct foo foo printfoo thread n foo void foo void void arg int printf thread id is lu n unsigned long void main void int err struct foo fp err null null if err err_exit err can t create thread err void fp if err err_exit err can t join with thread sleep printf parent starting second thread n err null null if err err_exit err can t create thread sleep printfoo parent n fp exit figure incorrect use of argument when we run this program on linux we get a out thread structure at foo a foo b foo c foo d parent starting second thread thread id is parent structure at foo a foo b foo c foo d of course the results vary depending on the memory architecture the compiler and the implementation of the threads library the results on solaris are similar a out thread structure at foo a foo b foo c foo d parent starting second thread thread id is parent structure at foo a foo b foo c foo d as we can see the contents of the structure allocated on the stack of thread have changed by the time the main thread can access the structure note how the stack of the second thread has overwritten the first thread stack to solve this problem we can either use a global structure or allocate the structure using malloc on mac os x we get different results a out thread structure at foo a foo b foo c foo d parent starting second thread thread id is parent structure at segmentation fault core dumped in this case the memory is no longer valid when the parent tries to access the structure passed to it by the first thread that exited and the parent is sent the sigsegv signal on freebsd the memory hasn t been overwritten by the time the parent accesses it and we get thread structure at foo a foo b foo c foo d parent starting second thread thread id is parent structure at foo a foo b foo c foo d even though the memory is still intact after the thread exits we can t depend on this always being the case it certainly isn t what we observe on the other platforms one thread can request that another in the same process be canceled by calling the function include pthread h int tid returns if ok error number on failure in the default circumstances will cause the thread specified by tid to behave as if it had called with an argument of however a thread can elect to ignore or otherwise control how it is canceled we will discuss this in detail in section note that doesn t wait for the thread to terminate it merely makes the request a thread can arrange for functions to be called when it exits similar to the way that the atexit function section can be used by a process to arrange that functions are to be called when the process exits the functions are known as thread cleanup handlers more than one cleanup handler can be established for a thread the handlers are recorded in a stack which means that they are executed in the reverse order from that with which they were registered the function schedules the cleanup function rtn to be called with the single argument arg when the thread performs one of the following actions makes a call to responds to a cancellation request makes a call to with a nonzero execute argument if the execute argument is set to zero the cleanup function is not called in either case removes the cleanup handler established by the last call to a restriction with these functions is that because they can be implemented as macros they must be used in matched pairs within the same scope in a thread the macro definition of can include a character in which case the matching character is in the definition example figure shows how to use thread cleanup handlers although the example is somewhat contrived it illustrates the mechanics involved note that although we never intend to pass zero as an argument to the thread start up routines we still need to match calls to with the calls to otherwise the program might not compile include apue h include pthread h void cleanup void arg printf cleanup n char arg void void arg printf thread start n cleanup thread first handler cleanup thread second handler printf thread push complete n if arg return void return void void void arg printf thread start n cleanup thread first handler cleanup thread second handler printf thread push complete n if arg void void int main void int err void tret err null void if err err_exit err can t create thread err null void if err err_exit err can t create thread err tret if err err_exit err can t join with thread printf thread exit code ld n long tret err tid2 tret if err err_exit err can t join with thread printf thread exit code ld n long tret exit figure thread cleanup handler running the program in figure on linux or solaris gives us a out thread start thread push complete thread start thread push complete cleanup thread second handler cleanup thread first handler thread exit code thread exit code from the output we can see that both threads start properly and exit but that only the second thread cleanup handlers are called thus if the thread terminates by returning from its start routine its cleanup handlers are not called although this behavior varies among implementations also note that the cleanup handlers are called in the reverse order from which they were installed if we run the same program on freebsd or mac os x we see that the program incurs a segmentation violation and drops core this happens because on these systems is implemented as a macro that stores some context on the stack when thread returns in between the call to and the call to pthread_cleanup_pop the stack is overwritten and these platforms try to use this now corrupted context when they invoke the cleanup handlers in the single unix specification returning while in between a matched pair of calls to and pthread_cleanup_pop results in undefined behavior the only portable way to return in between these two functions is to call by now you should begin to see similarities between the thread functions and the process functions figure summarizes the similar functions process primitive thread primitive description fork create a new flow of control exit exit from an existing flow of control waitpid get exit status from flow of control atexit register function to be called at exit from flow of control getpid get id for flow of control abort request abnormal termination of flow of control figure comparison of process and thread primitives by default a thread termination status is retained until we call for that thread a thread underlying storage can be reclaimed immediately on termination if the thread has been detached after a thread is detached we can t use the function to wait for its termination status because calling for a detached thread results in undefined behavior we can detach a thread by calling include pthread h int tid returns if ok error number on failure as we will see in the next chapter we can create a thread that is already in the detached state by modifying the thread attributes we pass to thread synchronization when multiple threads of control share the same memory we need to make sure that each thread sees a consistent view of its data if each thread uses variables that other threads don t read or modify no consistency problems will exist similarly if a variable is read only there is no consistency problem with more than one thread reading its value at the same time however when one thread can modify a variable that other threads can read or modify we need to synchronize the threads to ensure that they don t use an invalid value when accessing the variable memory contents when one thread modifies a variable other threads can potentially see inconsistencies when reading the value of that variable on processor architectures in which the modification takes more than one memory cycle this can happen when the memory read is interleaved between the memory write cycles of course this behavior is architecture dependent but portable programs can t make any assumptions about what type of processor architecture is being used figure shows a hypothetical example of two threads reading and writing the same variable in this example thread a reads the variable and then writes a new value to it but the write operation takes two memory cycles if thread b reads the same variable between the two write cycles it will see an inconsistent value thread a thread b time figure interleaved memory cycles with two threads to solve this problem the threads have to use a lock that will allow only one thread to access the variable at a time figure shows this synchronization if it wants to read the variable thread b acquires a lock similarly when thread a updates the variable it acquires the same lock thus thread b will be unable to read the variable until thread a releases the lock thread a thread b time figure two threads synchronizing memory access we also need to synchronize two or more threads that might try to modify the same variable at the same time consider the case in which we increment a variable figure 9 the increment operation is usually broken down into three steps read the memory location into a register increment the value in the register write the new value back to the memory location if two threads try to increment the same variable at almost the same time without synchronizing with each other the results can be inconsistent you end up with a value that is either one or two greater than before depending on the value observed when the second thread starts its operation if the second thread performs step before the first thread performs step the second thread will read the same initial value as the first thread increment it and write it back with no net effect if the modification is atomic then there isn t a race in the previous example if the increment takes only one memory cycle then no race exists if our data always appears to be sequentially consistent then we need no additional synchronization our operations are sequentially consistent when multiple threads can t observe inconsistencies in our data in modern computer systems memory accesses take multiple bus cycles and multiprocessors generally interleave bus cycles among multiple processors so we aren t guaranteed that our data is sequentially consistent thread a thread b contents of i time figure 9 two unsynchronized threads incrementing the same variable in a sequentially consistent environment we can explain modifications to our data as a sequential step of operations taken by the running threads we can say such things as thread a incremented the variable then thread b incremented the variable so its value is two greater than before or thread b incremented the variable then thread a incremented the variable so its value is two greater than before no possible ordering of the two threads can result in any other value of the variable besides the computer architecture races can arise from the ways in which our programs use variables creating places where it is possible to view inconsistencies for example we might increment a variable and then make a decision based on its value the combination of the increment step and the decision making step isn t atomic which opens a window where inconsistencies can arise mutexes we can protect our data and ensure access by only one thread at a time by using the pthreads mutual exclusion interfaces a mutex is basically a lock that we set lock before accessing a shared resource and release unlock when we re done while it is set any other thread that tries to set it will block until we release it if more than one thread is blocked when we unlock the mutex then all threads blocked on the lock will be made runnable and the first one to run will be able to set the lock the others will see that the mutex is still locked and go back to waiting for it to become available again in this way only one thread will proceed at a time this mutual exclusion mechanism works only if we design our threads to follow the same data access rules the operating system doesn t serialize access to data for us if we allow one thread to access a shared resource without first acquiring a lock then inconsistencies can occur even though the rest of our threads do acquire the lock before attempting to access the shared resource a mutex variable is represented by the data type before we can use a mutex variable we must first initialize it by either setting it to the constant for statically allocated mutexes only or calling if we allocate the mutex dynamically by calling malloc for example then we need to call before freeing the memory to initialize a mutex with the default attributes we set attr to null we will discuss mutex attributes in section to lock a mutex we call if the mutex is already locked the calling thread will block until the mutex is unlocked to unlock a mutex we call if a thread can t afford to block it can use to lock the mutex conditionally if the mutex is unlocked at the time is called then will lock the mutex without blocking and return otherwise will fail returning ebusy without locking the mutex example figure illustrates a mutex used to protect a data structure when more than one thread needs to access a dynamically allocated object we can embed a reference count in the object to ensure that we don t free its memory before all threads are done using it include stdlib h include pthread h struct foo int int more stuff here struct foo int id allocate the object struct foo fp if fp malloc sizeof struct foo null fp fp id if fp null free fp return null continue initialization return fp void struct foo fp add a reference to the object fp fp fp void struct foo fp release a reference to the object fp if fp last reference fp fp free fp else fp figure using a mutex to protect a data structure we lock the mutex before incrementing the reference count decrementing the reference count and checking whether the reference count reaches zero no locking is necessary when we initialize the reference count to in the function because the allocating thread is the only reference to it so far if we were to place the structure on a list at this point it could be found by other threads so we would need to lock it first before using the object threads are expected to add a reference to it by calling when they are done they must call to release the reference when the last reference is released the object memory is freed in this example we have ignored how threads find an object before calling even though the reference count is zero it would be a mistake for to free the object memory if another thread is blocked on the mutex in a call to we can avoid this problem by ensuring that the object can t be found before freeing its memory we ll see how to do this in the examples that follow deadlock avoidance a thread will deadlock itself if it tries to lock the same mutex twice but there are less obvious ways to create deadlocks with mutexes for example when we use more than one mutex in our programs a deadlock can occur if we allow one thread to hold a mutex and block while trying to lock a second mutex at the same time that another thread holding the second mutex tries to lock the first mutex neither thread can proceed because each needs a resource that is held by the other so we have a deadlock deadlocks can be avoided by carefully controlling the order in which mutexes are locked for example assume that you have two mutexes a and b that you need to lock at the same time if all threads always lock mutex a before mutex b no deadlock can occur from the use of the two mutexes but you can still deadlock on other resources similarly if all threads always lock mutex b before mutex a no deadlock will occur you ll have the potential for a deadlock only when one thread attempts to lock the mutexes in the opposite order from another thread sometimes an application architecture makes it difficult to apply a lock ordering if enough locks and data structures are involved that the functions you have available can t be molded to fit a simple hierarchy then you ll have to try some other approach in this case you might be able to release your locks and try again at a later time you can use the interface to avoid deadlocking in this case if you are already holding locks and is successful then you can proceed if it can t acquire the lock however you can release the locks you already hold clean up and try again later example in this example we update figure to show the use of two mutexes we avoid deadlocks by ensuring that when we need to acquire two mutexes at the same time we always lock them in the same order the second mutex protects a hash list that we use to keep track of the foo data structures thus the hashlock mutex protects both the fh hash table and the hash link field in the foo structure the mutex in the foo structure protects access to the remainder of the foo structure fields include stdlib h include pthread h define nhash define hash id unsigned long id nhash struct foo fh nhash hashlock struct foo int int struct foo protected by hashlock more stuff here struct foo int id allocate the object struct foo fp int idx if fp malloc sizeof struct foo null fp fp id if fp null free fp return null idx hash id hashlock fp fh idx fh idx fp fp hashlock continue initialization fp return fp void struct foo fp add a reference to the object fp fp fp struct foo int id find an existing object struct foo fp hashlock for fp fh hash id fp null fp fp if fp id fp break hashlock return fp void struct foo fp release a reference to the object struct foo tfp int idx fp if fp last reference fp hashlock fp need to recheck the condition if fp fp fp hashlock return remove from list idx hash fp tfp fh idx if tfp fp fh idx fp else while tfp fp tfp tfp tfp fp hashlock fp fp free fp else fp fp figure using two mutexes comparing figure with figure we see that our allocation function now locks the hash list lock adds the new structure to a hash bucket and before unlocking the hash list lock locks the mutex in the new structure since the new structure is placed on a global list other threads can find it so we need to block them if they try to access the new structure until we are done initializing it the function locks the hash list lock and searches for the requested structure if it is found we increase the reference count and return a pointer to the structure note that we honor the lock ordering by locking the hash list lock in before locks the foo structure mutex now with two locks the function is more complicated if this is the last reference we need to unlock the structure mutex so that we can acquire the hash list lock since we ll need to remove the structure from the hash list then we reacquire the structure mutex because we could have blocked since the last time we held the structure mutex we need to recheck the condition to see whether we still need to free the structure if another thread found the structure and added a reference to it while we blocked to honor the lock ordering we simply need to decrement the reference count unlock everything and return this locking approach is complex so we need to revisit our design we can simplify things considerably by using the hash list lock to protect the structure reference count too the structure mutex can be used to protect everything else in the foo structure figure reflects this change include stdlib h include pthread h define nhash define hash id unsigned long id nhash struct foo fh nhash hashlock struct foo int protected by hashlock int struct foo protected by hashlock more stuff here struct foo int id allocate the object struct foo fp int idx if fp malloc sizeof struct foo null fp fp id if fp null free fp return null idx hash id hashlock fp fh idx fh idx fp fp hashlock continue initialization fp return fp void struct foo fp add a reference to the object hashlock fp hashlock struct foo int id find an existing object struct foo fp hashlock for fp fh hash id fp null fp fp if fp id fp break hashlock return fp void struct foo fp release a reference to the object struct foo tfp int idx hashlock if fp last reference remove from list idx hash fp tfp fh idx if tfp fp fh idx fp else while tfp fp tfp tfp tfp fp hashlock fp free fp else hashlock figure simplified locking note how much simpler the program in figure is compared to the program in figure the lock ordering issues surrounding the hash list and the reference count go away when we use the same lock for both purposes multithreaded software design involves these types of trade offs if your locking granularity is too coarse you end up with too many threads blocking behind the same locks with little improvement possible from concurrency if your locking granularity is too fine then you suffer bad performance from excess locking overhead and you end up with complex code as a programmer you need to find the correct balance between code complexity and performance while still satisfying your locking requirements function one additional mutex primitive allows us to bound the time that a thread blocks when a mutex it is trying to acquire is already locked the function is equivalent to but if the timeout value is reached will return the error code etimedout without locking the mutex the timeout specifies how long we are willing to wait in terms of absolute time as opposed to relative time we specify that we are willing to block until time x instead of saying that we are willing to block for y seconds the timeout is represented by the timespec structure which describes time in terms of seconds and nanoseconds example in figure we see how to use to avoid blocking indefinitely include apue h include pthread h int main void int err struct timespec tout struct tm tmp char buf lock lock printf mutex is locked n tout tmp localtime tout strftime buf sizeof buf r tmp printf current time is n buf tout seconds from now caution this could lead to deadlock err lock tout tout tmp localtime tout strftime buf sizeof buf r tmp printf the time is now n buf if err printf mutex locked again n else printf can t lock mutex again n strerror err exit figure 13 using here is the output from the program in figure 13 a out mutex is locked current time is am the time is now am can t lock mutex again connection timed out this program deliberately locks a mutex it already owns to demonstrate how works this strategy is not recommended in practice because it can lead to deadlock note that the time blocked can vary for several reasons the start time could have been in the middle of a second the resolution of the system clock might not be fine enough to support the resolution of our timeout or scheduling delays could prolong the amount of time until the program continues execution mac os x doesn t support yet but freebsd linux and solaris do support it although solaris still bundles it in the real time library librt solaris also provides an alternative function that uses a relative timeout reader writer locks reader writer locks are similar to mutexes except that they allow for higher degrees of parallelism with a mutex the state is either locked or unlocked and only one thread can lock it at a time three states are possible with a reader writer lock locked in read mode locked in write mode and unlocked only one thread at a time can hold a reader writer lock in write mode but multiple threads can hold a reader writer lock in read mode at the same time when a reader writer lock is write locked all threads attempting to lock it block until it is unlocked when a reader writer lock is read locked all threads attempting to lock it in read mode are given access but any threads attempting to lock it in write mode block until all the threads have released their read locks although implementations vary reader writer locks usually block additional readers if a lock is already held in read mode and a thread is blocked trying to acquire the lock in write mode this prevents a constant stream of readers from starving waiting writers reader writer locks are well suited for situations in which data structures are read more often than they are modified when a reader writer lock is held in write mode the data structure it protects can be modified safely since only one thread at a time can hold the lock in write mode when the reader writer lock is held in read mode the data structure it protects can be read by multiple threads as long as the threads first acquire the lock in read mode reader writer locks are also called shared exclusive locks when a reader writer lock is read locked it is said to be locked in shared mode when it is write locked it is said to be locked in exclusive mode as with mutexes reader writer locks must be initialized before use and destroyed before freeing their underlying memory a reader writer lock is initialized by calling we can pass a null pointer for attr if we want the reader writer lock to have the default attributes we discuss reader writer lock attributes in section the single unix specification defines the constant in the xsi option it can be used to initialize a statically allocated reader writer lock when the default attributes are sufficient before freeing the memory backing a reader writer lock we need to call to clean it up if allocated any resources for the reader writer lock frees those resources if we free the memory backing a reader writer lock without first calling any resources assigned to the lock will be lost to lock a reader writer lock in read mode we call to write lock a reader writer lock we call regardless of how we lock a reader writer lock we can unlock it by calling implementations might place a limit on the number of times a reader writer lock can be locked in shared mode so we need to check the return value of even though and have error returns and technically we should always check for errors when we call functions that can potentially fail we don t need to check them if we design our locking properly the only error returns defined are when we use them improperly such as with an uninitialized lock or when we might deadlock by attempting to acquire a lock we already own however be aware that specific implementations might define additional error returns the single unix specification also defines conditional versions of the reader writer locking primitives when the lock can be acquired these functions return otherwise they return the error ebusy these functions can be used to avoid deadlocks in situations where conforming to a lock hierarchy is difficult as we discussed previously example the program in figure illustrates the use of reader writer locks a queue of job requests is protected by a single reader writer lock this example shows a possible implementation of figure whereby multiple worker threads obtain jobs assigned to them by a single master thread include stdlib h include pthread h struct job struct job struct job tells which thread handles this job more stuff here struct queue struct job struct job initialize a queue int struct queue qp int err qp null qp null err qp null if err return err continue initialization return insert a job at the head of the queue void struct queue qp struct job jp qp jp qp jp null if qp null qp jp else qp jp list was empty qp jp qp append a job on the tail of the queue void struct queue qp struct job jp qp jp null jp qp if qp null qp jp else qp jp list was empty qp jp qp remove the given job from a queue void struct queue qp struct job jp qp if jp qp qp jp if qp jp qp null else jp jp else if jp qp qp jp jp jp else jp jp jp jp j_prev qp find a job for the given thread id struct job struct queue qp id struct job jp if qp return null for jp qp jp null jp jp if jp id break qp return jp figure using reader writer locks in this example we lock the queue reader writer lock in write mode whenever we need to add a job to the queue or remove a job from the queue whenever we search the queue we grab the lock in read mode allowing all the worker threads to search the queue concurrently using a reader writer lock will improve performance in this case only if threads search the queue much more frequently than they add or remove jobs the worker threads take only those jobs that match their thread id off the queue since the job structures are used only by one thread at a time they don t need any extra locking 5 reader writer locking with timeouts just as with mutexes the single unix specification provides functions to lock reader writer locks with a timeout to give applications a way to avoid blocking indefinitely while trying to acquire a reader writer lock these functions are and these functions behave like their untimed counterparts the tsptr argument points to a timespec structure specifying the time at which the thread should stop blocking if they can t acquire the lock these functions return the etimedout error when the timeout expires like the function the timeout specifies an absolute time not a relative one condition variables condition variables are another synchronization mechanism available to threads these synchronization objects provide a place for threads to rendezvous when used with mutexes condition variables allow threads to wait in a race free way for arbitrary conditions to occur the condition itself is protected by a mutex a thread must first lock the mutex to change the condition state other threads will not notice the change until they acquire the mutex because the mutex must be locked to be able to evaluate the condition before a condition variable is used it must first be initialized a condition variable represented by the data type can be initialized in two ways we can assign the constant to a statically allocated condition variable but if the condition variable is allocated dynamically we can use the function to initialize it we can use the function to deinitialize a condition variable before freeing its underlying memory unless you need to create a conditional variable with nondefault attributes the attr argument to can be set to null we will discuss condition variable attributes in section we use to wait for a condition to be true a variant is provided to return an error code if the condition hasn t been satisfied in the specified amount of time the mutex passed to protects the condition the caller passes it locked to the function which then atomically places the calling thread on the list of threads waiting for the condition and unlocks the mutex this closes the window between the time that the condition is checked and the time that the thread goes to sleep waiting for the condition to change so that the thread doesn t miss a change in the condition when returns the mutex is again locked the function provides the same functionality as the function with the addition of the timeout tsptr the timeout value specifies how long we are willing to wait expressed as a timespec structure just as we saw in figure 13 we need to specify how long we are willing to wait as an absolute time instead of a relative time for example suppose we are willing to wait minutes instead of translating minutes into a timespec structure we need to translate now minutes into a timespec structure we can use the function section 6 to get the current time expressed as a timespec structure however this function is not yet supported on all platforms alternatively we can use the gettimeofday function to get the current time expressed as a timeval structure and translate it into a timespec structure to obtain the absolute time for the timeout value we can use the following function assuming the maximum time blocked is expressed in minutes include sys time h include stdlib h void maketimeout struct timespec tsp long minutes struct timeval now get the current time gettimeofday now null tsp now tsp now usec to nsec add the offset to get timeout value tsp minutes if the timeout expires without the condition occurring will reacquire the mutex and return the error etimedout when it returns from a successful call to or a thread needs to reevaluate the condition since another thread might have run and already changed the condition there are two functions to notify threads that a condition has been satisfied the function will wake up at least one thread waiting on a condition whereas the function will wake up all threads waiting on a condition the posix specification allows for implementations of to wake up more than one thread to make the implementation simpler when we call or we are said to be signaling the thread or condition we have to be careful to signal the threads only after changing the state of the condition example figure shows an example of how to use a condition variable and a mutex together to synchronize threads include pthread h struct msg struct msg more stuff here struct msg workq qready qlock void void struct msg mp for qlock while workq null qready qlock mp workq workq mp qlock now process the message mp void struct msg mp qlock mp workq workq mp qlock qready figure using a condition variable the condition is the state of the work queue we protect the condition with a mutex and evaluate the condition in a while loop when we put a message on the work queue we need to hold the mutex but we don t need to hold the mutex when we signal the waiting threads as long as it is okay for a thread to pull the message off the queue before we call we can do this after releasing the mutex since we check the condition in a while loop this doesn t present a problem a thread will wake up find that the queue is still empty and go back to waiting again if the code couldn t tolerate this race we would need to hold the mutex when we signal the threads 6 spin locks a spin lock is like a mutex except that instead of blocking a process by sleeping the process is blocked by busy waiting spinning until the lock can be acquired a spin lock could be used in situations where locks are held for short periods of times and threads don t want to incur the cost of being descheduled spin locks are often used as low level primitives to implement other types of locks depending on the system architecture they can be implemented efficiently using test and set instructions although efficient they can lead to wasting cpu resources while a thread is spinning and waiting for a lock to become available the cpu can t do anything else this is why spin locks should be held only for short periods of time spin locks are useful when used in a nonpreemptive kernel besides providing a mutual exclusion mechanism they block interrupts so an interrupt handler can t deadlock the system by trying to acquire a spin lock that is already locked think of interrupts as another type of preemption in these types of kernels interrupt handlers can t sleep so the only synchronization primitives they can use are spin locks however at user level spin locks are not as useful unless you are running in a real time scheduling class that doesn t allow preemption user level threads running in a time sharing scheduling class can be descheduled when their time quantum expires or when a thread with a higher scheduling priority becomes runnable in these cases if a thread is holding a spin lock it will be put to sleep and other threads blocked on the lock will continue spinning longer than intended many mutex implementations are so efficient that the performance of applications using mutex locks is equivalent to their performance if they had used spin locks in fact some mutex implementations will spin for a limited amount of time trying to acquire the mutex and only sleep when the spin count threshold is reached these factors combined with advances in modern processors that allow them to context switch at faster and faster rates make spin locks useful only in limited circumstances the interfaces for spin locks are similar to those for mutexes making it relatively easy to replace one with the other we can initialize a spin lock with the function to deinitialize a spin lock we can call the function only one attribute is specified for spin locks which matters only if the platform supports the thread process shared synchronization option now mandatory in the single unix specification recall figure 5 the pshared argument represents the process shared attribute which indicates how the spin lock will be acquired if it is set to then the spin lock can be acquired by threads that have access to the lock underlying memory even if those threads are from different processes otherwise the pshared argument is set to and the spin lock can be accessed only from threads within the process that initialized it to lock the spin lock we can call either which will spin until the lock is acquired or which will return the ebusy error if the lock can t be acquired immediately note that doesn t spin regardless of how it was locked a spin lock can be unlocked by calling note that if a spin lock is currently unlocked then the function can lock it without spinning if the thread already has it locked the results are undefined the call to could fail with the edeadlk error or some other error or the call could spin indefinitely the behavior depends on the implementation if we try to unlock a spin lock that is not locked the results are also undefined if either or returns then the spin lock is locked we need to be careful not to call any functions that might sleep while holding the spin lock if we do then we ll waste cpu resources by extending the time other threads will spin if they try to acquire it 6 barriers barriers are a synchronization mechanism that can be used to coordinate multiple threads working in parallel a barrier allows each thread to wait until all cooperating threads have reached the same point and then continue executing from there we ve already seen one form of barrier the function acts as a barrier to allow one thread to wait until another thread exits barrier objects are more general than this however they allow an arbitrary number of threads to wait until all of the threads have completed processing but the threads don t have to exit they can continue working after all threads have reached the barrier we can use the function to initialize a barrier and we can use the function to deinitialize a barrier when we initialize a barrier we use the count argument to specify the number of threads that must reach the barrier before all of the threads will be allowed to continue we use the attr argument to specify the attributes of the barrier object which we ll look at more closely in the next chapter for now we can set attr to null to initialize a barrier with the default attributes if the function allocated any resources for the barrier the resources will be freed when we deinitialize the barrier by calling the function we use the function to indicate that a thread is done with its work and is ready to wait for all the other threads to catch up the thread calling is put to sleep if the barrier count set in the call to is not yet satisfied if the thread is the last one to call thereby satisfying the barrier count all of the threads are awakened to one arbitrary thread it will appear as if the function returned a value of the remaining threads see a return value of this allows one thread to continue as the master to act on the results of the work done by all of the other threads once the barrier count is reached and the threads are unblocked the barrier can be used again however the barrier count can t be changed unless we call the function followed by the function with a different count example figure 16 shows how a barrier can be used to synchronize threads cooperating on a single task include apue h include pthread h include limits h include sys time h define nthr number of threads define numnum number of numbers to sort define tnum numnum nthr number to sort per thread long nums numnum long snums numnum b ifdef solaris define heapsort qsort else extern int heapsort void int const void const void endif compare two long integers helper function for heapsort int complong const void const void long long long long if return else if return else return worker thread to sort a portion of the set of numbers void void arg long idx long arg heapsort nums idx tnum sizeof long complong b go off and perform more work return void merge the results of the individual sorted ranges void merge long idx nthr long i minidx sidx num for i i nthr i idx i i tnum for sidx sidx numnum sidx num for i i nthr i if idx i i tnum nums idx i num num nums idx i minidx i int snums sidx nums idx minidx idx minidx main unsigned long i struct timeval start end long long startusec endusec double elapsed int err tid create the initial set of numbers to sort srandom for i i numnum i nums i random create threads to sort the numbers gettimeofday start null b null nthr for i i nthr i err tid null void i tnum if err err_exit err can t create thread b merge gettimeofday end null print the sorted list startusec start start endusec end end elapsed double endusec startusec printf sort took seconds n elapsed for i i numnum i printf ld n snums i exit figure 16 using a barrier this example shows the use of a barrier in a simplified situation where the threads perform only one task in more realistic situations the worker threads will continue with other activities after the call to returns in the example we use eight threads to divide the job of sorting million numbers each thread sorts million numbers using the heapsort algorithm see knuth for details then the main thread calls a function to merge the results we don t need to use the return value from to decide which thread merges the results because we use the main thread for this task that is why we specify the barrier count as one more than the number of worker threads the main thread counts as one waiter if we write a program to sort million numbers with heapsort using thread only we will see a performance improvement when comparing it to the program in figure 16 on a system with cores the single threaded program sorted million numbers in seconds on the same system using threads in parallel and thread to merge the results the same set of million numbers was sorted in 91 seconds 6 times faster summary in this chapter we introduced the concept of threads and discussed the posix primitives available to create and destroy them we also introduced the problem of thread synchronization we discussed five fundamental synchronization mechanisms mutexes reader writer locks condition variables spin locks and barriers and we saw how to use them to protect shared resources exercises modify the example code shown in figure to pass the structure between the threads properly in the example code shown in figure 14 what additional synchronization if any is necessary to allow the master thread to change the thread id associated with a pending job how would this affect the function apply the techniques shown in figure 15 to the worker thread example figures and 14 to implement the worker thread function don t forget to update the function to initialize the condition variable and change the and functions to signal the worker threads what difficulties arise which sequence of steps is correct lock a mutex change the condition protected by the mutex signal threads waiting on the condition unlock the mutex or lock a mutex change the condition protected by the mutex unlock the mutex 4 signal threads waiting on the condition 5 what synchronization primitives would you need to implement a barrier provide an implementation of the function this page intentionally left blank thread control introduction in chapter we learned the basics about threads and thread synchronization in this chapter we will learn the details of controlling thread behavior we will look at thread attributes and synchronization primitive attributes which we ignored in the previous chapter in favor of the default behavior we will follow this with a look at how threads can keep data private from other threads in the same process then we will wrap up the chapter with a look at how some process based system calls interact with threads thread limits we discussed the sysconf function in section 5 4 the single unix specification defines several limits associated with the operation of threads which we didn t show in figure 2 as with other system limits the thread limits can be queried using sysconf figure summarizes these limits as with the other limits reported by sysconf use of these limits is intended to promote application portability among different operating system implementations for example if your application requires that you create four threads for every file you manage you might have to limit the number of files you can manage concurrently if the system won t let you create enough threads name of limit description name argument maximum number of times an implementation will try to destroy the thread specific data when a thread exits section 6 maximum number of keys that can be created by a process section 6 minimum number of bytes that can be used for a thread stack section maximum number of threads that can be created in a process section figure thread limits and name arguments to sysconf figure 2 shows the values of the thread limits for the four implementations described in this book if the implementation limit is indeterminate no limit is listed this doesn t mean that the value is unlimited however note that although an implementation may not provide access to these limits that doesn t mean that the limits don t exist it just means that the implementation doesn t provide us with a way to get at them using sysconf limit freebsd 8 linux 2 mac os x 6 8 solaris pthread_destructor_iterations 4 4 4 no limit 256 024 no limit 2 16 8 8 pthread_threads_max no limit no limit no limit no limit figure 2 examples of thread configuration limits thread attributes the pthread interface allows us to fine tune the behavior of threads and synchronization objects by setting various attributes associated with each object generally the functions for managing these attributes follow the same pattern each object is associated with its own type of attribute object threads with thread attributes mutexes with mutex attributes and so on an attribute object can represent multiple attributes the attribute object is opaque to applications this means that applications aren t supposed to know anything about its internal structure which promotes application portability instead functions are provided to manage the attributes objects 2 an initialization function exists to set the attributes to their default values another function exists to destroy the attributes object if the initialization function allocated any resources associated with the attributes object the destroy function frees those resources 4 each attribute has a function to get the value of the attribute from the attribute object because the function returns on success or an error number on failure the value is returned to the caller by storing it in the memory location specified by one of the arguments 5 each attribute has a function to set the value of the attribute in this case the value is passed as an argument by value in all the examples in which we called in chapter we passed in a null pointer instead of passing in a pointer to a structure we can use the structure to modify the default attributes and associate these attributes with threads that we create we use the function to initialize the structure after calling the structure contains the default values for all the thread attributes supported by the implementation to deinitialize a structure we call if an implementation of allocated any dynamic memory for the attribute object will free that memory in addition will initialize the attribute object with invalid values so if it is used by mistake will return an error code the thread attributes defined by posix are summarized in figure posix defines additional attributes in the thread execution scheduling option intended to support real time applications but we don t discuss them here in figure we also show which platforms support each thread attribute name description freebsd 8 linux 2 mac os x 6 8 solaris detachstate detached thread attribute guardsize guard buffer size in bytes at end of thread stack stackaddr lowest address of thread stack stacksize minimum size in bytes of thread stack figure 3 posix thread attributes in section 5 we introduced the concept of detached threads if we are no longer interested in an existing thread termination status we can use to allow the operating system to reclaim the thread resources when the thread exits if we know that we don t need the thread termination status at the time we create the thread we can arrange for the thread to start out in the detached state by modifying the detachstate thread attribute in the structure we can use the function to set the detachstate thread attribute to one of two legal values to start the thread in the detached state or to start the thread normally so its termination status can be retrieved by the application we can call to obtain the current detachstate attribute the integer pointed to by the second argument is set to either or depending on the value of the attribute in the given structure example figure 4 shows a function that can be used to create a thread in the detached state include apue h include pthread h int makethread void fn void void arg int err tid attr err attr if err return err err attr if err err tid attr fn arg attr return err figure 4 creating a thread in the detached state note that we ignore the return value from the call to in this case we initialized the thread attributes properly so shouldn t fail nonetheless if it does fail cleaning up would be difficult we would have to destroy the thread we just created which might already be running asynchronous to the execution of this function when we choose to ignore the error return from the worst that can happen is that we leak a small amount of memory if had allocated any but if succeeded in initializing the thread attributes and then failed to clean up we have no recovery strategy anyway because the attributes structure is opaque to the application the only interface defined to clean up the structure is and it just failed support for thread stack attributes is optional for a posix conforming operating system but is required if the system supports the xsi option in the single unix specification at compile time you can check whether your system supports each thread stack attribute by using the and symbols if one of these symbols is defined then the system supports the corresponding thread stack attribute alternatively you can check for support at runtime by using the and parameters to the sysconf function we can manage the stack attributes using the and functions with a process the amount of virtual address space is fixed since there is only one stack its size usually isn t a problem with threads however the same amount of virtual address space must be shared by all the thread stacks you might have to reduce your default thread stack size if your application uses so many threads that the cumulative size of their stacks exceeds the available virtual address space on the other hand if your threads call functions that allocate large automatic variables or call functions many stack frames deep you might need more than the default stack size if you run out of virtual address space for thread stacks you can use malloc or mmap see section 14 8 to allocate space for an alternative stack and use to change the stack location of threads you create the address specified by the stackaddr parameter is the lowest addressable address in the range of memory to be used as the thread stack aligned at the proper boundary for the processor architecture of course this assumes that the virtual address range used by malloc or mmap is different from the range currently in use for a thread stack the stackaddr thread attribute is defined as the lowest memory address for the stack this is not necessarily the start of the stack however if stacks grow from higher addresses to lower addresses for a given processor architecture the stackaddr thread attribute will be the end of the stack instead of the beginning an application can also get and set the stacksize thread attribute using the and functions the function is useful when you want to change the default stack size but don t want to deal with allocating the thread stacks on your own when setting the stacksize attribute the size we choose can t be smaller than the guardsize thread attribute controls the size of the memory extent after the end of the thread s stack to protect against stack overflow its default value is implementation defined but a commonly used value is the system page size we can set the guardsize thread attribute to to disable this feature no guard buffer will be provided in this case also if we change the stackaddr thread attribute the system assumes that we will be managing our own stacks and disables stack guard buffers just as if we had set the guardsize thread attribute to if the guardsize thread attribute is modified the operating system might round it up to an integral multiple of the page size if the thread s stack pointer overflows into the guard area the application will receive an error possibly with a signal the single unix specification defines several other optional thread attributes intended for use by real time applications we will not discuss them here threads have other attributes not represented by the structure the cancelability state and the cancelability type we discuss them in section 4 synchronization attributes just as threads have attributes so too do their synchronization objects in section 6 7 we saw how spin locks have one attribute called the process shared attribute in this section we discuss the attributes of mutexes reader writer locks condition variables and barriers 4 mutex attributes mutex attributes are represented by a structure whenever we initialized a mutex in chapter we accepted the default attributes by using the constant or by calling the function with a null pointer for the argument that points to the mutex attribute structure when dealing with nondefault attributes we use to initialize a structure and to deinitialize one the function will initialize the structure with the default mutex attributes there are three attributes of interest the process shared attribute the robust attribute and the type attribute within posix the process shared attribute is optional you can test whether a platform supports it by checking whether the symbol is defined you can also check at runtime by passing the parameter to the sysconf function although this option is not required to be provided by posix conforming operating systems the single unix specification requires that xsi conforming operating systems do support it within a process multiple threads can access the same synchronization object this is the default behavior as we saw in chapter in this case the process shared mutex attribute is set to as we shall see in chapters 14 and 15 mechanisms exist that allow independent processes to map the same extent of memory into their independent address spaces access to shared data by multiple processes usually requires synchronization just as does access to shared data by multiple threads if the process shared mutex attribute is set to a mutex allocated from a memory extent shared between multiple processes may be used for synchronization by those processes we can use the function to query a structure for its process shared attribute we can change the process shared attribute with the function the process shared mutex attribute allows the pthread library to provide more efficient mutex implementations when the attribute is set to which is the default case with multithreaded applications the pthread library can then restrict the more expensive implementation to the case in which mutexes are shared among processes the robust mutex attribute is related to mutexes that are shared among multiple processes it is meant to address the problem of mutex state recovery when a process terminates while holding a mutex when this happens the mutex is left in a locked state and recovery is difficult threads blocked on the lock in other processes will block indefinitely we can use the function to get the value of the robust mutex attribute to set the value of the robust mutex attribute we can call the function there are two possible values for the robust attribute the default is which means that no special action is taken when a process terminates while holding a mutex in this case use of the mutex can result in undefined behavior and applications waiting for it to be unlocked are effectively stalled the other value is this value will cause a thread blocked in a call to to acquire the lock when another process holding the lock terminates without first unlocking it but the return value from is eownerdead instead of applications can use this special return value as an indication that they need to recover whatever state the mutex was protecting if possible the details of what state is being protected and how it can be recovered will vary among applications note that the eownerdead error return isn t really an error in this case because the caller will own the lock using robust mutexes changes the way we use because we now have to check for three return values instead of two success with no recovery needed success but recovery needed and failure however if we don t use robust mutexes then we can continue to check only for success and failure of the four platforms covered in this text only linux 3 2 currently supports robust pthread mutexes solaris supports robust mutexes only in its solaris threads library see the solaris manual page for more information however in solaris robust pthread mutexes are supported if the application state can t be recovered the mutex will be in a permanently unusable state after the thread unlocks the mutex to prevent this problem the thread can call the function to indicate that the state associated with the mutex is consistent before unlocking the mutex if a thread unlocks a mutex without first calling then other threads that are blocked while trying to acquire the mutex will see error returns of enotrecoverable if this happens the mutex is no longer usable by calling beforehand a thread allows the mutex to behave normally so it can continue to be used the type mutex attribute controls the locking characteristics of the mutex posix 1 defines four types a standard mutex type that doesn t do any special error checking or deadlock detection a mutex type that provides error checking a mutex type that allows the same thread to lock it multiple times without first unlocking it a recursive mutex maintains a lock count and isn t released until it is unlocked the same number of times it is locked thus if you lock a recursive mutex twice and then unlock it the mutex remains locked until it is unlocked a second time a mutex type providing default characteristics and behavior implementations are free to map it to one of the other mutex types for example linux 3 2 maps this type to the normal mutex type whereas freebsd 8 maps it to the error checking type the behavior of the four types is summarized in figure 5 the unlock when not owned column refers to one thread unlocking a mutex that was locked by a different thread the unlock when unlocked column refers to what happens when a thread unlocks a mutex that is already unlocked which usually is a coding mistake mutex type relock without unlock unlock when not owned unlock when unlocked deadlock undefined undefined returns error returns error returns error allowed returns error returns error undefined undefined undefined figure 5 mutex type behavior we can use the function to get the mutex type attribute to change the attribute we can use the function recall from section 11 6 6 that a mutex is used to protect the condition that is associated with a condition variable before blocking the thread the and the functions release the mutex associated with the condition this allows other threads to acquire the mutex change the condition release the mutex and signal the condition variable since the mutex must be held to change the condition it is not a good idea to use a recursive mutex if a recursive mutex is locked multiple times and used in a call to the condition can never be satisfied because the unlock done by doesn t release the mutex recursive mutexes are useful when you need to adapt existing single threaded interfaces to a multithreaded environment but can t change the interfaces to your functions because of compatibility constraints however using recursive locks can be tricky and they should be used only when no other solution is possible example figure 6 illustrates a situation in which a recursive mutex might seem to solve a concurrency problem assume that and are existing functions in a library whose interfaces can t be changed because applications exist that call them and those applications can t be changed to keep the interfaces the same we embed a mutex in the data structure whose address x is passed in as an argument this is possible only if we have provided an allocator function for the structure so the application doesn t know about its size assuming we must increase its size when we add a mutex to it this is also possible if we originally defined the structure with enough padding to allow us now to replace some pad fields with a mutex unfortunately most programmers are unskilled at predicting the future so this is not a common practice if both and must manipulate the structure and it is possible to access it from more than one thread at a time then and must lock the mutex before manipulating the structure if must call we will deadlock if the mutex type is not recursive we could avoid using a recursive mutex if we could release x x lock x x lock x x lock x lock figure 6 recursive locking opportunity the mutex before calling and reacquire it after returns but this approach opens a window where another thread can possibly grab control of the mutex and change the data structure in the middle of this may not be acceptable depending on what protection the mutex is intended to provide figure 7 shows an alternative to using a recursive mutex in this case we can leave the interfaces to and unchanged and avoid a recursive mutex by providing a private version of called to call we must hold the mutex embedded in the data structure whose address we pass as the argument the body of contains a copy of and now simply acquires the mutex calls and then releases the mutex if we didn t have to leave the interfaces to the library functions unchanged we could have added a second parameter to each function to indicate whether the structure is locked by the caller it is usually better to leave the interfaces unchanged if we can however instead of polluting them with implementation artifacts the strategy of providing locked and unlocked versions of functions is usually applicable in simple situations in more complex situations such as when the library needs to call a function outside the library which then might call back into the library we need to rely on recursive locks x x lock x x x x lock figure 7 avoiding a recursive locking opportunity example the program in figure 8 illustrates another situation in which a recursive mutex is necessary here we have a timeout function that allows us to schedule another function to be run at some time in the future assuming that threads are an inexpensive resource we can create a thread for each pending timeout the thread waits until the time has been reached and then it calls the function we ve requested the problem arises when we can t create a thread or when the scheduled time to run the function has already passed in these cases we simply call the requested function now from the current context since the function acquires the same lock that we currently hold a deadlock will occur unless the lock is recursive include apue h include pthread h include time h include sys time h extern int makethread void void void struct void void function void argument struct timespec time to wait define sectonsec seconds to nanoseconds if defined defined bsd define id fl req rem nanosleep req rem endif ifndef define define usectonsec microseconds to nanoseconds void int id struct timespec tsp struct timeval tv gettimeofday tv null tsp tv tsp tv usectonsec endif void void arg struct tip tip struct arg 0 tip null tip tip free arg return 0 void timeout const struct timespec when void func void void arg struct timespec now struct tip int err now if when now when now when now tip malloc sizeof struct if tip null tip func tip arg tip when now if when now tip when now else tip tip sectonsec now when err makethread void tip if err 0 return else free tip we get here if a when now or b malloc fails or c we can t make a thread so we just call the function now func arg attr mutex void retry void arg int mutex perform retry steps mutex main void int err condition arg struct timespec when if err attr 0 err_exit err failed if err attr 0 err_exit err can t set recursive type if err mutex attr 0 err_exit err can t create recursive mutex continue processing mutex check the condition under the protection of a lock to make the check and the call to timeout atomic if condition calculate the absolute time when we want to retry when when 10 seconds from now timeout when retry void unsigned long arg mutex continue processing exit 0 figure 8 using a recursive mutex we use the makethread function from figure 4 to create a thread in the detached state because the func function argument passed to the timeout function will run in the future we don t want to wait around for the thread to complete we could call sleep to wait for the timeout to expire but that gives us only second granularity if we want to wait for some time other than an integral number of seconds we need to use nanosleep or both of which allow us to sleep at higher resolution on systems that don t define we define in terms of nanosleep however freebsd 8 0 defines this symbol to support and but doesn t support only linux 3 2 0 and solaris 10 currently support additionally on systems that don t define we provide our own implementation of that calls gettimeofday and translates microseconds to nanoseconds the caller of timeout needs to hold a mutex to check the condition and to schedule the retry function as an atomic operation the retry function will try to lock the same mutex unless the mutex is recursive a deadlock will occur if the timeout function calls retry directly 4 2 reader writer lock attributes reader writer locks also have attributes similar to mutexes we use to initialize a structure and to deinitialize the structure the only attribute supported for reader writer locks is the process shared attribute it is identical to the mutex process shared attribute just as with the mutex process shared attributes a pair of functions is provided to get and set the process shared attributes of reader writer locks although posix defines only one reader writer lock attribute implementations are free to define additional nonstandard ones 4 3 condition variable attributes the single unix specification currently defines two attributes for condition variables the process shared attribute and the clock attribute as with the other attribute objects a pair of functions initialize and deinitialize condition variable attribute objects the process shared attribute is the same as with the other synchronization attributes it controls whether condition variables can be used by threads within a single process only or from within multiple processes to find the current value of the process shared attribute we use the function to set its value we use the function the clock attribute controls which clock is used when evaluating the timeout argument tsptr of the function the legal values are the clock ids listed in figure 6 8 we can use the function to retrieve the clock id that will be used by the function for the condition variable that was initialized with the object we can change the clock id with the function curiously the single unix specification doesn t define the clock attribute for any of the other attribute objects that have a wait function with a timeout 4 4 barrier attributes barriers have attributes too we can use the function to initialize a barrier attributes object and the function to deinitialize a barrier attributes object the only barrier attribute currently defined is the process shared attribute which controls whether a barrier can be used by threads from multiple processes or only from within the process that initialized the barrier as with the other attribute objects we have one function to get the attribute value and one function to set the value the value of the process shared attribute can be either accessible to threads from multiple processes or accessible to only threads in the process that initialized the barrier 5 reentrancy we discussed reentrant functions and signal handlers in section 10 6 threads are similar to signal handlers when it comes to reentrancy in both cases multiple threads of control can potentially call the same function at the same time basename getservent catgets getdate getutxent crypt getenv getutxid putenv getgrent getutxline pututxline getgrgid gmtime rand getgrnam hcreate readdir gethostent hdestroy setenv getlogin hsearch setgrent getnetbyaddr setkey getnetbyname setpwent getnetent lgamma setutxent getopt lgammaf strerror dirname getprotobyname lgammal strsignal dlerror getprotobynumber localeconv strtok getprotoent localtime system encrypt getpwent ttyname endgrent getpwnam unsetenv endpwent getpwuid nftw wcstombs endutxent getservbyname wctomb getservbyport ptsname figure 9 functions not guaranteed to be thread safe by posix 1 if a function can be safely called by multiple threads at the same time we say that the function is thread safe all functions defined in the single unix specification are guaranteed to be thread safe except those listed in figure 9 in addition the ctermid and tmpnam functions are not guaranteed to be thread safe if they are passed a null pointer similarly there is no guarantee that wcrtomb and wcsrtombs are thread safe when they are passed a null pointer for their argument implementations that support thread safe functions will define the symbol in unistd h applications can also use the argument with sysconf to check for support of thread safe functions at runtime prior to version 4 of the single unix specification all xsi conforming implementations were required to support thread safe functions with however thread safe function support is now required for an implementation to be considered posix conforming with thread safe functions implementations provide alternative thread safe versions of some of the posix 1 functions that aren t thread safe figure 10 lists the thread safe versions of these functions the functions have the same names as their non thread safe relatives but with an appended at the end of the name signifying that these versions are reentrant many functions are not thread safe because they return data stored in a static memory buffer they are made thread safe by changing their interfaces to require that the caller provide its own buffer readdir_r strtok_r figure 10 alternative thread safe functions if a function is reentrant with respect to multiple threads we say that it is thread safe this doesn t tell us however whether the function is reentrant with respect to signal handlers we say that a function that is safe to be reentered from an asynchronous signal handler is async signal safe we saw the async signal safe functions in figure 10 4 when we discussed reentrant functions in section 10 6 in addition to the functions listed in figure 10 posix 1 provides a way to manage file objects in a thread safe way you can use flockfile and ftrylockfile to obtain a lock associated with a given file object this lock is recursive you can acquire it again while you already hold it without deadlocking although the exact implementation of the lock is unspecified all standard i o routines that manipulate file objects are required to behave as if they call flockfile and funlockfile internally although the standard i o routines might be implemented to be thread safe from the perspective of their own internal data structures it is still useful to expose the locking to applications this allows applications to compose multiple calls to standard i o functions into atomic sequences of course when dealing with multiple file objects you need to beware of potential deadlocks and to order your locks carefully if the standard i o routines acquire their own locks then we can run into serious performance degradation when doing character at a time i o in this situation we end up acquiring and releasing a lock for every character read or written to avoid this overhead unlocked versions of the character based standard i o routines are available these four functions should not be called unless they are surrounded by calls to flockfile or ftrylockfile and funlockfile otherwise unpredictable results can occur i e the types of problems that result from unsynchronized access to data by multiple threads of control once you lock the file object you can make multiple calls to these functions before releasing the lock this amortizes the locking overhead across the amount of data read or written example figure 11 shows a possible implementation of getenv section 7 9 this version is not reentrant if two threads call it at the same time they will see inconsistent results because the string returned is stored in a single static buffer that is shared by all threads calling getenv we show a reentrant version of getenv in figure this version is called it uses the function to ensure that the function is called only once per process regardless of how many threads might race to call at the same time we ll have more to say about the function in section 12 6 to make reentrant we changed the interface so that the caller must provide its own buffer thus each thread can use a different buffer to avoid interfering with the others note however that this is not enough to make thread safe to make thread safe we need to protect against changes to the environment while we are searching for the requested string we can use a mutex to serialize access to the environment list by and putenv we could have used a reader writer lock to allow multiple concurrent calls to but the added concurrency probably wouldn t improve the performance of our program by very much for two reasons first the environment list usually isn t very long so we won t hold the mutex for too long while we scan the list second calls to getenv and putenv are infrequent so if we improve their performance we won t affect the overall performance of the program very much even though we can make thread safe that doesn t mean that it is reentrant with respect to signal handlers if we were to use a nonrecursive mutex we would run the risk that a thread would deadlock itself if it called from a signal handler if the signal handler interrupted the thread while it was executing we would already be holding locked so another attempt to lock it would block causing the thread to deadlock thus we must use a recursive mutex to prevent other threads from changing the data structures while we look at them and to prevent deadlocks from signal handlers the problem is that the pthread functions are not guaranteed to be async signal safe so we can t use them to make another function async signal safe programmers on gnu linux have two sets of input output functions at their disposal the standard c library provides i o functions printf fopen and so on the linux kernel itself provides another set of i o operations that operate at a lower level than the c library functions because this book is for people who already know the c language we ll assume that you have encountered and know how to use the c library i o functions often there are good reasons to use linux low level i o functions many of these are kernel system and provide the most direct access to underlying system capa bilities that is available to application programs in fact the standard c library i o routines are implemented on top of the linux low level i o system calls using the latter is usually the most efficient way to perform input and output operations and is sometimes more convenient too the c standard library provides iostreams with similar functionality the standard c library is also available in the c language see chapter linux system calls for an explanation of the difference between a system call and an ordinary function call throughout this book we assume that you re familiar with the calls described in this appendix you may already be familiar with them because they re nearly the same as those provided on other unix and unix like operating systems and on the platform as well if you re not familiar with them however read on you ll find the rest of the book much easier to understand if you familiarize yourself with this material first b reading and writing data the first i o function you likely encountered when you first learned the c language was printf this formats a text string and then prints it to standard output the gener alized version fprintf can print the text to a stream other than standard output a stream is represented by a file pointer you obtain a file pointer by opening a file with fopen when you re done you can close it with fclose in addition to fprintf you can use such functions as fputc fputs and fwrite to write data to the stream or fscanf fgetc fgets and fread to read data with the linux low level i o operations you use a handle called a file descriptor instead of a file pointer a file descriptor is an integer value that refers to a particu lar instance of an open file in a single process it can be open for reading for writing or for both reading and writing a file descriptor doesn t have to refer to an open file it can represent a connection with another system component that is capable of send ing or receiving data for example a connection to a hardware device is represented by a file descriptor see chapter devices as is an open socket see chapter interprocess communication section sockets or one end of a pipe see section pipes include the header files fcntl h sys types h sys stat h and unistd h if you use any of the low level i o functions described here b opening a file to open a file and produce a file descriptor that can access that file use the open call it takes as arguments the path name of the file to open as a character string and flags specifying how to open it you can use open to create a new file if you do pass a third argument that specifies the access permissions to set for the new file if the second argument is the file is opened for reading only an error will result if you subsequently try to write to the resulting file descriptor similarly causes the file descriptor to be write only specifying produces a file descriptor that can be used both for reading and for writing note that not all files may be opened in all three modes for instance the permissions on a file might forbid a particular process from opening it for reading or for writing a file on a read only device such as a cd rom drive may not be opened for writing you can specify additional options by using the bitwise or of this value with one or more flags these are the most commonly used values n specify to truncate the opened file if it previously existed data written to the file descriptor will replace previous contents of the file n specify to append to an existing file data written to the file descriptor will be added to the end of the file n specify to create a new file if the filename that you provide to open does not exist a new file will be created provided that the directory containing it exists and that the process has permission to create files in that directory if the file already exists it is opened instead n specify with to force creation of a new file if the file already exists the open call will fail if you call open with provide an additional third argument specifying the per missions for the new file see chapter security section file system permissions for a description of permission bits and how to use them for example the program in listing b creates a new file with the filename speci fied on the command line it uses the flag with open so if the file already exists an error occurs the new file is given read and write permissions for the owner and owning group and read permissions only for others if your umask is set to a nonzero value the actual permissions may be more restrictive umasks when you create a new file with open some permission bits that you specify may be turned off this is because your umask is set to a nonzero value a process umask specifies bits that are masked out of all newly created files permissions the actual permissions used are the bitwise and of the permissions you specify to open and the bitwise complement of the umask to change your umask from the shell use the umask command and specify the numerical value of the mask in octal notation to change the umask for a running process use the umask call passing it the desired mask value to use for subsequent open calls for example calling this line umask in a program or invoking this command umask specifies that write permissions for group members and read write and execute permissions for others will always be masked out of a new file permissions listing b create file c create a new file include fcntl h include stdio h include sys stat h include sys types h include unistd h int main int argc char argv the path at which to create the new file char path argv the permissions for the new file mode s_iwgrp create the file int fd open path mode if fd an error occurred print an error message and bail perror open return return here the program in action create file testfile ls l testfile rw rw r samuel users feb testfile create file testfile open file exists note that the length of the new file is because the program didn t write any data to it b closing file descriptors when you re done with a file descriptor close it with close in some cases such as the program in listing b it not necessary to call close explicitly because linux closes all open file descriptors when a process terminates that is when the program ends of course once you close a file descriptor you should no longer use it closing a file descriptor may cause linux to take a particular action depending on the nature of the file descriptor for example when you close a file descriptor for a network socket linux closes the network connection between the two computers communicating through the socket linux limits the number of open file descriptors that a process may have open at a time open file descriptors use kernel resources so it good to close file descriptors when you re done with them a typical limit is file descriptors per process you can adjust this limit with the setrlimit system call see section getrlimit and setrlimit resource limits for more information b writing data write data to a file descriptor using the write call provide the file descriptor a pointer to a buffer of data and the number of bytes to write the file descriptor must be open for writing the data written to the file need not be a character string write copies arbitrary bytes from the buffer to the file descriptor the program in listing b appends the current time to the file specified on the command line if the file doesn t exist it is created this program also uses the time localtime and asctime functions to obtain and format the current time see their respective man pages for more information listing b timestamp c append a timestamp to a file include fcntl h include stdio h include string h include sys stat h include sys types h include time h include unistd h return a character string representing the current date and time char now time null return asctime localtime now int main int argc char argv the file to which to append the timestamp char filename argv get the current timestamp char timestamp open the file for writing if it exists append to it otherwise create a new file int fd open filename 0666 compute the length of the timestamp string length strlen timestamp write the timestamp to the file write fd timestamp length all done close fd return here how the timestamp program works timestamp tsfile cat tsfile thu feb timestamp tsfile cat tsfile thu feb thu feb note that the first time we invoke timestamp it creates the file tsfile while the second time it appends to it the write call returns the number of bytes that were actually written or if an error occurred for certain kinds of file descriptors the number of bytes actually writ ten may be less than the number of bytes requested in this case it up to you to call write again to write the rest of the data the function in listing b demonstrates how you might do this note that for some applications you may have to check for special conditions in the middle of the writing operation for example if you re writ ing to a network socket you ll have to augment this function to detect whether the network connection was closed in the middle of the write operation and if it has to react appropriately d online resources his appendix lists some places to visit on the internet to learn more about programming for the gnu linux system d general information n http www advancedlinuxprogramming com is this book home on the internet here you can download the full text of this book and program source code find links to other online resources and get more information about pro gramming gnu linux the same information can also be found at http www newriders com n http www linuxdoc org is the home of the linux documentation project this site is a repository for a wealth of documentation faq lists howtos and other documentation about gnu linux systems and software appendix d online resources d information about gnu linux software n http www gnu org is the home of the gnu project from this site you can download a staggering array of sophisticated free software applications among them is the gnu c library which is part of every gnu linux system and contains many of the functions described in this book the gnu project site also provides information about how you can contribute to the development of the gnu linux system by writing code or documentation by using free soft ware and by spreading the free software message n http www kernel org is the primary site for distribution of the linux kernel source code for the trickiest and most technically detailed questions about how linux works the source code is the best place to look see also the documentation directory for explanation of the kernel internals n http www linuxhq com also distributes linux kernel sources patches and related information n http gcc gnu org is the home of the gnu compiler collection gcc gcc is the primary compiler used on gnu linux systems and it includes compilers for c c objective c java chill and fortran n http www gnome org and http www kde org are the homes of the two most popular gnu linux windowing environments gnome and kde if you plan to write an application with a graphical user interface you should familiarize yourself with either or both d other sites n http developer intel com provides information about intel processor archi tectures including the architecture if you are developing for linux and you use inline assembly instructions the technical manuals available here will be very useful n http www amd com devconn provides similar information about amd line of microprocessors and its special features n http freshmeat net is an index of open source software generally for gnu linux this site is one of the best places to stay abreast of the newest releases of gnu linux software from core system components to more obscure specialized applications m http www linuxsecurity com contains information techniques and links to software related to gnu linux security the site is of interest to users system administrators and developers f gnu general public version june copyright free software foundation inc temple place suite boston ma usa everyone is permitted to copy and distribute verbatim copies of this license docu ment but changing it is not allowed preamble the licenses for most software are designed to take away your freedom to share and change it by contrast the gnu general public license is intended to guarantee your freedom to share and change free software to make sure the software is free for all its users this general public license applies to most of the free software foundation software and to any other program whose authors commit to using it some other free software foundation software is covered by the gnu library general public license instead you can apply it to your programs too when we speak of free software we are referring to freedom not price our general public licenses are designed to make sure that you have the freedom to distribute copies of free software and charge for this service if you wish that you receive source code or can get it if you want it that you can change the software or use pieces of it in new free programs and that you know you can do these things this license can also be found online at http www gnu org copyleft gpl html to protect your rights we need to make restrictions that forbid anyone to deny you these rights or to ask you to surrender the rights these restrictions translate to certain responsibilities for you if you distribute copies of the software or if you modify it for example if you distribute copies of such a program whether gratis or for a fee you must give the recipients all the rights that you have you must make sure that they too receive or can get the source code and you must show them these terms so they know their rights we protect your rights with two steps copyright the software and offer you this license which gives you legal permission to copy distribute and or modify the software also for each author protection and ours we want to make certain that everyone understands that there is no warranty for this free software if the software is modified by someone else and passed on we want its recipients to know that what they have is not the original so that any problems introduced by others will not reflect on the original authors reputations finally any free program is threatened constantly by software patents we wish to avoid the danger that redistributors of a free program will individually obtain patent licenses in effect making the program proprietary to prevent this we have made it clear that any patent must be licensed for everyone free use or not licensed at all the precise terms and conditions for copying distribution and modification follow terms and conditions for copying distribution and modification this license applies to any program or other work which contains a notice placed by the copyright holder saying it may be distributed under the terms of this general public license the program below refers to any such program or work and a work based on the program means either the program or any derivative work under copyright law that is to say a work containing the program or a portion of it either verbatim or with modifications and or trans lated into another language hereinafter translation is included without limita tion in the term modification each licensee is addressed as you activities other than copying distribution and modification are not covered by this license they are outside its scope the act of running the program is not restricted and the output from the program is covered only if its contents constitute a work based on the program independent of having been made by running the program whether that is true depends on what the program does you may copy and distribute verbatim copies of the program source code as you receive it in any medium provided that you conspicuously and appropri ately publish on each copy an appropriate copyright notice and disclaimer of warranty keep intact all the notices that refer to this license and to the absence of any warranty and give any other recipients of the program a copy of this license along with the program you may charge a fee for the physical act of transferring a copy and you may at your option offer warranty protection in exchange for a fee you may modify your copy or copies of the program or any portion of it thus forming a work based on the program and copy and distribute such modifica tions or work under the terms of section above provided that you also meet all of these conditions n a you must cause the modified files to carry prominent notices stating that you changed the files and the date of any change n b you must cause any work that you distribute or publish that in whole or in part contains or is derived from the program or any part thereof to be licensed as a whole at no charge to all third parties under the terms of this license n c if the modified program normally reads commands interactively when run you must cause it when started running for such interactive use in the most ordinary way to print or display an announcement including an appropriate copyright notice and a notice that there is no warranty or else saying that you provide a warranty and that users may redistribute the program under these conditions and telling the user how to view a copy of this license exception if the program itself is interactive but does not normally print such an announcement your work based on the program is not required to print an announcement these requirements apply to the modified work as a whole if identifiable sec tions of that work are not derived from the program and can be reasonably considered independent and separate works in themselves then this license and its terms do not apply to those sections when you distribute them as separate works but when you distribute the same sections as part of a whole which is a work based on the program the distribution of the whole must be on the terms of this license whose permissions for other licensees extend to the entire whole and thus to each and every part regardless of who wrote it thus it is not the intent of this section to claim rights or contest your rights to work written entirely by you rather the intent is to exercise the right to control the distribution of derivative or collective works based on the program in addition mere aggregation of another work not based on the program with the program or with a work based on the program on a volume of a storage or distribution medium does not bring the other work under the scope of this license you may copy and distribute the program or a work based on it under section in object code or executable form under the terms of sections and above provided that you also do one of the following n a accompany it with the complete corresponding machine readable source code which must be distributed under the terms of sections and above on a medium customarily used for software interchange or n b accompany it with a written offer valid for at least three years to give any third party for a charge no more than your cost of physically perform ing source distribution a complete machine readable copy of the corre sponding source code to be distributed under the terms of sections and above on a medium customarily used for software interchange or n c accompany it with the information you received as to the offer to dis tribute corresponding source code this alternative is allowed only for noncommercial distribution and only if you received the program in object code or executable form with such an offer in accord with subsection b above the source code for a work means the preferred form of the work for making modifications to it for an executable work complete source code means all the source code for all modules it contains plus any associated interface definition files plus the scripts used to control compilation and installation of the exe cutable however as a special exception the source code distributed need not include anything that is normally distributed in either source or binary form with the major components compiler kernel and so on of the operating sys tem on which the executable runs unless that component itself accompanies the executable if distribution of executable or object code is made by offering access to copy from a designated place then offering equivalent access to copy the source code from the same place counts as distribution of the source code even though third parties are not compelled to copy the source along with the object code you may not copy modify sublicense or distribute the program except as expressly provided under this license any attempt otherwise to copy modify sublicense or distribute the program is void and will automatically terminate your rights under this license however parties who have received copies or rights from you under this license will not have their licenses terminated so long as such parties remain in full compliance you are not required to accept this license since you have not signed it however nothing else grants you permission to modify or distribute the program or its derivative works these actions are prohibited by law if you do not accept this license therefore by modifying or distributing the program or any work based on the program you indicate your acceptance of this license to do so and all its terms and conditions for copying distributing or modifying the program or works based on it each time you redistribute the program or any work based on the program the recipient automatically receives a license from the original licensor to copy distribute or modify the program subject to these terms and conditions you may not impose any further restrictions on the recipients exercise of the rights granted herein you are not responsible for enforcing compliance by third parties to this license if as a consequence of a court judgment or allegation of patent infringement or for any other reason not limited to patent issues conditions are imposed on you whether by court order agreement or otherwise that contradict the condi tions of this license they do not excuse you from the conditions of this license if you cannot distribute so as to satisfy simultaneously your obligations under this license and any other pertinent obligations then as a consequence you may not distribute the program at all for example if a patent license would not per mit royalty free redistribution of the program by all those who receive copies directly or indirectly through you then the only way you could satisfy both it and this license would be to refrain entirely from distribution of the program if any portion of this section is held invalid or unenforceable under any particu lar circumstance the balance of the section is intended to apply and the section as a whole is intended to apply in other circumstances it is not the purpose of this section to induce you to infringe any patents or other property right claims or to contest validity of any such claims this section has the sole purpose of protecting the integrity of the free software distribution system which is implemented by public license practices many people have made generous contributions to the wide range of software distributed through that system in reliance on consistent application of that system it is up to the author donor to decide if he or she is willing to distribute software through any other system and a licensee cannot impose that choice this section is intended to make thoroughly clear what is believed to be a con sequence of the rest of this license if the distribution and or use of the program is restricted in certain countries either by patents or by copyrighted interfaces the original copyright holder who places the program under this license may add an explicit geographical distribu tion limitation excluding those countries so that distribution is permitted only in or among countries not thus excluded in such case this license incorporates the limitation as if written in the body of this license the free software foundation may publish revised and or new versions of the general public license from time to time such new versions will be similar in spirit to the present version but may differ in detail to address new problems or concerns each version is given a distinguishing version number if the program specifies a version number of this license which applies to it and any later version you have the option of following the terms and conditions either of that version or of any later version published by the free software foundation if the program does not specify a version number of this license you may choose any version ever published by the free software foundation if you wish to incorporate parts of the program into other free programs whose distribution conditions are different write to the author to ask for permission for software which is copyrighted by the free software foundation write to the free software foundation we sometimes make exceptions for this our decision will be guided by the two goals of preserving the free status of all derivatives of our free software and of promoting the sharing and reuse of soft ware generally no warranty because the program is licensed free of charge there is no warranty for the program to the extent permit ted by applicable law except when otherwise stated in writing the copyright holders and or other parties provide the program as is without warranty of any kind either expressed or implied including but not limited to the implied warranties of merchantability and fitness for a particular purpose the entire risk as to the quality and performance of the program is with you should the program prove defective you assume the cost of all necessary servicing repair or correction in no event unless required by applicable law or agreed to in writing will any copyright holder or any other party who may modify and or redistribute the program as permitted above be liable to you for damages including any general special incidental or consequential damages arising out of the use or inability to use the program including but not lim ited to loss of data or data being rendered inaccu rate or losses sustained by you or third parties or a failure of the program to operate with any other programs even if such holder or other party has been advised of the possibility of such damages how to apply these terms to your new programs end of terms and conditions how to apply these terms to your new programs if you develop a new program and you want it to be of the greatest possible use to the public the best way to achieve this is to make it free software which everyone can redistribute and change under these terms to do so attach the following notices to the program it is safest to attach them to the start of each source file to most effectively convey the exclusion of warranty and each file should have at least the copyright line and a pointer to where the full notice is found one line to give the program name and an idea of what it does copyright yyyy name of author this program is free software you can redistribute it and or modify it under the terms of the gnu general public license as published by the free software foundation either version of the license or at your option any later version this program is distributed in the hope that it will be useful but without any warranty without even the implied warranty of merchantability or fitness for a particular purpose see the gnu general public license for more details you should have received a copy of the gnu general public license along with this program if not write to the free software foundation inc temple place suite boston ma usa also add information on how to contact you by electronic and paper mail if the program is interactive make it output a short notice like this when it starts in an interactive mode gnomovision version copyright year name of author gnomovision comes with absolutely no warranty for details type show w this is free software and you are welcome to redistribute it under certain conditions type show c for details the hypothetical commands show w and show c should show the appropriate parts of the general public license of course the commands you use may be called some thing other than show w and show c they could even be mouse clicks or menu items whatever suits your program you should also get your employer if you work as a programmer or your school if any to sign a copyright disclaimer for the program if necessary here is a sample alter the names yoyodyne inc hereby disclaims all copyright interest in the program gnomovision which makes passes at compilers written by james hacker signature of ty coon april ty coon president of vice this general public license does not permit incorporating your program into propri etary programs if your program is a subroutine library you may consider it more use ful to permit linking proprietary applications with the library if this is what you want to do use the gnu library general public license instead of this license fsf gnu inquiries questions to gnu gnu org comments on these web pages to webmasters www gnu org send other questions to gnu gnu org copyright notice above free software foundation inc temple place suite boston ma usa updated jul jonas when writing a program you frequently can t know how much memory the program will need when it runs for example a line read from a file at runtime might have any finite length c and c programs use malloc free and their variants to dynamically allocate memory while the program is running the rules for dynamic memory use include these n the number of allocation calls calls to malloc must exactly match the number of deallocation calls calls to free n reads and writes to the allocated memory must occur within the memory not outside its range n the allocated memory cannot be used before it is allocated or after it is deallocated because dynamic memory allocation and deallocation occur at runtime static program analysis rarely find violations instead memory checking tools run the program col lecting data to determine if any of these rules have been violated the violations a tool may find include the following n reading from memory before allocating it n writing to memory before allocating it n reading before the beginning of allocated memory n writing before the beginning of allocated memory n reading after the end of allocated memory n writing after the end of allocated memory n reading from memory after its deallocation n writing to memory after its deallocation n failing to deallocate allocated memory n deallocating the same memory twice n deallocating memory that is not allocated it is also useful to warn about requesting an allocation with bytes which probably indicates programmer error table a indicates four different tools diagnostic capabilities unfortunately no single tool diagnoses all the memory use errors also no tool claims to detect reading or writing before allocating memory but doing so will probably cause a segmentation fault deallocating memory twice will probably also cause a segmentation fault these tools diagnose only errors that actually occur while the program is running if you run the program with inputs that cause no memory to be allocated the tools will indicate no memory errors to test a program thoroughly you must run the program using dif ferent inputs to ensure that every possible path through the program occurs also you may use only one tool at a time so you ll have to repeat testing with several tools to get the best error checking table a capabilities of dynamic memory checking tools x indicates detection and o indicates detection for some cases erroneous behavior malloc mtrace ccmalloc electric checking fence read before allocating memory write before allocating memory read before beginning of allocation x write before beginning of allocation o o x read after end of allocation x write after end of allocation x x read after deallocation x write after deallocation x failure to deallocate memory x x deallocating memory twice x x deallocating nonallocated memory x x zero size memory allocation x x in the sections that follow we first describe how to use the more easily used malloc checking and mtrace and then ccmalloc and electric fence a a program to test memory allocation and deallocation we ll use the malloc use program in listing a to illustrate memory allocation deal location and use to begin running it specify the maximum number of allocated memory regions as its only command line argument for example malloc use creates an array a with character pointers that do not point to anything the program accepts five different commands n to allocate b bytes pointed to by array entry a i enter a i b the array index i can be any non negative number smaller than the command line argument the number of bytes must be non negative n to deallocate memory at array index i enter d i n to read the pth character from the allocated memory at index i as in a i p enter r i p here p can have an integral value n to write a character to the pth position in the allocated memory at index i enter w i p n when finished enter q we ll present the program code later in section a and illustrate how to use it a malloc checking the memory allocation functions provided by the gnu c library can detect writing before the beginning of an allocation and deallocating the same allocation twice defining the environment variable to the value causes a program to halt when such an error is detected note the environment variable ending under score there is no need to recompile the program we illustrate diagnosing a write to memory to a position just before the beginning of an allocation export malloc use please enter a command a please enter a command w please enter a command d aborted core dumped export turns on malloc checking specifying the value causes the program to halt as soon as an error is detected using malloc checking is advantageous because the program need not be recom piled but its capability to diagnose errors is limited basically it checks that the alloca tor data structures have not been corrupted thus it can detect double deallocation of the same allocation also writing just before the beginning of a memory allocation can usually be detected because the allocator stores the size of each memory allocation just before the allocated region thus writing just before the allocated memory will corrupt this number unfortunately consistency checking can occur only when your program calls allocation routines not when it accesses memory so many illegal reads and writes can occur before an error is detected in the previous example the illegal write was detected only when the allocated memory was deallocated a finding memory leaks using mtrace the mtrace tool helps diagnose the most common error when using dynamic memory failure to match allocations and deallocations there are four steps to using mtrace which is available with the gnu c library modify the source code to include mcheck h and to invoke mtrace as soon as the program starts at the beginning of main the call to mtrace turns on tracking of memory allocations and deallocations specify the name of a file to store information about all memory allocations and deallocations export memory log run the program all memory allocations and deallocations are stored in the logging file using the mtrace command analyze the memory allocations and deallocations to ensure that they match mtrace the messages produced by mtrace are relatively easy to understand for example for our malloc use example the output would look like this free was never alloc d malloc use c memory not freed address size caller at malloc use c these messages indicate an attempt on line of malloc use c to free memory that was never allocated and an allocation of memory on line that was never freed mtrace diagnoses errors by having the executable record all memory allocations and deallocations in the file specified by the environment variable the executable must terminate normally for the data to be written the mtrace command analyzes this file and lists unmatched allocations and deallocations a using ccmalloc the ccmalloc library diagnoses dynamic memory errors by replacing malloc and free with code tracing their use if the program terminates gracefully it produces a report of memory leaks and other errors the ccmalloc library was written by armin bierce you ll probably have to download and install the ccmalloc library yourself download it from http www inf ethz ch personal biere projects ccmalloc unpack the code and run configure run make and make install copy the ccmalloc cfg file to the directory where you ll run the program you want to check and rename the copy to ccmalloc now you are ready to use the tool the program object files must be linked with ccmalloc library and the dynamic linking library append lccmalloc ldl to your link command for instance gcc g wall pedantic malloc use o o ccmalloc use lccmalloc ldl execute the program to produce a report for example running our malloc use pro gram to allocate but not deallocate memory produces the following report ccmalloc use file name a out does not contain valid symbols trying to find executable in current directory using symbols from ccmalloc use to speed up this search specify file ccmalloc use in the startup file ccmalloc please enter a command a please enter a command q ccmalloc report total of allocated deallocated garbage bytes allocations number of checks number of counts retrieving function names for addresses done reading file info from gdb done sorting by number of not reclaimed bytes done number of call chains number of ignored call chains number of reported call chains number of internal call chains number of library call chains bytes of garbage allocated in allocation in in main at malloc use c in allocate at malloc use c in malloc at src wrapper c the last few lines indicate the chain of function calls that allocated memory that was not deallocated to use ccmalloc to diagnose writes before the beginning or after the end of the allocated region you ll have to modify the ccmalloc file in the current directory this file is read when the program starts execution a electric fence written by bruce perens electric fence halts executing programs on the exact line where a write or a read outside an allocation occurs this is the only tool that discovers illegal reads it is included in most gnu linux distributions but the source code can be found at http www perens com freesoftware as with ccmalloc your program object files must be linked with electric fence library by appending lefence to the linking command for instance gcc g wall pedantic malloc use o o emalloc use lefence as the program runs allocated memory uses are checked for correctness a violation causes a segmentation fault emalloc use electric fence copyright c bruce perens please enter a command a please enter a command r segmentation fault using a debugger you can determine the context of the illegal action by default electric fence diagnoses only accesses beyond the ends of allocations to find accesses before the beginning of allocations instead of accesses beyond the end of allocations use this code export to find accesses to deallocated memory set to more capabilities are described in the libefence manual page electric fence diagnoses illegal memory accesses by storing each allocation on at least two memory pages it places the allocation at the end of the first page any access beyond the end of the allocation on the second page causes a segmentation fault if you set to it places the allocation at the beginning of the second page instead because it allocates two memory pages per call to malloc electric fence can use an enormous amount of memory use this library for debugging only a choosing among the different memory debugging tools we have discussed four separate incompatible tools to diagnose erroneous use of dynamic memory how does a gnu linux programmer ensure that dynamic mem ory is correctly used no tool guarantees diagnosing all errors but using any of them does increase the probability of finding errors to ease finding dynamically allocated memory errors separately develop and test the code that deals with dynamic memory this reduces the amount of code that you must search for errors if you are using c write a class that handles all dynamic memory use if you are using c minimize the number of functions using allocation and deallocation when testing this code be sure to use only one tool at a one time because they are incompatible when testing a program be sure to vary how the program executes to test the most commonly exe cuted portions of the code which of the four tools should you use because failing to match allocations and deallocations is the most common dynamic memory error use mtrace during initial development the program is available on all gnu linux systems and has been well tested after ensuring that the number of allocations and deallocations match use electric fence to find illegal memory accesses this will eliminate almost all memory errors when using electric fence you will need to be careful to not perform too many allocations and deallocations because each allocation requires at least two pages of memory using these two tools will reveal most memory errors a source code for the dynamic memory program listing a shows the source code for a program illustrating dynamic memory alloca tion deallocation and use see section a a program to test memory allocation and deallocation for a description of how to use it listing a malloc use c dynamic memory allocation checking example use c dynamic memory allocation functions invoke the program using one command line argument specifying the size of an array this array consists of pointers to possibly allocated arrays when the programming is running select among the following commands o allocate memory a index memory size o deallocate memory d index o read from memory r index position within allocation o write to memory w index position within allocation o quit q the user is responsible for obeying or disobeying the rules on dynamic memory use ifdef mtrace include mcheck h endif mtrace include stdio h include stdlib h include assert h allocate memory with the specified size returning nonzero upon success void allocate char array size array malloc size deallocate memory void deallocate char array continues listing a continued free void array read from a position in memory void char array int position char character array position write to a position in memory void char array int position array position a int main int argc char argv char array unsigned char command unsigned char int int error ifdef mtrace mtrace endif mtrace if argc fprintf stderr array size n argv return strtoul argv array char calloc sizeof char assert array follow the user commands while error printf please enter a command getchar assert eof switch case a fgets command sizeof command stdin if sscanf command u i allocate array else error break case d fgets command sizeof command stdin if sscanf command u deallocate array else error break case r fgets command sizeof command stdin if sscanf command u i array_size array else error break case w fgets command sizeof command stdin if sscanf command u i size_or_position array_size array size_or_position else error break case q free void array return default error free void array return a profiling now that your program is hopefully correct we turn to speeding its execution using the profiler gprof you can determine which functions require the most execu tion time this can help you determine which parts of the program to optimize or rewrite to execute more quickly it can also help you find errors for example you may find that a particular function is called many more times than you expect in this section we describe how to use gprof rewriting code to run more quickly requires creativity and careful choice of algorithms obtaining profiling information requires three steps compile and link your program to enable profiling execute your program to generate profiling data use gprof to analyze and display the profiling data before we illustrate these steps we introduce a large enough program to make profiling interesting a a simple calculator to illustrate profiling we ll use a simple calculator program to ensure that the calcula tor takes a nontrivial amount of time we ll use unary numbers for calculations some thing we would definitely not want to do in a real world program code for this program appears at the end of this chapter a unary number is represented by as many symbols as its value for example the number is represented by x by xx and by xxx instead of using x our program represents a non negative number using a linked list with as many elements as the number value the number c file contains routines to create the number add to a number subtract from a number and add subtract and multiply numbers another function converts a string holding a non negative decimal number to a unary number and a function converts from a unary number to an int addition is imple mented using repeated addition of while subtraction uses repeated removal of multiplication is defined using repeated addition the unary predicates even and odd each return the unary number for if and only if its one operand is even or odd respectively otherwise they return the unary number for the two predicates are mutually recursive for example a number is even if it is zero or if one less than the number is odd thus far we have used kmalloc and kfree for the allocation and freeing of memory the linux kernel offers a richer set of memory allocation primitives however in this chapter we look at other ways of using memory in device drivers and how to opti mize your system memory resources we do not get into how the different architec tures actually administer memory modules are not involved in issues of segmentation paging and so on since the kernel offers a unified memory manage ment interface to the drivers in addition we won t describe the internal details of memory management in this chapter but defer it to chapter the real story of kmalloc the kmalloc allocation engine is a powerful tool and easily learned because of its similarity to malloc the function is fast unless it blocks and doesn t clear the mem ory it obtains the allocated region still holds its previous content the allocated region is also contiguous in physical memory in the next few sections we talk in detail about kmalloc so you can compare it with the memory allocation techniques that we discuss later the flags argument remember that the prototype for kmalloc is include linux slab h void kmalloc size int flags among other things this implies that you should explicitly clear any memory that might be exposed to user space or written to a device otherwise you risk disclosing information that should be kept private the first argument to kmalloc is the size of the block to be allocated the second argument the allocation flags is much more interesting because it controls the behavior of kmalloc in a number of ways the most commonly used flag means that the allocation internally per formed by calling eventually which is the source of the pre fix is performed on behalf of a process running in kernel space in other words this means that the calling function is executing a system call on behalf of a process using means that kmalloc can put the current process to sleep waiting for a page when called in low memory situations a function that allocates memory using must therefore be reentrant and cannot be running in atomic con text while the current process sleeps the kernel takes proper action to locate some free memory either by flushing buffers to disk or by swapping out memory from a user process isn t always the right allocation flag to use sometimes kmalloc is called from outside a process context this type of call can happen for instance in inter rupt handlers tasklets and kernel timers in this case the current process should not be put to sleep and the driver should use a flag of instead the ker nel normally tries to keep some free pages around in order to fulfill atomic alloca tion when is used kmalloc can use even the last free page if that last page does not exist however the allocation fails other flags can be used in place of or in addition to and although those two cover most of the needs of device drivers all the flags are defined in linux gfp h and individual flags are prefixed with a double underscore such as in addition there are symbols that represent frequently used combina tions of flags these lack the prefix and are sometimes called allocation priorities the latter include used to allocate memory from interrupt handlers and other code outside of a process context never sleeps normal allocation of kernel memory may sleep used to allocate memory for user space pages it may sleep like but allocates from high memory if any high memory is described in the next subsection these flags function like but they add restrictions on what the ker nel can do to satisfy the request a allocation is not allowed to perform any filesystem calls while disallows the initiation of any i o at all they are used primarily in the filesystem and virtual memory code where an allo cation may be allowed to sleep but recursive filesystem calls would be a bad idea the allocation flags listed above can be augmented by an oring in any of the follow ing flags which change how the allocation is carried out this flag requests allocation to happen in the dma capable memory zone the exact meaning is platform dependent and is explained in the following section this flag indicates that the allocated memory may be located in high memory normally the memory allocator tries to return cache warm pages pages that are likely to be found in the processor cache instead this flag requests a cold page which has not been used in some time it is useful for allocating pages for dma reads where presence in the processor cache is not useful see the section direct memory access in chapter for a full discussion of how to allocate dma buffers this rarely used flag prevents the kernel from issuing warnings with printk when an allocation cannot be satisfied this flag marks a high priority request which is allowed to consume even the last pages of memory set aside by the kernel for emergencies these flags modify how the allocator behaves when it has difficulty satisfying an allocation means try a little harder by repeating the attempt but the allocation can still fail the flag tells the allocator never to fail it works as hard as needed to satisfy the request use of is very strongly discouraged there will probably never be a valid reason to use it in a device driver finally tells the allocator to give up immediately if the requested memory is not available memory zones both and have a platform dependent role although their use is valid for all platforms the linux kernel knows about a minimum of three memory zones dma capable memory normal memory and high memory while allocation normally happens in the normal zone setting either of the bits just mentioned requires memory to be allo cated from a different zone the idea is that every computer platform that must know about special memory ranges instead of considering all ram equivalents will fall into this abstraction dma capable memory is memory that lives in a preferential address range where peripherals can perform dma access on most sane platforms all memory lives in this zone on the the dma zone is used for the first mb of ram where leg acy isa devices can perform dma pci devices have no such limit high memory is a mechanism used to allow access to relatively large amounts of memory on bit platforms this memory cannot be directly accessed from the ker nel without first setting up a special mapping and is generally harder to work with if your driver uses large amounts of memory however it will work better on large sys tems if it can use high memory see the section high and low memory in chapter for a detailed description of how high memory works and how to use it whenever a new page is allocated to fulfill a memory allocation request the kernel builds a list of zones that can be used in the search if is specified only the dma zone is searched if no memory is available at low addresses allocation fails if no special flag is present both normal and dma memory are searched if is set all three zones are used to search a free page note however that kmalloc cannot allocate high memory the situation is more complicated on nonuniform memory access numa systems as a general rule the allocator attempts to locate memory local to the processor per forming the allocation although there are ways of changing that behavior the mechanism behind memory zones is implemented in mm c while ini tialization of the zone resides in platform specific files usually in mm init c within the arch tree we ll revisit these topics in chapter the size argument the kernel manages the system physical memory which is available only in page sized chunks as a result kmalloc looks rather different from a typical user space malloc implementation a simple heap oriented allocation technique would quickly run into trouble it would have a hard time working around the page boundaries thus the kernel uses a special page oriented allocation technique to get the best use from the system ram linux handles memory allocation by creating a set of pools of memory objects of fixed sizes allocation requests are handled by going to a pool that holds sufficiently large objects and handing an entire memory chunk back to the requester the mem ory management scheme is quite complex and the details of it are not normally all that interesting to device driver writers the one thing driver developers should keep in mind though is that the kernel can allocate only certain predefined fixed size byte arrays if you ask for an arbitrary amount of memory you re likely to get slightly more than you asked for up to twice as much also programmers should remember that the smallest allocation that kmalloc can handle is as big as or bytes depending on the page size used by the system architecture there is an upper limit to the size of memory chunks that can be allocated by kmal loc that limit varies depending on architecture and kernel configuration options if your code is to be completely portable it cannot count on being able to allocate any thing larger than kb if you need more than a few kilobytes however there are better ways than kmalloc to obtain memory which we describe later in this chapter lookaside caches a device driver often ends up allocating many objects of the same size over and over given that the kernel already maintains a set of memory pools of objects that are all the same size why not add some special pools for these high volume objects in fact the kernel does implement a facility to create this sort of pool which is often called a lookaside cache device drivers normally do not exhibit the sort of memory behavior that justifies using a lookaside cache but there can be exceptions the usb and scsi drivers in linux use caches the cache manager in the linux kernel is sometimes called the slab allocator for that reason its functions and types are declared in linux slab h the slab allocator implements caches that have a type of they are created with a call to const char name size offset unsigned long flags void constructor void unsigned long flags void destructor void unsigned long flags the function creates a new cache object that can host any number of memory areas all of the same size specified by the size argument the name argument is associated with this cache and functions as housekeeping information usable in tracking prob lems usually it is set to the name of the type of structure that is cached the cache keeps a pointer to the name rather than copying it so the driver should pass in a pointer to a name in static storage usually the name is just a literal string the name cannot contain blanks the offset is the offset of the first object in the page it can be used to ensure a par ticular alignment for the allocated objects but you most likely will use to request the default value flags controls how allocation is done and is a bit mask of the fol lowing flags setting this flag protects the cache from being reduced when the system is look ing for memory setting this flag is normally a bad idea it is important to avoid restricting the memory allocator freedom of action unnecessarily this flag requires each data object to be aligned to a cache line actual alignment depends on the cache layout of the host platform this option can be a good choice if your cache contains items that are frequently accessed on smp machines the padding required to achieve cache line alignment can end up wasting significant amounts of memory however this flag requires each data object to be allocated in the dma memory zone there is also a set of flags that can be used during the debugging of cache alloca tions see mm slab c for the details usually however these flags are set globally via a kernel configuration option on systems used for development the constructor and destructor arguments to the function are optional functions but there can be no destructor without a constructor the former can be used to ini tialize newly allocated objects and the latter can be used to clean up objects prior to their memory being released back to the system as a whole constructors and destructors can be useful but there are a few constraints that you should keep in mind a constructor is called when the memory for a set of objects is allocated because that memory may hold several objects the constructor may be called multiple times you cannot assume that the constructor will be called as an immediate effect of allocating an object similarly destructors can be called at some unknown future time not immediately after an object has been freed constructors and destructors may or may not be allowed to sleep according to whether they are passed the flag where ctor is short for constructor for convenience a programmer can use the same function for both the constructor and destructor the slab allocator always passes the flag when the callee is a constructor once a cache of objects is created you can allocate objects from it by calling void cache int flags here the cache argument is the cache you have created previously the flags are the same as you would pass to kmalloc and are consulted if needs to go out and allocate more memory itself to free an object use void cache const void obj when driver code is finished with the cache typically when the module is unloaded it should free its cache as follows int cache the destroy operation succeeds only if all objects allocated from the cache have been returned to it therefore a module should check the return status from a failure indicates some sort of memory leak within the mod ule since some of the objects have been dropped one side benefit to using lookaside caches is that the kernel maintains statistics on cache usage these statistics may be obtained from proc slabinfo a scull based on the slab caches scullc time for an example scullc is a cut down version of the scull module that imple ments only the bare device the persistent memory region unlike scull which uses kmalloc scullc uses memory caches the size of the quantum can be modified at compile time and at load time but not at runtime that would require creating a new memory cache and we didn t want to deal with these unneeded details scullc is a complete example that can be used to try out the slab allocator it differs from scull only in a few lines of code first we must declare our own slab cache declare one cache pointer use it for all devices the creation of the slab cache is handled at module load time in this way create a cache for our quanta scullc null null no ctor dtor if return enomem this is how it allocates memory quanta allocate a quantum using the memory cache if dptr data dptr data if dptr data goto nomem memset dptr data and these lines release memory for i i qset i if dptr data i dptr data i finally at module unload time we have to return the cache to the system release the cache of our quanta if scullc_cache the main differences in passing from scull to scullc are a slight speed improvement and better memory use since quanta are allocated from a pool of memory fragments of exactly the right size their placement in memory is as dense as possible as opposed to scull quanta which bring in an unpredictable memory fragmentation memory pools there are places in the kernel where memory allocations cannot be allowed to fail as a way of guaranteeing allocations in those situations the kernel developers cre ated an abstraction known as a memory pool or mempool a memory pool is really just a form of a lookaside cache that tries to always keep a list of free memory around for use in emergencies a memory pool has a type of defined in linux mempool h you can cre ate one with int void the argument is the minimum number of allocated objects that the pool should always keep around the actual allocation and freeing of objects is handled by and which have these prototypes typedef void int void typedef void void element void the final parameter to is passed to and if need be you can write special purpose functions to handle memory allocations for mempools usually however you just want to let the kernel slab allocator handle that task for you there are two functions and that perform the impedance matching between the memory pool allocation proto types and and thus code that sets up memory pools often looks like the following cache pool mempool_free_slab cache once the pool has been created objects can be allocated and freed with void pool int void void element pool when the mempool is created the allocation function will be called enough times to create a pool of preallocated objects thereafter calls to attempt to acquire additional objects from the allocation function should that allocation fail one of the preallocated objects if any remain is returned when an object is freed with it is kept in the pool if the number of preallocated objects is cur rently below the minimum otherwise it is to be returned to the system a mempool can be resized with int pool int int this call if successful resizes the pool to have at least objects if you no longer need a memory pool return it to the system with void pool you must return all allocated objects before destroying the mempool or a kernel oops results if you are considering using a mempool in your driver please keep one thing in mind mempools allocate a chunk of memory that sits in a list idle and unavailable for any real use it is easy to consume a great deal of memory with mempools in almost every case the preferred alternative is to do without the mempool and simply deal with the possibility of allocation failures instead if there is any way for your driver to respond to an allocation failure in a way that does not endanger the integ rity of the system do things that way use of mempools in driver code should be rare and friends if a module needs to allocate big chunks of memory it is usually better to use a page oriented technique requesting whole pages also has other advantages which are introduced in chapter to allocate pages the following functions are available unsigned int flags returns a pointer to a new page and fills the page with zeros unsigned int flags similar to but doesn t clear the page unsigned int flags unsigned int order allocates and returns a pointer to the first byte of a memory area that is poten tially several physically contiguous pages long but doesn t zero the area the flags argument works in the same way as with kmalloc usually either or is used perhaps with the addition of the flag for memory that can be used for isa direct memory access operations or when high memory can be used order is the base two logarithm of the number of pages you are requesting or freeing i e for example order is if you want one page and if you request eight pages if order is too big no contiguous area of that size is available the page allocation fails the function which takes an inte ger argument can be used to extract the order from a size that must be a power of two for the hosting platform the maximum allowed value for order is or cor responding to or pages depending on the architecture the chances of an order allocation succeeding on anything other than a freshly booted system with a lot of memory are small however if you are curious proc buddyinfo tells you how many blocks of each order are avail able for each memory zone on the system when a program is done with the pages it can free them with one of the following functions the first function is a macro that falls back on the second void unsigned long addr void unsigned long addr unsigned long order if you try to free a different number of pages from what you allocated the memory map becomes corrupted and the system gets in trouble at a later time it worth stressing that and the other functions can be called at any time subject to the same rules we saw for kmalloc the functions can fail to allocate memory in certain circumstances particularly when is used therefore the program calling these allocation functions must be prepared to handle an alloca tion failure although kmalloc sometimes fails when there is no available memory the kernel does its best to fulfill allocation requests therefore it easy to degrade system responsiveness by allocating too much memory for example you can bring the computer down by pushing too much data into a scull device the system starts crawling while it tries to swap out as much as possible in order to fulfill the kmalloc request since every resource is being sucked up by the growing device the com puter is soon rendered unusable at that point you can no longer even start a new process to try to deal with the problem we don t address this issue in scull since it is just a sample module and not a real tool to put into a multiuser system as a pro grammer you must be careful nonetheless because a module is privileged code and can open new security holes in the system the most likely is a denial of service hole like the one just outlined although described shortly should really be used for allocating high memory pages for reasons we can t really get into until chapter a scull using whole pages scullp in order to test page allocation for real we have released the scullp module together with other sample code it is a reduced scull just like scullc introduced earlier memory quanta allocated by scullp are whole pages or page sets the variable defaults to but can be changed at either compile or load time the following lines show how it allocates memory here the allocation of a single quantum if dptr data dptr data void dptr order if dptr data goto nomem memset dptr data dptr order the code to deallocate memory in scullp looks like this this code frees a whole quantum set for i i qset i if dptr data i unsigned long dptr data i dptr order at the user level the perceived difference is primarily a speed improvement and bet ter memory use because there is no internal fragmentation of memory we ran some tests copying mb from to and then from to the results showed a slight improvement in kernel space processor usage the performance improvement is not dramatic because kmalloc is designed to be fast the main advantage of page level allocation isn t actually speed but rather more efficient memory usage allocating by pages wastes no memory whereas using kmalloc wastes an unpredictable amount of memory because of allocation granularity but the biggest advantage of the functions is that the pages obtained are completely yours and you could in theory assemble the pages into a linear area by appropriate tweaking of the page tables for example you can allow a user pro cess to mmap memory areas obtained as single unrelated pages we discuss this kind of operation in chapter where we show how scullp offers memory mapping something that scull cannot offer the interface for completeness we introduce another interface for memory allocation even though we will not be prepared to use it until after chapter for now suffice it to say that struct page is an internal kernel structure that describes a page of memory as we will see there are many places in the kernel where it is necessary to work with page structures they are especially useful in any situation where you might be deal ing with high memory which does not have a constant address in kernel space the real core of the linux page allocator is a function called struct page int nid unsigned int flags unsigned int order this function also has two variants which are simply macros these are the versions that you will most likely use struct page unsigned int flags unsigned int order struct page unsigned int flags the core function takes three arguments nid is the numa node id whose memory should be allocated flags is the usual allocation flags and order is the size of the allocation the return value is a pointer to the first of possi bly many page structures describing the allocated memory or as usual null on failure simplifies the situation by allocating the memory on the current numa node it calls with the return value from as the nid parameter and of course omits the order parameter and allocates a sin gle page to release pages allocated in this manner you should use one of the following void struct page page void struct page page unsigned int order void struct page page void struct page page if you have specific knowledge of whether a single page contents are likely to be resident in the processor cache you should communicate that to the kernel with for cache resident pages or this information helps the memory allocator optimize its use of memory across the system vmalloc and friends the next memory allocation function that we show you is vmalloc which allocates a contiguous memory region in the virtual address space although the pages are not con secutive in physical memory each page is retrieved with a separate call to the kernel sees them as a contiguous range of addresses vmalloc returns the null address if an error occurs otherwise it returns a pointer to a linear memory area of size at least size numa nonuniform memory access computers are multiprocessor systems where memory is local to specific groups of processors nodes access to local memory is faster than access to nonlocal memory on such systems allocating memory on the correct node is important driver authors do not normally have to worry about numa issues however we describe vmalloc here because it is one of the fundamental linux memory alloca tion mechanisms we should note however that use of vmalloc is discouraged in most situations memory obtained from vmalloc is slightly less efficient to work with and on some architectures the amount of address space set aside for vmalloc is rela tively small code that uses vmalloc is likely to get a chilly reception if submitted for inclusion in the kernel if possible you should work directly with individual pages rather than trying to smooth things over with vmalloc that said let see how vmalloc works the prototypes of the function and its rela tives ioremap which is not strictly an allocation function is discussed later in this section are as follows include linux vmalloc h void vmalloc unsigned long size void vfree void addr void ioremap unsigned long offset unsigned long size void iounmap void addr it worth stressing that memory addresses returned by kmalloc and are also virtual addresses their actual value is still massaged by the mmu the mem ory management unit usually part of the cpu before it is used to address physical memory vmalloc is not different in how it uses the hardware but rather in how the kernel performs the allocation task the virtual address range used by kmalloc and features a one to one mapping to physical memory possibly shifted by a constant value the functions don t need to modify the page tables for that address range the address range used by vmalloc and ioremap on the other hand is completely syn thetic and each allocation builds the virtual memory area by suitably setting up the page tables this difference can be perceived by comparing the pointers returned by the alloca tion functions on some platforms for example the addresses returned by vmalloc are just beyond the addresses that kmalloc uses on other platforms for example mips ia and they belong to a completely different address range addresses available for vmalloc are in the range from to both symbols are defined in asm pgtable h addresses allocated by vmalloc can t be used outside of the microprocessor because they make sense only on top of the processor mmu when a driver needs a real physical address such as a dma address used by peripheral hardware to drive the system bus you can t easily use vmalloc the right time to call vmalloc is when actually some architectures define ranges of virtual addresses as reserved to address physical memory when this happens the linux kernel takes advantage of the feature and both the kernel and addresses lie in one of those memory ranges the difference is transparent to device drivers and other code that is not directly involved with the memory management kernel subsystem you are allocating memory for a large sequential buffer that exists only in software it important to note that vmalloc has more overhead than because it must both retrieve the memory and build the page tables therefore it doesn t make sense to call vmalloc to allocate just one page an example of a function in the kernel that uses vmalloc is the system call which uses vmalloc to get space for the module being created code and data of the module are later copied to the allocated space using in this way the module appears to be loaded into contiguous memory you can verify by look ing in proc kallsyms that kernel symbols exported by modules lie in a different memory range from symbols exported by the kernel proper memory allocated with vmalloc is released by vfree in the same way that kfree releases memory allocated by kmalloc like vmalloc ioremap builds new page tables unlike vmalloc however it doesn t actually allocate any memory the return value of ioremap is a special virtual address that can be used to access the specified physical address range the virtual address obtained is eventually released by calling iounmap ioremap is most useful for mapping the physical address of a pci buffer to virtual kernel space for example it can be used to access the frame buffer of a pci video device such buffers are usually mapped at high physical addresses outside of the address range for which the kernel builds page tables at boot time pci issues are explained in more detail in chapter it worth noting that for the sake of portability you should not directly access addresses returned by ioremap as if they were pointers to memory rather you should always use readb and the other i o functions introduced in chapter this requirement applies because some platforms such as the alpha are unable to directly map pci memory regions to the processor address space because of differ ences between pci specs and alpha processors in how data is transferred both ioremap and vmalloc are page oriented they work by modifying the page tables consequently the relocated or allocated size is rounded up to the nearest page boundary ioremap simulates an unaligned mapping by rounding down the address to be remapped and by returning an offset into the first remapped page one minor drawback of vmalloc is that it can t be used in atomic context because internally it uses kmalloc to acquire storage for the page tables and therefore could sleep this shouldn t be a problem if the use of isn t good enough for an interrupt handler the software design needs some cleaning up a scull using virtual addresses scullv sample code using vmalloc is provided in the scullv module like scullp this module is a stripped down version of scull that uses a different allocation function to obtain space for the device to store data the module allocates memory pages at a time the allocation is done in large chunks to achieve better performance than scullp and to show something that takes too long with other allocation techniques to be feasible allocating more than one page with is failure prone and even when it succeeds it can be slow as we saw earlier vmalloc is faster than other functions in allocating several pages but somewhat slower when retrieving a single page because of the overhead of page table building scullv is designed like scullp order specifies the order of each allocation and defaults to the only difference between scullv and scullp is in allocation management these lines use vmalloc to obtain new memory allocate a quantum using virtual addresses if dptr data dptr data void vmalloc dptr order if dptr data goto nomem memset dptr data dptr order and these lines release memory release the quantum set for i i qset i if dptr data i vfree dptr data i if you compile both modules with debugging enabled you can look at their data allocation by reading the files they create in proc this snapshot was taken on an system salma cat tmp bigfile dev head proc scullpmem device qset order sz item at qset at salma cat tmp bigfile dev head proc scullvmem device qset order sz item at qset at the following output instead came from an system rudo cat tmp bigfile dev head proc scullpmem device qset order sz item at qset at rudo cat tmp bigfile dev head proc scullvmem device qset order sz item at qset at the values show two different behaviors on physical addresses and virtual addresses are mapped to completely different address ranges and while on computers vmalloc returns virtual addresses just above the mapping used for physical memory per cpu variables per cpu variables are an interesting kernel feature when you create a per cpu variable each processor on the system gets its own copy of that variable this may seem like a strange thing to want to do but it has its advantages access to per cpu variables requires almost no locking because each processor works with its own copy per cpu variables can also remain in their respective processors caches which leads to significantly better performance for frequently updated quantities a good example of per cpu variable use can be found in the networking subsystem the kernel maintains no end of counters tracking how many of each type of packet was received these counters can be updated thousands of times per second rather than deal with the caching and locking issues the networking developers put the sta tistics counters into per cpu variables updates are now lockless and fast on the rare occasion that user space requests to see the values of the counters it is a simple matter to add up each processor version and return the total the declarations for per cpu variables can be found in linux percpu h to create a per cpu variable at compile time use this macro type name if the variable to be called name is an array include the dimension information with the type thus a per cpu array of three integers would be created with int per cpu variables can be manipulated without explicit locking almost remember that the kernel is preemptible it would not do for a processor to be preempted in the middle of a critical section that modifies a per cpu variable it also would not be good if your process were to be moved to another processor in the middle of a per cpu variable access for this reason you must explicitly use the macro to access the current processor copy of a given variable and call when you are done the call to returns an lvalue for the current processor version of the variable and disables preemption since an lvalue is returned it can be assigned to or operated on directly for example one counter in the networking code is incremented with these two statements sockets_in_use you can access another processor copy of the variable with variable int if you write code that involves processors reaching into each other per cpu vari ables you of course have to implement a locking scheme that makes that access safe dynamically allocated per cpu variables are also possible these variables can be allocated with void type void size align in most cases does the job you can call in cases where a particular alignment is required in either case a per cpu variable can be returned to the system with access to a dynamically allocated per cpu variable is done via void int this macro returns a pointer to the version of corresponding to the given if you are simply reading another cpu version of the variable you can deref erence that pointer and be done with it if however you are manipulating the current processor version you probably need to ensure that you cannot be moved out of that processor first if the entirety of your access to the per cpu variable happens with a spinlock held all is well usually however you need to use to block preemption while working with the variable thus code using dynamic per cpu vari ables tends to look like this int cpu cpu ptr cpu work with ptr when using compile time per cpu variables the and macros take care of these details dynamic per cpu variables require more explicit protection per cpu variables can be exported to modules but you must use a special version of the macros per_cpu_var to access such a variable within a module declare it with type name the use of instead of tells the compiler that an external reference is being made if you want to use per cpu variables to create a simple integer counter take a look at the canned implementation in linux h finally note that some architectures have a limited amount of address space available for per cpu vari ables if you create per cpu variables in your code you should try to keep them small obtaining large buffers as we have noted in previous sections allocations of large contiguous memory buff ers are prone to failure system memory fragments over time and chances are that a truly large region of memory will simply not be available since there are usually ways of getting the job done without huge buffers the kernel developers have not put a high priority on making large allocations work before you try to obtain a large memory area you should really consider the alternatives by far the best way of per forming large i o operations is through scatter gather operations which we discuss in the section scatter gather mappings in chapter acquiring a dedicated buffer at boot time if you really need a huge buffer of physically contiguous memory the best approach is often to allocate it by requesting memory at boot time allocation at boot time is the only way to retrieve consecutive memory pages while bypassing the limits imposed by on the buffer size both in terms of maximum allowed size and limited choice of sizes allocating memory at boot time is a dirty tech nique because it bypasses all memory management policies by reserving a private memory pool this technique is inelegant and inflexible but it is also the least prone to failure needless to say a module can t allocate memory at boot time only driv ers directly linked to the kernel can do that one noticeable problem with boot time allocation is that it is not a feasible option for the average user since this mechanism is available only for code linked in the ker nel image a device driver using this kind of allocation can be installed or replaced only by rebuilding the kernel and rebooting the computer when the kernel is booted it gains access to all the physical memory available in the system it then initializes each of its subsystems by calling that subsystem initializa tion function allowing initialization code to allocate a memory buffer for private use by reducing the amount of ram left for normal system operation boot time memory allocation is performed by calling one of these functions include linux bootmem h void unsigned long size void unsigned long size void unsigned long size void unsigned long size the functions allocate either whole pages if they end with or non page aligned memory areas the allocated memory may be high memory unless one of the versions is used if you are allocating this buffer for a device driver you proba bly want to use it for dma operations and that is not always possible with high memory thus you probably want to use one of the variants it is rare to free memory allocated at boot time you will almost certainly be unable to get it back later if you want it there is an interface to free this memory however void unsigned long addr unsigned long size note that partial pages freed in this manner are not returned to the system but if you are using this technique you have probably allocated a fair number of whole pages to begin with if you must use boot time allocation you need to link your driver directly into the kernel see the files in the kernel source under documentation kbuild for more infor mation on how this should be done quick reference the functions and symbols related to memory allocation are include linux slab h void kmalloc size int flags void kfree void obj the most frequently used interface to memory allocation include linux mm h flags that control how memory allocations are performed from the least restric tive to the most the and priorities allow the current process to be put to sleep to satisfy the request and disable filesystem operations and all i o operations respectively while allocations can not sleep at all __gfp_nowarn __gfp_nofail these flags modify the kernel behavior when allocating memory include linux malloc h kmem_cache_create char name size offset unsigned long flags constructor destructor int cache create and destroy a slab cache the cache can be used to allocate several objects of the same size flags that can be specified while creating a cache flags that the allocator can pass to the constructor and the destructor functions void kmem_cache_t cache int flags void kmem_cache_t cache const void obj allocate and release a single object from the cache proc slabinfo a virtual file containing statistics on slab cache usage include linux mempool h mempool_create int mempool_free_t void data void pool functions for the creation of memory pools which try to avoid memory alloca tion failures by keeping an emergency list of allocated items void pool int void void element pool functions for allocating items from and returning them to memory pools unsigned long int flags unsigned long int flags unsigned long int flags unsigned long order the page oriented allocation functions returns a single zero filled page all the other versions of the call do not initialize the contents of the returned page int unsigned long size returns the allocation order associated to size in the current platform according to the argument must be a power of two and the return value is at least void unsigned long addr void unsigned long addr unsigned long order functions that release page oriented allocations struct page int nid unsigned int flags unsigned int order struct page unsigned int flags unsigned int order struct page unsigned int flags all variants of the lowest level page allocator in the linux kernel void struct page page void struct page page unsigned int order void struct page page void struct page page various ways of freeing pages allocated with one of the forms of include linux vmalloc h void vmalloc unsigned long size void vfree void addr include asm io h void ioremap unsigned long offset unsigned long size void iounmap void addr functions that allocate or free a contiguous virtual address space ioremap accesses physical memory through virtual addresses while vmalloc allocates free pages regions mapped with ioremap are freed with iounmap while pages obtained from vmalloc are released with vfree include linux percpu h type name type name macros that define and declare per cpu variables variable int variable variable macros that provide access to statically declared per cpu variables void type void size align void void variable functions that perform runtime allocation and freeing of per cpu variables int void void variable int obtains a reference to the current processor therefore preventing pre emption and movement to another processor and returns the id number of the processor returns that reference to access a dynamically allocated per cpu variable use with the id of the cpu whose version should be accessed manipulations of the current cpu version of a dynamic per cpu variable should probably be surrounded by calls to and include linux bootmem h void unsigned long size void unsigned long size void unsigned long size void unsigned long size void unsigned long addr unsigned long size functions which can be used only by drivers directly linked into the kernel that perform allocation and freeing of memory at system bootstrap time although some devices can be controlled using nothing but their i o regions most real devices are a bit more complicated than that devices have to deal with the external world which often includes things such as spinning disks moving tape wires to distant places and so on much has to be done in a time frame that is differ ent from and far slower than that of the processor since it is almost always undesir able to have the processor wait on external events there must be a way for a device to let the processor know when something has happened that way of course is interrupts an interrupt is simply a signal that the hardware can send when it wants the processor attention linux handles interrupts in much the same way that it handles signals in user space for the most part a driver need only register a handler for its device interrupts and handle them properly when they arrive of course underneath that simple picture there is some complexity in particular interrupt handlers are somewhat limited in the actions they can perform as a result of how they are run it is difficult to demonstrate the use of interrupts without a real hardware device to generate them thus the sample code used in this chapter works with the parallel port such ports are starting to become scarce on modern hardware but with luck most people are still able to get their hands on a system with an available port we ll be working with the short module from the previous chapter with some small addi tions it can generate and handle interrupts from the parallel port the module name short actually means short int it is c isn t it to remind us that it handles interrupts before we get into the topic however it is time for one cautionary note interrupt handlers by their nature run concurrently with other code thus they inevitably raise issues of concurrency and contention for data structures and hardware if you succumbed to the temptation to pass over the discussion in chapter we under stand but we also recommend that you turn back and have another look now a solid understanding of concurrency control techniques is vital when working with interrupts preparing the parallel port although the parallel interface is simple it can trigger interrupts this capability is used by the printer to notify the lp driver that it is ready to accept the next character in the buffer like most devices the parallel port doesn t actually generate interrupts before it instructed to do so the parallel standard states that setting bit of port or whatever enables interrupt reporting a simple outb call to set the bit is performed by short at module initialization once interrupts are enabled the parallel interface generates an interrupt whenever the electrical signal at pin the so called ack bit changes from low to high the simplest way to force the interface to generate interrupts short of hooking up a printer to the port is to connect pins and of the parallel connector a short length of wire inserted into the appropriate holes in the parallel port connector on the back of your system creates this connection the pinout of the parallel port is shown in figure pin is the most significant bit of the parallel data byte if you write binary data to dev you generate several interrupts writing ascii text to the port won t generate any interrupts though because the ascii character set has no entries with the top bit set if you d rather avoid wiring pins together but you do have a printer at hand you can run the sample interrupt handler using a real printer as shown later however note that the probing functions we introduce depend on the jumper between pin and being in place and you need it to experiment with probing using our code installing an interrupt handler if you want to actually see interrupts being generated writing to the hardware device isn t enough a software handler must be configured in the system if the linux kernel hasn t been told to expect your interrupt it simply acknowledges and ignores it interrupt lines are a precious and often limited resource particularly when there are only or of them the kernel keeps a registry of interrupt lines similar to the registry of i o ports a module is expected to request an interrupt channel or irq for interrupt request before using it and to release it when finished in many situa tions modules are also expected to be able to share interrupt lines with other driv ers as we will see the following functions declared in linux interrupt h implement the interrupt registration interface int unsigned int irq handler int void struct unsigned long flags const char void void unsigned int irq void the value returned from to the requesting function is either to indicate success or a negative error code as usual it not uncommon for the function to return ebusy to signal that another driver is already using the requested interrupt line the arguments to the functions are as follows unsigned int irq the interrupt number being requested handler int void struct the pointer to the handling function being installed we discuss the arguments to this function and its return value later in this chapter unsigned long flags as you might expect a bit mask of options described later related to interrupt management const char the string passed to is used in proc interrupts to show the owner of the interrupt see the next section void pointer used for shared interrupt lines it is a unique identifier that is used when the interrupt line is freed and that may also be used by the driver to point to its own private data area to identify which device is interrupting if the interrupt is not shared can be set to null but it a good idea anyway to use this item to point to the device structure we ll see a practical use for in the sec tion implementing a handler the bits that can be set in flags are as follows when set this indicates a fast interrupt handler fast handlers are executed with interrupts disabled on the current processor the topic is covered in the sec tion fast and slow handlers this bit signals that the interrupt can be shared between devices the concept of sharing is outlined in the section interrupt sharing this bit indicates that the generated interrupts can contribute to the entropy pool used by dev random and dev urandom these devices return truly random num bers when read and are designed to help application software choose secure keys for encryption such random numbers are extracted from an entropy pool that is contributed by various random events if your device generates interrupts at truly random times you should set this flag if on the other hand your interrupts are predictable for example vertical blanking of a frame grabber the flag is not worth setting it wouldn t contribute to system entropy anyway devices that could be influenced by attackers should not set this flag for example network drivers can be subjected to predictable packet timing from outside and should not contribute to the entropy pool see the comments in drivers char random c for more information the interrupt handler can be installed either at driver initialization or when the device is first opened although installing the interrupt handler from within the mod ule initialization function might sound like a good idea it often isn t especially if your device does not share interrupts because the number of interrupt lines is lim ited you don t want to waste them you can easily end up with more devices in your computer than there are interrupts if a module requests an irq at initialization it prevents any other driver from using the interrupt even if the device holding it is never used requesting the interrupt at device open on the other hand allows some sharing of resources it is possible for example to run a frame grabber on the same interrupt as a modem as long as you don t use the two devices at the same time it is quite common for users to load the module for a special device at system boot even if the device is rarely used a data acquisition gadget might use the same interrupt as the second serial port while it not too hard to avoid connecting to your internet service pro vider isp during data acquisition being forced to unload a module in order to use the modem is really unpleasant the correct place to call is when the device is first opened before the hardware is instructed to generate interrupts the place to call is the last time the device is closed after the hardware is told not to interrupt the processor any more the disadvantage of this technique is that you need to keep a per device open count so that you know when interrupts can be disabled this discussion notwithstanding short requests its interrupt line at load time this was done so that you can run the test programs without having to run an extra pro cess to keep the device open short therefore requests the interrupt from within its initialization function instead of doing it in as a real device driver would the interrupt requested by the following code is the actual assignment of the variable i e determining which irq to use is shown later since it is not rele vant to the current discussion is the base i o address of the parallel inter face being used register of the interface is written to enable interrupt reporting if result short null if result printk short can t get assigned irq i n else actually enable it assume this is a parallel port outb the code shows that the handler being installed is a fast handler doesn t support interrupt sharing is missing and doesn t contribute to system entropy is missing too the outb call then enables inter rupt reporting for the parallel port for what it worth the and architectures define a function for query ing the availability of an interrupt line int unsigned int irq unsigned long flags this function returns a nonzero value if an attempt to allocate the given interrupt suc ceeds note however that things can always change between calls to and the proc interface whenever a hardware interrupt reaches the processor an internal counter is incre mented providing a way to check whether the device is working as expected reported interrupts are shown in proc interrupts the following snapshot was taken on a two processor pentium system root montalcino bike corbet write src short m proc interrupts 4848108 io apic edge timer xt pic cascade io apic edge rtc io apic level io apic level io apic edge nmi loc err mis the first column is the irq number you can see from the irqs that are missing that the file shows only interrupts corresponding to installed handlers for example the first serial port which uses interrupt number is not shown indicating that the modem isn t being used in fact even if the modem had been used earlier but wasn t in use at the time of the snapshot it would not show up in the file the serial ports are well behaved and release their interrupt handlers when the device is closed the proc interrupts display shows how many interrupts have been delivered to each cpu on the system as you can see from the output the linux kernel generally handles interrupts on the first cpu as a way of maximizing cache locality the last two col umns give information on the programmable interrupt controller that handles the inter rupt and that a driver writer does not need to worry about and the name of the device that have registered handlers for the interrupt as specified in the argument to the proc tree contains another interrupt related file proc stat sometimes you ll find one file more useful and sometimes you ll prefer the other proc stat records several low level statistics about system activity including but not limited to the number of interrupts received since system boot each line of stat begins with a text string that is the key to the line the intr mark is what we are looking for the fol lowing truncated snapshot was taken shortly after the previous one intr 4406 the first number is the total of all interrupts while each of the others represents a single irq line starting with interrupt all of the counts are summed across all processors in the system this snapshot shows that interrupt number has been used times even though no handler is currently installed if the driver you re testing acquires and releases the interrupt at each open and close cycle you may find proc stat more useful than proc interrupts another difference between the two files is that interrupts is not architecture depen dent except perhaps for a couple of lines at the end whereas stat is the number of fields depends on the hardware underlying the kernel the number of available inter rupts varies from as few as on the sparc to as many as on the ia and a few other systems it interesting to note that the number of interrupts defined on the is currently not as you may expect this as explained in include asm irq h depends on linux using the architectural limit instead of an imple mentation specific limit such as the interrupt sources of the old fashioned pc interrupt controller the following is a snapshot of proc interrupts taken on an ia system as you can see besides different hardware routing of common interrupt sources the output is very similar to that from the bit system shown earlier 1705 io sapic level sapic perfmon io sapic level 47 146 io sapic level usb uhci io sapic edge io sapic edge keyboard io sapic edge ps mouse sapic timer although some larger systems explicitly use interrupt balancing schemes to spread the interrupt load across the system sapic ipi nmi err autodetecting the irq number one of the most challenging problems for a driver at initialization time can be how to determine which irq line is going to be used by the device the driver needs the information in order to correctly install the handler even though a programmer could require the user to specify the interrupt number at load time this is a bad prac tice because most of the time the user doesn t know the number either because he didn t configure the jumpers or because the device is jumperless most users want their hardware to just work and are not interested in issues like interrupt num bers so autodetection of the interrupt number is a basic requirement for driver usability sometimes autodetection depends on the knowledge that some devices feature a default behavior that rarely if ever changes in this case the driver might assume that the default values apply this is exactly how short behaves by default with the parallel port the implementation is straightforward as shown by short itself if not yet specified force the default on switch case break case break case break the code assigns the interrupt number according to the chosen base i o address while allowing the user to override the default at load time with something like insmod short ko irq x defaults to so defaults to some devices are more advanced in design and simply announce which interrupt they re going to use in this case the driver retrieves the interrupt number by read ing a status byte from one of the device i o ports or pci configuration space when the target device is one that has the ability to tell the driver which interrupt it is going to use autodetecting the irq number just means probing the device with no additional work required to probe the interrupt most modern hardware works this way fortunately for example the pci standard solves the problem by requiring peripheral devices to declare what interrupt line they are going to use the pci standard is discussed in chapter unfortunately not every device is programmer friendly and autodetection might require some probing the technique is quite simple the driver tells the device to generate interrupts and watches what happens if everything goes well only one interrupt line is activated although probing is simple in theory the actual implementation might be unclear we look at two ways to perform the task calling kernel defined helper functions and implementing our own version kernel assisted probing the linux kernel offers a low level facility for probing the interrupt number it works for only nonshared interrupts but most hardware that is capable of working in a shared interrupt mode provides better ways of finding the configured interrupt num ber anyway the facility consists of two functions declared in linux interrupt h which also describes the probing machinery unsigned long void this function returns a bit mask of unassigned interrupts the driver must pre serve the returned bit mask and pass it to later after this call the driver should arrange for its device to generate at least one interrupt int unsigned long after the device has requested an interrupt the driver calls this function passing as its argument the bit mask previously returned by returns the number of the interrupt that was issued after if no inter rupts occurred is returned therefore irq can t be probed for but no cus tom device can use it on any of the supported architectures anyway if more than one interrupt occurred ambiguous detection returns a negative value the programmer should be careful to enable interrupts on the device after the call to and to disable them before calling additionally you must remember to service the pending interrupt in your device after the short module demonstrates how to use such probing if you load the module with probe the following code is executed to detect your interrupt line provided pins and of the parallel connector are bound together int count do unsigned long mask mask enable reporting clear the bit set the bit interrupt disable reporting udelay give it some time mask if none of them printk short no irq reported by probe n if more than one line has been activated the result is negative we should service the interrupt no need for lpt port and loop over again loop at most five times then give up while count if printk short probe failed i times giving up n count note the use of udelay before calling depending on the speed of your processor you may have to wait for a brief period to give the interrupt time to actu ally be delivered probing might be a lengthy task while this is not true for short probing a frame grabber for example requires a delay of at least 20 ms which is ages for the proces sor and other devices might take even longer therefore it best to probe for the interrupt line only once at module initialization independently of whether you install the handler at device open as you should or within the initialization func tion which is not recommended it interesting to note that on some platforms powerpc most mips imple mentations and both sparc versions probing is unnecessary and therefore the previous functions are just empty placeholders sometimes called useless isa non sense on other platforms probing is implemented only for isa devices anyway most architectures define the functions even if they are empty to ease porting exist ing device drivers do it yourself probing probing can also be implemented in the driver itself without too much trouble it is a rare driver that must implement its own probing but seeing how it works gives some insight into the process to that end the short module performs do it yourself detec tion of the irq line if it is loaded with probe the mechanism is the same as the one described earlier enable all unused inter rupts then wait and see what happens we can however exploit our knowledge of the device often a device can be configured to use one irq number from a set of three or four probing just those irqs enables us to detect the right one without having to test for all possible irqs the short implementation assumes that and are the only possible irq val ues these numbers are actually the values that some parallel devices allow you to select the following code probes by testing all possible interrupts and looking at what happens the trials array lists the irqs to try and has as the end marker the tried array is used to keep track of which handlers have actually been registered by this driver int trials int tried int i count install the probing handler for all possible lines remember the result for success or ebusy in order to only free what has been acquired for i trials i i tried i trials i short probe null do none got yet enable 0xff toggle the bit disable udelay give it some time the value has been set by the handler if none of them printk short no irq reported by probe n if more than one line has been activated the result is negative we should service the interrupt but the lpt port doesn t need it and loop over again do it at most times while count end of loop uninstall the handler for i trials i i if tried i trials i null if printk short probe failed i times giving up n count you might not know in advance what the possible irq values are in that case you need to probe all the free interrupts instead of limiting yourself to a few trials to probe for all interrupts you have to probe from irq to irq where is defined in asm irq h and is platform dependent now we are missing only the probing handler itself the handler role is to update according to which interrupts are actually received a value in means nothing yet while a negative value means ambiguous these values were chosen to be consistent with and to allow the same code to call either kind of probing within short c int irq void struct regs if irq found if irq irq ambiguous return the arguments to the handler are described later knowing that irq is the interrupt being handled should be sufficient to understand the function just shown fast and slow handlers older versions of the linux kernel took great pains to distinguish between fast and slow interrupts fast interrupts were those that could be handled very quickly whereas handling slow interrupts took significantly longer slow interrupts could be sufficiently demanding of the processor and it was worthwhile to reenable inter rupts while they were being handled otherwise tasks requiring quick attention could be delayed for too long in modern kernels most of the differences between fast and slow interrupts have dis appeared there remains only one fast interrupts those that were requested with the flag are executed with all other interrupts disabled on the current processor note that other processors can still handle interrupts although you will never see two processors handling the same irq at the same time so which type of interrupt should your driver use on modern systems is intended only for use in a few specific situations such as timer interrupts unless you have a strong reason to run your interrupt handler with other interrupts disabled you should not use this description should satisfy most readers although someone with a taste for hard ware and some experience with her computer might be interested in going deeper if you don t care about the internal details you can skip to the next section the internals of interrupt handling on the this description has been extrapolated from arch kernel irq c arch kernel apic c arch kernel entry s arch kernel c and include asm h as they appear in the kernels although the general concepts remain the same the hardware details differ on other platforms the lowest level of interrupt handling can be found in entry s an assembly language file that handles much of the machine level work by way of a bit of assembler trick ery and some macros a bit of code is assigned to every possible interrupt in each case the code pushes the interrupt number on the stack and jumps to a common segment which calls defined in irq c the first thing does is to acknowledge the interrupt so that the interrupt con troller can go on to other things it then obtains a spinlock for the given irq number thus preventing any other cpu from handling this irq it clears a couple of status bits including one called that we ll lookat shortly and then looks up the handler for this particular irq if there is no handler there nothing to do the spinlock is released any pending software interrupts are handled and returns usually however if a device is interrupting there is at least one handler registered for its irq as well the function is called to actually invoke the handlers if the handler is of the slow variety is not set interrupts are reenabled in the hardware and the handler is invoked then it just a matter of cleaning up running software interrupts and getting back to regular work the reg ular work may well have changed as a result of an interrupt the handler could a process for example so the last thing that happens on return from an interrupt is a possible rescheduling of the processor probing for irqs is done by setting the status bit for each irq that cur rently lacks a handler when the interrupt happens clears that bit and then returns because no handler is registered when called by a driver needs to search for only the irq that no longer has set implementing a handler so far we ve learned to register an interrupt handler but not to write one actually there nothing unusual about a handler it ordinary c code the only peculiarity is that a handler runs at interrupt time and therefore suffers some restrictions on what it can do these restrictions are the same as those we saw with kernel timers a handler can t transfer data to or from user space because it doesn t execute in the context of a process handlers also cannot do anything that would sleep such as calling allocating memory with anything other than or locking a semaphore finally handlers cannot call schedule the role of an interrupt handler is to give feedback to its device about interrupt reception and to read or write data according to the meaning of the interrupt being serviced the first step usually consists of clearing a bit on the interface board most hardware devices won t generate other interrupts until their interrupt pending bit has been cleared depending on how your hardware works this step may need to be performed last instead of first there is no catch all rule here some devices don t require this step because they don t have an interrupt pending bit such devices are a minority although the parallel port is one of them for that reason short does not have to clear such a bit a typical task for an interrupt handler is awakening processes sleeping on the device if the interrupt signals the event they re waiting for such as the arrival of new data to stick with the frame grabber example a process could acquire a sequence of images by continuously reading the device the read call blocks before reading each frame while the interrupt handler awakens the process as soon as each new frame arrives this assumes that the grabber interrupts the processor to signal successful arrival of each new frame the programmer should be careful to write a routine that executes in a minimum amount of time independent of its being a fast or slow handler if a long computa tion needs to be performed the best approach is to use a tasklet or workqueue to schedule computation at a safer time we ll look at how work can be deferred in this manner in the section top and bottom halves our sample code in short responds to the interrupt by calling and printing the current time into a page sized circular buffer it then awakens any read ing process because there is now data available to be read int irq void struct regs struct timeval tv int written tv write a byte record assume is a multiple of written sprintf char n int tv int tv written written awake any reading process return this code though simple represents the typical job of an interrupt handler it in turn calls which is defined as follows static inline void volatile unsigned long index int delta unsigned long new index delta barrier don t optimize these two together index new new this function has been carefully written to wrap a pointer into the circular buffer without ever exposing an incorrect value the barrier call is there to block compiler optimizations across the other two lines of the function without the barrier the compiler might decide to optimize out the new variable and assign directly to index that optimization could expose an incorrect value of the index for a brief period in the case where it wraps by taking care to prevent in inconsistent value from ever being visible to other threads we can manipulate the circular buffer pointers safely without locks the device file used to read the buffer being filled at interrupt time is dev shortint this device special file together with dev shortprint wasn t introduced in chapter because its use is specific to interrupt handling the internals of dev shortint are specifically tailored for interrupt generation and reporting writing to the device generates one interrupt every other byte reading the device gives the time when each interrupt was reported if you connect together pins and of the parallel connector you can generate interrupts by raising the high bit of the parallel data byte this can be accomplished by writing binary data to dev or by writing anything to dev shortint the following code implements read and write for dev shortint struct file filp char buf count int wait while wait if schedule wait if current a signal arrived return erestartsys tell the fs layer to handle it is the number of readable data bytes if wrapped if count count if buf char count return efault short_tail count return count struct file filp const char buf count int written odd unsigned long port output to the parallel data latch void address void if while written count written odd address else the shortint device accomplishes its task by alternately writing 0x00 and to the parallel port while written count outb written odd port count return written the other device special file dev shortprint uses the parallel port to drive a printer you can use it if you want to avoid connecting pins and of a d connector the write implementation of shortprint uses a circular buffer to store data to be printed while the read implementation is the one just shown so you can read the time your printer takes to eat each character in order to support printer operation the interrupt handler has been slightly modi fied from the one just shown adding the ability to send the next data byte to the printer if there is more data to transfer handler arguments and return value though short ignores them three arguments are passed to an interrupt handler irq and regs let look at the role of each the interrupt number int irq is useful as information you may print in your log messages if any the second argument void is a sort of client data a void argument is passed to and this same pointer is then passed back as an argument to the handler when the interrupt happens you usually pass a pointer to your device data structure in so a driver that manages several instances of the same device doesn t need any extra code in the interrupt handler to find out which device is in charge of the current interrupt event typical use of the argument in an interrupt handler is as follows static int irq void struct regs struct dev now dev points to the right hardware item the typical open code associated with this handler looks like this static void struct inode inode struct file filp struct dev hwinfo minor inode dev irq flags sample dev return the last argument struct regs is rarely used it holds a snapshot of the processor context before the processor entered interrupt code the registers can be used for monitoring and debugging they are not normally needed for regular device driver tasks interrupt handlers should return a value indicating whether there was actually an interrupt to handle if the handler found that its device did indeed need attention it should return otherwise the return value should be you can also generate the return value with this macro handled where handled is nonzero if you were able to handle the interrupt the return value is used by the kernel to detect and suppress spurious interrupts if your device gives you no way to tell whether it really interrupted you should return enabling and disabling interrupts there are times when a device driver must block the delivery of interrupts for a hopefully short period of time we saw one such situation in the section spin locks in chapter often interrupts must be blocked while holding a spinlock to avoid deadlocking the system there are ways of disabling interrupts that do not involve spinlocks but before we discuss them note that disabling interrupts should be a relatively rare activity even in device drivers and this technique should never be used as a mutual exclusion mechanism within a driver disabling a single interrupt sometimes but rarely a driver needs to disable interrupt delivery for a specific interrupt line the kernel offers three functions for this purpose all declared in asm irq h these functions are part of the kernel api so we describe them but their use is discouraged in most drivers among other things you cannot disable shared interrupt lines and on modern systems shared interrupts are the norm that said here they are void int irq void int irq void int irq calling any of these functions may update the mask for the specified irq in the pro grammable interrupt controller pic thus disabling or enabling the specified irq across all processors calls to these functions can be nested if is called twice in succession two calls are required before the irq is truly reen abled it is possible to call these functions from an interrupt handler but enabling your own irq while handling it is not usually good practice not only disables the given interrupt but also waits for a currently executing interrupt handler if any to complete be aware that if the thread calling holds any resources such as spinlocks that the interrupt handler needs the system can deadlock differs from in that it returns immedi ately thus using is a little faster but may leave your driver open to race conditions but why disable an interrupt sticking to the parallel port let look at the plip net work interface a plip device uses the bare bones parallel port to transfer data since only five bits can be read from the parallel connector they are interpreted as four data bits and a clock handshake signal when the first four bits of a packet are trans mitted by the initiator the interface sending the packet the clock line is raised causing the receiving interface to interrupt the processor the plip handler is then invoked to deal with newly arrived data after the device has been alerted the data transfer proceeds using the handshake line to clock new data to the receiving interface this might not be the best imple mentation but it is necessary for compatibility with other packet drivers using the parallel port performance would be unbearable if the receiving interface had to han dle two interrupts for every byte received therefore the driver disables the inter rupt during the reception of the packet instead a poll and delay loop is used to bring in the data similarly because the handshake line from the receiver to the transmitter is used to acknowledge data reception the transmitting interface disables its irq line during packet transmission disabling all interrupts what if you need to disable all interrupts in the kernel it is possible to turn off all interrupt handling on the current processor with either of the following two func tions which are defined in asm system h void unsigned long flags void void a call to disables interrupt delivery on the current processor after sav ing the current interrupt state into flags note that flags is passed directly not by pointer shuts off local interrupt delivery without saving the state you should use this version only if you know that interrupts have not already been disabled elsewhere turning interrupts back on is accomplished with void unsigned long flags void void the first version restores that state which was stored into flags by while enables interrupts unconditionally unlike does not keep track of multiple calls if more than one function in the call chain might need to disable interrupts should be used in the kernel there is no way to disable all interrupts globally across the entire system the kernel developers have decided that the cost of shutting off all inter rupts is too high and that there is no need for that capability in any case if you are working with an older driver that makes calls to functions such as cli and sti you need to update it to use proper locking before it will work under top and bottom halves one of the main problems with interrupt handling is how to perform lengthy tasks within a handler often a substantial amount of work must be done in response to a device interrupt but interrupt handlers need to finish up quickly and not keep inter rupts blocked for long these two needs work and speed conflict with each other leaving the driver writer in a bit of a bind linux along with many other systems resolves this problem by splitting the inter rupt handler into two halves the so called top half is the routine that actually responds to the interrupt the one you register with the bottom half is a routine that is scheduled by the top half to be executed later at a safer time the big difference between the top half handler and the bottom half is that all interrupts are enabled during execution of the bottom half that why it runs at a safer time in the typical scenario the top half saves device data to a device specific buffer sched ules its bottom half and exits this operation is very fast the bottom half then per forms whatever other work is required such as awakening processes starting up another i o operation and so on this setup permits the top half to service a new interrupt while the bottom half is still working almost every serious interrupt handler is split this way for instance when a net work interface reports the arrival of a new packet the handler just retrieves the data and pushes it up to the protocol layer actual processing of the packet is performed in a bottom half the linux kernel has two different mechanisms that may be used to implement bot tom half processing both of which were introduced in chapter 7 tasklets are often the preferred mechanism for bottom half processing they are very fast but all tasklet code must be atomic the alternative to tasklets is workqueues which may have a higher latency but that are allowed to sleep the following discussion works once again with the short driver when loaded with a module option short can be told to do interrupt processing in a top bottom half mode with either a tasklet or workqueue handler in this case the top half executes quickly it simply remembers the current time and schedules the bottom half pro cessing the bottom half is then charged with encoding this time and awakening any user processes that may be waiting for data tasklets remember that tasklets are a special function that may be scheduled to run in soft ware interrupt context at a system determined safe time they may be scheduled to run multiple times but tasklet scheduling is not cumulative the tasklet runs only once even if it is requested repeatedly before it is launched no tasklet ever runs in parallel with itself since they run only once but tasklets can run in parallel with other tasklets on smp systems thus if your driver has multiple tasklets they must employ some sort of locking to avoid conflicting with each other tasklets are also guaranteed to run on the same cpu as the function that first sched ules them therefore an interrupt handler can be secure that a tasklet does not begin executing before the handler has completed however another interrupt can cer tainly be delivered while the tasklet is running so locking between the tasklet and the interrupt handler may still be required tasklets must be declared with the macro name function data name is the name to be given to the tasklet function is the function that is called to execute the tasklet it takes one unsigned long argument and returns void and data is an unsigned long value to be passed to the tasklet function the short driver declares its tasklet as follows void unsigned long the function is used to schedule a tasklet for running if short is loaded with tasklet it installs a different interrupt handler that saves data and schedules the tasklet as follows int irq void struct regs struct timeval cast to stop volatile warning record that an interrupt arrived return the actual tasklet routine will be executed shortly so to speak at the system convenience as mentioned earlier this routine performs the bulk of the work of handling the interrupt it looks like this void unsigned long unused int savecount written we have already been removed from the queue the bottom half reads the tv array filled by the top half and prints it to the circular text buffer which is then consumed by reading processes first write the number of interrupts that occurred before this bh written sprintf char bh after n savecount written then write the time values write exactly bytes at a time so it aligns with do written sprintf char n int int written while awake any reading process among other things this tasklet makes a note of how many interrupts have arrived since it was last called a device such as short can generate a great many interrupts in a brief period so it is not uncommon for several to arrive before the bottom half is executed drivers must always be prepared for this possibility and must be able to determine how much work there is to perform from the information left by the top half workqueues recall that workqueues invoke a function at some future time in the context of a spe cial worker process since the workqueue function runs in process context it can sleep if need be you cannot however copy data into user space from a workqueue unless you use the advanced techniques we demonstrate in chapter the worker process does not have access to any other process address space the short driver if loaded with the wq option set to a nonzero value uses a work queue for its bottom half processing it uses the system default workqueue so there is no special setup code required if your driver has special latency requirements or might sleep for a long time in the workqueue function you may want to create your own dedicated workqueue we do need a structure which is declared and initialized with the following static struct this line is in void void null our worker function is which we have already seen in the previous section when working with a workqueue short establishes yet another interrupt handler that looks like this int irq void struct regs grab the current time information struct timeval short_incr_tv queue the bh don t worry about multiple enqueueing short_wq record that an interrupt arrived return as you can see the interrupt handler looks very much like the tasklet version with the exception that it calls to arrange the bottom half processing interrupt sharing the notion of an irq conflict is almost synonymous with the pc architecture in the past irq lines on the pc have not been able to serve more than one device and there have never been enough of them as a result frustrated users have often spent much time with their computer case open trying to find a way to make all of their peripherals play well together modern hardware of course has been designed to allow the sharing of interrupts the pci bus requires it therefore the linux kernel supports interrupt sharing on all buses even those such as the isa bus where sharing has traditionally not been sup ported device drivers for the kernel should be written to work with shared inter rupts if the target hardware can support that mode of operation fortunately working with shared interrupts is easy most of the time installing a shared handler shared interrupts are installed through just like nonshared ones but there are two differences the bit must be specified in the flags argument when requesting the interrupt the argument must be unique any pointer into the module address space will do but definitely cannot be set to null the kernel keeps a list of shared handlers associated with the interrupt and can be thought of as the signature that differentiates between them if two drivers were to register null as their signature on the same interrupt things might get mixed up at unload time causing the kernel to oops when an interrupt arrived for this rea son modern kernels complain loudly if passed a null when registering shared interrupts when a shared interrupt is requested succeeds if one of the following is true the interrupt line is free all handlers already registered for that line have also specified that the irq is to be shared whenever two or more drivers are sharing an interrupt line and the hardware inter rupts the processor on that line the kernel invokes every handler registered for that interrupt passing each its own therefore a shared handler must be able to recognize its own interrupts and should quickly exit when its own device has not interrupted be sure to return whenever your handler is called and finds that the device is not interrupting if you need to probe for your device before requesting the irq line the kernel can t help you no probing function is available for shared handlers the standard prob ing mechanism works if the line being used is free but if the line is already held by another driver with sharing capabilities the probe fails even if your driver would have worked perfectly fortunately most hardware designed for interrupt sharing is also able to tell the processor which interrupt it is using thus eliminating the need for explicit probing releasing the handler is performed in the normal way using here the argument is used to select the correct handler to release from the list of shared han dlers for the interrupt that why the pointer must be unique a driver using a shared handler needs to be careful about one more thing it can t play with or if it does things might go haywire for other devices sharing the line disabling another device interrupts for even a short time may create latencies that are problematic for that device and it user generally the programmer must remember that his driver doesn t own the irq and its behavior should be more social than is necessary if one owns the interrupt line running the handler as suggested earlier when the kernel receives an interrupt all the registered han dlers are invoked a shared handler must be able to distinguish between interrupts that it needs to handle and interrupts generated by other devices loading short with the option shared installs the following handler instead of the default int irq void struct regs int value written struct timeval tv if it wasn t short return immediately value inb if value return clear the interrupting bit outb value the rest is unchanged tv written sprintf char 08u n int tv 100000000 int tv short_incr_bp short_head written short_queue awake any reading process return an explanation is due here since the parallel port has no interrupt pending bit to check the handler uses the ack bit for this purpose if the bit is high the interrupt being reported is for short and the handler clears the bit the handler resets the bit by zeroing the high bit of the parallel interface data port short assumes that pins and are connected together if one of the other devices sharing the irq with short generates an interrupt short sees that its own line is still inactive and does nothing a full featured driver probably splits the work into top and bottom halves of course but that easy to add and does not have any impact on the code that implements sharing a real driver would also likely use the argument to determine which of possibly many devices might be interrupting note that if you are using a printer instead of the jumper wire to test interrupt man agement with short this shared handler won t work as advertised because the printer protocol doesn t allow for sharing and the driver can t know whether the interrupt was from the printer the proc interface and shared interrupts installing shared handlers in the system doesn t affect proc stat which doesn t even know about handlers however proc interrupts changes slightly all the handlers installed for the same interrupt number appear on the same line of proc interrupts the following output from an system shows how shared interrupt handlers are displayed xt pic timer 453971 xt pic xt pic cascade xt pic libata xt pic rtc xt pic acpi xt pic syskonnect sk 4391962 xt pic uhci_hcd xt pic xt pic xt pic nmi loc err mis this system has several shared interrupt lines irq is used for the serial ata and ieee controllers irq has several devices including an ide controller two usb controllers an ethernet interface and a sound card and irq also is used by two usb controllers interrupt driven i o whenever a data transfer to or from the managed hardware might be delayed for any reason the driver writer should implement buffering data buffers help to detach data transmission and reception from the write and read system calls and overall sys tem performance benefits a good buffering mechanism leads to interrupt driven i o in which an input buffer is filled at interrupt time and is emptied by processes that read the device an output buffer is filled by processes that write to the device and is emptied at interrupt time an example of interrupt driven output is the implementation of dev shortprint for interrupt driven data transfer to happen successfully the hardware should be able to generate interrupts with the following semantics for input the device interrupts the processor when new data has arrived and is ready to be retrieved by the system processor the actual actions to perform depend on whether the device uses i o ports memory mapping or dma for output the device delivers an interrupt either when it is ready to accept new data or to acknowledge a successful data transfer memory mapped and dma capable devices usually generate interrupts to tell the system they are done with the buffer the timing relationships between a read or write and the actual arrival of data were introduced in the section blocking and nonblocking operations in chapter 6 a write buffering example we have mentioned the shortprint driver a couple of times now it is time to actually take a look this module implements a very simple output oriented driver for the parallel port it is sufficient however to enable the printing of files if you chose to test this driver out however remember that you must pass the printer a file in a for mat it understands not all printers respond well when given a stream of arbitrary data the shortprint driver maintains a one page circular output buffer when a user space process writes data to the device that data is fed into the buffer but the write method does not actually perform any i o instead the core of looks like this while written count hang out until some buffer space is available space if space if space goto out move data into the buffer if space written count space count written if char buf space up return efault space buf space written space if no output is active make it active flags if flags out written a semaphore controls access to the circular buffer obtains that semaphore just prior to the code fragment above while holding the sema phore it attempts to feed data into the circular buffer the function returns the amount of contiguous space available so there is no need to worry about buffer wraps if that amount is the driver waits until some space is freed it then copies as much data as it can into the buffer once there is data to output must ensure that the data is written to the device the actual writing is done by way of a workqueue function must kick that function off if it is not already running after obtaining a separate spinlock that controls access to variables used on the consumer side of the output buffer including it calls if need be then it just a matter of noting how much data was written to the buffer and returning the function that starts the output process looks like the following static void void if should never happen return set up our missed interrupt timer expires jiffies timeout and get the process going the reality of dealing with hardware is that you can occasionally lose an interrupt from the device when this happens you really do not want your driver to stop for evermore until the system is rebooted that is not a user friendly way of doing things it is far better to realize that an interrupt has been missed pick up the pieces and go on to that end shortprint sets a kernel timer whenever it outputs data to the device if the timer expires we may have missed an interrupt we look at the timer function shortly but for the moment let stick with the main output functionality that is implemented in our workqueue function which as you can see above is scheduled here the core of that function looks like the following flags have we written everything if empty shortp_empty_queue nope write another byte else if somebody waiting maybe wake them up if shortp_out_tail page_size shortp_out_queue flags since we are dealing with the output side shared variables we must obtain the spinlock then we look to see whether there is any more data to send out if not we note that output is no longer active delete the timer and wake up anybody who might have been waiting for the queue to become completely empty this sort of wait is done when the device is closed if instead there remains data to write we call to actually send a byte to the hardware then since we may have freed space in the output buffer we consider waking up any processes waiting to add more data to that buffer we do not perform that wakeup unconditionally however instead we wait until a minimum amount of space is available there is no point in awakening a writer every time we take one byte out of the buffer the cost of awakening the process scheduling it to run and putting it back to sleep is too high for that instead we should wait until that pro cess is able to move a substantial amount of data into the buffer at once this tech nique is common in buffering interrupt driven drivers for completeness here is the code that actually writes the data to the port static void void unsigned char cr inb something happened reset the timer jiffies timeout strobe a byte out to the device shortp_out_tail shortp_out_tail if udelay cr if udelay cr sp_control here we reset the timer to reflect the fact that we have made some progress strobe the byte out to the device and update the circular buffer pointer the workqueue function does not resubmit itself directly so only a single byte will be written to the device at some point the printer will in its slow way consume the byte and become ready for the next one it will then interrupt the processor the interrupt handler used in shortprint is short and simple static int irq void struct regs if return remember the time and farm off the rest to the workqueue function shortp_workqueue return since the parallel port does not require an explicit interrupt acknowledgment all the interrupt handler really needs to do is to tell the kernel to run the workqueue func tion again what if the interrupt never comes the driver code that we have seen thus far would simply come to a halt to keep that from happening we set a timer back a few pages ago the function that is executed when that timer expires is static void unsigned long unused unsigned long flags unsigned char status if return flags status inb sp_status if the printer is still busy we just reset the timer if status status expires jiffies timeout shortp_out_lock flags return otherwise we must have dropped an interrupt shortp_out_lock flags shortp_irq null null if no output is supposed to be active the timer function simply returns this keeps the timer from resubmitting itself when things are being shut down then after tak ing the lock we query the status of the port if it claims to be busy it simply hasn t gotten around to interrupting us yet so we reset the timer and return printers can at times take a very long time to make themselves ready consider the printer that runs out of paper while everybody is gone over a long weekend in such situations there is nothing to do other than to wait patiently until something changes if however the printer claims to be ready we must have missed its interrupt in that case we simply invoke our interrupt handler manually to get the output process moving again the shortprint driver does not support reading from the port instead it behaves like shortint and returns interrupt timing information the implementation of an inter rupt driven read method would be very similar to what we have seen however data from the device would be read into a driver buffer it would be copied out to user space only when a significant amount of data has accumulated in the buffer the full read request has been satisfied or some sort of timeout occurs quick reference these symbols related to interrupt management were introduced in this chapter include linux interrupt h int unsigned int irq handler unsigned long flags const char void void unsigned int irq void calls that register and unregister an interrupt handler include linux irq h h int unsigned int irq unsigned long flags this function available on the and architectures returns a nonzero value if an attempt to allocate the given interrupt line succeeds include asm signal h flags for sa_interrupt requests installation of a fast handler as opposed to a slow one installs a shared handler and the third flag asserts that interrupt timestamps can be used to generate system entropy proc interrupts proc stat filesystem nodes that report information about hardware interrupts and installed handlers unsigned long void int unsigned long functions used by the driver when it has to probe to determine which interrupt line is being used by a device the result of must be passed back to after the interrupt has been generated the return value of is the detected interrupt number int x the possible return values from an interrupt handler indicating whether an actual interrupt from the device was present void int irq void int irq void int irq a driver can enable and disable interrupt reporting if the hardware tries to gen erate an interrupt while interrupts are disabled the interrupt is lost forever a driver using a shared handler must not use these functions void unsigned long flags void unsigned long flags use to disable interrupts on the local processor and remember their previous state the flags can be passed to to restore the previous interrupt state void void void void functions that unconditionally disable and enable interrupts on the current processor while chapter introduced the lowest levels of hardware control this chapter pro vides an overview of the higher level bus architectures a bus is made up of both an electrical interface and a programming interface in this chapter we deal with the programming interface this chapter covers a number of bus architectures however the primary focus is on the kernel functions that access peripheral component interconnect pci peripher als because these days the pci bus is the most commonly used peripheral bus on desktops and bigger computers the bus is the one that is best supported by the ker nel isa is still common for electronic hobbyists and is described later although it is pretty much a bare metal kind of bus and there isn t much to say in addition to what is covered in chapters and the pci interface although many computer users think of pci as a way of laying out electrical wires it is actually a complete set of specifications defining how different parts of a computer should interact the pci specification covers most issues related to computer interfaces we are not going to cover it all here in this section we are mainly concerned with how a pci driver can find its hardware and gain access to it the probing techniques discussed in the sections module parameters in chapter and autodetecting the irq number in chapter can be used with pci devices but the specification offers an alternative that is preferable to probing the pci architecture was designed as a replacement for the isa standard with three main goals to get better performance when transferring data between the computer and its peripherals to be as platform independent as possible and to simplify add ing and removing peripherals to the system the pci bus achieves better performance by using a higher clock rate than isa its clock runs at or mhz its actual rate being a factor of the system clock and mhz and even mhz implementations have recently been deployed as well moreover it is equipped with a bit data bus and a bit extension has been included in the specification platform independence is often a goal in the design of a computer bus and it an especially important feature of pci because the pc world has always been dominated by processor specific interface standards pci is cur rently used extensively on ia alpha powerpc and ia systems and some other platforms as well what is most relevant to the driver writer however is pci support for autodetec tion of interface boards pci devices are jumperless unlike most older peripherals and are automatically configured at boot time then the device driver must be able to access configuration information in the device in order to complete initialization this happens without the need to perform any probing pci addressing each pci peripheral is identified by a bus number a device number and a function number the pci specification permits a single system to host up to buses but because buses are not sufficient for many large systems linux now supports pci domains each pci domain can host up to buses each bus hosts up to devices and each device can be a multifunction board such as an audio device with an accompanying cd rom drive with a maximum of eight functions therefore each function can be identified at hardware level by a bit address or key device drivers written for linux though don t need to deal with those binary addresses because they use a specific data structure called to act on the devices most recent workstations feature at least two pci buses plugging more than one bus in a single system is accomplished by means of bridges special purpose pci peripher als whose task is joining two buses the overall layout of a pci system is a tree where each bus is connected to an upper layer bus up to bus at the root of the tree the cardbus pc card system is also connected to the pci system via bridges a typical pci system is represented in figure where the various bridges are highlighted the bit hardware addresses associated with pci peripherals although mostly hid den in the struct object are still visible occasionally especially when lists of devices are being used one such situation is the output of lspci part of the pciutils package available with most distributions and the layout of information in proc pci and proc bus pci the sysfs representation of pci devices also shows this addressing scheme with the addition of the pci domain information when the hardware address is displayed it can be shown as two values an bit bus number and an bit some architectures also display the pci domain information in the proc pci and proc bus pci files figure layout of a typical pci system device and function number as three values bus device and function or as four values domain bus device and function all the values are usually displayed in hexadecimal for example proc bus pci devices uses a single bit field to ease parsing and sort ing while proc bus busnumber splits the address into three fields the following shows how those addresses appear showing only the beginning of the output lines lspci cut d host bridge ram memory ram memory usb controller multimedia audio controller bridge isa bridge usb controller usb controller usb controller cardbus bridge ide interface ethernet controller network controller firewire ieee vga compatible controller cat proc bus pci devices cut 0020 0049 0080 tree sys bus pci devices sys bus pci devices devices devices devices devices devices devices devices 00 00 00 0 devices 00 00 0 00 devices 00 00 1 00 devices 00 00 00 0 devices 00 00 0 00 0 devices 00 00 0 00 0 devices 00 00 0 00 0 devices 00 00 0 00 0 devices 00 00 0 00 0 devices 00 00 0 all three lists of devices are sorted in the same order since lspci uses the proc files as its source of information taking the vga video controller as an example means 00 14 0 when split into domain bits bus bits device bits and function bits the hardware circuitry of each peripheral board answers queries pertaining to three address spaces memory locations i o ports and configuration registers the first two address spaces are shared by all the devices on the same pci bus i e when you access a memory location all the devices on that pci bus see the bus cycle at the same time the configuration space on the other hand exploits geographical addressing configuration queries address only one slot at a time so they never collide as far as the driver is concerned memory and i o regions are accessed in the usual ways via inb readb and so forth configuration transactions on the other hand are performed by calling specific kernel functions to access configuration registers with regard to interrupts every pci slot has four interrupt pins and each device function can use one of them without being concerned about how those pins are routed to the cpu such routing is the responsibility of the computer platform and is imple mented outside of the pci bus since the pci specification requires interrupt lines to be shareable even a processor with a limited number of irq lines such as the can host many pci interface boards each with four interrupt pins the i o space in a pci bus uses a bit address bus leading to gb of i o ports while the memory space can be accessed with either bit or bit addresses bit addresses are available on more recent platforms addresses are supposed to be unique to one device but software may erroneously configure two devices to the same address making it impossible to access either one but this problem never occurs unless a driver is willingly playing with registers it shouldn t touch the good news is that every memory and i o address region offered by the interface board can be remapped by means of configuration transactions that is the firmware initial izes pci hardware at system boot mapping each region to a different address to avoid collisions the addresses to which these regions are currently mapped can be read from the configuration space so the linux driver can access its devices without probing after reading the configuration registers the driver can safely access its hardware the pci configuration space consists of bytes for each device function except for pci express devices which have 4 kb of configuration space for each function and the layout of the configuration registers is standardized four bytes of the config uration space hold a unique function id so the driver can identify its device by look ing for the specific id for that peripheral in summary each device board is geographically addressed to retrieve its configuration registers the information in those registers can then be used to perform normal i o access without the need for further geographic addressing it should be clear from this description that the main innovation of the pci interface standard over isa is the configuration address space therefore in addition to the usual driver code a pci driver needs the ability to access the configuration space in order to save itself from risky probing tasks for the remainder of this chapter we use the word device to refer to a device func tion because each function in a multifunction board acts as an independent entity when we refer to a device we mean the tuple domain number bus number device number and function number boot time to see how pci works we start from system boot since that when the devices are configured actually that configuration is not restricted to the time the system boots hotpluggable devices for example cannot be available at boot time and appear later instead the main point here is that the device driver must not change the address of i o or memory regions you ll find the id of any device in its own hardware manual a list is included in the file pci ids part of the pciutils package and the kernel sources it doesn t pretend to be complete but just lists the most renowned vendors and devices the kernel version of this file will not be included in future kernel series when power is applied to a pci device the hardware remains inactive in other words the device responds only to configuration transactions at power on the device has no memory and no i o ports mapped in the computer address space every other device specific feature such as interrupt reporting is disabled as well fortunately every pci motherboard is equipped with pci aware firmware called the bios nvram or prom depending on the platform the firmware offers access to the device configuration address space by reading and writing registers in the pci controller at system boot the firmware or the linux kernel if so configured performs config uration transactions with every pci peripheral in order to allocate a safe place for each address region it offers by the time a device driver accesses the device its mem ory and i o regions have already been mapped into the processor address space the driver can change this default assignment but it never needs to do that as suggested the user can look at the pci device list and the devices configuration registers by reading proc bus pci devices and proc bus pci the former is a text file with hexadecimal device information and the latter are binary files that report a snapshot of the configuration registers of each device one file per device the indi vidual pci device directories in the sysfs tree can be found in sys bus pci devices a pci device directory contains a number of different files tree sys bus pci devices 00 0 sys bus pci devices 00 0 class config device irq power state resource vendor the file config is a binary file that allows the raw pci config information to be read from the device just like the proc bus pci provides the files vendor device and class all refer to the specific values of this pci device all pci devices provide this information the file irq shows the current irq assigned to this pci device and the file resource shows the current memory resources allocated by this device configuration registers and initialization in this section we look at the configuration registers that pci devices contain all pci devices feature at least a byte address space the first bytes are standard ized while the rest are device dependent figure shows the layout of the device independent configuration space vendor id device id command reg status reg revis ion id class code cache line latency timer header type bist expansion rom base address rese rved irq line irq pin figure the standardized pci configuration registers as the figure shows some of the pci configuration registers are required and some are optional every pci device must contain meaningful values in the required regis ters whereas the contents of the optional registers depend on the actual capabilities of the peripheral the optional fields are not used unless the contents of the required fields indicate that they are valid thus the required fields assert the board capabil ities including whether the other fields are usable it interesting to note that the pci registers are always little endian although the standard is designed to be architecture independent the pci designers sometimes show a slight bias toward the pc environment the driver writer should be careful about byte ordering when accessing multibyte configuration registers code that works on the pc might not work on other platforms the linux developers have taken care of the byte ordering problem see the next section accessing the config uration space but the issue must be kept in mind if you ever need to convert data from host order to pci order or vice versa you can resort to the functions defined in asm byteorder h introduced in chapter knowing that pci byte order is little endian describing all the configuration items is beyond the scope of this book usually the technical documentation released with each device describes the supported registers what we re interested in is how a driver can look for its device and how it can access the device configuration space three or five pci registers identify a device vendorid deviceid and class are the three that are always used every pci manufacturer assigns proper values to these read only registers and the driver can use them to look for the device additionally the fields subsystem vendorid and subsystem deviceid are sometimes set by the ven dor to further differentiate similar devices let look at these registers in more detail vendorid this bit register identifies a hardware manufacturer for instance every intel device is marked with the same vendor number there is a global regis try of such numbers maintained by the pci special interest group and manu facturers must apply to have a unique number assigned to them deviceid this is another bit register selected by the manufacturer no official registra tion is required for the device id this id is usually paired with the vendor id to make a unique bit identifier for a hardware device we use the word signa ture to refer to the vendor and device id pair a device driver usually relies on the signature to identify its device you can find what value to look for in the hardware manual for the target device class every peripheral device belongs to a class the class register is a bit value whose top bits identify the base class or group for example ethernet and token ring are two classes belonging to the network group while the serial and parallel classes belong to the communication group some driv ers can support several similar devices each of them featuring a different signa ture but all belonging to the same class these drivers can rely on the class register to identify their peripherals as shown later subsystem vendorid subsystem deviceid these fields can be used for further identification of a device if the chip is a generic interface chip to a local onboard bus it is often used in several com pletely different roles and the driver must identify the actual device it is talking with the subsystem identifiers are used to this end using these different identifiers a pci driver can tell the kernel what kind of devices it supports the struct structure is used to define a list of the different types of pci devices that a driver supports this structure contains the following fields vendor device these specify the pci vendor and device ids of a device if a driver can handle any vendor or device id the value should be used for these fields subvendor subdevice these specify the pci subsystem vendor and subsystem device ids of a device if a driver can handle any type of subsystem id the value should be used for these fields class these two values allow the driver to specify that it supports a type of pci class device the different classes of pci devices a vga controller is one example are described in the pci specification if a driver can handle any type of sub system id the value should be used for these fields this value is not used to match a device but is used to hold information that the pci driver can use to differentiate between different devices if it wants to there are two helper macros that should be used to initialize a struct structure vendor device this creates a struct that matches only the specific vendor and device id the macro sets the subvendor and subdevice fields of the structure to this creates a struct that matches a specific pci class an example of using these macros to define the type of devices a driver supports can be found in the following kernel files drivers usb host ehci hcd c static const struct handle any usb 0 ehci controller pci_class_serial_usb 0 unsigned long end all zeroes drivers busses c static struct pci_vendor_id_intel pci_device pci_vendor_id_intel 0 these examples create a list of struct structures with an empty struc ture set to all zeros as the last value in the list this array of ids is used in the struct described below and it is also used to tell user space which devices this specific driver supports this structure needs to be exported to user space to allow the hotplug and module loading systems know what module works with what hardware devices the macro accomplishes this an example is pci this statement creates a local variable called that points to the list of struct later in the kernel build process the depmod pro gram searches all modules for the symbol if that symbol is found it pulls the data out of the module and adds it to the file lib modules modules pcimap after depmod completes all pci devices that are supported by modules in the kernel are listed along with their module names in that file when the kernel tells the hotplug system that a new pci device has been found the hotplug system uses the modules pcimap file to find the proper driver to load registering a pci driver the main structure that all pci drivers must create in order to be registered with the kernel properly is the struct structure this structure consists of a num ber of function callbacks and variables that describe the pci driver to the pci core here are the fields in this structure that a pci driver needs to be aware of const char name the name of the driver it must be unique among all pci drivers in the kernel and is normally set to the same name as the module name of the driver it shows up in sysfs under sys bus pci drivers when the driver is in the kernel const struct pointer to the struct table described earlier in this chapter int probe struct dev const struct id pointer to the probe function in the pci driver this function is called by the pci core when it has a struct that it thinks this driver wants to control a pointer to the struct that the pci core used to make this decision is also passed to this function if the pci driver claims the struct that is passed to it it should initialize the device properly and return 0 if the driver does not want to claim the device or an error occurs it should return a negative error value more details about this function follow later in this chapter void remove struct dev pointer to the function that the pci core calls when the struct is being removed from the system or when the pci driver is being unloaded from the kernel more details about this function follow later in this chapter int suspend struct dev state pointer to the function that the pci core calls when the struct is being suspended the suspend state is passed in the state variable this function is optional a driver does not have to provide it int resume struct dev pointer to the function that the pci core calls when the struct is being resumed it is always called after suspend has been called this function is optional a driver does not have to provide it in summary to create a proper struct structure only four fields need to be initialized static struct name ids probe probe remove remove to register the struct with the pci core a call to is made with a pointer to the struct this is traditionally done in the mod ule initialization code for the pci driver static int void return note that the function either returns a negative error number or 0 if everything was registered successfully it does not return the number of devices that were bound to the driver or an error number if no devices were bound to the driver this is a change from kernels prior to the 6 release and was done because of the following situations on systems that support pci hotplug or cardbus systems a pci device can appear or disappear at any point in time it is helpful if drivers can be loaded before the device appears to reduce the time it takes to initialize a device the 6 kernel allows new pci ids to be dynamically allocated to a driver after it has been loaded this is done through the file that is created in all pci driver directories in sysfs this is very useful if a new device is being used that the kernel doesn t know about just yet a user can write the pci id values to the file and then the driver binds to the new device if a driver was not allowed to load until a device was present in the system this interface would not be able to work when the pci driver is to be unloaded the struct needs to be unregis tered from the kernel this is done with a call to when this call happens any pci devices that were currently bound to this driver are removed and the remove function for this pci driver is called before the func tion returns static void void old style pci probing in older kernel versions the function was not always used by pci drivers instead they would either walk the list of pci devices in the system by hand or they would call a function that could search for a specific pci device the ability to walk the list of pci devices in the system within a driver has been removed from the 2 6 kernel in order to prevent drivers from crashing the kernel if they hap pened to modify the pci device lists while a device was being removed at the same time if the ability to find a specific pci device is really needed the following functions are available struct unsigned int vendor unsigned int device struct from this function scans the list of pci devices currently present in the system and if the input arguments match the specified vendor and device ids it increments the reference count on the struct variable found and returns it to the caller this prevents the structure from disappearing without any notice and ensures that the kernel does not oops after the driver is done with the struct returned by the function it must call the function to decre ment the usage count properly back to allow the kernel to clean up the device if it is removed the from argument is used to get hold of multiple devices with the same signa ture the argument should point to the last device that has been found so that the search can continue instead of restarting from the head of the list to find the first device from is specified as null if no further device is found null is returned an example of how to use this function properly is struct dev dev null if dev use the pci device dev this function can not be called from interrupt context if it is a warning is printed out to the system log struct unsigned int vendor unsigned int device unsigned int unsigned int struct from this function works just like but it allows the subsystem vendor and subsystem device ids to be specified when looking for the device this function can not be called from interrupt context if it is a warning is printed out to the system log struct struct bus unsigned int devfn this function searches the list of pci devices in the system on the specified struct for the specified device and function number of the pci device if a device is found that matches its reference count is incremented and a pointer to it is returned when the caller is finished accessing the struct it must call all of these functions can not be called from interrupt context if they are a warning is printed out to the system log enabling the pci device in the probe function for the pci driver before the driver can access any device resource i o region or interrupt of the pci device the driver must call the function int struct dev this function actually enables the device it wakes up the device and in some cases also assigns its interrupt line and i o regions this happens for example with cardbus devices which have been made completely equivalent to pci at the driver level accessing the configuration space after the driver has detected the device it usually needs to read from or write to the three address spaces memory port and configuration in particular accessing the configuration space is vital to the driver because it is the only way it can find out where the device is mapped in memory and in the i o space because the microprocessor has no way to access the configuration space directly the computer vendor has to provide a way to do it to access configuration space the cpu must write and read registers in the pci controller but the exact implemen tation is vendor dependent and not relevant to this discussion because linux offers a standard interface to access the configuration space as far as the driver is concerned the configuration space can be accessed through bit bit or bit data transfers the relevant functions are prototyped in linux pci h int struct dev int where val int struct dev int where val int struct dev int where val read one two or four bytes from the configuration space of the device identi fied by dev the where argument is the byte offset from the beginning of the con figuration space the value fetched from the configuration space is returned through the val pointer and the return value of the functions is an error code the word and dword functions convert the value just read from little endian to the native byte order of the processor so you need not deal with byte ordering int struct dev int where val int struct dev int where val int struct dev int where val write one two or four bytes to the configuration space the device is identified by dev as usual and the value being written is passed as val the word and dword functions convert the value to little endian before writing to the periph eral device all of the previous functions are implemented as inline functions that really call the following functions feel free to use these functions instead of the above in case the driver does not have access to a struct at any paticular moment in time int struct bus unsigned int devfn int where val int struct bus unsigned int devfn int where val int struct bus unsigned int devfn int where val just like the functions but struct and devfn variables are needed instead of a struct int struct bus unsigned int devfn int where val int struct bus unsigned int devfn int where val int struct bus unsigned int devfn int where val just like the functions but struct and devfn variables are needed instead of a struct the best way to address the configuration variables using the functions is by means of the symbolic names defined in linux pci h for example the follow ing small function retrieves the revision id of a device by passing the symbolic name for where to static unsigned char struct dev revision dev revision return revision accessing the i o and memory spaces a pci device implements up to six i o address regions each region consists of either memory or i o locations most devices implement their i o registers in memory regions because it generally a saner approach as explained in the section i o ports and i o memory in chapter however unlike normal memory i o regis ters should not be cached by the cpu because each access can have side effects the pci device that implements i o registers as a memory region marks the difference by setting a memory is prefetchable bit in its configuration register if the memory region is marked as prefetchable the cpu can cache its contents and do all sorts of optimization with it nonprefetchable memory access on the other hand can t be optimized because each access can have side effects just as with i o ports peripher als that map their control registers to a memory address range declare that range as nonprefetchable whereas something like video memory on pci boards is prefetch able in this section we use the word region to refer to a generic i o address space that is memory mapped or port mapped an interface board reports the size and current location of its regions using configura tion registers the six bit registers shown in figure 2 whose symbolic names are through since the i o space defined by pci is a bit address space it makes sense to use the same configuration interface the information lives in one of the low order bits of the base address pci registers the bits are defined in linux pci h for memory and i o if the device uses a bit address bus it can declare regions in the bit memory space by using two consecutive registers for each region low bits first it is possible for one device to offer both bit regions and bit regions in the kernel the i o regions of pci devices have been integrated into the generic resource management for this reason you don t need to access the configuration variables in order to know where your device is mapped in memory or i o space the preferred interface for getting region information consists of the following functions unsigned long struct dev int bar the function returns the first address memory address or i o port number associated with one of the six pci i o regions the region is selected by the inte ger bar the base address register ranging from 0 5 inclusive unsigned long struct dev int bar the function returns the last address that is part of the i o region number bar note that this is the last usable address not the first address after the region unsigned long struct dev int bar this function returns the flags associated with this resource resource flags are used to define some features of the individual resource for pci resources associated with pci i o regions the information is extracted from the base address registers but can come from elsewhere for resources not associated with pci devices all resource flags are defined in linux ioport h the most important are if the associated i o region exists one and only one of these flags is set these flags tell whether a memory region is prefetchable and or write protected the latter flag is never set for pci resources by making use of the functions a device driver can completely ignore the underlying pci registers since the system already used them to structure resource information pci interrupts as far as interrupts are concerned pci is easy to handle by the time linux boots the computer firmware has already assigned a unique interrupt number to the device and the driver just needs to use it the interrupt number is stored in configu ration register which is one byte wide this allows for as many as interrupt lines but the actual limit depends on the cpu being used the driver doesn t need to bother checking the interrupt number because the value found in is guaranteed to be the right one if the device doesn t support interrupts register is 0 other wise it nonzero however since the driver knows if its device is interrupt driven or not it doesn t usually need to read thus pci specific code for dealing with interrupts just needs to read the configura tion byte to obtain the interrupt number that is saved in a local variable as shown in the following code beyond that the information in chapter applies result dev myirq if result deal with error the rest of this section provides additional information for the curious reader but isn t needed for writing drivers a pci connector has four interrupt pins and peripheral boards can use any or all of them each pin is individually routed to the motherboard interrupt controller so interrupts can be shared without any electrical problems the interrupt controller is then responsible for mapping the interrupt wires pins to the processor hardware this platform dependent operation is left to the controller in order to achieve plat form independence in the bus itself the read only configuration register located at is used to tell the computer which single pin is actually used it worth remembering that each device board can host up to eight devices each device uses a single interrupt pin and reports it in its own configuration register different devices on the same device board can use different interrupt pins or share the same one the register on the other hand is read write when the com puter is booted the firmware scans its pci devices and sets the register for each device according to how the interrupt pin is routed for its pci slot the value is assigned by the firmware because only the firmware knows how the motherboard routes the different interrupt pins to the processor for the device driver however the register is read only interestingly recent versions of the linux kernel under some circumstances can assign interrupt lines without resorting to the bios hardware abstractions we complete the discussion of pci by taking a quick look at how the system han dles the plethora of pci controllers available on the marketplace this is just an informational section meant to show the curious reader how the object oriented lay out of the kernel extends down to the lowest levels the mechanism used to implement hardware abstraction is the usual structure con taining methods it a powerful technique that adds just the minimal overhead of dereferencing a pointer to the normal overhead of a function call in the case of pci management the only hardware dependent operations are the ones that read and write configuration registers because everything else in the pci world is accom plished by directly reading and writing the i o and memory address spaces and those are under direct control of the cpu thus the relevant structure for configuration register access includes only two fields struct int read struct bus unsigned int devfn int where int size val int write struct bus unsigned int devfn int where int size val the structure is defined in linux pci h and used by drivers pci pci c where the actual public functions are defined the two functions that act on the pci configuration space have more overhead than dereferencing a pointer they use cascading pointers due to the high object orientedness of the code but the overhead is not an issue in operations that are performed quite rarely and never in speed critical paths the actual implementa tion of dev where val for instance expands to dev bus ops read bus devfn where val the various pci buses in the system are detected at system boot and that when the struct items are created and associated with their features including the ops field implementing hardware abstraction via hardware operations data structures is typ ical in the linux kernel one important example is the struct data structure it is defined in asm alpha machvec h and takes care of everything that may change across different alpha based computers a look back isa the isa bus is quite old in design and is a notoriously poor performer but it still holds a good part of the market for extension devices if speed is not important and you want to support old motherboards an isa implementation is preferable to pci an additional advantage of this old standard is that if you are an electronic hobbyist you can easily build your own isa devices something definitely not possible with pci on the other hand a great disadvantage of isa is that it tightly bound to the pc architecture the interface bus has all the limitations of the processor and causes endless pain to system programmers the other great problem with the isa design inherited from the original ibm pc is the lack of geographical addressing which has led to many problems and lengthy unplug rejumper plug test cycles to add new devices it interesting to note that even the oldest apple ii computers were already exploiting geographical addressing and they featured jumperless expansion boards despite its great disadvantages isa is still used in several unexpected places for example the series of mips processors used in several palmtops features an isa compatible expansion bus strange as it seems the reason behind these unex pected uses of isa is the extreme low cost of some legacy hardware such as based ethernet cards so a cpu with isa electrical signaling can easily exploit the awful but cheap pc devices hardware resources an isa device can be equipped with i o ports memory areas and interrupt lines even though the processors support kb of i o port memory i e the proces sor asserts address lines some old pc hardware decodes only the lowest address lines this limits the usable address space to ports because any address in the range 1 kb to kb is mistaken for a low address by any device that decodes only the low address lines some peripherals circumvent this limitation by mapping only one port into the low kilobyte and using the high address lines to select between different device registers for example a device mapped at can safely use port and so on if the availability of i o ports is limited memory access is still worse an isa device can use only the memory range between kb and 1 mb and between mb and mb for i o register and device control the kb to 1 mb range is used by the pc bios by vga compatible video boards and by various other devices leaving lit tle space available for new devices memory at mb on the other hand is not directly supported by linux and hacking the kernel to support it is a waste of pro gramming time nowadays the third resource available to isa device boards is interrupt lines a limited num ber of interrupt lines is routed to the isa bus and they are shared by all the interface boards as a result if devices aren t properly configured they can find themselves using the same interrupt lines although the original isa specification doesn t allow interrupt sharing across devices most device boards allow it interrupt sharing at the software level is described in the section interrupt sharing in chapter isa programming as far as programming is concerned there no specific aid in the kernel or the bios to ease access to isa devices like there is for example for pci the only facilities you can use are the registries of i o ports and irq lines described in the section installing an interrupt handler in chapter the programming techniques shown throughout the first part of this book apply to isa devices the driver can probe for i o ports and the interrupt line must be auto detected with one of the techniques shown in the section autodetecting the irq number in chapter the helper functions and friends have been briefly introduced in the sec tion using i o memory in chapter and there nothing more to say about them the plug and play specification some new isa device boards follow peculiar design rules and require a special initial ization sequence intended to simplify installation and configuration of add on inter face boards the specification for the design of these boards is called plug and play pnp and consists of a cumbersome rule set for building and configuring jumperless isa devices pnp devices implement relocatable i o regions the pc bios is respon sible for the relocation reminiscent of pci in short the goal of pnp is to obtain the same flexibility found in pci devices with out changing the underlying electrical interface the isa bus to this end the specs define a set of device independent configuration registers and a way to geographi cally address the interface boards even though the physical bus doesn t carry per board geographical wiring every isa signal line connects to every available slot geographical addressing works by assigning a small integer called the card select number csn to each pnp peripheral in the computer each pnp device features a unique serial identifier bits wide that is hardwired into the peripheral board csn assignment uses the unique serial number to identify the pnp devices but the csns can be assigned safely only at boot time which requires the bios to be pnp the problem with interrupt sharing is a matter of electrical engineering if a device drives the signal line inac tive by applying a low impedance voltage level the interrupt can t be shared if on the other hand the device uses a pull up resistor to the inactive logic level sharing is possible this is the norm nowadays how ever there still a potential risk of losing interrupt events since isa interrupts are edge triggered instead of level triggered edge triggered interrupts are easier to implement in hardware but don t lend themselves to safe sharing aware for this reason old computers require the user to obtain and insert a specific configuration diskette even if the device is pnp capable interface boards following the pnp specs are complicated at the hardware level they are much more elaborate than pci boards and require complex software it not unusual to have difficulty installing these devices and even if the installation goes well you still face the performance constraints and the limited i o space of the isa bus it much better to install pci devices whenever possible and enjoy the new technology instead if you are interested in the pnp configuration software you can browse drivers net c whose probing function deals with pnp devices the 2 6 kernel saw a lot of work in the pnp device support area so a lot of the inflexible interfaces have been cleaned up compared to previous kernel releases pc and pc currently in the industrial world two bus architectures are quite fashionable pc and pc both are standard in pc class single board computers both standards refer to specific form factors for printed circuit boards as well as electrical mechanical specifications for board interconnections the practical advan tage of these buses is that they allow circuit boards to be stacked vertically using a plug and socket kind of connector on one side of the device the electrical and logical layout of the two buses is identical to isa pc and pci pc so software won t notice any difference between the usual desktop buses and these two other pc buses pci and isa are the most commonly used peripheral interfaces in the pc world but they aren t the only ones here a summary of the features of other buses found in the pc market mca micro channel architecture mca is an ibm standard used in ps 2 computers and some laptops at the hardware level micro channel has more features than isa it supports multimaster dma bit address and data lines shared interrupt lines and geographical addressing to access per board configuration registers such registers are called programmable option select pos but they don t have all the features of the pci registers linux support for micro channel includes functions that are exported to modules a device driver can read the integer value to see if it is running on a micro channel computer if the symbol is a preprocessor macro the macro macro is defined as well if is undefined then is an inte ger variable exported to modularized code both and are defined in asm processor h eisa the extended isa eisa bus is a bit extension to isa with a compatible inter face connector isa device boards can be plugged into an eisa connector the addi tional wires are routed under the isa contacts like pci and mca the eisa bus is designed to host jumperless devices and it has the same features as mca bit address and data lines multimaster dma and shared interrupt lines eisa devices are configured by software but they don t need any particular operating system support eisa drivers already exist in the linux ker nel for ethernet devices and scsi controllers an eisa driver checks the value to determine if the host computer carries an eisa bus like is either a macro or a variable depending on whether is defined both symbols are defined in asm processor h the kernel has full eisa support for devices with sysfs and resource management functionality this is located in the drivers eisa directory vlb another extension to isa is the vesa local bus vlb interface bus which extends the isa connectors by adding a third lengthwise slot a device can just plug into this extra connector without plugging in the two associated isa connectors because the vlb slot duplicates all important signals from the isa connectors such standal one vlb peripherals not using the isa slot are rare because most devices need to reach the back panel so that their external connectors are available the vesa bus is much more limited in its capabilities than the eisa mca and pci buses and is disappearing from the market no special kernel support exists for vlb however both the lance ethernet driver and the ide disk driver in linux 2 0 can deal with vlb versions of their devices sbus while most computers nowadays are equipped with a pci or isa interface bus most older sparc based workstations use sbus to connect their peripherals sbus is quite an advanced design although it has been around for a long time it is meant to be processor independent even though only sparc computers use it and is optimized for i o peripheral boards in other words you can t plug additional ram into sbus slots ram expansion boards have long been forgotten even in the isa world and pci does not support them either this optimization is meant to simplify the design of both hardware devices and system software at the expense of some additional complexity in the motherboard this i o bias of the bus results in peripherals using virtual addresses to transfer data thus bypassing the need to allocate a contiguous dma buffer the motherboard is responsible for decoding the virtual addresses and mapping them to physical addresses this requires attaching an mmu memory management unit to the bus the chipset in charge of the task is called iommu although somehow more com plex than using physical addresses on the interface bus this design is greatly simpli fied by the fact that sparc processors have always been designed by keeping the mmu core separate from the cpu core either physically or at least conceptually actually this design choice is shared by other smart processor designs and is benefi cial overall another feature of this bus is that device boards exploit massive geo graphical addressing so there no need to implement an address decoder in every peripheral or to deal with address conflicts sbus peripherals use the forth language in their proms to initialize themselves forth was chosen because the interpreter is lightweight and therefore can be easily implemented in the firmware of any computer system in addition the sbus specifi cation outlines the boot process so that compliant i o devices fit easily into the sys tem and are recognized at system boot this was a great step to support multi platform devices it a completely different world from the pc centric isa stuff we were used to however it didn t succeed for a variety of commercial reasons although current kernel versions offer quite full featured support for sbus devices the bus is used so little nowadays that it not worth covering in detail here inter ested readers can look at source files in arch sparc kernel and arch sparc mm nubus another interesting but nearly forgotten interface bus is nubus it is found on older mac computers those with the family of cpus all of the bus is memory mapped like everything with the and the devices are only geographically addressed this is good and typical of apple as the much older apple ii already had a similar bus layout what is bad is that it almost impos sible to find documentation on nubus due to the close everything policy apple has always followed with its mac computers and unlike the previous apple ii whose source code and schematics were available at little cost the file drivers nubus nubus c includes almost everything we know about this bus and it interesting reading it shows how much hard reverse engineering developers had to do external buses one of the most recent entries in the field of interface buses is the whole class of external buses this includes usb firewire and parallel port based external bus these interfaces are somewhat similar to older and not so external technology such as pcmcia cardbus and even scsi conceptually these buses are neither full featured interface buses like pci is nor dumb communication channels like the serial ports are it hard to classify the software that is needed to exploit their features as it usually split into two levels the driver for the hardware controller like drivers for pci scsi adaptors or pci con trollers introduced in the section the pci interface and the driver for the specific client device like sd c handles generic scsi disks and so called pci drivers deal with cards plugged in the bus quick reference this section summarizes the symbols introduced in the chapter include linux pci h header that includes symbolic names for the pci registers and several vendor and device id values struct structure that represents a pci device within the kernel struct structure that represents a pci driver all pci drivers must define this struct structure that describes the types of pci devices this driver supports int struct drv int struct drv void struct drv functions that register or unregister a pci driver from the kernel struct unsigned int vendor unsigned int device struct from struct unsigned int vendor unsigned int device const struct from struct unsigned int vendor unsigned int device unsigned int unsigned int const struct from struct unsigned int class struct from functions that search the device list for devices with a specific signature or those belonging to a specific class the return value is null if none is found from is used to continue a search it must be null the first time you call either function and it must point to the device just found if you are searching for more devices these functions are not recommended to be used use the variants instead struct unsigned int vendor unsigned int device struct from struct unsigned int vendor unsigned int device unsigned int unsigned int struct from struct struct bus unsigned int devfn functions that search the device list for devices with a specific signature or belonging to a specific class the return value is null if none is found from is used to continue a search it must be null the first time you call either function and it must point to the device just found if you are searching for more devices the structure returned has its reference count incremented and after the caller is finished with it the function must be called int struct dev int where val int struct dev int where val int struct dev int where val int struct dev int where val int struct dev int where val int struct dev int where val functions that read or write a pci configuration register although the linux kernel takes care of byte ordering the programmer must be careful about byte ordering when assembling multibyte values from individual bytes the pci bus is little endian int struct dev enables a pci device unsigned long struct dev int bar unsigned long struct dev int bar unsigned long struct dev int bar functions that handle pci device resources programming principles and practice term fall syllabus index description prerequisites scheduling website instructor information course objectives student evaluation texts and library materials topic outline lectureschedule instructional laboratory assignments laboratory resources policies description the purpose of this course is to broaden the student view of software development topics include an scripting languages libraries and tools and techniques for program development and maintenance a theme for this course is quality programming in the small the idea is to teach the tools and techniques to write a quality component whether it is a stand alone program to do some small task or it is to be a part of a larger project these are fundamental principles skills required by professional programmers the university course calendar description of the class is as follows a hands on approach to software development at the individual and small team level application of software tools including scripting languages system utilities and libraries for construction of small software systems integrated with and motivated by programming practices system development testing and maintenance issues prerequisites or and math scheduling class day time class location tuesday and thursday a m to p m arts class duration september through december midterm exam in class october the last day for withdrawls is november labs section fridays section tuesdays section thursdays lab location all tutorials are held in room in the spinks addition of the thorvaldson building labs start week of september guest lecture the instructor will be at a conference on september and the class on that day will be taught by a guest lecturer website the website for this course is on the moodle server of the department of computer science the url is https moodle cs usask ca course view php id finally remember that if you need help e mail works hours a day and you ll probably even get a response in short order alternatively post something to one of the class forums on moodle teaching assistant scott johnston mail usask ca course objectives by the completion of this course students will be expected to be familiar with many common unix linux commands understand fundamental concepts regarding operating systems version control systems the software built process via compilation and linking portable representation of characters representation and storage of program variables in memory unix processes and the unix file system by the completion of this course students will be expected to be able to skillfully use complex unix linux shell commands write non trivial scripts in unix shell bash and awk search files using the grep command and regular expressions effectively utilize test driven design to write non trivial programs effectively use svn commands to manage the evolution of a non trivial program decompose a large program into cohesive separately compiled modules and to manage the compilation and linking of those modules into a single executable using a makefile use a line oriented c c debugger to examine the execution of a program and identify bugs in the code use more sophisticated debugging and testing techniques than used in first year build and use an object module library recognize and use improved styles of programming note that the above two lists are not exclusive course syllabus cmpt operating systems principles catalogue description an introduction to the principles of modern operating systems the synchronization and communication of cooperating processes process scheduling virtual memory file system design and organization introduction to distributed systems course objectives after completing this course students should be able to do the following tasks demonstrate and illustrate how application software accesses computer hardware through the abstrac tions provided by the operating system and how the operating system shares hardware resources between processes tasks threads and users utilize system library functions robustly in the implementation of applications that access operating system facilities correctly abstract operations through the use of application programmer interfaces and virtual function interfaces design implement and document system level software in a small team environment demonstrate the operation of well known theoretical algorithms with respect to deadlock process and disk scheduling and memory management illustrate the separation of policy and mechanism with examples from operating system design and implementation design algorithms to provide concurrent access to shared resources and implement these algorithms in the following environments in the c programming language unix processes various unix and windows threads packages explain time space and complexity tradeoffs in operating system implementation issues demonstrating their application with approximate solutions for various resource sharing problems modify existing system level source code to add new functionality compare and evaluate options for filesystem implementation and use as well as alternative mechanisms for concurrency control between processes student evaluation grading scheme requirements there will be equally weighted assignments due approximately every weeks a mid term exam held in class and a final examination during the regular examination period exact dates will be announced as the course progresses the approximate weightings for the assignments and examinations are as follows participation including online discussions assignments midterm exam october final exam total criteria that must be met to pass all components of the course must be complete in order to achieve a passing grade in the course this includes both exams and every assignment failure to do so will result in an automatic failure of the course submission of assignments must be a credible attempt to solve the problems in the assignment in the judgment of the marker final exam scheduling the registrar schedules all final examinations including deferred and supplemental examinations students are advised not to make travel arrangements for the exam period until the official exam schedule has been posted note all students must be properly registered in order to attend lectures and receive credit for this course textbook information required text title modern operating systems author andrew s tanenbaum publisher pearson prentice hall edition year isbn edition additional information this book covers more material than we can do in one course we will cover the first chapters with some discussion on later chapters if time permits it is likely that the edition will be sufficient for most students nevertheless you should ensure that you have access to a edition for possible reference to exercises and specific sections in the new textbook recommended texts additional information optional but not necessary for anyone this provides a different point of view title operating systems internals and design principles author william stallings publisher pearson prentice hall edition year isbn edition yet a third option title operating system concepts author abraham silberschatz peter baer galvin and greg gagne publisher john wiley and sons edition year isbn edition essential for everyone to have for their career many should already have this title the c programming language author brian kernighan and dennis ritchie publisher prentice hall edition year isbn lecture schedule topic topic subtopics introduction overview history of operating systems introduction to the com puting environment for this course possible review of c and unix linux readings chapter pp tanenbaum process description concurrency control and inter process communication processes threads and address spaces i o devices synchronization mutual exclusion semaphores monitors inter process communication deadlock examples duration weeks readings chapter section process scheduling types of scheduling uniprocessor scheduling multiprocessor scheduling examples duration weeks readings chapter section memory management address binding virtual memory address translation manage ment policies for demand paged virtual memory examples duration weeks readings chapter i o and file systems organizing the i o function disk architectures raid disk caching file organization secondary storage management pro tection and security examples duration weeks readings chapter and sections and course procedures facilities students will be using the linux windows computers in the spinks lab to do most of their work and are expected to be familiar with the use of these facilities this course will be administered with blackboard those unfamiliar with the use of blackboard are strongly encouraged to make use of the resources available to learn blackboard blackboard has a dis cussion forum and it will be used to disseminate answers to questions regarding assignments lecture material etc many important things may happen there that will not be repeated in class so daily reading of the bulletin board is required the course will make regular use of internet facilities page locations will be given during lectures and on blackboard assignment descriptions will be made available in electronic form only submissions will be in electronic form only part of the evaluation in the course will include participation including participation in the electronic component of the course through blackboard your involvement in this area of the course will be elec tronically tracked on a periodic basis assignments are to be done in groups of at most two students version control using svn at svn cs usask ca is required for every assignment and will be verified by submitting meaningful svn logs final note the purpose of cmpt is to provide a basic understanding of operating systems princi ples the parts of an operating system how they are structured the important policies governing their operation and the implementation issues although examples will be drawn from several operating systems throughout the course it is not the purpose of this course to provide training in any particular operating system the specific operating system that we will be developing code for is unix like in some rudimentary ways but is not in any way required to have the same types of functionality the class will follow the textbook in content but not necessarily organization students will be responsible for reading the text and learning the material in it as much as possible lecture time will be used primarily for highlighting specific material in the text covering supplementary material as required and answering student questions h syllabus edit mode is off syllabus cmpt syllabus course goals and requirements course goals the purpose of cmpt is to provide a deep understanding of operating systems principles the parts of an operating system how they are structured the important policies governing their operation and the implementation issues although examples will be drawn from several operating systems throughout the course it is not the purpose of this course to provide training in any particular operating system this course will have a significant project component lecture time will be used primarily for coverage of basic os implementation issues from material in the texts covering supplementary material as required project design meetings and reviews and answering student questions course objectives after completing this course students should be able to do the following tasks implement system calls in the linux operating system explain design principles influencing the structure of different os paradigms design the components of a simple but complete operating system design and implement an api for user processes to access os facilities implement integrate and document operating system components in a small team environment integrate device drivers and bootloaders into the operating system for communication with peripheral devices compare and evaluate design alternatives for processes threads schedulers and or memory manage ment evaluate research literature in operating systems design and implementation and explain open issues and potential solutions course requirements note requirements and expectations differ slightly between the undergrad course and the grad course there will be or assignments due approximately in late january and mid march respectively a course implementation project due in early april a mid term exam held in class in the beginning part of march and a final examination during the regular examination period exact dates will be announced as the course progresses the approximate weightings for the assignments and examinations are as follows component assignments implementation project research project paper presentation mid term exam final exam resources textbooks title modern operating systems author andrew s tanenbaum publisher pearson prentice hall edition year edition isbn additional information none type required resource title linux kernel development author robert love publisher addison wesley edition year third edition isbn 2946 additional information available as a free e book type recommended resource title advanced programming in the unix environment author w richard stevens stephen a rago publisher addison wesley edition year edition isbn 63773 additional information none type recommended resource title operating systems design and implementation author andrew s tanenbaum and albert s woodhull publisher pearson prentice hall edition year edition isbn type recommended resource course information and policies instructor contact information use of class time this course will be pragmatic and hands on in addition to the theoretical material a significant amount of class time will be spent on design programming and debugging the implementation project policies facilities students will be using the unix time sharing facilities of the spinks lab to do most of their work and are expected to be familiar with the use of these facilities virtual machine environments will be created on a subset of the machines for your kernel programming pleasure other a few miscellaneous points this course will be administered with blackboard the discussion forum will be used to disseminate answers to questions regarding assignments lecture material etc many important things may happen there that will not be repeated in class so reading of the bulletin board is required the course will make regular use of the world wide web and other internet facilities locations will be given during lectures and on blackboard assignment descriptions will be made available in electronic form only submissions will be in electronic form only additional information all students must be properly registered in order to attend lectures and receive credit for this course failure to write the final exam will result in failure of the course plagiarism is strongly forbidden as in all courses we will be checking assignments projects for plagiarism using nifty cheating detection software and penalties will be enforced refer to the university of saskatchewan policy on academic honesty for further details lessons introduction and overview introduction objectives overview topics review of in a nutshell with a little more january pm pm process description and control process description and control topics duration weeks january 00 pm pm booting and boot loaders examining source code of the bootloader for real operating systems to review and understand the architecture and memory structure of the intel processor possible discussion of arm assembly language and process for booting on arm processors duration weeks january 00 pm pm interrupts traps interrupts traps topics low level handling of external events duration weeks january 00 pm pm device drivers device drivers topics interfacing with objects in the real world duration week february 00 pm pm i o and file systems i o and file systems topics file organization secondary storage management examples duration weeks february 00 pm pm ipc and friends ipc and friends topics communicating between processes shared memory etc duration weeks march 00 pm pm network and system administration cmpt syllabus university of saskatchewan term description and learning objectives calendar description this course concerns deployment and maintenence of modern computer systems in an operational environment the course provides both conceptual knowledge and practical experience topics to be covered include architectures heterogeneous systems authentication and security network services including firewalls storage services performance analysis and tuning management and configuration of services and system resources system initialization drivers cross platform services policies and procedures instructor dwight makaroff thorvaldson available via appointment class meeting time tuesday thursday 00 p m room thorvaldson teaching assistant khadija rasul learning objectives upon successful completion of this course students should be able to demonstrate competency at the following tasks manage users files and software on a computer system installation consisting of clients and servers management install and configure networking services for intranet and internet domains networking administer network security policies in linux and windows environments security understand and apply techniques to interoperate computer systems comprised of linux and windows machines interoperability identify potential sources of poor computer performance and evaluate potential solutions performance debugging design small and medium sized business it infrastructure organization capacity planning develop scripting mechanisms and automated scripts for performing complicated administration tasks system scripting evaluate alternative policies and mechanisms for providing reliability features of computer system services and operations backups install and configure linux and windows virtual machines virtualization deploy systems to manage large amounts of data for a wide variety of users data centres resources required textbook the practice of system and network administration ed thomas a limoncelli christina j hogan and strata r chalup addison wesley isbn recommended additional readings resources unix and linux system administration handbook ed evi nemeth garth snyder trent r hein ben whaley prentice hall isbn linux administration a beginners guide ed wale soyinka mcgraw hill isbn essential system administration ed a frisch o reilly isbn tcp ip network administration ed c hunt o reilly isbn microsoft windows server administration essentials tom carpenter sybex isbn introducing windows server rtm edition mitch tulloch with the windows server team microsoft press isbn course topics policies evaluation course topics the topics of discussions assignments and lecture time shall include but not necessarily be limited to the following clusters vs supercomputers vs lans multi tiered server architectures heterogeneous systems authentication and security file servers disk configuration backups network file systems performance analysis and tuning managing system resources devices and drivers configuring and building kernels bootstrapping and system initialization network administration network services cross platform services firewalls proxies routers and name servers policies and procedures policies the course will be structured as informal lecture discussion information will be discussed from the various textbooks and readings with applications to deploying the concepts on systems consisting of resources in a virtual machine environment students will be expected to be prepared to discuss topics and complete exercises in classroom and lab environments the u of s policy on academc honesty will apply for this course the university policy will be the terms of reference for student academic conduct evaluation the course evaluation will consist of assignments a midterm exam components written and lab a term project and a final exam again components written and lab all components are mandatory in order to achieve a passing grade for the course all evaluation components must receive a passing grade to achieve a passing grade in the course assignments there will be approximately assignments consisting of a practical problem in system administration that students will be asked to solve and either write up a report on the solution or demonstrate the solution to the instructor or a teaching assistant outside of class time midterm exam there will be a mid term exam consisting of a written portion where students demonstrate understanding of the concepts behind system and network administration and a lab component where practical solutions will be required in a time limited environment the midterm will be held during class time in late february early march term project each student will propose a research project associated with one or more major topics of the course this could involve literature reviews industry evaluations or technical prototype development and evaluation final exam the final exam will have two components a written exam and a lab exam written during the exam period relative weight of evaluation components assignments midterm written midterm lab term project final exam written final exam lab total administration the course will be administered with the blackboard course tools this will include course notes discussion forums announcements and internal email tentative outline of topics lesson title date review and role of system administrators january limoncelli nemeth and soyinka organization of it systems documentation january limoncelli hardware and software configuration basics january limoncelli nemeth soyinka h w s w user mgmt continued january file systems and disk issues january nemeth soyinka tanenbaum networking intro january limoncelli soyinka nemeth startup and shutdown january nemeth soyinka startup and shutdown part january the kernel january nemeth soyinka observation automation logging and backups february limoncelli nemeth soyinka security introduced february limoncelli security continued nemeth soyinka february ethics february limoncelli local and network services and applications february limoncelli and nemeth soyinka services and applications continued february nemeth soyinka applications and services part march midterm march network connections and management march limoncelli nemeth soyinka network stuff continued march performance analysis and tuning march limoncelli cmpt foundations of concurrent programming catalogue description theory and practice of concurrent programming process interaction using shared variables and message passing parallel computing development of correct programs general problem solving techniques scientific computing distributed programming course objectives provide an introduction to foundational concepts paradigms and language support for concurrent pro gramming student evaluation grading scheme assignments midterm exam final exam total criteria that must be met to pass schedule assignment assigned due assignment assigned due midterm exam assignment assigned due assignment assigned due assignment assigned due midterm and final cumulative marks will be scaled attendence expectation students will be expected to know all information passed on during lectures and tutorials and through the webpage moodle and email if they miss a lecture or tutorial they are responsible for acquiring material covered in the session final exam scheduling the registrar schedules all final examinations including deferred and supplemental examinations students are advised not to make travel arrangements for the exam period until the official exam schedule has been posted note all students must be properly registered in order to attend lectures and receive credit for this course textbook information required text gregory andrews foundations of multithreaded parallel and distributed programming addison wesley recommended texts carlos a varela programming distributed computing systems a foundational approach mit press lecture schedule topics to be covered overview distributed programming message passing rpc and rendezvous paradigms for process interaction implementations shared variable programming processes and synchronization locks and barriers semaphores monitors implementations parallel programming scientific computing languages compilers libraries and tools course overview there will be about tutorials in total at the scheduled time but on dates to be decided later cmpt foundations of concurrent programming catalogue description theory and practice of concurrent programming process interaction using shared variables and message passing parallel computing development of correct programs general problem solving techniques scientific computing distributed programming prerequisite cmpt and cmpt or equivalent course objectives provide an introduction to foundational concepts paradigms language support and research for con current programming student evaluation grading scheme assignments midterm exam paper final exam total criteria that must be met to pass schedule assignment assigned due paper proposal due assignment assigned due midterm exam assignment assigned due assignment assigned due assignment assigned due paper due midterm and final cumulative marks will be scaled attendence expectation students will be expected to know all information passed on during lectures and tutorials and through the webpage moodle and email if they miss a lecture or tutorial they are responsible for acquiring material covered in the session final exam scheduling the registrar schedules all final examinations including deferred and supplemental examinations students are advised not to make travel arrangements for the exam period until the official exam schedule has been posted note all students must be properly registered in order to attend lectures and receive credit for this course textbook information required text gregory andrews foundations of multithreaded parallel and distributed programming addison wesley recommended texts carlos a varela programming distributed computing systems a foundational approach mit press lecture schedule topics to be covered overview distributed programming message passing rpc and rendezvous paradigms for process interaction implementations shared variable programming processes and synchronization locks and barriers semaphores monitors implementations parallel programming scientific computing languages compilers libraries and tools course overview there will be about tutorials in total at the scheduled time but on dates to be decided later foundations of concurrent programming nadeem jamali links web http agents usask ca http agents usask ca news moodle discussion group for cmpt cmpt introduction what you need to know architecture programming unix data structures discrete math plan for today overview details sequential vs concurrent sequential program sequence of actions that produce a result statements variables called a process task or thread of control concurrent program two or more processes that work together communication shared variables or synchronization message passing hardware single processor multiprocessor shared memory multicomputer separate memories network slower communication single processor multiprocessor shared memory multicomputer separate memories multithreaded applications what more than thread usually share cpu time why good way to organize modern software systems os timesharing servers pc windows browser applets user unix pipes this book sed eqn groff parallel applications what processes execute on their own processor why solve a problem faster or solve a larger problem two main algorithm programming styles iterative loops divide them up recursive divide and conquer with calls in parallel see chapters and for examples distributed applications what processes communicate over a network why offload work servers connect to remote data internet airlines banks scalable parallel computing on multicomputers and networks programming paradigms iterative parallelism recursive parallelism producers and consumers clients and servers interacting peers course course contents applications methods principles languages and libraries programming techniques implementations related courses os architecture algorithms various applications course continued course details go over the syllabus for next time skim preface and toc read chapter please note that the topics may be changed basics a different models of computing b virtual machines oo languages java c a core java concepts b core c concepts concurrency a threads b shared memory versus non shared memory communication a sockets non oo languages a erlang servers a java b c c erlang communication ii a serialization b reflection middleware i a corba b rmi middleware ii a remoting b web services service oriented systems resource oriented systems a soa b soap rest cloud computing a iaas b paas c saas mobile devices a native versus mobile web b hybrid policies missed examinations students who have missed an exam or assignment must contact their instructor as soon as possible arrangements to make up the exam may be arranged with the instructor missed exams throughout the year are left up to the discretion of the instructor if a student may make up the exam or write at a different time if a student knows prior to the exam that she he will not be able to attend they should let the instructor know before the exam final exams a student who is absent from a final examination through no fault of his or her own for medical or other valid reasons may apply to the college of arts and science dean office the application must be made within three days of the missed examination along with supporting documentary evidence deferred exams are written during the february mid term break for term courses and in early june for term and full year courses http www arts usask ca students transition tips php incomplete course work and final grades when a student has not completed the required course work which includes any assignment or examination including the final examination by the time of submission of the final grades they may be granted an extension to permit completion of an assignment or granted a deferred examination in the case of absence from a final examination extensions for the completion of assignments must be approved by the department head or dean in non departmentalized colleges and may exceed thirty days only in unusual circumstances the student must apply to the instructor for such an extension and furnish satisfactory reasons for the deficiency deferred final examinations are granted as per college policy in the interim the instructor will submit a computed percentile grade for the course which factors in the incomplete course work as a zero along with a grade comment of inf incomplete failure if a failing grade in the case where the instructor has indicated in the course outline that failure to complete the required course work will result in failure in the course and the student has a computed passing percentile grade a final grade of will be submitted along with a grade comment of inf incomplete failure if an extension is granted and the required assignment is submitted within the allotted time or if a deferred examination is granted and written in the case of absence from the final examination the instructor will submit a revised computed final percentage grade the grade change will replace the previous grade and any grade comment of inf incomplete failure will be removed for provisions governing examinations and grading students are referred to the university council regulations on examinations section of the calendar university of saskatchewan calendar exams grades grading system department policy on academic honesty students are expected to be academically honest in all of their scholarly work including course assignments and examinations academic honesty is defined and described in the department of computer science statement on academic honesty http www cs usask ca classes academichonesty shtml and the university of saskatchewan academic honesty website http www usask ca honesty students using the e handin system to submit an assignment for the first time will be prompted with a declaration of academic honesty after reading the declaration you will be presented with three options agree disagree or decide later if you choose decide later you will be prompted again the next time you log into the e handin system agreement or disagreement applies to all cmpt courses in which you are registered for classes that do not use the e handin system each student may be requested to submit a signed paper copy of the declaration of academic honesty to their course instructor an instructor at their option may ask students to sign the paper declaration even if their class is making use of e handin the department has decided to create a departmental academic dishonesty panel which consists of two faculty members appointed by the department head and one student member appointed by the csss if an instructor has reason to suspect academic dishonesty on an assignment the instructor will not return the original assignment to the student instead it will be forwarded along with all relevant evidence to the department academic dishonesty panel the panel will deliberate on the case possibly requiring testimony from the student involved the panel in consultation with the instructor will decide whether the case should be forwarded to the arts and science student academic affairs committee for a disciplinary hearing or whether the case should be handled locally within the department should the panel find no evidence of dishonesty the case will be excused in cases where dishonesty is determined the panel may decide to issue a warning and delete any mark associated with that assignment in such cases a formal warning letter will be sent to the student copied to the instructor and the arts and science student academic affairs committee and no other academic penalty will be applied if the student or instructor is dissatisfied with the panel decision either party may request that the case be reviewed by the student academic affairs committee the student academic affairs committee treats all cases according to the university policy and has the right to apply strict academic penalties see http www usask ca university reports shtml cs introduction to system software by wonsun ahn instructor wonsun ahn course website cs pitt edu wahn teaching what is system programming system programming is the act of writing system code then what is a system and what is system code a system in plain english according to the merriam webster dictionary means a group of related parts that work together in the context of computing a system is a group of software or hardware parts that work together to allow a computer to run hardware parts comprise components like the cpu the main memory and peripheral devices software parts comprise components like the operating system device drivers the compiler and also applications in particular applications are codes that perform tasks that a user wishes to apply the computer to such as word processing web browsing and gaming system code is everything else besides application code system code does not perform any user task in itself but allows application code to run on top of the given hardware components for example operating system code manages the hardware the cpu the main memory peripheral devices that applications run on top of it also provides common services to applications such as file storage service and network service another example is the compiler which translates applications written in programming languages such as java python or c to machine language understood by the underlying hardware why is learning system programming important well most of you are computer science or computer engineering majors or planning to be hopefully and your future jobs whether they be in research or industry will often involve building and modifying parts of a system and writing system code even if you do not directly write system code and write application code learning system programming will give you a deeper understanding of how your application interacts with the rest of the system this will help you program in a correct and performant fashion also it will help you understand when your program does not behave well what we will learn in this course the area of systems is too vast to cover in a single course so in this course we will focus on learning how an application interacts with the rest of the system and how various components of a system interact with each other we will only learn parts of the system that is relevant to this topic and the detailed implementation of each component will be dealt in more advanced coursework such as cs operating systems and cs structure of programming languages compilers most of you have probably done some application programming using programming languages such as java python or javascript these application centric languages typically run on top of a piece of software called a virtual machine this virtual as in not real machine software maintains a façade over the real machine and provides an illusion of an idealized hardware machine that is easy to program for example the java virtual machine provides the illusion of objects floating around in limitless space and performing tasks by calling methods on those objects however take that facade away and every program must live and operate in concrete hardware with limited hardware resources and varied hardware components what more that program must share the limited hardware resources with other programs and play nice with each other in fact the reason java programs don t have to deal with the nitty gritty details of the system is because the java virtual machine does all the dirty work beneath it so how does a program actually run and interact with the underlying hardware a program starts out its life by first being translated from programming language to machine language using a compiler the resulting machine language code is also called an executable since it can be executed directly on the machine without further translation an executable is also called a binary since the machine language looks like a series of binary and to the human eye when this binary is executed a process for this program is created by the operating system a process is an incarnation of a program in the system it state is represented in memory as code and data and execution takes place by continuously modifying this state in order for this internal memory state to be useful it must be made visible to the external world by writing to a file showing something on the monitor or sending data over the network this is where a lot of the system interaction takes place system code is typically organized into layers depending on how close the code is to the underlying computer hardware as an example here are various layers of the system using operating system code as an example layer applications user level libraries layer device independent operating system code file system network stack layer device dependent operating system code device drivers layer computer hardware applications interact with the system through system calls the application requesting something from the system and signals the system signaling to the application that some event of interest to the application has happened when an application places a system call the request traverses the various layers of an operating system to get to the underlying hardware as an example let say a chat application wants to send a message across the network layer the application writes a message to a network socket by calling a socket api in glibc the c runtime library the glibc library performs a write system call on the socket layer the message traverses the various layers of the tcp ip protocol stack to form a packet and then the packet gets sent to the underlying ethernet device driver layer the ethernet device driver talks to the underlying ethernet network card to write the packet into the internal buffer of the card layer the ethernet card sends along the packet in its buffer through the physical lan line through this course we will learn this entire process starting from how the compiler translates your program to machine code executable to how the operating system takes that executable and runs it on the given hardware system why learn c we will start the course by learning c but why go through the pains of learning a new language can we not stay in the nice world of java or python it is because c is the language in which the vast majority of system code is written in most operating systems are written in c most file systems are written in c most device drivers are written in c most compilers are written in c and for good reason c started its life as a language for writing operating system code unlike java or python a c program executes directly on the real hardware machine instead of an idealized virtual machine such as the java virtual machine as such it allows much greater control of the underlying hardware compared to java that also means you will be exposed to all the idiosyncrasies of the underlying hardware which will make your life harder as the saying goes with great power comes great responsibility you will know what this means first hand after your c program crashes on you a few times but after mastering c programming you will gain a much greater understanding of how system code works in short c does not provide the bells and whistles of a very high level language like java that makes programming easier but it does come with unique advantages that make it suitable for system programming or writing applications close to the system such as the java virtual machine cons hard to use pros efficiency control no concept of object oriented programming no language safety features such that a program either behaves or throws a well defined exception e g a null pointer exception no automatic garbage collection of memory can operate in resource constrained environments i e explicit memory management instead of garbage collection is very efficient and has little runtime overhead i e no language safety features means no runtime checking of safety properties allows for direct and raw control over hardware i e can access an arbitrary location in your dram or direct your cpu to start executing at an arbitrary location lets the programmer write parts of the program directly in assembly language i e crucial for direct management of hardware resources learning c is like swallowing the red pill in the movie matrix it is a painful process but it allows you to see the truth behind the idealized world of the virtual machine cs project blackjack exif viewer due sunday october at your first project is to write two programs in c that provide some experience with a wide range of the topics we have been discussing in class blackjack implementation points blackjack also known as is a multiplayer card game with fairly simple rules for this assignment you will be implementing a simplified version where a user can play against the computer who acts as dealer two cards are dealt to each player the dealer shows one card face up and the other is face down the player gets to see both of his or her cards and the total of them is added face cards kings queens and jacks are worth points aces are worth or points and all other cards are worth their face value the goal of the game is to get as close to blackjack without going over called busting the human player goes first making his or her decisions based on the single dealer card showing the player has two choices hit or stand hit means to take another card stand means that the player wishes no more cards and ends the turn allowing for the dealer to play the dealer must hit if their card total is less than and must stand if it is or higher whichever player gets closest to without exceeding it wins requirements and hints have the program intelligently determine if an ace should be interpreted as a or an by counting the number of aces dealt and adjusting the total down if over and an ace exists generating random numbers in c is a two step part first we need to seed the random number generator once per program the idiom to do this is srand unsigned int time null when we need random numbers we can use the rand function it returns an unsigned integer between and we can use modulus to reduce it to the range we need int value rand high low low remember that getting a card worth is more common because of the face cards so generate a random card not a random value you can assume there is an infinite deck of cards i e drawing an ace will not decrease the probability of drawing an ace the next time exif viewer points an exif tag is embedded in many image files taken on a digital camera to add metadata about the camera and exposure it is a complicated format but we can simplify it to where we can write a simple viewer that will work with many jpeg files a jpeg file begins with the byte sequence after that a special jpeg marker indicates an application specific segment called we will assume but you must verify that there is no section prior to this means the first bytes of a jpeg with an exif tag will be offset length value description jpeg start of file marker 0xe1 jpeg marker length of the block always big endian exif exif string 0x00 nul terminator and zero byte ii or mm endianness ii means intel little endian this is the start of the tiff header version number is always offset to start of exif block from start of tiff block byte of the file next an array of tiff tagged image file format tags will store the information we are looking for at offset we find a byte unsigned short count that tells us how many tags there will be in this section a tiff tag is bytes that are defined as offset length description tag identifier data type number of data items value or offset of data items loop through the file count times reading a single tiff tag at a time we will only be concerned with different tags in this section simply ignore any other tag that appears the three tags we are concerned with have the byte identifiers in the table below tag identifier data type description ascii string manufacturer string acsii string camera model string bit integer exif sub block address let us take the first one as an example we read in a byte tiff tag and find that its identifier field is its data type field will be which means that the data is encoded in an ascii string the number of data items field will tell us how many bytes our string has the final field in the tag can contain the value of the data itself if it fits in bytes or it can contain an offset to the data elsewhere in the file since an arbitrary string cannot fit in bytes in our case this value is an offset it an offset from the beginning of the tiff header which occurred at byte of the file so we seek to offset in the file and read each letter from that position in the file until we have read them all we ll encounter a nul terminator at the end at this point we ve read the manufacturer string we must seek back to the location in the file where we were reading tags and continue on to the next one to make this concrete say we encounter our manufacturer string tag while reading bytes of our file the tag would be the tells us how many bytes the string will be including the nul terminator the tells us to seek to bytes from the start of the file the bytes is the offset of the tiff header from the start of the file when we seek to offset we read in canon the manufacturer of the camera we must now seek back to offset so that we can read the next tag in this section if we encounter the identifier there is an additional exif block elsewhere in the file we can stop reading at this point even if we haven t read all count tags because the tiff format states that all identifiers must be in sorted order we will seek to the offset specified in this exif sub block tag again bytes there we ll repeat the above process one more time to get more specific information about the picture first read in a new count as an unsigned short next loop reading more byte tiff tags from the file this time we ll be concerned with the following fields tag identifier data type description bit integer width in pixels bit integer height in pixels bit integer iso speed fraction of bit unsigned integers exposure speed fraction of bit unsigned integers f stop fraction of bit unsigned integers lens focal length ascii string date taken the good news is that type means that the value is directly encoded in the last bytes of our tag and no seeking needs to be done type requires us to behave like we did with the string but rather than reading several single byte characters we will read unsigned ints display the ratio of the two numbers as shown in the example below what to do for your project you will make a utility that can print the contents of an existing tag if there make a program called exifview and make it so that it runs with the following command line it should print the contents of the exif tag to the console if present or give a message if not present or readable by our program output hints and requirements we need to treat these files as binary files rather than text files make sure to open the file correctly and to use fread for i o please use a structure to represent a jpeg tiff exif header and another struct to represent a tiff tag do not use a bunch of disjoint variables do your fread with the whole structure at once that is read an entire tag in one file operation if the header field does not contain the exif string in the right place print an error message that the tag was not found if the tiff header contains mm instead of ii print an error message that we do not support the endianness environment ensure that your program builds and runs on thoth cs pitt edu as that will be where we are testing we have provided two sample jpg files with valid exif tags according to our limitations copy them to your working directory on thoth by using the command the dot at the end is important as it represents the current directory in linux submission when you re done create a gzipped tarball as we did in the first lab of your commented source files and compiled executables name the gzipped tarball tar gz similarly as we did for copy your archive to the directory wahn submit make sure you name the file with your username and that you have your name in the comments of your source file please do not submit the sample picture files we provide note that this directory is insert only you may not delete or modify your submissions once in the directory if you ve made a mistake before the deadline resubmit with a number suffix like tar gz project what the password due tuesday november at description throughout most of your cs or coe studies you work creating or modifying programs or computers in a word building however sometimes the best way to learn about something is to break it in this project you will be deconstructing existing programs that each have a secret password or passphrase that needs to be input in order to unlock the program i am providing you with compiled executables each one requires you to enter a sequence of ascii characters to unlock unlocking the programs will draw upon the things we are studying this term you will also write a tool to help you with solving the first program in unix linux there is a program called strings that dumps out the sequences of ascii characters that are or more characters long part mystrings points the mystrings program should take a filename from the command line and read the bytes of the file looking for strings of printable characters printable characters are ascii values between and decimal and the ascii value the tab character a string is a run of at least consecutive printable characters and ends whenever a non printable character is encountered whenever you find such a string print it out one per line you can check the operation of your program via the real strings program and do a man strings to learn about how it works the output from your mystrings program should match exactly the output from strings a filename try comparing the output from object files o image files jpg and text files c the output should be the same regardless of file type make sure your program can handle strings that are arbitrarily long part passwords points for each of the three programs you will be required to provide two things the solution passphrase and a written description of your attempts to discover it stating what you learned to help you along the way you should relate your experiences back to the course material using the terms and concepts we ve discussed write it up in a formal organized fashion you do not need to describe every command you have tried or every wrong idea describe briefly your failed attempts and motivation but describe in detail your successful approach tools the most obvious tool you will need is a good debugger like gdb you may also find a hex viewer like od x useful objdump can do a lot of individual tasks that can be helpful additionally you might find the mystrings command you wrote somewhat useful environment for this project we will be working on thoth cs pitt edu when you login via ssh with your pitt account you will find a local directory under u syslab named with your username in this directory you will find the three executables and space to work on them if you store any files of your own in this directory note that it is not part of afs and only exists on this machine we will delete your directory when the term is over anything you want to save or backup should be copied into your afs private directory hints notes each program is written in c each program will have a different passphrase per student although how to find it will be consistent for everyone all passphrases will be printable ascii characters and be less than characters in length a passphrase may be different each run of a program make sure to test it several times there may be several passphrases that work try to describe them or explain why this is not an attempt to prove how clever i am each program will be solvable from course material and the standard tools on the system what to turn in your mystrings program and source code a written description for each program documenting your attempts to arrive at the solution and the passphrase itself submitted as a word doc or pdf document all in a tar gz file named with your user id copy your archive to the appropriate directory wahn submit project a custom malloc due tuesday november at 59pm description in our discussions of dynamic memory management we discussed the operation of the standard c library call malloc some implementations of malloc designate a region of a process address space from the symbol where the code and global data ends to brk as the heap we will be taking a slightly different approach and asking the os for a large region of memory to act as our heap as part of dynamic memory management we also discussed various algorithms for the management of the empty spaces that may be created after a malloc managed heap has had some of it allocations freed in this assignment you are asked to create your own version of malloc based upon the buddy allocator scheme details the buddy allocation algorithm requires a large free space that we can repeatedly divided to find a reasonably sized chunk to return we will request this initial free space directly from the os by requesting the os allocate us many contiguous pages using the mmap system call we will create an initial space of by doing the following void base mmap null map_private we can easily set to by using a left shift trick the pointer base will now point to a region of contiguously allocated in size that we can use for our buddy allocator as we discussed in class we will use a double linked list representation for the free lists each chunk in the free list contains chunk header information the prev pointer and the next pointer the chunk header is one byte long where the first bit is the occupancy bit and the remaining bits represent the size of the chunk the size is represented by where n is the size in bytes hence the bits can represent a size of up to bytes the occupancy bit and the size are required for the coalescing algorithm the minimal size of a chunk is bytes the block header and the pointers take up bytes the block header is always present in a chunk regardless of whether that chunk is allocated or free however the prev and next pointers are only needed when a chunk is free because they are only used to link chunks in a free list hence the size of usable space in an allocated chunk is n bytes one byte is taken up by the block header obviously your malloc should return a pointer to this usable space not a pointer to the chunk header now we should build a table of pointers to be the heads of the double linked lists of our chunks of each power of two size we support to the first pointer would be the head for the byte chunk list the last pointer would be the head for the gib chunk list we have discussed in class how to discover a buddy for a given chunk using the xor operation in c it is important to note that this only works when the base of the chunk is at address which is never going to be the case for us as mmap will return an address from the memory mapped area at high addresses in our address space that means to use the xor trick we will always need to subtract base from the address to get the offset into our region requirements you are to create three functions for this project a malloc replacement called void int size that allocates memory using the buddy allocation scheme a free replacement called void void ptr that deallocates a pointer that was originally allocated by the malloc you wrote above a function that dumps all the free blocks in all the free lists in your heap in a way very similar to the given in the memory management lab in wahn public heap heap c the ta will insert this function at various points in your code to check the integrity of your heap for grading purposes make sure you implement this function first or you will not get any points for implementing either malloc or free your free function should coalesce buddy free blocks as we described in class as you are developing you will want to create a driver program that tests your calls to your mallocs and frees for grading we will use the driver u syslab shared mallocdrv c in order to test that your code works mallocdrv c is designed to use the standard c library malloc as is make sure you modify the malloc and free macros defined at the top to your own functions to have it work with your code also you will need to include your mymalloc h header file appropriately the output from using your own buddy malloc and from using the c library malloc should be identical the output for the function immediately after initializing the heap should look as follows the heap shows the free lists for the bins that are all empty except for the largest bin of size there is one block in this bin that has the occupancy bit set to an offset of from the base and a size of or this block encompasses the entire heap initially environment for this project we will again be working on thoth cs pitt edu hints notes in c the sentinel value for the end of a linked list is having the next pointer set to null gdb is your friend no matter what you think after project try inserting the function yourself at various points in the test program it will help you detect immediately when your heap is corrupted and where what to turn in a header file named mymalloc h with the prototypes of your three functions a c file named mymalloc c with the implementations of your three functions the test program you used during your initial testing any documentation you provide to help us grade your project to create a tar gz file if your code is in a folder named execute the following commands where username is your username then copy your file to wahn submit cs project dev dice due thursday december at 59pm project description standard unix and linux systems come with a few special files like dev zero which returns nothing but zeros when it is read and dev random which returns random bytes in this project you will write a device driver to create a new device dev dice which returns a randomly selected roll of a sided die how it will work for this project we will need to create three parts the device driver the special file dev dice and a test program to convince ourselves it works the test program will be a solitaire implementation of the game of yahtzee driver implementation our device driver will be a character device that will implement a read function which is the implementation of the read syscall and returns an appropriate die roll a byte value from to as we discussed in class the kernel does not have the full c standard library available to it and so we need to get use a different function to get random numbers by including linux random h we can use the function which you can turn into a helper function to get a single byte yahtzee implementation yahtzee also known as poker dice is a multiplayer dice game with fairly simple rules for this assignment you will be implementing a solitaire version where a user plays by herself i highly suggest playing the game or at least reading the rules http en wikipedia org wiki yahtzee dice are rolled the player can choose to keep any of the dice and is allowed to reroll any they do not want to keep they may reroll up to twice but at the end of the third roll the five dice are finalized after the user has finalized their five dice they may place them into one of n categories event score upper section ones twos threes fours fives sixes the sum of the dice with the appropriate value lower section three of a kind four of a kind the total of all dice full house small straight 30 large straight yahtzee chance the sum of the dice we will not do anything special for a second yahtzee note that the player can score points in a category if the total points of the ones twos threes fours fives and sixes is or more the user gets a bonus of points your roll which dice to reroll your second roll which dice to reroll place dice into upper section lower section selection place dice into ones twos threes fours fives sixes selection your score so far is ones fours twos fives threes sixes the game ends when all categories have been assigned a point turns for this assignment you need to do the following write a program that allows a player to play yahtzee gets each die to display by reading one byte from the dev dice file hints and suggestions use the qsort function to sort the dice for scoring it makes finding multiples and straights much easier you may write the yahtzee program using stdlib rand for testing but make sure you eventually replace it with your read of dev dice installation and example on thoth cs pitt edu login and cd to your u syslab username directory tar xvfz shared tar gz cd open the makefile with the editor of your choice e g pico makefile we need to setup the path to compile against the proper version of the kernel to do this change the line kdir lib modules shell uname r build to kdir u syslab shared linux build the kernel object the arch is important because we are building a bit kernel on a bit machine download and launch qemu for windows users you can just double click the qemu win bat that is supplied in the zipfile available on my website mac users should download and install virtualbox and use the tty disk image provided in the main zipfile linux users are left to install qemu as appropriate and also use the tty disk image provided in the main zipfile when linux boots under qemu login using the root root account username password we now need to download the kernel module you just built into the kernel using scp load the driver using insmod we now need to make the device file in dev first we need to find the major and minor numbers that identify the new device 12 the output should be a number like the is the major and the is the minor we use mknod to make a special file the name will be hello and it is a character device the and the correspond to the major and minor numbers we discovered above if different use the ones you saw we can now read the data out of dev hello with a simple call to cat you should see hello world which came from the driver we can clean up by removing the device and unloading the module what to do next the code for the example we went through comes from http www linuxdevcenter com pub a linux devhelloworld a simple introduction to device drivers under linux html page read that while going through the source to get an idea of what the module is doing start with the third section entitled hello world using dev and read up until the author starts describing udev rules we are not using udev under qemu when you have an idea of what is going on make a new directory under your u syslab username directory called copy and rename the c from the example and copy over the makefile edit the makefile to build your new file change all the references of hello to building the driver to build any changes you have made on thoth in your directory simply if you want to force a rebuild of everything you may want to remove the object files first copying the files to qemu from qemu you will need to download the driver that you just built use scp to download the driver to a home directory root if root loading the driver into the kernel in qemu as root either by logging in or via su making the dev dice device like in the example we ll need to determine the major and minor numbers for this particular driver and use those numbers to make the dev dice file unloading the driver from the kernel in qemu as root either by logging in or via su then you can remove the dev dice file implementing and building the yahtzee program since thoth is a bit machine and qemu emulates a bit machine we should build with the flag running yahtzee we cannot run our yahtzee program on thoth cs pitt edu because its kernel does not have the device driver loaded however we can test the program under qemu once we have installed the new driver we first need to download yahtzee using scp as we did for the driver however we can just run it from our home directory without any installation necessary file backups one of the major contributions the university provides for the afs filesystem is nightly backups however the u syslab partition is not part of afs space thus any files you modify under your personal directory in u syslab are not backed up if there is a catastrophic disk failure all of your work will be irrecoverably lost as such it is my recommendation that you backup all the files you change under u syslab to your private directory frequently loss of work not backed up is not grounds for an extension you have been warned hints and notes printk is the version of printf you can use for debugging messages from the kernel in the driver you can use some standard c functions but not all they must be part of the kernel to work in the yahtzee program you may use any of the c standard library functions as root typing poweroff in qemu will shut it down cleanly if the module crashes it may become impossible to delete the file you created with mkdev in dev if that happens just grab a new disk image and start over it why we re developing in a virtual machine as opposed to a real one requirements and submission you need to submit your c file and the makefile your well commented yahtzee program source make a tar gz file named username tar gz copy it to wahn submit by the deadline for credit cs project multi threaded web server due tuesday december 2016 at 59pm description launching a browser and visiting a site on the internet involves at least two parties the web server and the requestor you in this assignment we will use pthreads and berkley sockets to implement a primitive web server which talks http to browsers over tcp ip http hypertext transfer protocol is a simple text based communication protocol by which a web browser requests documents from a web server and the web server replies for instance if you visit a web site such as http www example com test html your browser does the following connect to the ip address of http www example com test html obtained via dns lookup at port standard web server port send the http request message get test html http host www example com to which the server should reply assuming it finds the file http ok date thu nov 00 55 gmt content length connection close content type text html followed by a blank line and the contents of the file which is bytes if the file is not found it should return the infamous error code like so http not found to get the current date use a combination of time and localtime functions declared in time h refer to the manpages for their usage here is an example piece of code http www gnu org software libc manual time functions example html requirements your task is to use berkley sockets to accept get requests for html pages over http your main thread should wait for a connection to occur spawn off a worker thread and have that thread communicate to the requestor according to the above protocol when the thread is done it should add the request for that particular webpage to a file named stats txt note that this file needs to be exclusively accessed so you ll need to do some sort of synchronization each request should append something like the following to stats txt assuming the web client connected from ip and port get test html http host www example com client the ip and port of the client can be gathered when accepting the connection as we ve learned during the lectures ports and addresses port is the normal web server port but we can t all use it at the same time please use your designated personal port number listed on http www cs pitt edu wahn teaching misc ports pdf please use this port and only this port for an address of the machine we will simply refer to it as localhost or localhost reserved ip address 0 0 testing thoth cs pitt edu is firewalled from the outside world meaning that you will only be able to connect to your server from thoth itself in order to do this your best bet for testing is to use a few programs telnet localhost port wget http 0 0 port page html links no connect http 0 0 port page html telnet is a terminal emulator you can connect directly to your server and anything you type will be sent this means you can manually create a get request and you will see the reply of the server in plain text on your screen wget downloads files from the internet links is a text mode web browser no graphics no tables and minimal font support but it a real working browser each of the above web clients sends http requests of slightly different formats but they are all governed by the http protocol the request format is given in the below link http www org protocols html as you can see the request message header and message body is always separated by two crlfs in our simple interactions you can assume that the web client never sends the message body hence in your web server you can safely assume that you have finished receiving an http request when you receive two crlfs a crlf is a sequence of a cr ascii code and a lf ascii code character my advice is to work with two ssh windows open one with your server printing error messages to stderr and one that you are using one of the above programs to request pages submission you need to submit your well commented program source make a tar gz file as in the first assignment named username tar gz copy it to wahn submit by the deadline for credit cs thoth cs pitt edu we re going to use thoth cs pitt edu to do our work for this course this is a quad core machine with many gbs of ram and over a terabyte of disk space in other words a modern powerful machine to use it we first need to modify the environment to be more suitable for our purposes we do this by editing a file named which is a script that setups up the environment when you first log in modifying the manpath to be able to use the man pages you will need to do the following for it to work properly please enter the following commands after you first log in this will give you write permissions to and open the nano editor now scroll down to the end of the file until you see the line add the following lines spacing around the and characters need to be there save the file and quit basically you are directing the script to run another script located in opt sh which if you look inside it sets up the manpath environment variable the man command searches this path to look for the man pages lastly remove write permissions from for safety using the following command now will not run until the next time you log in so let force run the script so that it is applied to this login session run the following command to check that it worked type at the prompt if you see a help page for the ls command it worked if you see the following then you did something wrong if you get the above go back and check the and try again ask the ta for help if you can t figure it out modifying character encoding error messages from the compiler is crucial to debugging your program if you are having trouble seeing error messages from gcc when it has a compile error the issue may be because of character encoding the default in thoth is set to utf this is what you probably see when you echo the lang environment variable by typing the solution is to configure your terminal so that it can view utf properly for example in mac os x terminals there is a setting to change encoding in preferences advanced if that is not feasible you need to add the following line to the bottom of your using the method explained above then the next time you login and try echoing lang you ll get the following output now things should work fine on all terminals name lab introduction to unix and c this first lab is meant to be an introduction to computer environments we will be using this term you must have a pitt username to complete this lab note text in unix is case sensitive is different from is different from all filenames of concern in this lab are lowercase please follow the instructions as listed in this document here are a few common unix commands cd change directory ls list all files in current directory pwd display the current directory mv move or rename a file cp copy a file gcc compile a c file pico edit a text file part i to login to the computers you will need to use an ssh client the ssh client that we will be using is putty at home download from http www chiark greenend org uk sgtatham putty download html we will connect to the machine host thoth cs pitt edu when you login first you are placed in your home directory the command will list all of the files and directories there the one we are most concerned with is the private directory it is special in that only you can access files inside this directory it will keep your work safe from other people let move into the private directory so we can work there changes directory to the private directory for this class we ll keep all of our files organized into a directory make it by typing if you want to double check that it worked type ls to list we now want to move into the directory to do our actual work part ii while still in the directory type to make a directory for today lab now type nano is a very simple text editor a lot like notepad on windows it is one option for creating and editing code under unix linux type the following text in exactly as it is shown save the file by hitting ctrl o and then enter exit pico by typing ctrl x at the bottom of the pico window it shows what keys do special things the means to hold ctrl while pressing the key back at the prompt type which will make our program a file named will be in the directory if we type ls run it by typing part iii archives and project submission whenever you turn in a project for this course you will need to submit a copy of your code and executable to be graded we will try this now you are probably familiar with a zip file which does two things it archives a bunch of files into a single file and also compresses it to save space in unix we do this in two steps we create a tape archive tar and then compress it gzip first let us go back up to our directory now let us first make the archive type your username for the username part of the filename and then we can compress it which will produce a tar gz file we will then copy that file to the submission directory once a file is copied into that directory you cannot change it rename it or delete it if you make a mistake resubmit a new file with a new name being sure to include your username part iv manual pages if you ever want to see how a command works or you forget the various options you could use you can consult the man pages on the command by typing for example this will let you scroll through the online help about the ls command the space bar will scroll the document one screenful at a time and the enter key will move one line at a time at any time you can quit by pressing q in the space below explain the purpose of the s switch capital s not lowercase part v recording your work with script sometimes something will go awry in your program and you may not know the source of an error message from the compiler for us to help you we need to see the output of the compiler we can capture that with a program called script type now to read up on how it works now open the c file from part i by navigating to it and then opening it in pico remove the semicolon after the printf statement and save now issue the script command then compile the modified program this should result in an error message type exit or hit ctrl d to stop script from recording if you do an ls now you should see a file named typescript we can use the program called more to display the contents of this file more is also used on the man pages and can be operated in the same way if the file is longer than the screen it will allow you to scroll or to quit at anytime type to see the output you saw when you compiled the first lab helpful hints keep this sheet as a guide until you get comfortable in unix every time you want to write a program you will do the following after logging into your account if you want to use vim or emacs instead of nano that is fine you may prefer to edit files under a gui and use ftp to upload files the choice is yours however we will only officially support the steps described here remember case matters int short long long long unsigned int char float double long double the highest numbered file before the deadline will be the one that is graded however for simplicity please make sure you ve done all the work and included all necessary files before you submit anatomy of a program in memory gustavo duarte http duartes org gustavo blog post anatomyofaprograminmemory 5 gustavo duarte gustavo blog brain food for hackers anatomy of a program in memory jan memory management is the heart of operating systems it is crucial for both programming and system administration in the next few posts i ll cover memory with an eye towards practical aspects but without shying away from internals while the concepts are generic examples are mostly from linux and windows on bit this rst post describes how programs are laid out in memory each process in a multi tasking os runs in its own memory sandbox this sandbox is the virtual address space which in bit mode is always a block of memory addresses these virtual addresses are mapped to physical memory by page tables which are maintained by the operating system kernel and consulted by the processor each process has its own set of page tables but there is a catch once virtual addresses are enabled they apply to all software running in the machine including the kernel itself thus a portion of the virtual address space must be reserved to the kernel this does not mean the kernel uses that much physical memory only that it has that portion of address space available to map whatever physical memory it wishes kernel space is agged in the page tables as exclusive to privileged code http duartes org gustavo blog post cpu rings privilegeand protection ring or lower hence a page fault is triggered if user mode programs try to touch it in linux kernel space is constantly present and maps the same physical memory in all processes kernel code and data are always addressable ready to handle interrupts or system calls at any time by contrast the mapping for the user mode portion of the address space changes whenever a process switch happens blue regions represent virtual addresses that are mapped to physical memory whereas white regions are unmapped in the example above firefox has used far more of its virtual address space due to its legendary memory hunger the distinct bands in the address space correspond to memory segments like the heap stack and so on keep in mind these segments are simply a range of memory addresses and have nothing to do with intel style segments http duartes org gustavo blog post memory translation and segmentation anyway here is the standard segment layout in a linux process anatomy of a program in memory gustavo duarte http duartes org gustavo blog post anatomyofaprograminmemory 5 when computing was happy and safe and cuddly the starting virtual addresses for the segments shown above were exactly the same for nearly every process in a machine this made it easy to exploit security vulnerabilities remotely an exploit often needs to reference absolute memory locations an address on the stack the address for a library function etc remote attackers must choose this location blindly counting on the fact that address spaces are all the same when they are people get pwned thus address space randomization has become popular linux randomizes the stack http lxr linux no linux fs c memory mapping segment http lxr linux no linux arch mm mmap c and heap http lxr linux no linux arch kernel c by adding o sets to their starting addresses unfortunately the bit address space is pretty tight leaving little room for randomization and hampering its e ectiveness http www stanford edu blp papers asrandom pdf the topmost segment in the process address space is the stack which stores local variables and function parameters in most programming languages calling a method or function pushes a new stack frame onto the stack the stack frame is destroyed when the function returns this simple design possible because the data obeys strict lifo http en wikipedia org wiki lifo order means that no complex data structure is needed to track stack contents a simple pointer to the top of the stack will do pushing and popping are thus very fast and deterministic also the constant reuse of stack regions tends to keep active stack memory in the cpu caches http duartes org gustavo blog post intel cpu caches speeding up access each thread in a process gets its own stack it is possible to exhaust the area mapping the stack by pushing more data than it can t this triggers a page fault that is handled in linux by http lxr linux no linux mm mmap c which in turn calls http lxr linux no linux mm mmap c to check whether it appropriate to grow the stack if the stack size is below usually then normally the stack grows and the program continues merrily unaware of what just happened this is the normal mechanism whereby stack size adjusts to demand however if the maximum stack size has been reached we have a stack over ow and the program receives a segmentation fault while the mapped stack area expands to meet demand it does not shrink back when the stack gets smaller like the federal budget it only expands anatomy of a program in memory gustavo duarte http duartes org gustavo blog post anatomyofaprograminmemory 5 dynamic stack growth is the only situation http lxr linux no linux 6 1 arch mm fault c in which access to an unmapped memory region shown in white above might be valid any other access to unmapped memory triggers a page fault that results in a segmentation fault some mapped areas are read only hence write attempts to these areas also lead to segfaults below the stack we have the memory mapping segment here the kernel maps contents of les directly to memory any application can ask for such a mapping via the linux mmap http www kernel org doc man pages online pages mmap html system call implementation http lxr linux no linux 6 1 arch kernel c or createfilemapping http msdn microsoft com en us library vs aspx mapviewoffile http msdn microsoft com en us library vs aspx in windows memory mapping is a convenient and high performance way to do le i o so it is used for loading dynamic libraries it is also possible to create an anonymous memory mapping that does not correspond to any les being used instead for program data in linux if you request a large block of memory via malloc http www kernel org doc man pages online pages malloc html the c library will create such an anonymous mapping instead of using heap memory large means larger than bytes kb by default and adjustable via mallopt http www kernel org doc man pages online pages undocumented html speaking of the heap it comes next in our plunge into address space the heap provides runtime memory allocation like the stack meant for data that must outlive the function doing the allocation unlike the stack most languages provide heap management to programs satisfying memory requests is thus a joint a air between the language runtime and the kernel in c the interface to heap allocation is malloc http www kernel org doc man pages online pages malloc html and friends whereas in a garbage collected language like c the interface is the new keyword if there is enough space in the heap to satisfy a memory request it can be handled by the language runtime without kernel involvement otherwise the heap is enlarged via the brk http www kernel org doc man pages online pages brk html system call implementation http lxr linux no linux 6 28 1 mm mmap c to make room for the requested block heap management is complex http g oswego edu dl html malloc html requiring sophisticated algorithms that strive for speed and e cient memory usage in the face of our programs chaotic allocation patterns the time needed to service a heap request can vary substantially real time systems have special purpose allocators http rtportal upv es rtmalloc to deal with this problem heaps also become fragmented shown below finally we get to the lowest segments of memory bss data and program text both bss and data store contents for static global variables in c the di erence is that bss stores the contents of uninitialized static variables whose values are not set by the programmer in source code the bss memory area is anonymous it does not map any le if you say static int cntactiveusers the contents of cntactiveusers live in the bss the data segment on the other hand holds the contents for static variables initialized in source code this memory area is not anonymous it maps the part of the program binary image that contains the initial static values given in source code so if you say static int cntworkerbees the contents of cntworkerbees live in the data segment and start out as even though the data segment maps a le it is a private memory mapping which means that updates to memory are not re ected in the underlying le this must be the case otherwise assignments to global variables would change your on disk binary image inconceivable 15 anatomy of a program in memory gustavo duarte http duartes org gustavo blog post anatomyofaprograminmemory 5 twitter com m ailto duartes org the data example in the diagram is trickier because it uses a pointer in that case the contents of pointer gonzo a 4 byte memory address live in the data segment the actual string it points to does not however the string lives in the text segment which is read only and stores all of your code in addition to tidbits like string literals the text segment also maps your binary le in memory but writes to this area earn your program a segmentation fault this helps prevent pointer bugs though not as e ectively as avoiding c in the rst place here a diagram showing these segments and our example variables you can examine the memory areas in a linux process by reading the le proc maps keep in mind that a segment may contain many areas for example each memory mapped le normally has its own area in the mmap segment and dynamic libraries have extra areas similar to bss and data the next post will clarify what area really means also sometimes people say data segment meaning all of data bss heap you can examine binary images using the nm http manpages ubuntu com manpages intrepid en nm 1 html and objdump http manpages ubuntu com manpages intrepid en objdump 1 html commands to display symbols their addresses segments and so on finally the virtual address layout described above is the exible layout in linux which has been the default for a few years it assumes that we have a value for when that not the case linux reverts back to the classic layout shown below that it for virtual address space layout the next post discusses how the kernel keeps track of these memory areas coming up we ll look at memory mapping how le reading and writing ties into introduction to os why take this class why with mosse it mandatory it a great class it a great prof it easy not do not fool thyself it good for you life is not life anymore while this class is going on be careful specially if you re taking also compilers or some other hard programming class class outline book tanenbaum modern oss intro to oss including real time oss processes definition synchronization management memory virtual memory memory allocation io disks sensors actuators keyboards etc interprocess communication networking data transmission etc fault tolerance real time and security time permitting schedule and grading pop quizzes of grade about every weeks programming assignment nachos on unix of grade midterm of grade around march second exam of grade april class participation may carry of grade for extra credit project is self test for you to test whether you will die or not taking this class add drop period ends tuesday jan operating systems manages different resources cpu mem disk etc improves performance response time throughput etc allows portability enables easier programming no need to know what the underlying hardware interface between the hardware and the rest of the machine editors compilers user programs etc standard interface is typically done in two ways system calls control goes to the operating system library calls control remains with the user first generation of computers had no os single user all coding done directly in machine language memory resident code no other resources to manage second generation has basic os batch processing read input tape cards process output to tape or print third generation improved life multiprogramming careful partitioning of memory space drums and disks added for reading cards and spooling outputs simultaneous peripherals operations on line time sharing created several virtual machines fourth generation pcs and workstations cheaper faster more user friendly thank macs for interfaces unix precursor multics multiplexed information and computing services was the first modern os bell mit ge multics units unix berkeley improved on it paging virtual memory file systems signals interrupts networking networked oss are connected through a network but user needs to know the name type location of everything distributed oss e g amoeba mach locus provide transparency to user yielding one huge virtual machine specialized oss are built for specific purposes routing engines networking lisp machines ai time constrained applications real time internet www servers massively parallel uses supercomputers etc all these are coming together hard to identify boundaries anymore excellent marketing some good products oss started with dos disk os no nothing just very simple commands windows was a huge jump based on decades old technology initially developed at xerox then macs windows released in improved tremendously the state of the affairs for ms but still unreliable windows nt approaches unix distributions with more user friendly interface created at at t re written improved by berkeley att had majority control and good support reliable os osf open sw foundation now open group is a consortium of several companies to standardize unix different subgroups syscalls shells rt etc standardization is with respect to interfaces and not implementation of primitives impln is left to the implr modern applications are time constrained tel video etc real time playing an increasingly important role interface can be done at any level depends on level of security of os interface with the lower level layer gets translated machine dependent language used for accessing hardware main advantage of direct resource access is efficiency main advantage of indirect access is portability completely layered os why or why not controls and manages resources disks memory cpu sends receives control commands and data allows multiprogramming several programs at the same time in the same resource carries out communication between processes inter and intra processor manages interrupt handlers for hw and sw interrupts provides protection and security to processes prioritizes requests and manages multiple resources in a single machine eg multiprocessors or cpu io reqs os manages resources including management of processes creation deletion suspension comm synch main memory usage alloc de alloc which processes get it storage disk scheduling alloc de alloc swapping files io interfaces and devices eg keyboard caching memory protection authorization file and memory protection etc interprocess communication intra and inter machines command interpretation shells to xlate user to os typically includes the user interface that the os uses os structure hardware at the bottom layer accessing the lower layer thru the higher layers dos programs can access hw unix has controllers and dev drivers dd controlling devices system calls are the interface between user and os dds libraries and system programs invoke typical dos typical unix os structure interface can be done at any level depends on security machine dependent language used for accessing hw main advantage of direct resource access is efficiency less layers means less overhead ie better performance main advantage of indirect access syscall is portability modular approaches ind access have less flexibility since appls only access hw thru libraries and layering means that one level is defined in terms of the level below level is the hw level n is the user appls modular approach create well defined interfaces between any two layers create well defined properties of each layer attempt to decrease the number of layers to improve efficiency and performance the final goal is to make the os flexible and efficient create the layers such that each user perceives the machine as belonging solely to himself or herself this is the concept of a virtual machine which allows each user to avoid thinking about others processes language system calls are the interface between user and os access to the resources is done through priviledged instructions for protection user applications cannot execute in kernel mode user applications user libraries that invoke system procedures are executed to access resources via priviledged instructions called from this way no process can influence other executions on purpose or by accident resource protection example accounting priority information system calls can be divided into categories process control file manipulation device manipulation infomation maintenance communication special purpose oss can also have special primitives specification of deadlines priorities periodicity of processes specification of precedence constraints and or synchronization among processes examples of libraries are language constructs to carry out formatted printing examples of are primitives to create a process for example the reading of bytes of a file the user does fscanf the kernel requests a block of bytes from the device driver dd which talks to the controller of the disk to obtain a block of data the block is transfered into a buffer in the kernel address space the kernel then picks the bytes and copies them into the user specified location this way the kernel accesses kernel and user space but the user only accesses user space system programs do not interact directly with running user programs but define a better environmnt for the development of application programs sys programs include compilers file manipulation and modification editors linker loaders etc an important one is the command interpreter or shell which parses user input interprets it and executes it shells can either execute the command or invoke other system programs or system calls to do it trade offs performance increasing updating of commands different process types have different requirements different requirements beg for different languages assembly lisp prolog java rt c etc real time languages inform the os about its needs in order to enhance the predictability of its execution deadline of a thread by when do i need this done period of a thread what is the frequency of this task resources to be used amount of memory or semaphores precedence constraints door must be open for a robot to exit processes and threads what is a process what is a thread what types a program has one or more locus of execution each execution is called a thread of execution the set of threads comprise a process not an object or executable files must be executing each thread contains an instruction pointer ip a register with next instruction a stack for temporary data eg return addresses parameters a data area for data declared globally and statically a process thread is active while a program is not how to run a program the executable code is loaded onto memory where space is allocated to variables what types of vars a stack is allocated to the process for what registers are updated which registers control of execution goes to the process how process runs one instruction at a time in a cycle fetch the instruction from memory decode the instruction update the ip execute the instruction processes and threads revisited address space memory reserved for a process a heavyweight process has a single locus of execution per address space a single ip a single pcb a process control block pcb contains information pertaining to the process itself state running ready etc registers and flags stack pointer ip etc resource information memory cpu usage open files etc process id security and protection information accounting info who to bill limits similar to resource info processes and threads cont thread lightweight process of a process share some resources e g memory open files etc threads have their own stack ip local address space with only a single process in memory easy to manage for example if a process requests io eg read from keyboard it just stays in the memory waiting several threads or processes complicate things when io is requested why make other processes wait context switching takes place take a waiting thread out of the cpu and put a thread that is ready in cpu state diagram create pcb and other resources are setup end resources held are returned to the os freed context switching saves hw context updates pcb states are typically implemented as queues lists sets complete state diagram multiple threads and processes several problems with multitasking fairness in usage of cpu fairness in usage of other resources coordinated input and output lack of progress deadlocks and livelocks synchronization access to the same data item by several processes threads typical example deposits into account synchronization example at time t requests the withdrawal of and gets preempted at t requests the deposit of t reads balance at t adds at t writes new balance at t resumes and reads balance at t subtracts at t writes new balance t1c t2w what if order was changed other combinations more on synchronization semaphores and primitives serve to achieve mutual exclusion and synchronized access to resources clearly there is more delays associated with attemp ting to access a resource protected by semaphores non preemptive scheduling solves that problem as an aside which scheduling mechanism is more efficient preemptive or non preemptive within each type of scheduling pr or non pr one can choose which policy he she wants to use using semaphores when threads use semaphores they are attempting to reserve a resource for usage only the threads that succeed are allowed in the cs if there is a thread in the cs already other requesting threads are blocked waiting on an event when the thread exits the cs the os unblocks the waiting thread typically during the v the now unblocked thread becomes ready the os may decide to invoke the scheduler or not types of synchronization there are basic types of sync mutex barrier sync deadlocks when a program is written carelessly it may cause a deadlock each process will wait for the other process indefinitely how can we deal with these abnormalities avoidance prevention or detection and resolution which one is more efficient dining philosophers there were some hungry philosophers and some angry philosophers not the same not rastas each needed two forks each shared both his her forks possible deadlock situation how is it possible to have a literally starvation situation what is the best solution round robin fifo how good are fifo and rr important what is the metric to judge a solution by least overhead minimum starvation how to evaluate fairness assignment nachos survey questions due wednesday sept written answers hand in hardcopies not code general questions explain relations among three files under the code directory makefile makefile common makefile dep explain what each file does explain what each sub directory is for under the code directory they directories you should consider are bin filesys machine network test threads userprog and vm nachos time find the variable responsible for keep tracking the time in nachos what is it explain how nachos time advances is it the same as the real time what is the purpose of the private variable randomize in class timer list questions find in nachos code an example of constructing a list object using the list class defined in list cc h desribe how the object is instantiated whether it is sorted list or non sorted list study list h cc how is the list sorted by sortedinsert write the code no programming needed for instantiating a list object that can sort items in the increasing order of integer number using the list class main what does the rs flag do what does the d flag do how do we enable all flags besides writing them all out explain debug t entering main at line of main cc explain ifdef thread threadtest endif at line of main cc run nachos under the threads directory it will print out thread looped times thread looped times etc give names of thread and thread i e what are thread and thread thread questions what are the pcb contents in nachos explain the functionalities of thread yield and thread sleep differences how do thread restoreuserstate and addressspace restorestate differ scheduler questions what does currentthread space restorestate do what is the scope of the variable currentthread what is the name of ready queue in nachos how scheduler readytorun and scheduler run differ in functionality switch questions what information do we need to save for a context switch for the dec mips explain in a brief paragraph what happens in switch synch questions the implementation of semaphore in synch cc has a while loop which is different from the one in the textbook why do we need this while loop explain the following code in synch cc intstatus oldlevel interrupt setlevel intoff some code void interrupt setlevel oldlevel study the constructor and destructor functions for the semaphore class a queue is created and deleted in the respective functions why do we need a queue i e what is the purpose of the queue what are the items stored in the queue system questions explain the usage of timerinterrupthandler by nachos how can we use timerinterrupthandler as an operating system cpu scheduler what does the flag do where are the debug flags finally stored threadtest questions trace simpletest and threadtest in threadtest cc who calls threadtest what does the fork function call do in the line t fork simplethread what does the represent how do you instantiate a mutex semaphore in threadtest cc what does thread yield do what does thread sleep do how is this function different from thread yield utility questions what is enableflags what does assert do if the condition is false cs http people cs pitt edu melhem courses index html cs and cs introduction to high performance computing systems spring tuesdays and thursdays from pm to pm sennott square purpose this course is an introduction to the architecture of and software techniques for parallel and high performance computing systems the content includes fundamental architecture aspects of sharedmemory and distributedmemory systems as well as paradigms algorithms and languages used to program parallel systems students will complete a number of projects demonstrating specific applications on parallel processing systems textbooks about of the material covered will be from an introduction to parallel programming by peter pacheco publisher morgan kaufman the remaining will be in lecture slides and material publicly available on the web prerequisites and or knowledge of programming and fundamentals of computer systems programming assignments will use the c language the course will cover the following topics cs http people cs pitt edu melhem courses index html introduction to parallel systems an introduction to chip multiprocessor architectures models of parallel processing a taste of parallel algorithms and programs interconnection networks cache coherence in symmetric multiprocessors programming using multiple threads programming using the message passing interface mpi programming shared memory machines openmp programming using universal parallel c upc gpu architectures and cuda requirements and grading two exams homeworks and projects students will do a research project graphic processing units gpu history of gpus vga in early a memory controller and display generator connected to some video ram by vga controllers were incorporating some acceleration functions in a single chip graphics processor incorporated almost every detail of the traditional high end workstation graphics pipeline processors oriented to graphics tasks vertex pixel processing shading texture mapping rasterization more recently processor instructions and memory hardware were added to support general purpose programming languages opengl a standard specification defining an api for writing applications that produce and computer graphics cuda compute unified device architecture a scalable parallel programming model and language for gpus based on c c historical pc architecture contemporary pc architecture basic unified gpu architecture streaming multiprocessor special function unit rop raster opertastions pipeline tpc texture processing cluster tutorial cuda cyril zeller nvidia developer technology note these slides are truncated from a longer version which is publicly available on the web enter the gpu gpu graphics processing unit chip in computer video cards playstation xbox etc two major vendors nvidia and ati now amd nvidia corporation enter the gpu gpus are massively multithreaded manycore chips nvidia tesla products have up to scalar processors over concurrent threads in flight over gflops sustained performance users across science engineering disciplines are achieving or better speedups on gpus cs researchers can use gpus as a research platform for manycore computing arch pl numeric nvidia corporation enter cuda cuda is a scalable parallel programming model and a software environment for parallel computing minimal extensions to familiar c c environment heterogeneous serial parallel programming model nvidia tesla gpu architecture accelerates cuda expose the computational horsepower of nvidia gpus enable general purpose gpu computing cuda also maps well to multicore cpus nvidia corporation cuda programming model nvidia corporation o num_threads for i o i nu i i i can chose any seed here i is chosen i attr void i for i o i nu i i null i hits double double heterogeneous programming cuda serial program with parallel kernels all in c serial c code executes in a host thread i e cpu thread parallel kernel c code executes in many device threads across multiple processing elements i e gpu threads nvi kernel many concurrent threads one kernel is executed at a time on the device many threads execute each kernel each thread executes the same code on different data based on its threadid cuda threads might be physical threads as on nvidia gpus gpu thread creation and context switching are essentially free or virtual threads threadid nvidia corporation e g cpu core might execute multiple cuda threads hierarchy of concurrent threads threads are grouped into thread blocks kernel grid of thread blocks threadid thread block thread block thread block n by definition threads in the same block may synchronize with nv barriers threads wait at the barrier until all threads in the same block reach the barrier transparent scalability thread blocks cannot synchronize so they can run in any order concurrently or sequentially this independence gives scalability a kernel scales across any number of parallel cores core device nvidia corporation implicit barrier between dependent kernels nblocks blksize a b c nblocks blksize c c heterogeneous memory model cudamemcpy nvidia corporation kernel memory access per thread per block thread registers local memory on chip off chip uncached block per device kernel kernel on chip small fast off chip large uncached persistent across kernel launches kernel i o nvidia corporation physical memory layout local memory resides in device dram use registers and shared memory to minimize local memory use host can read and write global memory but not shared memory host dram cpu chipset device dram local memory global memory nvidia corporation series architecture thread processors execute kernel threads multiprocessors each contains thread processors one double precision unit shared memory enables thread cooperation multiprocessor nvidia corporation thread processors execution model software hardware thread thread processor threads are executed by thread processors thread blocks are executed on multiprocessors thread blocks do not migrate thread block multiprocessor several concurrent thread blocks can reside on one multiprocessor limited by multiprocessor resources shared memory and register file grid device a kernel is launched as a grid of thread blocks only one kernel can execute on a device at one time nvidia corporation cuda programming basics part i software stack and memory management compiler any source file containing language extensions like must be compiled with nvcc nvcc is a compiler driver invokes all the necessary tools and compilers like cudacc g cl nvcc can output either c code cpu code that must then be compiled with the rest of the application using another tool ptx or object code directly an executable requires linking to runtime library cudart core library cuda nvidia corporation compiling nvidia corporation gpu memory allocation release host cpu manages device gpu memory cudamalloc void pointer nbytes cudamemset void pointer int value count cudafree void pointer int n int nbytes sizeof int int cudamalloc void nbytes cudamemset nbytes cudafree nvidia corporation data copies cudamemcpy void dst void src nbytes enum cudamemcpykind direction direction specifies locations host or device of src and dst blocks cpu thread returns after the copy is complete doesn t start copying until previous cuda calls complete enum cudamemcpykind cudamemcpyhosttodevice cudamemcpydevicetohost cudamemcpydevicetodevice nvidia corporation data movement example int main void int n nbytes i nbytes n sizeof float float malloc nbytes float malloc nbytes cudamalloc void nbytes cudamalloc void nbytes for i i n i i f i cudamemcpy nbytes cudamemcpyhosttodevice cudamemcpy nbytes cudamemcpydevicetodevice cudamemcpy nbytes cudamemcpydevicetohost for i i n i assert i i free free cudafree cudafree return nvidia corporation data movement example int main void int n nbytes i nbytes n sizeof float float malloc nbytes float malloc nbytes cudamalloc void nbytes cudamalloc void nbytes for i i n i i f i cudamemcpy nbytes cudamemcpyhosttodevice cudamemcpy nbytes cudamemcpydevicetodevice cudamemcpy nbytes cudamemcpydevicetohost for i i n i assert i i free free cudafree cudafree return nvidia corporation data movement example int main void int n nbytes i nbytes n sizeof float float malloc nbytes float malloc nbytes cudamalloc void nbytes cudamalloc void nbytes for i i n i i f i cudamemcpy nbytes cudamemcpyhosttodevice cudamemcpy nbytes cudamemcpydevicetodevice cudamemcpy nbytes cudamemcpydevicetohost for i i n i assert i i free free cudafree cudafree return nvidia corporation data movement example int main void int n nbytes i nbytes n sizeof float float malloc nbytes float malloc nbytes cudamalloc void nbytes cudamalloc void nbytes for i i n i i f i cudamemcpy nbytes cudamemcpyhosttodevice cudamemcpy nbytes cudamemcpydevicetodevice cudamemcpy nbytes cudamemcpydevicetohost for i i n i assert i i free free cudafree cudafree return nvidia corporation data movement example int main void host device int n nbytes i nbytes n sizeof float float malloc nbytes float malloc nbytes cudamalloc void nbytes cudamalloc void nbytes for i i n i i f i cudamemcpy nbytes cudamemcpyhosttodevice cudamemcpy nbytes cudamemcpydevicetodevice cudamemcpy nbytes cudamemcpydevicetohost for i i n i assert i i free free cudafree cudafree return nvidia corporation data movement example int main void int n nbytes i nbytes n sizeof float float malloc nbytes float malloc nbytes cudamalloc void nbytes cudamalloc void nbytes for i i n i i f i cudamemcpy nbytes cudamemcpyhosttodevice cudamemcpy nbytes cudamemcpydevicetodevice cudamemcpy nbytes cudamemcpydevicetohost for i i n i assert i i free free cudafree cudafree return nvidia corporation data movement example int main void host device int n nbytes i nbytes n sizeof float float malloc nbytes float malloc nbytes cudamalloc void nbytes cudamalloc void nbytes for i i n i i f i cudamemcpy nbytes cudamemcpyhosttodevice cudamemcpy nbytes cudamemcpydevicetodevice cudamemcpy nbytes cudamemcpydevicetohost for i i n i assert i i free free cudafree cudafree return nvidia corporation data movement example int main void int n nbytes i nbytes n sizeof float float malloc nbytes float malloc nbytes cudamalloc void nbytes cudamalloc void nbytes for i i n i i f i cudamemcpy nbytes cudamemcpyhosttodevice cudamemcpy nbytes cudamemcpydevicetodevice cudamemcpy nbytes cudamemcpydevicetohost for i i n i assert i i free free cudafree cudafree return nvidia corporation data movement example int main void int n nbytes i nbytes n sizeof float float malloc nbytes float malloc nbytes cudamalloc void nbytes cudamalloc void nbytes for i i n i i f i cudamemcpy nbytes cudamemcpyhosttodevice cudamemcpy nbytes cudamemcpydevicetodevice cudamemcpy nbytes cudamemcpydevicetohost for i i n i assert i i free free cudafree cudafree return nvidia corporation cuda programming basics part ii kernels thread hierarchy threads launched for a parallel section are partitioned into thread blocks grid all blocks for a given launch thread block is a group of threads that can synchronize their execution communicate via shared memory nvidia corporation executing code on the gpu kernels are c functions with some restrictions cannot access host memory must have void return type no variable number of arguments varargs not recursive no static variables function arguments automatically copied from host to device nvidia corporation function qualifiers kernels designated by function qualifier global function called from host and executed on device must return void other cuda function qualifiers device function called from device and run on device cannot be called from host code host function called from host and executed on host default host and device qualifiers can be combined to generate both cpu and gpu code nvidia corporation launching kernels modified c function call syntax kernel dg db execution configuration dg dimension and size of grid in blocks two dimensional x and y blocks launched in the grid dg x dg y db dimension and size of blocks in threads three dimensional x y and z threads per block db x db y db z unspecified fields initialize to nvidia corporation more on thread and block ids threads and blocks have ids so each thread can decide what data to work on block id or thread id or host device grid block block grid block block block block simplifies memory addressing when block processing multidimensional data image processing solving pdes on volumes nvidia corporation thread thread thread thread thread thread thread thread thread thread thread thread thread thread thread execution configuration examples equivalent assignment using constructor functions kernel nvidia corporation cuda built in device variables all global and device functions have access to these automatically defined variables griddim dimensions of the grid in blocks at most blockdim dimensions of the block in threads blockidx block index within the grid threadidx thread index within the block nvidia corporation unique thread ids built in variables are used to determine unique thread ids map from local thread id threadidx to a global id which can be used as array indices blockidx x blockdim x threadidx x blockidx x blockdim x threadidx x nvidia corporation minimal kernels global void kernel int a int idx blockidx x blockdim x threadidx x a idx output global void kernel int a int idx blockidx x blockdim x threadidx x a idx blockidx x output global void kernel int a int idx blockidx x blockdim x threadidx x a idx threadidx x output nvidia corporation increment array example cpu program void int a int cuda program n global void int int n int idx int idx blockidx x blockdim x for idx idx n idx if idx n threadidx x a idx a idx idx idx void main void main a n dimblock blocksize dimgrid ceil n float blocksize dimgrid dimblock n nvidia corporation host synchronization all kernel launches are asynchronous control returns to cpu immediately kernel executes after all previous cuda calls have completed cudamemcpy is synchronous control returns to cpu after copy completes copy starts after all previous cuda calls have completed cudathreadsynchronize blocks until all previous cuda calls complete nvidia corporation host synchronization example copy data from host to device cudamemcpy numbytes cudamemcpyhosttodevice execute the kernel ceil n float blocksize blocksize n run independent cpu code copy data from device back to host cudamemcpy a_h numbytes cudamemcpydevicetohost nvidia corporation variable qualifiers gpu code device stored in global memory large high latency no cache allocated with cudamalloc device accessible by all threads lifetime application shared qualifier implied stored in on chip shared memory very low latency specified by execution configuration or at compile time accessible by all threads in the same thread block lifetime thread block unqualified variables scalars and built in vector types are stored in registers arrays may be in registers or local memory nvidia corporation gpu thread synchronization void syncthreads synchronizes all threads in a block generates barrier synchronization instruction no thread can pass this barrier until all threads in the block reach it used to avoid raw war waw hazards when accessing shared memory allowed in conditional code only if the conditional is uniform across the entire thread block nvidia corporation gpu atomic integer operations requires hardware with compute capability compute capability compute capability compute capability atomic operations on integers in global memory associative operations on signed unsigned ints add sub min max and or xor increment decrement exchange compare and swap atomic operations on integers in shared memory requires compute capability optimizing cuda execution model software hardware thread thread processor threads are executed by thread processors thread blocks are executed on multiprocessors thread blocks do not migrate thread block multiprocessor several concurrent thread blocks can reside on one multiprocessor limited by multiprocessor resources shared memory and register file grid device a kernel is launched as a grid of thread blocks only one kernel can execute on a device at one time nvidia corporation warps and half warps thread block threads threads threads warps multiprocessor a thread block consists of thread warps a warp is executed physically in parallel simd on a multiprocessor half warps device memory a half warp of threads can coordinate global memory accesses into a single transaction nvidia corporation memory architecture host cpu chipset device dram local global constant texture nvidia corporation memory architecture memory location cached access scope lifetime register on chip n a r w one thread thread local off chip no r w one thread thread shared on chip n a r w all threads in a block block global off chip no r w all threads host application constant off chip yes r all threads host application texture off chip yes r all threads host application nvidia corporation outline overview hardware memory optimizations data transfers between host and device device memory optimizations execution configuration optimizations instruction optimizations summary nvidia corporation host device data transfers device to host memory bandwidth much lower than device to device bandwidth gb peak pci e gen vs gb peak gtx minimize transfers intermediate data can be allocated operated on and deallocated without ever copying them to host memory group transfers one large transfer much better than many small ones nvidia corporation page locked data transfers cudamallochost allows allocation of page locked pinned host memory enables highest cudamemcpy performance gb on pci e gb on pci e see the bandwidthtest cuda sdk sample use with caution allocating too much page locked memory can reduce overall system performance test your systems and apps to learn their limits nvidia corporation overlapping data transfers and computation async and stream apis allow overlap of or data transfers with computation cpu computation can overlap data transfers on all cuda capable devices kernel computation can overlap data transfers on devices with concurrent copy and execution roughly compute capability stream sequence of operations that execute in order on gpu operations from different streams can be interleaved stream id used as argument to async calls and kernel launches nvidia corporation asynchronous data transfers asynchronous host device memory copy returns control immediately to cpu cudamemcpyasync dst src size dir stream requires pinned host memory allocated with cudamallochost overlap cpu computation with data transfer default stream cudamemcpyasync a_d a_h size cudamemcpyhosttodevice kernel grid block a_d cpufunction overlapped nvidia corporation overlapping kernel and data transfer requires concurrent copy and execute deviceoverlap field of a cudadeviceprop variable kernel and transfer use different non zero streams a cuda call to stream blocks until all previous calls complete and cannot be overlapped example cudastreamcreate cudastreamcreate cudamemcpyasync dst src size dir kernel grid block overlapped nvidia corporation gpu cpu synchronization context based cudathreadsynchronize blocks until all previously issued cuda calls from a cpu thread complete stream based cudastreamsynchronize stream blocks until all cuda calls issued to given stream complete cudastreamquery stream indicates whether stream is idle returns cudasuccess cudaerrornotready does not block cpu thread nvidia corporation gpu cpu synchronization stream based using events events can be inserted into streams cudaeventrecord event stream event is recorded then gpu reaches it in a stream recorded assigned a timestamp gpu clocktick useful for timing cudaeventsynchronize event blocks until given event is recorded cudaeventquery event indicates whether event has recorded returns cudasuccess cudaerrornotready does not block cpu thread nvidia corporation outline overview hardware memory optimizations data transfers between host and device device memory optimizations measuring performance effective bandwidth coalescing shared memory textures execution configuration optimizations instruction optimizations summary nvidia corporation theoretical bandwidth device bandwidth of gtx ddr gb memory clock hz memory interface bytes specs report gb use b gb conversion rather than whichever you use be consistent nvidia corporation effective bandwidth effective bandwidth for copying array of n floats n b element time in secs gb array size bytes read and write b gb or nvidia corporation outline overview hardware memory optimizations data transfers between host and device device memory optimizations measuring performance effective bandwidth coalescing shared memory textures execution configuration optimizations instruction optimizations summary nvidia corporation coalescing global memory access of or bit words by a half warp of threads can result in as few as one or two transaction if certain access requirements are met depends on compute capability and have stricter access requirements examples float bit data global memory half warp of threads aligned segment floats aligned segment floats nvidia corporation coalescing compute capability and k th thread must access k th word in the segment or k th word in contiguous segments for bit words not all threads need to participate coalesces transaction out of sequence transactions misaligned transactions nvidia corporation coalescing compute capability and higher coalescing is achieved for any pattern of addresses that fits into a segment of size for bit words for bit words for and bit words smaller transactions may be issued to avoid wasted bandwidth due to unused words transaction segment transactions and segments transaction segment nvidia corporation coalescing examples effective bandwidth of small kernels that copy data effects of offset and stride on performance two gpus gtx compute capability peak bandwidth of gb fx compute capability peak bandwidth of gb nvidia corporation coalescing examples global void offsetcopy float odata float idata int offset int xid blockidx x blockdim x threadidx x offset odata xid idata xid nvidia corporation outline overview hardware memory optimizations data transfers between host and device device memory optimizations measuring performance effective bandwidth coalescing shared memory textures execution configuration optimizations instruction optimizations summary nvidia corporation shared memory hundred times faster than global memory cache data to reduce global memory accesses threads can cooperate via shared memory use it to avoid non coalesced access stage loads and stores in shared memory to re order non coalesceable addressing nvidia corporation maximize use of shared memory shared memory is hundreds of times faster than global memory threads can cooperate via shared memory not so via global memory a common way of scheduling some computation on the device is to block it up to take advantage of shared memory partition the data set into data subsets that fit into shared memory handle each data subset with one thread block load the subset from global memory to shared memory syncthreads perform the computation on the subset from shared memory each thread can efficiently multi pass over any data element syncthreads if needed copy results from shared memory to global memory nvidia corporation example square matrix multiplication c a b of size n x n without blocking one thread handles one element of c a and b are loaded n times from global memory wastes bandwidth poor balance of work to bandwidth n nvidia corporation example square matrix multiplication example c a b of size n x n with blocking one thread block handles one m x m sub matrix csub of c a and b are only loaded n m times from global memory much less bandwidth much better balance of work to bandwidth nvidia corporation shared memory architecture many threads accessing memory therefore memory is divided into banks successive bit words assigned to successive banks each bank can service one address per cycle a memory can service as many simultaneous accesses as it has banks multiple simultaneous accesses to a bank result in a bank conflict conflicting accesses are serialized bank bank bank bank bank bank bank bank bank nvidia corporation bank addressing examples no bank conflicts linear addressing stride no bank conflicts random permutation thread thread thread thread thread thread thread thread bank bank bank bank bank bank bank bank thread thread thread thread thread thread thread thread bank bank bank bank bank bank bank bank thread bank thread bank nvidia corporation bank addressing examples way bank conflicts linear addressing stride way bank conflicts linear addressing stride thread thread thread thread thread thread thread thread thread bank bank bank bank bank bank bank bank bank thread thread thread thread thread thread thread thread thread bank bank bank bank bank bank bank nvidia corporation shared memory bank conflicts shared memory is as fast as registers if there are no bank conflicts profiler signal reflects conflicts the fast case if all threads of a half warp access different banks there is no bank conflict if all threads of a half warp read the identical address there is no bank conflict broadcast the slow case bank conflict multiple threads in the same half warp access the same bank must serialize the accesses cost max of simultaneous accesses to a single bank nvidia corporation shared memory example transpose each thread block works on a tile of the matrix naïve implementation exhibits strided access to global memory idata odata elements transposed by a half warp of threads nvidia corporation coalescing through shared memory access columns of a tile in shared memory to write contiguous data to global memory requires syncthreads since threads access data in shared memory stored by other threads idata odata tile elements transposed by a half warp of threads nvidia corporation outline overview hardware memory optimizations execution configuration optimizations instruction optimizations summary nvidia corporation occupancy thread instructions are executed sequentially so executing other warps is the only way to hide latencies and keep the hardware busy occupancy number of warps running concurrently on a multiprocessor divided by maximum number of warps that can run concurrently limited by resource usage registers shared memory nvidia corporation blocks per grid heuristics of blocks of multiprocessors so all multiprocessors have at least one block to execute of blocks of multiprocessors multiple blocks can run concurrently in a multiprocessor blocks that aren t waiting at a syncthreads keep the hardware busy subject to resource availability registers shared memory of blocks to scale to future devices blocks executed in pipeline fashion blocks per grid will scale across multiple generations nvidia corporation register dependency read after write register dependency instruction result can be read cycles later scenarios cuda ptx to completely hide the latency run at least threads warps per multiprocessor at least occupancy 75 threads do not have to belong to the same thread block nvidia corporation register pressure hide latency by using more threads per multiprocessor limiting factors number of registers per kernel per multiprocessor partitioned among concurrent threads amount of shared memory per multiprocessor partitioned among concurrent threadblocks compile with ptxas options v flag use maxrregcount n flag to nvcc n desired maximum registers kernel at some point spilling into local memory may occur reduces performance local memory is slow nvidia corporation optimizing threads per block choose threads per block as a multiple of warp size avoid wasting computation on under populated warps facilitates coalescing more threads per block higher occupancy granularity of allocation eg compute capability max threads multiprocessor threads block occupancy threads block can have occupancy heuristics minimum threads per block only if multiple concurrent blocks or threads a better choice usually still enough regs to compile and invoke successfully this all depends on your computation so experiment replicas why and why not reliability tolerance to component failures tolerance to corrupted data performance benefits scalability allows for concurrent access to resources data objects processors reduces the delay for geographically dispersed resources disadvantages cost of replications is high maintaining consistency of multiple copies is tough implementation is harder e g different users have different needs of number of replicas and more or less accurate consistency models a group of otherwise authorized users of a specific service is said to deny service to another group of otherwise authorized users if the former group makes the specified service unavailable to the latter group for a period of time which exceeds the intended and advertised waiting time dos attacks aim at reducing legitimate utilization of network and or server resources through resource destruction exploit bugs in the os resource exhaustion vulnerability exploitation e g syn attack brute force flooding network level e g lots of packets as in udp floods service level e g flash crowds a large number of attack hosts request service from the victim server at a high rate for instance download files from an ftp server or get web pages from an www server front ends form a tree with the back ends as its logical root tree level of each front end depends on its attack tolerance front ends can be the bottleneck that gets attacked it usually can withstand a good amount of attack traffic to join the network or reconfigure a front end performs parent registration address registration legitimate client network level dos attacks flood network resources service level dos attacks exploit vulnerabilities to crash servers service level dos attacks flood server resources so that legitimate clients packets will be dropped distinguish attack packets requests from legitimate packets requests quickly accurately low false positives and false negatives and efficiently small overhead primary metrics legitimate response time legitimate throughput prevention detection recovery mitigation network level network level puzzles packetscore red pd replication overlay based heavy hitter detection dcap pushback move capabilities ip hopping service level application level puzzles reservation based schemes ddos shield shadow honeypots kill bots replication honeypots are decoy resources to trap attackers useful in detecting worm infected hosts however honeypots are at fixed locations separate from real servers in roaming honeypots the locations of honeypots are continuously changing unpredictable to non compliant attackers disguised within servers unique un spoofable user identifier dealing with proxy servers is an open problem white list allow packets from certain users ips not scalable because list grows with number of users black list do not allow certain ips or users more scalable attackers users with increasing costs of energy consumption and cool ing power management in server clusters has become an increasingly important design issue current clusters for real time applications are designed to handle peak loads where all servers are fully utilized in practice peak load conditions rarely happen and clusters are most of the time underutilized this creates the opportunity for using slower frequencies and thus smaller energy consumption with lit tle or no impact on the quality of service qos for exam ple performance and timeliness in this work we present a cluster wide qos aware tech nique that dynamically reconfigures the cluster to reduce energy consumption during periods of reduced load more over we also investigate the effects of local qos aware power management using dynamic voltage scaling dvs since most real world clusters consist of machines of dif ferent kind in terms of both performance and energy con sumption we focus on heterogeneous clusters for validation we describe and evaluate an implemen tation of the proposed scheme using the apache webserver in a small realistic cluster our experimental results show that using our scheme it is possible to save up to of the total energy consumed by the servers maintaining average response times within the specified deadlines and number of dropped requests within the required amount introduction until recently performance had been the main concern in server farms but energy consumption has also become a main concern in such systems due to the importance of customer care service in commercial installations and the importance of timely responses for embedded server clus ters current clusters are typically designed to handle peak supported by nfs through the secure citi project ani and the power autonomous networks project ani claudio scordino is a phd student at the university of pisa visiting the university of pittsburgh loads however peak load conditions rarely happen in prac tice and clusters are most of the time underutilized in fact their loads often vary significantly depending on the time of the day or other external factors therefore the average pro cessor use of such systems may be even less than with respect to their peak capacity clusters with high peak power need complex and expen sive cooling infrastructures to ensure the proper operation of the servers with power densities increasing due to in creasing performance demands and tighter packing proper cooling becomes even more challenging fans driving the cooling system may consume up to of the total sys tem power in some commercial servers 18 and man ufacturers are facing the problem of building powerful sys tems without introducing additional techniques such as liq uid cooling electricity cost is a significant fraction of the operation cost of data centers for example a google rack consumes about a month including cooling which is at least of the operation cost with this fraction likely to increase in the future these issues are even more critical in embedded clus ters typically untethered devices in which peak power has an important impact on the size of the system while energy consumption determines the device lifetime exam ples include satellite systems or other mobile devices with multiple computing platforms such as the mars rover and robotics platforms power management pm mechanisms can be divided into two categories cluster wide and local cluster wide mechanisms involve global decisions such as turning on and off cluster machines according to the load lo cal techniques put unused or underutilized resources in low power states for example self refresh standby and off modes for dram chips dynamic voltage scaling dvs and low power states for the cpu disk shutdown etc a pm mechanism local or cluster wide is qos aware if it reduces the power consumption while guaranteeing a cer tain amount of quality of service qos such as average response times or percentage of deadlines met to the best of our knowledge this is the first power management scheme that is simultaneously a cluster wide i e turning on and off machines b designed for hetero geneity c qos aware and power aware at the local servers i e deadline aware d measurement based contrary to theoretical modeling relying on measurements is the key to our approach e implementation oriented and f per forming reconfiguration decisions at runtime our scheme is realistic because most clusters have one or more front ends are composed of different kind of ma chines and need both local and cluster wide qos aware pm schemes while the methodology and the algorithms proposed apply to any kind of cluster we show their use in a web server context our measurements show a reduc tion of energy consumption equal to using only the local pm using the on off scheme and using both schemes with respect to delays the local pm added while on off added about in all cases the av erage delay was quite small with respect to deadlines the remainder of the paper is organized as follows we first present related work in section the cluster model is given in section the cluster wide pm scheme is ex plained in section while the local real time dvs scheme is presented in section both schemes are then evaluated in section in section we state our conclusions related work with energy consumption emerging as a key aspect of cluster computing much recent research has focused on pm in server farms a first characterization of power con sumption and workload in real world webservers was made in dvs was proposed as the main technique to re duce energy consumption in such systems dvs and re quest batching techniques were further evaluated in software peak power control techniques were investigated in however these studies considered only power consumption of processor and main memory in single processor systems the problem of cluster configuration i e turning on and off cluster machines for homogeneous clusters was first ad dressed in an offline algorithm determines the number of servers needed for a given load cluster reconfiguration is then performed online by a process running on a server using thresholds to prevent too frequent reconfigurations even though there is no explicit qos consideration the authors have extended their work to heterogeneous clus ters in models have been added for throughput and power consumption estimation reconfiguration decisions are made online based on the precomputed information and the predicted load the authors also proposed to add request types to improve load estimation in our work differs from the above previous studies in the following ways we consider qos directly individual servers are both power aware and qos aware we rely on offline measurements instead of using models reconfigura tion decisions i e number of active servers and load dis tribution are not expensive and are performed online and reconfiguration thresholds are based on the time needed to boot shutdown a server one of the first attempts to combine cluster wide and lo cal pm techniques proposed five different policies com bining dvs and cluster configuration however the the ory behind this work relies on a homogeneous clusters and cannot be easily extended to heterogeneous machines and b the often incorrect assumption that power is a cubic function of the cpu frequency another work proposed to use the cluster load instead of the average cpu frequency as the criteria for turning on off machines however this study assumed homogeneous clusters as well to the best of our knowledge this work is the first attempt to com bine cluster wide and local techniques in the context of het erogeneous clusters in real time computing dynamic voltage and fre quency scaling has been explored to reduce energy con sumption dvs schemes typically focus on minimizing cpu energy consumption while meeting a performance re quirement dvs work for aperiodic tasks in single processors includes offline and online algorithms assum ing worst case execution times automatic dvs for linux with distinction between background and interactive jobs and use of knowledge about the distribution of job lengths for voltage scaling decisions however these techniques typically aim at reducing the energy con sumed only by the cpu and do not take into account other devices such as memory power supplies or disk that contribute with an important fraction to the to tal energy consumed by the system in our model instead servers can put their resources in low power states and no assumption is made about their local pm schemes most related to our local scheme is sharma et al work on adaptive algorithms for dvs for a qos enabled web server their scheme uses a theoretical utilization bound derived in to guarantee the qos of web requests however they take into account only local pm assuming that a good load balancing algorithm is used at the front end in that sense our works are complementary since we describe how to achieve such load balancing cluster model this section introduces the cluster model that we con sider see figure a front end machine receives requests from clients and redirects them to a set of processing nodes henceforth referred to as servers the front end is not a processing node and has three main functions a accept ing aperiodic requests from clients b distributing the load to servers and c reconfiguring the cluster i e turning servers on off to reduce the global energy consumption while keeping the overall performance within a prespeci fied qos requirement after receiving a request the front end communicates to the client to which server the request must be sent using http redirection then the client sends its request directly to the server in our cluster scheme each request is an aperiodic task i e no assumptions are made about task arrival times and is assigned a deadline the specification of the qos is system wide and is in our case the percentage of deadlines met the way to achieve the soft real time properties will be presented in detail in the next sections each server in the heterogeneous cluster performs the same service i e all servers can process all requests no restriction is imposed regarding any aspect of their com putation process scheduling cpu performance memory speed bandwidth disk speed bandwidth power consump tion network bandwidth etc in addition servers periodi cally inform the front end about their current load to aid the front end in load distribution and cluster configuration de cisions after a request has been processed by a server the result is returned directly to the client without the front end as intermediary note that a more common cluster design is with the front end acting as a proxy i e acting as intermediary between clients and servers in our webserver example choosing one configuration or the other i e proxy versus no proxy with redirection is simply a configuration option and the proposed scheme in this paper works equally well with either type of front end in our experiments for high loads above we had to use the no proxy architec ture shown in figure as a proxy front end cannot fully utilize the cluster in our experimental setup our front end has only one gbe network interface card when using redirection instead of proxying the links and internal references should use the full url to guaran tee that all the requests are sent to the front end this way redirection works with either the http or the http protocols in http the client may keep multiple con nections open to the front end and the servers it was redi rected to but all the requests will be sent to the front end first the aspects related to cluster configuration pm and load distribution performed by the front end will be presented in detail in the next section local pm is performed indepen dently by each server without front end control and will be presented in section front end power management our proposed front end follows a very general frame work that is applicable to any heterogeneous cluster to achieve this goal we cannot impose any restriction on figure cluster architecture server characteristics however for ease of presentation definitions and examples emphasize web server clusters load definition and estimation the front end determines the number of active servers to meet the desired level of qos while minimizing cluster en ergy consumption the number of servers is computed of fline or online as a function of the system load thus defin ing load correctly is a crucial step a measure of the load for clusters is the number of requests received per second measured over some recent interval clearly depending on the kind of service under consideration other definitions of load may be more appropriate such as the bandwidth for a file server at runtime the front end needs to correctly estimate or observe the load in order to make pm decisions and to per form load distribution the load estimation can be further improved by using feedback from the servers as observed in previous work load estima tion can be greatly improved by considering request types the type of a request may be conveniently determined only by the header e g the name of the requested file notice that the number of types is a design issue on one hand different types may not be necessary if the variability of the time to service a request is low on the other hand each request could be of a different type leading to an im proved estimation but also to a higher overhead to measure all different types of requests and update statistics tables in the case of a web server there are two main types of requests with different computational characteristics static and dynamic pages static pages reside in server memory and do not require much computation dynamic pages in stead are created on demand through the use of some exter nal language e g perl or php for this reason dynamic pages typically require more computation than static ones consider a generic server and let astatic and adynamic be the average execution times to serve a static and a dy namic page respectively at the maximum cpu speed for example for one server in our cluster we measured an av erage execution time astatic for static pages and adynamic 5ms for dynamic pages on average the time needed by the cpu to serve nstatic static requests and ndynamic dynamic requests is thus nstaticastatic ndynamicadynamic seconds if the number of requests is observed over a period of monitor period seconds then the load of the machine serving the requests is once the local pm scheme the os and the scheduling policy have been decided for a server the power consump tion as function of the load and the maximum load can be determined through simple measurements in our experiments after choosing the local pm scheme see section we measured the average power consump tion for a load in increments then we interpolated the points to have values in increments we measured the total power consumed by the whole machines not only by their cpus in our case recording the average power consumption for a given load over a period of minutes load nstaticastatic ndynamicadynamic monitor period was sufficient to obtain a good average we measured ac power directly with a simple power meter with accu notice that this definition of load assumes a cpu bound server this is normal for most web servers because much of the data are already in memory 27 in fact on all our machines we have noticed that the bottleneck of the system was the cpu however for systems with different bottle necks e g disk i o or network bandwidth another defini tion of load may be more appropriate in fact the definition of load should account for the bottleneck resource note that even though web requests may exhibit a large variation in execution times using the average values astatic and adynamic in equation results in a very accurate load es timation this is because web requests are relatively small and numerous we define the maximum load of a server as the max imum number of requests that it can handle meeting the of deadlines the front end never directs more than the maximum load to a server the cluster load is de fined as the sum of the current loads of all active servers therefore the maximum load that the cluster can handle is the sum of the maximum loads of all servers at runtime the cluster load i e both variables nstatic and ndynamic is observed every monitor period seconds the value of monitor period is a design issue related to the tradeoff between response time and overhead in our cluster values in the order of a few seconds were found suitable server information in order to reduce the global power consumption at run time we furnish the front end with information about the average power consumption of each server for any differ ent value of its load servers can reduce their own power consumption in a number of different ways such as using dvs and low power states for the cpu self refresh modes for memory stopping disk spinning after some time of idle ness etc moreover each server may use a different os or a different scheduling policy such as a standard round robin or a real time policy to give higher priority to static pages with respect to dynamic ones no assumption is made at the front end about local pm schemes racy hence the whole process required at most few hours for each machine clearly identical machines need not to be measured twice the curve representing the power consumption of each server of our cluster is shown in fig ure the last point on each curve represents the maximum load that meets our qos specification i e of dead lines met normalized to the fastest machine in the cluster the parameters for each machine are reported in table on page 140 load figure power consumption vs load for servers the information about power consumption of servers can then be used by the front end at runtime since the front end controls the load for each server and the power consump tion of each server for a given load is known the front end has enough information for pm decisions notice that the cooling costs of the room have not been taken into account however since these costs are expected to be proportional to the ac power drawn they are automatically reduced by minimizing cluster power consumption we now present all the information and the corre sponding notation about each server needed at the front end level boot timei and shutdown timei represent the time to boot and to shutdown server i including the time to start and finish the webserver process of the server max loadi is the maximum load of server i that can satisfy the qos requirement off poweri is the power con sumed when the server is off some components such as the wake on lan interface used to power up the machine an additional interval of time to account for the error when measuring the current load in general server i is turned on when the cluster load reaches may not be completely off finally power vs loadi is an i boot time max loadi load increment entries recording the measured max loadj i monitor period max load increase power consumption of server i for each value of the load in load increment percents we used the first entry of the array denotes the idle power i e no load on off policy this section describes the key idea behind our cluster wide pm scheme the front end besides distributing the load to servers to minimize global power consumption de termines the cluster configuration by turning on off servers below is the algorithm used by the front end to decide which servers will be turned on off the algorithm turns machines on and off in a specific order which is based on the power efficiency of servers i e the integral of power consumption versus load in our case according to the values of figure the ordering for our cluster is transmeta blue silver green in some situations we may need to change the order at runtime as explained later the front end turns on servers as the cluster load in creases however since the boot time is not negligi ble we need to turn machines on before they are actually needed for this reason the front end maintains a variable called max load increase which specifies the maximum load variation that the cluster is prepared to sustain during monitor period this is essentially the maximum slope of the load characterization for the cluster the on off policy relies on two tables computed of fline the first table called mandatory servers keeps the load at which to turn servers on and is used to de termine the lowest number of servers needed at a certain load to satisfy the qos requirement for example con sider a cluster with three servers having maximum loads max max and max respectively suppose that monitor period is sec onds max load increase is equal to and the boot time is seconds for every machine ideally we need only one server when the cluster load is less than two servers when load is between and and all servers when load is higher than however if we account for the time to boot a new machine and we suppose that the cluster load is checked periodically every monitor period seconds the table becomes mandatory servers thus the first server is always on whereas the second and third servers are turned on when the cluster load reaches 35 and 85 respectively in fact if we consider the boot time of a new server we have to account for a potential j the second table called power servers precomputes the number of servers needed to minimize the power con sumption for a given load unlike the previous table this table is computed considering the power consumption of servers and is used to distribute the current load among active servers for a given value of n we compute the power consumption of the cluster as follows we start con sidering a load equal to zero and we increase its value in load increment increments for any increment of the load we evaluate which server can handle it in order to min imize the overall energy consumption to determine the load at which n servers become more power efficient than n we follow this procedure con sidering both cases of n and n machines respectively the load at which n servers consume less power than n servers is the value after which the n th server is turned on the server to be turned on is the next one according to the power efficiency order the complexity of computing the two tables is o n where n is the number of servers for mandatory servers and o n m for power servers i load increment in our cluster the time to compute these two tables was less than which was negligible compared to monitor period that is in the range of seconds thus this computation can also be performed online for example a new ordering of the servers and an online recalculation of the tables become necessary when a server crashes a high level view of the front end on off policy is presented in figure every monitor period seconds the load is estimated according to equation then the request counters are reset the number of mandatory servers nmandatory is determined by a lookup in the mandatory servers table if nmandatory is higher than the current number of active servers ncurrent all needed servers are immediately turned on each server can be in one of the following states off boot on or shutdown after receiving the boot com mand such as a wake on lan packet the server i moves from the off to the boot state it stays in this state for boot timei seconds i e until it starts the server process then informs the front end that it is available for processing moving to the on state when server i is shutdown it stays in the shutdown state for shutdown timei seconds after load increase equal to boot time max load increase that the front end changes its state to off moreover if we suppose that the load is checked periodi cally every monitor period seconds we have to introduce the variable cmd in figure can have three different values none boot or shutdown this variable allows to mandatory servers and power servers contain the optimal transition points in the discrete space for continu ous space see in practice however power functions may have concave regions this means that a server with an abrupt power increase at some load x may not be allocated more than x load even though the power may become flat above x ǫ making it a good target for load allocation a simple fix to the problem is to consider the average power consumption over a larger interval rather than the exact value at each load this effectively results in smoothing the power functions in our case although the measured power functions have concave regions we have found that no smoothing was necessary request distribution policy the front end distributes the incoming requests to a subset of the current servers that are in the on state load allocation is a table containing the estimated load allocated to each server and is computed with the same procedure used to determine the power servers table in o m n time the load allocation is computed every monitor period seconds after the on off decisions another table called load accumulated stores the ac cumulated load of each server and is reset after computing load allocation the server i with the minimum weight w load accumulatedi i load allocationi figure on off policy describe the use of thresholds when turning on off servers if no server is in transition i e all servers are in the on or off states a server may be turned on or off as decided after a lookup in the power servers table to be conser vative only one server at a time is turned on or off server i is turned off if the system is in state cmd shutdown for at least time shutdowni time booti consecutive sec onds which is the rent to own threshold see step fig ure similarly server i is turned on if cmd boot for time booti consecutive seconds see step figure notice that these thresholds do not apply to the mandatory servers which are started immediately the running time of the online part of the algorithm every monitor period sec onds is negligible because it is in the microsecond range the complexity is o n but can be improved to o by increasing the table size from n to m that is storing all entries in an array for convex and linear power functions tables gets the next request notice that wi can be higher than when the load is underestimated the server that re ceives the request updates its accumulated load and thus in creases its weight by adding astatic monitor period or adynamic monitor period depending on the request type the complexity to find the server with minimum weight is o n with a straightforward implementation but can be improved to o logn using a tree implementation issues we implemented our pm scheme in the apache web server we created an apache module called mod on off which makes on off decisions moreover we extended an existing module mod backhand to support our distribution policy mod backhand is a module responsible for load distri bution in apache clusters it allows servers to exchange information about their current usage of resources it also provides a set of candidacy functions to forward requests from an overloaded server to other less utilized servers ex amples of such functions are byload which selects as can didate the least loaded server and bycost which considers a cost for each request we added a new candidacy function called byenergy to implement our request distribution policy notice that only front end machines use this function in addition servers provide some feedback about their current real time utiliza tion as explained in section to front ends we used this feedback to prevent the overloading of the servers in partic ular the server with the minimum wi is selected providing that it is not overloaded the mod on off module communicates with mod backhand through shared memory on initializa tion mod on off acquires server information and computes both mandatory servers and power servers tables mod on off executes periodically every monitor period seconds on each invocation it performs the following tasks a computes the current load based on the counters nstatic and ndynamic that are incremented in the apache post read request phase b looks up in the table to determine the number of servers needed for the next period c computes the load allocation table for the active servers not shown in figure d turns on by send ing wake on lan packets and off by invoking special cgi scripts servers and finally e resets the counters nstatic ndynamic and load accumulated in addition it displays at runtime the estimated power and energy consumption of each server based on the power vs load and load accumulated tables server power management in addition to front end directed cluster reconfigurations i e turning on off machines the servers perform their own local pm to reduce power consumption of unutilized or underutilized resources we present an example of a qos aware dvs scheme and we discuss an implementation us ing the apache webserver local dvs policy we rely on a local real time scheme where each request is an aperiodic task and is assigned a deadline each request type has a deadline to allow for more accurate load estimation we consider a soft real time system in which the sched ule is not generated by a real time scheduler and the com putation time ci is the average execution time i e astatic or adynamic not the worst case let di be the time re maining to the deadline then the real time utilization of a u and sets the cpu frequency to the closest value higher than u fmax note that dvs architectures may have inefficient oper ating frequencies which exist when there are higher frequencies that consume less energy a simple online tool for inefficient frequency elimination has been provided in removal of inefficient operating frequencies is the first step in any dvs scheme this was not necessary in our servers because surprisingly all frequencies were efficient although we had a different experience with other systems we tested implementation issues we implemented an apache module called mod cpufreq responsible for cpu speed settings at the user level on athlon machines the cpu speed was changed by writing to the sys file system using the cpufreq interface on the transmeta machine the speed was changed by writing to a model specific register msr since the register cannot be written from user level we added two system calls for setting and reading its value 13 after detecting the available frequencies our module creates an apache process that periodically sets the cpu frequency according to the current value of u we chose as period to match any default linux kernel the measured overhead for changing voltage frequency in the machines is approximately to compute u the module needs to know the type i e static or dynamic and the arrival time of each request at every request arrival called apache post read request phase the arrival time and the deadline are recorded with µs accuracy and stored in a hash table in shared memory the request type is determined from the name of the re quested file thus a single queue traversal is necessary to compute u in fact the current value of u depends on all queued requests therefore the complexity is o r where r is the number of requests queued the overhead is neg ligible requests are removed from the queue after being served called apache logging request phase a problem we encountered during the implementation was that our scheme worked very well except for fast ma chines serving a large amount of small static pages in this case those machines were not increasing their speed re sulting in a large number of dropped requests a further investigation revealed that the value of u was close to zero we did not see this phenomenon on slower machines such as transmeta nor using bigger pages the problem was that server is defined as u ci i di the requests were served too fast in approximately µs if the cpu is the bottleneck of the system as in our case the cpu frequency to handle this rate of requests is u fmax where fmax is the highest possible frequency of the cpu each server periodically computes its utilization such short requests were queued served and removed from the queue before other requests were added to the queue thus at any time only a few requests usually just one was in the queue and when mod cpufreq recomputed the utiliza tion it resulted in an underestimation of u in other words even though the requests were received and queued at the os level apache was not able to see them because it is a user level server and it has no information about requests stored at the os level we called this problem the short request overload problem phenomenon a simple fix was to compute the utilization also over a recent interval of time interval size we used transmeta blue silver urecent nstaticastatic ndynamicadynamic interval size green we would like to keep the server utilization urecent be low a certain threshold we used threshold the minimum frequency that does that is urecent fmax thus our module sets the cpu speed to max u urecent fmax table idle busy power consumption in watts for each note that sharma et al work with a kernel webserver khttpd aware of small requests at the os level has a nice synergy with our approach and could be used in lieu of our scheme exploring the composition of our cluster configuration and sharma or other similar dvs work is left for future work the problem with including such work in our scheme is exactly the reason why the authors discon tinued the development of khttpd the difficulty of main taining developing and debugging a kernel level server evaluation server at each frequency servers proposed in which we implemented at user level in our mod cpufreq module this scheme adjusts the speed of the processor to the minimum speed that maintains a quantity called synthetic utilization below the theoretical utilization bound ubound that ensures that all deadlines are met 120 to evaluate our qos aware pm scheme we used a small cluster composed by one front end and different servers every machine ran gentoo linux as operating system and apache servers the parameters of the machines are shown in tables and the cluster has been tested using clients connected to the cluster with a gbe interface and gbps switch the clients generate up to 186 requests per second which cor responds to a total maximum cluster load equal to all loads were normalized to that of silver machine a total cluster load of 05 or corresponds on average to re quests second considering request types however greatly improves the prediction as requests second may corre spond to a load ranging from 02 if ndynamic to default linux sharma scheme our scheme 80 load figure comparison of dvs policies 32 if nstatic we assigned deadlines of and for requests of static and dynamic pages we set max load increase 005 therefore we had mandatory servers 000 062 012 012 and power servers 000 dvs policy as first experiment we evaluated the effectiveness of our local dvs scheme we compared our mod cpufreq module with the default pm in linux i e halt instruction when idle and with sharma dvs scheme for qos aware web the measured power consumption of each scheme on the blue machine is shown as function of the load in figure the graph shows that our scheme outperforms the other schemes especially for the mid range load values higher savings are obtained on machines with a more convex power function the power function of the blue machine is rather linear see figure in fact for a rate of requests sec approximately 28 load the average processor frequency is 25ghz using our scheme and 5ghz using sharma s scheme but the amount of energy saved is only impor tantly we observed that both schemes maintained the qos level above even at the highest load transmeta transmeta crusoe mb kb blue amd athlon mobile mb silver amd athlon kb green front end amd athlon amd athlon mobile 1gb 512 kb mb not applicable 90 table parameters of the machines of the cluster overall scheme to evaluate the overall scheme we performed many ex periments with and without the cluster wide pm scheme on off scheme and with and without the local pm scheme dvs scheme for each load value we measured the power consumption of the entire machine not only cpu for each scheme independently see figure for fairness we used the load balancing policy in section for all the schemes the on off policy allows a striking reduction of the en ergy consumption for low values of the load because ob viously it allows to turn off unutilized servers in fig ure we can see that when load the cluster consump tion is around because each athlon server consumes when in the off state and the transmeta also consumes when in the on state the dvs technique instead has its biggest impact whenever a new server is turned on since not all active servers are fully utilized however its im portance decreases as the utilization of the active servers increases for high values of the load in our case at being on at all times and running at maximum frequency adding dvs local pm had a very small impact on the de lay with the average delay measured at however with on off scheme we measured an average delay equal to 29ms without dvs and 83ms with dvs in both cases the average delay was not higher than of the no pm delay and was quite small with respect to deadlines request type request type ms cgi 6 kb html 2 84 ms cgi 71 kb html 58 ms cgi 98 kb html 80 ms cgi 23 9 10 kb html 87 ms cgi 06 10 kb html 10 74 kb html 20 kb html 62 2 kb html 86 kb html 2 kb html 6 56 40 kb html 67 kb html 58 kb html 80 4 kb html 4 94 60 kb html 1 46 6 kb html 3 38 above 70 kb html 5 27 table 3 web server statistics percentage of accesses approximate size for static pages and running time for dynamic pages or higher all servers are on therefore the on off technique does not allow to reduce energy consumption in those sit uations however there is still room for the dvs technique that becomes more important than the on off technique the energy consumption of all servers without any power management scheme was 1 32kw h on average we measured energy savings of 17 using dvs using on off and using both schemes it is worth noting that the front end estimation of the to tal energy consumed when using dvs was extremely ac curate the difference from the actual values was less than 1 for example when using the on off scheme the mea sured value was 72kw h while the front end estimated value was 725kw h the resolution of our power energy meter is 0 01kw h 150 0 0 20 40 60 80 load to measure the impact of cluster wide and local pm schemes in the loss of qos we ran many four hour ex periments with workloads derived from actual webserver traces and generated with the same shape of statistics taken from our cs pitt edu domain see table 3 the average de lay observed at the client side without any pm scheme was 29ms a small response time is due to all machines figure 5 evaluation of cluster wide and local techniques 7 conclusions and future work we have presented a new qos aware power manage ment scheme that combines cluster wide on off and lo cal dvs power management techniques in the context of heterogeneous clusters we have also described and evalu ated an implementation of the proposed scheme using the apache webserver in a small realistic cluster our experimental results show that a our load estima tion is very accurate b the on off policy allows a striking reduction of the power consumption c dvs is very im portant whenever a new server is turned on or as shown before when all servers are on d as expected for high values of the load the on off technique does not help to reduce energy consumption but there is still room for dvs using both techniques we saved up to of the total energy with a limited loss in terms of qos in the worst case the average delay was increased by at most 50 and was still very small when compared to the deadlines as immediate future work we plan to investigate the use of both suspend to disk and suspend to ram techniques to reduce the time to boot and shutdown a server we also plan an integration of our cluster pm schemes with other grid like or cluster e g condor load balancing schemes mark d hill university of wisconsin madison michael r marty google augmenting amdahl law with a corollary for multicore hardware makes it relevant to future generations of chips with multiple processor cores obtaining optimal multicore performance will require further research in both extracting more parallelism and making sequential cores faster we enter the multicore era we re at an inflection point in the computing landscape computing vendors have announced chips with multiple processor cores moreover vendor road maps promise to repeatedly double the number of cores per chip these future chips are variously called chip multiprocessors multicore chips and many core chips designers must subdue more degrees of freedom for multicore chips than for single core designs they must address such questions as how many cores should cores use simple pipelines or powerful multi issue pipeline designs should cores use the same or different micro architectures in addition designers must concurrently manage power from both dynamic and static sources although answering these questions for today multi core chip with two to eight cores is challenging now it will become much more challenging in the future sources as varied as intel and the university of california berkeley predict a hundred if not a thousand cores as the amdahl law sidebar describes this model has important consequences for the multicore era to complement amdahl software model we offer a corol lary of a simple model of multicore hardware resources our results should encourage multicore designers to view the entire chip performance rather than focusing on core efficiencies we also discuss several important limitations of our models to stimulate discussion and future work a corollary for multicore chip cost to apply amdahl law to a multicore chip we need a cost model for the number and performance of cores that the chip can support we first assume that a multicore chip of given size and technology generation can contain at most n base core equivalents where a single bce implements the baseline core this limit comes from the resources a chip designer is willing to devote to processor cores with caches it doesn t include chip resources expended on shared caches interconnection networks memory controllers and so on rather we simplistically assume that these nonprocessor resources are roughly constant in the mul ticore variations we consider we are agnostic on what limits a chip to n bces it might be power area or some combination of power area and other factors second we assume that micro architects have techniques for using the resources of multiple bces to create a core with greater sequential performance let the performance of a single bce core be we assume that architects can expend the resources of r bces to create a powerful core with sequential per formance perf r architects should always increase core resources when perf r r because doing so speeds up both sequential and parallel execution when perf r r however the trade off begins increasing core performance aids sequential execution but hurts parallel execution ieee published by the ieee computer society july amdahl law everyone knows amdahl law but quickly for gets it thomas puzak ibm most computer scientists learn amdahl law in school let speedup be the original execution time divided by an enhanced execution time the modern version of amdahl law states that if you enhance a fraction f of a computation by a speedup s the overall speedup is finally amdahl argued that typical values of f were large enough to favor single processors despite their simplicity amdahl arguments held and mainframes with one or a few proces sors dominated the computing landscape they also largely held in the minicomputer and personal computer eras that followed as recent technology trends usher us into the multicore era amdahl law is still relevant speedup enhanced f s s amdahl equations assume however that the computation problem size doesn t change when running on enhanced machines that is the frac amdahl law applies broadly and has important corollaries such as attack the common case when f is small optimi zations will have little effect the aspects you ignore also limit speedup as s approaches infinity speedup is bound by f four decades ago gene amdahl defined his law for the special case of using n processors cores in parallel when he argued for the single processor approach validity for achieving large scale computing capa bilities he used a limit argument to assume that a fraction f of a program execution time was infinitely parallelizable with no scheduling overhead while the remaining fraction f was totally sequential without presenting an equation he noted that the speedup on n processors is governed by tion of a program that is parallelizable remains fixed john gustafson argued that amdahl law doesn t do justice to massively parallel machines because they allow computations previously intrac table in the given time constraints a machine with greater parallel computation ability lets com putations operate on larger data sets in the same amount of time when gustafson arguments apply parallelism will be ample in our view how ever robust general purpose multicore designs should also operate well under amdahl more pessimistic assumptions references g m amdahl validity of the single processor approach to achieving large scale computing capabilities proc am federation of information processing societies conf afips press pp j l gustafson reevaluating amdahl law comm acm speedup parallel f n n may pp our equations allow perf r to be an arbitrary func tion but all our graphs follow shekhar and and show two hypothetical symmetric multicore chips for n assume perf r in other words we assume efforts under amdahl law the speedup of a symmetric that devote r bce resources will result in sequential multicore chip relative to using one single bce core performance thus architectures can double per depends on the software fraction that is parallelizable formance at a cost of four bces triple it for nine bces and so on we tried other similar functions for example r but found no important changes to our results symmetric multicore chips a symmetric multicore chip requires that all its cores have the same cost a symmetric multicore chip with a f the total chip resources in bces n and the bce resources r devoted to increase each core perfor mance the chip uses one core to execute sequentially at performance perf r it uses all n r cores to exe cute in parallel at performance perf r n r overall we get resource budget of n bces for example can sup port cores of one bce each four cores of four bces each or in general n r cores of r bces each our equa speedup symmetric f f r perf r perf r n tions and graphs use a continuous approximation instead consider figure it assumes a symmetric multi of rounding down to an integer number of cores figures core chip of n bces and perf r the x axis computer figure varieties of multicore chips a symmetric multicore with one base core equivalent cores b symmetric multicore withfourfour bcecores and c asymmetricmulticorewithonefour bcecoreand one bcecores thesefiguresomitimportant structures such as memory interfaces shared caches and interconnects and assume that area not power is a chip limiting resource figure speedup of a b symmetric c d asymmetric and e f dynamic multicore chips with n bces a c and e or n bces b d and f july gives resources used to increase each core perfor mance a value says the chip has base cores while a value of r uses all resources for a single core lines assume different values for the parallel fraction f the y axis gives the symmet with more resources to execute sequentially at per formance perf r in the parallel fraction however it gets performance perf r from the large core and performance from each of the n r base cores overall we get ric multicore chip speedup relative to its running on one single bce base core the maximum speedup for f for example is using eight cores at a cost of two bces each speedup asymmetric f f perf r perf r n r similarly figure illustrates how tradeoffs change when moore law allows n bces per chip with f for example the maximum speedup of occurs with cores of bces each result amdahl law applies to multicore chips because achieving the best speedups requires fs that are near thus finding parallelism is still critical implication researchers should target increasing f through architectural support compiler techniques pro gramming model improvements and so on this implication is the most obvious and important recall however that a system is cost effective if speedup exceeds its costup multicore costup is the multicore system cost divided by the single core system cost because this costup is often much less than n speedups less than n can be cost effective result using more bces per core r can be opti figure shows asymmetric speedup curves for n bces while figure gives curves for n bces these curves are markedly different from the cor responding symmetric speedups in figures and the symmetric curves typically show either immediate performance improvement or performance loss as the chip uses more powerful cores depending on the level of parallelism in contrast asymmetric chips often reach a maximum speedup between the extremes result asymmetric multicore chips can offer poten tial speedups that are much greater than symmetric multicore chips and never worse for f and n for example the best asymmetric speedup is whereas the best symmetric speedup is implication researchers should continue to investi gate asymmetric multicore chips including dealing with the scheduling and overhead challenges that amdahl mal even when performance grows by only for a model doesn t capture given f the maximum speedup can occur at one big core n base cores or with an intermediate number of middle sized cores recall that for n and f the maximum speedup occurs using bces per core implication researchers should seek methods of increasing core performance even at a high cost result moving to denser chips increases the likeli hood that cores will be nonminimal even at f result denser multicore chips increase both the speedup benefit of going asymmetric and the optimal performance of the single large core for f and n an example not shown in our graphs the best speedup is at a hypothetical design with one core of bces and single bce cores implication researchers should investigate methods of speeding sequential performance even if they appear minimal base cores are optimal at chip size n but locally inefficient for example perf r this is more powerful cores help at n implication as moore law leads to larger multi core chips researchers should look for ways to design more powerful cores asymmetric multicore chips an alternative to a symmetric multicore chip is an asymmetric or heterogeneous multicore chip in which one or more cores are more powerful than the others with the simplistic assumptions of amdahl law it makes most sense to devote extra resources to increase only one core capability as figure shows with a resource budget of n bces for example an asymmetric multicore chip can have one four bce core and one bce cores one nine bce core and seven one because these methods can be globally efficient as they reduce the sequential phase when the chip other n r cores are idle dynamic multicore chips what if architects could have their cake and eat it too consider dynamically combining up to r cores to boost performance of only the sequential component as fig ure shows this could be possible with for example thread level speculation or helper threads in sequen tial mode this dynamic multicore chip can execute with performance perf r when the dynamic techniques can use r bces in parallel mode a dynamic multicore gets performance n using all base cores in parallel overall we get bce cores and so on in general the chip can have n r cores because the single larger core uses r resources and speedup dynamic f n r f f leaves n r resources for the one bce cores amdahl law has a different effect on an asym perf r n figure displays dynamic speedups when using r metric multicore chip this chip uses the one core cores in sequential mode for perf r for n computer bces while figure gives curves for n bces as the graphs show performance always gets better as the software can exploit more bce resources to improve the sequential component practical consid erations however might keep r much smaller than its maximum of n result dynamic multicore chips can offer speed ups that can be greater and are never worse than asymmetric chips with identical perf r functions with amdahl sequential parallel assumption how ever achieving much greater speedup than asymmetric chips requires dynamic techniques that harness more cores for sequential mode than is possible today for f and n for example effectively harness ing all cores would achieve a speedup of which is much greater than the comparable asymmet ric speedup of this result follows because we assume that dynamic chips can both gang all resources together for sequential execution and free them for parallel execution implication researchers should continue to inves tigate methods that approximate a dynamic multicore chip such as thread level speculation and helper threads even if the methods appear locally inefficient as with asymmetric chips the methods can be globally efficient although these methods can be difficult to apply under amdahl extreme assumptions they could flourish for software with substantial phases of intermediate level parallelism simple as possible but no simpler amdahl law and the corollary we offer for multicore hardware seek to provide insight to stimulate discussion and future work nevertheless our specific quantitative figure dynamic multicore chip with one bce cores essimists will bemoan our model simplicity and lament that much of the design space we explore can t be built with known techniques we charge you the reader to develop better models and more importantly to invent new software and hardware designs that realize the speedup potentials this article displays moreover research leaders should temper the current pendulum swing from the past underemphasis on parallel research to a future with too little sequen tial research to help you get started we provide slides from a keynote talk as well as the code examples for this article models at www cs wisc edu multifacet amdahl results are suspect because the real world is much more complex currently hardware designers can t build cores that achieve arbitrary high performance by adding more resources nor do they know how to dynamically har ness many cores for sequential use without undue perfor mance and hardware resource overhead moreover our models ignore important effects of dynamic and static power as well as on and off chip memory system and interconnect design software is not just infinitely parallel and sequential software tasks and data movements add overhead it more costly to develop parallel software than sequen tial software furthermore scheduling software tasks on asymmetric and dynamic multicore chips could be difficult and add overhead to this end tomer morad and his and joann paul and brett developed sophisticated models that question the validity of amdhal law to future systems especially embedded ones on the other hand more cores might advantageously allow greater parallelism from larger problem sizes as john gustafson envisioned acknowledgments we thank shailender chaudhry robert cypher anders landin josé f martínez kevin moore andy phelps thomas puzak partha ranganathan karu sankaralingam mike swift marc tremblay sam williams david wood and the wisconsin multi facet group for their comments or proofreading the us national science foundation supported this work in part through grants eia cns ccr cns cns and cns donations from intel and sun microsystems also helped fund the work mark hill has significant financial interest in sun microsystems the views expressed herein aren t necessarily those of the nsf intel google or sun microsystems july a rising horizon in chip fabrication is the integration technology it stacks two or more dies vertically with a dense high speed interface to increase the device density and reduce the delay of interconnects significantly across the dies however a major challenge in technology is the increased power density which gives rise to the concern of heat dissipation within the processor high temperatures trigger voltage and frequency throttlings in hardware which degrade the chip performance moreover high temperatures impair the processor reliability and reduce its lifetime to alleviate this problem we propose in this paper an os level scheduling algorithm that performs thermal aware task scheduling on a chip our algorithm leverages the inherent thermal variations within and across different tasks and schedules them to keep the chip temperature low we observed that vertically adjacent dies have strong thermal correlations and the scheduler should consider them jointly compared with other intuitive algorithms such as a random and a round robin algorithm our proposed algorithm brings lower peak temperature and average temperature on chip moreover it can remove on average of thermal emergency time and result in performance improvement over the base case on thermally homogeneous heterogeneous floorplans index terms processors thermal aware scheduling introduction the integration technology has gained significant attention recently this is a technology that reduces wiring both within and across disparate dies as wiring has become a major latency area and power overhead in modern microprocessors studies have shown that wires can consume more than of the power within a cmp the technology provides vertical stacking of two or more dies with a dense high speed interface reducing the wire length by a factor of the square root of the number of layers used this significant reduction leads to improved performance and lower power dissipation on the interconnection one key challenge in die stacking is the heat generation from the internal active layers because the power density per unit volume increases drastically in this exacerbates existing hotspots and can create new hotspots within the chip especially when active logic circuits are vertically aligned for example the peak temperature can increase by c in a two layer implementation for an alpha like processor compared to a design other studies on logic logic stacking floorplans also show similar thermal constraint there are existing dynamic thermal management dtm techniques such as dynamic voltage and fre quency scaling dvfs at the architecture level to mit igate this problem hardware dtms can respond to thermal crisis quickly and control the temperature effi ciently by reducing the processor power but inevitably leads to degraded performance recently there has been an increasing interest in os assisted task scheduling on both single core and chip multiprocessors to alleviate the thermal condition on chip os assisted task scheduling can reduce the number of times dtms are triggered while still meeting the thermal constraint this technique not only improves the chip performance under the same thermal constraint but also does not require any hardware modifications hardware dtms are engaged only when task scheduling cannot keep the temperature below the thermal threshold in this paper we propose a heuristic os level tech nique that performs thermal aware task scheduling on a chip multiprocessor cmp the proposed technique aims to improve task performance by keeping the tem perature below the threshold to reduce the amount of dtms unlike previous thermal aware os task scheduler for single core or cmp our scheduler for chips must take into account the thermal conduction in the vertical direction early studies have shown that verti cally adjacent dies have strong thermal correlations for example a core in one layer could become hot because of a high power task running in the same vertical column but at a different layer based on these observations our proposed scheduler always considers the aggregated power of cores that are vertically aligned further when a core is overheated we choose to engage dtm on a vertically aligned core that generates the most power such an approach can greatly reduce the total power in one vertical column and quickly cool down the overheated core our experiments show that the proposed scheduler outperforms a random and a round robin scheduler on average we can remove of hardware dtms and obtain a speedup of over the baseline on the thermally homogeneous floorplan with an enhanced version featured with dynamic tuning scheme we can remove hardware dtms and result in performance improvement over the base case on the thermally heterogeneous floorplan the remainder of this paper is organized as follows section discusses previous related works section elaborates the motivation of our thermal aware heuristic algorithms section compares our proposed scheduling algorithm with other alternatives section introduces the experimental methodology section reports the results and compares different algorithms section concludes this paper prior work there have been many works recently investigating the performance potential and the challenges in cmp designs mysore et al proposed to stack on top of a normal processor a profiling die that can identify memory leakage perform diagnosis etc to save the area and power on the main die black et al studied the performance advantages and thermal challenges for stacking a large dram and sram cache on a processor as well as implementing a processor in two layers xie et al reported that the peak temperature in a chip of layers and one die per layer can be as high as c more importantly there is only a difference of a couple of degrees in the worst case between the hotspots in the top die and the bottom die this indicates a strong thermal correlation among adjacent layers in a processor to ensure better heat dissipation in a chip puttaswamy et al proposed a thermal herding design which lowers the power of the chip by splitting individual function unit blocks across multiple layers and places the most frequently switched part or activity closest to the heat sink alternatively adding thermal vias can also alleviate the thermal conditions within a chip goplen et al studied that proper placement of thermal vias in ic design can obtain a maximum of reduction in temperature in the multicore domain loh et al introduced different approaches for implementing single core and multicore processors particularly they pointed out that stack ing seperate cores in multicore design can significantly reuse the existing designs and the interface between the cores needs no more than a few thousand connec tions compared to the previous work this paper focuses mainly on software approaches to thermal management in cmp there have been proposals on os assisted thermal management for single core chip the hyb dtm technique controls temperature by limiting the execution of a hot job once it enters an alarm zone this is achieved by lowering the priority of the hot job so that the os allocates fewer timeslices to it and gives cool jobs relatively more timeslices to execute an ideal simulation study was performed in to show the benefits of interleaving hot and cool job executions however neither performance study nor task switching overhead was considered in the multicore domain choi et al compared and implemented three different task schedulers heat balancing deferred execution and threading with cool loops to leverage temporal and spa tial heat slacks among application threads the proposed mechanisms are implemented in chong et al proposed a mpsoc thermal optimization al gorithm that conducts task assignment scheduling and voltage scaling for a set of real time workloads the goal was to slowdown the workloads as long as the deadlines are met this is quite different from our approach which focuses on best performance and low thermal profile motivation and rationale a representative floorplan there have been a number of cmp floorplans as shown in figure a c proposed in literature in these figures cores are stacked on each other with extended cache or memory in between we observed that for a stacked chip to be scalable in layer count it is inevitable to encounter more than one active cores in one vertical core column no matter how the active cores and cache banks are placed in the floorplan further if we look at the distance of each core stack to the heatsink on either the top or bottom of the chip we can classify these flooplans into two categories fig chip multiprocessor floorplan options figure a and b represent the first category in which the distance of some core stacks e g core stack in a to the heatsink is different from others such as core stack these flooplans are thermally heterogeneous meaning that the heat dissipation property of different core stacks is different for example if the heatsink is on the bottom of the stacked chip as illustrated in figure core stack is further away from the heat sink than core stack thus heat dissipation for cores in stack will be more difficult than those in stack in contrast figure c has a rather homogeneous thermal property because all cores are equally distant from the heat sink our preliminary work focused only on homogeneous floorplans while this paper considers both despite these distinctions among different floorplans they still share some important property the heat from any core can quickly propagate vertically to other cores above and below for all these floorplans the cache layers almost serve as heat conductance between the core layers considering this commonality among various floorplans we choose to use the floorplan in fig ure d as a representative to first introduce the general rationales behind our scheduling algorithms then we will discuss details of our algorithms for homogeneous and heterogeneous flooplans respectively in figure d there are two layers and each layer contains four cores the cache banks are subsumed within each core vertically adjacent layers have strong thermal correlations fig a face to back die stacking structure adapted from and the corresponding thermal model similar to a regular processor where heat dissipates mostly in the vertical direction chips also have better heat conductivity in vertical than horizontal di rection this implies that vertically adjacent cores have larger thermal impact among each other than horizon tally adjacent cores we will use a simple heat transfer model to explain this phenomenon figure shows a basic two layer chip structure adapted from we use a face to back bonding technology for better scalability in layer count the top layer is thinned for better electrical characteristics and improved physical construction of the through silicon vias for power deliv ery and i o a thin die also has better heat conductivity than a thick die such as the bottom die as we can see the distance between the two active silicon dies are very small this directly determines the high heat conductivity between the two adjacent dies the heat transfer model for this chip is shown on the right of the figure here one die is modeled using one node its temperature and power are denoted as t and p respectively represents the thermal resistance between the two nodes amb represents the thermal resistance between the bottom node and the ambient air we omit the thermal capacitance here to model only the steady state temperature in our experiments later both thermal resistance and capacitance are modeled let and be the temperature relative to the ambient air in the bottom and top node respectively then amb amb hence the temperature difference between the two nodes is from the parameter used in literature is 0159k w represents the power generated by the entire die this value is in the range of for a typical single core processor therefore the temperature difference between the top and bottom die is merely a fig thermal correlation between adjacent dies such a strong thermal correlation between the two adjacent dies can also be demonstrated from our sim ulation figure shows a typical thermal profile of running eight threads concurrently on eight cores as floorplaned in figure the experimental setup will be introduced in section here eight threads are eight different benchmarks chosen from the benchmark suite we use we refer to vertically aligned two cores as a core stack we can see from figure that there are four distinct clusters of temperature curves each cluster has drastically different variations from others however each cluster has two lines that are very close to each other their variations are almost always synchronized the four clusters correspond to the four core stacks in the floorplan and the two lines in each cluster correspond to the temperature variation of the two cores per stack this experiment shows clearly the strong correlation between adjacent dies as the temperatures for different core stacks hardly have any dependencies among them but within each core stack the temperatures of the two cores are strongly correlated such correlation can still be observed for a layer floorplan in our experiments as the intermediate thin cache layers serve as good heat conductors among their vertical core neighbors the die layers further from the heat sink are usually hotter not only are the cores in a stack strongly correlated in their temperatures but also the ones on the top are usually hotter than those near the bottom this has also been noted in the literature for steady state temperatures for clarity we refer to the cores further from the heat sink as top cores as illustrated in figure the intuition is that the bottom cores are closer to the heat sink therefore their heat can be removed more quickly here we give a more analytical analysis taking into account the thermal capacitance as well suppose in the thermal model depicted in figure the thermal capacitance between the top die and ambient air is then scheduling algorithms the strong correlations among the cores in one stack leads to a scheduling that considers the entire stack as a whole the fact that top cores are hotter than the bottom cores suggests that threads within a core stack should be placed with care furthermore we take advantage of this observation and introduce a new voltage frequency scaling mechanism that results in the fastest temperature drop within the shortest amount of time once the peak temperature within a stack reaches the thermal thresh old in this section we present a sequence of thread scheduling algorithms since we have two categories of floorplans we will select a representative homogeneous floorplan as shown in figure c and a representative heterogeneous floor dt plan as shown in figure a both the homogeneous and heterogeneous floorplan will be applied with five algorithms baseline random round robin balancing as mentioned earlier which represents the power of a modern processor has a typical value range of represents how quickly temperature changes from the top die for a thin die within in a layer chip the thermal capacitance is reported as k dt is the temperature change rate within a short time from our experimental experience and many other results in the literature temperature varies slowly with time for example we observed a less than c increase in temperature in a window using hotspot for chips hence the right hand side of equation is usually positive with a range of therefore is usually higher than we also performed simulations to testify the above observation we intentionally put the coolest job lowest average temperature in a chip in our benchmark suite on the top die and the hottest job on the bottom die in a core stacked chip setting the temperatures of the two cores are shown in figure we can see that the top core has higher temperatures than the bottom layer almost always such an observation serves as a guideline to the development of our heuristic scheduling algorithm fig demonstration of the top die being hotter than the bottom die by core and our proposed balancing by stack algorithm the baseline we use the linux scheduler as our baseline algorithm in this scheduler each core has a task queue that keeps track of all running tasks on that core each queue contains two priority lists active and expired list at runtime the core selects to execute the tasks in the active list according to some policy once a task uses up its time quota it is moved to the expired list if all tasks are in the expired list an epoch has finished and the scheduler iterates the process by swapping the two lists each task in the active list has of cpu cycle quota depending on its own priority by default the core switches to a different task every thus in our core chip upon the scheduling interval of every the scheduler selects a task from each core active list according to its original policy and then assigns it to a different randomly selected core this algorithm is simple and has low context switch overhead compared to other algorithms introduced later however it may run into the risk of putting two hot tasks into the same core stack which may lead to extremely high temperature that results in long and harsh voltage frequency scaling penalty to both tasks moreover once a poor scheduling has been made it stays in that condition for a long period of time until the next scheduling time exacerbating the already serious thermal condition within the chip random baseline a quick fix of the baseline scheduler is to increase the scheduling frequency in the normal linux os any context switch interval between may be used a minimum of is recommended to avoid unnecessary context switch overhead we used as our scheduling interval mainly due to experimental restriction on collecting the power traces also is close to the thermal constant of the core under testing however the algorithm can be directly applied to any scheduling interval recommended in linux such as if those restrictions don t apply further we take into account the extra context switch overhead using an scheduling interval during our experiments we performed a real machine measurement on the time required to perform a single context switch for an interval it is a mild penalty that can be easily offset by the performance gain from a better scheduling with the improved baseline scheduling algorithm termed random to reflect the scheduling decision the chip can exit a poor thermal condition due to an un wise scheduling more quickly resulting in less harmful impact round robin the random scheduler may result in uneven distribu tion of power and temperature as tasks are assigned randomly to any core a round robin scheduler rr can overcome this by rotating tasks among cores in a fixed order periodically therefore after n iterations where n is the number of cores each task has executed on every core for one scheduling interval e g this can help balancing the power and temperature distribution in the long run temperature balancing by core an alternative way to balance the heat among the cores is to explicitly arrange the tasks according to their power consumption and the core temperatures essentially a high power task should be assigned to a low temperature core at each scheduling point the scheduler sorts the power consumption of all tasks and the current tempera ture of each core it then assigns the task with the highest power to the coolest core the highest power to the coolest core and so forth such a mechanism should perform a better job in balancing the temperature distribution among cores than rr however recall that there is a strong thermal cor relation between two adjacent layers and the cores in one stack have only a small difference in temperatures this implies that if a core stack contains the hottest core it probably also contains the hottest core when the temperature balancing by core algorithm is applied the tasks with the lowest and lowest power are scheduled to this hot core stack similarly the tasks with the highest and highest power will be scheduled to the coolest core stack after that the hottest coolest core stack will have the largest temperature drop rise which may lead to temperature oscillations and task thrashing between those two stacks potentially leading to more thermal emergencies in that case a rr or a random algorithm may be a better solution another issue with this mechanism is how the power consumption of each task is obtained recently there has been proposals on obtaining the runtime power consumption of an application through probing the per formance counters in a processor we also adopt this approach and assume that each core is equipped with such counters that can be used for power estimation note that our power estimation need not be very accu rate as we only need the sorted order of the power not the absolute values temperature balancing by stack the core based temperature balancing algorithm can create thrashing of tasks between the hottest core stack and the coolest core stack as we analyzed earlier this is because the algorithm while trying to balance the temperatures among all cores treats each core indepen dently however as adjacent dies have strong temper ature correlations cores in the same stack should in deed be considered together intuitively we can assume that each stack is a super core that has cores with similar temperatures hence scheduling of the tasks within three dimensions can be reduced to scheduling of super tasks within two dimensions apparently a super task is a set of tasks that are assigned to a super core i e a core stack algorithm for homogeneous floorplans we treat homogeneous and heterogeneous floorplans differently in this algorithm as their super cores have different thermal property we first elaborate on the algorithm for homogeneous floorplan super tasks let l be the number of layers in a chip and n be the number of cores per layer as a super core contains l cores a super task should also contain l tasks and there are n super tasks the scheduling of n super tasks among n super cores is now simply a problem where a balanced temperature distribution is desired hence we first balance the power among super tasks i e let each super task have about the same power and then balance the temperatures among super cores by scheduling a relatively high power super task onto a relatively cool super core to balance the power among super tasks we first sort the powers of all n l tasks let n be n initially empty bins we will fill powers into these bins such that each bin will contain l tasks and the total powers of each bin are about the same in descending order of powers we put each power value into a bin that has the smallest current total power among all bins this policy attempts to reduce the gap between the smallest and the largest total power in each step in order to generate a relatively balanced total power across n bins finally all powers within a bin form a super task we remark that our policy is only a heuristic as an optimum solution may require an exhaustive search we aim for a simple effective yet low complexity heuristic because the scheduler makes the decision at runtime task distribution among and within super cores the goal of producing super tasks is to generate relatively balanced power distribution across super cores once the super tasks are formed we sum up the temperatures of all l cores in a super core and sort them similar to the previous procedure we assign the hottest super core with the super task of the lowest power and so on figure shows an example of scheduling tasks onto a layer core per layer chip step a c depict the procedure except for how tasks within a super task are allocated onto different cores within a stack as discussed earlier the top cores are usually hotter than the bottom cores in a core stack hence we should allocate tasks of higher powers onto the bottom cores for better heat removal and tasks of lower powers onto the top cores for example if the temperatures of the cores from bottom up are strictly increasing then the tasks allocated to them should have strictly decreasing powers from bottom up figure last step illustrates this policy in a two layer floorplan our algorithm involves mostly sorting of the powers and temperatures therefore its time complexity is o nl log nl algorithm for heterogeneous floorplans the major difference in heterogeneous floorplans is that different super cores have different heat dissipation ca pability due to their varying distances to the heatsink for this reason even if two super cores are of the same present temperature the same super task assigned to them will result in different future temperatures there fore unlike the algorithm for homogeneous floorplans where the total power among super tasks should be well balanced the task bundling in heterogeneous floorplan should intentionally create power imbalance to generate balanced temperature distribution among super cores however it is difficult to estimate how much power difference we should create among super tasks because the future temperature depends on not only power but also the present temperature and duration of the power therefore for a given set of power values our algorithm forms the super tasks with minimum moderate and maximum total power difference denoted as min diff mod diff and max diff respectively and dynamically make the selection of super tasks let pn be n powers in ascending order super tasks with min diff mod diff and max diff are formed as follows assuming each super task contains l tasks max diff pl pl mod diff pl p2 pl min diff the principle is to balance the total pow ers of super tasks this is identical to the algorithm for homogeneous floorplans section intuitively when the temperature difference among super cores is large a super task with max diff is desired however if the power difference among tasks is also large using the max diff may be an overkill a mod diff combination may be sufficient therefore our decision relies on both the temperature gradient denoted as t among the super cores and the power range denoted as p of the tasks let fig the temperature balancing by stack algorithm scheduling procedure to sum up on every scheduling interval in our case the scheduler performs the following steps sort the powers of all tasks form super tasks sort the power sums of the super tasks from low to high for each super core sum up the temperatures for all cores sort the temperature sums for all super cores from high to low create a sequential one one mapping between the sorted super tasks and sorted super cores in each super core allocate the tasks in their in creasing power order onto the cores with decreas ing temperature order t θ p when θ is small the temperature gradient t is rel atively small compared with the power range p of the tasks super tasks of min diff are more appropriate in this situation because we need only to perform mild temperature adjustment on the other hand when θ is large a more aggressive task bundling to create power difference is necessary hence the selection will favor max diff during our experiments we use two heuristic θ val ues and as the thresholds for choosing different algorithms the choice of these two values is based on our experimental settings and may vary with thermal properties of the floorplan if θ falls in the range of min diff will be chosen if θ is in the range of mod diff is selected if θ is greater than max diff will be selected a new thermal management scheme a critical component in company with our proposed scheduling algorithm is how to handle thermal emer gencies once a core temperature increases above the hardware threshold conventionally such a core will be put to a low power state through dvfs in a chip since the top cores are usually hotter thermal emergencies usually occur in the top layers moreover our scheduler puts cooler tasks on the top layers which means that those tasks are more likely to undergo dvfs leading to their degraded performance the problems of such conventional thermal manage ment are twofold first the cooler tasks could be penal ized more often than the hotter tasks which brings a fairness issue among different tasks intuitively hotter tasks should be restrained by the system due to their potential harmful impact to the chip second applying dvfs to the cooler tasks on the top layers does not yield the same efficiency as in a chip this is because it takes longer time to cool down the top cores due to their high power neighbors at the bottom in fact it is because of those hot bottom tasks are top cores overly heated therefore a more rational thermal management should employ the scalings to the source of an overheating the bottom cores that are running high power tasks more formally when core a of a super core s is overheated the thermal management will select core b with the highest power in s to engage dvfs b may or may not be identical to a such a thermal management strategy solves the two above problems effectively first cool tasks are not penalized more often than hot tasks because if a cool task becomes a temperature victim the hot task that caused the problem is penalized sec ond all cores in s including a and b are quickly cooled because the total power of s is reduced with the maximum strength for example in figure if the super core containing the super task tripped a thermal emergency on the core and suppose the dfvs reduces the power of a core by half then our scheme will reduce the total power of this super core to while the conventional thermal management will only reduce it to as we can see if dvfs is applied to a relatively low power task the result is inferior because a task is being penalized but the total power in the chip is not reduced as much this is often the case for the temperature balancing by core scheduler as it tends to allocate cool tasks on the top layer since it is usually hotter as a result our mechanism brings down the tempera ture of the hotspot at the highest speed resulting in min imum penalty to the overall performance of this super core we will show later that our proposed temperature balancing by stack scheduling algorithm with improved thermal management results in the much less amount of thermal emergencies and the much better performance among all previous schemes experimental methodology floorplan setup our detailed experiments are conducted on floorplans as depicted in figure a and c each floorplan has four layers and a total of eight cores we simulated northwood cores in clock frequency each core has a size of 144 the remaining space is left for extended cache or memory so the die size is 289cm2 other physical parameters of the floorplan are similar to simulation tool and power trace collection we used hotspot version as our simulation tool we chose the grid model to experiment our floorplan we substituted the order runge kutta method with tilts to generate accurate tempera tures at high speed hotspot takes power traces as inputs and temperature variation within a die is a slower process compared to other metrics such as ipc hence we need to collect extended power traces to model realistic temperature variations such as warming up and cooling down due to task scheduling as mentioned earlier we adopt the re cently proposed performance counter based method to collect runtime hardware activities of a program on a real machine we obtained the power model cal ibrated from to produce long power traces for programs from a linux machine with a pentium core the traces contain powers for each functional unit and all traces are complete execution of the programs in for scheduling algorithms that require power informa tion balancing by core and balancing by stack we use the power in the last interval to predict the power in the next interval that is the scheduling decisions are based on local power information the scheduler needs not to know whether a program is globally hot or cool also we use the last power predictor in the scheduler due to its simplicity we experimented with more complex power predictors and found that their overhead both in time and space is not appropriate for on line scheduling most of the benchmarks exhibit power mis prediction rate our experiments show that an error within makes last power prediction accurate enough for the scheduler benchmark classification we first ran the power traces of each benchmark to obtain its temperature profile as shown in figure we next classified these benchmarks as hot power intensive cool power non intensive and mild between hot and cool after that we created workload com binations as listed in table each with one or more hot tasks the workload mixes without hot tasks are less thermally critical and thus are not considered here in table when the number of benchmarks in one combination is less than copies of the benchmarks will be created to ensure that every core in the floorplan has one task to run this resembles the situation of running parallel threads of the same program in multicore pro cessors fig temperatures of the benchmark in table the combination of benchmarks in simulation hc crafty mcf hc sixtrack swim hhcc bzip twolf art ammp hmmc wupwise equake applu ammp hm gzip mgrid hm parser equake hhmm crafty gzip mgrid apsi hhmmmccc gap twolf equake mgrid vortex ammp art swim hhhhcccc bzip gzip sixtrack wupwise ammp art mcf swim dvfs implementation and context switching overhead we modified hotspot to incorporate the hardware dvfs every of a scheduling interval hotspot checks if the temperature has trespassed the threshold if so the voltage is lowered from to and the frequency is reduced by we charge µs of overhead on every voltage frequency transition during a dvfs scaling if the temperature persists above the threshold after one the scaling continues and no additional dvfs switch overhead is charged we do not choose multi level dvfs scheme to avoid unnecessary switch overhead in every level transition other overheads in our proposed scheduler is mainly the increased number of context switches we measured this time in a linux machine by enforcing a large number of context switches between two tasks and calculating the average switch time from the increased execution time of these two tasks this quantity in our test machine is later we will see that our proposed scheduler can still outperform linux baseline scheduler even with much higher context switch frequency results and analysis the metrics we use to evaluate different scheduling algo rithms are peak temperature of all cores the reduction in time that a task stays above the thermal threshold termed thermal emergency reduction in later discus sion and performance improvement in terms of total execution time reduction of all tasks the peak tem perature indicates how well a scheduler can alleviate the worst cases of the thermal condition on chip the thermal emergency reduction indicates the capability of a scheduler to control the temperature below the hardware threshold the performance improvement is the result of both the thermal emergency reduction and the efficiency of lowering the temperature during an emergency next we present the results for homoge neous and heterogeneous floorplans separately homogeneous floorplan in the following we will introduce the experiment results on the thermally homogeneous floorplan five sched ulers baseline random roundrobin balancing by core balancing by stack are tested in the experiments thermal behavior comparison of different sched ulers first let us see a qualitative comparison among differ ent schedulers on the homogeneous floorplan figure shows a close up of temperature traces for cores running the hmmc workload under different schedul ing algorithms here we did not enforce dvfs at the threshold because otherwise many high temperature curves would be capped at the threshold as we can see the baseline algorithm can result in a large temperature gradient across different core stacks a c differ ence between the hottest and the coolest core stack is observed in this figure for random and rr scheduler the temperature gradient within the chip gradually reduces because their scheduling interval is much smaller than that in the baseline temperature gradi ent is between c in these schedulers finally both the balancing by core and our proposed balancing by stack schedulers create the smallest temperature gradient among all cores the temperature curves of all cores almost overlap entirely the width of the temperature band is c only indicating an excellent balance of temperature among the cores however the balancing by core scheduler generates more fluctuation note that an ideal temperature balancer would create a c among all cores hence our proposed balancing by stack algo rithm is only a couple of degrees from the ideal case peak temperature reduction balancing the temperatures across the chip can help to reduce the peak temperatures among all cores fig ure shows the peak temperature generated from each fig a zoom in of temperature variation over time under different scheduling algorithms scheduling algorithm assuming there are no dvfs em ployed otherwise the peak temperature is just the ther mal threshold we can see from the figures that baseline algorithm can generate the highest peak temperature of c the random rr balancing by core and balancing by stack can reduce the peak temperature bet ter and better our proposed balancing by stack schedul ing generates the second lowest peak temperature of c a c lower than the baseline and a mere c higher than that of balancing by core fig peak temperatures of different scheduling algo rithms thermal emergency reduction a direct benefit from scheduling the tasks is the reduced thermal emergency time i e the time a core temperature stays above the hardware thermal threshold note that this metric does not necessarily correlate with the peak temperatures reported in figure which are collected under no dvfs for example a relatively low peak temperature may still trip dvfs if temperature oscillates around the threshold often figure shows thermal emergency time reductions from different algorithms normalized to the baseline case as we can see the random rr and balancing by core can reduce the emer gency time by and on average respectively our balancing by stack algorithm removes the most emergency time in cases of benchmarks an average of reduction is observed with a range of also the balancing by core algorithm turns out to introduce as much emergency time as rr algorithms even with lower peak temperature this is because it tends to create temperature oscillations among core stacks as discussed in section and it tends to allocate cooler tasks on the top layer where dvfs is usually engaged for a long time the consequence is that the overall power in the entire chip is not reduced as much as in other schedulers where high power tasks can be scaled during emergencies therefore a balancing by core scheduler may not be a good scheduling candidate in practice fig thermal emergency time reductions in homoge neous floorplans performance improvement corresponding to the thermal emergencies removed our proposed balancing by stack algorithm achieves the best performance speedup among all algorithms dis cussed this is shown in figure the performance is the total execution time of all tasks in a workload the results are normalized to the baseline performance on average the balancing by stack achieves a speedup while the random rr and balancing by core algorithm achieve and improvement respectively this is primarily due to the amount of thermal emergencies our algorithm removed as well as the high efficiency in handling them with our new thermal management mechanism we also notice that in some occasions the performance may not improve even if the thermal emergency time is reduced this could happen when the temperature floats around the thermal threshold but does not in crease overly high in such a scenario there could be many dvfs triggered which introduce high transition penalty and overkills the gains from scheduling for example in the hmmc workload the balancing by core removed of thermal emergency time in the balancing by stack but its performance is worse than the balancing by stack our balancing by stack re moves more thermal emergency time than other sched ulers in th cases of benchmark combinations and therefore achieves the most performance improvement fig performance speedups for homogeneous floor plans heterogeneous floorplan in addition to the five algorithms applied to the homo geneous floorplan two additional algorithms are also tested for heterogeneous floorplans the first is the re vised balancing by stack algorithm with dynamic super task forming mechanisms the algorithm is designed to tackle the thermal heterogeneity of the floorplan as discussed in section the second is a pseudo optimal algorithm that tests the quality of each discussed algorithm we term this algorithm a step optimal since it tries all task bundling mechanisms and chooses the one that triggers the fewest dtms in one next step notice that this is not a true optimal algorithm which would go beyond one step to enumerate all possible schedules and pick the optimum one and so is termed step only although it is not realistic to adopt step optimal algorithm online due to its complexity it does indicate the potential for improvement of the discussed algorithms thermal emergency reduction figure shows the thermal emergency time reduction for different algorithms normalized to the baseline case as we can see random and rr perform relatively poorly compared with other algorithms because of the heterogeneity in the floorplan they achieve and of thermal emergency time reduction re spectively our proposed dynamic balancing by stack algorithm achieves a total of reduction only away from the step optimal on average and is better than the remaining algorithms for example it removes more emergency time than the original balancing by stack algorithm this indicates that dy namically tuning of the task bundling is very helpful to a thermally heterogeneous floorplan the balancing by core algorithm is slightly better than dynamic balancing by stack in three cases hhhhcccc hhmmmccc and hm gzip this is because when t and p do not change our dynamic balancing by stack algorithm will select the same one from the fixed power bundling schemes while a slight re ordering of core temperature will cause balancing by core to form a different and better power bundle more flexibly also balancing by core slightly surpasses step optimal in hhmmmccc and hm gzip workloads this is because the step optimal does not generate a global optimal schedule fig thermal emergency time reductions in heteroge neous floorplans performance improvement compared with the thermal emergencies removed our dynamic balancing by stack algorithm achieves the best performance speedups on average among all the algo rithms except step optimal figure shows that random and rr achieve and speedup respectively which is notebaly lower than and speedup shown in figure indicating that random and rr are not as helpful in heterogeneous floorplans as in homogeneous ones balancing by stack achieves speedup more than balancing by core though figure shows it removes less thermal emergency time than balancing by core the reason behind this is that balancing by core tends to generate many overhead in dtm mode switches though the total time above the emergency threshold is low which was reported in figure finally the dynamic balancing by stack algorithm achieves the best performance speedup of with neglegible gap from the step optimal fig performance speedups for heterogeneous floor plan conclusions we have demonstrated in this paper that os task scheduling is an effective approach to improve the ther mal conditions in chip multiprocessors it can reduce the peak temperature within the chip reduce the thermal emergency triggering amount and improve the overall performance of the executing tasks in particular we have shown that our proposed scheduling mechanism balancing by stack outperforms other intuitive algorithms in the thermally homogeneous floorplan because of the following three properties first our scheduler takes into account the high thermal corre lations among the layers in one core stack and schedules tasks in bundles second within every stack of cores hot tasks are allocated to the layers that are closest to the heat sink for best heat dissipation third upon a thermal emergency power scaling is engaged in a core stack whose temperature exceeds the threshold and to the core that generates the largest power in this stack this can quickly cool down the core stack reducing the performance penalty imposed to the task we also presented a revised balancing by stack algo rithm for heterogeneous floorplans this algorithm refined task bundling mechanism to adapt to the hetero geneity in thermal distribution among the cores such a refinement shows its strength over other algorithms in both thermal emergency reduction and performance speedup moreover it achieves nearly the full potential suggested by a local optimal algorithm we study scheduling problems motivated by recently developed techniques for micro processor thermal management at the operating systems level the general scenario can be described as follows the microprocessor temperature is controlled by the hardware thermal management system that continuously monitors the chip temperature and au tomatically reduces the processor speed as soon as the thermal threshold is exceeded some tasks are more cpu intensive than other and thus generate more heat during ex ecution the cooling system operates non stop reducing at an exponential rate the deviation of the processor temperature from the ambient temperature as a result the processor temperature and thus the performance as well depends on the order of the task execution given a variety of possible underlying architectures models for cooling and for hardware thermal management as well as types of tasks this scenario gives rise to a plethora of interesting and never studied scheduling problems we focus on scheduling real time jobs in a simplified model for cooling and thermal management a collection of unit length jobs is given each job specified by its release time deadline and heat contribution if at some time step the temperature of the system is τ and the processor executes a job with heat contribution h then the temperature at the next step is τ h the temperature cannot exceed the given thermal threshold t the objective is to maximize the throughput that is the number of tasks that meet their deadlines we prove that in the offline case computing the optimum schedule is np hard even if all jobs are released at the same time in the online case we show a competitive deterministic algorithm and a matching lower bound introduction background the problem of managing the temperature of processor systems is not new in fact the system builders had to deal with this challenge since the inception of computers since early the introduction of battery operated laptop computers and sensor systems highlighted the related issue of controlling the energy consumption most of the initial work on these problems was hardware and systems oriented and only during the last decade substantial progress has been achieved on developing models and algorithmic techniques for microprocessor temperature and energy management this work proceeded in several directions one direction is based on the fact that the energy consumption is a fast growing function of the processor speed or frequency thus we can save department of computer science university of california riverside ca usa supported by nsf grants oise and ccf cnrs lix umr ecole polytechnique palaiseau france supported by anr alpage laboratoire de l informatique du parall elisme ecole normale sup erieure de lyon ens lyon france energy by simply slowing down the processor here algorithmic research focussed on speed scaling namely dynamically adjusting the processor speed over time to optimize the energy consumption while ensuring that the system meets the desired performance requirements another technique applicable to the whole system not just the microprocessor involves power down strategies where the system is powered down or even completely turned off when some of its components are idle since changing the power level of a system introduces some overhead scheduling the work to minimize the overall energy usage in this model becomes a challenging optimization problem models have also been developed for the processor thermal behavior here the main objective is to ensure that the system temperature does not exceed the so called thermal threshold above which the processor cannot operate correctly or may even be damaged in this direction techniques and algorithms have been proposed for using speed scaling to optimize the system performance while maintaining the temperature below the threshold we refer the reader to the survey by irani and pruhs and references therein for more in depth information on the models and algorithms for thermal and energy management temperature aware scheduling the above models address energy and thermal man agement at the micro architecture level in contrast the problem we study in this paper addresses the issue of thermal management at the operating systems level most of the previ ous work in this direction focussed on multi core systems where one can move tasks between the processors to minimize the maximum temperature however as it has been recently discovered even in single core systems one can exploit variations in heat contributions among different tasks to reduce the processor temperature through appro priate task scheduling in this scenario the microprocessor temperature is controlled by the hardware dynamic thermal management dtm system that continuously monitors the chip temperature and automatically reduces the processor speed as soon as the thermal threshold maximum safe operating temperature is exceeded typically the frequency is reduced by half although it can be further reduced to one fourth or even one eighth if needed running at a lower frequency the cpu generates less heat the cooling system operates non stop reducing at an exponential rate the deviation of the processor temperature from the ambient temperature once the chip cools down to below the threshold the frequency is increased again different tasks use different microprocessor units in different ways in particular some tasks are more cpu intensive than other as a result the processor thermal behavior and thus the performance as well depends on the order of the task execution in particular yang et al point out that based on the standard model for the microprocessor thermal behavior for any given two tasks scheduling the hotter job before the cooler one results in a lower final temperature than after the reverse order they take advantage of this phe nomenon to reduce the number of dtm invocations thus improving the performance of the os scheduler with multitudes of possible underlying architectures for example single vs multi core systems models for cooling and hardware thermal management as well as types of jobs real time batch etc the scenario outlined above gives rise to a plethora of interesting and never yet studied scheduling problems our model we focus on scheduling real time jobs in a somewhat simplified model for cooling and thermal management the time is divided into unit time slots and each job has unit length these jobs represent unit slices of the processes present in the os scheduler queue we assume that the heat contributions of these jobs are known this is counterintu itive but reasonably realistic for as discussed in these values can be well approximated using appropriate prediction methods in our thermal model we assume without loss of generality that the ambient temperature is and that the heat contributions are expressed in the units of temperature that is by how much they would increase the chip temperature in the absence of cooling in reality during the execution of a job its heat contribution is spread over the whole time slot and so is the effect of cooling thus the final temperature can be expressed using an integral function in this paper we use a simplified model where we first take into account the job heat contribution and then apply the cooling where the cooling simply reduces the temperature by half finally we assume that only one processor frequency is available consequently if there is no job whose execution does not cause a thermal violation the processor must stay idle through the next time slot our results summarizing our scheduling problem can be now formalized as follows a collection of unit length jobs is given each job j with a release time rj deadline dj and heat contribution hj if at some time step the temperature of the system is τ and the processor executes a job j then the temperature at the next step is τ hj the temperature cannot exceed the given thermal threshold t the objective is to compute a schedule which maximizes the number of tasks that meet their deadlines we prove that in the offline case computing the optimum schedule is np hard even if all jobs are released at the same time and have equal deadlines in the online case we show a competitive deterministic algorithm and a matching lower bound terminology and notation the input consists of n unit length jobs that we number n each job j is specified by a triple rj dj hj n n q where rj is its release time dj is the deadline and hj is its heat contribution the time is divided into unit length slots and each job can be executed in any time slot in the interval rj dj by τu we denote the processor temperature at time u the initial temperature is and it changes according to the following rules if the temperature of the system at a time u is τu and the processor executes a job j then the temperature at time u is τu τu hj the temperature cannot exceed the given thermal threshold t without loss of generality we assume that t thus if τu hj then j cannot be executed at time u idle slots are treated as executing a job with heat contribution that is after an idle slot the temperature decreases by half given an instance as above the objective is to compute a schedule with maximum throughput where throughput is defined as the number of completed jobs extending the standard notation for scheduling problems we denote the offline version of this problem by ri hi pi ui in the online version denoted online ri hi pi ui jobs are available to the algo rithm at their release time scheduling decisions of the algorithm cannot depend on the jobs that have not yet been released example suppose we have four jobs specified in notation j rj dj hj schedule schedule figure example of two schedules figure shows these jobs and two schedules numbers above the schedules denote tem peratures in the first schedule when we schedule job at time the processor is too hot to execute job so it will not complete job at all in the second schedule we stay idle in step allowing us to complete all jobs the np completeness proofs in this section we prove that the scheduling problem ri hi pi ui is np hard for the sake of exposition we start with a proof for the general case and later we give a proof for the special case when all release times and deadlines are equal theorem the offline problem ri hi pi ui is np hard proof we use a reduction from the partition problem defined as follows we are given a set s of positive integers such that β ai β for all i where β ai the goal is to partition s into n subsets each subset with the total sum equal exactly β by the assumption on the ai each subset will have to have exactly elements this partition of s will be called a partition partition is well known to be np hard in the strong sense that is even if maxi ai p n for some polynomial p n we now describe the reduction for the given instance of partition we construct an instance of ri hi pi ui with jobs these jobs will be of two types i first we have jobs that correspond to the instance of partition for every i we create a job of heat contribution ai release time and deadline n β ii next we create n additional gadget jobs these jobs are tight meaning that their deadline is just one unit after the release time the first of these jobs has heat contri bution and release time then for each j n we have a job with heat contribution and release time j β we claim that s has a partition if and only if the instance of ri hi pi ui constructed above has a schedule with throughput that is with all jobs completed the main idea is this imagine that at some moment the temperature is and we want to schedule a job of heat contribution x for some integer x then we must first wait x time units so that the processor cools down to x x before we can schedule that job and then right at the completion of this job the temperature is again the analogous property holds in fact even if at the beginning the temperature was some τ except that then after completing this job the new temperature will be τ l τ x x x x x that is it may be different than τ but still strictly greater than with this observation in mind the proof of the above claim is quite easy first we show that if there is a solution to the scheduling problem then s has a partition note that the tight jobs divide the time into n intervals of length β each also each of the other jobs is scheduled in exactly one of these intervals this defines a partition of s into n sets now we claim that after every job execution the temperature is strictly more than this is true for the first job to be scheduled since it has heat contribution each other job in the instance including the tight jobs has heat contribution at least therefore right after its execution the temperature is at least already if we take only this job into account but there is also a declining but non zero temperature contribution from the first tight job so overall the temperature after every execution is strictly more than together with the earlier observation this implies that every non tight job of heat contri bution ai must be preceded by ai idle units thus using ai time slots in total therefore every set in the above mentioned partition has the total sum at most β since there are at most n sets in the partition the total sum of each must be exactly β this proves that s has a partition now we show the other implication namely that if s has a partition then there is a solution to the scheduling instance simply schedule the tight jobs at their release time this divides the time into n intervals of length β each assign each of the n sets in the partition to a distinct interval and schedule its jobs in this interval every integer ai in the set corresponds to a job of heat contribution ai and we schedule it preceded with ai idle time units the jobs of the set can be scheduled in an arbitrary order the important property being that since their total sum is β they all fit exactly in this interval after all jobs in one set are executed the temperature is exactly and during the execution the temperature does not exceed because we pad the schedule with enough idle slots all release time and deadline constraints are satisfied so the scheduling instance has a feasible solution as well it remains to show that the above instance of ri hi pi ui can be produced in polynomial time from the instance of partition indeed every number ai is mapped to some number ai which is described with o ai bits since we assumed that all numbers ai for i n are polynomial in n the reduction will take polynomial time and the proof is complete d the above construction used the constraints of the release times and deadlines to fix tight jobs that force a partition of the time into intervals we can actually prove a stronger result namely that the problem remains np complete even if all release times are and all deadlines are equal why is it interesting one common approach in designing on line algorithms for scheduling is to compute the optimal schedule for the pending jobs and use this schedule to make the decision as to what execute next the set of pending jobs forms an instance where all jobs have the same release time our np hardness result does not necessarily imply that the above method cannot work assuming that we do not care about the running time of the online algorithm but it makes this approach much less appealing since reasoning about the properties of the pending jobs is likely to be very difficult theorem the offline problem ri hi pi ui is strongly np hard even for the special case when jobs are released at time and all deadlines are equal proof the reduction is from numerical matching in this problem we are given sets a b c of n non negative integers each and a positive integer β a dimensional numerical matching is a set of n triples a b c a b c such that each number is matched exactly once appears in exactly one triple and all triples a b c in the set satisfy a b c β numerical matching is known to be np complete even when the values of all numbers are bounded by a polynomial in n it is referenced as problem in clearly this problem is quite similar to partition that we used in the previous proof without loss of generality we can assume that every x a b c satisfies x β x a b c x βn we now describe the reduction let be the constant and the function α x the instance of ri hi pi ui will have jobs all with release time and deadline these jobs will be of two types i first we have jobs that correspond to the instance of numerical matching for every a a there is a job of heat contribution a for every b b there is a job of heat contribution b and for every c c there is a job of heat contribution c we call these jobs respectively a jobs b jobs and c jobs ii next we have n gadget jobs the first of these jobs has heat contribution and the remaining ones we call these jobs respectively and jobs we claim that the instance a b c β has a numerical dimensional matching if and only if the instance of ri hi pi ui that we constructed has a schedule with all jobs completed not later than at time the idea of the proof is that the gadget jobs are so hot that they need to be scheduled only every th time unit separating the time into n blocks of consecutive time slots each every remaining job has a heat contribution that consists of two parts a constant part or and a tiny variable part that depends on the instance of the matching problem this constant part is so large that in every block there is a single a job a single b job and a single c job and they must be scheduled in that order this defines a partitioning of a b c into triplets of the form a b c a b c since the gadget jobs are so hot they force every triple a b c to satisfy a b c β we now make this argument formal suppose there is a solution to the instance of numerical matching we con struct a schedule where all jobs complete at or before time schedule the job at time and all other gadget jobs every th time slot now the remaining slots are grouped into blocks consisting of consecutive time slots each for i n associate the i th triple a b c from the matching with the i th block and the corresponding a b and c jobs are executed in this block in the order a b c see figure a b c a b c a b c a b c figure the structure of the schedules in the proof by construction every job meets the deadline so it remains to show that the temperature never exceeds the non gadget jobs have all heat contribution smaller than by assumption so execution of a non gadget job cannot increase the temperature to above as long as the temperature before was not greater than now we show by induction that right after the execution of a gadget job the temperature is exactly this is clearly the case after execution of the first job since its heat contribution is now let u be the time when a job is scheduled and suppose that at time u the temperature was let a b c be the triple associated with the block consisting of time slots between u and u then by a b c β at time u the temperature is a b c α a b c α this shows that at time u after scheduling a job the temperature is again we conclude that the schedule is feasible now we show the remaining implication suppose the instance of ri hi pi ui constructed above has a schedule where all jobs meet the deadline we show that there exists a matching of a b c we first show that this schedule must have the form from figure first note that since all jobs have deadline all jobs must be scheduled without any idle time between time and this means that the gadget job of heat contribution must be scheduled at time because that is the only moment of temperature also note that every job has heat contribution at least now we claim that all jobs have to be scheduled every th time unit this holds because two units after scheduling a job the temperature is at least therefore two executions of jobs must be at least time units apart and this is only possible if they are scheduled exactly at times for i n we claim that after every execution of a gadget job the temperature is at least τ clearly this is the case after the execution of the job now assume that at time for some i n the claim holds then at time after the execution of the next job the temperature is at least τ τ we show now that every block contains exactly one a job one b job and one c job in that order towards contradiction suppose that some a job is scheduled at the position of a block say at time for some i n its heat contribution is at least therefore the temperature at time would be at least τ contradicting that a job is scheduled at that time a similar argument shows that a jobs cannot be scheduled at position in a block and therefore the position of a block is always occupied by an a job by an analogous reasoning we show that a b job cannot be scheduled at the position of some block it it were scheduled there the temperature at the end of block would be at least τ again contradicting that a job is scheduled at the end of the block we showed that every block contains jobs that correspond to some triple a b c a b c it remains to show that each such triple satisfies a b c β let ai bi ci be the triple corresponding to the i th block for i n define and ti ai bi ci 374 ai bi ci β thus ti represents the contribution of the ith block and a following gadget job to the temper ature right after this gadget job this implies that for k n the temperature at time is exactly k k iti by assumption n ai bi ci nβ and thus n i i i ti n define pi ti for i n from the previous paragraph n pi i as mentioned earlier k k iti is the temperature at time so we have k k iti therefore for all k n we get k k k k ti i i k k k iti k k i k k we conclude that for k n we have k i to complete the proof it remains to show that pi for all i for this will imply that ai bi ci β which in turn implies that there is a matching we prove this claim by contradiction suppose that not all pi are zero let f be the smallest index such that pf and pf clearly f by the minimality of f for every k f we have pk and pk pf there are σi i f such that j σi then f f j f f pj σipj σi pi σfpf because all terms are non negative and pf this contradicts it remains to show that the above instance of hi pi ui can be produced in polynomial time from the instance of numerical matching indeed every number x a b c is mapped to some fraction where both the denominator and numerator are linear in x and β therefore if we represent fractions by writing the denominator and numerator and not as some rounded decimal expansion the reduction will take polynomial time and the proof is complete d theorem implies that other variants of temperature aware scheduling are np hard as well consider for example the problem hi pi cmax where the objective is to minimize the makespan that is the maximum completion time in the decision version of this problem we ask if all jobs can be completed by some given deadline c which is exactly what we proved above to be np hard it also gives us np hardness of hi pi cj to prove this we can use the decision version of this problem where we ask if there is a schedule for which the total completion time is at most n n where n is the number of jobs an online competitive algorithm in this section we show that there is a competitive algorithm for online ri hi pi ui we will show in fact that a large class of deterministic algorithms is competitive given a schedule we will say that a job j is pending at time u if j is released not expired that is rj u dj and j has not been scheduled before u if the temperature at time u is τu and j is pending then we call j admissible if τu hj that is j is not too hot to be executed we say that a job j dominates a job k if j is both not hotter and has the same or smaller deadline than k that is hj hk and dj dk if at least one of these inequalities is strict then we say that j strictly dominates k an online algorithm is called reasonable if at each step i it schedules a job whenever one is admissible the non waiting property and if there is one ii it schedules an admissible job that is not strictly dominated by another pending job the class of reasonable algorithms contains for example the following two natural algorithms coolestfirst always schedule a coolest admissible job if there is any breaking ties in favor of jobs with earlier deadlines earliestdeadlinefirst always schedule an admissible job if there is one with the earliest deadline breaking ties in favor of cooler jobs if two jobs have the same deadlines and heat contributions both algorithms give preference to one of them arbitrarily theorem any reasonable algorithm for online ri hi pi ui is competitive proof let a be any reasonable algorithm we fix some instance and we compare the schedules produced by a and the adversary on this instance the proof is based on a charging scheme that maps jobs executed by the adversary to jobs executed by a in such a way that no job in a schedule gets more than two charges type charge a type charge a a type charge adv adv hq hp adv hj hp hj hj hj hj figure four types of charges the vertical inequality signs between the schedules show the relation between the temperatures we now describe this charging scheme see figure for illustration there will be three types of charges depending on whether a is busy or idle at a given time step and on the relative temperatures of the schedules of a and the adversary the temperature at time u in the schedules of and the adversary will be denoted by τu and τul respectively suppose that at some time u schedules a job k while the adversary schedules a job j or that the adversary is idle we treat this case as if executing a job j with hj then step u will be called a relative heating step if k is strictly hotter than j that is hk hj note that if τv τvl v for some time v then a relative heating step must have occurred before time consider now a job j scheduled by the adversary say at time v the charge of j is defined as follows type charges if schedules a job k at time v charge j to k otherwise we are idle and then we have two more cases type charges suppose that a is hotter than the adversary at time v but not at v that is τu τul and τu τul in this case we charge j to the job q executed by a in the last relative heating step before v as explained above this step exists type charges suppose now that either a is not hotter than the adversary at v or a is hotter than the adversary at v in other words τv τvl or τv τvl note that in the latter case we must also have τv τvl as well since the algorithm is idle we claim that τv hj which means that neither j or any job f with hf hj can be pending at v to justify this we consider the two sub cases of the condition for type charges if τv τvl the claim is trivial since then τv hj τvl hj because the adversary executes j at time v so assume now that τv τvl since a is idle we have τv therefore hj τvl and the claim follows because τv as well from the above claim j was scheduled by at some time u v to find a job that we can charge j to we construct a chain of jobs j jl jll j with strictly decreasing heat contributions further all jobs in this chain except j will be executed by at relative heating steps this chain will be determined uniquely by j and we will charge j to j if at time u the adversary is idle or schedules an equal or hotter job then j j that is we charge j to itself its copy in a schedule otherwise if the adversary schedules jl at time u then jl is strictly cooler than j that is hjl hj now we claim that the algorithm schedules jl at some time before v indeed if jl is scheduled before u we are done otherwise jl is pending at u and since the algorithm never schedules a dominated job we must have djl dj v by our earlier observation and by hjl hj if did not schedule jl before v then jl would have been admissible at v contradicting the fact is idle at v so now the chain is j jl let ul be the time when schedules jl if the adversary is idle at time ul or if jl is not hotter than the job executed by the adversary at time ul we take j jl otherwise we take jll to be the job executed by the adversary at time ul and so on this process must end at some point since we deal with strictly cooler and cooler jobs so the job j is well defined this completes the description of the charging scheme now we show that any job sched uled by will get at most two charges obviously each job in schedule gets at most one type charge in between any two time steps that satisfy the condition of the type charge there must be a relative heating step so the type charges are assigned to distinct relative heating steps as for type charges every chain defining a type charge is uniquely defined by the first considered job and these chains are disjoint therefore type charges are assigned to distinct jobs now let k be a job scheduled by at some time v by the previous paragraph k can get at most one charge of each type we claim that k cannot get charges of each type and indeed if k receives a type charge then the adversary is not idle at time v and schedules some job f if k also receives a type charge then v must be a relative heating step that is hk hf but to receive a type charge k would be the last job j in a chain of some job j and since the chain was not extended further we must have hk hf so type type and type charges cannot coincide summarizing the argument we have that every job scheduled by the adversary is charged to some job scheduled by a and every job scheduled by a receives no more than charges therefore the competitive ratio of a is not more than d a lower bound on the competitive ratio theorem every deterministic online algorithm for online ri hi pi ui has com petitive ratio at least proof fix some deterministic online algorithm we the adversary release a job in notation j rj dj hj if schedules it at time we release at time a tight job and schedule it followed by schedule is too hot at time to execute job if does not schedule job at time then we schedule it at and release at time and schedule a tight job at time in this case cannot complete both jobs and without violating the thermal threshold in both cases we schedule two jobs while a schedules only one completing the proof see figure d adv adv a a figure the lower bound for deterministic algorithms final comments many open problems remain perhaps the most intriguing one is to determine the randomized competitive ratio for the problem we studied the proof of theorem can easily be adapted to prove the lower bound of but we have not been able to improve the upper bound of this is in fact the main focus of our current work on this scheduling problem one approach based on theorem would be to randomly choose at the beginning of computation two different reasonable algorithms each with probability and then deterministically execute the chosen i so far we have been able to show that for many natural choices for and say coolestfirst and earliestdeadlinefirst this approach does not work extensions of the cooling model can be considered where the temperature after executing j is τ hj r for some r even this formula however is only a discrete approximation for the true model see for example and it would be interesting to see if the ideas behind our competitive algorithm can be adapted to these more realistic cases in reality thermal violations do not cause the system to idle but only to reduce the frequency with frequency reduced to half a unit job will execute for two time slots several frequency levels may be available we assumed that the heat contributions are known this is counter intuitive but not unrealistic since the jobs in our model are unit slices of longer jobs prediction methods are available that can quite accurately predict the heat contribution of each slice based on the heat contributions of the previous slices nevertheless it may be interesting to study a model where exact heat contributions are not known other types of jobs may be studied for real time jobs one can consider the case when not all jobs are equally important which can be modeled by assigning weights to jobs and maxi mizing the weighted throughput for batch jobs other objective functions can be optimized for example the flow time one more realistic scenario would be to represent the whole processes as jobs rather then their slices this naturally leads to scheduling problems with preemption and with jobs of arbitrary processing times when the thermal threshold is reached the execution of a job is slowed down by a factor of here a scheduling algorithm may decide to preempt a job when another one is released or say when the processor gets too hot finally in multi core systems one can explore the migrations say moving jobs from hotter to cooler cores to keep the temperature under control this leads to even more scheduling problems that may be worth to study networked end systems such as desktops and set top boxes are often left powered on but idle leading to wasted energy consumption an alternative would be for these idle systems to enter low power sleep modes un fortunately today a sleeping system sees degraded func tionality ﬁrst a sleeping device loses its network pres ence which is problematic to users and applications that expect to maintain access to a remote machine and sec ond sleeping can prevent running tasks scheduled dur ing times of low utilization e g network backups var ious solutions to these problems have been proposed over the years including wake on lan wol mechanisms that wake hosts when speciﬁc packets arrive and the use of a proxy that handles idle time trafﬁc on behalf of a sleep ing host as of yet however an in depth evaluation of the potential for energy savings and the effectiveness of proposed solutions has not been carried out to remedy this in this paper we collect data directly from en terprise users on their end host machines capturing net work trafﬁc patterns and user presence indicators with this data we answer several questions what is the po tential value of proxying or using magic packets which protocols and applications require proxying how com prehensive does proxying need to be for energy beneﬁts to be compelling and so on we ﬁnd that although there is indeed much potential for energy savings trivial approaches are not effective we also ﬁnd that achieving substantial savings requires a careful consideration of the tradeoffs between the proxy complexity and the idle time functionality available to users and that these tradeoffs vary with user environ ment based on our ﬁndings we propose and evaluate a proxy architecture that exposes a minimal set of apis to support different forms of idle time behavior international computer science institute intel research university of california berkeley lawrence berkeley national laboratories introduction recent years have seen rising concern over the energy consumption of our computing infrastructure a recent study estimates that in the u s alone energy con sumption for networked systems approaches twh with an associated cost of around billion dollars about of this consumption can be attributed to homes and enterprises and the remaining to net works and data centers our focus in this paper is on re ducing the consumed in homes and enterprises to put this in perspective this energy twh is roughly equivalent to the yearly output of nuclear plants of equal concern is that this consumption has grown and continues to grow at a rapid pace in response to these energy concerns computer ven dors have developed sophisticated power management techniques that offer various options by which to reduce computer power consumption broadly these techniques all build on hardware support for sleep s states and frequency voltage scaling processor p states the former is intended to reduce power consumption during idle times by powering down sub components to different extents while the latter reduces power con sumption while active by lowering processor operating frequency and voltage during active periods of low sys tem utilization of these sleep modes offer the greatest reduction in the power draw of machines that are idle for example a typical sleeping desktop draws no more than as compared to at least when on but idle an order of magnitude reduction it is thus unfortunate that sleep modes are not taken advantage of to anywhere close to their fullest potential surveys of ofﬁce buildings have shown that about two thirds of desktops are fully on at night with only asleep our own measurements section reveal that enterprise desktops remain idle for an average of hours day time that could in theory be spent mostly sleeping relative to an idle machine the only loss of functional ity to a sleeping machine is twofold first since a sleep ing computer cannot receive or transmit network mes sages it effectively loses its presence on the network this can lead to broken connections and sessions when the machine resumes e g a sleeping machine does not renew its dhcp lease and hence loses its ip address and also prevents remote access to a sleeping computer this loss of functionality is problematic in an increas ingly networked world for example a user at home might want to access ﬁles on his desktop at work an on the road user might want to download ﬁles from his home machine to his handheld system administrators might desire access to enterprise machines for software updates security checks and so forth in fact some en terprises require that users not power off their desk tops to ensure administrators can access machines at all times the second problematic scenario is when users or administrators deliberately want to schedule tasks to run during idle times e g network backups that run at night critical software updates and so on unfortu nately these drawbacks cause users to forego the use of sleep modes leading to wasteful energy consumption the above observations are not new having been re peatedly articulated also by some of the authors in both the technical literature and popular press likewise there have been two long standing pro posals on how to tackle the problem the ﬁrst is to gen eralize the old technology of wake on lan wol an ethernet computer networking standard that allows a ma chine to be turned on or woken up remotely by a special magic packet a second more heavyweight proposal has been to use a proxy that handles idle time trafﬁc on behalf of a sleeping host waking the sleeping host when appropriate thus both problem wasted energy consumption by idle computers and proposed solutions wake up packets and or proxies for sleeping machines have existed for a while now in fact the technology for wol has been implemented and deployed although not widely used we explore possible causes for this later in the paper however the recent focus on energy con sumption has led to renewed interest in the topic with calls for research calls for standardization and even some commercial prototypes as yet how ever there has been little systematic and in depth evalua tion of the problem or its solutions what savings might such solutions enable what is the broader design space for solutions what if any might be the role of standard ization are these the right long term solutions etc in this paper we explore these questions by studying user behavior and network trafﬁc in an enterprise envi ronment speciﬁcally we focus on answering the follow ing questions is the problem worth solving just how much energy is squandered due to poor computer sleeping habits this will tell us the potential energy savings these solutions stand to enable and hence the complexity they warrant also is proxying really needed to realize these potential savings or can we hope that wol sufﬁces to maintain network presence while still sleeping usefully what network traffic do idle machines see un derstanding this will shed light on how this idle time traf ﬁc might be dealt with and consequently what protocols and applications might trigger wake up packets and or require proxying on the face of it it would seem like an idle machine ought not to be engaged in much useful activity and hence ideally one might hope that a small number of wake up events are required and or that a rel atively small set of protocols must be proxied to realize useful savings what is the design space for a proxy in general the space appears large different proxy implementations might vary in the complexity they undertake in terms of what work is handled by the proxy vs waking the ma chine to do so in some cases one might opt for a rela tively simple proxy that for example only responds to certain protocols such as arp speciﬁed by the dmtf standard and netbios but more complex proxies are also conceivable for example a proxy might take on application speciﬁc processing such as initiat ing completing bittorrent downloads during idle times and so forth likewise there are many conceivable de ployment options a proxy might run at a network mid dlebox e g ﬁrewall nat etc at a separate machine on each subnet or even at individual machines e g on its nic on a motherboard controller or on a usb attached lightweight microengine given this breadth of options we are interested in whether one can iden tify a minimal proxy architecture that exposes a set of open apis that would accommodate a spectrum of design choices and deployment models doing so appears im portant because a proxy potentially interacts with a diver sity of system components and even vendors hardware power management operating systems higher layer ap plications network switches nics etc and hence iden tifying a core set of open apis would allow different ven dors to co exist and yet innovate independently for ex ample an application developer should be able to deﬁne the manner in which his application interacts with the proxy with no concern for whether the proxy is deployed at a ﬁrewall a separate machine or a nic what implications does proxying have for future protocol and system design the need for a proxy arises largely because network protocols and applica tions were never designed with energy efﬁciency in mind nor to usefully exploit or even co exist with power man agement in modern pcs and operating systems while proxies offer a pragmatic approach to dealing with this mismatch for currently deployed protocols and software one might also take a longer term view of the problem and ask how we might redesign protocols applications or even hardware power management to eventually obvi ate the need for such proxying altogether in this paper we study the network related behavior of users and machines in enterprise and home environ ments and evaluate each of the above questions in sec tions to respectively measurement data and methodology we collected network and user level activity traces from approximately client machines belonging to intel corporation employees for a period of approximately weeks the machines running windows xp include both desktops and notebooks approximately are desktops and the rest notebooks our trace collection software was run at the individ ual end hosts themselves and hence in the case of note books trace collection continued uninterrupted as the user moved between enterprise and home enabling us to analyze trafﬁc from both of these environments our packet traces were collected using windump to capture user activity we developed an application that sampled a number of user activity indicators at one sec ond intervals the user activity indicators we collected included keyboard activity and mouse movements and clicks noticeable gaps in the traces occur when the host was turned off put to sleep or in hibernation thus each end host is associated with a trace of its network and user activity we then used bro to reassemble connection level information from each packet level trace thus for the week duration of our measurement study we have the following information for each end host a packet level pcap trace capturing packet headers for the entire duration per second indicators of user presence at the machine the set of all connections incoming and outgoing as reconstructed by bro from the packet traces the result is a repository of trace data to pro cess this we developed a custom tool that extends the publicly available wireshark network protocol an alyzer with different function callbacks implementing the additional processing required for our study low power proxying potential and need in this section we estimate the energy wasted by home and ofﬁce computers that remain powered on even when idle i e even when there is no human interacting with the computer subsequently we investigate whether very simple approaches e g the computer is woken up to process every network packet and then returns to sleep immediately after would sufﬁce in allowing hosts to sleep more while preserving their network presence how much energy is squandered by not sleeping virtually all modern computers support advanced sleep states as deﬁned in the acpi speciﬁcation figure distribution of the split among off idle and active periods across users these states vary in their characteristics whether the cpu is powered off how much memory state is lost which buses are clocked and so on however common to all states is that the cpu stops executing instructions and hence the computer appears to be powered down thus although these sleep states conserve energy the un desirable side effect is that a sleeping computer effec tively falls off the network making it unavailable for remote access and unable to perform routine tasks that may have been scheduled at particular times this leads many users to disable power management altogether and instead leave machines running for example stud ies have shown that approximately of the pcs in of ﬁce buildings remain powered on overnight and almost all of these have power management disabled to more carefully quantify the amount of wasted en ergy and hence potential savings we analyzed the trace data collected at our enterprise machines to determine whether a machine has a locally present and active user we examine the recorded mouse and keyboard activity for the machine if no such activity is recorded for minutes we say that the machine is idle we use min utes because it is the default timeout recommended by energystar for putting machines to sleep and because it represents a simple and fairly liberal approximation for the notion of idle ness for which a standard deﬁnition does not exist we maintain this deﬁnition of idle ness for the remainder of the paper at any point in time we classify a machine as being in one of four possible states a on and actively used we call this active b on but not used idle c in a sleep state such as or and d powered down off note that this notion of idle refers here to the user and not the machine being inactive in figure we present this data for our enterprise desk tops we focus here on the desktops since this represents the potential energy savings an enterprise could garner because the bulk of our traces come from mobile users we have a limited number of desktops we see that the fraction of time when these machines are active is quite low falling below on average moreover the aver home office all usage environment second long bins for inter packet gaps figure average number of directed and broad cast multicast packets received on average by a network host at home and in the ofﬁce age fraction of time when machines are idle is high about similar to other studies we note that a small fraction of our desktops only out use sleep mode at all overall this indicates that there is a tremendous opportunity for energy savings on enterprise desktops the opportunity on our corporate laptops exists too but is moderate because we found that our laptop users were more likely to employ aggressive sleeping conﬁgurations that come pre conﬁgured on laptops while the sample of the desktop machines in our exper iments is small the results are consistent with existing studies we therefore use these measured idle times to extrapolate the energy that could be saved by sleeping instead of remaining idle there are estimated to be about million desktop pcs in the us data summarized in assuming an power consumption of an idle pc and assuming these machines are idle for of the time this amounts to roughly twh year of wasted electricity or billion dollars at us per kwh is low power proxying needed before developing new solutions to reducing host idle times we investigate whether very simple approaches like waking up for ev ery packet can deliver these savings while maintaining full network presence in this approach which we denote wop wake on packet the machine is woken up for every packet it needs to receive directed or broadcast and put back to sleep after the packet is served the per formance of such an approach depends on whether the inter packet gap ipg is smaller or comparable to the time it takes to transition in and out of sleep if it isn t then there is no gain over simply leaving the machine in an idle state to examine the trafﬁc during idle times we used both our desktop and laptop machines we consider both types even though we re primarily interested in desktops be cause this gives us a signiﬁcantly larger set of samples we separate the idle time trafﬁc into two categories of ﬁce and home in figure we plot the average number of packets sec for idle trafﬁc both in the ofﬁce and at home in the ofﬁce environment the average number of packets figure histogram of the fraction of the idle time made up of inter packet gaps of different size per second is roughly while at home it is roughly this indicates a fairly constant level of background chat ter on the network independent of the user activity be cause this number is an average we need to understand if these packets occur in bursts or not if the packets are bursty most of the time then there may still be opportu nities to sleep as the host can be woken up to service a burst of packets and then be put to sleep for some reason able period of time certainly more than a few seconds if these packets occur fairly evenly spaced then it is not worth going to sleep unless the time to transition in and out of sleep is very small on the order of to seconds to quantify the burstiness level of our trafﬁc we group inter packet gaps into second long bins i e etc we then compute the sum of the inter packet gaps in each of these bins and ﬁnally compute the fraction of total idle time represented by each bin we present these results in figure for both home and ofﬁce envi ronments in the ofﬁce over of the time the ipg is less than seconds although the distribution is more uniformly spread for the home environment we still see that roughly of the time the ipg is less than seconds overall we observe that a neither of the en vironments enjoys many long periods of quiet time b we ﬁnd this distribution to be very different for the two environments in home networks the distribution has a much heavier tail the trafﬁc is burstier and we do see longer periods of quiet time we now translate these observations into actual sleep time in order to perform this computation we must con sider a representative value for the time interval it takes the host to wake up process the packet and then go to sleep again we call this the transition time denoted ts today typical machines take seconds to enter sleep and seconds to fully resume from as mea sured in a recent study therefore it is reasonable to assume an average transition time ts of when a packet arrives the machine is woken up to serve the packet after processing a packet the machine only goes to sleep again if it knows the next packet will not arrive before it transitions to sleep this idealized test sorted users figure the fraction of idle time users can sleep if they wake up for every packet across different environments for a transition time ts thus assumes that the host knows the future incoming packet stream and captures the best the machine could do in terms of energy savings figure presents the fraction of idle time for which users can sleep assuming the policy described above the results are rather dramatically different for across environments in the ofﬁce there is almost no oppor tunity to sleep for the majority of the users this indi cates that the magic packet like approach will not suc ceed in saving any energy for machines in a typical cor porate ofﬁce environment for the home environment we see that roughly half the users can sleep for over of their idle times thus in these environments a transition time coupled with a wop type policy can be somewhat effective however these estimates assume perfect knowledge of future trafﬁc arrivals and also fre quent transitions in and out of sleep in practice we ex pect the achievable savings would be somewhat lower nonetheless this does suggest that efforts to reduce sys tem transition times in future hardware could obviate the need for more complex power saving strategies in certain environments we conclude that while signiﬁcant opportunity for sleep exists capitalizing on this opportunity requires so lutions that go beyond merely waking the host to han dle network trafﬁc we thus consider solutions based on proxying idle time trafﬁc in the following sections deconstructing traffic in the previous section we saw that by just waking up to handle all packets our ability to increase a machine sleep time is limited in particular we see virtually no energy savings in the dominant ofﬁce environments this suggests that we need an approach that is more discrim inating in choosing when to wake hosts this leads us to an alternate solution to the wol which is to employ a network proxy whose job is to handle idle time trafﬁc on behalf of one or more sleeping hosts packets destined for a sleeping host are intercepted by or routed to de figure composition of incoming and outgoing traf ﬁc during idle times for home and ofﬁce environments based on communication paradigms pending on the proxy deployment model its proxy at this point the proxy must know what to do with this in tercepted trafﬁc broadly the proxy must choose between three reactions a ignore drop the packet b respond to the packet on behalf of the machine or c wake up the machine to service it to make a judicious choice the proxy must have some knowledge of network trafﬁc what trafﬁc is safely ignorable what applications do packets belong to which applications are essential and so forth in this section we do a top down deconstruc tion of the idle time trafﬁc traces aimed at learning the answers to these questions traffic classes by communication paradigm to begin we look at all packets exchanged during idle periods and classify each packet as either being a broad cast multicast or unicast packet within these broad traf ﬁc classes we further partition the trafﬁc by whether the packets are incoming or outgoing for both the home and ofﬁce environments we separate incoming and outgoing trafﬁc because we expect them to look different in terms of the proportion of each class in different directions e g most end hosts ought to send little broadcast traf ﬁc similarly we look at different usage environments because it is intuitive that the dominant protocols and ap plications used in each environment may differ since we expect these differences we treat them as such to avoid mischaracterizations the breakdown of our trafﬁc ac cording to all these partitions in depicted in fig we note that outgoing trafﬁc is dominated by unicast trafﬁc since as expected each host generates little broad cast or multicast trafﬁc we also ﬁnd that incoming trafﬁc at a host sees signiﬁcant proportions of all three classes of trafﬁc and this is true in both enterprise and home environments this suggests that a power saving proxy might have to tackle all three trafﬁc classes to see signif icant savings so far we looked at trafﬁc volumes as indicative of the need to proxy the corresponding trafﬁc type we now di rectly evaluate the opportunity for sleep represented by each trafﬁc type to understand the maximum sleeping opportunities we consider for a moment an idealized scenario in which we use our proxy to ignore all incom ing packets from either or both of the broadcast and mul ticast trafﬁc classes a machine always wakes up for uni cast packets fig shows the sleep potential in four sce narios a ignore only broadcast and wake for the rest c ignore only multicast and wake for the rest c ignore both broadcast and multicast for comparison purposes home office we also include the results for a scenario d in which we wake up for all packets this comparison allows us to compare the beneﬁts derived from these four different proxy policies for each user we computed the fraction of its idle time that could have been spent sleeping un der the scenario in question we use a transition time of ts and the results are averaged over users for both home and ofﬁce environments we make the following observations i broadcast and multicast are largely responsible for poor sleep if we can proxy these then we can recuper ate over of the idle time in home environments and in the ofﬁce where previously sleep was barely possible we can now sleep for over of the idle time ii doing away with only one of either broadcast or multicast is not very effective we suspect this is due to the periodicity of multicast and broadcast protocols and evaluate this in later sections more generally the graph clearly indicates a valuable conclusion if we re looking to narrow the set of traf ﬁc classes to proxy then multicast and broadcast traf ﬁc appear to be clear low hanging fruit and should be our primary candidates for proxying that said proxying unicast trafﬁc appears key to achieving higher savings beyond in the enterprise and hence should not be dismissed either we thus continue for now to study all three trafﬁc types of course whether these potential savings can actually be realized depends on whether a particular trafﬁc type can indeed be handled by a proxy without waking the host this depends on the speciﬁc protocols and applica tions within that class and hence in the remainder of this section we proceed in turn to deconstruct each of broad cast multicast and unicast trafﬁc deconstructing broadcast our goal in this section is to evaluate individual broad cast protocols looking for which of these protocols are the main offenders in terms of preventing hosts from sleeping and what purpose do these protocols serve and how might a proxy handle them answering the ﬁrst question requires a measure of protocol badness with respect to preventing hosts from sleeping we use two metrics for our evaluation the ﬁrst is simply the total volume of traffic due to the protocol in question while high volume trafﬁc often makes sleep harder this is an figure average sleep opportunity when ignoring mul ticast and or broadcast trafﬁc for different environments imperfect metric since the in ability to sleep depends as much on the precise temporal packet arrival pattern due to the protocol as on packet volumes nonetheless we re tain trafﬁc volume as an intuitive although indirect mea sure of protocol badness our second metric which we term the half sleep time denoted ts more directly measures a protocol role in preventing sleep we deﬁne the half sleep time for a protocol or trafﬁc type p as the largest host transition time that would be required to allow the host to sleep for at least of its idle time under the scenario where the machine wakes up for all packets of type p and ignores all other trafﬁc in effect ts quantiﬁes the intuition that if we ignore all trafﬁc other than that due to the protocol of interest then a protocol whose packets arrive spaced far enough apart in time is more conducive to sleep since the host has sufﬁcient time to transition in and out of sleep in more detail ts is computed from our traces as follows we measure the total time a given host can sleep assuming it wakes up for all the packets of the protocol under consideration and ignores all others we compute this number for all hosts and take the average this gives us an upper bound on achievable sleep if the protocol is handled by waking the host we estimate this sleep duration for different values of the host transition time ts ranging from seconds ideal to minutes the largest of these transition times ts that allows the host to sleep for over of its idle time is the protocol ts intuitively ts indicates the extent to which a pro tocol is sleep friendly since protocols with large val ues of ts could simply be handled by allowing the machine to wake up whereas those with low values of ts imply that to achieve useful sleep the proxy must handle such trafﬁc without waking the host for our evaluation we classify each packet by protocol and rank them by both metrics trafﬁc volume and the half sleep time we begin by measuring trafﬁc volume we then establish the top ranking protocols by volume and use these as candidates for our second metric the half sleep time when presenting the top ranking proto cols by each of the metrics we consider the proto cols whose trafﬁc volumes represents more than of office home protocol of traffic protocol of traffic hsrp ssdp pim igmp eigrp other ssdp hsrp igmp other total total figure protocol composition of incoming broadcast trafﬁc in both ofﬁce and home environments ranked by per protocol trafﬁc volumes office home all bcast all bcast arp arp nbdgm nbdgm nbns nbns ipx figure protocol composition for broadcast protocols ranked by ts the total trafﬁc at the host and the protocols with a half sleep time of less than minutes table and present our results for broadcast trafﬁc for complete ness we also present the value of ts when consider ing all broadcast trafﬁc together in terms of trafﬁc volumes we see that the bulk of broadcast trafﬁc is in the cause of address resolution and various service discovery protocols e g arp netbios name service nbns the simple service discovery protocol used by upnp devices ssdp these proto cols are well represented in both home and ofﬁce lans a second well represented category of trafﬁc is from router speciﬁc protocols e g routing protocols imple mented on top of the ipx in terms of the half sleep time we see that broadcast as a whole allows very little sleep in the ofﬁce achiev ing sleep would require very fast transitions be tween and seconds not feasible with today hard ware support the situation in home lans is signiﬁ cantly better ts in terms of protocols we see that the greatest offenders are similar to those from our trafﬁc volume analysis namely arp netbios data grams nbdgm and name queries nbns and ipx on closer examination we ﬁnd that most of these of fending protocols could be easily handled by a proxy for example ipx is safely ignorable arp trafﬁc that is not destined to the machine in question is likewise safely ignorable for arp queries destined to the machine it would be fairly straightforward for a proxy to automati cally construct and generate the requisite response with out having to wake the host figure protocol composition for incoming multicast trafﬁc in both ofﬁce and home enviroments ranked by per protocol trafﬁc volumes office home all mcast all mcast hsrp ssdp pim hsrp igmp igmp ssdp figure protocol composition for incoming multicast trafﬁc in both ofﬁce and home environments ranked by ts deconstructing multicast table and present our protocol rankings for multicast trafﬁc again we also present the value of ts when considering all multicast trafﬁc taken to gether we see that multicast trafﬁc as a whole can be a bad offender in enterprise environments with an ts it turns out that this is largely caused by router trafﬁc the hot standby router protocol hsrp protocol independent multicast pim eigrp etc this trafﬁc is either absent e g pim or greatly re duced e g hsrp in home environments which ex plains why multicast is much less problematic in homes with an ts minutes compared to for broadcast the good news is that all router trafﬁc hsrp pim igrp is safely ignorable in fact many modern ether net cards already include a hardware multicast ﬁlter that discards most unwanted multicast trafﬁc as with broadcast trafﬁc we also see signiﬁcant trafﬁc contributed by service discovery protocols in this case ssdp the simple service discovery protocol used by upnp devices once again for protocols such as ssdp and igmp it is fairly straightforward for a proxy to auto matically respond to incoming trafﬁc without waking the host doing so would require some amount of state at the proxy such as the list of multicast groups the interface belongs to and the services running on the machine deconstructing unicast finally we present our protocol ranking for unicast traf ﬁc in tables and because much of unicast traf ﬁc is either tcp or udp and this level of classiﬁca tion is unlikely to be informative we further break each figure fraction of packets generated by incoming vs outgoing connections for home and ofﬁce both received and transmitted packets figure protocol composition of incoming unicast trafﬁc in ofﬁce enviroments ranked by per protocol traf ﬁc volumes office home all ucast all ucast tcp udp udp dns dce rpc tcp dns smb nbns http figure protocol composition of incoming unicast trafﬁc in ofﬁce environments ranked by ts port app tcp keep alives udp tcp tcp tcp tcp tcp udp tcp syns many dns dce rpc smb cifs bigfix dns http bigfix many 4min 8min 8min 15min figure protocol composition for unicast trafﬁc based on tcp and udp ports ran ked by ts down by session layer protocol with an additional map ping from ports in table unfortunately unlike the case of broadcast and multicast with unicast it is harder to deduce the ultimate purpose for much of this trafﬁc since even the session or application level protocol iden tiﬁers are fairly generic one exception is the bigfix application listed in fig bigfix is an enterprise soft ware patching service that checks security compliance of enterprise machines based on the frequency and volume of bigfix trafﬁc we see it appears to have been conﬁg ured by an over zealous system administrator stymied in our attempts to deconstruct unicast trafﬁc based on whether and how it might be proxied we try an alternate strategy we classify tcp and udp pack ets based on the connections they belong to and catego rize connections as incoming vs outgoing our interest in this classiﬁcation is because we suspect that a large portion of packets are likely to belong to outgoing con nections and while a host might wake for incoming con nections waking for outgoing connections might well be avoidable for reasons discussed below from the results in fig we see that outgoing connections do indeed dominate now for a sleeping machine there are three possibilities for these outgoing connections the con nection was initiated by the host before the idle period in this case such trafﬁc might not be ignorable if the host proxy wants to maintain this connection hence we hope this percentage of trafﬁc is small the connec tion was initiated but failed the connection was ini tiated by the host after the start of the idle period for a sleeping host these connections would either simply never have been initiated if the connection were deemed unncessary or the host would be deliberately woken to initiate these connections if the connection were deemed necessary as for services scheduled to run during idle times for the former the trafﬁc can simply be ignored from our accounting and in the latter case such sched uled processing is easily batched and hence needn t dis rupt sleep hence for all but the ﬁrst case waking the machine might be avoidable we plot this breakdown of outgoing connections in figure we see that only a relatively small percentage of outgoing connections al ways less than belong to the ﬁrst category which might require waking the host based on this we specu late that it might be possible to eliminate or ignore much of even unicast trafﬁc early in this section we asked whether one might iden tify a small set of of protocols or proxy behaviors that could yield signiﬁcant savings we ﬁnd that the answer is positive in the case of multicast and broadcast but less clear for unicast trafﬁc in the next section we consider the implications of our trafﬁc analysis for proxy design don t wake don t ignore hsrp arp pim nbdgm icmp igmp ssdp arp for me nbns dhcp for me home office all table protocols that shouldn t cause a wake up too expen sive in terms of sleep and protocols that should not be ignored for correctness ignorable hsrp pim arp for others ipx llc eigrp dhcp figure for outgoing connections the fraction of incoming packets that belong to new connections and failed connection attempts a measurement driven approach to proxy design having studied the nature of idle time trafﬁc we now ap ply our ﬁndings to the design of a practical power saving proxy we start in section by extracting the high level design implications of our trafﬁc analysis from the previ ous section building on this in section we illustrate mechanical response protocol state arp ip address nbns nb names of machine and local services ssdp names of local plug n play services igmp multicast groups the inter face belongs to icmp ip address nbdgm nb names of machine and local services ignores pkts not destined to host wakes host for rest the space of design tradeoffs by considering four speciﬁc examples of proxies in section we distill our ﬁnd ings into a proposal for a core proxy architecture that of fers a single framework capable of supporting the broad design space we identify design implications at minimum a power saving proxy should a allow the host to sleep for a signiﬁcant fraction of the time and b maintain the basic network presence of the host by ensuring remote entities can still address and reach the machine and the services it supports beyond this we have a signiﬁcant margin of freedom in choosing how a proxy might handle the remaining idle time trafﬁc and applications viewed through this lens our results from section lead us to differentiate idle time trafﬁc along two different dimensions the ﬁrst classiﬁes trafﬁc based on the need to proxy the trafﬁc in question don t wake protocols these are protocols that gen erate sustained and periodic trafﬁc and hence ideally would be dealt with by a proxy without waking the host since otherwise the host would enjoy little sleep exam ples of such protocols identiﬁed in the previous section include igmp pim arp table lists a set of protocols we classify as don t wake don t ignore protocols these are protocols that re quire attention to ensure the correct operation of higher layer protocols and applications for example we must ensure the dhcp lease on an ip address must be main tained and that a machine must respond to netbios name queries to ensure the services it runs over netbios remain addressable the protocols we identiﬁed as don t ignore are listed in table note that the list of don t wake and don t ignore protocols need not be mutually table protocols that can be handled by ignoring or by me chanical response we classify dhcp as ignorable because we choose to schedule the machine to wake up and issue dhcp requests to renew the ip lease an infrequent event exclusive for example arp trafﬁc is both frequent and critical and hence falls under both categories policy dependent traffic for the remainder of traf ﬁc the choice of whether and how a proxy should handle the trafﬁc is a matter of the tradeoff the user or soft ware designer is seeking to achieve between the sophis tication of idle time functionality the complexity of the proxy implementation and energy savings we shall ex plore these tradeoffs in the context of concrete proxy im plementations in section a complementary dimension along which we can clas sify trafﬁc is based on the complexity required to proxy the trafﬁc in question a ignorable drop this is trafﬁc that can safely be ignored section identiﬁed several such protocols and the top ranked of these are listed in table comparing tables and we see that fortunately there is a sig niﬁcant overlap between don t wake and ignorable protocols policy dependent trafﬁc applications that are deemed unimportant to maintain during idle times could likewise be ignored while don t ignore protocols obviously cannot be b handled via mechanical responses this includes in coming outgoing protocol trafﬁc for which it is easy to construct the required response request using little to no state transferred from the sleeping ho nction is some what subjective based for example a proxy can easily respond to netbios name queries asking about local netbios services once these services are known by the proxy table lists key protocols that can be dealt with through mechanical responses c require specialized processing this covers proto col trafﬁc that if proxied would require more complex state maintenance transfer creation processing and up date between the proxy and host for example consider a proxy that takes on the role of completing ongoing downloads on behalf of a sleeping host this requires that the proxy learn the status of ongoing and sched uled downloads the addresses of peers etc and more over that the proxy appropriately update transfer state at the host once it resumes in theory specialized process ing would be attractive for policy dependent traf ﬁc that is both important and frequently occurring since otherwise we could simply drop unimportant trafﬁc and wake the host to process infrequent trafﬁc of course in addition to the the above classes a c for trafﬁc that a proxy doesn t ignore but doesn t want know to handle a proxy always has the option of waking the host essentially the decision of whether to handle desired trafﬁc in the proxy versus waking the host represents a tradeoff between the complexity of a proxy implementation and the sleep time of hosts example proxies we now present four concrete proxy designs derived from the distinctions drawn above we select these prox ies to be illustrative of the design tradeoffs possible but also representative of practical and useful proxy designs proxy we start with a very simple proxy that ignores all trafﬁc listed as ignorable in table and wakes the machine to handle all other incoming trafﬁc besides clearly ignorable protocols we choose to also ignore trafﬁc generated by the bigﬁx application tcp port which we previously identiﬁed section to be one of the big offenders we do so because this traf ﬁc is a not representative for non intel machines and b the application is very badly conﬁgured sending very large amounts of trafﬁc for little offered functionality making sleep almost impossible this proxy is simple it requires no mechanical or spe cialized processing at the same time because it makes the conservative choice of waking the host for all traf ﬁc not known to be safely ignorable this proxy is fully transparent to users and applications in the sense that the effective behavior of the sleeping machine is never different from had it been idle except for the perfor mance penalties due to the additional wake up time proxy our second proxy is also fully transparent but takes on greater complexity in order to reduce the fre quency with which the machine must be woken this proxy ignores all trafﬁc listed as ignorable in table and issues responses for protocol trafﬁc listed in the same table as to be handled with mechanical responses and wakes the machine for all other incoming trafﬁc since this proxy needs more state to generate mechani cal responses e g the netbios names of local services needed to answer nbns queries it can also use this ex tra information to selectively ignore more packets than proxy e g ignore all netbios datagrams not des tined for local services proxy our third proxy generates even deeper savings by only maintaining a small set of applications chosen by the user operable during idle times while ignoring all other trafﬁc we use telnet ssh vnc smb ﬁle sharing and netbios as our applications of interest this proxy performs the same actions and as implemented by proxy ignore and responds to the same set of proto cols but it wakes up for all trafﬁc belonging to any of telnet ssh vnc smb ﬁle sharing and netbios and drops any other incoming trafﬁc relative to our pre vious example proxy is less transparent in that the machine appears not to be sleeping for some select re mote applications but is inaccessible to all others proxy all the above proxies implement functionality related to handling incoming packets in our ﬁnal proxy we also consider waking up for scheduled tasks initiated locally this proxy behaves identically to proxy with respect to incoming packet but supports an additional action wake up for the following tasks for which we assume that the system is conﬁgured to wake up in order to perform them regular network backups anti virus mcafee software updates ftp trafﬁc for auto matic software updates and intel speciﬁc updates evaluating tradeoffs in the following we compare the sleep achievable by our proposed proxies and com pare it with the baseline wop case we perform this eval uation for both ofﬁce and home environments and in each case we evaluate possible values for transition times ts and seconds the ﬁrst of these is a very optimistic transition time not achievable today using sleep states but foreseeable in the near future today microsoft vista speciﬁcations require computers to resume from sleep in under the second is representative of the shortest transitions achiev able today and the last is representative of a setting that allows almost a minute for processing sub sequent relevant network packets before going to sleep again the advantage of using a very short timer before going to sleep is the increased achievable sleep the dis advantage is that the delay penalty for waking the host will be incurred at more packets in the extreme case of very short sleep timers this could make remote appli cations sluggish and un responsive for the wake events generated by scheduled tasks we use a longer transition time and thus a longer sleep timer value of since such tasks usually take longer time to complete ts ts ts a office environment ts ts ts b home environment sists of a trigger an action and a timeout triggers are either timer events or regular expressions describing some network trafﬁc of interest when a trig ger timer event ﬁres or if an incoming packet matches a trigger regular expression the proxy executes the cor responding action if the action involves waking the host the timeout value speciﬁes the minimum period of time for which the host must stay awake before contemplating sleep again to resolve multiple matching rules standard techniques such as ordering the rules by speciﬁcity pol icy etc can be used the proxy table must also include a default rule that determines the treatment of packets that do not match on any of the explicitly enumerated rules we propose the following actions drop the incoming packet is dropped figure savings achieved by different proxies in home and ofﬁce environments examining the performance of our proxies we make the following high level observations a at one end of the spectrum proxy the simplest is inadequate in ofﬁce environments and borderline adequate in home environments b at the other end of the spectrum we have proxy which only handles a select number of applications but in return achieves good sleep in all sce narios more than of idle time even in the ofﬁce and with a transition time of c the efﬁciency of proxy depends heavily on environment while the additional complexity compared to proxy makes it a good ﬁt in home environments sleeping close to even for ts having to handle all trafﬁc makes it a worse ﬁt for the ofﬁce sleeping for the same transition time this shows that unless they support a large number of rules transparent proxies are a better ﬁt for home but not the ofﬁce d the best tradeoff between functionality and savings and therefore the appropriate proxy conﬁguration depends on the operating environ ment e since scheduled wake ups are typically infre quent the impact they have on sleep is minimal in our case proxy sleeps almost as much as proxy in all considered scenarios a strawman proxy architecture our study leads us to propose a simple proxy architecture that offers a uniﬁed framework within which we can ac commodate the multiplicity of design options identiﬁed above the proposal we present is a high level one since our intent here is merely to provide an initial sketch of an architecture that could serve as the starting point for future discussion on standardization efforts the core of our proposal is a table the power proxy table ppt that stores a list of rules each rule de scribes the manner in which a speciﬁed trafﬁc type should be handled by the proxy when idle a rule con wake the proxy wakes the host and forwards the pack ets to it other packets buffered while waiting for the wake will be forwarded as well respond template state the proxy uses the speciﬁed template to craft a response based on the in coming packet and some state stored by the proxy this action is used to generate mechanical responses as de scribed below redirect handle the proxy forwards the packet to a destination speciﬁed by the handle parameter this is used to accommodate specialized processing as de scribed below a response template is a function that computes the mechanical response based on the incoming packet and one or more immutable pieces of state this means that our function does not maintain or change any state there is no state carried over between successive incoming packets such as sequence numbers and no state trans fer between the proxy and the host upon wake up we choose to support this functionality because a it is rel atively simple to implement in practice and b it covers most of the non application speciﬁc trafﬁc as shown in section and illustrated in our proxy examples to accommodate more specialized processing we as sume developers will write application speciﬁc stubs and then enter a redirect rule into the proxy ppt where the handle speciﬁes the location to which the proxy should send the packet such stubs can run on machine accessible over the network e g a server dedicated to proxying for many sleeping machines in a corporate lan or on a low power micro engine supported on the local host e g a controller on the motherboard or a usb connected gumstick in all these cases the han dle would be speciﬁed by its address for example a ip address port combination the redirect abstraction thus allows us to accommodate specialized processing with out embedding application speciﬁc knowledge into the core proxy architecture the external api to this proxy is twofold apis to figure example click implementation activate deactivate the proxy as the host enters exits sleep and apis to insert and delete rules the process by which to install and execute stubs is outside of the core proxy speciﬁcation which only provides the mechanism to register and invoke such stubs the architecture is ag nostic to where the proxy runs allowing implementations in hardware e g at host nics in pc software e g a proxy server running on the same lan or in network equipment e g a ﬁrewall nat box finally the use of timer events to wake a host already exists today our contribution here is merely to integrate the mechanism into a uniﬁed proxy architecture proxy prototype implementation to illustrate the feasibility of our architecture we build a simple proxy prototype using the click modular router we choose to deploy the proxying function ality in a standalone machine responsible for maintaining the network presence of several hosts on the same lan to allow our proxy let us call it p to sniff the trafﬁc for each host we ensure that p shares the same broadcast domain with these hosts this can be achieved either by connecting the proxy and the machines to a common net work hub or by conﬁguring the lan switch to forward all trafﬁc to the port that serves p in our initial design we don t implement proxies that involve transferring state between the host and the proxy instead p learns the pieces of state required e g the ip address and the netbios name for each host by sniff ing host trafﬁc and extracting the state exchanged e g arp and nbns exchanges this design circumvent the need for any end host modiﬁcations and support proxy ing for machines with different hardware platforms new and old and operating systems the proxy requires min imal conﬁguration a list of the mac addresses of the hosts that need to be proxied and can be incremen tally deployed as a low power stand alone network box once low power proxying standards are developed the design can be extended to support state transfer and achieve even deeper energy savings our prototype implements very basic proxying func tionality but the software architecture presented in fig ure can be easily extended to more protocols and use cases currently we support three types of actions wake respond and drop the proxy awakes its hosts for tcp connection requests incoming tcp syn packets and incoming netbios name queries for the host nb name if such a wake packet for a sleeping host arrives p buffers the request sends a magic packet to wake the host and relays the buffered packet once the host be comes available the proxy responds automatically to incoming arp requests and drops all other incoming packets in relation to the examples discussed in sec tion this prototype has a simple and non transparent design to determine whether a host is awake the proxy sends periodic arp queries to each host if these queries receive no response the host is assumed to be asleep when the proxy attempts to wake a host and fails repeat edly the host is assumed to be off rather than just asleep and the proxy ceases to maintain its network presence figure presents the software architecture of our click proxy and highlights the mapping between click modules and the generic categories of triggers actions and state discussed in the strawman proxy architecture we test our click based proxy implementation by in stalling it on one of our enterprise desktops and con ﬁguring the proxy to maintain the network presence of several ibm thinkpad laptops we use this deployment to measure the delays experienced by applications wak ing a sleeping host and ﬁnd these to be surprisingly low on average and at maximum much lower than the tcp syn timeout these delays includes the host wake up delay and the additional time re quired for the proxy to detect the state change and relay the buffered packet causing the wake we defer a comprehensive deployment based evaluation to future work power aware system redesign in this section we consider approaches that might assist in reducing idle time energy consumption by either sim plifying the implementation of proxies or altogether ob viating the need for proxying software redesign our idle trafﬁc analysis shows that solutions relying on wake on lan functionality face the following chal lenges i it is difﬁcult to decide if various packets and protocols warrant a machine wake up ii hosts receive many packets even when idle per second on average iii many protocols exchange packets periodically pre venting long quiet periods when hosts could sleep these challenges could be dealt with at both application and protocol level power aware application configuration today appli cations and services are typically designed or conﬁgured without taking into account their potential impact on the power management at end systems for example in sec tion we discussed a tool called bigﬁx that checks if network hosts conform to intel corporate security spec iﬁcations this application was conﬁgured to perform these checks very aggressively continuously generating large amounts of trafﬁc under a wol approach this ap plication alone would have made prolonged sleep virtu ally impossible this is a perfect example of the behaviour that could be avoided by conﬁguring applications to be more power aware and perform periodic tasks less frequently reduc ing the volume of network trafﬁc seen by hosts protocol specification the decision to ignore or wake on a packet can be difﬁcult and involves protocol pars ing maintaiing a long set of ﬁlters and rules and for some protocols host or application speciﬁc state to eliminate the complexity of this decision and al low hosts to sleep longer even when using very simple rules for waking protocols could be augmented to carry explicit power related information in their packets an example of such information would be a simple bit indi cating whether a packet can be ignored protocol redesign we believe these principles should be followed when designing power aware protocols consideration when using broadcast and multicast we saw earlier that broadcast and multicast are mainly re sponsible for keeping hosts awake this type of trafﬁc could be substantially reduced by redesigning protocols to use broadcasts sparingly some protocols are partic ularly inefﬁcient in this respect for example all net bios datagrams are always sent over ethernet broadcast frames these frames are received by all hosts on the lan and then discarded by most of them this ranks nbdgm as one of the top offenders yet this could be easily avoided by using unicast transmissions when pos sible another approach is based on the observation that many service discovery protocols have redundant func tionality this redundant functionality could conceivable be replaced by a single service that can be shared by a multiplicity of applications synchronization of periodic traffic one way to in crease the number of long periods of network quies cence would be to identify protocols that use periodic updates message exchanges and try to synchronize or bulk these exchanges together this would allow ma chines to periodically wake up process all notiﬁcations and request and resume sleep complementing soft state many protocols e g ssdp netbios etc maintain and update state using peri odic broadcast notiﬁcations for such protocols and for similar applicatios it would be essential to make them disconnection tolerant by providing complemen tary state query mechanisms that could be used quickly build up to date copies of the soft state upon waking this would enable ignoring any soft state notiﬁcations today such query mechanisms exist only for some of these protocols and they are often inefﬁcient hardware redesign a general goal of energy saving mechanisms especially hardware designs is to lead the industry towards energy proportional computing if energy consumption of a machine would accurately reﬂect its level of utilization the energy would be zero when idle sleep states are a step in this direction p states low power active opera tion are another related to this it would be very desir able to expose power saving states s states that feature better transition times even if they offer smaller savings given the small inter packet gaps these states will come in handier than the deep sleep ones related work the notion that internetworked systems waste energy due to idle periods has been frequently reiterated network presence proxying for the purpose of saving energy in end devices was ﬁrst proposed over ten years ago by christensen et al in follow up work the authors quantify the potential savings using trafﬁc traces from a single dormitory ac cess point and in examine the trafﬁc received at a single idle machine to identify dominant protocols and discuss whether these can be safely ignored our work draws inspiration from this early work extending it with a large scale and more in depth evaluation of idle time trafﬁc in enterprise and home environments a more re cent proposal postulates the notion of selective con nectivity whereby a host can dictate or manage its net work connectivity going to sleep when it does not want to respond to trafﬁc there is an extensive literature on energy saving tech niques for individual pc platforms broadly these aim for reduced power draws at the hardware level and faster transition times at the system level these offer a com plementary approach to reducing the power draw of idle machines if and when these techniques lead us to perfectly energy proportional computers the idle time consumption will be less problematic and proxying will fade in importance so far however achieving such en ergy proportionality has proved challenging in parallel work the authors build a prototype proxy supporting bit torrent and im as example applica tions our work considers a broader proxy design space evaluating the tradeoffs between design options and the resultant energy savings informed by detailed analysis of network trafﬁc in relation to our design space their proxy supports bt and im using application stubs conclusions in general the question of how a proxy should handle the user idle time trafﬁc presents a complex tradeoff be tween balancing the complexity of the proxy the amount of energy saved and the sophistication of idle time func tionality through the use of an unusual dataset collected directly on endhosts we explored the potential savings requirements and effectiveness of technologies that aim to put endhost machines to sleep when users are idle for the ﬁrst time here we dissect the different categories of trafﬁc that are present during idle times and quan tify which of them have trafﬁc arrival patterns that pre vent periods of deep sleep we see that broadcast and multicast trafﬁc constitute a substantial amount of the background chatter due to service discovery and routing protocols our data also revealed a signiﬁcant amount of outgoing connections generated in part by enterprise ap plications we tried to identify which trafﬁc can be ig nored and found that most of the broadcast and multicast trafﬁc as well as roughly of outgoing connections appears safely ignorable handling unicast trafﬁc is more involved because it harder to infer the intent of such traf ﬁc and often needs some state information to be main tained on the proxy after having studied our trafﬁc and the sleep poten tial those patterns contain we discuss the design space for proxies and evaluate the savings offered by sam ple proxy designs these cases reveal the tradeoffs be tween design complexity available functionality and en ergy savings and discuss the appropriateness of vari ous design points in different use environments such as home and ofﬁce finally we present a general and ﬂexible strawman proxy architecture and we build an extensible click based proxy that exempliﬁes one way in which this ar chitecture can be implemented a content aware block placement algorithm for reducing pram storage bit writes brian wongchaowart marian k iskander sangyeun cho march motivation suppose that you had a block storage device with the following characteristics there are many free blocks that can be overwritten the cost of writing a new block depends on the choice of which free block is overwritten how can you minimize the cost of writing a sequence of new blocks to such a device offline problem the offline version of the problem can easily be reduced to maximum weight bipartite matching we need to match new blocks to be written with free blocks to overwrite the edge between a new block x and a free block a has cost w d x a where d x a is the cost of overwriting a with x hamming distance or flip n write cost and w is a constant chosen so that w d x a is always positive efficient polynomial time algorithms for maximum weight bipartite matching are known for a table of results see piotr sankowski maximum weight bipartite matching in matrix multiplication time theoretical computer science no online problem in the online version of the problem new blocks to be written arrive one at a time the natural greedy algorithm is to write a new block to the free location with the least cost this greedy algorithm is not optimal however everything on competitiveness has been removed from this version of my slides implementing the greedy algorithm given a set of free blocks and a new block to be written how can one quickly find the free block closest to the new block the one that costs the least to overwrite this is the well studied nearest neighbor search problem given a set s of points find the point in s that is closest to a specified point not necessarily in s since the points that we are interested in are binary vectors manhattan distance here equivalent to hamming distance is the right distance metric in the case of flip n write one can search for the nearest neighbor of each of the two possible representations of a new block elias if we want to locate a point close to a given point in constant time we can try using a hash function specifically our goal is to partition the space of possible bit strings points into regions such that two points that lie in the same region are close to each other the hash value of a point is an identifier for the region in which it lies an ideal partitioning is a set of spheres of radius d that completely fill the space which guarantees that points that lie in the same sphere will be within distance of one another l rivest on the optimality of elias algorithm for performing best match searches information processing 681 elias observed that given a perfect error correcting code with length n and minimum distance between codewords the codewords are precisely the centers of spheres of radius d that completely fill the space of possible binary vectors of length n finding the unique codeword within hamming distance d of an arbitrary point is just the decoding problem for an error correcting code that can correct up to d errors given a new block to be written the sphere in which it lies and possibly neighboring spheres can be searched until a free block is found the major problem with this scheme is that it is difficult to decode a binary vector of bytes efficiently block signature computation algorithm in my paper divide a block into equal size sets of bits and use a vector containing the approximate count of bits in each set as the block signature hash value problems blocks with the same signature may not be similar similar blocks may not have the same signature nonuniform distribution of signature values even for uniform random data figure distribution of signature values for uniformly distributed random data figure distribution of signature values for the write request data blocks of the jpeg trace figure distribution of signature values for the write request data blocks of the dng trace figure distribution of signature values for the write request data blocks of the kernelbuild trace figure distribution of signature values for the write request data blocks of the swsusp trace figure distribution of signature values for the write request data blocks of the bt trace figure distribution of signature values for the write request data blocks of the cg trace figure distribution of signature values for the write request data blocks of the ft trace figure distribution of signature values for the write request data blocks of the mg trace sets block bits set total bits search distance limit 79 table percentage of the random trace requiring a bit write sets block bits set total bits search distance limit 04 91 table percentage of the permutation trace requiring a bit write sets block bits set total bits search distance limit 48 48 48 48 table percentage of the jpeg trace requiring a bit write sets block bits set total bits search distance limit 54 99 table percentage of the dng trace requiring a bit write sets block bits set total bits search distance limit 68 32 32 32 32 32 32 table percentage of the kernelbuild trace requiring a bit write sets block bits set total bits search distance limit 54 32 32 54 32 32 32 32 32 32 91 table percentage of the swsusp trace requiring a bit write trace initial pram contents zeros bt cg ft mg bt 49 49 cg 49 49 49 ft 48 49 48 49 mg 48 49 48 93 table percentage of the nas snapshot traces requiring a bit write when dcw is used with random block placement trace initial pram contents zeros bt cg ft mg bt 89 48 cg 32 96 ft 43 56 mg 41 table percentage of the nas snapshot traces requiring a bit write when signature based block placement is used with sets per block bits per set and a search distance limit of trace initial pram contents zeros bt cg ft mg bt 91 83 cg 67 ft 43 78 mg 04 table percentage of the nas snapshot traces requiring a bit write when dcw is used with a manual block placement strategy summary the number of bit programming operations needed to store a new data block in a pram storage device depends on the current contents of the location at which the block is written we proposed a signature based block placement algorithm for reducing the number of bit writes required to store a sequence of new data blocks which saves energy with the right parameter settings our block placement algorithm was able to reduce the number of bit writes needed to as low as of the number needed when dcw data comparison write alone is used this figure was achieved without reading and comparing multiple free blocks temperature induced reliability issues are among the major chal lenges for multicore architectures thermal hot spots and ther mal cycles combine to degrade reliability this research presents new reliability aware job scheduling and power management ap proaches for chip multiprocessors accurate evaluation of these policies requires a novel simulation framework that can capture architecture level effects over tens of seconds or longer while also capturing thermal interactions among cores resulting from dynamic scheduling policies using this framework and a set of new thermal management policies this work shows that techniques that offer similar performance energy and even peak temperature can differ significantly in their effects on the expected processor lifetime categories and subject descriptors c performance of systems modeling techniques general terms reliability introduction the microprocessor industry has moved to chip multiprocessing to enable the scaling of performance beyond the limits of unipro cessor execution as the chip area shrinks the power density grows for new process technologies causing higher temperatures there fore our success at finding ways to profitably use the available tran sistors has a cost as we are now faced with significant challenges in managing the power and thermal effects on these chips high temperatures increase the cost of cooling degrade reliability and reduce performance a number of mechanisms for thermal control are currently avail able for multicore processors including job scheduling job mi gration dynamic voltage and frequency scaling etc this paper presents a framework for evaluating the effectiveness of these tech niques in various combinations and presents effective new policies for managing thermal effects the primary goal of managing temperature is to prevent pro cessor failure this research is focused on hard failures which permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee sigmetrics performance june seattle wa usa copyright acm cause permanent damage to the underlying circuit and the phys ical and electrical phenomena that induce them silicon devices have a number of failure modes that are impacted by temperature and in some cases these modes are at odds thermal management techniques that reduce the rate of one failure mode may exacerbate another in fact we show that techniques that are nearly identical in performance power and even peak temperature can differ by a factor of two in expected processor lifetime therefore it is crit ical that we have a model of power temperature and particularly reliability that incorporates all critical failure modes this paper in troduces such an integrated modeling framework shows that some policies have unintended consequences when all sources of failure are considered and proposes new policies that provide significant gains in processor lifetime with little loss in performance most power and thermal management techniques have focused primarily on controlling peak temperature although several types of failures clearly scale with the peak temperature that factor alone does not accurately model all types of failures other failures such as cracks and fatigue failures are created not by sustained high temperatures but rather by the repeated heating and cooling of sec tions of the processor this phenomenon is referred to as thermal cycling the particular failures that our infrastructure models in clude electromigration time dependent dielectric breakdown and the thermal cycling induced errors mentioned above failing to in clude thermal cycling in the failure model can lead to misleading results and highly suboptimal temperature mitigation strategies modeling thermal cycles is difficult the primary challenge is the need to accurately model these systems over the timescales which thermal cycles occur this far exceeds the ability of cur rent processor modeling techniques which typically simulate sys tem behavior at instruction or cycle level therefore we intro duce new performance modeling mechanisms integrated with our power thermal and reliability models that allows accurate model ing of execution behavior over tens or hundreds of seconds being able to capture all the thermal failure effects is critical to an accu rate understanding of processor lifetime we show for example that some proposed mechanisms that appear to improve reliability if thermal cycling effects are ignored actually have the opposite ef fect when thermal cycling is taken into account in this work we define several new scheduling and power man agement policies this research shows that the most critical factors for increasing processor lifetime with acceptable performance are the asymmetric thermal characteristics of the cores cores in the center having very different properties than those on the edges etc the frequency of migration which can both inhibit sleep states and cause thermal cycling our most effective policy that em ploys voltage frequency scaling as well as our best one that does not both account for the location asymmetry and reduce the num ber of thread movements we present new scheduling policies that can decrease the failure rate by a factor of two over naive manage ment with a performance cost of less than this paper is organized as follows section discusses recent work in thermal and reliability management section describes the integrated performance power thermal and reliability sim ulation framework we provide the details of the experimental methodology in section and explain the thermal management techniques in section a thorough evaluation of the techniques is presented in section and we conclude in section background and related work in this section we provide an overview of previously proposed dynamic thermal management dtm techniques and also sim ulation methodologies for modeling performance power and re liability many of the dtm techniques are reactive in nature depending on sensors to indicate temperatures beyond assigned thresholds and adapting the processor to either reduce or migrate activity to bring the temperature down brooks and martonosi introduced the concept of dynamic thermal management in reaction to thermal measurements some thermal management techniques stall execution or migrate compu tation to other units to control temperature a well known example of such techniques is clock gating which freezes all dynamic op erations until the thermal emergency is over causing typically sig nificant performance cost clock gating is implemented in intel pentium donald et al propose a dynamic thermal management tech nique for simultaneous multithreading smt architectures their technique selectively manages the execution of integer or floating point intensive threads to prevent hot spots in the register files to identify integer or floating point intensive threads hardware event counters are sampled during execution in activity migration the heat is spread by moving computation to a different location on the die migration can happen at multiple levels from one core to another or within a core existing redundancy in a superscalar pipeline can be utilized for controlling temperature in this technique the power density is controlled by balancing the utilization of issue queues regis ter files and alus fetch gating alternates between fetching and stalling fetch in order to reduce the activity and power density in the pipeline skadron et al introduce a feedback control loop to control the duty cycle for fetch gating another class of thermal management techniques use dynamic voltage and frequency scaling dvfs where the system is able to alter the processor frequency and supply voltage dynamically to respond to thermal emergencies dvfs can use different num ber of steps for the global voltage and frequency settings ranging from two in intel speedstep technology to for the intel xscale in hybrid thermal management for mild levels of thermal stress fetch gating where we stall fetch but allow other stages of the pipeline to proceed with previously fetched instructions is used as the response mechanism when the overhead of the fetch gat ing increases and instruction level parallelism ilp cannot suffi ciently hide the ill performance effect dtm switches to dvfs another hybrid dtm technique minimizes the performance im pact by proactive use of software techniques like thermal aware process scheduling combined with reactive use of hardware tech niques such as clock gating donald et al evaluate various combinations of dvfs clock gating i e stop go and migration for managing the temperature of multicore processors they show that distributed dvfs combined with thread migration pro vides the best performance among different alternatives prior work has also investigated low overhead temperature aware task scheduling at the operating system level for multiprocessor system on chips mpsoc and the authors proposed an adap tive probabilistic policy addressing both temperature variations and hot spots the adaptive scheduling technique is combined with thread migration or dvfs to further improve thermal behavior murali et al propose a technique that assigns frequencies to different cores in an mpsoc to guarantee meeting the thermal con straints in the offline phase the set of feasible frequencies for different temperature and workload constraints are calculated by solving convex optimization models at run time the management policy selects the appropriate frequency values that meet the cur rent workloads and operating conditions in heat and run the authors propose a dtm solution for chip multiprocessors with smt cores smt thread assignment is used to maximize processor resource utilization by co scheduling threads that use complementary resources before cooling is neces sary in this way the cost of thread migration is reduced few papers in the thermal management literature have taken re liability explicitly into account reliability management has been mostly addressed previously as a way of optimizing the policies or architecture at design time the reliability aware microprocessor ramp provides a reliability model at the architecture level for temperature related intrinsic hard failures it analyzes the ef fects of application behavior on reliability and optimizes the archi tectural configuration and the voltage frequency setting statically at design time to meet the reliability target previous work also shows that aggressive power management can adversely affect re liability due to fast thermal cycles and optimization methods that consider reliability constraints can provide energy savings while improving the mpsoc lifetime to the best of our knowledge a simulation framework to evalu ate the reliability impact of dynamic management policies in a fast and accurate way has not been introduced previously the sim point tool also addresses the problem of long simulation times but it provides clustering analysis to identify a few representative points that can be simulated to predict the performance of the en tire application instead we want to capture the entire behavior rather than summarize however we use simpoint phase iden tification mechanism to capture a complete phase trace as part of our simulation process biesbrouck et al use individual pro gram phase information a complete phase trace not unlike ours to guide multithreaded simulation this is accomplished by creating a co phase matrix which represents the per thread performance for each potential combination of the single threaded phase behav iors that occur when multiple programs are run together although ramp also integrates an architecture level performance sim ulator with a power model and a thermal simulator it does not in clude the phase based approach we introduced using our frame work we are able to simulate much longer periods of real life exe cution in reasonable simulation time a novel framework for multi core reliability modeling this area of research presents new methodological challenges that require tools and solutions radically different than traditional architectural investigation this section describes the entire simu lation infrastructure but with a focus on the two most novel aspects of the framework which are the long time frame performance mod eling and the integrated reliability model overview for a study such as this one it is critical that we have a fully figure design flow integrated performance power and thermal model of the entire chip multiprocessor this is because we are modeling interactive scheduling techniques that observe the temperature and possibly power characteristics of the processor and make scheduling deci sions accordingly for example it is impossible to completely de couple the performance and the thermal models however full in teractive architecture level simulation is also not possible as just a single simulation at these time frames corresponding to several minutes of real execution time could require months to complete our simulation framework is shown in figure the perfor mance modeling front end combines a full program phase profile combined with detailed architecture level simulation of every dis tinct program phase at all possible frequency settings including both performance and power characteristics this characteriza tion all goes into a database that can be queried as the full cmp simulation progresses in this way we can model the effects of changing frequency stopping or migrating jobs etc without fur ther architecture level simulation after scheduling decisions are made and the resulting performance and power data produced we can model time varying temperature effects across the entire chip the temperature curves are then fed into the reliability models pro ducing the expected failure rates long term performance modeling to accurately model the reliability of the system including ther mal cycling effects we need to capture temporal thermal behavior over time periods orders of magnitude longer than typically mod eled in architectural simulation at the same time we would like to capture various types of effects that the architectural simula tion provides e g workload dependent utilization of specific ar chitectural structures and their impact on power and temperature the time varying behavior of individual applications etc this re quires the development of new simulation tools and methodologies not currently available we initially use simpoint to capture the phases of each application but instead of capturing one or a few representative phases we use it to capture a complete phase profile of each ap plication beginning to end then using the performance sim ulator integrated with the wattch power modeling tool and utilizing a finite number of simulation samples for each phase we can reconstruct the power and execution properties of the complete program in fact we do this for all voltage and frequency settings available so that we can reconstruct the complete program even in the face of an arbitrary number of voltage frequency changes we capture these program traces in a database which can be queried by the schedule manager at distinct intervals given a program start point an interval length in cycles and a frequency setting the query tool returns the average instructions per second ipc and power levels across the interval and the point in execu tion the program reaches at the end of the interval thus at runtime the scheduling manager can make decisions about thread migra tion thread stoppage or voltage frequency changes and query the database to model the precise effects this framework relies on two simplifying assumptions that are critical to making this problem tractable the first is that the time constants over which temperature varies do not require us to fully capture cycle by cycle variances in the temperature portion of the model the instruction level variations are captured in the perfor mance model but only summarized in the latter stages this allows us to replace the cycle by cycle data with a stair step graph pre senting performance and power behavior as constant at the average values over individual intervals this way we capture the program behavior with little loss of accuracy the second assumption is that the behavior of individual threads is separable this is accurate because we model systems with pri vate caches which is a likely architectural scenario in future systems at cores and above the interconnect cost of a shared cache would be extremely high this assumption has been used and demonstrated to be accurate even on research that does not require this type of long simulation even with the small core counts in current multicores the amd dual core and quad core opteron the ibm and the coming intel nehalem pro cessor all have private and caches for shared caches interactions between threads will be higher and system level ac curacy will be reduced however recent research on multicore caching has focused on reducing those interactions and in the extreme the proposed techniques could be configured to make the shared caches essentially act as private caches thus even in that scenario we can represent a reasonable system accurately this as sumption makes it difficult to model parallel applications with any significant communication between threads if that communication impacts the runtime characteristics of the application phase modeling we used simpoint to identify the various phases within the applications and to characterize complete program execution a program execution is divided into intervals of million instruc tions once we assign each interval to a representative phase we represent a program execution by a phase id trace thus at any instruction during a program execution we use this file to determine the current phase and to identify points of transition be tween phases by running simulations at each phase point in and compos ing performance power statistics with the phase id trace we cre ate both a power and a performance trace the scheduling manager then accesses these traces via the query tool power modeling and management power modeling requires coupling the execution traces obtained from with a tool that computes the power consumption for each functional unit this coupling converts the performance parame ters e g cache accesses branch predictions etc into estimates for transistor switching and then the power model utilizes these estimates for calculating the instantaneous power values transistors consume power when they switch output values but they also leak power even when they do not switch the former is referred to as dynamic power and historically has been the dom inant factor however as technology shrinks leakage power be comes increasingly important we utilized wattch for the dy namic power modeling of cores in our framework we integrated wattch with to provide dynamic and cycle accurate power mea surements for each application to model power dissipation of caches we used cacti an integrated memory performance area leakage and dynamic power model and obtained the typi cal power consumption of a memory block with the given size and properties and then used these values throughout the simulation we developed a power model for by scaling the parameters within wattch to match published power values for technol ogy the variation in dynamic power range we observed matches the power distribution on a similar core on which the major ity of applications had less than power dissipation difference from the other applications among the applications sampled in that distribution were the spec suite which we use in this study we compute the leakage power of cpu cores based on struc ture areas temperature and supply voltage for the process technology we assume a leakage power density of at to account for the temperature and voltage we used the second order polynomial model proposed by su et al this model computes the change in leakage power for the given differ ential temperature and voltage values we determined the coeffi cients in the polynomial model empirically to match the normalized leakage values in the paper this model is found to match closely with measurements and we found the leakage values produced were in line with expected values i e of the total power consumption based on the technology one of the techniques we investigate to manage power is dy namic power management dpm dpm puts cores in sleep state to save energy we implemented a fixed timeout policy which is one of the commonly used dpm policies for each core the pol icy waits for a timeout period when the core is idle and then turns off the core this is to ensure that we do not turn off cores for very short idle times where turning off the core would not amortize the cost of transitioning to and from the sleep state the time period to amortize the cost of going to sleep is called the breakeven time tbe we assume a sleep state power value of and based on the active and idle power dissipation values we computed the tbe to be around a simple and effective way to set the timeout period is ttimeout tbe thread management and thermal mod eling we implemented a scheduling manager which enables the simu lation of a large array of thread management policies the mech anisms available for managing temperature include adjusting the frequency voltage of a core dvfs putting an idle core into a low power sleep mode dpm migrating computation off of a hot core and policies that stop activity on a hot core i e clock or fetch gating we present the specific policies we model in section in each policy the scheduling manager makes a set of decisions after each scheduling interval and it may incorporate performance and thermal information from the prior interval after making those decisions for each thread and core the scheduling manager then queries the performance database to obtain the power and perfor mance behavior of each core over the next interval our simulation sampling intervals ms are shorter than a scheduling interval so there would be multiple exchanges with the performance database before another scheduling decision is made since the scheduling manager keeps track of performance and power information it also has the responsibility of modeling com plex phenomena such as the delay from thread migrations the model simulates the effects on power and performance for the fol lowing phenomena thread migration dvfs starting a new appli cation on a core core sleep and core wakeup the assumptions we made for several of the delays modeled are presented in table but one of the more complex phenomena deserves special attention we modeled two aspects of the cost of thread migration among cores we measured the software overhead in full system mode as the time for linux to migrate a thread from one core to an other idle core and to start execution this thread migration takes table delay and power model assumptions table hotspot parameters less than µs we also attributed architecture overhead to cold start effects in the branch predictor caches tlbs etc we mea sured cold start effects by inducing many random migrations for each benchmark and computing the average loss in performance the average loss was µs but varied wildly by benchmark i e from to µs note that cold start effects dominate the migration penalty to address the highly variable overhead we modeled a distinct migration penalty for each benchmark automated thermal modeling requires power traces for each unit as input in addition to the chip and package characteristics such as die thickness heat sink convection properties etc therefore we feed the detailed power trace derived by the combination of the scheduling manager and the performance power database into the thermal model we modified hotspot version block model settings to model the thermal characteristics of the core die we used the steady state temperature of each unit as the ini tial temperature values we summarize the hotspot parameters in table we calculated the die characteristics based on the trends reported for process technology the described methodology allows us to do full program simu lation with simple lookups of sampled simulation data this sac rifices some accuracy however the rate at which temperature changes typically dwarfs the time of even complete phases so we would expect this technique to actually sacrifice little accuracy we validated our methodology by comparing the results with di rect wattch power output for each phase simulation point of each spec benchmark we ran and wattch for of sim ulated execution and gathered power statistics every we compared the power statistics of wattch and our framework and we found that our phase based approach has error over all table shows the detailed results for the benchmarks with input set program had the largest average error of the low error margin in our power computation methodology translates to even lower error in temperature computation because of the thermal time constants to verify the accuracy of our method ology in terms of the temperature response we experimented with as it has the highest power error margin figure shows one particular worst case data point the temperature trace for a core running and then going to sleep state on a system running threads the wattch thermal trace corresponds to the detailed power trace sampled at and the phase trace is the thermal output of running the same workload and using our power computation methodology we observe that the trace gener ated with our methodology closely matches the trace sampled at a similar to the em failure rate equation we use to repre sent the first half of the equation both em and tddb failure rates are exponentially dependent on temperature λt ddb e ea kt e ea kt overall table power estimation error of our front end tool com pared with respect to wattch thermal cycling tc is caused by the large difference in ther mal expansion coefficients of metallic and dielectric materials and leads to cracks and other permanent failures the thermal cycling effect is modeled by the coffin mason equation slow ther mal cycles happen because of low frequency power changes such as power on off cycles fast cycles occur due to events such as power management decisions although lower frequency cycles have generally received more attention recent work shows that thermal cycles due to power or workload variations can also de grade reliability the failure rate due to thermal cycling is formulated as in equation time ms wattch phase λ t to q f in this equation t is the temperature cycling range the elas tic portion of the thermal cycle is shown as to elastic thermal figure comparison of temperature responses for us ing two simulation methodologies higher granularity as is one of the most power variant ap plications the rest of the benchmarks demonstrate even less differ ence because thermal cycling effects are insignificant unless the temperature variations are more than a few degrees these results are more than accurate enough to capture both temperature induced and cycle induced effects once we generate a full thermal trace we use this trace as input to our reliability model described in the next section reliability modeling our work targets temperature induced reliability problems our simulation and modeling framework allows us to evaluate schedul ing policies based on their success in reducing the failure rate due to thermal hot spots and thermal cycles achieving a lower failure rate increases the mean time to failure which is the expected lifetime of the circuit the most commonly studied temperature induced intrinsic hard failure mechanisms are electromigration time depen dent dielectric breakdown and thermal cycling electromigration em occurs in interconnects as a result of the momentum transfer from electrons to ions that construct the inter connect lattice and leads to hard failures such as opens and shorts in metal lines the em failure rate λem based on black model is given in equation in the equation ea is the activation energy k is the boltzmann constant t is the temperature j and jcrit are the current density and the threshold current density respectively and is a material dependent constant we represent the first half stress refers to reversible deformation occurring during a cycle and to should be subtracted from the total strain range typically to t so the to component can be dropped from the equation is a material dependent constant q is the coffin manson exponent and f is the frequency of thermal cycles note that the coffin manson equation computes the number of cy cles to failure therefore the mttf in years is the number of cycles multiplied by the period of the cycles computing the frequency of cycles is not straightforward in a simulation of an irregular dynamic system to resolve this prob lem we observed the recent temperature history on each core to compute t and f we set the initial length of the history win dow to seconds and adjusted the length dynamically depending on how many cycles were observed for example if no tempera ture cycles were observed in the last interval we incremented the history window length to capture slower cycles t is the temper ature differential we observed in the last interval we set a higher band of and a lower band of of the temperature range recorded in the last interval and counted the number of times the temperature exceeded the higher band or went below the lower band and used that to calculate the number of cycles in this pe riod in this way we could account for the contribution of cycles with varying temperature differentials and varying periods to combine the failure rates we used the sum of failure rates model as in ramp this model assumes that all the individ ual failure rates are independent mean time to failure mttf is λ for constant failure rates therefore we averaged the failure rate observed throughout the simulation and computed the corre sponding average mttf the average mttf value reported for technology is years 32 of the equation with the term which can be considered as a for moderate temperatures at technology srinivasan et constant an average technology circuit dependent value al demonstrate that the contribution of electromigration di electric breakdown and cycling to the overall failure rate are sim λ a j j n ea kt ea kt ilar to each other this allows us to calibrate the constants in each em crit e λem e failure equation t ddb and to give a system mttf time dependent dielectric breakdown tddb is a wear out mechanism of the gate dielectric and failure occurs when a conduc tive path is formed in the dielectric tddb is caused by the electric field and temperature and the failure rate is defined in equation of years at nominal temperature we used the same constants all throughout the experiments which means that the relative im pact of different failure mechanisms might change depending on the conditions for example if the temperature is high then the effect of em or tddb is higher than tc core core core core l212 l214 core core core core l28 l210 core core core core l24 l26 core core core core l20 l22 figure floorplan of the core cpu table architectural parameters we also examined thermal gradients which refer to the temper ature differences among different locations on the die however we do not explicitly include the effects of gradients in our over all reliability model this is because although thermal gradients can induce hard errors their primary effect is on device latencies which are then manifested as an increase in timing errors rather than hard failures methodology this section describes other details of our simulation infrastruc ture that impact the results shown in the following sections these in general are details that are relatively independent of our frame work described in section and easily changed such as the pro cessor core model the workload etc the simulator out of order execution model is based on simplescalar and provides a detailed model of an alpha processor anticipating continued scaling of core counts the cpu we model is a core multiprocessor manufactured at the floorplan for this cpu is provided in figure each core has out of order issue a private data cache instruction cache cache and memory channels each core possesses three volt age and frequency settings for dynamic voltage frequency scaling at 187v at and at which represent dvfs settings of original step and step respectively the architectural parameters of each core are depicted in table creating representative workloads is a challenge in a core en vironment to assist this process we classify all bench marks in terms of their variability and memory boundedness dis cussed below the distinction between cpu bound and memory bound applications is particularly important in this study because it impacts how performance scales as the frequency changes we model both homogeneous and heterogeneous workloads in terms of the applications cpu or memory boundedness as our exe cution model does not extend to parallel programs the homoge neous workloads stand in for both homogeneous server type work loads and parallel applications with few stalls for communication however our homogeneous and heterogeneous multiprogrammed workloads best represent a server environment where the average lifetime of the processor can significantly affect overall costs we use the ratio of memory bus transactions to instructions as a metric to classify applications as memory or cpu bound as sug gested by wu et al we classify applications along several other dimensions by constructing our workloads from applica tions with different phase variability power savings potential and cpu memory boundedness we seek to represent a wide range of real world workloads table describes each workload we model workloads with threads our cmp architecture is constructed to not have thermal issues when lightly loaded which is the expected behavior for the next few processor generations we construct both homo geneous and heterogeneous workloads and cpu bound memory bound and mixed workloads the mixed workloads contain appli cations from both extremes as well as some in the middle of our categorization in the time frames we model several of the appli cations complete execution in those cases we continually restart the application at the beginning to get consistent behavior across the experiment a common performance metric on multicore platforms is a raw count of ipc however this metric gives undeserved bias towards high ipc threads as performance may be increased by running more cpu bound threads to circumvent this difficulty we used the fair speedup metric fs fs is computed by finding the har monic mean of each thread speed up over a baseline policy of running the thread at the highest frequency and voltage although some applications complete multiple times during our simulations we compute fs in such a way that the overall contri bution of each application is the same reliability aware scheduling the simulation and modeling infrastructure described allows us to design and evaluate several job allocation and thermal manage ment strategies we divide these techniques into three categories those that change what is running on a core via gating or migra tion those that continue to execute the same thread but change speed via dvfs and hybrids that combine the two types each of these methods can be integrated with dynamic power management dpm as well dpm turns off cores after they have been idle for a given timeout period the scheduling and thermal management policies evaluate the system characteristics at every scheduling period and make a decision accordingly in all cases the scheduling tick is set to every the threshold tempera ture for all the temperature triggered policies is c the default policy keeps the initial assignment of jobs to cores fixed and no workload migration or voltage frequency scaling occurs on the fly migration and gating scheduling policies these techniques attempt to move computation off of hot cores either via migration or stalled execution as a response to a thermal event high temperature or as a matter of policy runs each core at the default highest frequency and voltage setting until a core reaches the thermal threshold at this point the core is stalled and the clock is gated to reduce power wkload name description homogeneous cpu bound cores utilized benchmarks sixtrack hom_16_mem homogeneous mem bound mcf het_16_cpu heterogeneous cpu bound mesa crafty sixtrack heterogeneous mem bound mcf equake swim heterogeneous mix mcf mesa sixtrack equake swim applu twolf crafty apsi lucas heterogeneous cpu bound mesa crafty vortex1 sixtrack heterogeneous cpu bound mesa crafty vortex1 sixtrack heterogeneous mix mcf mesa sixtrack swim crafty apsi lucas heterogeneous mix mcf mesa sixtrack equake eon_rushmeier swim twolf crafty apsi lucas table workload characteristics execution characteristics of the threads in determining core tem perature thus those techniques ended up constantly moving jobs between hot and cold cores figure thread assignment strategy for balance location consumption if the core temperature goes below the temperature threshold in the next sampling interval execution continues we assume that each core can be clock gated individually migration sends jobs that have exceeded a thermal threshold to the coolest core that has not been assigned a new thread during the current scheduling period if the coolest core selected is already running a job we swap the jobs among the hot and cool cores this technique can be thought of as an extension of core hopping or activity migration techniques to the case of many cores and many threads balance assigns jobs with the highest committed ipc during the last interval i e between the last two scheduling ticks to cores that have the lowest temperature this scheduling idea represents a more proactive form of migration in which threads are dynamically assigned to locations before thermal thresholds necessitate action is similar to balance but instead of assigning the threads with the highest committed ipc to the coolest cores it assigns them to cores that are expected to be coolest based on location the cores on the corner locations of the floorplan are expected to be the coolest the remaining cores on the sides are ex pected to be the second coolest and the cores in the center of the floorplan are hottest this is because the temperature of a core is a result not only of activity on that core but also on the activity of its neighbors higher number of active neighbors results in hot ter cores figure shows the strategy we used to assign jobs to to cores where the jobs have decreasing committed ipc ip ip ip whereas the optimal alloca tion of threads to cores might diverge from the allocation shown in the figure depending on the ipc difference among threads this allocation generally results in temperature characteristics close to the best allocation with this scheme and threads for example cores labeled and in this figure would always be idle we also experimented with heuristics that choose a thread next core allocation based on the temperature of the thread current core e g move the thread on the hottest to the coolest core but these heuristics performed poorly in multicore architectures like the one we model location is a more significant factor than the methods with voltage frequency scaling this set of techniques rely exclusively on dynamic voltage and frequency scaling to control thermal dynamics they differ in how and when dvfs is applied dvfs threshold reduces voltage and frequency v f one step at a time when a core temperature exceeds a thresh old after reducing the v f to the step setting if the core is still above the threshold in the next scheduling interval uses the step setting when a core temperature is below the threshold the v f setting is increased again one step at each scheduling interval dvfs location uses a fixed v f setting for each core and there is no dynamic scaling at runtime as the center cores tend to heat up more quickly the four cores in the center of the floorplan have the setting the corner cores are typically the coolest cores hence they use the original v f setting the rest of the cores i e the eight remaining cores on the sides have the setting dvfs performance reduces the voltage and fre quency dynamically on a core depending on the memory bound edness of the current application phase previously it was shown that cpu intensive tasks do not gain much in terms of energy sav ings from running at low frequencies and conversely it is benefi cial to run memory bound tasks at a lower frequency as their performance is much more tolerant of frequency scaling dvfs performance seeks to reduce the overall chip temperature with min imal performance cost by proactively scaling back those applica tions that are least impacted to determine the memory bound phases we use a cycles per instruction cpi based metric µ as defined by dhiman et al it compares the observed cpi with a potential cpi we might have gotten without memory events if the µ is near one the applica tion is cpu bound if it is low the application is memory bound note that µ can also take negative values analysis on our own application set confirms that this metric tracks extremely well with performance degradation in the presence of dvfs if the µ observed in the last interval is less than then we use the setting and we find less than performance loss during those phases if µ we apply the setting which induces less than loss in performance for µ we do not perform any v f scaling when µ if we used the scaling for cpu bound applications the performance loss would be in the range of dvfs behaves exactly like unless a core reaches a thermal threshold if the tem perature exceeds the threshold on a core then the policy activates to reduce the temperature on that core after the core tem perature returns within threshold we switch back to this technique wins if by proactively slowing a thread that is tolerant of frequency changes it can enable a nearby thread that is not so tolerant of frequency change to forego a dvfs slowdown techniques combining workload alloca tion and dvfs in investigating the interaction of scheduling and dvfs policies we employ to represent the scheduling policies it has useful properties in terms of both reliability and performance it does only enough migration to find the best location for each thread then only migrates when application characteristics change dvfs threshold works by initially us ing to assign potentially hotter threads to cooler locations on the die if this technique fails to keep a given core un der the specified threshold the core employs until it is under the thermal threshold uses the location policy to allocate jobs to cores and runs at the same time to decide on the v f settings of the cores dvfs location assigns the location v f settings as in the policy to cores and performs balan for allocating the threads this tends to have the effect of assigning the most memory bound threads in the center zone which runs at the setting uses ipc in assigning threads to locations when combined with dvfs we must account for the v f and its effect on the measured ipc thus if a core is running at a lower v f setting we scale the measured ipc based on the average per formance hit observed at that v f level experimental results in this section we demonstrate that the framework we proposed allows us to evaluate a large set of previously proposed and new scheduling algorithms in terms of performance power tempera ture and processor lifetime reliability we show that having a fully integrated model including a reliability model that accounts for all the major causes of temperature induced hard failures sheds some new light on cmp scheduling section identified a wide assortment of thread management policies in evaluating those policies in various execution scenar ios this section attempts to sort out the key issues facing the de signer of a multicore thread management policy such as how to properly combine scheduling migration policies dvfs policies and dpm policies how to address peak temperature effects without exacerbating thermal cycling whether to use reactive or proactive dvfs policies and how to address thermal asym metries in the chip multiprocessor we group the experiments in four major categories section looks at full core utilization scenarios with a varying number of memory bound and cpu bound threads using the five thread workloads from table for these experiments threads are initially placed on the cores randomly i e with neither a clearly good or bad initial allocation section examines systems that are less than fully utilized with or jobs i e or idle cores sec tion takes a deeper look at the consequences of the initial alloca tion of idle cores finally section investigates how reliability performance and energy vary when the system has dpm capabil ities and which schedulers best complement dpm to achieve the desired trade offs for reliability energy and performance to deliver a fair comparison we present the energy and per formance of the policies in addition to reliability mean time to failure this is in lieu of trying to create a single artificial metric that captures all three such user defined metrics are susceptible to providing results that are specific to the assumptions made while creating the metric or while weighing the individual parameters we normalized all results in the following sections with respect to the default case of no thermal management i e all threads run ning full speed on the initially assigned cores hence the y axes in our mttf performance temperature and energy plots demon strate the normalized values for these parameters this allows us to evaluate each policy on the same scale srinivasan et al 32 reported the average mttf of the spec suite simulated for at of supply voltage as years and our model is calibrated to the same value however if the reliabil ity model was re calibrated to assume a shorter or longer mttf the policies would still display the same trends only the absolute numbers would change depending on process technology baseline mttf and the system being modeled therefore we show results based on the change in mttf values rather than absolute num bers so that the dependence on the absolute calibration is minimal full core utilization sections to examine the case where all cores are ac tively running threads techniques utilizing workload allocation this section analyzes the workload allocation policies ability to improve thermal characteristics the policies that we analyzed in this section include migration balance and balan the results in figure which are the average val ues for all the thread workloads indicate that migration bal ance and have little impact on reliability in this scenario this is because cores are fully utilized and most of our workloads are highly homogeneous in the one heterogeneous workload the effect is still small in that case the balance and policies each improve reliability by with minimal impact less than on perfor mance energy and average temperature thus in the absence of idle cores these policies are less effective than the ones with volt age and frequency scaling which we discuss in section the policy was notably different than the policies dis cussed above as it has the ability to cool a core even in the absence of idle cores improved the mttf by but with a hefty decrease in performance and a increase in energy consumption average temperature of the processor was reduced by the policy is prone to creating large temperature variations due to switching among active and idle states however the frequency of stalling resuming execution was high enough that the temperature variations were of a relatively low magnitude and the reliability of the core was dominated by the thermal hot spots only i e no significant increase in cycling based failures techniques with voltage frequency scaling figure shows the effect of the dvfs policies on reliability when the cores are fully utilized dvfs has a much more signifi cant impact than the workload allocation policies due to its ability to reduce temperature even in the face of full utilization in particular we find several key insights in these results first it is important to always keep an eye on peak temperature by selectively choosing which threads to scale sacrifices very little in performance but does lag a bit behind in mttf in comparison to other dvfs policies this is because it ignores thermal warnings figure comparison of workload allocation techniques reacts upon reaching a threshold as well and as a result loses some performance but it has one of the lowest failure rates second we see significant benefits of proactive techniques over traditional reactive techniques it is interesting to note that the other dvfs policies beat along all axes which is particularly sur prising on the performance front this is surprising because only scales when it has to and the other dvfs poli cies default to upon reaching the threshold temperature the reason that other dvfs policies perform better is that proactively scaling a thread whose performance is tolerant to scaling reduces the temperature in that area and often prevents other neighboring threads from reaching the threshold third we see that it is critical that our thread management policy understands the inherent thermal asymmetry of the multicore sys tem an asymmetry that will exist in all likelihood for any mul ticore greater than four cores the policy that provides the best balance among all three metrics is with a failure rate that is half of the baseline and a minimal performance loss of default to further investigate this point we compared loca with homogeneous proactive scaling all cores at dvfs and all cores at dvfs among these dvfs techniques still demonstrate the best trade off point the dvfs result improved performance over by less than but gave up in processor lifetime the dvfs in creased reliability significantly but more than doubled the perfor mance cost compared to our techniques are easily adapted to other sources of asymmetry such as process variations as long as we can quantify the effects of such variations on the thermal and power properties of each core hybrid techniques we examine the hybrid techniques in this section and show the results in figure when we compare the hybrid policies against the dvfs based policies we see that dvfs based policies are improved little by combining them with job allocation policies again this is due to the limited gains from reorganizing running threads on a fully utilized system impact of partial utilization it is expected that most multicore systems will be utilized less than most of the time this is true especially for the cmps in the server domain to evaluate the impact of scheduling mech anisms on reliability when some cores are idle we used the and thread workloads described in table a cpu bound and a mixed cpu bound memory bound workload for each of the and thread cases the results represent the average of the cpu bound and mixed cases for the and thread experiments at the beginning of each simulation we decided which cores to leave idle figure comparison of dvfs based techniques figure comparison of hybrid techniques by choosing the allocation with the lowest peak temperature once we determined the active cores we performed the initial placement of threads on these cores randomly we first focus on the case with active threads in figure although this utilization is close to the full utilization examples explored in section the impact on the reliability of the various policies changes significantly policies with frequent workload re allocation i e balance mi gration result in poorer reliability with respect to the other poli cies the balance policy assigns jobs to cores based on tempera ture rather than location and often mistakes a core that is cool now for a core that will stay cool in the future migration policies that focus heavily on current temperatures are prone to this type of error the migration result has the same issue policies that migrate more than necessary have two distinct reliability disadvantages over the other techniques first migrating too often will tend to thwart the dpm manager which does not put a core to sleep until it has been idle for awhile this increases the time cores spend running at hot ter temperatures second migration causes thermal cycling this was the dominant cause of the low mttf results as the power variations between idle and active states create cycles of a signifi cant magnitude we examine the effects of migrations in detail in section the policy achieves times improvement in mttf however this comes at the cost of a drastic performance and energy cost although could be utilized effectively as a back up policy for thermal emergencies to guarantee that temperature does not exceed a given peak value it is inefficient if used frequently among the dvfs policies achieves the best perfor mance of less than degradation while results in the longest system life time with a improvement the hybrid policy seems to provide the best trade off point among the policies as it achieves almost the same mttf as with better performance and lower energy consumption the reason the hybrid scheduling policies still pro vide only small gains over dvfs policies alone is that we start the experiments with an optimal placement of idle cores we examine this further in section we expect that as technology scaling continues the bandwidth for performing voltage scaling will decrease due to the leakage power and transistor threshold voltage limitations this situation will require other mechanisms for managing power and tempera ture is our best candidate for workload alloca figure effect of system utilization idle cores figure effect of system utilization idle cores tion as it significantly increases reliability with negligible perfor mance loss figure shows the core utilization results because chip tem peratures are lower overall the magnitude of potential reliability gains is reduced in fact policies that only react to thermal thresh olds see no activity in this scenario e g migration etc and they give the same results as the default policy as the core temperatures do not exceed the threshold policies that proactively look for opportunities can still improve processor lifetime signifi cantly and even provides small gains policies that proactively migrate based on current temperature balance make mistakes and create thermal cycling effect of initial idle core locations in this section we examine each policy ability to adapt to dif ferent initial workload mappings on the processor topology for ex ample what happens when the initial mapping of threads to cores is highly suboptimal this could happen with a topology ignorant scheduler a likely scenario early on or just because of jobs enter ing or leaving the system we examined several ways of performing the initial allocation best possible worst possible and an in order placement of jobs on cores the best case i e the case with the lowest peak temperature for active threads is leaving the center cores idle and for active threads when cores and are idle the worst case oc curs when the corner cores are idle specifically the worst case for a system with active cores is leaving the cores and idle similarly when cores are active leaving two of the corner cores on the opposite sides idle such as cores and represent the worst assignment the in order initial assignment allocates all available threads on the cores starting from core ascending this method initially leaves cores idle when threads are ac tive and cores idle with threads are active the in order method attempts to model a naive scheduler that assigns jobs to cores using a first available strategy we have observed notable differences in reliability between the experiments for example the policy experiences a reduction in mttf in comparison to the best allocation when either the worst or in order idle core locations are used this de crease in reliability is comparable to the default policy re duction in mttf when using the in order and worst case initial assignments on the other hand when was combined with bal we were able to achieve a level of reliability to match that of the optimal initial placement this indicates that one of the major roles of the allocation policy is reassigning thread topologies to assist other policies that optimally set core voltage and frequency thus in a real cmp system it is critical to com bine a conservative migration technique i e one which avoids un necessary migrations and does not create cycling with dvfs tech niques in the absence of an intelligent migration and scheduling policy it is difficult to avoid detrimental configurations over time interactions with power management dynamic power management dpm takes advantage of pro longed core idleness to put the core into a sleep mode in sleep the power consumption of the core is greatly diminished each of the policies presented is compatible with dynamic power manage ment but some are able to use dpm opportunities better taking a closer look at two extremes we first examine two policies mi gration and for the workload with cpu and memory bound threads comparing the thermal traces for migration and figure the migration policy suffers significant thermal cy cle variations for migration we demonstrate the thermal cycles observed on two cores due to frequent re allocation of workloads for the policy we show all the cores thermal traces and observe that each core temperature is stable and lower than the threshold this stability along with a lower peak temperature results in significantly higher reliability turns out to be the best policy when paired with dpm and it pro vides an increase in mttf of over migration while the per formance difference is only so we see that scheduling poli cies which effectively manage thread locations and dpm policies can reduce processor temperatures and improve reliability at the same time dpm can also lead to greater thermal cycling which can counteract some of the mttf gains that result from the lower power levels of sleeping cores the adverse effect of dpm on re liability due to thermal cycles is also emphasized in previous work thus when we include the effects of thermal cycling fail ures we observe that the traditional assumptions for finding op timal strategies are incomplete it would be wise to re design the dpm policies with a reliability perspective despite dpm possible impact on reliability we do see fig ure that even in the face of this cycling phenomena dpm was an overall win for all policies with the exception of balance in this figure we show the average results over heterogeneous cpu bound workloads the reason that balance received no benefit from using dpm is its proactive mechanism that keeps moving hot threads to colder cores the result is that no core is idle long enough to trigger the sleep mode on the other end of the spectrum migration and show gains of and in mttf for the average case respectively the reliability improvement in dvfs based techniques are less prominent and range between mttf increase the policy receives a large benefit in energy from using dpm mechanisms reducing power consumption by in com parison to the no dpm case if confronted with a design choice that figure a cycles caused by the migration policy b stable thermal profile of migrations all corner center side balance migration balance_loc 54 v f setting changes 83 balance_loc 53 balance_loc table number of migrations and v f changes per second figure mttf and energy effects of dpm requires the simplicity of dpm could help regain much of the energy lost from the constant start and stop of individual cores in table we show the number of migrations and number of v f setting changes per second for the policies to provide a more com plete understanding of the runtime behavior the policies that are not listed do not utilize migrations or dvfs the columns marked as all corner center and side refer to the average number across all cores across only the corner cores center cores and side cores respectively the results are with dpm and for the cpu bound heterogeneous workload with threads i e idle cores mi gration has a significantly higher number of thread movements in comparison to other policies almost times more than bal the low migration count of is a result of its ability to match the performance characteristics of applications with the thermal behavior of cores compared to bal only combining with dvfs in creases the frequency of migrations as the temperature profile of the cores vary more when their v f settings are dynamically ad justed among the dvfs policies has the lowest number of changes as it only alters the v f setting of applications tolerant to operating at a slower speed also reduces the fre quency of changes in comparison to as it proactively adjusts the v f setting and triggers the thermal threshold fewer times to better understand the tension between the different failure mechanisms figure presents a breakdown of the contribution of different failure types to reliability this figure demonstrates the normalized average failure rate for our two best and two worst policies i e best worst in terms of their average mttf results the workload for this experiment is the heterogeneous cpu bound workload with threads recall that the failure rate is inversely proportional to mttf this figure shows that balance and mi gration reduce the probability of failures due to electromigration em and dielectric breakdown tddb if we ignored the effect figure contributions of failure mechanisms of thermal cycles we would conclude that reliability had increased however because of the number of thread migrations they create large thermal cycles tc the and the hybrid bal policies on the other hand reduce the failure rates caused by thermal hot spots without introducing a significant amount of thermal cycling failures note that in the default case as there is no workload re allocation temperature is stable and no cycles are observed conclusions this paper analyzes how job scheduling and power management policies affect system lifetime it demonstrates a novel cmp simu lation framework which is able to simulate thermal dynamics over far longer time periods than typical architectural simulators at high accuracy it evaluates a number of techniques in terms of their ef fect on reliability temperature energy and performance the results in this paper provide several key insights that will serve us well in the design of future thermal management policies it is critical to consider thermal cycling effects in addition to peak temperature effects we saw two policies that erroneously appear to increase lifetime when thermal cycling was ignored thermal cycling is not a significant effect in a fully utilized sys tem as the variance in power between running threads was not shown to be sufficiently high to cause harmful effects however when cores are idle it is important that we manage the idle cores in a way that does not exacerbate thermal cycling conservative policies that minimize migration not only reduce thermal cycling but also maximize our ability to exploit sleep states via dpm understanding thermal asymmetries which are either due to the layout of the processor or due to process variation is critical to effective thermal management not understanding thermal variance causes much unnecessary movement because we can not discern between a hot thread and a hot core understanding the thermal variance allows us to employ an asymmetric thermal policy that accounts for and even exploits that asymmetry proactive techniques that apply dvfs to frequency tolerant ap plications can raise the performance of the entire system this is somewhat non intuitive as the frequency tolerant applications are also the coolest applications however by lowering overall temperatures chip wide this allows the hot applications to run longer without triggering thermal events in future work we will be addressing reliability management of multithreaded multicore systems we will seek to provide a com prehensive understanding of how parallel workloads differ from single threaded benchmarks and propose novel management tech niques to address the particular characteristics of such workloads abstract speed scaling is a power management technique that involves dynamically changing the speed of a processor this gives rise to dual objective scheduling problems where the operating system both wants to conserve energy and optimize some quality of service qos measure of the resulting schedule in the most investigated speed scaling problem in the literature the qos constraint is deadline feasibility and the objective is to minimize the energy used the standard assumption is that the power consumption is the speed to some constant power α we give the first non trivial lower bound namely eα α on the competitive ratio for this problem this comes close to the best upper bound which is about we analyze a natural class of algorithms called qoa where at any time the processor works at q times the minimum speed required to ensure feasibility assuming no new jobs arrive for cmos based processors and many other types of devices α that is they satisfy the cube root rule when α we show that qoa is competitive improving upon the previous best guarantee of achieved by the algorithm optimal available oa so when the cube root rule holds our results reduce the range for the optimal competitive ratio from to we also analyze qoa for general α and give almost matching upper and lower bounds introduction current processors produced by intel and amd allow the speed of the processor to be changed dynamically intel speedstep and amd powernow technolo gies allow the windows xp operating system to dynamically change the speed of such a processor to conserve energy in this setting the operating system must not only have a job selection policy to determine which job to run but also a speed scaling policy to determine the speed at which the job will be run all theoretical studies we know of assume a speed to power function p sα where is the the work of h l chan was done when he was a postdoc in university of pittsburgh k pruhs was supported in part by nsf grants cns ccf iis and ccf and an ibm faculty award speed and α is some constant energy consumption is power integrated over time the operating system is faced with a dual objective optimization problem as it both wants to conserve energy and optimize some quality of service qos measure of the resulting schedule the first theoretical study of speed scaling algorithms was in the seminal paper by yao demers and shenker in the problem introduced in the qos objective was deadline feasibility and the objective was to minimize the energy used to date this is the most investigated speed scaling problem in the literature in this problem each job i has a release time ri when it arrives in the system a work requirement wi and a deadline di by which the job must be finished the deadlines might come from the application or might arise from the system imposing a worst case quality of service metric such as maximum response time or maximum slow down it is clear that an optimal job selection policy is earliest deadline first edf thus the remaining issue is to find an online speed scaling policy to minimize energy the story to date yao demers and shenker showed that the optimal offline schedule can be effi ciently computed by a greedy algorithm proposed two natural online speed scaling algorithms average rate avr and optimal available oa conceptually avr is oblivious in that it runs each job in the way that would be optimal if there were no other jobs in the system that is avr runs each job i in parallel with other jobs at the constant speed wi di ri through out interval ri di the algorithm oa maintains the invariant that the speed at each time is optimal given the current state and under the assumption that no more jobs will arrive in the future in particular let w x denote the amount of unfinished work that has deadline within x time units from the cur rent time then the current speed of oa is maxx w x x another online al gorithm bkp is proposed in bkp runs at speed e v t at time t where v t maxt t w t et e t t e t t and w t is the amount of work that has release time at least deadline at most and that has already arrived by time t clearly if w is the total work of jobs that are released after and have deadline before then any algorithm must have an average speed of at least w during thus bkp can be viewed as computing a lower bound on the average speed in an online manner and running at e times that speed table summarizes the previous results the competitive ratio of avr is at most this was first shown in and a simpler amortized local competitiveness analysis was given in the competitive ratio of avr is least δ α where δ is a function of α that approaches zero as α approaches infinity the competitive ratio of oa is exactly αα where the upper bound is proved using an amortized local competitiveness argument thus the competitive ratio of avr is strictly inferior to that of oa the competitive ratio of bkp is at most α α αeα which is about for large α it is better than that of oa only for α on the other hand the lower bounds for previous results algorithm general α α α upper lower upper lower upper lower general α avr 1αα α α δ α 48 oa αα αα bkp α α α α e our contributions algorithm general α α α upper lower upper lower upper lower general eα α qoa eα α α table results on the competitive ratio for energy minimization with deadline fea sibility general algorithms are rather weak somewhat surprisingly the best known lower bound instance is the worst possible instance consisting of two jobs shows a lower bound of on the competitive ratio using a two job instance if one tries to find the worst job instances the calculations get messy quickly the most interesting value of α seems to be three most importantly in current cmos based processors the speed satisfies the well known cube root rule that the speed is approximately the cube root of the power the power is also roughly proportional to the cube of the speed in many common de vices machines such as vehicles automobiles and some types of motors it seems likely that α would be in the range for most conceivable devices the best known guarantee for α in this range is αα achieved by oa which evaluates to for α and for α our motivating goal is to focus on the case that α and to a lesser extent on α and to obtain better algorithms and lower bounds in these cases our contributions we show using an amortized local competitiveness analysis that if q is set to then the competitive ratio of qoa is at most eα this bound is approximately when α and when α using an analysis specialized to the specific cases that α and α we show that qoa is at worst competitive when α and at worst competitive when α our main technical idea is to introduce a new potential function which is quite different from the one used in the analysis of oa in and the potential function used to analyze avr in this is necessary since potential functions similar to those used earlier cannot yield guarantees of the form cα where c is independent of α the potential function we use is more similar to the one used in to analyze a speed scaling algorithm for the different objective of minimizing flow time plus energy however here we will need a different analysis approach the analysis in and almost all of the amortized local competitiveness analyses in the speed scaling literature rely critically on the young inequality however in the current setting young inequality gives a bound that is too weak to be useful when analyzing qoa the key insight that allows us to avoid the use of young inequality was to observe that certain expressions that arise in the analysis are convex which allows us to reduce the analysis of the general case down to just two extreme cases to the best of our knowledge this convexity technique can replace all of the uses of young inequality in the speed scaling literature in all cases the resulting bound that one obtains using this convexity technique is at least as good as the bound that one obtains using young inequality and the resulting proof is simpler and more intuitive in some cases this convexity technique gives a better bound for example if one applies this convexity technique to the analysis of the laps algorithm in one obtains a bound on the competitive ratio of o α whereas using young technique one can only get a bound of o in section we consider lower bounds we give the first non trivial lower bound on the competitive ratio for any algorithm we show that every deter ministic algorithm must have a competitive ratio of at least eα α the base of the exponent e is the best possible since bkp achieves a ratio of about for α this raises the best known lower bound a modest amount from to the instance is identical to the one used in to lower bound the competi tive ratio with respect to the objective of minimizing the maximum speed the innovation required to get a lower bound for energy is to categorize the variety of possible speed scaling policies in such a way that one can effectively reason about them given the general lower bound of eα α and that bkp achieves a ratio with base of exponent e a natural question is whether there is some choice of the parameter q for which the competitive ratio of qoa varies with e as the base of the exponent somewhat surprisingly we show that this is not the case and the base of the exponent cannot be improved beyond in particular we show that the competitive ratio of qoa is at least α we note that this lower bound is quite close to our upper bound for qoa especially as α increases our results are summarized in the last two rows of table in particular we give asymptotically matching upper and lower bounds for qoa and reduce the range for the optimal competitive ratio in the case that the cube root rule holds from to and in the case that α from obtained in to due to the limitation of space some proofs are omitted and will be given in the full paper other related results there are now enough speed scaling papers in the literature that it is not prac tical to survey all such papers here we limit ourselves to those papers most related to the results presented here a naive implementation of yds runs in time o this can be improved to o if the intervals have a tree structure li yao and yao gave an implementation that runs in o log n time for the general case for hard real time jobs with fixed priorities yun and kim showed that it is np hard to compute a minimum energy schedule they also gave a fully polynomial time approximation scheme for the problem kwon and kim gave a polynomial time algorithm for the case of a processor with discrete speeds li and yao gave an algorithm with running time o d n log n where d is the number of speeds a simpler algorithm with this running time can be found in albers mu ller and schmelzer consider the problem of finding energy efficient deadline feasible schedules on multiprocessors showed that the of fline problem is np hard and gave o approximation algorithms also gave online algorithms that are o competitive when job deadlines occur in the same order as their release times chan et al considered the more general and realistic speed scaling setting where there is an upper bound on the maxi mum processor speed they gave an o competitive algorithm based on oa recently bansal chan and pruhs investigated speed scaling for deadline feasibility in devices with a regenerative energy source such as a solar cell formal problem statement a problem instance consists of n jobs job i has a release time ri a deadline di ri and work wi in the online version of the problem the scheduler learns about a job only at its release time at this time the scheduler also learns the exact work requirement and the deadline of the job we assume that time is continuous a schedule specifies for each time a job to be run and a speed at which to run the job the speed is the amount of work performed on the job per unit time a job with work w run at a constant speed thus takes w time to complete more generally the work done on a job during a time period is the integral over that time period of the speed at which the job is run a schedule is feasible if for each job i work at least wi is done on job i during ri di note that the times at which work is performed on job i do not have to be contiguous if the processor is run at speed then the power is p sα for some constant α the energy used during a time period is the integral of the power over that time period our objective is to minimize the total energy used by the schedule an algorithm a is said to be c competitive if for any job sequence the energy usage of a is at most c times that of the optimal schedule upper bound analysis of qoa our goal in this section is to show that qoa is about eα competitive when q α we wish to point out that q α is not necessarily the optimum value of q for general α it is not clear how to obtain the optimum choice of q since it involves solving a system of high degree algebraic inequalities however the lower bound for qoa will imply that the choice q α is close to optimum for the case of α and that of α we can explicitly determine the optimum choice of q which gives better competitive ratios for these cases we use an amortized local competitiveness analysis and use a potential func tion φ t that is a function of time in this setting the value of φ t will be energy and thus the derivative of φ t with respect to time will be power we need that φ is initially and finally zero let sa and so be the current speed of the online algorithm qoa in our case and the optimal algorithm opt respectively then in order to establish that the online algorithm is c competitive it is sufficient to show that the following key equation holds at all times sα d φ c sα the fact that equation establishes c competitiveness follows by integrating this equation over time and from the fact that φ is initially and finally for more information on amortized local competitiveness arguments see before defining the potential function φ that we use we need to introduce some notation we always denote the current time as for any t t let wa t t denote the total amount of work remaining in qoa at with deadline in t t define wo t t similarly for opt recall that qoa runs at speed q maxt wa t t which is q times the speed that oa would run let d t t max wa t t wo t t denote the amount of additional work left under the online algorithm that has deadline in t t we define a sequence of time points iteratively as follows let be the time such that d is maximized if there are several such points we choose the furthest one given ti let ti ti be the furthest point that maximizes d ti ti ti ti we use gi to denote d ti ti ti ti note that gi is a non negative monotonically decreasing sequence we first bound the offline and online speed which will be useful in our analysis lemma i so maxt wo t t ii sa and sa qso we are now ready to define the potential function φ that we use in our analysis of qoa φ β ti ti gα i where β is some constant which will be set to qα α α α we now make some observations about the potential function φ φ is obvi ously zero before any jobs are released and after the last deadline job arrivals do not affect φ since d t t does not change upon a job arrival for any t and t similarly job completions by either qoa or optimal do not change φ since it is a continuous function of the unfinished work and the unfinished work on a job continuously decreases to as it completes finally structural changes in the ti and gi do not change the value of φ in particular if decreases for instance if online is working faster than offline on jobs with deadline in then at some point becomes equal to and the intervals and merge together upon this merge the potential does not change as at this point similarly if offline works too fast the interval tk tk which contains the earliest deadline among the unfinished jobs under offline might split into two critical intervals tk t and t tk but again this change does not affect φ since at the time of splitting the value of g for the newly formed intervals is identical to the value of the interval tk tk thus to complete our analysis we are left to show the following lemma lemma for general α set q α β c α α α α α consider a time t where no jobs are released no jobs are com pleted by qoa or optimal and there are no structural changes to the ti nor gi then equation sα dφ dt c sα holds at time t proof suppose first that wa wo in this case d and is basically infinity note that dφ dt since φ remains zero until wa wo therefore dφ dt c because sa qso and a o c qα α α α qα hence we assume wa wo in the following without loss of generality both opt and qoa schedule jobs according to earliest deadline first and hence qoa is working on a job with deadline at most let t be deadline of the job that opt is working on and let k be such that tk t tk first consider the case that k when both qoa and opt work decreases the quantities gk and gk stay unchanged and gk in creases note that is decreasing and the rate of decrease is the same as the rate that time passes therefore the rate of change of gα is d t t g t t αgα sa d gα αgα α gα for the rate of change of tk tk gα we note that tk tk stays unchanged also the rate of change of d tk tk may be so or depending on whether wa tk tk is greater than wo kk tk therefore d t t g t t αgα tk tk so k k k k k k tk tk αgα αgα thus to show sα dφ dt c sα it suffices to show that a o sα β αgα α gα c sα now consider the case that k note that for i neither gi nor ti ti changes so we need not consider these terms in the potential function the rate of change of gα is d t t g t t αgα t0 sa so d t0 gα t1 t0 αgα α gα which leads to the same inequality as equation hence we will focus on equation and show that it is true for the stated values of q c and β we consider the left hand side of equation as a function of sa while g and so are fixed note that it is a convex function of sa since sa qso it suffices to show that equation holds at the endpoints sa and sa qso if sa the left hand side of equation becomes qαgα βqαgα βαgα β α gα csα qα βαq β α gα βαgα cs taking derivative with respect to so we get that this is maximized at so satisfying α csα βgα and hence β g substituting this for and canceling gα it follows that we need to satisfy the following equation qα βαq β α β α β α if sa qso the left hand side of equation becomes qα g α α α α βqα g gα βαg β α g cs qα g α β qα α gα βα q gα cs setting so x and canceling gα it follows that we need to satisfy qα x α β qα α βα q x cxα we set q α and β c qαηα where η α α with these choices of q β and c αq and to establish equation it is sufficient to show that qα β which is trivially true since η similarly equation is equivalent to x α αηα ηα α x ηα for all x since α it suffices to show that x α αηα ηα to see this note that if we take the derivative of the left side of equation we obtain that the maximum is attained at x such that x α ηα and hence x η for this value of x the left side of equation evaluates to and hence the result follows hence equation is satisfied and the lemma follows now our main theorem follows as a direct consequence theorem qoa is α α α α competitive for general α note that for large values of α this bound on the competitive ratio of qoa is approximately eα fo r α this bound on the competitive ratio of qoa evaluates to 52 which is already better than the best known bound of however for the cases of α and α we can determine the optimum values of q and β to obtain theorems and theorem if q 54 then qoa is competitive for α proof we follow the same proof structure as that for lemma to obtain the inequalities and by putting α it follows that we need to satisfy 2β β x β q x we wrote a program to determine the values of q and β that minimize c the best values we obtained are q 54 β 78 and c 73 it is easy to check that the first inequality is satisfied the left hand side of the second inequality becomes 96x2 65x 73 which can be shown to be negative by differentiation hence and are satisfied and the theorem follows theorem if q and β then qoa is 391 competitive for α lower bounds in this section we show that any algorithm is at least eα competitive note that we assume α is fixed and is known to the algorithm we first give an adversarial strategy for constructing a job instance such that any algorithm uses at least eα times the energy of the optimal energy expenses are becoming an increasingly important fraction of data center operating costs at the same time the energy expense per unit of computation can vary sig nificantly between two different locations in this paper we characterize the variation due to fluctuating electricity prices and argue that existing distributed systems should be able to exploit this variation for significant economic gains electricity prices exhibit both temporal and geographic vari ation due to regional demand differences transmission inef ficiencies and generation diversity starting with historical electricity prices for twenty nine locations in the us and network traffic data collected on akamai cdn we use sim ulation to quantify the possible economic gains for a realistic workload our results imply that existing systems may be able to save millions of dollars a year in electricity costs by being cognizant of locational computation cost differences categories and subject descriptors c computer communication networks distributed systems general terms economics management performance introduction with the rise of internet scale systems and cloud com puting services there is an increasing trend toward massive geographically distributed systems the largest of these are made up of hundreds of thousands of servers and several data centers a large data center may require many megawatts of electricity enough to power thousands of homes millions of dollars must be spent annually on the electric ity needed to power one such system furthermore these already large systems are increasing in size at a rapid clip outpacing data center energy efficiency gains and elec tricity prices are expected to rise permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee sigcomm august barcelona spain copyright acm 594 figure estimated annual electricity costs for large companies servers and infrastructure mwh these are conservative estimates meant to be lower bounds see for derivation details for scale we have included the actual consumption and utility bill for the mit campus including dormitories and labs organizations such as google microsoft amazon ya hoo and many other operators of large networked systems cannot ignore their energy costs a back of the envelope cal culation for google suggests it consumes more than worth of electricity annually figure a modest reduc tion would therefore exceed a million dollars every year we project that even a smaller system like akamai consumes an estimated worth of electricity the conventional approach to reducing energy costs has been to reduce the amount of energy consumed new cooling technologies architectural redesigns dc power multi core servers virtualization and energy aware load balanc ing algorithms have all been proposed as ways to reduce the power demands of data centers that work is complemen tary to ours this paper develops and analyzes a new method to reduce the energy costs of running large internet scale systems it relies on two key observations electricity prices vary in those parts of the u s with wholesale electricity markets prices vary on an hourly basis and are often not well correlated at different lo cations moreover these variations are substantial as much as a factor of from one hour to the next if when computational demand is below peak we can dy namically move demand i e route service requests to places with lower prices we can reduce energy costs large distributed systems already incorporate request routing and replication we observe that most internet scale systems today are geographically distributed with paper covers work done outside akamai and does not rep resent the official views of the company akamai seldom pays directly for electricity it pays for it indirectly as part of co location expenses machines at tens or even hundreds of sites around the world to provide clients good performance and to tolerate faults these systems implement some form of dynamic request routing to map clients to servers and often have mechanisms to replicate the data necessary to process requests at multiple sites we hypothesize that by exploiting these observations large systems can save a significant amount of money using mech anisms for request routing and replication that they already implement to explore this hypothesis we develop a simple cost aware request routing policy that preferentially maps requests to locations where energy is cheaper our main contribution is to identify the relevance of elec tricity price differentials to large distributed systems and to estimate the cost savings that could result in practice if the scheme were deployed problem specification given a large system composed of server clusters spread out geographically we wish to map client requests to clusters such that the total electricity cost in dollars not joules of the system is minimized for sim plicity we assume that the system is fully replicated addi tionally we optimize for cost every hour with no knowledge of the future this rate of change is slow enough to be com patible with existing routing mechanisms but fast enough to respond to electricity market fluctuations finally we in corporate bandwidth and performance goals as constraints existing frameworks already exist to optimize for bandwidth and performance modeling them as constraints makes it possible to add our process to the end of the existing opti mization pipeline note that our analysis is concerned with reducing cost not energy our approach may route client requests to distant locations to take advantage of cheap energy these longer paths may cause overall energy consumption to rise slightly energy elasticity the maximum reduction in cost our approach can achieve hinges on the energy elasticity of the clusters this is the degree to which the energy consumed by a cluster depends on the load placed on it ideally clusters would draw no power in the absence of load in the worst case there would be no difference between the peak power and the idle power of a cluster present state of the art sys tems fall somewhere in the middle with idle power being around of peak a system with inelastic clusters is forced to always consume energy everywhere even in re gions with high energy prices without adequate elasticity we cannot effectively route the system power demand away from high priced areas zero idle power could be achieved by aggressively consol idating turning off under utilized components and always activating only the minimum number of machines needed to handle the offered load at present achieving this without impacting performance is still an open challenge however there is an increasing interest in energy proportional servers and dynamic server provisioning techniques are being ex plored by both academics and industry results to conduct our analysis we use trace driven simulation with real world hourly and daily energy prices obtained from a number of data sources we look at months of hourly electricity prices from us locations our request traces come from the akamai content distribu tion network cdn we obtained days worth of request traffic data five minute load for each server cluster located at a commercial data center in the u s we used these data sets to estimate the performance of our simple cost aware routing scheme under different constraints we show that existing systems can reduce energy costs by at least without any increase in bandwidth costs or sig nificant reduction in client performance assuming a google like energy elasticity an akamai like server dis tribution and bandwidth constraints for large companies this can exceed a million dollars a year savings rapidly increase with energy elasticity in a fully elastic system with relaxed bandwidth constraints we can reduce energy cost by over around if we impose strict bandwidth constraints without a significant increase in client server distances allowing client server distances to increase leads to in creased savings if we remove the distance constraint a dynamic solution has the potential to beat a static solution i e place all servers in cheapest market by a substantial margin maximum savings versus maximum savings presently energy cost aware routing is relevant only to very large companies however as we move forward and the energy elasticity of systems increases not only will this routing technique become more relevant to the largest sys tems but much smaller systems will also be able to achieve meaningful savings paper organization in the next section we provide some background on server electricity expenditure and sketch the structure of us energy markets in section we present data about the variation in regional electric prices section describes the akamai data set used in this paper section outlines the energy consumption model used in the simu lations covered in section section considers alternative mechanisms for market participation section presents some ideas for future work before we conclude background this section first presents evidence that electricity is be coming an increasingly important economic consideration and then describes the salient features of the wholesale elec tricity markets in the u s the scale of electricity expenditures in absolute terms servers consume a substantial amount of electricity in servers and data centers accounted for an estimated million mwh of us electricity con sumption costing about billion dollars at worst by data center energy use could double at best by re placing everything with state of the art equipment we may be able to reduce usage in to half the current level most companies operating internet scale systems are se cretive about their server deployments and power consump tion figure shows our estimates for several such com panies based on back of the envelope the in wh n pidle ppeak pidle u p u e ppeak where n is server count ppeak is server peak power in watts pidle is idle power and u is average server utilization rto region some regional hubs isone new england boston ma bos maine me connecticut ct nyiso new york nyc albany capitl buffalo west pjm import pjm pjm eastern chicago chi virgina dom new jersey nj miso midwest peoria il minnesota mn indiana cinergy figure the different regions studied in this paper the listed hubs provide a sense of rto coverage and a reference to map electricity market location identifiers hub to real locations palo alto server numbers are from public disclosures for ebay and rackspace earnings report to calculate energy we have made the following assumptions average data cen ter power usage effectiveness pue is and is cal culated based on peak power average server utilization is around average peak server power usage is watts based on measurements of actual servers at akamai and idle servers draw of their peak power our numbers for microsoft are based on company statements and energy figures mentioned in a promotional video to estimate google power consumption we assumed servers based on an old widely circulated number operating at watts each a pue of and average utilization around such a system would consume more than mwh and would incur an an nual electricity bill of nearly million at per mwh wholesale rate these numbers are consistent with an in dependent calculation we can make comscore estimated that google performed about 2b searches day in august and google officially stated recently that each search takes kj of energy on average presumably amor tized to include indexing and other costs thus search alone works out to 105 mwh in google servers handle gmail youtube and many other applications so our earlier estimates seem reasonable google may well have more than a million servers so an annual electric bill ex ceeding wouldn t be surprising akamai electricity costs represent indirect costs not seen by the company itself like others who rely on co location facilities akamai seldom pays directly for electricity power is mostly built into the billing model with charges based on provisioned capacity rather than consumption in section we discuss why our ideas are relevant even to those not directly charged per unit of electricity they use wholesale electricity markets although market details differ regionally this section pro vides a high level view of deregulated electricity markets providing a context for the rest of the paper the discus sion is based on markets in the united states generation electricity is produced by government util ities and independent power producers from a variety of sources in the united states coal dominates nearly followed by natural gas nuclear power and hydroelectric generation measure of data center energy efficiency different regions may have very different power genera tion profiles for example in hydroelectric sources accounted for of the power generated in washington state while in texas of the energy was generated us ing natural gas and coal transmission producers and consumers are connected to an electric grid a complex network of transmission and distribution lines electricity cannot be stored easily so supply and demand must continuously be balanced in addition to connecting nearby nodes the grid can be used to transfer electricity between distant locations the united states is divided into eight reliability regions with varying degrees of inter connectivity congestion on the grid transmission line losses est in and boundaries between regions introduce distribution inefficien cies and limit how electricity can flow market structure in each region a pseudo government al body a regional transmission organization rto man ages the grid figure an rto provides a central author ity that sets up and directs the flow of electricity between generators and consumers over the grid rtos also provide mechanisms to ensure the short term reliability of the grid additionally rtos administer wholesale electricty mar kets while bilateral contracts account for the majority of the electricity that flows over the grid wholesale electric ity trading has been growing rapidly and presently covers about of total electricity wholesale market participants can trade forward contracts for the delivery of electricity at some specified hour in or der to determine prices for these contracts rtos such as pjm use an auctioning mechanism power producers present supply offers possibly price sensitive consumers present demand bids possibly price sensitive and a coordinating body determines how electricity should flow and sets prices the market clearing process sets hourly prices for the dif ferent locations in the market the outcomes depend not only on bids and offers but also account for a number of constraints grid connectivity reliability etc each rto operates multiple parallel wholesale markets there are two common market types day ahead markets futures provide hourly prices for delivery during the following day the outcome is based on expected real time markets spot are balancing markets where prices are calculated every five minutes or so based on actual conditions rather than expectations typically this market accounts for a small fraction of total energy transactions less than of total in nyiso generally speaking the most expensive active generation resource determines the market clearing price for each hour the rto attempts to meet expected demand by activating the set of resources with the lowest operating costs when demand is low the base load power plants such as coal and nuclear can fulfill it when demand rises additional re sources such as natural gas turbines need to be activated security constraints line losses and congestion costs also impact price when transmission system restrictions such as line capacities prevent the least expensive energy sup plier from serving demand congestion is said to exist more ahead markets not discussed here are analogous jan may sep jan may sep jan may sep jan may figure daily averages of day ahead peak prices at different hubs the elevation in correlates with record high natural gas prices and does not affect the hydroelectric dominated northwest the northwest consistently experiences dips near april this seems to be correlated with seasonal rainfall correlated with the global economic downturn recent prices in all four locations exhibit a downward trend real time min real time hourly day ahead hourly 03 03 time est edt figure the real time market is more variable at short time scales than the day ahead market standard devi ations for prices at the nyc hub are shown averaged using different window sizes body of economic literature deals with the structure and evo lution of energy markets market failures and arbitrage opportunities for securities traders e g empirical market analysis figure comparing price variation in different whole sale markets for the new york city hub the top graph shows a period when prices were similar across all mar kets the bottom graph shows a period when there was significantly more volatility in the real time market expensive generation units will then need to be activated driving up prices some markets include an explicit conges tion cost component in their prices surprisingly negative prices can show up for brief periods representing conditions where if energy were to be consumed at a specific location at a specific time the overall efficiency of the system would increase market boundaries introduce economic transaction ineffi ciencies as we shall see later even geographically close lo cations in different markets tend to see uncorrelated prices part of the problem is that different markets have evolved using different rules pricing models etc clearly the market for electricity is complex in addition to the factors mentioned here many local idiosyncrasies ex ist in this paper we use a relatively simple market model that assumes the following real time prices are known and vary hourly the electric bill paid by the service operator is propor tional to consumption and indexed to wholesale prices the request routing behavior induced by our method does not significantly alter prices and market behavior the validity of the second assumption depends upon the extent to which companies hedge their energy costs by con tractually locking in fixed pricing see section a large we posit that imperfectly correlated variations in local electricity prices can be exploited by operators of large geo graphically distributed systems to save money rather than presenting a theoretical discussion we take an empirical ap proach grounding our analysis in historical market data ag gregated from government sources trade publication archives and public data archives maintained by the dif ferent rtos we use price data for locations covering january through march price variation geographic price differentials are what really matter to us but it is useful to first get a feel for the behaviour of individual prices daily variation figure shows daily average prices for four from january through april although prices are relatively stable at long time scales they exhibit a significant amount of day to day volatility short term spikes seasonal trends and dependencies on fuel prices and consumer demand some locations in the figure are visibly correlated but hourly prices are not correlated different market types spot and futures markets have different price dynamics figures and illustrate the difference for nyc compared to the day ahead market the hourly real time rt market is more volatile with more high frequency variation and a lower average price the underlying five minute rt prices are even more volatile northwest is an important region but lacks an hourly wholesale market forcing us to omit the region from the remain der of our analysis location rto mean stdev kurt chicago il pjm indianapolis in miso 44 palo alto ca caiso 54 34 richmond va pjm boston ma isone 66 new york ny nyiso figure real time market statistics covering hourly prices from january through march statistics are from the trimmed data different rtos nyiso isone pjm miso ercot caiso 05 89 78 samples 37 96 82 est distance between two hubs km figure the relationship between price correlation distance and parent rto each point represents a pair of hubs hubs pairs and the correlation coef ficient of their hourly prices samples hourly price change mwh hourly price change mwh each red points represent paired hubs from different rtos blue points are labelled with the rto of both a palo alto b chicago pjm figure histograms of hour to hour change in real time hourly prices for two locations over the month period both distributions are zero mean gaussian like with very long tails for the remainder of this paper we focus exclusively on the rt market our goal is to exploit geographically uncor related volatility something that is more common in the rt market we restrict ourselves to hourly prices but speculate that the additional volatility in five minute prices provides further opportunities figure provides additional statistics for hourly rt prices paloalto minus richmond austin minus richmond sat sun mon tue wed thu fri sat sun mon tue wed thu fri sat sat sun mon tue wed thu fri sat sun mon tue wed thu fri sat time edt hour to hour volatility as seen in figure the hour to hour variation in nyc rt prices can be dramatic fig ure shows the distribution of the hourly change for palo alto and chicago at each location the price per mwh changed hourly by or more roughly of the time a step represents of the mean price for chicago fur thermore the minimum and maximum price during a single day can easily differ by a factor of the existence of rapid price fluctuations reflects the fact that short term demand for electricity is far more elastic than supply electricity cannot always be efficiently moved from low demand areas to high demand areas and producers cannot always ramp up or down easily geographic correlation our approach would fail if hourly prices are well correlated at different locations however we find that locations in different regional markets are never highly correlated even when nearby and that locations in the same region are not always well correlated figure shows a scatter plot of pairwise correlation and geographic no pairs were negatively correlated note how correlation decreases with distance further note the impact of rto market boundaries most pairs drawn from the same rto lie above the correlation line while all pairs from different regions lie below we also see have verified our results using subsets of the data e g last months mutual information ix y shifted signals etc y much more clearly divides the data between same rto and different rto pairs suggesting that the small overlap in figure is due to the existence of non linear relationships within nyiso figure variation of price differentials with time a surprising lack of diversity within some regions la and palo alto have a coefficient of hourly prices are not correlated at short time scales but we should not expect prices to be independent natural gas prices for example will introduce some coupling see figure between distant locations price differentials figure shows hourly price differentials for two pairs of locations over an eight day period both pairs have mean differentials close to zero the three locations are far from each other and in different rtos we see price spikes some extend far off the scale the largest is and extended periods of price asymmetry sometimes the asymmetry favours one sometimes the other this suggests that a pre determined assignment of clients to servers is not optimal differential distributions consider two locations in order for our dynamic approach to yield substantial savings over a static solution the price differential between those locations must vary in time and the distribution of this dif ferential should ideally have a zero mean and a reasonably high variance such a distribution would imply that neither site is strictly better than the other but also that a dynamic solution always buying from whichever site is least expen sive that hour could yield meaningful savings additionally the dynamic approach could win when presented with two locations having uncorrelated periods of price elevation and ercot not detected by the correlation coefficient 55 87 52 31 price difference mwh price difference mwh price difference mwh price difference mwh price difference mwh a paloalto virginia b austin virginia c boston nyc d chicago virginia e chicago peoria figure price differential histograms for five location pairs and months of hourly prices jan may sep jan may sep jan may sep jan figure paloalto virginia price differential distribu tions for each month the monthly median prices and inter quartile range are shown figure shows the pairwise differential distributions for some locations for the data the california virginia figure and texas virginia figure dis tributions are zero mean with a high variance there are many other such boston nyc figure is skewed since boston tends to be cheaper than nyc but nyc is less expensive of the time the savings are greater than mwh of the time thus even with such a skewed distribution there exists an opportunity to dynamically exploit differentials for meaningful savings unsurprisingly a number of pairs exist where one location is strictly better than the other and dynamic adaptation is unnecessary chicago virginia figure is an example virginia is less expensive of the time but the savings almost never exceed mwh the dispersion introduced by a market boundary can be seen in the dynamically exploitable chicago peoria distribu tion figure evolution in time the price differential distributions do not remain static in time figure shows how the paloalto virginia distribution changed from month to month a sustained price asymmetry may exist for many months before reversing itself the spread of prices in one month may double the next month time of day price differentials depend on the time of day for instance because california and virginia are in different time zones peak demand does not overlap this is likely an important factor shaping the price differential figure shows how the hour of day affects the differ entials for three location pairs for paloalto virginia we see a strong dependency on the hour before eastern virginia has a significant edge by the situation has re versed from 4pm neither is better for boston nyc we see a different kind of dependency from neither are other pairs a set of hubs with µ σ and pairs a set of hubs with µ σ hour of day est edt figure price differential distributions median and inter quartile range for each hour of the day 05 00 california virginia differential duration hours figure for paloalto virginia short lived price dif ferentials account for most of the time site is better at all other times boston has the edge the effect of hour of day on chicago peoria is less clear differential duration we define the duration of a sus tained price differential as the number of hours one location is favoured over another by more than mwh as soon as the differential falls below this threshold or reverses to favour the other location we mark the end of the differential figure shows how much time was spent in short duration price differentials for paloalto virginia short differentials hrs are more frequent than other types medium length differentials hrs are common differentials that last longer than a day are rare for a balanced pair like this akamai traffic and bandwidth in order to understand the interaction of real workloads with electricity prices we acquired a data set detailing traffic on akamai infrastructure the data covers days worth of traffic on a large subset of akamai servers with a peak of over million hits sec figure the region traffic is the subset of servers for which we have electricity price data we use the akamai traffic because it is a realistic work load akamai has over content provider customers in the us hence the traffic represents a broad user base global traffic usa traffic region subset 00 00 00 00 02 00 00 01 09 00 00 utc time figure traffic in the akamai data set we see a peak hit rate of over million hits per second of this about million hits come from the us the traffic in this data set comes from roughly half of the servers akamai runs in comparison in total akamai sees around billion hits day however akamai does not use aggressive server power management their cdn is sensitive to latency and their workload contains a large fraction of computationally triv ial hits e g fetches of well cached objects so our work is far less relevant to akamai than to systems where more energy elasticity exists and workloads are computationally intensive furthermore in mapping clients to servers aka mai system balances a number of concerns trying to opti mize performance handle partially replicated cdn objects optimize network bandwidth costs etc traffic data traffic data was collected at minute in tervals on servers housed in akamai public clusters aka mai has two types of clusters public and private pri vate clusters are typically located inside of universities large companies small isps and isps outside the us these clus ters are dedicated to serving a specific user base e g the members of a university community and no others pub lic clusters are generally located in commercial co location centers and can serve any users world wide for any user not served by a private cluster akamai has the freedom to choose which of its public clusters to direct the user clients that end up at public clusters tend to see longer network paths than clients that can be served at private clusters the minute data contains for each public cluster the number of hits and bytes served to clients a rough geogra phy of where those clients originated and the load in each of the clusters in addition we surveyed the hardware used in the different clusters and collected values for observed server power usage we also looked at the top level mapping sys tem to see how name servers were mapped to clusters in the data we collected the geographic localization of clients is coarse they are mapped to states in the us or countries if multiple clusters exist in a city we aggregate them together and treat them as a single cluster this affects our calculation of client server distances in bandwidth costs an important contributor to data center costs is bandwidth and there may be large differ ences between costs on different networks and sometimes on the same network over time bandwidth costs are signif icant for akamai and thus their system is aggressively op timized to reduce bandwidth costs we note that changing akamai current assignments of clients to clusters to reduce energy costs could increase its bandwidth costs since they have been optimized already right now the portion of co location cost attributable to energy is less than but still a significant fraction of the cost of bandwidth the relative cost of energy versus bandwidth has been rising this is primarily due to decreases in bandwidth costs we cannot cannot ignore bandwidth costs in our analysis the complication is that the bandwidth pricing specifics are considered to be proprietary information therefore our treatment of bandwidth costs in this paper will be relatively abstract akamai does not view bandwidth prices as being geo graphically differentiated in some instances a company as large as akamai can negotiate contracts with carriers on a nationwide basis smaller regional providers may provide transit for free prices are usually set per network port us ing the basic billing model traffic is divided into five minute intervals and the percentile is used for billing our approach in this paper is to estimate percentiles from the traffic data and then to constrain our energy price rerouting so that it does not increase the percentile bandwidth for any location client server distances lacking any network level data on clients we use geographic distance as a coarse proxy for network performance in our simulations we see some evidence of geo locality in the akamai traffic data but there are many cases where clients are not mapped to the near est cluster geographically one reason is that geographical distance does not always correspond to optimal network per formance another possibility is that the system is trying to keep those clients on the same network even if akamai servers on that network are geographically far away yet another possibility is that clients are being moved to distant clusters because of bandwidth constraints modeling energy consumption in order to estimate by how much we can reduce energy costs we must first model the system energy consumption for each cluster we use data from the akamai cdn as a representative real world workload this data is used to de rive a distribution of client activity cluster sizes and cluster locations we then use an energy model to map prices and cluster traffic allocations to electricity expenses the model is admittedly simplistic our goal is not to provide accurate figures but rather to estimate bounds on savings cluster energy consumption we model the energy consumption of a cluster as be ing proportional roughly linear to its utilization multiple studies have shown that cpu utilization is a good estimator for power usage our model is adapted from google empirical study of a data center in which their model was found to accurately less than error predict the dy namic power drawn by a group of machines racks we augment this model to fill in some missing pieces and parametrize it using other published studies and measure ments of servers at akamai let pcluster be the power usage of a cluster and let ut be its average cpu utilization between and at time t pcluster ut f n v ut n ǫ where n is the number of servers in the cluster f is the fixed power v is the variable power and ǫ is an empirically derived correction constant see f n n pidle p u e ppeak v ut n n ppeak pidle ur where pidle is the average idle power draw of a single server ppeak is the average peak power and the exponent r is an empirically derived constant equal to see the equa tion for v is taken directly from the original paper a linear model r was also found to be reasonably accurate we added the pue component since the google study did not account for cooling etc with power management the idle power consumption of a server can be as low as of the peak power consump tion which can range from without power management an off the shelf server purchased in the last several years averages around and draws of its peak power when idle based on measured values ultimately we want to use this model in simulation to estimate the maximum percentage reduction in the energy costs of some server deployment pattern consequently the absolute values chosen for ppeak and pidle are unimportant their ratio is what matters in fact it turns out that the ergy dissipated by each packet passing through a core router would be as low as a µj per medium sized packet we must also consider what happens if the new routes overload existing routers if we use enough additional band width through a router it may have to be upgraded to higher capacity hardware increasing the energy significantly how ever we could prevent this by incorporating constraints like the bandwidth constraints we use simulation projecting savings in order to test the central thesis of this paper we con ducted a number of simulations quantifying and analysing the impact of different routing policies on energy costs and client server distance our results show that electricity costs can plausibly be re duced by up to and that the degree of savings primarily depends on the energy elasticity of the system in addition to bandwidth and performance constraints we simulate akamai bandwidth constraints and show that overall system costs can be reduced we also sketch the relation ship between client server distance and savings finally we investigate how delaying the system reaction to price dif ferentials affects savings simulation strategy we constructed a simple discrete time simulator that step ped through the akamai usage statistics letting a routing module with a global view of the network allocate traffic to clusters at each time step using these allocations we mod eled each cluster energy consumption and used observed hourly market prices to calculate energy expenditures be pcluster pcluster is critical in determining the savings that fore presenting the results we provide some details about can be achieved using price differential aware routing ideally pcluster would be zero an idle cluster would consume no energy at present achieving this without im pacting performance is still an open challenge however there is an increasing interest in energy proportional com puting and dynamic server provisioning techniques are being explored by both academics and industry we are confident that pcluster will continue to fall increase in routing energy in our scheme clients may be routed to distant servers in search of cheap energy from an energy perspective this network path expansion represents additional work that must be performed by something if this increase in energy were significant network providers might attempt to pass the additional cost on to the server operators given what we know about bandwidth pricing a small increase in routing energy should not impact bandwidth prices alter natively server operators may bear all the increased energy costs suppose they run the intermediate routers a simple analysis suggests that the increased path lengths will not significantly alter energy consumption routers are not designed to be energy proportional and the energy used by a packet to transit a router is many orders of magnitude below the energy expended at the endpoints e g google kj query we estimate that the average energy needed for a packet to pass through a core router is on the order of mj further we estimate that the incremental en reported for a cisco gsr router mid sized pack ets sec and watts measured our simulation setup electricity prices we used hourly real time market prices for twenty nine different locations hubs however we only have traffic data for akamai public clusters in nine of these locations therefore most of the simulations focused on these nine locations our data set contained months of price data spanning january through march unless noted otherwise we assumed the system reacted to the previous hour prices traffic and server data the akamai workload data set contains minute samples for the hits per second ob served at public clusters in twenty five cities for a period of days and some hours each sample also provides a map specifying where hits originated grouping clients by state and which city they were routed to we had to discard seven of these cities because of a lack of electricity market data for them the remaining eighteen cities were grouped by electricity market hub as nine clus ters in our day simulation we used the traffic incident on these nine clusters in order to simulate longer periods we derived a syn thetic workload from the day akamai workload us traf fic only we calculated an average hit rate for every hub and client state pair we produced a different average for each hour of the day and each day of the week additionally the akamai data allowed us to derive capac power consumption of idle router is the peak power in the future power aware hardware may reduce this disparity between the marginal and average energy energy model parameters idle power pue figure the system energy elasticity is key in de termining the degree of savings price conscious routing can achieve further obeying existing bandwidth constraints reduces but does not eliminate savings the graph shows day savings for a number of different pue and pidle values with a distance threshold the savings for each energy model are given as a per centage of the total electricity cost of running akamai actual routing scheme under that energy model ity constraints and the percentile hits and bandwidth for each cluster capacity estimates were derived using ob served hit rates and corresponding region load level data provided by akamai our simulations use hits rather than the bandwidth numbers from the data most of our simulations used akamai geographic server distribution although the details of the distribution may introduce artifacts into our results this is a real world distri bution as such we feel relying on it rather than relying on synthetic distributions makes our results more compelling routing schemes in our simulations we look at two routing schemes akamai original allocation and a dis tance constrained electricity price optimizer given a client the price conscious optimizer maps it to a cluster with the lowest price only considering clusters within some maximum radial geographic distance for clients that do not have any clusters within that maximum distance the routing scheme finds the closest cluster and considers any other nearby clusters if the selected cluster is nearing its capacity or the boundary the optimizer iteratively finds another good cluster the price optimizer has two parameters that modulate its behaviour a distance threshold and a price threshold any price differentials smaller than the price threshold are ignored we use mwh setting the distance threshold to zero gives an optimal distance scheme select the cluster geographically closest to client setting it to a value larger than the east west coast distance gives an optimal price scheme always select the cluster with the lowest price we are not proposing this as a candidate for implemen tation but it allows us to benchmark how well a price conscious scheme could do and to investigate trade offs be tween distance constraints and achievable savings energy model we use the cluster energy model from section we simulated the running cost of the system using a number of different values for the peak server power ppeak idle server power pidle and the pue this section discusses normalized costs and pidle is always expressed as a percentage of ppeak some energy parameters that we used optimistic future idle pue cutting edge google idle pue state of the art idle pue disabled power management idle pue client server distance given a client origin state and the server location hub our distance metric calcu lates a population density weighted geographic distance we used census data to derive basic population density functions for each us state when the traffic contains clients from outside the us we ignore them in the distance calculations we use this function as a coarse measure for network dis tance the granularity of the akamai data set does not pro vide enough information for us to estimate network latency between clients and servers or even to accurately calculate geographic distances between clients and servers at the turn of the year days of traffic we begin by asking the question what would have hap pened if an akamai like system had used price conscious routing at the end of how would this have com pared in cost and client server distance to the current rout ing methods employed by akamai energy elasticity we find that the answer hinges on the energy elasticity characteristics of the system figure illustrates this when consumption is completely propor tional to load using price conscious routing could eliminate of the electricity expenditure of akamai traffic alloca tion without appreciably increasing client server distances as idle server power and pue rise we see a dramatic drop in possible savings at google s published elasticity level idle pue the maximum savings have dropped to inelasticity constrains our ability to route power demand away from high prices bandwidth costs a reduced electric bill may be over shadowed by increased bandwidth costs figure therefore also shows the savings when we prevent clusters from hav ing higher percentile hit rates than were observed in the akamai data we see that constraining bandwidth in this way may cause energy savings to drop down to about a third of their earlier values however the good news is that these savings are reductions in the total operating cost by jointly optimizing bandwidth and electricity it should be possible to acquire part of the economic value represented by the difference between savings with and without band width constraints distance and savings the savings in figure do not represent a free lunch the mean client server distance may need to increase to leverage market diversity the price conscious routing scheme we use has a dis tance threshold parameter allowing us to explore how higher client server distances lead to lower electric bills figure shows how increasing the distance threshold can be used to reduce electricity costs figure shows how client server distances change in response to changes in the threshold at a distance threshold of the percentile estimated client server distances is at most this should provide an acceptable level of performance the dis tance between boston and alexandria in virginia is about and network rtts are around at this threshold using the future energy model the sav ings is significant between obey constraints and there is an elbow at a threshold of causing both savings and distances to jump the distance between boston and chicago is about after this increas ing the threshold provides diminishing returns 00 75 00 distance threshold km 1500 distance threshold km figure day electricity costs fall as the distance threshold is increased the costs shown here are for a idle pue model normalized to the cost of the akamai allocation figure month electricity costs fall as the distance threshold is increased the costs shown here are for a idle pue model normalized to the cost of the synthetic akamai like allocation boston dc boston chicago mean distance ignore percent ignore mean distance percent 1500 2500 distance threshold km ma ny il va nj ma ny il va nj figure increasing the distance threshold allows the routing of clients to cheaper clusters further away figure ma ny il va nj ma ny il va nj shows corresponding falling cost figure shows results for a specific set of server energy parameters but other parameters give scaled curves with the same basic shapes this follows analytically from our energy model equations in the difference in scale can be seen in figure synthetic workload months of prices the previous section uses a very small subset of the price data we have using a synthetic workload derived from the original day one we ran simulations covering january through march 2009 our results show that savings increase above those for the day period figure shows how electricity cost varied with the dis tance threshold analogous to figure the results are similar to what we saw for the day case but maximum savings are higher notably thresholds above in figure do not exhibit sharply diminishing returns like those seen in in order to normalize prices we used statis tics of how akamai routed clients to model an akamai like router and calculated its month cost figure breaks down the savings by cluster showing the change in cost for each cluster the largest savings is shown at nyc this is not surprising since the highest peak prices tend to be in nyc these savings are not achieved by always routing requests away from nyc the likelihood of requests being routed to nyc depends on the time of day we simulated other server distributions evenly distributed across all hubs heterogeneous distributions etc and saw similar decreasing cost distance curves figure change in per cluster cost for month sim ulations with different distance thresholds this uses the future model and obeys constraints dynamic beats static in particular we see that when constraints are ignored the dynamic cost minimization solution can be substantially better than a static one in figure we see that the dynamic solution could reduce the electricity cost down to almost 55 while moving all the servers to the region with the lowest average price would only reduce cost down to reaction delays not reacting immediately to price changes noticeably re duces overall savings in our simulations we were conserva tive and assumed that there was a one hour delay between the market setting new prices and the system propagating new routes figure shows how increasing the reaction delay impacts prices first note the initial jump between an immediate reaction and a next hour reaction this implies achievable savings will exceed what we have calculated for systems that can update their routes in less than an hour further note the local minima at the hour mark this is probably because market prices can be correlated for a given hour from one day to the next the increase in cost is substantial with the idle pue energy model the maximum savings is around see figure so a subsequent increase in cost of would eliminate a large chunk of the savings delay in reacting to prices hours alternatively customers could enroll in triggered demand response programs agreeing to reduce their power usage in response to a request by the grid operators load reduction requests are sent out when electricity demand is high enough to put grid reliability at risk or rising demand requires the imminent activation of expensive unreliable generation as sets the advance notice given by the rto can range from days to minutes participating customers are compensated figure impact of price delays on electricity cost for a idle pue model with a distance threshold of actual electricity bills in this paper we assume that power bills are based on hourly market prices and on energy consumption addi tionally we assume that the decisions of server operators will not affect market prices the strength of this approach is that we can use price data to quantify how much money would have been saved however in reality achieving these savings would probably require a renegotiation of existing utility contracts further more rather than passively reacting to spot prices active participation opens up additional possibilities existing contracts it is safe to say that most current contractual arrangements would reduce the potential sav ings below what our analysis indicates that said server operators should be able to negotiate deals that allow them to capture at least some of this value wholesale indexed electric billing plans are becoming in creasingly common throughout the us this allows small companies that do not participate directly in the wholesale market to take advantage of our techniques this billing structure appeals to electricity providers since risk is trans ferred to consumers for example in the mid west rto commonwealth edison offers a real time pricing program customers enrolled in it are billed based on hourly consumption and corresponding wholesale pjm miso loca tional market prices companies such as akamai renting space in co location facilities will almost certainly have to negotiate a new billing structure to get any advantage from our approach most co location centers charge by the rack each rack having a maximum power rating in other words a company like akamai pays for provisioned power and not for actual power used we speculate that as energy costs rise relative to other costs it will be in the interest of co location owners to charge based on consumption and possibly location there is evi dence that bandwidth costs are falling but energy costs are not even if new kinds of contracts do not arise server op erators may be able to sell their load flexibility through a side channel like demand response as discussed below by passing inflexible contracts selling flexibility distributed systems with energy elastic clusters can be more flexible than traditional con sumers operators can quickly and precipitously reduce power usage at a location by suspending servers and routing re quests elsewhere market mechanisms already exist that would allow operators to value and sell this flexibility some rtos allow energy users to bid negawatts nega tive demand or load reductions into the day ahead market auction this is believed to moderate prices based on their flexibility and load demand response vari ants exist in every market we cover in this paper even consumers using as little as a few racks can participate in such programs consumers can also be aggre gated into large blocs that reduce load in concert this is the approach taken by enernoc a company that collects many consumers packages them and sells their aggregate ability to make on demand reductions a package of hotels would for example reduce laundry volume in sync to ease power demand on the grid the good thing about selling flexibility as a product is that this is valued even where wholesale markets do not exist it even works if price differentials don t exist e g fixed price contracts or in highly regulated markets however we have ignored the demand side how do op erators construct bids for the day ahead auctions if they don t know next day client demand for each region what happens when operators are told to reduce power consump tion at a location when there is a concentration of active clients nearby in systems like akamai demand is gener ally predictable but there will be heavy traffic days that are impossible to predict there is anecdotal evidence that data centers have partic ipated in demand response programs however the ap plicability of demand response to single data centers is not widely accepted participating data centers may face addi tional downtime or periods of reduced capacity conversely when we look at large distributed systems participation in such programs is attractive especially when the barriers to entry are so low only a few racks per location are needed to construct a multi market demand response system future work some clear avenues for future work exist implementing joint optimization existing systems already have frameworks in place that engineer traffic to optimize for bandwidth costs performance and reliability dynamic energy costs represent another input that should be integrated into such frameworks rto interaction service operators can interact with rtos in many ways this paper has proposed a relatively passive approach in which operators monitor spot prices and react to favourable conditions as we discussed in section there are other market mechanisms in place that service operators may be able to exploit the optimal market par ticipation strategy is unclear weather differentials data centers expend a lot of energy running air cooling systems up to of total en ergy in modern systems when ambient temperatures are low enough external air can be used to radically reduce the power draw of the chillers at the same time weather temperature differentials are common this suggests that significant energy savings can be achieved by dynamically routing requests to sites where the heat generated by serv ing the request is most inexpensively removed unlike price differentials which reduce cost but not energy routing re quests to cooler regions may be able to reduce both environmental cost rather than attempting to min imize the dollar cost of the energy consumed a socially re sponsible service operator may instead choose to use an envi ronmental impact cost function the environmental impact of a service is time varying an obvious cost function is the carbon footprint of the energy used in grids that aggre gate electricity from diverse providers the footprint varies depending upon what generating assets are active whether power plants are operating near optimal capacity and what mixture of fuels they are currently using the variation oc curs at multiple time scales e g seasonal is there water to power hydro systems weekly what are the relative prices of various fossil fuels and hourly is the wind blowing or the tide going out additionally carbon is not the only pol lutant for instance power plants are the primary station ary sources of nitrogen oxide in the us due to variations in weather and atmospheric chemistry the timing and location of nox reductions determine their effectiveness in reducing ground level ozone conclusion the bounds derived in this paper should not be taken too literally our cost and traffic models are based on actual data but they do incorporate a number of simplifying as sumptions the most relevant assumptions are probably that operators can do better by buying electricity on the open market than through negotiated long term contracts and that the variable energy costs associated with ser vicing a request are a significant fraction of the total costs despite these caveats it seems clear that the nature of ge ographical and temporal differences in the price of electricity offers operators of large distributed systems an opportunity to reduce the cost of servicing requests it should be pos sible to augment existing optimization frameworks to deal with electricity prices thermal management of dram memory has become a critical is sue for server systems we have done to our best knowledge the first study of software thermal management for memory subsys tem on real machines two recently proposed dtm dynamic thermal management policies have been improved and imple mented in linux os and evaluated on two multicore servers a dell poweredge server and a customized intel server testbed the experimental results first confirm that a system level memory dtm policy may significantly improve system per formance and power efficiency compared with existing memory bandwidth throttling scheme a policy called dtm acg adap tive core gating shows performance improvement comparable to that reported previously the average performance improvements are and on the poweredge and the vs from the previous simulation based study respectively we also have surprising findings that reveal the weakness of the previous study the cpu heat dissipation and its impact on dram memories which were ignored are significant factors we have observed that the second policy called dtm cdvfs coordinated dynamic voltage and frequency scaling has much better perfor mance than previously reported for this reason the average im provements are and on the two machines vs from the previous study respectively it also significantly reduces the processor power by and energy by on average categories and subject descriptors b primary memory design styles general terms design management performance keywords dram memories thermal management permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee sigmetrics june annapolis maryland usa copyright acm 005 08 00 introduction thermal management has been a first order consideration in pro cessor and hard disk design for a long time and now it has become critically important in the design of dram memory subsystems this trend is driven by the wide adoption of multi core processors and the ever increasing demand for high capacity high bandwidth from memory subsystems for example a current small scale two way smp server provides peak memory bandwidth of gb s and maximum memory capacity of 32 gb to support up to eight processor cores the design of high bandwidth and high density dram subsystem leads to increasing power consumption and heat generation the maximum power consumption of the memory sub system can reach watts which is in the same range of the power consumed by the processors with this degree of power consump tion dram power and thermal management has become an ur gent and critical issue while studies have done on dram power management there lacks systematic research on dram thermal management modern computing systems are designed with cooling capabili ties to allow full system performance under normal operating con ditions as well as thermal solutions to safeguard the systems against adverse situations as for dram memory subsystems a simple thermal solution has been used in servers when the memory subsystem is approaching a critical thermal threshold the mem ory controller throttles the memory bandwidth another solution proposed for mobile systems shuts down a whole memory subsys tem in those systems memory thermal management is used as a protection mechanism that ensures safe operation and prevents thermal emergencies under abnormal scenarios these scenarios while not common do occur in practice they can be due to a poorly designed thermal solution in other subsystems system fan failure obstructions to airflow within a system thermally challeng ing workload mix or other reasons that cause a system to operate outside of its thermal design boundaries thermal management is also necessary since time constants with which silicon components can cross a critical thermal limit are much faster than the response time of system level cooling control such as fan our research goal is to design and implement memory dtm methods for server systems running in constrained thermal envi ronments to improve the systems performance and or power effi ciency thermal management can be a normal form when users or system operators make a decision to operate in more constrained thermal environments including unavoidable high ambient tem peratures the need to limit fan speed for acoustic reasons or the necessity to reduce cooling costs in data centers throttling memory bandwidth under a certain threshold can pre vent memory temperature from increasing thus avoiding any pos sibility of thermal emergency nevertheless it may limit the sys tem performance unnecessarily a carefully designed dtm dy namic thermal management scheme when used in combination with bandwidth throttling may improve system performance and or improve system power efficiency without putting the system in haz ard a recent study proposed two schemes that directly throt tle the processor execution through core gating selectively shut ting down processor cores or dvfs dynamic voltage and fre quency scaling for memory thermal management it evaluated those schemes using a dynamic dram thermal model and sim ulation and reported that integrating processor power management and execution control with memory thermal management yields significant gains in system performance and energy efficiency how ever the work has two limitations first the dram thermal model used for evaluation of different thermal management mechanisms has not been validated on real systems given the dependency be tween the accuracy of the thermal model and reported power and performance gains it is necessary to confirm results presented in the study by implementing and evaluating these approaches in real systems second due to inherent limitations of running long work load traces in a simulator the design space and parameters of the proposed schemes were not full explored and adequately analyzed to address these issues we evaluate the existing memory dtm schemes on two server systems our study uses measurements on real systems running multiprogramming workloads we have im plemented these schemes in a linux os and evaluated their perfor mance and power efficiency on two servers configured with latest generation hardware a dell poweredge server and an intel server testbed called and there after to obtain an accurate picture of the power and performance benefits of mechanisms evaluated in this paper we instrumented the with power and thermal sensors to get fine grain measurements at a component level to the best of our knowledge this is the first study of software thermal management for memory subsystem on real machines we have done comprehensive experiments and detailed analysis re garding performance memory temperature profiles and system power and energy consumption our experiments first confirm that the two recently proposed schemes significantly improve performance of real server systems in constrained thermal environments in ad dition we have encouraging findings that address the limitations of the previous work compared with the simple dtm bw dtm through mem ory bandwidth throttling method the dtm acg dtm through adaptive core gating scheme improves per formance by up to and on the and servers respectively and and on average respectively the improvements of dtm cdvfs dtm through coordinated dynamic voltage and frequency scaling are up to and and and on average on the two servers respectively the performance gain of the dtm cdvfs scheme mea sured on real systems is much better than the previously re ported simulation result which is only on average be sides the expected performance difference due to different configurations of the real systems and the simulated one our analysis indicates that the cpu heat dissipation and its influ ence on dram which were ignored in the previous study is a significant factor we have also found that the dtm cdvfs method improves the system power efficiency in addition to the performance gains it reduces the processor power rate by on the the energy consumed by the processor and mem ory is reduced by on average we have evaluated a new scheme called dtm comb that combines dtm acg and dtm cdvfs it may stop a sub set of cores and apply dvfs to the others our experimental results show that the new scheme may further improve the performance by up to when compared with the simulation based work this study is novel in memory dtm implementations and the experimental methods it is the first effort showing that memory dtm can be im plemented as part of os power management and work in conjunc tion with existing power management mechanisms e g dvfs the instrumentation we did on the testbed is unique and the established experimental method is more convincing than simulation we are able to do extensive experiments which bring several new insights the rest of this paper is organized as follows section presents background and related work of this study section discusses our design and implementation of the dram dtm schemes on real systems section describes the experimental methodology and workloads section analyzes the experimental results and finally section summarizes the study background and related work thermal management in computer systems thermal man agement has become a major research focus in recent years most studies so far have focused on processors and hard disks in server systems and data centers brooks and martonosi study different processor dtm mechanisms which include scaling the clock fre quency or voltage skadron et al develop a thermal model for individual functional blocks using thermal resistances and ca pacitances derived from the layout of the micro architecture struc tures they further extend the model to hotspot which mod els thermal behavior at microarchitecture level using a network of thermal resistances and capacitances and can identify the hottest unit on chip they also propose several system level dtm techniques for example migrating computation to underutilized hardware units from overheated ones li et al study the ther mal constraints in the design space of cmps donald and martonosi explore the design space of thermal management tech niques for multicore processors regarding the dtm for the hard disk drives gurumurthi et al develop models to capture the capacity performance and thermal behavior of disk drives they also present two dtm techniques for hard disks exploiting the thermal slack or throttling disk activities kim et al further develop a performance temperature simulator of disk drives and study the thermal behavior and management of storage systems us ing server workloads another important and related area is applying dtm techniques at the system and data center level moore et al use temperature aware workload placement algorithm to reduce data center cooling costs heath et al propose mercury a temperature emulation suite for servers they also develop and evaluate freon a system for managing thermal emergency in server clusters choi et al propose thermostat a cfd based tool to study thermal optimiza tions at run time in server systems as well as the layout optimiza tion in design phase this study focuses on dram memory subsystems in individual servers the proposed methods can be used along with the other methods thermal issue of and fb dimm memories with the rapid increase of dram capacity and bandwidth dram mem ory subsystem now consumes a significant portion of total system power the dram memory has a general trend of power increase of per year as the bandwidth doubles every three years the dram die size itself has been growing at a rate of per year in the latest generation server systems dram power consumption can be comparable to that of processors dram thermal problem has become a real issue recently for both dram and fully buffered dimm fb dimm a recent study has reported that on a mobile system the temperature of dram devices may exceed their thermal design point of c when running real work loads at an ambient temperature of c on sever platforms the recently deployed fb dimm has become a focus for dram thermal studies fb dimm is designed to support both high bandwidth and large capacity by using narrow and high frequency channels with point to point communication an amb advanced memory buffer is placed into each dimm for transferring data between the dram devices and the memory channel in fb dimm amb is a hot spot because of its high power density w att without ther mal control the temperature of amb can exceed its thermal design point of c in practice a product server may hold the amb temperature under c for safety additionally the heat gener ated by amb will spread to dram devices with a lower thermal design point of c making them potential hot spots dram thermal behavior as modeled in the micron power calculator and an intel data sheet the power consumption of a dram memory subsystem is almost linear to the memory throughput it consists of two parts static power which is mostly a constant but configuration dependent and dynamic power which is almost proportional to memory throughput if the memory through put is kept at a constant level the dram temperature will raise and stabilize in a few minutes the stable temperature can roughly be defined as tstable tambient p pi ψi where tambient is the dram ambient temperature ψi is the thermal resistance between a component i in the system and the dimm and pi is the power consumed by the component components in this equation include the dimm itself and may include adjacent dimms and other sub systems like processor or hard disk if the thermal interaction be tween them and the dimm is not negligible in fb dimm the power consumption of a typical amb is in the range of watts and that of a typical dram device is about watts dynamic thermal management schemes for memories in practice two dtm schemes have been used to prevent amb or dram devices from overheating in thermal shutdown the mem ory controller or the operating system periodically reads the tem perature of dimms from thermal sensors located on the dimm and if the reading exceeds a preset thermal threshold stops all ac cesses to memory until the temperature drops below the specified threshold by a certain margin in bandwidth throttling the memory controller gradually throttles memory throughput as tem perature starts to rise to prevent it from crossing critical shutdown thermal threshold the throttling is done by counting and limiting the number of dram row activations in a given time window lin et al propose a dynamic dram thermal model as well as the dtm acg and dtm cdvfs schemes and evaluate them using a simulator the two schemes are discussed in detail in sec tion we further extend this work in our paper and provide de tailed evaluation and analysis of these schemes on real systems other related work isci et al have proposed a run time phase prediction method to manage mobile processor power consumption for memory intensive applications they use dvfs during memory bound phases of a workload to reduce processor power by contrast dtm cdvfs is triggered in thermally con strained systems and the objective is to improve performance and reduce thermal heat exchange in addition to improving processor power efficiency and the evaluation in this study has been done in a multi core server system as opposed to a single core mobile platform since memory temperature change is much slower than program phase change thermal emergency is likely a more reliable trigger for dvfs with a performance target though phase predic tion can work when thermal emergency does not appear another study by isci et al proposes methods to use per core dvfs in managing the power budget of a multicore processor besides the difference that this study is focused on memory thermal manage ment per core dvfs is not available on our platforms design and implementation issues in general a dtm scheme can be divided two interdependent parts mechanism and policy the mechanism enforces dtm de cisions made by the policy and also provides inputs to it while the policy decides when and what thermal actions to trigger the goal of a dtm policy is to prevent memory from overheating or go ing above its maximum safe operating temperature this is ac complished by continuously monitoring memory temperature and forcing memory to reduce its power by putting a cap on memory throughput when the temperature crosses a predefined threshold called tcritical figure the bandwidth allowed at this point is drastically limited to protect that part leading to significant degra dation in system responsiveness and performance to smooth the effects of thermal management and reduce its im pact on system performance a well designed dtm policy may try to reduce memory bandwidth more gracefully when memory tem perature starts to approach a critical threshold to accom plish this a dtm policy usually defines another threshold called ttm where an adaptive dtm mechanism which supports multiple running levels starts to get activated the range ttm tcritical is normally broken into thermal zones with each thermal zone hav ing an associated set of actions that are designed to lower memory temperature thermal zones with higher temperatures trigger more aggressive actions necessary to bring memory temperature within the ttm threshold note that if temperature ever crosses tcritical threshold and reaches tshutdown point the memory controller will shut down the memory subsystem to avoid any physical damage to that part though this should never happen in a properly designed system with bandwidth throttling memory dtm mechanisms a memory dtm mechanism should generally consist of three components a memory temperature monitor or estimator a dtm policy trigger and a method for controlling memory temperature we have designed different mechanisms to support four thermal management policies namely dtm bw dtm acg dtm cdvfs and dtm comb and implemented them on two linux servers with intel xeon processors each dtm mechanism is an in tegration of hardware software components that together provide the required functions and capabilities to support dtm policy figure the idea of using thermal zone to guide the design of memory dtm policies thermal emergency level which is used in the discussions of our specific dtm implementations is the same concept temperature monitoring a dtm scheme makes thermal management decisions based on current and possibly past or pre dicted future memory temperature the temperature can be either measured if thermal sensors are available or conservatively pre dicted if otherwise temperature readings at multiple points should be obtained because a memory subsystem may have multiple hot spots where the temperature may potentially cross the critical ther mal point of that part if no thermal solution is used both of our servers use fb dimm based memory subsystems and by their con figurations the ambs are hot thermal sensors are located in the amb of each fb dimm thus the amb temperature can be directly measured and used for making dtm decisions in ad dition our system hosts multiple temperature sensors that measure the front panel temperature system ambient cpu inlet temperature memory inlet temperature and memory exhaust temperature those temperature readings do not affect dtm deci sions but allow us to get a detailed thermal profile during program execution policy trigger a dtm policy needs to periodically check whether any of the thermal thresholds have been crossed and in voke thermal control logic if necessary we implement the dtm policy as a monitoring utility which is periodically woken up by the os scheduler we used a default interval of one second in our experiments since dram thermal constants are large it takes roughly a few hundred seconds for dram devices or ambs to reach their tdps from idle state one second interval is adequate to trigger thermal management actions and protect memory from overheating it is also sufficiently long to avoid any noticeable per formance overhead from the monitor itself memory thermal control methods when a thermal thresh old is crossed and the event is detected by the monitoring util ity some actions need to be taken to lower memory temperature since dram power and therefore temperature are closely related to memory throughput with all other conditions such as airflow and inlet temperature being equal temperature can be lowered by reducing memory throughput we have used three approaches that control memory activity either from the memory side or the pro cessor side the first approach called bandwidth throttling sets a limit on the number of memory accesses that are allowed in a cer dram devices can be hot spots in other configurations of fb dimm tain time window intel chipset used in both of out servers allows clamping the number of dram row activations over a spec ified time window the dtm bw policy uses this capability to throttle memory throughput at different levels based on current thermal zone the second approach called core gating reduces memory throughput by limiting the number of active cores through cpu hot plug module in the linux kernel version when a cpu is unplugged it is logically removed from the os and placed into a sleep state by executing a halt instruction note that the over head of this operation is very small given one second granularity of our dtm policies the last approach uses the feature of proces sor voltage and frequency scaling to reduce memory throughput the xeon processors used in our servers support four differ ent frequency voltage operating points ghz 2125 v 667 ghz 1625 v 333 ghz v and ghz 0375 v in the latter two approaches bandwidth throttling is en abled when memory temperature is close to its thermal threshold to avoid any possibility of overheating memory dtm polices the objective of a dtm policy is to maximize the overall system performance without crossing the thermal threshold of any part of the system a secondary objective is to improve the overall system power efficiency in general dtm policies have to reduce mem ory throughput to lower memory temperature but their strategies can be different as discussed above it is important to note these dtm policies do not guarantee a certain level of memory tempera ture or activity rather they change memory or processor operating parameters with the aim of reducing memory bandwidth while in creasing system s ability to better tolerate constrained thermal en vironments if these policies are not effective in reducing memory temperature they will all enable bandwidth throttling as a safe guard when memory temperature crosses a critical tcritical threshold as shown in figure thermal emergency level and running level as dis cussed earlier a general approach in our dtm policy design is to quantize memory temperature into thermal zones or thermal emer gency levels and associate each level with a system thermal running level the use of running levels has appeared in real sys tems i e in bandwidth throttling of intel chipset in this study system running levels are defined in terms of processor fre quency number of active cores and allowed memory bandwidth in general a thermal running level with better system performance also generates more heat every time a policy module is executed it reads the temperature sensors determines the thermal emergency level and then decides the thermal running level for the next time interval if the new running level is different from the current one a thermal action will be taken to change the thermal running level table describes the settings of the thermal emergency levels and the thermal running levels for the two servers it is important to note that the number of the emergency levels and that of the running levels do not have to equal the number of thermal zones is based on the inherent relationship between dram temperature and memory bandwidth while the number of thermal running levels is based on the underlying hw capabilities for example there could be more than four running levels if the two processors are quad core also it is a coincidence that dtm acg and dtm cdvfs have the same number of running levels we used the following methodology to define thermal emergency level for our two systems on the intel we set tcritical threshold to c this threshold is based on a conservative c amb thermal threshold with a degree margin to ensure safe op eration of the system the intel is put into a hot box and the system ambient temperature is set to c which emulates a constrained thermal environment four thermal emergency lev els are then defined in decrements of degrees the is located as a standalone box in an air conditioned room with a system ambient temperature of c to better understand the thermal behavior of such a server with an ambient temperature of c we artificially lower the c amb thermal threshold by c and set tcritical to c accord ingly the thermal emergency levers for are then defined in decrements of degrees similar to the system note that in both systems the lowest thermal emergency level does not impose any constraints and al lows for full system performance for safety concern the chipset s bandwidth throttling is enabled when the system is in the highest thermal emergency level to ensure that overheating will not happen dtm bw policy this policy only performs bandwidth throt tling it resembles the bandwidth throttling in intel chipset and we use it as a reference to evaluate other dtm policies it uses the bandwidth limiting function of the chipsets which are avail able in both systems to cap the bandwidth usage according to the current thermal emergency level setting the limit to s on the will guarantee that the memory will not overheat and so does using the s limit on the as table describes four thermal running levels are used the limits are enforced in the chipset by limiting the number of dram row activations in a time window because close page mode is used bandwidth usage is mostly proportional to number of dram row activations the de fault window of is used which is suggested by the chipset designers dtm acg policy this policy uses core gating to indirectly reduce memory traffic it is based on the observation that when a subset of cores is disabled the contention for last level cache from different threads is reduced leading to reduced memory traf fic if the workload is bandwidth bound then system performance will be improved note that for a memory intensive workload the memory thermal constraint puts a limit on memory bandwidth that the program execution may utilize therefore the gain from mem ory traffic reduction can more than offset the loss of computation power from core gating both servers have two dual core proces sors we always retain one active core per processor to fully utilize their caches to reduce memory traffic therefore thermal running levels associated with the last two thermal emergency levels sup port the same number of active cores the main difference between them is that in as with other dtm policies described in this section maximum memory bandwidth throttling is also enforced note that when one or more cores are gated the threads allocated to these cores get migrated to active cores resulting in serialized sharing of core and cache resources with other threads we found that the default scheduling interval of of the linux kernel works well containing cache thrashing while maintaining fairness and smoothness in process scheduling dtm cdvfs policy this policy uses processor dvfs to indirectly throttle memory traffic the main source of performance improvement as to be showed in section is the reduced processor heat generation lowering processor voltage and frequency leads to reduced processor power consumption and heat exchange with other components including memory consequently the memory subsystem may run at high speed for longer time than normally allowed this effect has been observed on both servers but is more obvious on the intel because the processors are located right in front of fb dimms on the airflow path in that sys tem the four running levels under this policy correspond to four frequency and voltage settings of xeon cpu in this paper dtm cdvfs policy performs frequency voltage transitions on all processors and cores simultaneously and uniformly a policy that supports such differentiation is worth further investigation but we leave it to future work dtm comb policy dtm comb combines dtm acg and dtm cdvfs by integrating core gating and coordinated dvfs the goal of this policy is to take full advantage of the combined benefits offered by dtm cdvfs larger thermal headroom and reduced memory traffic enabled by core gating in dtm acg ta ble shows four thermal running levels defined for this policy by combining the number of active cores and processor operating fre quency similar to dtm acg we keep at least one core active for each cpu in dtm comb to fully utilize the caches experimental methodology hardware and software platforms we conducted our experiments on two small scale servers as standalone systems both machines use fb dimm and the mem ory hot spots are ambs therefore we are only concerned with amb temperatures thereafter dram devices can be hot spots in different configurations the first one is a dell pow eredge server put in an air conditioned room as a stan dalone system it has intel chipset and two dual core 0ghz intel xeon processors each has a shared way set associative cache and each core of the processor has a private instruction cache and a private data cache the ma chine has two fully buffered dimm fb dimm as the main memory the second machine is an intel ma chine which is instrumented for thermal and power study it has almost the same configuration as the except that it has four fb dimm on the we are able to mea sure the power consumption of fb dimm and processors and pro cessor exhaust temperature which is also the memory ambient tem perature on this system the instrumentation on the al tered the air flow inside the machine making it less stronger than as in a production machine the reported temperature readings should not be assumed as on a product machine as mentioned before we are more interested in the thermal be haviors of those machines in a constrained thermal environment thermal emergency level machine amb temp range c 84 amb temp range c 86 98 thermal running level machine dtm bw bandwidth no limit s s 0gb s dtm bw bandwidth no limit 0gb s 0gb s 0gb s dtm acg of active cores both dtm cdvfs frequency both dtm comb of cores frequency both 00ghz 67ghz 00ghz table thermal emergency levels and thermal running levels figure intel system with thermal sensors t figure the daughter card to emulate such an environment we put the intel into a hot box which allows us to set the system ambient temperature to c assuming a room temperature of c and an arbitrary c increase in system ambient temperature we put the into an air conditioned room of temperature c and artificially lower the thermal design point of amb by c in the implementation of dtm policies we use the two different machines to crosscheck our experiment results and the allows us to evaluate power and energy savings figure shows a system diagram of the server we instrumented the intel with sensors that measure volt age current and temperature of different system components the analog signals from the power and thermal sensors are routed to a custom designed daughter card that hosts an array of a d convert ers and low pass filters the daughter card is shown in figure the data from a d converters is sampled by a micro controller that stores all the digital sensor data in a local buffer the daughter card is connected to the host system through a lpc low pin count bus we have implemented a user space application that accesses the daughter card using linux lpc driver the application reads sensor data periodically and stores it to a log file in all experi ments in this paper we used a sampling rate of once per this sampling rate is sufficient given amb thermal constants and time scales of thermal management mechanisms evaluated in our stud ies the sample data are buffered inside the daughter card until the buffer is full then they are transferred to the system memory we have done an extensive evaluation to calibrate the sensors and en sure that sampling application does not introduce any overhead or artifacts in our measurements we have run benchmarks and syn thetic workloads with and without our sampling application and have never observed any measurable impact on their performance or system power consumption the two machines use the red hat enterprise linux with ker nel 2 performance data are collected by pfmon using perf mon kernel interface and libpfm library we enable the cpu hot plug remove functionality of the kernel to support core gating three types of performance statistics are collected using hardware counters numbers of retired uops cache accesses and cache misses we use the per thread mode of pfmon to collect statistics for each benchmark as discussed in section for dtm acg when one core on dual core processor is shut down two programs will share the remaining core in a round robin fashion the time slice for the sharing is by default linux kernel we also per form a sensitivity analysis by varying the time slice and the result will be shown in section 2 workloads we run multiprogramming workloads constructed from the spec benchmark the applications are compiled with intel c compiler and intel fortran compiler when the four core machines run four copies of a same application thir teen applications of spec reach higher amb temper ature than others wupwise swim mgrid applu vpr galgel art mcf equake lucas gap and apsi twelve out of the thirteen applications coincide with those selected by the simulation based study the only exception is gap to simplify the comparison between this work and the previous study we do not include gap in our experiments then we constructed eight multiprogramming workloads from these selected applications as shown in table 2 we ran all workloads twice and the differences in execution time are negligible the results of a single set of experiments are re ported some spec applications had more than one reference inputs have also run partial experiments on workloads constructed from spec table 2 workload mixes for those applications we run all inputs and count them as a single run in order to observe the long term memory temperature char acteristics we run the multiprogramming workloads as batch jobs for each workload its corresponding batch job mixes ten runs of every application contained in the workload when one program finishes its execution and releases its occupied processor a waiting program is assigned to the processor in a round robin way it is worth noting that at the end of the batch job there is small fraction of period that less than four applications running simultaneously we observed that the fraction was less than of total execution time on average we do not study dtm ts thermal shutdown in this work for the following reasons first dtm ts is a special case of dtm bw second it abruptly shuts down the whole system and makes system not running smoothly results and analysis in this section we first briefly describe the dram thermal emer gency observed on the servers we then present the performance results of the four dtm policies analyze the sources of perfor mance gain discuss the results of power saving and finally study the sensitivity of parameter selections experimental observation of dram thermal emergency we present our observation of amb temperature changes on the two server systems figure shows the amb temperature chang ing curves on the the when it runs homogeneous work loads described as follows the machine has open loop bandwidth throttling enabled by default in the chipset we disable this function for amb temperature below during the close to overheating periods c the function is enabled to limit the memory bandwidth under s for safety concern we run four copies of each program on the four cores on two processors simultane ously for each program we run a job batch with twenty copies in total to observe the amb temperature changes and report the results of the first five hundred seconds the data are collected ev ery one second the server has four dimms and the highest amb temperature among the four dimms is shown most time the third dimm has the highest amb temperature temperature changes of five selected programs are reported in the figure among them swim and mgrid are memory intensive and the other three are moderately memory intensive initially the machine is idle for a sufficiently long time for the amb tempera ture to stabilize at about c as it shows with swim and mgrid the temperature will reach in about seconds then it fluc tuates around c because of the bandwidth throttling for ma chine safety we have similar observations for other memory in tensive programs not shown the other three programs namely figure amb temperature curve for first seconds of execution galgel apsi and vpr are less memory intensive their temperatures rises in similar curves and then the temperature change patterns sta bilize under c figure shows the average amb temperatures of the when it runs the same homogeneous workloads unlike figure figure does not show memory overheating instead it shows how overheating would have happened for those workloads if the ambi ent temperature is high enough and no dtm is used the is put in a room with good air conditioning it also has a different cooling package therefore we are able to run memory intensive workloads without having the system to overheating the ambs or the dram devices additionally the server currently includes only two dimms if four dimms were used as we observed on the the amb temperature would be significantly higher than reported only the amb temperature of the first dimm is shown because it always has higher temperature than the other one the temperature sensors have noises which appear as high spikes in temperature readings which is visible in figure therefore we exclude sampling points with the highest temperatures to re move those spikes we have following observations first average amb tempera ture varies significantly across those homogeneous workloads ten programs have average amb temperature higher than c wup wise swim mgrid applu art mcf equake facerec lucas and as shown in the previous study and confirmed in our experiments using performance counters these ten programs have high miss rates consequently they have higher memory band width utilization higher memory power consumption and there fore higher amb temperatures than the other workloads four pro grams namely galgel gap and apsi have moderate memory bandwidth utilization and their average amb temperatures range between c and c the other twelve programs have small memory bandwidth utilization and their amb temperatures are be low 70 c second there are big gaps between the average and the highest amb temperatures we have found the main reason is that it takes a relatively long initial time around two hundred seconds for amb to reach a stable temperature additionally for some workloads the amb temperatures keep changing due to the pro gram phase changes in their lifespan 2 performance comparison of dtm polices figure compares the performance of the four dtm policies and a baseline execution with no memory thermal limit on the two servers as discussed in section on the we use an ar tificial thermal threshold of c to reveal the impact of memory thermal limit the no limit experiments are done without enforc figure amb temperature when memory is driven by homogeneous workloads on the without dtm control a dell b intel figure normalized running time ing that artificial tdp on the we are able to control the ambient temperature so we run the no limit experiments with an ambient temperature of c and run the other experiments with an ambient temperature of c we disable the built in bandwidth throttling feature of the chipset in the no limit experiments we have the following observations for workloads from spec first of all the results confirm that the use of sim ple bandwidth throttling dtm bw may severely downgrade the system performance on average the performance degradation is on the and on the our detailed statistics show that there is a strong correlation between the mem ory bandwidth utilization and the performance degradation for example all workloads except w and w have larger than slowdown with dtm bw while the slowdowns for w and w are and respectively on the the perfor mance counter data show that w has cache misses per microsecond which is the lowest among the eight workloads this means it is less memory intensive than others second the results also confirm that dtm acg may signifi cantly improve performance over dtm bw on average dtm acg improves the performance of workloads by on the and 2 on the the maximum im provement is 2 and respectively in comparison the previous simulation based study reports an average improve ment of using the same workloads the main source of im provement comes from the reduction on cache misses which will be detailed in section as for the difference in the results from the two servers several factors may contribute to it including the differences in cooling package memory bandwidth ambient temperature and the layout of the processors and dimms on moth erboard we also observe performance degradation of dtm acg over dtm bw on workload w which is on the and 2 on the respectively this scenario was not reported in the previous study as to be shown in figure dtm acg actually reduces the cache misses of w by on the and on the we believe that for this work load the dtm acg policy may stop processor cores too proac tively this is not a fundamental problem of the policy but in dicates that the policy may be further refined for certain types of workloads regarding dtm cdvfs we have surprising findings that are very different from the previous study on average dtm cdvfs may improve performance over dtm bw by on the and on the by contrast the previous study re ports only average improvement it is also remarkable that the scheme improves the performance of every program on ranging from to on the maximum improve ment is and only w has small performance degradation of the main reason behind the performance improvements as to be discussed in details in section is related to the thermal interaction between the processors and the memory the previous study did not consider the heat dissipation from the processor to the memory as the results indicate that factor should be significant in the dram thermal modeling and cannot be ignored in fact the performance improvement is larger on the than on the because on its motherboard the processors are physically closer to the dimms we will present more experimental results from the to support this finding we have also run two workloads from spec on w with applications milc soplex and gemsfdtd and w with libquantum lbm omnetpp and wrf the findings for workloads from still hold for them dtm bw degrades the performance by 21 and for w and w when compared with no limit respectively dtm acg improves perfor mance by and when compared with dtm bw respec tively dtm cdvfs has better performance for both workloads improving performance by and over dtm bw on the two servers respectively the performance of dtm comb is very close to that of dtm acg on average on both machines on average for spec workloads the performance of dtm comb is degraded by on and improved by 1 on compared with dtm acg the dtm comb may improve performance up to for w from spec it is remarkable that dtm comb can improve performance for w 2 w and w on when compared with no limit this is possible because we observe that for some programs the cache miss rate decreases sharply when running alone as shown later in section analysis of performance improvements by different dtm policies in this section we analyze the sources of performance improve ments by dtm acg dtm cdvfs and dtm comb when com pared with dtm bw reduction of cache misses it has been reported in the previous study that the improvement by dtm acg is mostly from the reduction of memory traffic which is from the reduction of cache misses when the shared cache is used by fewer programs cache contention is reduced and thus there will be fewer cache misses the previous study collected memory traffic data to demonstrate the correlation on our platforms we can only col lect the number of cache misses the total memory traffic con sists of cache refills from on demand cache misses cache write backs memory prefetches speculative memory accesses and other sources including cache coherence traffic nevertheless cache re fills are the majority part of memory traffic therefore the number of cache misses is a good indication of memory traffic figure shows the normalized number of cache misses on both machines we have several observations from the data first the number of cache misses changes very slightly by using dtm bw when compared with no limit this is expected be cause the number of on demand cache misses should have vir tually no change when memory bandwidth is throttled second the total number of cache misses does decrease significantly by dtm acg compared with that of dtm bw the reduction is up to 2 and on the and the respec tively the average reduction are and 29 respectively the result confirms the finding of the previous study that dtm acg reduces cache misses significantly on the other hand dtm cdvfs does not cause any visible changes of the total num ber of cache misses while the previous study reported memory traffic may be reduced due to the reduction of speculative mem ory accesses the difference is likely related to differences in the processor models particularly how many outstanding memory ac cesses are allowed and whether a speculative memory instruction is allowed to trigger an access to the main memory the dtm comb has very similar cache miss reduction as dtm acg the average reductions are and 1 on the and the respectively reduction of memory ambient temperature by dtm cdvfs as discussed earlier the performance of dtm cdvfs is compa rable to that of dtm acg in fact it is visibly better than dtm figure measured memory inlet temperature acg on the this is a surprise finding the previous study reports that dtm cdvfs has only slight performance ad vantage over dtm bw and the main benefit of dtm cdvfs was improved system power efficiency we speculated that the processor heat generation has an impact on the memory dimms which was ignored in the thermal modeling of the previous study if the processor is physically close enough to the dimms then the heat dissipation from the processor may further increase the dimm ambient temperature consequently the dimms may over heat more frequently than predicted by the thermal model in the previous study since dtm cdvfs improves the processor power efficiency it may reduce the heat generation from the processor and therefore alleviate the problem which will improve memory band width utilization if that is a significant factor then the observed performance improvement can be explained to confirm the above theory we have looked into the inside of each machine on both machines the processors and the dimms share the same set of cooling fans and the air flow first passes the processors then the dimms the processor and dimms are slightly misaligned along the cooling air flow on the on the one of the two processors is aligned with the dimms along the cooling air flow additionally the distance between the processors and the dimms is as close as about we collect the temperature readings through a sensor put on the air path between the processors and the dimms inside the such a sensor is not available on the data show a strong correlation between the memory inlet temperature difference and the performance improvement of dtm cdvfs over dtm bw figure compares the average temperature of the four dtm schemes the system ambient temperature of the is set to c as the figure shows the cooling air is heated up by about c when it reaches the memory the processor exhaust memory inlet temperature is visibly lower with dtm cdvfs or dtm comb than with dtm bw or dtm acg for workloads w 1 to w as to be discussed in section 4 dtm cdvfs and dtm comb re duce processor power consumption but dtm acg does not when compared with dtm bw workloads w and w are exceptions for w the temperature is slightly higher with dtm cdvfs than with the other schemes and for w it is between dtm bw and dtm acg on average the temperature is c 46 c c and with dtm bw dtm acg dtm cdvfs and dtm comb respectively the maximum difference is 1 c we high light that we carefully calibrated those sensors and that random sen sor noises do not affect the accuracy of average temperature those differences seem to be small however the range of working tem peratures of memory intensive workloads is less than c on that a dell b intel figure normalized numbers of cache misses figure cpu power consumption server as shown in figure 4 therefore a one degree difference can have a noticeable impact on performance the result strongly suggests that the layout design of server inside should give more attention to the memory subsystem 4 comparison of power and energy consumption on the we are able to measure the power consump tion of individual system components including the processors dimms system fans and other components power consumption of processors and dimms we are only interested in the power consumption of the processors and dimms because for our workloads the power consumption of the other components is almost constant the processors consume slightly more than a third of the system power and the dimms consume slightly less than a third in our experiments we also found that the power consumption of the dimms is very close for all workloads except workload w which is less memory intensive than the oth ers part of the reason is that static power is a large component of fb dimm power therefore we only compare the processor power consumption figure shows the average power consumption with different dtm policies the data are normalized to those of dtm bw as expected dtm cdvfs and dtm comb consume less proces sor power than the other two policies on average the processor power consumption of dtm cdvfs and dtm comb is and 2 lower than that of dtm bw respectively there is a figure normalized energy consumption of dtm policies very small difference between the power consumption by dtm bw and dtm acg this is mainly due to the fact that the lat est generation processors are packed with a number of energy effi cient features they apply extensive clock gating to idle functional blocks when processors are stalled by the long latency memory ac cesses thus for memory intensive workloads with frequent last level cache misses most functional components in the processor core have already been clock gated yielding little additional benefit from gating the entire core energy consumption figure shows the total energy con sumption of processors and memory all values are normalized to those of dtm bw on average compared with dtm bw dtm acg dtm cdvfs and dtm comb can save energy by and respectively the energy saving of dtm acg comes from the reduction of running time because its power con sumption is very close to that of dtm bw the energy savings for dtm cdvfs and dtm comb come from both power saving and reduction of running time sensitivity analysis of dtm parameters ambient temperature the performance of dtm policies shown in section 1 on the is from the experiments with a system ambient temperature of c and an amb tdp of c we also have run experiments on with a lower system ambient temperature of c and with an artificial amb thermal threshold of c this setting is the same as that used on the and has the same gap 64 c between the ambient figure normalized running time on intel at a room system ambient temperature c temperature and the tdp temperature as the first set of experiments on the the experiment has two purposes first by keeping the temperature gap the same while changing the ambient temperature the new result will help understand how the ambient temperature affects performance second because the performance improvements are different on the two servers the new result may reveal whether the difference is related to their differences in am bient temperatures figure compares the performance of four policies on in the new setting it indicates that the performance is very similar to that on the same machine with higher system ambient temper ature of c on average dtm bw degrades performance by over no limit the degradation is with the higher ambient temperature on average dtm acg and dtm cdvfs improve performance by 1 and 4 over dtm bw respec tively the improvements are 2 and with an ambient temperature of c figure b respectively the performance comparison regarding individual workload are also similar the similarity indicates that the performance of dtm schemes is strongly correlated to the gap between the ambient temperature and amb tdp processor frequency in previous experiments we run pro cessor cores at full speed ghz for dtm bw and dtm acg we also want to see what happens if a lower processor speed 2 ghz is used figure compares the performance with two pro cessor speeds for dtm bw and dtm acg on the first on average the performance with the lower processor speed is degraded by and compared with that with the higher speed for dtm bw and dtm acg respectively we find that the less memory intensive workload w has larger performance degradation than the others this is expected since the perfor mance of compute intensive workloads is more sensitive to proces sor frequency isci et al also present that the performance degrada tion is small for memory intensive workloads with low frequency mode if w is excluded the performance degradation is only 2 1 and for dtm bw and dtm acg respectively second dtm acg improves performance similarly under both modes on average the performance improvement is with the lower processor speed and is 2 with the higher speed re spectively when w is excluded the average performance im provement is and 4 respectively dtm tdp and thermal emergency levels figure shows the normalized running time averaged on all workloads on when the thermal design point tdp of amb changes the thermal emergency levels also change with the amb tdps figure comparison of performance between dtm acg and dtm bw under two different processor frequencies on intel figure normalized running time averaged for all workloads on with different amb tdps following the rationales discussed in section the performance of three amb tdps is shown c c and c as ex pected the performance loss is reduced with higher tdps com pared with that of no limit the performance of dtm bw is de graded by and with amb tdps of 88 c c and c respectively the performance improvement by three policies over dtm bw is similar under different amb tdps the performance improvement by dtm acg is and respectively they are and by dtm cdvfs and 2 1 and by dtm comb respec tively this result indicates that the three policies may work simi larly in systems with different thermal constraints switching frequency in linux scheduling for dtm acg in dtm acg two programs may share a processor core when an other core is disabled the time quantum used in process schedul ing is set to in the kernel by default figure compares the normalized running time and number of cache misses averaged for all workloads on with different time quantum settings the running time and number of cache misses are normalized to those with default time quantum for each workload the re sults show that the average normalized running time does not have visible changes when the base time quantum is longer than when it is set to a value shorter than both running time and number cache misses increase steadily the average running time is increased by 4 2 and 2 when the base time quantum is set to and respectively we find that the major reason figure normalized running time and number of cache misses averaged for all workloads on with different switching frequen cies for the performance degradation is the increase of cache misses the average number of cache misses is increased by and respectively this indicates that to avoid cache thrashing with dtm acg the default time slice cannot be shorter than for the processors with cache used in our experiments conclusion we have performed the first study of software dynamic thermal management dtm of memory subsystems on multicore systems running linux os it has validated the effectiveness of memory dtm methods in real systems with a new finding on the thermal interaction between the processor and memory in real machines future work includes the correction of the previous memory ther mal model more evaluation using commercial workloads and varying thermal constraints and the design of new memory dtm policies using control methods and additional inputs a large portion of the power budget in server environ ments goes into the i o subsystem the disk array in par ticular traditional approaches to disk power management involve completely stopping the disk rotation which can take a considerable amount of time making them less use ful in cases where idle times between disk requests may not be long enough to outweigh the overheads this pa per presents a new approach called drpm to modulate disk speed rpm dynamically and gives a practical imple mentation to exploit this mechanism extensive simulations with different workload and hardware parameters show that drpm can provide significant energy savings without com promising much on performance this paper also discusses practical issues when implementing drpm on server disks keywords server disks power management 1 introduction data centric services file and media servers web and e commerce applications and transaction processing sys tems to name a few have become commonplace in the computing environments of large and small business en terprises as well as research and academic institutions in addition other data centric services such as search engines and data repositories on the internet are sustaining the needs of thousands of users each day the commercial conse quences of the performance and or disruption of such ser vices have made performance reliability and availability the main targets for optimization traditionally however power consumption is increasingly becoming a major con cern in these systems 4 2 optimizing for power has been understood to be important for extending battery life in embedded mobile systems it is only recently that the im portance of power optimization in server environments has gained interest because of the cost of power delivery cost of cooling the system components and the impact of high operating temperatures on the stability and reliability of the components several recent studies have pointed out that data cen ters can consume several mega watts of power it has this research is supported in part by several nsf grants including and been observed that power densities of data centers could grow to over watts per square foot and that the ca pacity of new data centers for could require nearly twh around per year a considerable portion of this power budget on these servers is expected to be taken up by the disk subsystem wherein a large number of disks are employed to handle the load and storage ca pacity requirements typically some kind of i o paral lelism raid 30 is employed to sustain the high band width throughput needs of such applications which are in herently data centric while one could keep adding disks for this purpose at some point the consequent costs of in creasing power consumption may overshadow the benefits in performance disks even when idle spinning but not performing an operation can drain a significant amount of power for instance a server class ibm ultrastar disk is rated at w compare this to an intel xeon pro cessor clocked at 1 6 ghz which is rated at w when we go to specific server configurations e g a 4 way intel xeon smp clocked at 1 6 ghz with disks drawn from the disks consume times more power than the processors one possible solution is to use a large cache under the assumption that the i o workload will exhibit good locality caching can also potentially be used to delay writes as pro posed by colarelli et al 6 for archival and backup systems however in most servers though large caches are common they are typically used for prefetching to hide disk latencies since not all server workloads exhibit high temporal locality to effectively use the cache prefetching does not reduce the power consumption of the disks another way of alleviating this problem is by shutting down disks or at least stop them from spinning since the spindle motor for the disks consume most of the power as will be described in section many disks offer different power modes and one could choose to transition them to a low power mode when not in use idle which achieves this functionality e g stop the spinning such techniques have been effectively used 38 29 13 in lap top environments where the goal is mainly to save battery energy when in a low power mode the disk needs to be spun up to full speed before a request can be serviced and this latency is much more critical in servers than in a laptop setting further the application of such traditional mode control techniques for server environments is challenging where one may not have enough idleness and where perfor mance is more critical from the above discussion we see two extremes in op eration one that is performance efficient with disks spin ning all the time and the other where the goal is power optimization by stopping the spinning of the disk whenever there is a chance at the cost of performance in this paper we present a new option dynamic rotations per minute drpm where one could choose to dynamically operate between these extremes and adaptively move to whichever criterion is more important at any time the basic idea is to dynamically modulate the speed at which the disk spins rpm thereby controlling the power expended in the spin dle motor driving the platters slowing down the speed of spinning the platters can potentially provide quadratic with respect to the change in rpm power savings however a lower rpm can hurt rotational latencies and transfer costs when servicing a request at best linearly in addition to these rotational latencies and transfer costs disk accesses incur seek overheads to position the head to the appropriate track and this is not impacted by the rpm consequently it is possible to benefit more from power than one may lose in performance from such rpm modulations this drpm mechanism provides the following benefits over the tradi tional power mode control techniques referred to as tpm in this paper since tpm may need a lot more time to spin down the disk remain in the low power mode and then spin the disk back up there may not be a sufficient duration of idleness to cover all this time without delaying subse quent disk requests on the other hand drpm does not need to fully spin down the disk and can move down to a lower rpm and then back up again if re quired in a shorter time rpm change costs are more or less linear with the amplitude of the change the system can service requests more readily when they arrive the disk does not necessarily have to be spun back up to its full speed before servicing a request as is done in tpm one could choose to spin it up if needed to a higher speed than what it is at currently taking lower time than getting it from rpm to full speed or ser vice the request at the current speed itself while opt ing to service the request at a speed less than the full speed may stretch the request service time the exit la tency from a lower power mode would be much lower than in tpm drpm provides the flexibility of dynamically choos ing the operating point in power performance trade offs it allows the server to use state of the art disks fastest in the market and provides the ability to mod ulate their power when needed without having to live with a static choice of slower disks it also provides a larger continuum of operating points for servicing re quests than the two extremes of full speed or rpm this allows the disk subsystem to adapt itself to the load imposed on it to save energy and still provide the performance that is expected of it the drpm approach is somewhat analogous to volt age frequency scaling 31 in integrated circuits which pro vides more operating points for power performance trade offs than an on off operation capability a lower voltage usually accompanied with a slower clock for letting cir cuits stabilize provides quadratic power savings and the slower clock stretches response time linearly thus provid ing energy savings during the overall execution this is the first paper to propose and investigate a similar idea for disk power management the primary contribution of this paper is the drpm mechanism itself where we identify already available tech nology that allows disks to support multiple rpms more importantly we develop a performance and power model for such disks based on this technology showing how costs for dynamic rpm changes can be modeled the rest of this paper looks at evaluating this mechanism across different workload behaviors we first look at how well an optimal algorithm called that pro vides the maximum energy savings without any degrada tion in performance performs under different workloads and compare its pros and cons with an optimal version of tpm called which provides the maximum power savings for tpm without any degradation in perfor mance when the load is extremely high i e there are very few idle periods there is not much that can be done in terms of power savings if one does not want to compro mise at all on performance regardless of what technique one may want to use at the other end of the spectrum when there are very large idle periods we find provid ing good power savings as is to be expected since it com pletely stops spinning the disks as opposed to which keeps them spinning albeit at a slow speed how ever there is a wide range of intermediate operating condi tions when drpm turns out to give much better upto savings in the idle mode energy consumption even if one does not wish to compromise at all on performance it is also possible to integrate the drpm and tpm approaches wherein one could use tpm when idle times are very long and drpm otherwise finally this paper presents a simple heuristic that dy namically modulates disk speed using the drpm mecha nism and evaluates how well it performs with respect to where one has perfect knowledge of the future one could modulate this algorithm by setting tolerance lev els for degradation in response times to amplify the power savings we find that this solution comes fairly close to the power savings of which does not incur any response time degradation without significant penalties in response time and can sometimes even do better in terms of power savings the rest of this paper is organized as follows the next section gives an overview of the sources of energy con sumption in a disk and prior techniques for power optimiza tion section presents the drpm mechanism and the cost models for its implementation section 4 gives the experi mental setup and section gives results with comparing its potential with tpm and conducts a sensitiv ity analysis the details of our heuristic for online speed setting and its evaluation are given in section 6 section discusses some issues that arise when implementing a real drpm disk finally section 8 summarizes the contribu tions of this paper 2 disk power and tpm there are several components at the disk that contribute to its overall power consumption these include the spindle motor that is responsible for spinning the platters the actu ator that is responsible for the head movements seeks the electrical components that are involved in the transfer op erations the disk cache and other electronic circuitry of these the first two are mechanical components and typically overshadow the others and of these the spindle motor is the most dominant studies of power measurements on differ ent disks have shown that the spindle motor accounts for nearly of the overall idle power for a two platter disk and this can be as high as 81 34 for a ten platter server class disk consequently traditional power manage ment techniques at the higher level focus on addressing this issue by shutting down this motor when not in active use disk power management has been extensively studied in the context of single disk systems particularly for the lap top desktop environment many current disks offer differ ent power modes of operation such as active when the disk is servicing a request idle when it is spinning and ibm family of server disk drives ultrastar not serving a request and one or more low power modes that consume less energy than idle where the disk platters do not spin managing the energy consumption of the disk consists of two steps namely detecting suitable idle periods and then spinning down the disk to a low power mode when ever it is predicted that the action would save energy de 8 6 4 2 wds ultrastar xp dfhs sxx ultrastar xp dfms sxx tection of idle periods usually involves tracking some kind of history to make predictions on how long the next idle pe 3000 7000 disk rotation speed rpm riod would last if this period is long enough to outweigh spindown spinup costs the disk is explicitly spun down to the low power mode when an i o request comes to a disk in the spundown state the disk first needs to be spun up to service this request incurring additional exit latencies and power costs in the process many idle time predictors use a time threshold to find out the duration of the next idle pe riod a fixed threshold is used in wherein if the idle period lasts over 2 seconds the disk is spun down and spun back up only when the next request arrives the thresh old could itself be varied adaptively over the execution of the program a detailed study of idle time predic tors and their effectiveness in disk power management has been conducted in lu et al 26 provide an experimen tal comparison of several disk power management schemes proposed in literature on a single disk platform we broadly refer to these previous power mode control mechanisms as tpm in this paper it is to be noted that tpm has the disk spinning at either its full speed or fully stationary and does not allow intermediate rpms another power saving approach though orthogonal to this work is to replace a single disk with multiple smaller form factor disks that consume lower power as in dynamic rpm drpm the tpm techniques and our drpm mechanism can be used in conjunction with other techniques that can re duce disk accesses by aggregation and or caching using hardware os application support or place data to reduce head movements which can save actuator power to further the power savings however as was mentioned ear lier the spindle motor power needed to spin the disks is still the major power consumer which is expended even when the disk is not serving a request and is spinning as can be seen in figure 1 which shows the rpms and power consumption of different ibm server class disks over the years there appears to be a strong correlation between the rotational speed and the idle power in these disks though it should be noted that rpm is not the only variation across the technologies employed this motivates us to investi gate the possibility of modulating the rpm dynamically to adjust the power consumption 3 1 basics of disk spindle motors a detailed exposition of disk spindle motors spms can be found in 34 disk spms are permanent magnet dc brushless motors in order to operate as a brushless motor sensors are required inside the motor to provide the pulses necessary for commutation i e rotation these sensors may either be hall effect sensors or back emf sensors speed control of the motors can be achieved by using pulse width modulation pwm techniques which make use of the data from the sensors figure 1 ibm server disks idle power con sumption for each disk the form factor was fixed at 3 and the largest capacity config uration was chosen the idle power is rela tively independent of the form factor 34 we found that the idle power was not that strongly related to the capacity for instance two other ibm disks the ultrastar and the ultrastar are both 000 rpm disks with 1 and 2 gb capacity respectively while their idle power consumption is w and 3 w respectively a large accelerating torque is first needed to make the disks start spinning this high torque is essentially required to overcome the stiction forces caused by the heads sticking to the surface of the disk platter the use of technologies like load unload can ameliorate this problem by lift ing the disk arm from the surface of the platter these tech nologies also provide power benefits and are used for exam ple in ibm hard disk drives to implement the special idle 3 mode in addition to providing the starting torque the spm also needs to sustain its rpm once it reaches the intended speed one traditional approach in improving disk performance over the years has been to increase the rpm which re duces rotational latencies and transfer times which can prove beneficial in bandwidth bound applications how ever such increases can cause concern in additional is sues such as noise and non repeatable run outs nrros nrros are off track errors that can occur at higher rpms especially at high track densities these design considera tions in the development of high rpm disks have been ad dressed by the use of advanced motor bearing technologies like fluid 1 16 and air bearings 37 however the power associated with high rpms still re mains and this paper focuses on this specific aspect 3 2 analytical formulations for motor dynamics our drpm solution dynamically controls the spindle motor to change the rpm of the spinning platters the rpm selection capability can be provided by allowing the spindle motor control block of the hard disk controller to be programmable for example the desired rpm can be input via a programmable register in the hard disk controller the value read from this register can inturn be used by the spindle motor driver 3 to generate the requi site signals for operating the disk at that rpm we now present the time overhead needed to effect an rpm change and the power of the resulting state as a func tion of the rpm 3 2 1 calculating rpm transition times in order to calculate the time required for a speed change we need some physical data of the spindle motor this in formation for a specific disk is usually proprietary but there are dc brushless motors commercially available that we can use for this purpose we have obtained the necessary in formation from the datasheet of a maxon ec mm flat brushless permanent magnet dc motor 27 whose physi cal characteristics closely match those of a hard disk spindle motor table 1 summarizes the basic mechanical character istics of this motor spm current profile of multimode harddisk drive spm speed rpm figure 2 current drawn by sony multimode hard disk table 1 maxon ec motor characteristics the motor specifications give a formula for calculating the time in ms required for a speed change of rpm with a load inertia as the load on the spindle motor is the platter assembly we dismantled a 3 quantum hard disk and measured the weight of an individual platter using a sensitive balance and also its radius its weight was found to be 65 gm and radius was 4 7498 cm using these values and assuming platters per disk as in though we also have sensitivity results for different number of platters we calculated the moment of inertia of the load in gcm as where is the number of platters therefore we have 1 this shows that the time cost of changing the rpm of the disk is directly proportional linear to the amplitude of the rpm change 3 2 2 calculating the power consumption at an rpm level we briefy explain the dependence between the power con sumption of a motor and its rotation speed a detailed expo sition of this topic can be found in the motor voltage is related to the angular velocity rotation speed as where is a constant the power consumed by the motor is where r is the resistance of the motor therefore we have 2 this equation similar to that relating the power and voltage for cmos circuits indicates that a change in the rotation speed of the disk has a quadratic effect on its power con sumption in order to investigate whether this relationship holds true in the context of a hard disk we used an exper imental curve fitting approach there exists a commercial hard disk today the multimode hard disk drive from sony that indeed supports a variable speed spindle motor the speed setting on such a disk is accomplished in a more static pre configured fashion rather than modulating this during the course of execution the published current con sumption values of this disk for different rpm values pro vides insight on how the current drawn varies with the rpm figure 2 shows the current drawn by the spm of the mul timode hard disk repeated from 28 a simple curve fit of these data points clearly shows this quadratic relationship this relationship may appear somewhat different from the trends shown in figure 1 where one could argue that the relationship between rpm and power is linear however figure 1 shows the trend for disks of different generations where it is not only the rpm that changes but the hardware itself on the other hand equation 2 and the multimode hard disk current consumption profile figure 2 shows the relation between these two parameters is more quadratic for an individual disk drive this multimode disk is composed of only two platters while we are looking at server class disks that have several more platters 8 platters consequently we cannot di rectly apply this model to our environment on the other hand a study from ibm 34 projects the relation between idle power and rpm for 3 server class ibm disks in this study other design factors such as the change in the number of disk platters have been considered besides just the rpm to make the projections for the individual disks and we de pict the results from there by the points shown in figure 3 in our power modeling strategy for a variable rpm disk we employed two approaches to capture a quadratic and linear relationship respectively we took the points from the ibm study and used a quadratic curve to approximate their behavior as is shown by the solid curve in figure 3 to model the idle power as 3 drpm vs ibm data 25 tpm including the power consumption of each mode and the transition costs are illustrated in figure 4 we have considered several rpm operating levels i e different resolutions for stepping up down the speed of the spindle motor these step sizes are as low as rpm providing 13 steps between the extremes of and rpm rpm levels in all the default configuration that we use in our experiments is a disk raid array with a quadratic drpm power model and a step size of rpm 7000 9000 spm speed rpm figure 3 comparison of drpm model to the ibm projections we also performed a linear least squares fit through the points as is shown by the dotted line in figure 3 to model the idle power as parameter value parameters common to tpm and drpm 4 we have used both models in our experiments to study the potential of drpm in general we find that the results are not very different in the ranges of rpms that were stud ied between to as is evident even from figure 3 which shows the differences between linear and quadratic models within this range are not very significant equations 1 and 3 4 provide the necessary information in modeling the rpm transition costs and power character istics of our drpm strategy for the power costs of tran sitioning from one rpm to another we conservatively as sume that the power during this time is the same as that of the higher rpm state 4 experimental setup and workload de scription we conducted our evaluations using the disksim 8 simulator modeling a disk array for a server environ ment disksim provides a large number of timing and configuration parameters for specifying disks and the con trollers buses for the i o interface the simulator was aug mented with power models to record the energy consump tion of the disks when performing operations like data transfers seeks or when just idling our drpm implemen tation accounts for the queuing and service delays caused by the changes in the rpm of the disks in the array the default configuration parameters used in the simulations are given in table 2 many of which have been taken from the data sheet of the ibm ultrastar server hard disk the power consumption of the standby mode was calcu lated by setting the spindle motor power consumption to when calculating based on the method described in section 3 note that this value for the power consumption is very aggressive as the actual power consumption even in this mode is typically much higher for example its value is 72 w in the actual ultrastar disk however as we shall later show even with such a deep low power standby mode that is used by tpm drpm surpasses tpm in terms of energy benefits in several cases also in our power models we have accounted for the case that the power penalties for the active and seek modes in addition to idle power also depend upon the rpm of the disk the modes exploited by drpm specific parameters table 2 simulation parameters with the de fault configurations underlined disk spinups and spindowns occur from to rpm and vice versa respectively figure 4 tpm power modes since we want to demonstrate the potential of the drpm mechanism across a spectrum of operating conditions dif ferent loads long idle periods bursts of i o requests etc that server disks may experience and to evaluate the pros and cons of drpm over other power saving approaches we chose to conduct this study with several synthetic work loads where we could modulate such behavior the syn thetic workload generator injects a million i o requests with different inter arrival times and request parameters start ing sector request size and the type of access read write all the workloads consist of read requests and of all requests are sequential in nature these characteristics were chosen based on 33 since a closed system simula tion may alter the injected load based on service times of the disk array for previous requests we conducted an open system simulation with these workloads we considered two types of distributions for the inter arrival times namely exponential and pareto as is well understood exponential arrivals model a purely random poisson process and to a large extent models a regular traffic arrival behavior without burstiness on the other hand the pareto distribution introduces burstiness in ar rivals which can be controlled the pareto distribution is characterized by two parameters namely called the shape parameter and called the lower cutoff value the smallest value a pareto random variable can take 1 we chose a pareto distribution with a finite mean and infinite breakdown of energy consumption exp exp par par workload configuration variance for both distributions we varied the mean inter arrival time in ms as a parameter in pareto there are different ways by which the traffic can be generated for a given mean we set the to 1 ms and varied i e when the mean is increased the time between the bursts idleness tend to increase we use the term workload to define the combination of the distribution that is being used and the mean inter arrival time for this distribution for instance the workload par denotes a pareto traffic with a mean inter arrival time of ms we compare the schemes for each workload using three metrics namely total energy consumption over all the re quests idle mode energy consumption over all the re quests and response time per i o request t these can be defined as follows the total energy consumption is the energy con sumed by all the disks in the array from the beginning to the end of the simulation period we monitor all the disk activity states and their duration in each state and use this to calculate the overall energy consump tion by the disks integral of the power in each state over the duration in that state the idle mode energy consumption is the en ergy consumed by all the disks in the array while not servicing an i o request i e while not performing seeks or data transfers this value is directly im pacted by the spinning speed of the spindle motor the response time is the time between the request submission and the request completion averaged over all the requests this directly has a bearing on the de livered system throughput finally we use the terms power and energy inter changeably sometimes power optimization without performance degradation 1 energy breakdown of the workloads before we examine detailed results it is important to un derstand where power is being drained over the course of execution i e when the disk is transferring data active or positioning the head positioning or when it is idling idle figure gives the breakdown of energy consumption of two workloads from each of the inter arrival time distributions one at high and another at low load conditions into these three components when there is no power saving technique pareto probability distribution function is given by the mean is given by figure breakdown of for the differ ent workloads on the x axis each pair rep resents a workload defined by probability distribution mean inter arrival time pair employed the high and low loads also indicate that idle periods are low and high respectively as is to be expected when the load is light exp par the idle energy is the most dom inant component however we find that even when we move to high load conditions exp par the idle energy is still the most significant of the three while the positioning energy does become important at these high loads the results suggest that most of the benefits to gain are from optimizing the idle power in particular the spindle motor component which consumes 81 34 of this power consequently our focus in the rest of this section is on the idle power component by looking at how different schemes tpm and drpm exploit the idleness for energy savings 2 the potential benefits of drpm the power saving either with tpm or drpm is based on the idleness of disks between serving requests while in the latter case it is possible to get more savings by serv ing requests at a lower rpm this may result in perfor mance degradation in the first set of results we do not consider this to be an option i e we define a scheme called whose performance is not any different from the original disk subsystem which does not employ any power management technique further to investigate what could be the potential of drpm we assume the existence of an idle time prediction oracle which can exactly predict when the next request will arrive after serving each request consequently uses this prediction to find out how low an rpm it can go down to and then come back up to full speed before servicing the next request noting the times and energy required for doing such transitions to be fair the same oracle can be used by tpm as well for effecting power mode transitions and we call such a scheme where the disk is transitioned to the standby mode if the time to the next request is long enough to accommodate the spindown followed by a spinup note that can exploit much smaller idle times for power savings compared to on the other hand when the idle time is really long can save more energy by stopping the spinning completely while drpm can take it down to only rpm there fore in order to investigate the potential benefits if both these techniques were used in conjunction we have also considered a scheme called in the scheme we use the oracle to determine which of the two exponential traffic 80 70 30 pareto traffic 30 spindle motor which has to spin them as was described ear lier in figure 7 the effect of three different platter counts 4 and 16 has been shown for the two types of traf fic with different load conditions it can be seen that as the number of platters increases the savings drop this is because a larger weight is imposed on the spindle motor re quiring a higher torque for rpm changes thereby incurring more overheads nevertheless even at the 16 platter count which is significantly higher than those in use today we still find appreciable power savings even at high load conditions inter arrival time ms 25 75 inter arrival time ms sensitivity to platter count exponential traffic sensitivity to platter count pareto traffic 26 figure 6 savings in idle energy using and are pre sented for the quadratic power model 16 14 techniques saves the maximum energy for each idle time period we would like to point out that and do not put a bound on the energy savings inter arrival time ms 12 25 75 inter arrival time ms that one can ever get rather they give a bound when per formance cannot be compromised figure 6 presents the idle energy savings which was shown to be the major con tributor of overall energy for these schemes as a function of the inter arrival times in the two distributions when we first examine the exponential traffic results we note that the results confirm our earlier discussion wherein large inter arrival times favor at the other end of the spectrum when inter arrival times get very small there is not really much scope for any of these schemes to save energy if performance compromise is not an option how ever between these two extremes we find that provides much higher savings than it finds more idle time opportunities to transition to a lower rpm mode which may not be long enough for tpm as is to be ex pected the combined scheme approaches the better of the two across the spectrum of workloads when we next look at the pareto traffic results we find that the arrivals are fast enough due to burstiness of this distribution even at the higher mean values considered that consistently outperforms in the range under consideration it is also this reason that makes the energy savings of all the schemes with this traffic distri bution lower than that for exponential where the idle times are less varying the purpose of this exercise was to examine the potential of drpm with respect to tpm while not compromising on performance the rest of this section looks to understand ing the sensitivity of the power savings with this approach to different hardware and workload parameters since the sensitivity of drpm is more prominent at the intermediate load conditions where it was shown to give better savings than tpm we focus more on those regions in the rest of this paper 5 3 sensitivity analysis of 5 3 1 number of platters disks show significant variability in platter counts at one end the laptop disks have 1 or 2 platters while server class disks can have as many as 8 platters the number of platters has a consequence on the weight imposed on the figure 7 sensitivity to number of platters in the disk assembly while one may think that the need for storage capacity increase over time may necessitate more platters it is to be noted that this increase is usually achieved by denser media rather than by adding more platters for instance the ibm dfhs sxx and have storage capacities of 4 51 gb 7 gb and gb respec tively but their platter counts are 8 and 6 therefore we do not expect platter counts to increase significantly 5 3 2 quadratic vs linear power model as was discussed in section 3 we considered both quadratic and linear scaling models for the idle power consumption of the spindle motor at different rpms while the earlier results were presented with the quadratic model we com pare those results with the savings for with the linear model in figure 8 we can observe that the differ ences between these two models are not very significant though the linear model slightly under performs that of the quadratic as is to be expected this again confirms our ear lier observations that the differences between a linear and quadratic model are not very different across these ranges of rpm values consequently we find that even with a conservative linear power scaling model gives better energy savings than compare with figure 6 we have also conducted similar sensitivity analysis for other factors such as the step size employed for the spindle motor and the type of raid configuration the interested reader is referred to for the details in general we find that can provide significant energy sav ings across a wide spectrum of disk and array configura tions 6 a heuristic drpm algorithm having evaluated the potential of drpm without any performance degradation which requires an idle time pre ut previous n requests current n requests ut lt 5 ut lt 5 t1 ms lt t2 ms choice of low watermarks choice of low watermarks 10800 diff current ut for next n requests do nothing 4800 a b c figure the operation of the drpm heuristic for and in each figure for the choice of low watermarks the dotted line shows where low wm is before the heuristic is applied and the solid line shows the result of applying the scheme the percentage difference in the response times and between successive request windows is calculated a if then low wm is set to the maximum rpm for the next requests b if lies between the two tolerance limits the current value of low wm is retained c if then the value of low wm is set to a value less than the maximum rpm since is higher than of but lesser than 75 of in this example it is set two levels lower than the previous low wm if it was between 75 and 87 5 it would have been set three levels lower and so on exponential traffic 80 70 30 inter arrival time ms pareto traffic 30 5 25 75 inter arrival time ms sponse times to find points when performance degradation becomes more significant to ramp up the disks or to limit how low they can operate at those instants the array controller tracks average response times for request windows at the end of each window it calculates the percentage change in the response time over the past two windows if this percentage change is larger than an upper tolerance level then the con troller immediately issues a command to all the disks that are operating at lower rpms to ramp up to the full speed this is done by setting the low watermark at each disk to the full rpm which says that the disks are not supposed to operate below this figure 8 behavior of for a power model that relates the rpm and linearly diction oracle we next move on to describe a scheme that can be used in practice to benefit from this mechanism the goal is to save energy using the multiple rpm levels with out significantly degrading performance response time in this scheme i the array controller communicates a set of operating rpm values to the individual disks based on how performance characteristics response time of the workload evolve more specifically the controller speci fies watermarks for disk rpm extremes between which the disks should operate ii subsequently each disk uses local information to decide on rpm transitions periodically each disk inspects its request queue to check the number of requests waiting for it if this num ber is less than or equal to a specific value this can indicate a lower load and the disk ramps down its speed by one step it can so happen that over a length of time the disks may gradually move down to a very low rpm even with a high load and do not move back up consequently it is important to periodically limit how low an rpm the disks should be allowed to go to this decision is made by the array controller at the higher level which can track re value between an upper and lower tolerance level the controller keeps the at where it is since the response time is within the tolerance levels less than the lower tolerance level in which case the can be lowered even further the spe cific rpm that is used for the is calcu lated proportionally based on how much the response time change is lower than these three scenarios are depicted in figure which shows the choice of the for example differences in re sponse time changes with and eight possible values for the these are also the val ues used in the results to be presented and window sizes are though we have experimented with a more comprehensive design space in our experiments we set whereby the disks initiate a rampdown of their rpm based on whether their request queue is empty or not 6 1 results with drpm we have conducted extensive experiments to evaluate how well the above heuristic denoted as simply drpm in the rest of this paper fares not only in terms of its abso lute energy savings and response time degradation but also comparing it to the and static rpm choices where non drpm disks of lower rpms are used the complete set of experimental results is given in and we present the highlights here the first set of results in figure show the energy sav ings and response time degradation of our drpm heuristic with respect to not performing any power optimization re ferred to as baseline the energy savings are given with both the quadratic and linear power models discussed ear lier for two different inter arrival times in each of the two distributions note that these are savings and not just those for the idle energy ergy compared to the above heuristic which allows lower rpms for serving requests and also can incur higher transi tion costs in always getting back to the highest rpm these effects are more significant at higher loads smaller idle pe riods causing our heuristic to in fact give better energy savings than at lighter loads the long idle periods amortize such costs and the knowledge of how long they are helps transition directly to the appro priate rpm instead of lingering at higher rpms for longer times as is done in the heuristic scheme still the energy savings for the heuristic are quite good and are not far away from which has perfect knowledge of idle times the results for the heuristic have been shown with different choices for the window of requests for which the is recalculated a large window performs energy savings quadratic model 30 exp exp par par workload configuration exponential traffic ms mean energy savings linear model 30 exp exp par par workload configuration exponential traffic ms mean modulations at a coarser granularity thus allowing the disks to linger at lower rpms longer even when there may be some performance degradation this can result in greater energy savings for larger values as is observed in many cases the response time characteristics of the heuristic are shown as cdf plots in figure rather than as an av erage to more accurately capture the behavior through the execution it can happen that a few requests get inordi nately delayed while most of the requests incur very lit tle delays a cdf plot which shows the fraction of re quests that have response times lower than a given value on the x axis can capture such behavior while a simple av erage across requests cannot these plots show the base 1 baseline drpm 1 baseline drpm line behavior which is the original execution without any power savings being employed and is also the behavior of 9 8 7 6 5 4 3 2 1 drpm drpm 9 8 7 6 5 4 3 2 1 drpm drpm which does not alter the timing behavior of re quests the closeness of the cdf plots of the heuristic to the baseline curve is an indication of how good a job it does of limiting degradation in response time at higher loads it is more important to modulate the rpm levels at a finer granularity to ensure that the disks do not keep going down in rpms arbitrar ily we see that a finer resolution requests does tend to keep the response time cdf of the heuristic 5 120 response time ms pareto traffic ms mean 1 baseline drpm 5 20 120 response time ms pareto traffic 50 ms mean 1 baseline drpm close to the baseline in par and par 50 one can hardly discern differences between the baseline and the cor responding heuristic results remember that the pareto traf fic has bursts of i o requests followed by longer idle peri 9 8 7 6 5 4 3 2 1 drpm drpm 9 8 7 6 5 4 3 2 1 drpm drpm ods since our heuristic modulates the based on the number of requests rather than time this modula tion is done fast enough during the bursts so that the re sponse time of those requests are not significantly compro mised and is done slow enough during the longer idle peri ods that the energy savings are obtained during those times in the exponential traffic while there are some deviations from the baseline we are still able to keep over of re quests within a 5 response time degradation margin with 5 20 60 120 response time ms 0 5 20 60 120 response time ms a window while giving over energy savings in the quadratic model changing the power model from quadratic to linear does not change the trends as was pointed figure drpm heuristic scheme results the results are presented for referred to as drpm drpm 500 and drpm respectively we observe that we can get as good savings if not better in some cases especially with higher loads than which has already been shown to give good energy savings remember that services re quests at the highest rpm even if it transitions to lower rpms during idle periods this results in higher active en out earlier and we still find over 25 energy savings 6 2 controlling ut and lt for power performance trade offs the drpm heuristic provides two additional parameters in addition to already considered ut and lt for mod ulating the rpm control by keeping ut where it is and moving lt up closer to ut we can allow the disks to transition to even lower rpm levels thereby saving even more energy without compromising significantly on perfor mance this is shown by comparing the results for ut and lt 10 in figure a with those of the results in figure 10 at least for higher loads similarly one can bring the ut parameter closer to lt to reduce response time degradation without significantly changing the energy results this is shown by comparing the results for ut 8 and lt 5 in figure b with those of the results in figure 10 this heuristic thus provides an elegant approach for determining where one wants to operate in the power performance profile niques pwm achieves speed control by switching on and off the power supply to the motor at a certain fre quency called the duty cycle the choice of duty cy cle determines the motor speed the design of such speed control mechanisms can be found in 3 head fly height the height at which the disk head slider flies from the platter surface depends on the linear velocity of the spinning platter which can be expressed as 60 50 30 20 10 0 1 0 9 0 8 0 7 0 6 0 5 0 4 0 3 0 2 0 1 energy savings quadratic model exp exp workload configuration exponential traffic 100 ms mean 50 45 35 30 25 20 10 5 0 1 0 9 0 8 0 7 0 6 0 5 0 4 0 3 0 2 0 1 energy savings quadratic model exp 100 exp workload configuration exponential traffic 100 ms mean where is the radius of the disk and is the frequency of rotation measured in rpm the fly height needs to be more or less constant over the entire range of linear velocities supported by the given spindle system the papillon slider presented in 24 is capable of maintaining this constant fly height over the range of rpms that we have considered head positioning servo and data channel design in hard disks positioning the head requires accurate information about the location of the tracks this in formation is encoded as servo signals on special servo sectors that are not accessible by normal read write operations to the disk this servo information is given to the actuator to accurately position the head over the center of the tracks the servo information needs to be sampled at a certain frequency to position the head properly as the storage density increases the number of tracks per inch tpi increases requiring higher sampling frequencies this sampling frequency is di rectly proportional to the spinning speed of the disk therefore at lower it might not be possi ble to properly sample the servo information ad dresses this problem by designing a servo system that 0 5 10 20 60 120 response time ms 0 5 10 20 60 120 response time ms can operate at both low and high disk rpms along with a data channel that can operate over the entire range of 1 0 9 0 8 0 7 0 6 0 5 0 4 0 3 0 2 0 1 exponential traffic ms mean 1 0 9 0 8 0 7 0 6 0 5 0 4 0 3 0 2 0 1 exponential traffic ms mean data rates over the different rpms the data rate of a channel is directly proportional to idle time activities server environments optimize idle periods in disks to perform other operations such as validating the disk contents and optimizing for any errors 21 the fre quencies of such operations are much lower than the idle times themselves to really have a significant con sequence on the effectiveness of power saving tech niques still it is possible that drpm may be more 0 5 10 20 60 120 200 200 response time ms 0 5 10 20 60 120 200 200 response time ms useful for such activities since it allows those per a b figure controlling ut and lt for power performance tradeoffs a presents the re sults for ut 15 lt 10 b presents the results for ut 8 lt 5 7 issues in implementing drpm disks having demonstrated the potential of drpm it is impor tant to understand some of the ramifications in its physical realization providing speed control as mentioned in section 3 speed control in dc brush less pm motors can be achieved using pwm tech formance non critical operations to be undertaken at a relatively slow rpm for energy savings while tra ditional power mode control of transitioning the disk completely to a standby state prevents such activities smart disk capabilities the anticipated smart disks 22 32 provide an ex cellent platform for implementing drpm algorithms and also provide the flexibility of modulating the algo rithm parameters or even changing the algorithm en tirely during the course of execution the effect of rpm modulation on disk reliability needs further investigation on the one hand we have been in creasing the number of disks in arrays to not only enhance performance but also for availability this in turn has ac centuated the power problem which this paper has tried to address in doing so it is conceivable that we may need more disks for hot spares in case rpm modulation can worsen mttfs this vicious cycle between perfor mance power and availability warrants a further investiga tion which we plan to undertake in the future 8 concluding remarks this paper has presented a new approach to address the growing power problem in large disk arrays instead of completely spinning down disks which can incur signifi cant time and power costs this paper proposes to modulate the rpm of disks dynamically the resulting drpm mech anism has been shown to find more scope for power savings when idle times are not very long compared to traditional power management tpm techniques that have been pro posed for laptop desktop disks in addition it also allows the option of servicing requests at a lower rpm when per formance is not very critical to provide additional power savings finally it can be combined with tpm techniques to amplify the power savings we have proposed timing and power models for the drpm mechanism and have conducted a sensitivity anal ysis of different hardware parameters in addition we have presented a heuristic that can be used in practice to bene fit from the drpm mechanism to allow trade offs between power savings and performance benefits detailed simula tions have shown that we can get considerable energy sav ings without significantly compromising on performance it is to be noted that the heuristic presented here is one simple way of using the drpm mechanism though it is conceivable that one can optimize change this further to get higher power savings or to limit the performance degradation the design and analysis of algorithms requires a model of computation such a model should faithfully reflect the phys ical processes of computation so that a programmer can dis tinguish efficient computations from inefficient ones at the same time the model must be simple enough to be tractable and general enough to continue to apply as the underlying technologies evolve from a computer architect perspective a model of computation describes what the programmer ex pects and thereby provides criteria for evaluating architectural alternatives the models of computation that are prevalent today are based on operation counting assuming sequential program execu tion these models reflect the technology of the first several decades of computing memory accesses were as fast as alus so operation count determined execution time gates were ex pensive but wires were cheap the monetary cost of computing was determined by the hardware rather than the power bill over time each of these assumptions have been overturned and yet the models of computation have remained remarkably stable this has largely been made practical through innova tions in computer architecture for example caches and su perscalar execution have hidden the cost of memory accesses now the power wall is forcing a transition to explicitly parallel architectures and software and traditional models of computation no longer reflect the actual costs of computation parallel computing offers a way around the power wall be cause cmos technology allows operations to be performed with less energy by using more time thus a parallel algo rithm may perform more operations than its sequential coun terpart yet use less time and less energy by combining volt age scaling circuit design techniques and micro architectural trade offs energy and time can be traded over ranges of sev eral orders of magnitude when these energy time trade offs are considered the optimal algorithm for a task may be one that neither minimizes operation count nor computation depth while various models have been proposed for parallel compu tation such as prams and logp ckp we are aware of no prior model that can address the questions that arise from the energy time trade offs that are at the heart of current parallel computing technologies for example how can a programmer design energy time optimal al gorithms are there fundamental advantages to heterogeneous ar chitectures e g mixing fast and slow cores what on chip interconnect topologies are required to re alize energy time optimal computations what are the trade offs between latency throughput and power consumption addressing these questions requires an energy aware model the energy time trade offs afforded by cmos technology have been long understood dg and studied in tensively by the real time systems community since the semi nal paper by yao et al this has led to many papers that examine energy trade offs for scheduling problems for uniprocessors multiprocessors with or without precedence constraints etc e g ams07 in all of these papers the set of tasks to be performed is taken as a given likewise martin considers the energy time trade offs of various concurrent decompositions for al gorithms but does not examine the energy requirements of particular computational tasks such as addition or sorting position statement to make effective use of parallelism programmers and architects must have a model of computa tion that reflects the energy time trade offs of cmos technol ogy we are aware of no prior research that has presented a model that reflects these trade offs in the remainder of this paper we present such a model we use the model to de rive lower bounds for the complexity of addition multiplica tion and sorting we show existing algorithms that meet these bounds to within constant factors and we consider future di rections for energy aware algorithm design and analysis we present some surprising results for energy constrained min imal time computation for example if the inputs of a sort ing network are required to lie along a line then slow al gorithms such as bubble sort and fast algorithms such as odd even merge sort have the same asymptotic energy time complexity our construction for an optimal adder shows that broadcasting a bit to all o mesh locations within distance d of a source uses the same energy and time to within a con stant factor as sending the bit to a single location distance d away the algorithms that we present for sorting and multi plication are energy time optimal yet they minimize neither operation count nor computation depth an energy aware model we present two variants of our model a fairly abstract lower bound model that simplifies analysis and a more de tailed upper bound that ensures that the proposed algorithms can be implemented in vlsi technology in both models computations are performed by an ensemble of processing el ements pes a pe has o inputs o outputs and o bits of state a pe can compute an arbitrary function of its inputs and state in t units of time using t units of energy in figure a perimeter i o implementation of an adder other words et is constant computations are repre sented by task graphs with vertices of the task graph mapped to pes where vertices represent atomic operations this is a many to one mapping a pe may perform multiple operations in an algorithm as long as the precedence constraints ensure that these operations are performed during disjoint time inter vals to model the planar geometry of integrated circuits and printed circuit boards the lower bound model restricts a pe to having at most other pes within distance d of itself and the triangle inequality applies to communicate a bit between pes separated by distance d using time t requires d energy in other words et d which means that one bit can be sent d units of distance using d units of time and energy in the upper bound model each pe occupies a unit square and only nearest neighbor communication is allowed thus wires are realized as chains of pes cross overs corners etc can be realized by a pe with multiple inputs and multiple out puts further details for these models are presented in and omitted here due to space constraints preliminary results we start with addition a binary adder has two n bit input words and one n bit output word that encodes the sum at the beginning of the computation each input bit is initially held by a distinct pe at a predetermined location in the plane at the end of the computation each output bit is held by a pe at a predetermined location we first consider adders where the centers of the the in put pes lie along a line and likewise for the centers of the output pes figure depicts one such adder we call such layouts perimeter i o and note that such implementations are common in practice because the carry generated by the least significant bit can affect all bits of the result we can use an adder to broadcast one input bit to all of the output bits there must be two output pes separated by distance n because the output pes lie along a line by the triangle inequality there must be an output pe that is at least distance n away from the pe holding for the least significant bit lsb of one of the operand words sending one bit distance d requires energy e and time t with et d letting e denote the total energy required to perform the addition and t denote the time we conclude that the et complexity for perimeter i o im plementations of addition is in n this bound is tight it is readily achieved by a carry ripple adder or a brent kung figure an h tree implementation of an adder carry lookahead observing that a carry ripple and a carry lookahead adder have the same asymptotic com plexity in our model we see that when communication time and energy are taken into account the n log n time advantage of a fast adder is reduced to a constant factor more efficient adders can be achieved by allowing the in put and output pes to occupy arbitrary locations in the plane we will refer to such implementations as having planar i o following an argument analogous to that for the perimeter i o adder an adder with planar i o must have some output pe that is at least distance n away from the pe for the lsb of one of the operand words the et cost of sending the input bit to that output pe is in n n to obtain the matching upper bound we consider a carry look ahead adder organized as an h tree as shown in figure let the root of the tree be at level and the leaves at level m for k an edge between levels k and k is implemented by a chain of length m k pes we set the time for pes at level k and the pes in the chain from level k to level k to grinding out the sums yields e t o n and thus et o n matching the lower bound the lower and upper bound results together show that it is possible to broadcast a bit to all pes within distance d of a source for a constant factor of the cost of sending a bit to a single pe at distance d it is straightforward to show a com plexity for addition of et n if all pes are re quired to operate with the same computation time thus for addition a heterogeneous architecture is asymptotically supe rior to an homogeneous one we note that the communication pattern for addition is the same as that for the map reduce dwarf abc this suggests that for some computations heterogeneity may provide a substantial computational advan tage the multiplication problem can be formulated in a simi lar manner as addition we have shown that perime ter i o multipliers have an et complexity in o n and we focus here on the planar i o case following the classical arguments for the at complexity of multiplication shifting can be reduced to multiplication for each input bit there are at least output bits that are at least distance n away a pigeon hole argument shows that there must be a shift for which at least input bits must be sent a dis tance of at least n this yields a lower bound of et n n this bound is achieved by preparata at optimal multiplier as viewed in our scaling or heterogeneity of pes is a requirement for the brent kung design to meet this bound this is due to the few long wires per stage that must run at a faster speed problem algorithm implementation et addition carry ripple p o n brent kung p o n kogge stone p o n carry select o n h tree o n multiplication carry save p o n preparata o n sorting bubble sort p o n odd even merge sort p o n schnorr and shamir o n algorithms marked p are implemented with perimeter i o the other algorithms have input and output pes placed throughout the implementation table summary of et complexities model we discuss the relationship between the at and our model in more detail in section we now consider the problem of sorting n binary words of w bits each as for multiplication and addition we assume that each input bit is stored in a separate pe at a predeter mined location at the beginning of the algorithm and that at the end each output bit is stored in a predetermined pe for sorting we also assume that the bits for any given input or output word are stored in contiguous pes if w n then we can construct a permutation of the input data that forces the ith input word to go to pes that are distance n i w away this leads to a lower bound of et nw a matching upper bound is achieved by a variation on schnorr and shamir mesh sorting algorithm schnorr and shamir algorithm sorts n words on an array of n n word wise compare and swap modules in o n time using only nearest neighbor communication because their compare and swap module can work on either vertically or horizontally adjacent words in unit time a w bit version must have w hor izontal and w vertical wires and occupy area o this prevents their algorithm from achieving the nw lower bound we derived above we replace each compare and swap module with a tile of w w pes and only use nearest neighbor communication our implementation takes o nw time and each step requires o nw energy this establishes et o nw and matches the lower bound table summarizes et complexities that we have derived for the problems of addition multiplication and sorting due to length limitations we have not presented all of these algo rithms here details are given in what s next we have presented a simple model that accounts for energy time trade offs in computation and used it to analyze addi tion multiplication and sorting we note that all three of these problems are highly amenable for parallel implementations for example addition with an et complexity in n can have both the time and the energy for the computation grow slower than the input size as both can grow as n like wise for both multiplication and sorting can have en ergy and time both grow at a rate that is sublinear in the prob lem size our model is reminiscent of the at models for vlsi com plexity that were studied in the bk82 in fact if an algorithm uses only nearest neigh bor communication and achieves an at o f n then it achieves an et o f n as well if easily adapted to our model the two models however are not equivalent notice that there is no sense of trading area for time at the pe level where as our model supports energy time exchanges to vary both spatially and temporally across otherwise identical pes at bounds are typically based on cross sectional band width requirements while et bounds are based on the speed time distance that data must move this has significant im plications for example if we consider throughput instead of latency we can examine ap and ep where p is the period of the computation we note that ap bounds generally match their at counterparts because increasing latency with constant throughput increases both the amount of data to move and the amount of time to move it the required bandwidth is un changed thus at bounds cannot model the advantages of pipelining ep bounds on the other hand can be lower than their et counterparts because data can move slower if it has more time to reach its destination for example a pipelined add pass multiplier achieves an ep o even with pe rimeter i o which is lower than the o n bound for the et complexity the trade offs between energy throughput and latency merit further investigation as sketched above a deeply pipelined multiplier can use less energy per operation than a low latency design for the same throughput many but not all numeri cal algorithms are highly tolerant of latency in the floating point unit can we exploit this by building chips with many deeply pipelined floating point units and a few low latency ones likewise we have shown that the minimum et com plexities for addition multiplication and sorting require planar i o but the observation about multiplication suggests that pe rimeter i o implementations may be adequately optimally efficient in an ep model this would be good news for archi tects energy time optimal implementations may be possible where the functional units for primitive operations are imple mented with perimeter i o while meshes of cores may provide planar i o for larger algorithms furthermore by distinguish ing latency and throughput we can also consider streaming computations a model like the one we have presented here could help architects and algorithm designers explore trade offs such as these we have presented our model with pes defined at the bit level to ensure that our model accounts for all costs in an im plementation of an algorithm however this approach has the side effect of forcing all analysis to the bit level while we have established bounds for the three problems that we con sidered establishing lower bounds for bit level complexity is known to be a hard problem thus we plan to extend our model to allow pes that perform simple operations on words our analysis of addition and multiplication provides a basis for such an extension with a word based model we plan to examine common numerical tasks such as matrix multiplica tion and solving linear systems as well as more combinatorial problems such as graph algorithms and search problems power dissipation is the most critical bottleneck for com puter system performance existing algorithm design and anal ysis is based on obsolete models that ignore the underlying trade offs between energy and time thus new models are required before we can even describe the trade offs involved a key objective of today multicore processor designs is to increase performance without a proportional increase in energy consumption single core processors became overly com plex and their per watt performance deteriorated multicore processors try to ameliorate the problem by speeding up a workload with parallel pro cessing at a lower clock frequency and lower voltage but what is the precise relationship between parallel processing and energy consumption in intel shekhar borkar suggested that a perfect two way parallelization would lead to half the clock fre quency and voltage one quarter the energy consumption and one eighth the power density when compared with sequential execution of the same program in the same execu tion time see microarchitecture and design challenges for gigascale integration in this article we explore how much energy savings is possible with parallel processing if processors can dynami cally change their voltage and frequency this article is based on our paper corollaries to amdahl law for energy from ieee computer architecture letters in january we will address several questions what is the maxi mum energy improvement to be gained from paralleliza tion how can we determine the processor speed to achieve that improvement how does static power affect the energy optimal program speedup and energy consumption given a target speedup how do we set the processor speeds to min imize energy our exploration uses the same simple appli cation model as the well known amdahl law parallel applications having a serial section and a parallel section whose ratio is known reviewing amdahl law amdahl law provides a simple yet extremely useful method for predicting the potential performance of a par allel computer given the ratio of the serial and parallel work in a program and the number of processors available it has been widely applied in determining speedups even for sin gle processor systems it also known as the law of dimin ishing returns the following equation succinctly describes amdahl law where is the ratio of the serial parallel work in the program and is the number of processors we begin with the same input parameters as in amdahl law namely then we derive the minimum energy consumption one would get with optimal frequency alloca tion to the serial and parallel regions in a program while the execution time is unchanged we obtain when the dynamic power consumption of a processor run ning at clock frequency is proportional to in litera ture describing dynamic voltage and frequency scaling is energy corollaries to amdahl law we also assume that the dynamic power con sumption of a processor running at is nor malized to and that static power consumption is that is the ratio of static power to dynamic power at is our simple assumption about static power consumption allows us to reveal its effect in closed form derivations clock frequencies for the two regions in the work namely and are calculated as follows figure achievable dynamic energy improvement assuming and using and processors given the ratio of serial and parallel work in a program in these equations we assume for simplic ity that the execution time of a program region is determined by the amount of work e g and the processor speed e g in reality some program operations such as main memory access have latencies unrelated to the processor speed between and typically equation suggests that more parallelism larger and more processors larger help reduce energy consumption figure is a plot of equation formulating the problem for the purposes of these calculations we assume that processors can run at arbitrary clock frequencies subject to a maximum frequency using amdahl law in equation as a basis the speedup one would achieve with paral lelization and frequency scaling is subject to the following for the sake of simplicity we normalize the sequential execution time of the program as similarly we normalize the amount of work i e number of cycles in the program as therefore the maximum clock frequency has a rela tive speed of the amount of work in the serial portion of the program is represented by and the parallel portion by or figure shows this arrangement figure normalized work and time the parallel time is parti tioned into serial and parallel regions the time for the serial region is and the time for the parallel region is the parallel time less the time for the serial region for a given problem is fixed and for a given archi tecture and are fixed hence the energy consumption is a function of and specifically in equation the three terms represent energy for the serial portion energy for the parallel portion and energy for static power consumption during the whole execution time respectively we assume that dynamic power consumption of a processor running at is we do not consider the proces sor temperature as a factor hence the term for static energy is the product of the per processor power consumption rate the number of processors and the total execution time energy improvement with parallelization let consider the question of maximum energy improvement with parallel processing and which clock frequency achieves it we start with a special case the problem of obtaining the minimum energy consumption when is that is the pro gram execution time is identical to that of sequential execu tion at the maximum processor speed the condition is similar to setting a deadline sequential execution time by which to finish the computation of course one mayget larger energy savings by further decreasing with the condition we can rewrite equation as from equation we can derive the value of that minimizes energy consumption by setting to we get energy corollaries to amdahl law now we obtain the values of and to minimize using equations and specifically both and are a function of and in equa tions and equation shows the relationship between and when is minimized interestingly the ratio between the two frequencies is a function of but not of equation suggests that to achieve minimum energy consumption we need to slow the clock frequency in the parallel region of the program by compared with the frequency in the serial region figure illustrates this relationship finally from equations and we obtain the min imum energy consumption here the first term shows dynamic energy consump tion and the second term expresses static energy consump tion equation is simply taken from equation figure depicts the maximum energy improvement owing to par allelization when the number of processors varies from to and it clear that energy improvement is a function monotonically increasing with and figure shows how the overall energy changes as we adjust it also presents the value of that minimizes note that the optimal solution obtained for and is feasible because both clock frequencies are less than the processor maximum frequency determining the effect of static power amdahl law explores the effect of parallelization on speedup and we have described the effect of parallelization on energy consumption when the program execution time is unchanged i e however depending on the rate of static power consumption the minimum amount of total energy consumption may not occur at if static power consumption is high the processor will use the least energy at a higher clock frequency possibly resulting in on the other hand if static power consumption is low the processor will use the minimum energy at a slower clock fre quency let revisit the problem of minimizing total energy consumption without restricting for this we set the deriv atives of equation with respect to both and to zero we obtain the following figure dynamic energy consumption vs serial time for two cases and when the bound of is marked with x when and o when the minimum energy point in each curve at is marked with a filled rectangle with and we can use equations and to cal culate the optimum frequencies from which we can compute the minimum energy an inter esting observation is that at and the dynamic energy is given by the following which is equal to of the static energy in other words total energy consumption is minimized when the dynamic energy consumption is times the static energy consumption this relation holds during the execution of both the serial and parallel sections of the program theabove solution isapplicable onlyif both and are less than than however necessitating that if the ratio between static and dynamic power is large we can t maintain the aforementioned relationship between static and dynamic energy in that case we should set and energy corollaries to amdahl law figure changes the speedup of a program when its energy con sumption is minimized saturates at the maximum speedup that amdahl law dictates when we assume that find the values of and that minimize total energy con sumption denoting these values by and we obtain the following again these values result in dynamic power consump tion being times the static power consumption dur ing execution of the parallel portion of the program finally if static power consumption is so high that then the minimum energy is obtained when that is energy consumption is at the mini mum when the processors run at their maximum speed to finish the task as quickly as possible figure summarizes the relationship between and the speedup that results in minimum energy consumption in this figure the values of are divided into three regions when the solu tion for the optimum energy consumption problem is given by equations and when the solution is given by in equations and when the solu tion is given by and the speedup is that given by amdahl law in equation figure depicts the improvement ratio of the mini mum energy at different program speedups relative to the baseline sequential execution of a given application this plot clearly demonstrates that a smaller leads to a larger energy improvement ratio at any selected program speedup more over the greatest energy improvement ratio occurs at a lesser program speedup in other words one can slow the clock speed further to benefit from reducing dynamic energy to a greater degree before static energy starts to offset the benefit if is small energy performance trade offs so far we have focused on the problem of obtaining the minimum energy consumption with specific processor speeds hence program speedup given we have largely ignored the program performance in this section we will consider the trade offs between program perform ance and energy consumption the main question is how to set the clock frequency for a specified degree of performance because the static energy is immediately determined given we need only minimize the dynamic energy while meeting the program speedup requirement our solution is derived from equations and as follows where and are the optimal frequencies when in equations and we call the speedup interval in equation the linear fre quency scaling interval because the energy optimal be obtained by simply scaling by a factor of note that the upper bound of the condition in equation is equivalent to figure shows how the minimum energy con sumption changes as we target a different program speedup this figure also shows the contributions of dynamic and static energy consumption notice that figure energy improvement at different speedups compared with sequential execution the dynamic energy of the sequential region saturates at around that because cannot scale energy corollaries to amdahl law beyond finally when i e the maximum speedup dynamic energy is the same as for sequential execution another way to make the trade off between energy con sumption and program performance is to minimize the energy delay product rather than the total energy the energy delay product is as follows through similar analysis we get the following it is interesting to find that as in equation in fact by comparing the values obtained for with equations and we observe that they are the same equations replacing this simi larity also appears in the calculation of energy specifically we can compute the dynamic energy when is minimized task completion or stalling because of contention for shared resources this overhead has been examined in the context of scientific computing using supercomputers the impor tance of synchronization overhead in conventional comput ers grows as core counts increase the model we use to represent synchronization over head adds a synchronization function to the parallel portion this is in addition to the regular work the processor does and it increases with the number of cores although the exact form of synchronization depends on both the cpu or system architecture and the program executed prior work has found to be close for many architectures see h p flatt a simple model for parallel processing figure is a plot similar to that of figure using that estimation with after using the same derivation process described ear lier we discovered something unintuitive the relation between and when minimizing energy consumption is independent of the synchronization function although this might seem surprising it makes sense when considering that the relative serial and parallel portions of work do not factor into the relation between and either the synchroniza tion cost effectively raises the portion of parallel work along with the total work conclusions we have considered the problem of minimizing total energy consumption for a given architecture values of and a given problem with a known ratio of parallelizable work value of we have analytically derived the formula for the processor speeds that minimize energy consumption which is equal to of the static energy adding the overhead of parallelization analytical models of computer performance sacrifice accuracy for simplicity some aspects of a computer system are not mod eled and one might wonder about the effect of these missing properties in this section we examine the impact of one such property synchronization overhead synchronization overhead is a broad term we use to describe any overhead incurred when using multiple processors together it includes time spent communicating by using locks or barriers waiting because of uneven figure optimal energy given the speedup of total energy is the sum of dynamic and static energy this plot also shows dynamic energy for the sequential region the thick dotted line shows the sequential machine energy consumption given these parameters the maximum speedup amdahl law is and according to equation the dynamic energy of the sequential region saturates at according to equation energy corollaries to amdahl law figure achievable dynamic energy improvement after accounting for a synchronization overhead of as in figure and have shown that at those speeds dynamic energy con sumption is equal to the static energy consumption hence to minimize energy this relation between static and dynamic energy should be maintained as long as the proces sor does not exceed its maximum allowable clock frequency in that case the maximum speed should be used in many systems it is desirable to strike a trade off between energy consumption and performance by minimiz ing the energy delay product rather than the total energy our results show that the optimal energy delay is obtained when and and our results also show that the frequency relation of allows us to optimize both energy and the energy delay product our formulas also show that for a given processor implementation and the minimum total energy is a monotonically decreasing function of the number of processors as long as the parallel section of code can be executed on processors hence from the viewpoint of total energy consumption all available processors should be running however note that this result and all results in this article assume that the processors in the system con sume static power even when executing serial code if indi vidual processors can be turned off and back on with low overhead then the formula for total energy in equation should be changed such that is replaced b y in this case our analysis indicates that the minimum total energy is independent of the number of processors used for executing the parallel section of a program the energy delay product is minimized when the maximum number of available processors executes the parallel code the minimum amount of energy is consumed when clock speeds are equal during the serial and parallel sections which again results in static energy equaling the dynamic energy editor note sangyeun cho is an assistant professor in the department of computer science at the university of pittsburgh in pennsylvania michael moeng is a graduate student in that department dr rami melhem is the department chairman and a professor of computer science this paper highlights the growing importance of storage energy con sumption in a typical data center and asserts that storage energy re search should drive towards a vision of energy proportionality for achieving significant energy savings our analysis of real world en terprise workloads shows a potential energy reduction of using an ideally proportional system we then present a prelimi nary analysis of appropriate techniques to achieve proportionality chosen to match both application requirements and workload char acteristics based on the techniques we have identified we believe that energy proportionality is achievable in storage systems at a time scale that will make sense in real world environments introduction energy consumption of data centers is a significant portion of their operational cost and has become a major concern both for their own ers and the countries in which they are placed thus a large amount of recent work has focused on improving the energy efficiency of data centers in all aspects distribution of power data center cool ing and energy consumption of it components among these dif ferent aspects reducing the energy consumption of it components servers storage networking equipment etc plays an important role since improving its energy efficiency also reduces the required capacity of the cooling and power distribution systems of all the it components in a data center servers have been the dominant energy consumers while a single core microprocessor in consumed w of energy a disk consumed around w thus server energy consumption attracted significant research attention and typical server energy usage has decreased consider ably over the last few years today an idling power gated core consumes as little as w of energy and an active core consumes around w on the other hand disk drive power consumption has remained relatively unchanged per drive as a result storage is consuming an increasing percentage of energy in the data center recent work has shown that in a typical data center today accounts for up to of the energy consumption of all it com ponents we expect storage energy consumption to continue increasing in the future as data volumes grow and disk performance and capac ity scaling slow a recent study by idc makes the follow ing observations that back this trend storage unit acquisition storage in the rest of this paper refers to the collection of independently managed dedicated clusters subsystems for storage and not direct attached storage will likely outpace server unit acquisition during the next five years storage capacity per drive is increasing more slowly which will force the acquisition of more drives to accommodate growing ca pacity requirements data centers are predicted to move towards drives that typically consume more energy per gbyte than their equivalents and performance improvements per drive have not and will not keep pace with capacity improvements thus im proving iops per watt continues to be a challenge attesting to this growing importance of storage energy consumption epa an nounced energystar specifications for storage components in april a rich body of existing work e g has al ready investigated energy efficiency of storage systems however a significant fraction of this work assumes the existence of hard disks with dynamic rpm capability e g however drpm drives are not being commercialized in quantity by any major drive vendor due to physical and cost constraints nevertheless the increasing importance of storage energy has spurred innovations in hard disk design such as multiple idle modes and just in time seeks and solid state disks ssds are poised to improve iops per watt dramatically in light of this we argue that the research community should renew interest in improving storage energy consumption at the storage subsystem and the data center level more importantly we claim that improving energy efficiency alone is not adequate and that significant efforts must be focused on achieving energy propor tionality for storage systems energy proportionality was first proposed for servers by barroso and holzle this work observed that servers over a six month period spent most of their time between and percent utilization thus the authors argued that energy usage should vary as utilization levels vary specifically energy proportionality sug gests that as the amount of work done increases so can the energy consumed to perform it this paper investigates whether this concept of energy proportion ality can and should be extended to storage we argue that an energy proportional storage system is useful in two different scenarios when performance matters most storage energy consumption should vary with the performance requirement this perspective is important for normal operation of a data center when energy matters most performance provided by the storage should be regulated according to energy constraints this perspec tive could be important for operation of data centers that are experi encing a transient e g brownout or chronic energy constraint the first perspective performance matters most is more achiev able in the short term but we believe that the second perspective is important in the long term especially as the focus shifts to stor age energy management ultimately storage energy management should be part of an integrated data center energy management archi tecture focusing on energy proportionality will enable storage en ergy management to be coordinated with server network and cool ing energy management the rest of our paper makes the following contributions ex ploration of the benefit from an energy proportional storage sys tem to that end we first study the variation in performance de mands on storage systems in real world enterprise data center envi ronments and estimate the potential energy savings section outlining of techniques that can be used to build energy proportional storage systems and systematic analysis of these techniques sec tion and illustrating how storage application requirements and workload characteristics map to storage energy proportionality techniques section energy proportionality matters this section motivates the importance of an energy proportional storage architecture by studying the variation in utilization seen by units of storage methodology we use two trace sets collected at volume level and extent fixed sized partition of a volume level respectively for this study the volume level trace was collected from the data center of a financial corporation the data center contained large storage systems with a total of volumes mounted on raid arrays each with or hard disks i o data metrics include i o rate read to write ratio random to sequential ratio and average transfer size for each volume in the storage system was collected by a centralized inde pendent server every minutes and stored in a database this data was later compacted into per day averages on a per volume basis we use these per day samples over days in our study apart from using i o rate to understand the variation in the performance demands we compute utilization level for each volume as instantaneous i o rate max i o rate for that volume the extent level trace was collected by running a public storage benchmark modeling a transactional workload for an extended time period the setup consisted of an ibm pseries computer running aix connected to an ibm storage system with raid arrays each array consisting of or disks fourteen tbyte volumes were allocated on this storage system and the benchmark accessed different extents in those volumes a monitoring module recorded i o rate periodically for each unique extent on all volumes figure average fraction of time vs volume utilization volume level trace figure heatmap depicting i o rate across volumes for the volume trace note that the volumes are sorted using the median i o rate from bottom to top on the y axis darker color depicts higher i o activity figure heatmap depicting i o rate across extents in a sin gle volume in the extent trace darker color depicts higher i o activity impact of energy proportionality figure depicts the average percentage of time a volume spent at each utilization level for the volume level trace on average a vol ume is idle for of the time first two columns for the re maining a decreasing amount of time is spent at each utiliza tion level clearly this shows that on average volume utilization in this storage system is highly variable with a significant amount of time spent at moderate utilization levels this highlights the need for storage systems to exhibit energy proportional behavior while figure depicts the average behavior across all volumes in the trace figure shows the i o activity over time for each volume we see that most volumes have variable usage over time however a few volumes do maintain a consistently high or low i o rate for a majority of the time top and bottom portions of the figure thus both energy efficiency and proportionality are essential for potential energy savings given the i o distribution seen in the volume trace we now es timate the energy savings in an ideally energy proportional system in such a system energy usage would follow utilization which on average is less than therefore the potential for energy sav ings in an ideally proportional system exceeds when compared to a system always running at utilization and hence energy independent of system demands to understand whether there are more opportunities at a finer grain we also studied an extent level trace described previously figure which is typical of all the volumes in the trace shows that some extents experience variable i o activity over time while others remain largely idle a few extents have much higher average i o activity than other non idle extents depicted by the hor izontal dark bands this indicates the potential to achieve propor tionality at a granularity finer than entire volumes or arrays for example if the most active of extents were resident on enter prise class ssd disks and the remaining on sata disks we calculate energy savings of nearly in contrast to a system com pletely built from enterprise disks while the acquisition cost of the system would be about the same moreover extents can be easily moved compared to volumes to and back from different disk tiers when their utilization changes storage energy proportionality given the significant savings from an ideally energy proportional storage architecture we now explore various techniques that can be leveraged to achieve proportionality we begin by understanding the techniques used to achieve server energy proportionality in order to identify analogous approaches for storage if possible servers vs storage techniques a broad class of techniques for achieving server energy proportion ality are based on dynamic voltage and frequency scaling dvfs e g dvfs provides lower energy active modes modes where servers can continue to perform work at lower performance and al lows relatively fast switching between modes on the other hand there is currently no powerful analogue in storage that directly en ables fine grain scaling of performance as a tradeoff for energy another class of server techniques focuses on achieving propor tionality by migrating workloads such that servers either operate at their peak or can be suspended this is in contrast to the above techniques that scale performance and are considered more powerful since lower performance states are less energy efficient on the other hand the time scale to adapt for these techniques is higher since it involves process migration although this class of techniques is directly applicable to storage the overhead incurred in storage will likely be higher given its statefulness and amount of data moved moreover data availability requirements often imply that data should always be reachable within small number of mil liseconds or seconds storage techniques we explore several energy saving techniques a majority from ex isting literature and a few novel techniques inspired by server side approaches and energy saving hard disk modes these techniques applied alone or in concert with each other can contribute to an en ergy proportional storage architecture the first set of techniques use intelligent data placement and or continuous data migration resulting in large time scale to adapt ex amples include consolidation aggregation of data into fewer storage devices whenever performance requirements permit tiering migration placement movement of data into storage de vices that best fit its performance requirements e g write off loading diversion of newly written data to enable spin ning down disks for longer periods coupled with opportunistic movement of data to storage devices when they become active the same technique can be used to efficiently service peaks in i o activity the next class of techniques benefit from the availability of hardware based active and inactive low energy modes in disks as such the overhead of these techniques is low since it does not in volve data movement examples include adaptive seek speeds allow trading off performance for power reduction by slowing the seek and waiting an additional rotational delay before servicing the i o workload shaping batching i o requests to allow hard disks to enter low power modes for extended periods or to allow workload mix optimizations opportunistic spindown spinning down hard disks when idle for a given period spindown maid maintaining disks with unused data spundown most of the time either by concentrating important data or tuning caching and prefetching algorithms to save power in all cases trying to increase idle periods of disks with low load finally another class of techniques reduces the amount of space required to store data directly resulting in energy savings for exam ple dedup compression involves storing smaller amounts of data using very efficient representations of the data framework for technique selection given the above techniques an important question to answer is which techniques are suited for a given storage application this section outlines a framework to make such recommendations different techniques present different tradeoffs with respect to their potential to alter application performance and incurred over head versus resulting benefit for example a technique that lever ages opportunistic spindown can potentially cause the peak response time of an application to be as high as the spin up delay sec onds similarly techniques that rely on data migration incur signif icant overhead and hence rely on workloads that amortize the over head to result in energy savings thus our framework selects ap propriate techniques using two inputs application performance requirements and application workload characteristics in the rest of this section we categorize these inputs based on their inter action with the techniques and then match the different categories with the appropriate techniques to achieve energy efficiency and pro portionality application performance requirements since the above techniques may impact application performance their usage must be aligned with the requirements of the storage ap plications to understand this better we propose three different cat egories of storage applications although the range of applications probably lies more along a continuum than in discrete categories taking into account two factors sensitivity to average response time and sensitivity to peak response time e g from spin up delay high sensitivity to peak response time high sensitivity to av erage response time these are often critical business applica tions typically using san storage which have short default timeouts transactional databases are an example low sensitivity to peak response time high sensitivity to aver age response time these are often but not always important busi ness or consumer applications which require good storage perfor mance but can tolerate occasional delays examples include web storage multimedia streaming and general user file storage low sensitivity to peak response time low sensitivity to av erage response time these are often archival backup applications where multi second delays are tolerated and response time is not table attributes of techniques for storage energy proportionality technique app category time scale granularity potential to alter performance consolidation hours coarse can lengthen response times tiering migration minutes hours coarse can lengthen response times write off loading milliseconds coarse adds background process that can impact application adaptive seek speeds milliseconds fine can lengthen response times workload shaping seconds fine can lengthen response times opportunistic spindown seconds fine delays due to spinup spindown maid of seconds medium delays due to spinup dedup compression n a n a delays in accessing data due to assembling from repository or decompression so important examples include medical or generic archival backup and ediscovery because performance requirements vary between these cate gories different techniques will be appropriate for applications in different categories for example it may never be permissible to spindown a hard disk supporting a high peak response time intoler ant application whereas opportunistic spindown may be acceptable for an application which is tolerant of high peak response time but generally requires average response time in the tens of milliseconds table presents a summary of which storage techniques can be used by different application categories as well as the tradeoffs pre sented by these techniques using the following metrics time scale to adapt some techniques affect the environment in milliseconds e g slowing seek speeds while others take seconds e g spinning down disks minutes or hours e g migrating data granularity of control some techniques are fine grain e g ad justment of i o dispatch rate while others can be coarse grain e g migrating a volume potential to alter performance some techniques can introduce small delays to access times e g spinup while others will not e g consolidating workload characteristics the extent of energy savings and the incurred overhead of different techniques is significantly impacted by the workload characteristics of the applications using the storage system an important characteristic for data movement based techniques is the steadiness of the workload a steady workload exhibits a con stant utilization over time this enables these techniques to place data optimally and accrue benefits over time even if the work load is not perfectly steady these techniques benefit if the work load shows periodicity on the other hand these techniques cannot quickly respond to variation in the utilization of the workload and can only adapt to average utilization thereby reducing the benefit for convenience we refer to these features as workload stability in fact given the higher overhead of data movement based techniques workload stability decides if such techniques are applicable another workload characteristic that affects the amount of benefit from the techniques is their extent of utilization workloads with low utilization most of the time can benefit from spindown whereas workloads with high utilization majority of the time have fewer spin down opportunities as an example we categorize the workloads in the volume level trace we first break up the volumes based on their stability and then their average utilization overall we see that almost of the volumes exhibit stability we then divide these stable volumes based on their utilization we observe that of the volumes are highly utilized and have utilization more than of the time table volume categorization for the financial data center workload key h high load l low load p peaks in load v vx variable load lowest highest i o rate category h l p v vol data in this group can benefit from suitable tiering and consolidation since the required time scale for change is rather high such data can also be colocated with data experiencing low utilization another of the volumes have mostly idle data units with utilization more than of the time such data units can be consolidated and migrated onto slower devices which may then be spundown if the application category allows for the spinup delay write offloading and workload shaping techniques are also suitable the next of volumes consist of data units that are mostly un der utilized but have bursty peaks nominally with utilization more than of the time and utilized more than of the time in this case peaks can be serviced by fine grain migration which has relatively low overhead hard disks which allow slow seeks would also provide a quick to change method of aligning i o performance characteristics and energy usage as i o load changes in addition write offloading techniques allow for handling of small peaks for write traffic while also allowing spindown of hard disks holding most of the data the remaining of volumes show a fairly variable workload we further divide the volumes in this category into four more groups characterized by their maximum i o rate in comparison with the maximum i o rate observed in the system the last four columns of the table such that holds volumes with lowest i o rate and holds volumes of highest i o rate we see that most of the vol umes have low i o rate these need a smaller range of energy utilization points fine grain migrations across different raid tiers raid vs raid could potentially be useful volumes in v4 have the most variability in their i o rate and therefore need techniques with small time scale to adapt slow seek techniques and fine grain migrations across device types ssd sas sata would be the most suitable alignment of techniques table expresses the application response requirements in two di mensions as being sensitive or not to lengthening of peak re sponse time and as being sensitive or not to drive spinup delays it also factors in workload characteristics we specifically focus on stability since it affects the applicability of a technique as opposed to the other characteristics that only impact the amount of benefit for example stability indicates if an adaptive algorithm will be inef table framework for mapping storage application perfor mance requirements and workload characteristics to energy saving techniques techniques c consolidation t tier ing migration s opportunistic spin down maid w write off loading a adaptive seek speeds h workload shaping d dedup compression sensitivity to avg resp time sensitivity to peak resp time stability of workload techniques c t s w a h d yes yes no yes no no yes no no no yes key applicable fective due to unexpected shifts the more unpredictable the future the less likely that a high cost overhead technique will be useful given those three parameters we suggest in the table which tech niques are most likely to be effective in which mix of application requirement and workload finally we also considered whether individual techniques would clash with each other or be complementary on the whole we feel that the techniques need not clash meaning that a number of them may be deployed effectively in the same system however tech niques that generally are complementary might clash on individual resources or at points in time a simple example of this would be opportunistically spinning down of some drives while the system was planning to migrate data off the drives to turn them off an other example might be lengthening seek times on a drive where workshaping algorithms are getting ready to execute a batch of i o requests the lesson from this is that as these techniques are in corporated into a system careful architecture is required to ensure appropriate decisions about usage of the techniques conclusion given the increasing energy consumption by storage systems and the availability of ssds and newer energy saving modes this paper argued for a renewed attention towards improving storage energy consumption more importantly we showed that achieving energy proportionality can have significant benefit above and beyond using individual energy efficient components analysis of enterprise data center workloads indicated reduction in storage energy under ideal storage energy proportionality while another indicated a po tential reduction our work also outlined the challenges and analyzed techniques for building energy proportional systems while aligning the techniques with application requirements and workload characteristics based on the techniques we have identified we be lieve that energy proportionality is achievable in storage systems at a time scale that will make sense in real world environments currently we are investigating potential energy savings from real world enterprise settings and building a simulation infrastructure to experimentally demonstrate the utility of the techniques we have identified we believe that storage energy management must be cou pled into data center energy management and energy proportional ity helps realize that goal without such a integral approach energy management will not be able to approach optimal conservation this paper presents the design challenges posed by a new class of ultra low power devices referred to as energy harvesting ac tive networked tags enhants enhants are small flexible and self reliant in terms of energy devices that can be attached to objects that are traditionally not networked e g books clothing and produce thereby providing the infrastructure for various novel tracking applications examples of these applications include lo cating misplaced items continuous monitoring of objects items in a store boxes in transit and determining locations of disaster sur vivors recent advances in ultra low power wireless communica tions ultra wideband uwb circuit design and organic electronic harvesting techniques will enable the realization of enhants in the near future in order for enhants to rely on harvested energy they have to spend significantly less energy than bluetooth zig bee and ieee devices moreover the harvesting com ponents and the ultra low power physical layer have special char acteristics whose implications on the higher layers have yet to be studied e g when using ultra low power circuits the energy re quired to receive a bit is an order of magnitude higher than the en ergy required to transmit a bit these special characteristics pose several new cross layer research problems in this paper we de scribe the design challenges at the layers above the physical layer point out relevant research directions and outline possible starting points for solutions categories and subject descriptors c computer communication networks network archi tecture and design wireless communication general terms algorithms design performance keywords ultra low power communications energy efficient networking en ergy harvesting energy scavenging ultra wideband uwb permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee mobicom september beijing china copyright acm sensor networks enhants rfids figure enhants in comparison to sensor networks and rfids introduction this paper focuses on the networking challenges posed by a new class of ultra low power devices that we refer to as energy harvesting active networked tags enhants enhants are small flexible and self reliant in terms of energy devices that can be attached to arbitrary objects that are traditionally not networked books clothing produce etc enhants will enable novel object tracking applications such as recovery of lost items and continu ous monitoring of objects proximity to each other the realiza tion of enhants is based on recent advances in the areas of solar and piezoelectric energy harvesting as well as ultra low power wireless communications in particular recent novel cir cuit designs that employ ultra wideband uwb communications provide new levels of ultra low power operation at the orders of nj bit at short ranges moreover solar energy harvesting based on organic semiconductors allows having flexible solar panels thereby allowing a pervasive use of tags the wireless industry is already taking the first steps towards the design of energy harvesting ultra low power tags hence fol lowing the transition from barcodes to rfids we envision a future transition from rfids to enhants that network actively communicate with one another and with enhant friendly devices in order to forward information over a multihop network operate at ultra low power spend a few nano joules or less on every transmitted bit harvest energy collect and store energy from sources such as light motion and temperature gradients are energy adaptive alter communications and network ing to satisfy energy and harvesting constraints exchange small messages exchange limited information basically ids using low data rates possibly in several trans mission bursts transmit to short ranges communicate only when in close proximity to meters to one another are thin flexible and small a few square cm at most as shown in figure in terms of complexity throughput size and energy requirements enhants fit between rfids and sen sor networks similarly to rfids the tags can be affixed to com monplace objects however enhants will have a power source will be able to communicate in distributed multihop fashion and will not have to rely on high power readers compared to sen sor nodes enhants will operate at significantly lower data rates and will consume less energy moreover unlike sensor nodes en hants will transmit mostly id information either in order to an nounce themselves or to query for specific enhants despite these differences some of the results obtained for sensor networks see should apply to enhants enhants will be enablers for the internet of things and as such will support a variety of tracking and monitoring applications be yond what rfid permits while rfids make it possible to identify an object enhants will make it possible to search for an object and to continuously track objects whereabouts and their proxim ity to each other rfids are typically activated only when placed near a reader and only report on themselves enhants on the other hand can operate continuously achieve pervasive coverage due to their networking capabilities and can report on themselves and other enhants around them these enhants capabilities enable many exciting applications such as for example continu ous peer monitoring of merchandize in transit where enhants would be able to identify if a particular box has been taken out at any point during the journey one application that we plan to demonstrate in the near future is a misplaced library book locator the initial prototype will enable library books to identify those among themselves that are signifi cantly misplaced e g in an incorrect section and report the mis placement to accomplish this task each book is assigned a unique id using an assignment scheme closely related to the dewey deci mal classification each book has a solar powered tag whose power output is sufficient to transmit and receive information within a radius of one meter or less and to perform some basic process ing nearby books wirelessly exchange ids and ids of books that appear out of place are further forwarded through the network of books eventually propagating to sink nodes a long term objective could be to place books in an arbitrary order and both determine the book order and propagate that information to a central server using the multihop wireless network this type of system can sig nificantly simplify the organization of physical objects the same building blocks used in the library application can en able several other applications in particular a large variety of items can be tracked and a range of possible desirable or undesirable con figurations of objects can be queried for and can trigger reports examples include finding items with particular characteristics in a store locating misplaced items e g keys or eyeglasses and lo cating survivors of disasters such as structural collapse to enable these applications various protocols have to be de signed although networking protocols for energy harvesting nodes recently started gaining attention see section to the best of our knowledge the cross layer in teractions between circuit design energy harvesting communica tions and networking have not been studied in depth current rf transceiver designs communication and networking protocols and energy harvesting and management techniques have been devel oped in isolation and are inadequate for the envisioned enhants applications hence based on our experience with hardware de sign communications and networking we outline the cross layer design challenges that are posed by this new technology we note that although there are many hardware specific design challenges they are out of scope for this paper enhants are likely to be implemented by combining flexible electronics technologies a k a organic electronics with cmos chips supporting impulse radio uwb flexible technologies can realize energy harvesters solar cells piezoelectric thermal etc passive rf components and batteries by embedding cmos chips in enhants very low power computation memory and commu nication functions can be added this technology platform allows for thin flexible and very low cost enhant fabrication by using impulse radio uwb data is encoded by very short pulses at the order of nano seconds and the energy consumption is very low to better understand the higher layer design challenges we first discuss energy harvesting and energy storage techniques energy storage is required in order to use the harvested energy in periods in which harvesting is not possible we later describe models that take these techniques and their characteristics into account when determining the uwb pulse patterns duty cycles and overall en ergy consumption some of the interesting characteristics include the differences in performance between a capacitor and a battery and between indoor and outdoor solar energy harvesting next we discuss ultra low power uwb communications and present a number of design challenges these include a paradigm shift resulting from the fact that when using ultra low power com munications it is energetically cheaper to transmit data than to re ceive data moreover ultra low power tags will operate with inac curate clocks thereby requiring the redesign of low power methods based on coordination of wakeup periods finally enhants may have additional circuitry e g accurate clocks that can be pow ered up in some harvesting states and can be used to assist other enhants determining how and when to use this circuitry is an other challenge we then focus on the design of enhants communications and networking protocols these protocols have to determine the state of the tags sleeping communicating etc the operation within a state transmit receive rate control etc and the coordination with peer tags all these have to be based on the harvesting states of the tags and their capabilities clearly energy harvesting shifts the na ture of energy aware protocols from prolonging the finite lifespan of a device to enabling perpetual life and from minimizing energy expenditure to optimizing it the challenges that we describe in clude not only determining the state and the operation model within the state but also the use of a harvesting channel as a mean for nodes synchronization finally we draw parallels between energy harvesting enhants and large scale manufacturing systems we show that the harvested energy can be treated similarly to inventory in production and stor age systems we show how inventory control models extensively studied in the inventory management field apply to en hants we then discuss the various challenges resulting from the fact that enhants compose a distributed network with many stochastic components and outline a number of open problems we are currently building a testbed of enhants in which we will evaluate various approaches using energy harvesting and ultra low power hardware we conclude by briefly describing the imple mentation phases and the hardware components this paper is organized as follows in section we describe related work in sections and we discuss energy harvesting en ergy storage ultra low power communications and the challenges they pose for designing higher layer protocols section describes the challenges posed by enhants communications and network ing and section discusses the application of inventory control models to enhants finally in section we present our plan for designing an enhants testbed related work while the idea of pervasive networks of objects has been pro posed before e g in the smart dust project the harvest ing and communications technologies have recently reached a point where networked energetically self reliant tags are becoming prac tical as mentioned above enhants fall between rfids and sensor networks to our knowledge most of the networking research in the area of rfid focuses on the scheduling of query responses by passive tags e g extensively studied sensor networks are designed to deal with energy bandwidth and other resource constraints see and references therein yet the underlying communication mechanisms draw power at rates that are too high in environments with weak energy sources indoor lighting strain vibration enhants will employ impulse radio uwb communications whose corresponding mac protocols e g have been proposed for communications and accurate ranging in sensor net works the recent ieee standard is based on im pulse radio uwb and inherits many of the ieee zig bee functionalities limited hardware data is currently available but we can assume that the overhead to provide ranging high data rates up to and backward compatibility will lead to en ergy consumption largely exceeding the energy we envision avail able in enhants energy efficiency in wireless networks has long been a subject of research see reviews in comparison only a few works have considered energy harvesting one of the major directions in exploiting energy harvesting is adaptive duty cycling in sensor networks in particular describe adaptation of the duty cycle to the characteristics of a periodic energy source and de velops a battery centric not requiring an energy source model duty cycle adaptation algorithm taking energy harvesting into ac count when making routing decisions is studied in re cently jointly calculates link flows and data collection rates for energy harvesting sensor networks dynamic activation of energy harvesting sensors has been stud ied in a system where mobile nodes deliver har vested energy to different areas of the network is presented in a system where an energy restricted node can outsource packet retransmissions is described in a power subsystem for hydro watch is described in which also provides an overview of de ployments that rely on energy harvesting to the best of our knowl edge current deployments use much more energy than enhants will have available finally there is an increasing industry interest in bringing to gether low power communications and energy harvesting e g particularly texas instruments has recently put on the mar ket a solar energy harvesting development kit geared towards sensor developments although it is a first step towards enhants this kit is made of rigid materials and is more than the size of the enhants envisioned in this work energy harvesting in order to outline the higher layer design challenges we briefly describe two major harvesting methods and possible energy storage methods then we discuss the effects of the different methods on the design and operation of higher layer protocols in sections figure an organic semiconductor based small molecule so lar cell series array developed in the columbia laboratory for unconventional electronics clue and we discuss different approaches to the design of such higher layer protocols energy sources many environmental sources of energy are potentially available for harvesting by small devices these include temperature dif ferences electromagnetic energy airflow and vibrations below we focus on the most promising harvesting technologies for enhants solar energy and piezoelectric motion harvesting other harvesting technologies pose similar design challenges solar energy light is one of the most useful energy sources with typical irradiance total energy projected and available for col lection ranging from in direct sunlight to in brightly lit residential indoor environments notice the significant difference office retail and laboratory environments are typically brighter than residential settings but get much less light energy than outdoor environments the efficiency of a solar en ergy harvesting device is defined as the percentage of the avail able energy that is actually harvested conventional single crystal and polycrystalline solar cells such as those that are commonly used in calculators have efficiency of around in direct sunlight however their efficiency declines with a decline in energy availability they are less efficient with dimmer sources which is important to note due to considerable irradiance difference between direct sunlight and indoor illumination conventional so lar panels are also inflexible rigid which makes it difficult to at tach them to non rigid items such as clothing and paperback books an emerging and less explored option is solar energy harvest ing based on organic semiconductors an array of organic solar cells that we recently designed is shown in figure with this technology solar cells can be made flexible moreover organic semiconductor based panels operate with constant efficiencies over different brightness levels however their efficiency is typically which is much lower than the efficiency of conven tional inorganic solar panels to put the numbers in perspective consider a system with a organic semiconductor cell outdoors the system will har vest under the assumption that reception of a single bit requires the achievable data rate will be the achievable data rate with indoor lighting will be another potential source of energy is piezoelectric motion en ergy it can be generated by straining a material e g squeez ing or bending flexible items an example is energy harvesting through footfall where a harvesting device is placed in a shoe and piezoelectric energy is generated and captured with each step that reception is more expensive than transmission for more details see section e consumption rate e watts note that the energy charge rate r de pends both on the harvesting rate and the properties of the energy r storage for example when a battery is used r is positive only when the voltage at the energy harvesting component exceeds the c internal chemical potential of the battery when a capacitor is used figure an abstraction of an energy harvesting system for the upper layers the energy storage capacity c the current energy level e the energy charge rate r and the energy consumption rate e unlike solar harvesting piezoelectric harvesting may be somewhat controlled by the user piezoelectric harvesting is characterized by the energy captured per actuation at a particular strain usually in it was shown how to harvest per deflection with a strain of ap proximately from straining polyvinylidene fluoride pvdf a highly compliant piezoelectric polymer assume that a of material is employed in an environment where it is strained times per second this would provide if similar to above we assume that transmission of a bit costs the bit rate that can be supported is energy storage without the ability to store energy a device can operate only when directly powered by environmental energy for a tag energy storage components need to be compact and efficient and need to have very low self discharge rates from a higher layer point of view it is also important to have storage elements that are straight forward to measure and control rechargeable batteries are an excellent option for energy stor age and numerous battery options are available thin film batteries are particularly attractive for enhants since they are environmen tally friendly and can be made flexible however a battery needs to be supplied with a voltage exceeding the internal chemical potential typically in order to start storing provided energy this implies that charge generated at a low voltage such as that possi bly produced during low harvester excitation for example when a solar cell is located in a very dimly lit place cannot be stored without voltage upconversion capacitors can also be used for energy storage capacitors can receive any charge which exceeds their stored voltage and be cy cled many more times than batteries the disadvantage of using capacitors however is that as a capacitor gets more charged it be comes more difficult to add charge and large electrolytic capacitors self discharge over hours or days the energy density how much energy can be stored per unit of volume of capacitors is also much lower a typical battery can store about whereas high performance ceramic capacitors can store cm3 different enhants applications will require different types of energy storage for example a tag which frequently experiences shallow charging and discharging events will need a capacitor for an acceptable lifetime whereas an enhant that needs to oper ate for a long period of time without recharging and to store large amounts of energy say to charge all day and discharge all night will need the energy density that a battery offers higher layer view of harvesting the complexity of the harvesting system needs to be captured in a relatively simple way for the higher layers a possible abstraction is shown in figure the system is characterized by the maximum energy storage capacity c joules the currently available energy level e joules the energy charge rate r watts and the energy the relationship of r and energy harvesting rate varies with e the energy consumption rate e is controlled by higher layer algorithms and with low duty cycle is mostly affected by communication and networking protocols the effect of these protocols on e will be discussed in sections and the available energy e and energy charge rate r can be mea sured directly from the energy harvesting and power conditioning components the maximum storage capacity c should be known nominally from the tag design but more precise characterization is possible for example if a battery is used c can be projected from the battery age the storage temperature history and the charge level over time as the battery continues to age and cycle through charge discharge events the capacity will predictably decrease the r values of different enhants operating in the same envi ronment will be significantly different our experiments with com mercial hardware show a lot of variability in the harvesting rates of identical harvesting devices under identical light conditions i e identical solar cells in the same location harvest energy at rates that can differ by about in addition the r values will differ for closely located solar cells due to differences in how the cells are located with respect to light sources and obstacles low power communications ultra wide band uwb impulse radio ir is a compelling tech nology for short range ultra low power wireless communications it uses very short pulses on the order of nano seconds that are transmitted at regular time intervals with the data encoded in the pulse amplitude phase frequency or position at low data rates the short duration of the pulses allows most circuitry in the transmitter or receiver to be shut down between pulses resulting in significant power savings compared to narrow band systems practical cmos ir circuits with energy consumption on the or der of a nj per bit have been recently demonstrated for exam ple in a uwb receiver and transmitter require bit and bit respectively at a pulse rate of recent pub lications as well as our ongoing research demonstrate that uwb ir transceivers in the band with data rates in the to range and a transmitter energy of less than bit and receiver energy of less than bit are within reach in this section we outline our envisioned design of uwb trans ceivers for enhants and the resulting higher layer challenges these customized circuits will support ultra low energy consump tion and will be integrated with energy harvesting devices while supporting networking capabilities energy costs a paradigm shift the first networking challenge emerging from the design of the ultra low power transceivers is that the energy to receive a bit is much higher than the energy to transmit a bit this is significantly different from traditional wlans where the energy to transmit is higher than the energy to receive and where they are on the same order this requires novel networking algorithms for enhants since many legacy algorithms are developed under the assumption of transmission being more expensive than reception consumption is measured at a particular pulse rate since per bit parameters differ at different bit rates note that at lower bit rates the energy per bit can increase due to the impact of fixed pulse independent circuitry in conventional systems in which narrow band modulated sinu soids are transmitted the transmitter has to be active for the entire duration of the signal transmission as mentioned above in uwb very short pulses convey information so the transmitter and re ceiver can wake up for very short time intervals to generate and receive pulses and can sleep between subsequent pulses the re ceiver energy is mostly spent on running low noise amplification and data detection circuits that are consuming energy whenever the device listens to the medium hence there is no difference in terms of energy between receiving information and listening to the medium the new energy tradeoffs call for the design of new algorithmic approaches for example in order to take the burden off a receiver a transmitter would have to enable a receiver to listen to the medium for short time intervals e g repeat its pulses many times in such a way that a receiver listening to a short window of pulses would get all the information transmitted in section we discuss in more detail the effect of this phenomenon on the design of higher layer protocols inaccurate clocks accurate on chip references or clocks cannot be powered down and consume a lot of energy they have to be avoided to achieve ultra low power operation one viable solution is to use energeti cally cheap clocks e g clocks available from ultra low power ring oscillators however the frequency of such clocks will vary sig nificantly from tag to tag and its stability over time is also poor a uwb receiver has to wake up at certain times in order to re ceive pulses determining these times with inaccurate clocks im poses major challenges and hence while inaccurate clocks save en ergy they increase the energy spent on reception moreover tra ditional low power sleep wake protocols see heavily rely on the use of accurate time slots hence eliminating the availability of accurate clocks in a tag requires redesigning protocols that were originally designed specifically for energy efficient networking a high power mode in some cases it may be beneficial to spend more energy than what is typically spent by a tag e g when the battery is fully charged e c and the tag is harvesting energy in such cases a tag can operate in a high power mode enhant circuitry can be designed to contain optional power hungry hardware modules that would allow for example for a more accurate faster clock to be turned on other components that could be considered are more sensitive receiver stages more selective filters and more elaborate pulse detection methods the inclusion of such components will increase the probability of successful communications but all per formance enhancements will require additional power the challenge is to realize the benefits of these high power modes at the higher layers an example of a clear benefit is a tag with an accurate clock that helps other tags to synchronize another exam ple is running the receiver or transmitter more often to help other nodes to detect each other and to establish communication given a set of high power physical layer capabilities numerous questions arise what is the right harvesting status to use stop using each of these capabilities how will using these capabilities in one tag af fect other tags is it enough to have these capabilities in a subset of the tags and what is the right size of the subset communications networking we now outline enhants related communications and network ing challenges recall that given the power constraints enhants will be communicating within ranges of to meters we iden tify three states of pairwise enhant communications and note that the rate of energy consumption e can be adjusted within each state and also by moving between states we also outline the chal lenges related to routing information dissemination and network security some of the concepts discussed in this section are well known in sensor networking hence we highlight the new chal lenges such as presence of transmit only devices and making use of the always open harvesting channel pairwise enhant communications in pairwise enhant communications three states can be iden tified independent paired and communicating to control its en ergy spending a tag can move between states with respect to each of its neighbors in each particular state a tag can consume differ ent amounts of energy e depending on its own energy parameters c e r and when relevant on the energy parameters of other en hants involved in communications in the independent state a tag does not maintain contact with the other tag in this state the tag needs to decide how much energy it wants to spend on listening to the medium and transmitting pulses to enable others to find it the amount of energy consumed can be controlled by changing the spacing between transmitted pulses and listening periods as well as by changing the overall duty cycle if a tag is very low on energy it could transmit pulses but not listen to the medium this transmit only mode is feasible and logical for enhants since as described in section it is ener getically cheaper for a tag to transmit than to listen accommo dating the presence of such transmit only devices is an interesting networking challenge to start communicating enhants need to synchronize with each other two enhants will be able to start pairing when a pulse burst sent by one tag overlaps with a listening interval of the other tag or when one tag overhears another tag communications with third parties since enhants activity intervals are functions of their energy levels the time to pair is also a function of the tags energy levels once paired enhants need to remain synchronized by pe riodically exchanging short bitstreams the paired state is simi lar to low power modes of ieee and bluetooth however it should be noted that the keep alive messages enhants ex change are short pulse bursts rather than beacons that include tens of bytes the minimum frequency of burst exchanges is limited by the devices clock drifts if high power nodes discussed in section are present it would help with both improving the time to pair and keeping devices synchronized communicating enhants need to coordinate their transmis sions in order to ensure that they do not run out of energy to make joint decisions on communication rates the enhants need to exchange information about their energy states enhants are so energy constrained that exchanges of their energy parameters c e r e may be too costly it is a challenge to determine how much information enhants should exchange e g exchange their current c e r e values or current c e r e values along with a set of predictions of future values and how frequently the information exchange should be conducted in the communicating state a tag energy consumption e is closely related to its data rate lower data rates allow less trans mission and listening enhants need to communicate at ultra low data rates which necessitates making provisions for enhants taking pauses between transmissions of bits this delay tolerance on the bit level is a challenge for networking protocols that often consider a packet as an atomic unit communications of multiple enhants a benefit of the harvesting system is that enhants in close proximity will be subject to common stimuli through their energy harvesting channels examples include lights turning on off or run ning with modulated intensity e g the variation in fluores cent lighting or vibrations felt by more than one tag for instance when a light is turned on a tag can assume that the energy param eters of all its neighbors change and behave accordingly infor mation about the relative similarities or differences between en hants stimuli can provide information about proximity and can be used for synchronization via a channel which is effectively al ways open in communication with each of its neighbors a tag decides on both a state of communication and in the chosen state rate of en ergy consumption e when many devices are involved in commu nication the decisions are far from trivial enhants joint energy decisions on states and rates are a large scale optimization prob lem and a suitable solution for the problem needs to be calculated by low power enhants without extensive exchange of control in formation developing the algorithms that will make this possible and will take into account the realistic considerations discussed in sections and is one of the major challenges for enhants higher layer challenges enhants capabilities also influence the design of higher layer protocols such as protocols for routing and information dissemi nation a variety of energy efficient routing schemes proposed for ad hoc and sensor networks can serve as starting points for en hants routing both information pulling extracting data through a query and information pushing enhants proactively exchang ing information to assist in a pull should be used deciding on the right levels of push and pull is an interesting problem that has strong relation to work in caching and peer to peer networks security and privacy are very important issues for the proposed enhants applications lightweight techniques that have been de signed for sensor networks and for rfids can serve as starting points in enhants security research moreover congestion con trol and interference resolution techniques for enhants could be a future research direction currently short communication ranges and low transmission rates ensure that congestion and interference are not primary concerns enhants as an inventory system in section we outlined enhants communications and network ing challenges and noted that enhants joint decisions on energy management are a large scale optimization problem in this section we introduce inventory control theory as a tool that can potentially be used to approach this problem much like the harvest of a farmer the harvest of a tag needs to be carefully managed in spending their harvest the farmer and the tag both make sure they neither waste harvest due to storage space limitations nor run out of harvest when it is needed this type of problem is not well studied in wireless networking however it has been examined in depth in the mature field of inventory manage ment in this section we show how concepts developed in the inventory management field apply to enhants an energy harvesting tag can be viewed as a manufacturing sys tem composed of a factory and a warehouse consider the ab straction of the energy harvesting system shown in figure which identifies system parameters c r e and e a real world factory warehouse has a finite capacity a rate at which products are manu factured supply rate a rate at which products are purchased de mand rate and a level of inventory all of which directly cor battery level e c figure application of the economic production quantity epq model to the enhants domain with a periodic on off energy source i the energy charge rate r t ii the energy level e t and iii the energy consumption rate e t respond to the listed harvesting system parameters a warehouse inventory management system strives to ensure that the demand is met and that there are no shortages similarly the tag energy management system should ensure that it utilizes its resources to communicate efficiently and does not run out of energy below we explore the similarities between the enhants domain and the inventory management domain we give two examples of direct applications of inventory management models to the enhants do main and demonstrate that due to the distributed operation and the dependencies between enhants parameters see section a net work composed of enhants is more complicated than a manufac turing system this calls for the extension of the well established inventory theory models to the enhants domain deterministic model our first example considers a tag which harvests energy from an on off periodic energy source shown on the the upper graph of figure such an on off source can be found for example in an office environment where an indoor lighting system is turned on in the morning and off at night the source is on during the pe riod tp in which the tag charges at a constant rate r and it is off during td throughout both periods the tag consumes energy at a constant rate ec in the inventory management domain this scenario directly matches the classic economic production quantity epq model it can be easily shown that in this model the highest consumption rate that can be maintained is ec tp r td tp for the enhants domain the consumption rate can be directly translated to bit rate duty cycle or level of communications consider a scenario where a tag with a solar cell of efficiency is located on a dimly lit shelf in an enclosed office where the irradiance is cm2 when the office light is on and when it is off if the light in this office is turned on for hours per day the tag can spend energy at a constant rate of 08µw assuming that reception of one bit requires similar to section this tag will be able to maintain the data rate of throughout the entire diurnal office cycle figure shows an example of all the parameters of this system which directly correspond to the epq model dynamics of the sup ply the demand and the inventory in a warehouse as well as in a tag the epq model is very simple yet it demonstrates an en couraging resemblance of a tag energy harvesting system and a large scale factory warehouse system stochastic model for environments where energy sources are not deterministic other models are needed an example of an inventory model that q e c e min which the behavior of one significantly affects the behavior of the others while some inventory theory models consider multiple warehouses multi echelon models they will need to be extended to capture the complexity of enhants further while in a manu facturing system the central controller has complete knowledge in a network of tags distributed low complexity algorithms using par tial knowledge will have to be employed extending inventory man agement models to handle the unpredictability of enhants and enhants dependencies and creating realistic and implementable enhants energy spending algorithms is an exciting challenge figure application of the order point order quantity q in ventory model to the enhants domain i energy charge rate r t ii the energy level e t and iii the energy consumption rate e t applies directly to the enhants domain is the order point order quantity q model which takes the stochastic nature of demand for inventory into account to avoid shortages in this model the level of inventory is tracked and when it falls below a predetermined level additional q items are ordered this results in the inventory level curve similar to the middle graph in figure the following enhant energy spending policy similar to the battery state based strategy described in results in the same inventory level battery level dynamics a tag spends energy at a constant rate ec but if the tag battery level drops below a prede termined value the tag switches to a safety mode in which it spends energy at a rate not exceeding a minimal rate emin the val ues for and emin should be selected such that a tag in the safety mode is able to function at some level for example pair with a few of its neighbors the tag stays in the minimum spending mode un til its battery level reaches q then it returns to spending energy at its normal ec rate an example of applying the q policy to a tag with a stochastic source is shown in figure novel energy inventory models while as shown above certain inventory management models directly map to the enhants domain enhants and particu larly networks of enhants call for extensions of existing inven tory management models compared to the inventory management domain in the enhants domain the environment is more random with fewer parameters known with certainty and fewer parameters under control for example while warehouse capacity is usually known and constant a tag storage capacity c may not be accu rately known and may change over time also an enhant cannot manufacture more inventory energy when needed and does not control the cycles of production moreover in inventory manage ment the demand is usually stochastic but the supply is mostly de terministic in the enhants domain both the supply energy har vested and the demand communications are likely to be stochas tic existing inventory models need to be extended to take into account this uncertainty and randomness a particular challenge in enhants is the dependency of energy spending of different communicating tags a tag should spend en ergy on transmissions only if the tag it communicates with is ready to spend energy on reception hence one tag energy spending ne cessitates another tag energy spending moreover the transition of different tags between states are somewhat correlated hence the resulting system can be viewed as a network of factories in as can be seen in figure the tag can somewhat control its inventory level by adjusting its energy spending rate testbed design experimentation with the various device designs and algorithms in real world settings is crucial in order to better understand the de sign considerations however wireless mote designs are optimized for wireless sensor applications and typically use ieee rf transceivers these transceivers do not leave room for experimentation with physical layer communications protocols hence we are in the process of building enhants prototypes in the first phase these prototypes will be based on commercial off the shelf cots components currently they are physically much larger and consume more power than the targeted enhant they do not include a uwb transceiver flexible solar cell and a custom battery but will serve as a platform for preliminary ex periments one prototype is based on a mote attached to a taos light to frequency converter for light mea surements and to an si solar cell for energy harvesting another prototype is based on a labjack and a taos light to frequency converter using these prototypes we have been per forming energy harvesting measurements in various environments these measurements will enable us to develop real world energy supply models that will support the development of the algorithms described in sections and in addition we have been using the based prototype to emulate harvesting aware communica tions protocols in the next phase we will replace the cots components with custom designed hardware flexible harvester similar to the one in figure and a uwb transceiver this platform will allow us to demonstrate initial feasibility for the transceiver and harvester and to test networking protocols with the actual energy budgets further testbed information and results will be available at conclusions we believe that ultra low power energy harvesting active net worked tags enhants are enablers for a new type of a wireless network which lies in the domain between sensor networks and rfids while rfids make it possible to identify an object which is in proximity to a reader enhants make it possible to search for an object on a network of devices and continuously monitor objects locations and proximity to each other enhants enable novel tracking applications such as recovery of lost items locat ing items with particular characteristics continuous monitoring of merchandize and assistance in locating survivors of a disaster enhants necessitate rethinking of communication and network ing principles and require careful examination of the particularities of ultra low power and energy harvesting technologies we have shown that the nature of enhants requires a cross layer approach to enable effective communications and networking between de vices with severe power and harvesting constraints in order to discuss the design challenges we outlined several important char acteristics of enhants pointed out a number of open problems and possible research directions and introduced inventory control the energy efficiency of computer systems is an important concern in a variety of contexts in data centers reducing energy use improves operating cost scalability reliability and other factors for mobile devices energy consumption directly affects functionality and usability we propose and motivate joulesort an external sort benchmark for evaluat ing the energy efficiency of a wide range of computer systems from clusters to handhelds we list the criteria challenges and pitfalls from our experience in creating a fair energy efficiency benchmark using a commercial sort we demon strate a joulesort system that is over as energy efficient as last year estimated winner this system is quite differ ent from those currently used in data centers it consists of a commodity mobile cpu and laptop drives connected by server style i o interfaces categories and subject descriptors h information systems database management systems general terms design experimentation measurement performance keywords benchmark energy efficiency power servers sort introduction in contexts ranging from large scale data centers to mobile devices energy use in computer systems is an important concern in data center environments energy efficiency affects a number of factors first power and cooling costs are signifi cant components of operational and up front costs today a typical data center with racks consuming total power costs to power and to cool per year with permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee sigmod june beijing china copyright acm of up front costs for cooling equipment these costs vary depending upon the installation but they are growing rapidly and have the potential eventually to outstrip the cost of hardware second energy use has implications for density reliability and scalability as data centers house more servers and consume more energy removing heat from the data center becomes increasingly difficult since the reliability of servers and disks decreases with increased temperature the power consumption of servers and other components limits the achievable density which in turn lim its scalability third energy use in data centers is starting to prompt environmental concerns of pollution and excessive load placed on local utilities energy related concerns are severe enough that companies like google are starting to build data centers close to electric plants in cold weather cli mates all these concerns have led to improvements in cooling infrastructure and in server power consumption for mobile devices battery capacity and energy use di rectly affect usability battery capacity determines how long devices last constrains form factors and limits functional ity since battery capacity is limited and improving slowly device architects have concentrated on extracting greater energy efficiency from the underlying components such as the processor the display and the wireless subsystems in isolation to drive energy efficiency improvements we need bench marks to assess their effectiveness unfortunately there has been no focus on a complete benchmark including a work load metric and guidelines to gauge the efficacy of energy optimizations from a whole system perspective some efforts are under way to establish benchmarks for energy efficiency in data centers but are incomplete other work has emphasized metrics such as the energy delay product or per formance per watt to capture energy efficiency for proces sors and servers without fixing a workload moreover while past emphasis on processor energy efficiency has led to improvements in overall power consumption there has been little focus on the i o subsystem which plays a significant role in total system power for many important workloads and systems in this paper we propose joulesort as a holistic bench mark to drive the design of energy efficient systems joule sort uses the same workload as the other external sort bench marks but its metric incorporates total energy which is a combination of power consumption and perfor mance the benchmark can be summarized as follows sort a fixed number of randomly permuted byte records with byte keys the sort must start with input in a file on non volatile store and finish with output in a file on non volatile store there are three scale categories for joulesort and records the winner in each category is the system with the minimum total energy use we choose sort as the workload for the same basic rea son that the terabyte sort minutesort pennysort and performance price sort benchmarks do it is simple to state and balances system component use sort stresses all core components of a system memory cpu and i o sort also exercises the os and filesystem sort is a portable workload it is applicable to a variety of systems from mobile devices to large server configurations another natural reason for choosing sort is that it represents sequen tial i o tasks in data management workloads joulesort is an i o centric benchmark that measures the energy efficiency of systems at peak use like previous sort benchmarks one of its goals is to gauge the end to end ef fectiveness of improvements in system components to do so joulesort allows us to compare the energy efficiencies of a variety of disparate system configurations because of the simplicity and portability of sort previous sort bench marks have been technology trend bellwethers for example foreshadowing the transition from supercomputers to clus ters similarly an important purpose of joulesort is to chart past trends and gain insight into future trends in energy ef ficiency beyond the benchmark definition our main contributions are twofold first we motivate and describe pitfalls sur rounding the creation of a fair energy efficiency benchmark we justify our fairest formulation which includes three scale factors that correspond naturally to the dominant classes of systems found today mobile desktop and server al though we support both daytona commercially supported and indy no holds barred categories for each scale we concentrate on daytona systems in this paper second we present the winning joulesort system that is over more efficient sortedrecs joule for than last year estimated winner sortedrecs joule for this system shows that a focus on energy effi ciency leads to a unique configuration that is hard to find pre assembled our winner balances a low power mobile processor with numerous laptop disks connected via server class pci e i o cards and uses a commercial sort nsort the rest of the paper is organized as follows in section we estimate the energy efficiency of past sort benchmark winners which suggests that existing sort benchmarks can not serve as surrogates for an energy efficiency benchmark section details the criteria and challenges in designing joulesort and lists issues and guidelines for proper energy measurement in section we measure the energy con sumption of unbalanced and balanced systems to motivate our choices in designing our winning system the balanced system shows that the i o subsystem is a significant part of total power section provides an in depth study of our joule sort system using nsort in particular we show that the most energy efficient cost effective and best performing configuration for this system is when the sort is cpu bound year figure estimated energy efficiency of previous winners of sort benchmarks we also find that both the choice of filesystem and in memory sorting algorithm affect energy efficiency section discusses the related work and section presents limitations and fu ture directions historical trends in this section we seek to understand if any of the exist ing sort benchmarks can serve as a surrogate for an energy efficiency benchmark to do so we first estimate the sort edrecs joule ratio a measure of energy efficiency of the past decade sort benchmark winners this analysis reveals that the energy efficiency of systems designed for pure per formance i e minutesort terabyte sort and datamation winners has improved slowly moreover systems designed for price performance i e pennysort winners are compar atively more energy efficient and their energy efficiency is growing rapidly however since our joulesort sys tem energy efficiency is well beyond what growth rates would predict for this year pennysort winner we conclude that existing sort benchmarks do not inherently provide an incentive to optimize for energy efficiency supporting the need for joulesort methodology figure shows the estimated sortedrecs joule metric for the past sort benchmark winners since we compute these metrics from the published performance records and our own estimates of power consumption since energy use was not reported we obtain the performance records and hardware configuration information from the sort bench mark website and the winners posted reports we estimate total energy during system use with a straight forward approach from the power management community since cpu memory and disk are usually the main power consuming system components we use individual estimates of these to compute total power for memory and disks we use the hp enterprise configurator power calcu lator to yield a fixed power of per disk and per dimm some of the sort benchmark reports only mention total memory capacity and not the number of dimms in those cases we assume a dimm size appropriate to the era of the report the maximum power specs for cpus usually quoted as thermal design power tdp are much higher than the peak numbers seen in common use thus we derate these power ratings by a factor although a bit con servative this approach allows reasonable approximations for a variety of systems when uncertain we assume the newest possible generation of the reported processor as of the sort benchmark record because a given cpu power consumption improves with shrinking feature sizes finally to account for power supplies inefficiencies which can vary widely and other components we scale total system power derived from component level estimates by for single node systems we use a higher factor for clusters to account for additional components such as networking management hardware and redundant power supplies our power estimates are intended to illuminate coarse his torical trends and are accurate enough to support the high level conclusions in this section we experimentally vali dated this approach against some server and desktop class systems and its accuracy was between and analysis although previous sort benchmark winners were not con figured with power consumption in mind they roughly re flect the power characteristics of desktop and higher end sys tems in their day thus from the data in figure we can in fer qualitative information about the relative improvements in performance price performance and energy efficiency in the last decade figure compares the energy efficiency of previous sort winners using the sortedrecs joule ratio and supports the following observations systems optimized for price performance i e pennysort winners clearly are more energy efficient than the other sort benchmark winners which were optimized for pure perfor mance there are two reasons for this effect first the price performance metric motivates system designers to use fewer components and thus less power second it provides incentive to use cheaper commodity components which for a given performance point traditionally have used less en ergy than expensive high performance components the energy efficiency of cost conscious systems has im proved faster than that of performance optimized systems which have hardly improved others have also observed a flat energy efficiency trend for cluster hardware much of the growth in the pennysort curve is from the last two indy winners which have made large leaps in energy efficiency in algorithmic improvements and a minimal hardware configuration played a role in this improvement but most importantly cpu design trends had finally swung toward energy efficiency the processor used in the pennysort winner has the clock frequency of its immediate prede cessor while only consuming the power overall the sort had better performance than the previous data point while using the power the pennysort win ner gputerasort increased energy efficiency by introduc ing a new system component the graphics processing unit gpu and utilizing it very effectively the chosen gpu is inexpensive and comparable in power consumption to the cpu but it provides better streaming memory bandwidth than the cpu this latest winner in particular shows the danger of rely ing on energy benchmarks that focus only on specific hard ware like cpu or disks rather than end to end efficiency such specific benchmarks would only drive and track im benchmark srecs sec srecs srecs j pennysort yr yr yr minute terabyte and datamation yr n a yr table this table shows the estimated yearly growth in pure performance price performance and energy efficiency of past winners provements of existing technologies and may fail to antici pate the use of potentially disruptive technologies since price performance winners are more energy efficient we next examine whether the most cost effective sort implies the best achievable energy efficient sort to do so we first estimate the growth rate of sort winners along multiple di mensions table shows the growth rate of past sort bench mark winners along three dimensions performance sort edrecs sec price performance sortedrecs and energy efficiency sortedrecs joule we separate the growth rates into two categories based on the benchmark optimization goal price or pure performance since the goal drives the system design for each category we calculate the growth rate as follows we choose the best system according to the metric in each year and fit the result with an exponential table shows that pennysort systems are improving al most at the pace of moore law along the performance and price performance dimensions the pure performance sys tems however are improving much more slowly as noted elsewhere more importantly our analysis shows much slower esti mated growth in energy efficiency than in the other two metrics for both benchmark categories given last year estimated pennysort winner provides srecs j our current joulesort winner at srecs j is nearly the expected value of srecs j for this year this result suggests that we need a benchmark focused on en ergy efficiency to promote development of the most energy efficient sorting systems and allow for disruptive technologies in energy efficiency irrespective of cost benchmark design in this section we detail the criteria and challenges in de signing an energy efficiency benchmark we describe some of the pitfalls of our initial specifications and how the bench mark has evolved we also specify rules of the benchmark with respect to both workload and energy measurement criteria although past studies have proposed energy efficiency met rics or power measurement techniques none provide a complete benchmark a workload a metric of comparison and rules for running the workload and mea suring energy consumption moreover these studies tradi tionally have focused on comparing existing systems rather than providing insight into future technology trends we set out to design an energy oriented benchmark that addresses these drawbacks with the criteria below in mind while achieving all these criteria simultaneously is hard we strive to encompass them as much as possible energy efficiency the benchmark should measure a sys tem bang for the buck where bang is work done and the cost reflects some measure of power use e g average power peak power total energy and energy delay to drive practical improvements in power consumption cost should reflect both a system performance and power use a sys tem that uses almost no power but takes forever to complete a task is not practical so average and peak power are poor choices thus there are two reasonable cost alternatives energy a product of execution time and power or energy delay a product of execution time and energy the former weighs performance and power equally while the latter pop ular in cpu centric benchmarks places more emphasis on performance since there are other sort benchmarks that emphasize performance we chose energy as the cost peak use a benchmark can consider system energy in three important modes idle peak use or a realistic combi nation of the two although minimizing idle mode power is useful evaluating this mode is straightforward real world workloads are often a combination but designing a broad benchmark that addresses a number of scenarios is difficult to impossible hence we chose to focus our bench mark on an important but simpler case energy efficiency during peak use energy efficiency at peak is the opposite extreme from idle and gives an upper bound on work that can be done for a given energy this operating point influ ences design and provisioning constraints for data centers as well as mobile devices in addition for some applications e g scientific computing near peak use can be the norm holistic and balanced a single component cannot accu rately reflect the overall performance and power character istics of a system therefore the workload should exercise all core components and stress them roughly equally the benchmark metrics should incorporate energy used by all core components inclusive and portable we want to assess the energy ef ficiencies of a wide variety of systems pdas laptops desk tops servers clusters etc thus the benchmark should include as many architectures as possible and be as unbi ased as possible it should allow innovations in hardware and software technology moreover the workload should be implementable and meaningful across these platforms history proof in order to track improvements over gen erations of systems and identify future profitable directions we want the benchmark specification to remain meaningful and comparable as technology evolves representative and simple the benchmark should be representative of an important class of workloads on the sys tems tested it should also be easy to set up execute and administer workload we begin with external sort as specified in the previous sort benchmarks as the workload because it covers most of our criteria the task is to sort a file containing randomly permuted byte records with byte keys the input file must be read from and the output file written to a non volatile store and all intermediate files must be deleted the output file must be newly created it cannot overwrite the input file this workload is representative because most platforms from large to small must manage an ever increasing sup ply of data to do so they all perform some type of i o centric tasks critical for their use for example large scale websites run parallel analyzes over voluminous log data across thousands of machines laptops and servers con tain various kinds of filesystems and databases cell phones pdas and cameras store retrieve and process multimedia data from flash memory with previous sort implementations on clusters super computers smps and pcs as evidence we believe sort is portable and inclusive it stresses i o memory and the cpu making it holistic and balanced moreover the fastest sorts tend to run most components at near peak utilization so sort is not an idle state benchmark finally this work load is relatively history proof while the parameters have changed over time the essential sorting task has been the same since the original datamationsort benchmark was proposed in metric after choosing the workload the next challenge is choos ing the metric by which to evaluate and compare different systems there are many ways to define a single metric that takes both power and performance into account we list some alternatives that we rejected describe why they are inappropriate and choose the one most consistent with the criteria presented in section fixed energy budget the most intuitive extension of minutesort and pennysort is to fix a budget for energy consumption and then com pare the number of records sorted by different systems while staying within that energy budget this approach has two drawbacks first the power consumption of current plat forms varies by several orders of magnitude less than for handhelds to over for servers and much more for clusters or supercomputers if the fixed energy budget is too small larger configurations can only sort for a frac tion of a second if the energy budget is more appropriate to larger configurations smaller configurations would run out of external storage to be fair and inclusive we would need multiple budgets and categories for different classes of systems second and more importantly from a practical bench marking perspective finding the number of records to fit into an energy budget is a non trivial task due to unavoid able measurement error there are inaccuracies in synchro nizing readings from a power meter to the actual runs and from the power meter itself for the one we used since energy is the product of power and time it is suscep tible to variation in both quantities so this choice is not simple fixed time budget similar to the minute and performance price sort we can fix a time budget e g one minute within which the goal is to sort as many records as possible the winners for the minute and performance price sorts are those with the min imum time and maximum sortedrecs respectively sim ilarly our first proposal for joulesort specified measuring energy and used sortedrecs joule as the ratio to maximize there are two problems with this approach which are illustrated by figure this figure shows the srecs j ra tio for varying input sizes n with our winning joulesort system we see that the ratio varies considerably with n there are two distinct regions records which 07 08 0e records sorted figure this figure shows the best measured en ergy efficiency of our winning system at vary ing input sizes corresponds to pass sorts and records which corresponds to pass sorts to get the best performance for pass sorts we stripe the input and output across disks using and use disks for temporary runs for pass sorts we stripe the input and output across disks see section for more system details with a fixed time budget approach the goals of our benchmark can be undermined in the following ways for both one and two pass sorts sort progress incentive first in any time budget ap proach there is no way to enforce continual progress sys tems will continue sorting only if the marginal cost of sort ing an additional record is lower than the cost of sleeping for the remaining time this tradeoff becomes problematic when an additional record moves the sort from pass to pass in the pass region of figure the sort is i o lim ited so it does not run twice as fast as a pass sort it goes fast enough however to provide about better efficiency than pass sorts if the system was designed to have a suf ficiently low sleep state power then with a minute budget the best approach would be to sort records which takes sec and sleep for the remaining sec re sulting in a best srecs j thus for some systems a fixed time budget defaults into assessing efficiency when no work is done violating our criteria sort complexity second even in the pass region total energy is a complex function of many performance factors that vary with n total i o memory accesses comparisons cpu utilization and effective parallelism figure shows that once the sort becomes cpu bound records the srecs j ratio trends slowly downward because total en ergy increases superlinearly with n the ratio for the largest sort is lower than the peak this decrease is in part because sorting work grows as o n lg n due to compar isons and the o notation hides constants and lower order overheads this effect implies that the metric is biased to ward systems that sort fewer records in the allotted time that is even if two fully utilized systems a and b have same true energy efficiency and a can sort twice as many records as b in a minute the sortedrecs joule ratio will favor b note since this effect is small our relative com parisons and conclusions in section remain valid our choice fixed input size the final option that we considered and settled upon was to fix the number of records sorted as in the terabyte sort benchmark and use total energy as the metric to min imize for the same fairness issues as in the fixed energy case we decided to have three scales for the input size and records similar to tpc h and declare win ners in each category for consistency henceforth we use mb gb and tb for and bytes respectively for a fixed input size minimum energy and maximum sorte drecs joule are equivalent metrics in this paper we prefer the latter because like an automobile mileage rating it highlights energy efficiency more clearly this approach has advantages and drawbacks but offers the best compromise given our criteria these scales cover a large spectrum and naturally divide the systems into classes we expect laptops desktops and servers moreover since energy is a product of power and time a fixed work ap proach is the simplest formulation that provides an incentive to optimize power consumption and performance both are important concerns for current computer systems one disadvantage is that as technologies improve scales must be added at the higher end and may need to be dep recated at the lower end for example if the performance of joulesort winners improves at the rate of moore law year a system which sorts a in sec to day would only take sec in years once all relevant systems require only a few seconds for a scale that scale becomes obsolete since even the best performing sorts are not improving with moore law we expect these scales to be relevant for at least years finally because compar ison across scales is misleading our approach is not fully history proof categories as with the other sort benchmarks we pro pose two categories for joulesort daytona for commer cially supported sorts and indy for no holds barred im plementations since daytona sorts are commercially sup ported the hardware components must be off the shelf and unmodified and run a commercially supported os as with the other sort benchmarks we expect entrants to report the cost of the system measuring energy there are a number of issues surrounding the proper ac counting of energy use specific proposals in the power management community for measuring energy are being de bated and are still untested in the large once these are agreed upon we plan to adopt the relevant portions for this benchmark as a start we propose guidelines for three areas the boundaries of the system to be measured envi ronmental constraints and energy measurement system boundaries our aim is to account for all en ergy consumed to power the physical system executing the sort all power is measured from the wall and includes any conversion losses from power supplies for both ac and dc systems power supplies are a critical component in deliv ering power and in the past have been notoriously ineffi cient some dc systems especially mobile devices can run from batteries and those batteries must eventually be recharged which also incurs conversion loss while the loss from recharging may be different from the loss from system cpu memory disk os fs intel xeon ghz ddr linux xfs blade transmeta efficeon ghz sdram windows ntfs intel core duo windows xp ntfs table the unbalanced systems measured in exploring energy efficiency tradeoffs for sort the adapter that powers a device directly for simplicity we allow measurements that include only adapters all hardware components used to sort the input records from start to finish idle or otherwise must be included in the energy measurement if some component is unused but cannot be powered down or physically separated from adja cent participating components then its power use must be included if there is any potential energy stored within the system e g in batteries the net change in potential energy must be no greater than zero joules with confidence or it must be included within the energy measurement environment the energy costs of cooling are important and cooling systems are variegated and operate at many lev els in a typical data center there are air conditioners blow ers and recirculators to direct and move air among aisles and heat sinks and fans to distribute and extract heat away from system components given recent trends in energy density future systems may even have liquid cooling it is diffi cult to incorporate anticipate and enforce rules for all such costs in a system level benchmark for simplicity we only include a part of this cost one that is easily measurable and associated with the system being measured we specify that a temperature between c should be maintained at the system inlets or within foot of the system if no inlet exists energy used by devices physically attached to the sorting hardware that remove heat to maintain this temper ature e g fans must be included energy use total energy is the product of average power over the sort execution and wall clock time as with the other sort benchmarks wall clock time is measured using an external software timer the easiest method to measure power for most systems will be to insert a digital power me ter between the system and the wall we intend to leverage the minimum power meter requirements from the spec power draft in particular the meter must report real power instead of apparent power since real power reflects the true energy consumed and charged for by utilities while we do not penalize for poor power factors a power factor measured anytime during the sort run should be re ported finally since energy measurements are often noisy a minimum of three consecutive energy readings must be re ported these will be averaged and the system with mean energy lower than all others including previous years with confidence will be declared the winner summary in summary the joulesort benchmark is as follows sort a fixed number of randomly permuted byte records with byte keys the sort must start with input in a file on non volatile store and finish with output in a file on non volatile store there are three scale categories for joulesort and records the total true energy consumed by the entire physical system executing the sort while maintaining an ambi ent temperature between should be reported the winner in each category is the system with the maximum sortedrecs joule i e minimum energy joulesort is a reasonable choice among many possible options for an energy oriented benchmark it is an i o centric system level energy efficiency benchmark that in corporates performance power and some cooling costs it is balanced portable representative and simple we can use it to compare different existing systems to evaluate the energy efficiency balance of components within a given system and to evaluate different algorithms that use these components these features allow us to chart past trends in energy efficiency and hopefully will help predict future trends a look at different systems in this section we measure the energy and performance of a sort workload on both unbalanced and balanced sort ing systems we analyze a variety of systems from laptops to servers that were readily available in our lab for the unbalanced systems the goal of these experiments is not to painstakingly tune these configurations rather we present results to explore the system hardware space with respect to power consumption and energy efficiency for sort after looking at unbalanced systems we present a balanced file server that is our default winner we use insights from these experiments to justify the approach for constructing our joulesort winner see section unbalanced systems configurations table shows the details of the unbal anced systems we evaluated spanning a reasonable spec trum of power consumption in servers and personal com puters we include a server an older low power blade and an a modern laptop we chose the laptop be cause it is designed for whole system energy conservation and and for comparison we turned off the laptop display for these experiments for we only used blade in an enclosure that holds and as per our rules report the power of the entire system sort workload we use ordinal technology commercial nsort software which was the terabyte sort daytona winner it uses asynchronous i o to overlap reading writ ing and sorting operations it performs both one and two pass sorts we tuned nsort parameters to get the best performing sort for each platform unless otherwise stated we use the radix in memory sort option recs power w time srecs j cpu util 596 727 1323 table energy efficiency of unbalanced systems power measurement to measure the full system ac power consumption we used a digital power meter inter posed between the system and the wall outlet we sampled this power at a rate of once per second the meter used was brand electronics model which reports true power with accuracy in this paper we always re port the average power over several trials and the standard deviation in the average power results the joulesort results for our unbalanced systems are shown in table since disk space on these systems was limited we chose to run the benchmark at and a smaller dataset to allow fair comparison we see that the server is the fastest but the laptop is most energy efficient system uses over more power than but only provides better performance although disks can provide more sequential bandwidth was limited by its smartarray i o controller to mb in each pass sys tem the blade is not as bad as the results show because blade enclosures are most efficient only when fully popu lated the enclosure power without any blades was when we subtract this from the total power we get an upper bound of srecs j for for all these systems the standard deviation of total power during sort was at most the power factor pf for and were and respectively the cpus for all three systems were highly underutilized in particular attains an energy efficiency similar to that of last year estimated winner gputerasort by barely us ing its cores since the cpu is usually the highest power component these results suggest that building a system with more i o to complement the available processing capacity should provide better energy efficiencies balanced server in this section we present a balanced system that usually functions as a fileserver in our lab table shows the com ponents used during the sort and coarse breakdowns of total system power the main system is an hp proliant that includes a motherboard cpu low power laptop disk and a high throughput sas i o controller for the storage we use two disk trays one that holds the input and output files and the other which holds the temp disks each tray has disks and can hold a maximum of the disk trays and main system all have dual power supplies but for these experiments we powered them through one each for all our experiments the system has bit ubuntu linux and the xfs filesystem installed table shows that for a server of this kind the disks and their enclosures consume roughly the same power as the rest of the system when a tray is fully populated with disks table a balanced fileserver the idle power is w and with disks the idle power is w there clearly are inefficiencies when the tray is under utilized to estimate the power of the dimms we added two dimms and measured the system power with and without the dimms we found that the dimms use both during sort and at idle for this system we found the most energy efficient config uration by experimenting with a dataset by varying the number of disks used we found that even with the inef ficiencies the best performing setup uses disks split across two trays this effect happens because the i o con troller offers better bandwidth when data is shipped across its two channels a sort provides on av erage for each phase across the trays while only when the all disks are within a tray the average power of the system with only one tray is and with two trays is as a result with two trays the system attains a best srecs j instead of srecs j with one tray the tray disk setup is also when the sort becomes cpu bound when we reduce the system to disks the i o performance and cpu utilization drop and when we increase the system to disks the performance and uti lization remain the same in both cases total energy is higher than the disk point so this balanced cpu bound configuration is also the most energy efficient table shows the performance and energy characteristics of the disk setup for sorts this system takes nearly more power than but provides over the through put this system srecs j ratio beats the laptop and last year estimated winner even with a larger input ex periments similar to those for the dataset show that this setup provides just enough i o to keep the two cores fully utilized on both passes and uses the minimum energy for the scale thus at all scales the most energy efficient and best performing configuration for this system is when sort is cpu bound and balanced comp model price power cpu intel core duo tdp motherboard asus vm dh n a case psu apevia x navigator bk n a disk ctrl highpoint rocket raid disk ctrl highpoint rocket raid memory kingston ddr2 63 spec disk hitachi travelstar rpm gb a i spec adapters table winning system summary in conclusion from experimenting with these systems we learned cpu is wasted in unbalanced systems the most energy efficient server configuration is when the sys tem is cpu bound an unbalanced laptop is almost as energy efficient as a balanced server moreover current lap top drives use vs w less power than our server sata drives while offering around vs mb the bandwidth these observations suggest a reasonable ap proach for building the most energy efficient sorting system is to use mobile class cpus and disks and connect them via a high speed i o interconnect joulesort winner in this section we first describe our winning joulesort configuration and report its performance we then study this system through experiments that elucidate power and performance characteristics of this system winning configuration given limited time and budget our goal was to convinc ingly overtake the previous estimated winner rather than to try numerous combinations and construct an absolute op timal system as as result we decided to build a daytona system and solely use nsort as the software our design strategy for an energy efficient sort was to build a balanced sorting system out of low power components after esti mating the sorting efficiency of potential systems among a limited combination of modern low power processors and laptop disks we assembled the configuration in table this system uses a modern low power cpu with fre quency states and a tdp of for the highest state we use a motherboard that supports both a mobile cpu and multiple disk controllers to keep the cores busy few such boards exist because they target a niche market this one includes two pci e slots one channel and one channel to fill those slots we use controllers that hold and sata drives respectively finally our configuration uses low power laptop drives which support the sata in terface they offer an average ms seek time and their measured sequential bandwidth through xfs is around mb hitachi specs list an average for read and write and for active idle we use two dimms whose specs report for each finally the case comes with a power supply our optimal configuration uses disks because the pci e cards hold disks maximum and the i o performance of the motherboard controller with more than disk is poor the input and output files are striped across a disk array configured via and the remaining disks are inde pendent for the temporary runs for all experiments we use linux kernel and the xfs filesystem unless otherwise stated in the idle state at the lowest cpu frequency we measured w for this system table shows the performance of the system which at tains srecs j when averaged over consecutive runs the pure performance statistics are reported by nsort we configure it to use radix sort as its in memory sort algo rithm and use transfer sizes of for the input output array and for the temporary storage our system is faster than gputerasort and consumes an estimated less power the power use during sort is more than idle in the output pass the cpu is underutilized see ta ble max for cores and the bandwidth is lower than in the input pass because the output pass requires ran dom i os we pin the cpu to mhz which section shows is the most energy efficient frequency for the sort varying system size in these experiments we vary the system size disks and controllers and observe our system pure performance cost efficiency and energy efficiency we investigate these met rics using a dataset for the first two metrics we set the cpu to its highest frequency and report the metrics for the most cost effective and best performing configurations at each step we start with disks attached to the cheaper disk controller and at each step use the minimum cost hardware to support an additional disk thus we switch to the disk controller for configurations with disks and use both controllers combined for disks finally we add a disk directly to the motherboard for the disk con figuration figure shows the performance records sec and cost efficiency with increasing system size the disk config uration is both the best performing and most cost efficient point each additional disk on average increases system cost by about and improves performance by on average these marginal changes vary they are larger for small sys tem size and smaller for larger system sizes the disk point drops in cost efficiency because it includes the expen sive disk controller without a commensurate performance increase although the motherboard and controllers limit the system to disks we speculate that additional disks would not help since the first pass of the sort is cpu bound next we look at how energy efficiency varies with with system size at each step we add the minimum energy hard ware to support the added disk and report the most energy efficient setup we set the cpu frequency to at all points to get the best energy efficiency see section for convenience we had one extra os disk on the mother board from which we boot and which was unused in the sort for all but the last point the power measurements include this disk but this power is negligible at idle system recs srecs j energy kj power w time sec bw in out total mb cpu util max pf table low power 41 03 99 219 154 table server 2920 282 179 table performance of winning joulesort systems disks used 10000 2000 disks used figure shows how performance price and perfor mance varies with system size sort idle disks used figure shows how power varies with system size figure shows idle power at the lowest frequency state versus average power during sort at mhz for the same system configurations with the system at idle and only the motherboard disk installed our measurements show that the disk controller uses and the disk one uses thus for points between disks we use only the disk controller between we use the only disk controller and for or more we use both figure shows jumps at these transitions the idle line indicates adding a disk increases power by 1w during sorting adding a disk increases total power on average at sizes fewer than disks and 0w on average for more than disks these increases reflect end to end utilization of the cpu disk controllers etc figure shows the energy efficiency with increasing num ber of disks used in the sort the curve is similar to the price performance curve in figure the average increase figure shows how energy efficiency varies with system size in energy at each step is while the average increase in performance is about the disk point again is a local minimum because it incurs the power of the larger controller without enough disks to take advantage of it the sort is cpu bound in the most energy efficient configuration there are two main points to take away from these exper iments first the similar shapes of these curves reflect that the base dollar and energy costs of the system are high com pared to the marginal dollar and energy cost of disks if we used server class disks that are similar in cost but consume the power of mobile disks we would see different cost and energy efficiency curves second for the components we chose the best performing most cost efficient and most energy efficient configurations are identical modulo the cpu frequency moreover in this best configuration the system is balanced with just enough i o bandwith to keep the cpu fully utilized for the first pass software matters next we vary the filesystem and in memory sort algo rithm to see how they affect energy efficiency the winning configuration uses the xfs filesystem and a radix sort figure examines the effect of changing the filesystem to reiserfs and the sort algorithm to merge sort at different cpu frequencies for a dataset as expected power consumption steadily increases with frequency in all cases the power consumptions of xfs with radix sort and merge sort are similar at all frequen cies reiserfs however consumes less power and also is less energy efficient all three configurations show improved energy efficiency from mhz to mhz and then level off or decrease this result indicates that the sorts are cpu bound at the lower frequencies reiserfs shows a im provement in performance between the lowest and highest 2324 on demand cpu freq mhz 10000 6000 2000 ing this sort cpu utilization was an average per core so we assign to the cpu subsystem similarly we discount the copying test for its cpu utilization and estimate that the i o subsystem uses these estimates combine to and almost match error the 5w measured increase due to the disk sort thus our tests imply that about of the power increase during sort is from the i o subsystem and from the cpu subsystem we found similar proportions at smaller system sizes vary dimms and power supply since nsort uses only a fraction of the available mem ory for these experiments we ran experiments with only dimm power use and execution time were statistically in distinguishable from the dimm case during sort power figure shows how average power and energy ef ficiency vary with cpu frequency for a sort frequencies while xfs radix improves only and xfs merge improves only by reiserfs has worse energy efficiency mainly because it provides less sequential bandwidth and thus worse perfor mance than xfs although we tuned each configuration this result may be an artifact of our setup and not an in herent flaw of reiserfs similarly the merge sort also gives worse energy efficiency than radix entirely because its per formance is worse the graph also shows the power and energy efficiency of the linux on demand cpu frequency scaling policy which is within of the lowest execution time and of the lowest power for all three configurations for reiserfs the on demand policy offers the same efficiency as the best con figuration in summary these experiments show that the algorithms and underlying software used for sort affect en ergy efficiency mainly through performance approximate cpu vs i o breakdown we performed some micro benchmarks exercising the i o subsystem disks plus controllers and the cpu subsystem cpu plus memory separately to determine how much each contributes to the increase in power during sort the system at the disk point consumes 5w more during sort than when the system is at idle with mhz cpu frequency this increase is nearly of the total power during sort our benchmarks suggest that the i o subsystem consumes a much greater fraction of this power increase than the cpu subsystem we first performed a test to help approximate the contri bution from the i o subsystem in this test we copied data from a disk array to another disk array which had the same average disk bandwidth as the disk sort we found that the system power increased by 5w during this test the cpu utilization was out of max for cores next we performed an experiment to approximate the con tribution from the cpu subsystem we put a small input file on a ram disk and repeatedly sorted it this test pegged the cpu to utilization and the power increase was using the above values and assuming that cpu subsystem power increases linearly with cpu utilization we estimate its contribution during the disk sort as follows dur use is also within measurement error at idle we replaced the power supply with a one and found that the power consumption during sort and idle increased by this suggests that at load or less efficiencies of the two power supplies are similar note the power factors for the laptops and desktop sys tems in this paper including our winner are well below low power factors are problematic in data cen ters because power delivery mechanisms need to be over provisioned to carry additional current for loads with low power factors utilities often charge extra for this pro visioning similar to server class systems power supplies will need to provide power factor correction for systems like our winner to become a reality in data centers summary we describe the daytona joulesort system that is over as energy efficient as last year pennysort win ner the gputerasort for this system we show the most energy efficient sorting configuration is when the sort is cpu bound and balanced this configuration is also the best performing and most cost efficient it will be interesting to see how long this relationship holds we see that filesystem and in memory sort choice mainly affect energy efficiency through performance rather than power for this system in this paper we focused on building a balanced system with low power off the shelf components targeted for the scale unfortunately because of hardware limita tions and market availability we could not easily scale this system to the category in the future we expect sys tems in other classes to win the and categories but for completeness we report in table the best configu rations we encountered for those categories related work our related work falls into three categories we first dis cuss the history of sort benchmarks and large scale sorting techniques next we cover the previous work on metrics for evaluating energy efficiency finally we briefly discuss work on techniques for reducing energy consumption in systems sort benchmarks and techniques the original datamation sort benchmark was a pure per formance benchmark that measured the time to sort a mil lion records in the developers of alphasort recognized that the benchmark was losing its relevance be cause startup and shutdown would eventually dominate the time to sort such a small number of records they there fore proposed two variants minutesort and pennysort hop ing they would remain relevant as technology improved at the pace of moore law recognizing that pennysort was biased against large configurations by allowing too small a time budget researchers then proposed the performance price sort which is tied to the minutesort time budget the time budget approach undermines the goals of joulesort since the original datamation sort benchmark there have been many different implementations of external sort on a variety of platforms from desktops to supercomputers the sort benchmark website maintained by jim gray lists the winners and briefly surveys past trends energy benchmarks several different metrics have been proposed for evalu ating the energy efficiency of computer systems in gonzalez et al proposed the energy delay product as the metric of energy efficient microprocessor design alter natively the metric of performance per watt is also widely used to evaluate processors energy efficiency this met ric emphasizes performance less than the energy delay prod uct which is equivalent to performance squared per watt energy efficiency metrics tailored to data centers have also been proposed sun space watts and performance metric swap considers the rack space taken up by a hardware configuration along with its power and performance in an effort to promote data center compaction metrics based on exergy which is the energy converted into less efficient forms such as heat take into account every aspect of the data center from processors to the cooling infrastructure however these metrics are not applicable to the entire range of systems we want to evaluate with joulesort comparatively little work has been done on workloads for energy efficiency benchmarks in the embedded domain the eembc energybench benchmarks provide a physical in frastructure to evaluate a single processor energy efficiency on any of eembc existing mobile benchmark suites in the enterprise domain the spec power and performance committee is currently developing an energy benchmark suite for servers and the united states environmental pro tection agency energystar program is developing a way to rate the energy efficiency of servers and data centers energy efficiency there is a large body of prior work on energy efficiency for example at the component and system levels many studies have been devoted to algorithms for dynamically exploiting different power states in processors memory and disks in order to promote energy ef ficiency in clusters and data centers research has focused on energy efficient workload distribution and power budget ing e g other studies have focused at the application level including energy aware user interfaces and fidelity aware energy management conclusions in this section we summarize the limitations of joulesort speculate on future energy efficient systems and wrap up limitations joulesort does not address all possible energy related con cerns since joulesort focuses on data management tasks it misses some important energy relevant components for mul timedia applications joulesort omits displays which are an important component of total power for mobile devices gpus also consume significant p ower and a re u biquitous in desktop systems although we can use gpus to sort our benchmark does not require their use as a result it loses relevance for applications where these components are es sential there are other energy related concerns in data centers beyond system power that were difficult to incorporate at a high level cooling requires lowering ambient temper ature and extracting heat away from systems joule sort accounts only for part of the second delivering power to systems incurs losses at the rack and data center level which are ignored in joulesort moreover many systems are used as an ensemble in data centers with sophisti cated scheduling techniques to trade performance for lower energy among systems rather than at the component level as a system level benchmark joulesort may not identify the benefits o f uch methods greener systems we speculate on two emerging technologies that may im prove the energy efficiency of systems for the scale flash m emory a ppears to b e a p romising torage technology driven by the mobile device market per byte it is about cheaper than dram and provides sequential read and write bandwidth close to that of disks more importantly random read i os with flash a re f aster t han d isk and flash consumes less power than d isks the random reads allow interesting modifications t o t raditional pass sorting algorithms to date the largest cards at reasonable cost are we anticipate a system such as a laptop or low power embedded device that can leverage multiple flash devices as the next winner for the larger scales an intriguing option is a hybrid sys tem using a low power cpu laptop disks and a gpu the gputerasort has shown that gpus can provide much bet ter in memory sorting bandwidth than cpus using a motherboard that supports more i o controllers and a gpu we could scale our system to use more disks an interest ing question is whether these performance benefits might be offset by the recent trend in gpus to consume more power closing this paper proposes joulesort a simple balanced energy efficiency benchmark we present a complete benchmark a workload metric and guidelines and justify our choices we also present a winner that is over as efficient as last year estimated winner today this system is hard to find p re assembled i t c onsists o f a c ommodity mobile class cpu and laptop disks connected through server class pci e i o cards the details of joulesort already have undergone signifi cant changes since its inception since joulesort has not yet been tried in the wild we fully expect further revisions and fine tuning t o k eep i t f air a nd r elevant nevertheless we look forward to its use in guiding energy efficiency opti mizations in future systems we consider the setting of a multiprocessor where the speeds of the m proces sors can be individually scaled jobs arrive over time and have varying degrees of parallelizability a nonclairvoyant scheduler must assign the processes to processors and scale the speeds of the processors we consider the objective of energy plus flow time we assume that a processor running at speed uses power sα for some constant α for processes that may have side effects or that are not checkpointable we show an ω m α α bound on the competitive ratio of any randomized algorithm for checkpointable processes without side effects we give an o log m competitive al gorithm thus for processes that may have side effects or that are not checkpointable the achievable competitive ratio grows quickly with the number of processors but for checkpointable processes without side effects the achievable competitive ratio grows slowly with the number of processors we then show a lower bound of ω α m on the competitive ratio of any randomized algorithm for checkpointable processes without side effects introduction due to the power related issues of energy and temperature major chip manufacturers such as intel amd and ibm now produce chips with multiple cores processors and with dy namically scalable speeds and produce associated software such as intel speedstep and amd powernow that enables an operating system to manage power by scaling processor speed currently most multiprocessor chips have only a handful of processors but chip de signers are agreed upon the fact that chips with hundreds to thousands of processors will dominate the market in the next decade the founder of chip maker tilera asserted that a corollary to moore law will be that the number of cores processors will double every months according to the well known cube root rule a cmos based processor running at speed will have a dynamic power p of approximately in the algorithmic literature this is the university of hong kong hlchan cs hku hk york university jeff cs yorku ca supported in part by nserc canada university of pittsburgh kirk cs pitt edu supported in part by an ibm faculty award and by nsf grants cns ccf iis and ccf usually generalized to p sα thus in principle p processors running at speed p could do the work of one processor running at speed but at pα of the power but in spite of this chip makers waited until the power costs became prohibitive before switching to multiprocessor chips because of the technical difficulties in getting p speed p processors to come close to doing the work of one speed processor this is particularly true when one has many processors and few processes where these processes have widely varying degrees of parallelizability that is some processes may be considerably sped up when simultaneously run on multiple processors while some processes may not be sped up at all this could be because the underlying algorithm is inherently sequential in nature or because the process was not coded in a way to make it easily parallelizable to investigate this issue we adopt the following general model of parallelizability used in each process consists of a sequence of phases each phase consists of a positive real number that denotes the amount of work in that phase and a speedup function that specifies the rate at which work is processed in this phase as a function of the number of processors executing the process the speedup functions may be arbitrary other than we assume that they are nondecreasing a process doesn t run slower if it is given more processors and sublinear a process satisfies brent theorem that is increasing the number of processors doesn t increase the efficiency of computation the operating system needs a process assignment policy for determining at each time which processors if any a particular process is assigned to we assume that a process may be assigned to multiple processors in tandem with this the operating system will also need a speed scaling policy for setting the speed of each processor in order to be implementable in a real system the speed scaling and process assignment policies must be online since the system will not in general know about processes arriving in the future further to be implementable in a generic operating system these policies must be nonclairvoyant since in general the operating system does not know the size work of each process when the process is released to the operating system nor the degree to which that process is parallelizable so a nonclairvoyant algorithm only knows when processes have been released and finished in the past and which processes have been run on each processor at each time in the past the operating system has competing dual objectives as it both wants to optimize some schedule quality of service objective as well as some power related objective in this pa per we will consider the formal objective of minimizing a linear combination of total re sponse flow time the schedule objective and total energy used the power objective in the conclusion we will discuss the relationship between this energy objective and a temper ature objective this objective of flow plus energy has a natural interpretation suppose that the user specifies how much improvement in flow call this amount ρ is necessary to justify spending one unit of energy for example the user might specify that he is willing to spend erg of energy from the battery for a decrease of micro seconds in flow then the optimal schedule from this user perspective is the schedule that optimizes ρ times the energy used plus the total flow by changing the units of either energy or time one may assume without loss of generality that ρ so the problem we want to address here is how to design a nonclairvoyant process as signment policy and a speed scaling policy that will be competitive for the objective of flow plus energy the case of a single processor was considered in in the single processor case the parallelizability of the processes is not an issue if all the processes arrive at time then in the optimal schedule the power at time t is θ nt where nt is the number of active processes at time t the algorithm considered in runs at a speed of δ α for some constant δ the process assignment algorithm considered in is latest arrival processor sharing laps laps was proposed in in the context of running processes with arbitrary speedup functions on fixed speed processors and it was shown to be scalable i e ǫ speed o competitive for the objective of total flow time in this setting laps is parameterized by a constant β and shares the processing power evenly among the βnt most recently arriving processes note that the speed scaling policy and laps are both nonclairvoyant showed that by picking δ and β appropriately the resulting algorithm is α competitive for the objective of flow plus energy on a single speed scalable processor our results here we consider extending the results in to the setting of a multiprocessor with m processors it is straight forward to note that if all of the work is parallelizable then the multiprocessor setting is essentially equivalent to the uniprocessor setting to gain some intuition of the difficulty that varying speedup functions pose let us first consider an instance of one process that may either be sequential or parallelizable if an algorithm runs this process on few of the processors then the algorithm competitive ratio will be bad if the process is parallelizable and the optimal schedule runs the process on all of the processors note that if the algorithm wanted to be competitive on flow time it would have to run too fast to be competitive on energy if an algorithm runs this process on many of the processors then the algorithm competitive ratio will be bad if the process is sequential and the optimal schedule runs the process on few processors if the algorithm wanted to be competitive on energy it would have to run too slow to be competitive on flow time formalizing this argument we show in section a lower bound of ω m α α on the competitive ratio of any randomized nonclairvoyant algorithm against an oblivious adversary with an additional assumption that we will now discuss at first glance such a strong lower bound for such an easy instance might lead one to conclude that there is no way that the scheduler can be expected to guarantee reasonably competitive schedules but on further reflection one realizes that an underlying assumption in this lower bound is that only one copy of a process can be run if a process does not have side effects that is if the process doesn t change effect anything external to itself then this assumption is not generally valid one could run multiple copies of a process simultaneously with each copy being run on a different number of processors and halt computation when the first copy finishes for example in the instance in the previous paragraph one could be o competitive if the process didn t have side effects by running one copy on a single processor and running one copy on the rest of the processors generalizing this approach one can obtain a o log m competitive algorithm for instances consisting of processes that have no side effects and where the speed up function doesn t change unfortunately we show in section that such a result can not be obtained if processes can have multiple phases with different speed up functions we accomplish this by showing that the competitive ratio of any randomized nonclairvoyant algorithm that runs multiple independent copies of a process against an oblivious adversary is ω mω α contemplating this second lower bound it suggests that to be reasonably competitive the algorithm must be able to process work on all copies of a job at the maximum rate of work processing on any copy if a processes had small state so that the overhead of checkpointing isn t prohibitive one might reasonably approximate this by checkpointing saving the state of each copy periodically and then restarting each copy from the point of execution of the copy that made the most progress in section we formalize this intuition we give a process assignment algorithm multilaps which is a modification of laps we show that by combining multilaps with the natural speed scaling algorithm one obtains an o log m competitive algorithm if all copies process work at the rate of the fastest copy there are two steps in the analysis of multilaps the first step is to show that there is a worst case instance where every speedup function is parallel up to some number of processors and then is constant this shows that the worst case speedup functions for speed scalable processors are more varied than for fixed speed processors where it is sufficient to restrict attention to only parallelizable and sequential speedup functions the second step in the analysis of multilaps is a reduction to essentially the analysis of laps in a uniprocessor setting technically we need to analyze laps when some work is sequential that is it has the special property that it is processed at unit rate independent of the speed of the processor we then discuss how to generalize the analysis of laps in to allow sequential work using techniques from in section we then show a lower bound of ω α m on the competitive ratio of any nonclairvoyant randomized algorithm against an oblivious adversary for checkpointable processes without side effects in fact this lower holds even if the rate that a process is processed is the sum not the maximum of rate of the various copies thus in summary for processes that may have side effects or that are not checkpointable the achievable competitive ratio grows quickly with the number of processors but for check pointable processes without side effects the achievable competitive ratio grows slowly with the number of processors this shows the importance of being able to efficiently checkpoint multiple copies of a process in a setting of processes with varying degrees of parallelizability and individually speed scalable multiprocessors related results we start with some results in the literature about scheduling with the objective of total flow time on a single fixed speed processor it is well known that the online clairvoyant algorithm shortest remaining processing time srpt is optimal the competitive ratio of any deterministic nonclairvoyant algorithm is ω and the competitive ratio of every randomized algorithm against an oblivious adversary is ω log n a randomized version of the multi level feedback queue algorithm is o log n competitive the nonclair voyant algorithm shortest elapsed time first setf is scalable that is it is ǫ speed o competitive for any arbitrarily small but fixed ǫ setf shares the processor equally among all processes that have been run the least we now consider scheduling processes with arbitrary speedup functions on fixed speed processors for the objective of total flow time the algorithm round robin rr also called equipartition and processor sharing that shares the processors equally among all processes is ǫ speed o competitive as mentioned before laps is scalable we now consider speed scaling algorithms on a single processor for the objective of flow plus energy give efficient offline algorithms we now describe the results for online clairvoyant algorithms this setting was studied in a sequence of papers which culminated in the following result the scheduling algorithm that uses shortest remaining processing time srpt for process assignment and power equal to one more than the number of active processes for speed scaling is ǫ competitive for the objective of total flow plus energy on arbitrary work unit weight processes even if the power function is arbitrary so clairvoyant algorithms can be o competitive independent of the power function showed that nonclairvoyant algorithms can not be o competitive if the power function is growing too quickly the case of weighted flow time has also been studied the scheduling algorithm that uses highest density first hdf for process assignment and power equal to the fractional weight of the active processes for speed scaling is ǫ competitive for the objective of fractional weighted flow plus energy on arbitrary work arbitrary weight processes an o competitive algorithm for weighted flow plus energy can then be obtained using the known resource augmentation analysis of hdf extend some of the results for the unbounded speed model to a model where there is an upper bound on the speed of a processor there are many related scheduling problems with other objectives and or other assump tions about the processors and instance surveys can be found in formal problem definition and notations an instance consists of a collection j jn where job ji has a release arrival time ri and a sequence of phases jqi each phase is an ordered pair wq γq where i i i i i q is a positive real number that denotes the amount of work in the phase and γq is a function called the speedup function that maps a nonnegative real number to a nonnegative real number γq p represents the rate at which work is processed for phase q of job i when one copy of the job is run on p processors running at speed if these processors are running at speed then work is processed at a rate of sγq p a schedule specifies for each time and for each copy of a job a nonnegative real number specifying the number of processors assigned to the copy of the job and a nonnegative real speed we thus assume that if several processors are working on the same instance copy of a job then they must all run at the same speed but different copies can run at different speeds the number of processors assigned at any time can be at most m the number of processors note that formally a schedule does not specify an assignment of copies of jobs to processors a nonclairvoyant algorithm only knows when processes have been released and finished in the past and which processes have been run on each processor each time in the past in particular a nonclairvoyant algorithm does not know wq nor the current phase q nor the speedup function γq in this paper we consider several different models depending on how the processing on different copies interact assume multiple copies of job i are run with the speed and number of processors assigned to the k th copy being sk and pk in the independent processing model if copy k is running in a phase with speedup function γ then work is processed on this copy at rate skγ pk independent of the rate of processing on the other copies in the maximum processing model if each copy of job ji is in a phase with speedup function γ then each copy processes work at a rate of maxk skγ pk in the sum processing model if each copy of job ji is in a phase with speedup function γ then each copy processes work at a rate of k skγ pk note that as a consequence of these definitions in the maximum processing model and in the sum processing model it is the case that each copy of each job is always at the same point in its execution the completion time of a job ji denoted ci is the first point of time when all the work on some copy of the job has been processed note that in the language of scheduling we are assuming that preemption is allowed that is a job maybe be suspended and later restarted from the point of suspension a job is said to be active at time t if it has been released but has not completed i e ri t ci the response flow time of job ji is ci ri which is the length of the time interval during which the job is active let nt be the number of active jobs at time t another formulation of total flow time is ntdt when running at speed a processor consumes p sα units of energy per unit time where α is some fixed constant we call p the power function a phase of a job is parallelizable if its speedup function is γ p p increasing the number of processors allocated to a parallelizable phase by a factor of increases the rate of processing by a factor of a phase of a job is parallel up to q processors if γ p p for p q and γ p q for p q a speedup function γ is nondecreasing if and only if γ γ whenever a speedup function γ is sublinear if and only if γ γ whenever p2 we assume all speedup functions γ in the input instance are nondecreasing and sublinear we further assume that all speedup functions satisfy γ p p for p this natural assumption means that when a job ji is assigned to a single processor and shares this processor with other jobs the rate that ji is processed is the fraction of the processor that ji receives times the speed of the processor let a be an algorithm and j an instance we denote the schedule output by a on j as a j we let fa j and ea j denote the total flow time and energy incurred in a j let costa j fa j ea j denote the cost we will use m as a short hand for multilaps let opt be the optimal algorithm that always minimizes total flow time plus energy a randomized algorithm a is c competitive or has competitive ratio c if for all instances j e costa j c costopt j lower bounds for single copy and non checkpointing algorithms in this section we show that the competitive ratio must grow quickly with the number of processors if only one copy of each job can be running lemma or if multiple copies are allowed but no checkpointing is allowed lemma we first start with a couple basic lemmas about optimal schedules that will be useful throughout the paper lemma consider a job with work w and with a single phase with a speedup function that is parallelizable up to q processors assume that the job is run on p q processors then the optimal speed is α for a cost of θ w assume that the job is run on p q processors then the optimal speed is α for a cost of θ α proof first consider that case that p q let be the speed of the processors the flow plus energy is then w psα w w sα this is minimized by setting α ps ps ps for a cost of θ w α p energy is then w psα w this is minimized by setting α giving a cost of qs qs θ α q α p lemma consider a job with work w with a single phase with a speedup function that is parallelizable up to q processors the optimal schedule uses p q processors run at speed α q α for a cost of θ w proof from the proof of lemma we know that if the algorithm allocates p q speed processors to this job the cost is minimized by when for a cost of θ w α this is minimized by making p as big as possible namely p q from the proof of lemma we know that if the algorithm allocates p q speed processors to this job the cost is minimized when giving a cost of θ α q this is minimized by making p α as small as possible namely p q thus in either case the optimal scheduling policy uses p q processors run at speed for a cost of θ w α lemma any randomized nonclairvoyant algorithm that only runs one copy of each job must be ω m α competitive against an oblivious adversary that must specify the input a priori proof applying yao technique we give a probability distribution over the input and show that every deterministic algorithm a will have an expected competitive ratio of ω m α α the instance will be selected uniformly at random from two possibilities the first possible instance consists of one job with α units of parallelizable work the second possible instance will consist of job with one unit of work that is parallel up to one processor by plugging these parameters into lemma one can see that the optimal cost is θ for both instances let p denote the number of processors used by the algorithm a by lemma the cost for the algorithm a is either ω m α or ω α depending on the instance both the maximum and the average of these two costs is minimized by balancing these two costs which is accomplished by setting p α this shows that the competitive ratio is ω m α α lemma in the independent processing model any randomized nonclairvoyant algorithm must be ω m α α competitive against an oblivious adversary that must specify the input a priori proof applying yao technique we give a probability distribution over the inputs with the property that every deterministic algorithm a will have expected competitive ratio ω m α the random instance consists of a single job with an infinitely large num ber of phases each phase will be randomly chosen to be one of the two job instances given in lemma that is each phase will either be parallelizable or parallel up to one processor and the optimal cost for each phase will be θ consider a particular copy of the job run by a because each phase is so small we can assume that the algorithm a allocates a fixed number of processors p running at a fixed speed for the duration of the phase by the proof of lemma no matter what the algorithm does the probability is at least a half that it occurs a cost of ω m α during this phase one can think of the phases as bernoulli trials with outcomes being cost ω m α with probability at least a half and smaller cost with probability at most a half applying a chernoff bound with high probability the algorithm has cost ω m α on nearly half of the stages while the optimal cost on each stage is by a union bound the probability that any copy has average cost per phase much less than ω m α is small analysis of multilaps in this section we assume that multiple copies of a job may be run simultaneously each copy of a job may be assigned a different number of processors but each processor running this copy must be run at the same speed we assume that at each moment in time the rate that work is processed on each copy of a job is the maximum of the rates of the different copies so all copies of a job are always at the same point of execution we give a nonclairvoyant algorithm multilaps for this setting and show that it is o log m competitive for flow time plus energy we now describe the algorithm laps from and the algorithm multilaps that we introduce here we then give some underlying motivation for the design of multilaps algorithm laps let δ and β be real constants at any time t the processor speed is δ na α where na is the number of active jobs at time t the processor processes the βna active jobs with the latest release times by splitting the processing equally among these jobs for our purposes in this paper we will take δ algorithm multilaps let β be a real number that parametrizes multilaps let µ consider any time t let na be the number of active jobs at t each of the βna active jobs with the latest release times will be run at this point in time call these jobs the late jobs for each late job ji a primary copy of ji is run on a group of pa µ m processors where each processor in this group is run at speed sa na α note that the µ m primary copies of the late jobs are equally sharing a µ fraction of the processors furthermore for each late job ji there are log pa secondary copies of ji run the jth j log pa secondary copy of ji is run on a group of processors where each processor in this group is run at speed α intuition behind the design of multilaps let us give a bit of intuition behind the design of multilaps if µ was and no secondary copies were run then multilaps would essentially be adopting the strategy of laps of sharing the processing power evenly among the latest arriving β fraction of the jobs laps is o competitive when all work is parallelizable up to the number of available processors however if a primary copy of a job is run on many processors the online algorithm may be wasting a lot of energy if this work is not highly parallelizable to account for this possibility multilaps runs the primary copy a little faster freeing up some processors to run secondary copies of the job on fewer processors and at a faster speed the number of processors running the secondary copies are geometrically decreasing by a factor of while the speeds are increasing by a factor of α thus each copy of a job is using approximately the same power intuitively one of the copies is running the late job on the right number of processors thus multilaps uses a factor of o log m more energy than optimal because of the log m different equi power copies of the job setting µ guarantees that that multilaps doesn t use more than m processors the rest of this section is devoted to proving the following theorem theorem in the maximum processing model multilaps is o log m competitive for total flow time plus energy overview of the proof of theorem we now give an overview of the structure of our proof of theorem in lemma we show how to reduce the analysis of multilaps on arbitrary instances to the analysis of multilaps on canonical instances we define an instance to be canonical if the speedup function for each job phase is parallel up to the number po of processors that opt uses on that phase and is constant there after the value of po may be different for each phase more specifically we show how to construct a canonical instance j from an arbitrary instance k such that the cost of multilaps on k is identical to the cost of multilaps on j and the optimal cost for j is at most the optimal cost for k we then define a variation of the uniprocessor setting that we call the sequential set ting in the sequential setting a job can have sequential phases which are phases that are processed at a unit rate independent of the computational resources assigned to the job we then show how to reduce the analysis of multilaps on canonical instances to the analysis of laps in the sequential setting more precisely from an arbitrary canonical instance j we show how to create an instance j for the sequential setting we show in lemma that the flow time for multilaps on j is identical to the flow time of laps on j and the energy used by multilaps on j is at most o log m times the energy used by laps on j we then need to relate the optimal schedule for j to the optimal schedule for j to accomplish this we classify each phase of a job in j as either saturated or unsaturated depend ing on the relationship between the speedup function and how many processors multilaps uses for this phase we consider two instances derived from j an instance jsat consisting of only the saturated phases in j and an instance juns consisting of only the unsaturated phases in j we then consider two instances derived from the instance j an instance j consisting of parallel phases in j and an instance j consisting of sequential phases in j the transformation of j to j transforms phases in jsat to phases in j and transforms phases in juns to phases in j it will be clear that the optimal cost for j is at least the optimal cost for jsat plus the optimal cost for juns we then show in lemma that the optimal cost for jsat is at least the optimal cost for j and in lemma that the optimal cost for juns is at least the optimal cost for j we then discuss how to generalize the analysis of laps in using techniques from to show that the cost of laps is at most a constant factor larger than the optimal cost for j plus the optimal cost for j this line of reasoning allows us to prove our theorem as follows costm k costm j o log m costlaps j o log m costopt j costopt j o log m costopt jsat costopt juns o log m costopt j o log m costopt k the first and final equalities follow from lemma the second equality follows from lemma the third equality follows from the analysis of laps in the sequential setting the fourth equality follows from lemma and lemma the fifth equality will be an obvious consequence of the definitions of jsat and juns we now execute the proof strategy that we have just outlined we first show that there is a worst case instance for multilaps that is canonical lemma let k be any input instance there is a canonical instance j such that fm j fm k em j em k and costopt j costopt k proof we construct j by modifying each job in k as follows consider an infinitesimally small phase of a job in k with work w and speedup function γ let po be the number of processors that opt allocates to this phase when scheduling k we modify this phase so that the new speedup function is γ p p γ po for p po and γ p γ po for p po note that multilaps may process this phase in several copies of this job assume that the i th copy is processed by pi processors of speed si due to the modification of speedup function the rate of processing for the i th copy changes from γ pi si to γ pi si if pi po then the rate of processing on the i th copy does not increase since γ is nondecreasing now consider a copy where pi po by the definition of γ the rate of processing γ pi si piγ po si since pi po and since γ is sublinear γ po γ pi plugging this back in we get po po pi that the rate of processing for copy is at most siγ pi so multilaps doesn t finish this phase in the modified instance before it can finish the phase in k we then decrease the work of this phase so that the time when this phase is first completed among all of the copies is identical to when it completes in multilaps k note that by construction the schedule of multilaps on this modified instance is identical to multilaps k while opt may get better performance due to the reduction of work finally we create j by multiplying both the work of this phase and the speedup function by the same factor of po o to make the final speed up function for this phase parallel up to po processors this change does not effect the schedules of either multilaps and opt definition of the sequential setting everything is defined identically as in subsection with the following two exceptions firstly there is only a single processor secondly job phases can be sequential which in the context of this paper means that work in this phase is processed at a rate of independent of the fraction of the processor assigned to the job and the speed of the processor so sequential work is processed at rate even if it is run at a speed much greater than or is not even run at all sequential work doesn t correspond to any realistic situation but is merely mathematical construct required for the proof definition of the transformation of a canonical instance j into the instance j in the sequential setting we transform each job in j into a job in j by modifying each phase of the original job at each point in time consider a phase and the copy in multilaps with the highest processing rate on this phase let γpo be the speedup function of the phase which is parallel up to po processors we say the phase is currently saturated m βna po and unsaturated otherwise note that µ m is the number of processors assigned to the primary copy in multilaps thus a phase is saturated if all copies in multilaps are processing in the parallel range of γpo and unsaturated otherwise consider the case that the phase is saturated the copy with the highest processing rate in multilaps is the one with pa µ m processors of speed sa na α giving a rate βna µ m α of m na α α na we modify this phase to be fully parallelizable and scale βna m βna down the work by a factor of α note that the processing rate of laps is na α so it a will complete the phase using the same time consider the case that the phase is unsaturated let r be the fastest rate that any copy in multilaps is processing work in this phase we modify this phase to be sequential and scale down the work by a factor of r by the definition of sequential the processing rate of laps on this phase is so it will complete the phase using the same time as multilaps we now show that the cost of multilaps on j is at most a log factor more than the cost of laps on j in the sequential setting lemma costm j flaps j o log m elaps j from this we can conclude that costm j o log m costlaps j proof by construction the flow time for multilaps j is identical to the flow time for laps j we now show that the energy used by multilaps j is at most a log factor more than the energy used by laps j the power in multilaps j is the sum of the powers in the m processors note that for each of the βna late jobs multilaps allocates pa processors of speed sa to a primary copy of this job recall that pa µ m and sa na α multilaps also runs log pa βna µ m secondary copies where the i th copy is run on processors of speed α hence the total power for multilaps j is βna µ m βna na µ m α α log pa i α α µ α na βna log pa laps j runs at speed α and hence power n since p m we conclude that e j a a a m o log m elaps j we now want to show a lower bound for opt j to state this lower bound we need to introduce some notation define jsat to be the instance obtained from j by removing all unsaturated phases in each job and directly concatenating the saturated phases define juns to be the instance obtained from j by removing all saturated phases and directly concatenating the unsaturated phases define j to be the instance obtained from j by removing all sequential phases in each job and directly concatenating the parallel phases define j to be the instance obtained from j by removing all parallel phases in each job and directly concatenating the sequential phases note that the transformation from j to j transforms a phase in jsat to a phase in j and transforms a phase in juns to a phase seq obviously opt can only gain by scheduling jsat and juns separately that is costopt j costopt jsat costopt juns we now want to show that costopt j o costopt jsat and costopt j o costopt juns lemma costopt j o costopt jsat proof we construct a schedule opt j from the schedule opt jsat phase by phase each phase in opt j will end no later than the corresponding phase in opt jsat and the schedule for opt j will use less energy than the schedule opt jsat consider a infinitesimal saturated phase in jsat let po and so be the number of processors and speed allocated by opt jsat to this phase by the definition of canonical the phase is parallelizable up to po processors thus opt jsat is processing at a rate of poso define opt j so that it runs at speed αposo on this phase since the transformation scales down the work by a factor of α opt j will complete the phase at the same time as opt jsat now we need to argue that at any point of time the power for opt jsat will be at least the power for opt jpar the power at this time in opt jsat is p j po j so j where the sum is over all jobs j it is processing and po j and so j are the number and speed of the processors allocated to j keeping r j po j so j fixed p is minimized by having all the so j to be the same fixed value so this gives r j po jso som and α p p α αm r m rα by our definition of opt j the power p in opt j can be bounded as follows α p j α m po j so j α α α rα p lemma costopt j o costopt juns proof consider a unsaturated phase in juns that is parallel up to po processors we gra ciously allow opt juns to schedule each phase in juns in isolation of the other phases this only improves opt juns consider a particular phase in juns with a speedup function that is parallel up to po processors and that has work w by lemma the total flow time plus energy incurred for opt juns is θ w opt j will allocate zero processors to the corresponding phase and process the phase at rate since the work in j is sequential hence opt j incurs no energy cost for this phase so to finish the proof we will show that the flow time for this phase in opt j is at most the cost of this phase in opt juns namely θ w recall that in the transformation from j to j this work is scaled down by the fastest rate that this phase is processed by any copy in multilaps consider the copy in multilaps that is processing in the parallel range with the most number of processors i e the copy with processors such that is maximized and at most po since the phase is unsaturated po by the definition of multilaps the processing rate of this copy is at least α α po α α o thus the work in this phase in j and the flow time for this phase in opt j is at most α o one can extend the analysis for laps in the uniprocessor setting to the sequential setting using the techniques used in we refer the reader to for full details and just give the underlying intuition here the analysis uses amortized local competitiveness that is it is shown that at every time plaps nlaps dφ dt c popt nopt where p denotes power n denotes the number of active jobs φ is the potential function and c is the desired competitive ratio so when plaps nlaps is large the processing of laps lowers the potential function φ enough to make the equation true now consider the sequential setting the difficulty that arises is that the processing that laps does on sequential jobs may not lower the potential function however if the number of sequential phases that laps is processing is very small then raising the speed of laps by a small amount will be enough so that the potential function decrease sufficiently quickly due to the processing on the non sequential jobs if the number of sequential phases is large at a particular time then the increase in flow time that laps is experiencing on these jobs is also experienced by the adversary at some point in time this increase in flow time experienced by the adversary pays for the increase in flow time for laps at this point of time note that by definition of laps the power used by laps is comparable to the increase in flow time experience by laps we can thus derive the following theorem theorem costlaps j o costopt j costopt j lower bound for checkpointable multiple copies we show here that even if the rate that the work is processed is the sum of the rate of the copies every randomized algorithm is poly log competitive theorem in the sum processing model the competitive ratio for every randomized nonclairvoyant algorithm is ω α m against an oblivious adversary that must specify the input a priori proof applying yao technique we give a probability distribution over the input and show that every deterministic algorithm a will have an expected competitive ratio of ω α m there are θ log m possible instances each selected with equal probability for each j log m instance jj will consist of one job with work wj α j and speedup function p where γq p is the speed up function that is parallelizable up to q processors by applying lemma opt jj allocates pj processors each of speed sj α resulting in a cost of θ now consider any deterministic nonclairvoyant algorithm a rounding the number of processors a copy is run on to a factor of two doesn t change the objective by more than a constant factor and there is no significant benefit from running two copies on an equal number of processors since the algorithm is nonclairvoyant it will gain no information about the identity of jj until some copy finishes since the power function is convex it is best for the algorithm to run each copy at constant speed thus we can assume that the algorithm runs log m copies of the job with copy i run on processors at at some constant speed si note that the algorithm can set si if it doesn t want to run a copy on that many processors the power of copy i is p pisiα and the total power for a is i i let r i j si denote the rate that the copy i is processing work on job jj because we are assuming that the work completed on a job is the sum of that completed by the groups working on it we have that rj i r i j is the rate that a completes work on job jj and maximize this time we bound this maximum denoted by t as follows min t j tj o log m j tj o log m i r i j wj j i o log m si wj o i j si log m o i si j i wj j i log m wj log m i j i α j j i log m α j o si α j log m i j i j i log m α j o si i log m o log m i α i i α α i p subject to p i pi the sum i pi is maximized by setting each pi to log m giving o p α o p α the total cost for a is fa ea t p t p t log m α ω α m recalling that the optimal cost is o the result follows conclusion in summary we have shown that for jobs that may have side effects or that are not check pointable the achievable competitive ratio grows quickly with the number of processors and for checkpointable jobs without side effects the achievable competitive ratio grows slowly with the number of processors there seem to be several interesting lines of research spawned by these results most obviously the upper and lower bounds on the competitive ratio for checkpointable jobs without side effects are not quite tight it is plausible that one could obtain tight upper and lower bounds by being more careful in the analyses one might also consider the situation where the power objective is temperature it is not clear how to best formalize this problem the most obvious approach is to include a constraint on temperature that is you can not exceed the threshold of the processor if the processor cools according to newton law then the temperature is approximately the maximum energy used over any time interval of a particular length where the length of the interval is determined by the cooling parameter of the device if the intervals are long then the temperature constraint is essentially an energy constraint but optimizing any reasonable scheduling objective subject to an energy constraint is known to be difficult with power having become a critical issue in the oper ation of data centers today there has been an increased push towards the vision of energy proportional comput ing in which no power is used by idle systems very low power is used by lightly loaded systems and proportion ately higher power at higher loads unfortunately given the state of the art of today hardware designing individ ual servers that exhibit this property remains an open chal lenge however even in the absence of redesigned hard ware we demonstrate how optimization based techniques can be used to build systems with off the shelf hardware that when viewed at the aggregate level approximate the behavior of energy proportional systems this paper ex plores the viability and tradeoffs of optimization based ap proaches using two different case studies first we show how different power saving mechanisms can be combined to deliver an aggregate system that is proportional in its use of server power second we show early results on deliver ing a proportional cooling system for these servers when compared to the power consumed at utilization re sults from our testbed show that optimization based sys tems can reduce the power consumed at utilization to for server power and for cooling power introduction with power having become a critical issue in the opera tion of data centers the concept of energy proportional computing or energy scaledown is drawing in creasing interest a system built according to this prin ciple would in theory use no power when not being uti lized with power consumption growing in proportion to utilization over the last few years a number of tech niques have been developed to make server processors more efficient including better manufacturing techniques and the use of dynamic voltage and frequency scaling dvfs for runtime power optimization while the savings are significant this has lead to cpus no longer being responsible for the majority of power con sumed in servers today instead subsystems that have not been optimized for power efficiency such as network cards hard drives graphics processors fans and power supplies have started dominating the power consumed by systems especially during periods of low utilization instead of waiting for all these different technologies to deliver better energy efficiency this paper advocates that energy proportional computing can be approximated by using software to control power usage at the ensem ble level an ensemble is defined as a logical collection of servers and could range from an enclosure of blades a single rack groups of racks to even an entire data center it was previously difficult to dynamically balance work loads to conserve power at the ensemble level for a num ber of reasons unnecessarily shutting down applications to simply restart them elsewhere is looked upon as a high risk high cost change because of the performance impact the risk to system stability and the cost of designing cus tom control software however the re emergence of vir tualization and the ability to live migrate entire vir tual machines vms consisting of oss and applications in a transparent and low overhead manner will enable a new category of systems that can react better to changes in workloads at the aggregate level by moving work loads off under utilized machines and then turning idle ma chines off it should now be possible to approximate at an ensemble level the behavior found in theoretical energy proportional systems however virtualization is not a magic bullet and a na ıve approach to consolidation can hurt application per formance if server resources are overbooked or lead to re duced power savings when compared to the maximum pos sible this paper therefore advocates a more rigorous ap proach in the optimization of ensemble systems including the use of performance modeling optimization and con trol theory finally instead of only looking at more tradi tional server components such as storage cpus and the network optimization based systems should also consider control of other components such as server fans and com puter room air conditioners cracs as cooling costs are rapidly becoming a limiting factor in the design and operation of data centers today we use two case studies to demonstrate that it is possi ble by applying optimizations at the ensemble layer to de the power consumed by servers and exhibit power usage behavior close to that of an energy proportional system second we demonstrate how a power and workload aware cooling controller can exhibit the same behavior for cool ing equipment such as server fans case studies 2000 1000 no dvfs dvfs dvfs off average enclosure utilization it has been advocated that optimization based algorithms should be preferred over ad hoc heuristics in making sys tem runtime and management decisions we will therefore not stress this point further but through the use of two case studies show how optimization can be used to deliver energy proportionality at the ensemble layer it should be stressed that the focus of this section is not on the use of any particular algorithm but instead on how non energy proportional systems can be combined to approxi mate the behavior of an energy proportional system experimental setup to evaluate energy proportionality in the case studies we used an hp bladesystem enclosure with pro liant server blades and fans each blade was equipped with gb of ram and two amd he dual core processors each processor has p states a volt age and frequency setting corresponding to frequencies of and ghz the blades and the fans are equally divided among two rows on the front and back ends of the enclosure respectively each blade is cooled by multiple fans and each fan draws cool air in through the front of the blades the enclosure allows us to measure blade temperatures and the power consumed by different components we used xen with both the administrative domain and the vms using the para virtualized linux kernel the xen administrative domain is stored on local disks while the vms use a storage area network san we used vms configured with mb of ram a gb virtual hard drive and one virtual cpu we set each vm memory to a low value to allow us to evaluate a large configuration space for workload place ment in production environments the same effect could be achieved at runtime through the use of page sharing or ballooning we used gamut to experiment with different vm and physical server utilizations server energy proportionality we examined the energy proportionality in the enclosure layer using three different policies the first policy no each result presented above is an average of approximately readings over a minute interval figure enclosure power usage blades network dvfs uses no power saving features the second policy dvfs uses hardware based voltage and frequency scaling and is very similar to linux ondemand governor the third policy dvfs off uses dvfs plus a vm migra tion controller that consolidates virtual machines and turns idle machines off its algorithm uses the blade power models for the different p states and sensor readings from resource monitoring agents the optimization problem is similar to that in with constraints on the blade cpu and memory utilization to prevent overbooking examples of other constraints that could be modeled include network and storage utilization our experience has shown that al gorithms such as bin packing or simulated annealing work well in this scenario for this case study we varied each vm utilization in the range of of a single core and mea sured the power consumed by the enclosure the results are presented in figure where the x axis shows the en closure utilization by all vms as a percentage of total capacity of all the blades as each server has cores the utilization also corresponds to each vm utilization as a percentage of a single core the measured power shown on the y axis includes the power consumed by the blades as well as the networking san and management modules with no dvfs we notice a very small power range w between and utilization and the minimum power used is 406 w or of the power consumed at utilization significantly away from the theoretical minimum of w once dvfs is enabled the power range increases but as seen in the figure the system is still not energy proportional and consumes w at utilization it is only when we look at the dvfs off policy that the system starts approximating energy propor tionality at the ensemble level the range of power used between and utilization is 164 w and at utilization there is only a w difference or of the power consumed at utilization from the theoretical minimum of w 500 6000 speed rpm static fan speed reactive fan controller predictive fan controller average enclosure utilization figure fan power consumption and model note from the figure that at high utilizations above all three policies have the same power usage as they are all running near peak performance while the dvfs policy only shows a noticeable difference when it can switch to a lower p state the dvfs off policy starts showing benefit around utilization as it can start con solidating vms and turning machines off even at utilization zero power usage was not achieved with dvfs off for a number of reasons first at least one active server is needed to host all the vms second our enclosure contained two network switches two san switches two management modules and six power supplies most of these components are not energy proportional finally like all industry standard servers even off servers have an active management processor that is used for network management tasks such as remote kvm and power cycling cooling energy proportionality while we showed how energy proportionality could be achieved for server power in section cooling equip ment also consumes a significant part of the total power used by a data center in particular server fans can con sume between of total server power and at the data center level cooling can account for as much as 50 of the total power consumed in this case study we therefore examine how intelligent fan control can be used to achieve better energy proportionality for server cooling resources we continue to use the experimental setup described in section and use the dvfs off pol icy presented in section for managing server power the objective of fan control is to provide enough cool air for the blades so that the server temperatures can be main tained below thresholds for thermal safety reasons in this paper we specifically evaluate two different fan controllers anomalous reading noticeable in figure is for the no dvfs and dvfs settings at and utilization levels while the processor reports being at the highest p state we recorded a sharp drop in power usage at these points we believe that the processor is using an internal power saving scheme that it disables at high utilizations each result presented above is an average of approximately readings over a minute interval figure fan power a reactive fan controller rfc and a predictive fan controller pfc the reactive fan controller rfc is a simple feedback controller that changes fan speeds based on the data gath ered from the hardware temperature sensors present on all the blades because of the complexity of sharing fans between blades the rfc synchronously changes the speed of all fans in each row there are two rows of blades and fans in the enclosure in response to the maximum observed temperature for that row when the maximum temperature is above the threshold the fan speeds are in creased to provide a larger volume of air flow to cool the servers and vice versa the rfc is similar to commercial fan controllers used in industry today the predictive fan controller pfc aims to minimize the total fan power consumption without violating temper ature constraints it uses temperature sensors in the blades as well as software sensors to monitor server utilization figure shows the power model for a fan in our enclosure where the fan power is a cubic function of the fan speed which closely matches the measured fan power note that the volume air flow rate is approximately proportional to the fan speed also note that each fan provides different levels of cooling resource i e cool air to each individual blade according to the location of the fan with respect to the blade in this regard each fan has a unique cooling effi ciency with respect to each blade this provides an oppor tunity to minimize the fan power consumption by explor ing the variation in cooling efficiency of different fans for different blades along with the time varying demands of the workloads we built a thermal model empirically that explicitly captures the cooling efficiency between each pair of blade and fan this thermal model together with the blade and the fan power models is used by the pfc to predict future server temperatures for any given fan speed and measured server utilization by representing this as a convex constrained optimization problem the pfc is able to use an off the shelf convex optimization solver to set the fan speeds to values that potentially minimize the aggregate power consumption by the fans while keeping the blade temperatures below their thresholds the results for the two controllers are presented in fig ure the figure also includes the power consumed by setting the fans to a static speed that independent of the workloads is guaranteed to keep temperatures below the threshold under normal ambient operating conditions when examining the rfc performance it is helpful to note the relationship between vm utilization and fan speed for a given row of fans the fan speed is directly controlled by the maximum cpu temperature in the row furthermore cpu temperature is a function of blade uti lization and blade ambient or inlet temperature blade uti lization however does not always directly correlate to vm utilization for example with each vm utilization set to one vcpu the system cannot fit more than vms per machine with an overall blade utilization of four cpus however with a reduced vm utilization of each blade can accommodate vms with an overall blade utilization of as a blade cpu temperature will be much higher with a utilization of vs it will require a greater amount of cooling and therefore use more power due to increased fan speeds similarly if a blade is located in an area of increased ambient temperature that blade could also drive fan speed higher if and when it be comes utilized even if the utilization levels are relatively low these factors are responsible for the rfc operating in two power bands approximately between w when the utilization ranges between and between w when the utilization ranges between even at utilization the rfc still uses of the peak power used at utilization in contrast due to its knowledge of the cooling efficien cies of different fans the demand levels of the individual blades and their ambient temperatures the pfc is able to set fan speeds individually and avoid the correlated behav ior exhibited by controllers like the rfc overall the pfc induces an approximately linear relationship between the fan power and the aggregate enclosure utilization and at utilization the pfc only consumes of the power it uses at utilization the use of model based opti mization also allows the pfc to perform significantly bet ter than the rfc when we compare the two controllers the pfc can reduce fan power usage by at both and utilization at utilization the pfc only consumes of the power used by the rfc at however the pfc and the rfc with zero load is un able to reduce its power usage to w this was not a de ficiency of our optimization based approach but was due to the fact that the fans were also responsible for cooling the blade enclosure networking and management mod ules we therefore had to lower bound the fan speeds to ensure that these non energy proportional components did not accidentally overheat assumptions even though we used a homogeneous set of machines in our cases studies our experience has shown that these al gorithms can be extended with different power and thermal models to control an ensemble composed of heterogeneous hardware further current generation of processors from both intel and amd support cpuid masking that al lows vms to migrate between processors from different families this work also assumes that it is possible to mi grate vm identities such as ip and network mac addresses with the vms while this is generally not a problem in a single data center migrating storage area network san identities can sometimes be problematic however vendor products such as hp virtual connect and emulex vir tual hba have introduced a layer of virtualization in the storage stack to solve this problem note that it might be difficult to adopt the optimization based solutions similar to those proposed in this paper to applications that depend on locally attached storage for their input data however the fact that such locally attached storage systems usually replicate data for relia bility and availability reasons might provide a possi ble solution in such a scheme the optimization algo rithms could be made aware of the dependencies between the vms and the locations of their datasets given this information the algorithms could find a suitable consoli dated mapping of vms to physical machines in order to make this scheme effective a higher degree of replication might be needed to give the algorithms more flexibility in making placement decisions this change essentially boils down to a tradeoff between the cost of increased storage capacity versus energy savings finally our approach approximates energy proportion ality by turning machines off concerns have been previ ously raised about reliability of both servers and disk drives due to an increased number of on off cycles however our conversations with blade system designers have shown that it should not affect server class machines or given the large number of on off cycles supported by disk drives their internal storage systems during their normal lifetime aggressive consolidation might also hurt application avail ability if the underlying hardware is unreliable as most applications tend to be distributed for fault tolerance in troducing application awareness into the consolidation al gorithm to prevent it from consolidating separate instances of the same application on the same physical machine will address this issue concluding remarks this paper has shown that it is possible to use optimization based techniques to approximate energy proportional be havior at the ensemble level even though our techniques result in some added complexity in the system in the form of models optimization routines and controllers we be lieve the power savings are significant e nough t o justify it however better instrumentation can help us get even closer to theoretical energy proportionality for example if temperature sensors which are relatively cheap to de ploy were installed in the networking and san switches it would have allowed our fan controller to have more com plete knowledge of the thermal condition inside the enclo sure so that more efficient fan speed optimization could be achieved in addition we have used cpu utilization in this case study as a proxy for application level performance to directly evaluate and manage application performance we will need sensors that measure application level metrics such as throughput and response time these sensors can be provided by parsing application logs or by monitoring user requests and responses while the controllers shown in this paper assumed a sin gle management domain further study needs to be done to show how they would work in a federated environment where information would need to be shared between con trollers at different management layers and possibly from different vendors we believe that some of the distributed management task force dmtf standards would help address at least some of these issues prior work exists that looked at data center level cool ing efficiency b y m anipulation o f c rac u nit ettings or by temperature aware workload placement how ever given that these studies only looked at total power consumption a more careful investigation is needed in the context of this paper for example the model based opti mization approach we used in the predictive fan controller may be applied to the crac unit control problem stud ied in to achieve energy proportionality for the cooling equipment at the data center level finally even though this paper demonstrated energy proportionality at the ensemble layer this does not pre clude the need for better energy efficiency for individual components such as disks memory and power supplies we believe that new hardware designs with finer levels of power control will help in designing energy efficient sys tems at both the single server and ensemble layer power management is increasingly important in computer communications systems not only is the energy consumption of the internet becoming a significant fraction of the energy consumption of developed countries but cooling is also becoming a major concern consequently there is an important tradeoff in modern system design between reducing energy use and maintaining good performance there is an extensive literature on power management reviewed in a common technique which is the focus of the current paper is dynamic speed scaling this dynamically reduces the processing speed at times of low workload since processing more slowly uses less energy per operation this is now common in many chip designs in particular speed scaling has been proposed for many network devices such as switch fabrics tcp offload engines and ofdm modulation clocks this paper studies the efficacy of dynamic speed scaling analytically the goal is twofold i to elucidate the structure of the optimal speed scaling scheme e g how should the speed depend on the current workload ii to compare the performance of dynamic speed scaling designs with that of designs that use static processing speeds e g how much improvement does dynamic speed scaling provide there are many analytic studies of speed scaling designs beginning with yao et al the focus has been on either i the goal of minimizing the total energy used in order to complete arriving jobs by their deadlines e g or ii the goal of minimizing the average response time of jobs i e the time between their arrival and their completion of service given a set energy heat budget e g 19 web settings typically have neither job completion deadlines nor fixed energy budgets instead the goal is to optimize a tradeoff between energy consumption and mean response time this model is the focus of the current paper in particular the performance metric considered is e t e e βt where t is the response time of a job e is the expected energy expended on that job and βt controls the relative cost of delay this performance metric has attracted attention recently the related analytic work falls into two categories worst case analyses and stochastic analyses the former provide specific simple speed scalings guaranteed to be within a constant factor of the optimal performance regardless of the workload e g in contrast stochastic re sults have focused on service rate control in the m m model under first come first served fcfs scheduling which can be solved numerically using dynamic programming one such approach is reviewed in section iii c unfortunately the structural insight obtained from stochastic models has been limited our work extends the stochastic analysis of dynamic speed scaling we focus on the m gi queue under processor sharing ps scheduling which serves all jobs currently in the system at equal rates we focus on ps because it is a tractable model of current scheduling policies in cpus web servers routers etc based on the model section ii and the speed scaling we consider section iii our analysis makes three main contributions we provide bounds on the performance of dynamic speed scaling section iv a surprisingly these bounds show that even an idealized version of dynamic speed scaling im proves performance only marginally compared to a simple scheme where the server uses a static speed when busy and speed when idle at most a factor of for typical pa rameters and often less see section v counterintuitively these bounds also show that the power optimized response time remains bounded as the load grows we provide bounds and asymptotics for the speeds used by the optimal dynamic speed scaling scheme sections iv b and iv c these results provide insight into how the speeds scale with the arriving load the queue length and the relative cost of energy further they uncover a connection between the optimal stochastic policy and results from the worst case community section iv we illustrate through analytic results and numerical experi ments that though dynamic speed scaling provides limited performance gains it dramatically improves robustness to mis estimation of workload parameters and bursty traffic section vi ii model and notation in order to study the performance of dynamic speed scaling we focus on a simple model an m gi ps queue with controllable service rates dependent on the queue length in this model jobs arrive to the server as a poisson process with rate λ have intrinsic sizes with mean µ and depart at rate snµ when there are n jobs in the system under static schemes the constant service rate is denoted by define the load as ρ λ µ and note that this ρ is not the fraction of time the server is busy the performance metric we consider is e t e e βt where t is the response time of a job and e is the energy expended on a job it is often convenient to work with the expected cost per unit time instead of per job by little law this can be written as z e n λe f βt where n is the number of jobs in the system and f determines the power used when running at speed the remaining piece of the model is to define the form of f prior literature with the notable exception of has typically assumed that f is convex and often that f is a polynomial specifically a cubic that is because the dynamic power of cmos is proportional to v where v is the supply voltage and f is the clock frequency operating at a higher frequency requires dynamic voltage scaling dvs to a higher voltage nominally with v f yielding a cubic relationship to validate the polynomial form of f we consider data from real nm chips in fig the voltage versus speed data comes from the intel pxa pentium m pro cessor and the tcp offload engine studied in specifically the nbb trace at c in fig interestingly the dynamic power use of real chips is well modeled by a polynomial scaling of speed to power but this polynomial is far from cubic in fact it is closer to quadratic indicating that the voltage is scaled down less aggressively than linearly with speed as a result we will model the power used by running log freq fig dynamic power for an intel pxa a tcp offload engine and a pentium m the slopes of the fitted lines are 66 and respectively the scope of this paper and we leave the question of including both leakage and dynamic power for future work iii power aware speed selection when provisioning processing speed in a power aware manner there are three natural thresholds in the capability of the server i static provisioning the server uses a constant static speed which is determined based on workload charac teristics so as to balance energy use and response time ii gated static provisioning the server gates its clock setting if no jobs are present and if jobs are present it works at a constant rate chosen to balance energy use and response time iii dynamic speed scaling the server adapts its speed to the current number of requests present in the system the goal of this paper is to understand how to choose optimal speeds in each of these scenarios and to contrast the relative merits of each scheme clearly the expected cost is reduced each time the server is allowed to adjust its speed more dynamically this must be traded against the costs of switching such as a delay of up to tens of microseconds to change speeds the important question is what is the magnitude of improvement at each level for our comparison at speed by λ f βt sα β we will use idealized versions of each scheme in particular in each case we will assume that the server can be run at any desired speed in and ignore switching costs where α and β takes the role of βt but has dimension time α the cost per unit time then becomes sα thus in particular the dynamic speed scaling is a significant idealization of what is possible in practice however our results will suggest that it provides very little performance z e n β improvement over an ideally tuned gated static scheme in this section we will derive expressions for the optimal we will often focus on the case of α to provide intuition clearly this is an idealized model since in reality only a few discrete speeds can be used the impact of the workload parameters ρ β and α can often be captured using one simple parameter γ ρ α which is a dimensionless measure thus we will state our results in terms of γ to simplify their form also it will often be convenient to use the the dimensionless unit of speed α though we focus on dynamic power in this paper it should be noted that leakage power is increasingly important it represents of the power use of current and near future chips however analytic models for leakage are much less understood and so including leakage in our analysis is beyond speeds in cases i and ii for case iii we will describe a numerical approach for calculating the optimal speeds which is due to george and harrison though this numerical approach is efficient it provides little structural insight into the structure of the dynamic speeds or the overall performance providing such results will be the focus of section iv a the optimal static speed the simplest system to manage power is one which selects an optimal speed and then always runs the processor at that speed this case which we call pure static is the least power aware scenario we consider and will be used simply as a benchmark for comparison even when the speed is static the optimal design can be power aware since the optimal speed can be chosen so that it trades off the cost of response time and energy appropriately in particular we can write the cost per unit time as z ρ β then differentiating and solving for the minimizer gives that the optimum occurs when ρ and sα ρ βρ α b the optimal static speed for a gated system the next simplest system is when the processor is allowed two states halted or processing we model this situation with a server that runs at a constant rate except when there are no jobs in the system at which point it sets using zero dynamic power to determine the optimal static speed we proceed as we did in the previous section if the server can gate its clock the energy cost is only incurred during the fraction of time the server is busy ρ the cost per unit time then becomes c optimal dynamic speed scaling a popular alternative to static power management is to allow the speed to adjust dynamically to the number of requests in the system the task of designing an optimal dynamic speed scaling scheme in our model can be viewed as a stochastic control problem we start with the following observation which simplifies the problem dramatically an m gi ps system is well known to be insensitive to the job size distribution this still holds when the service rate is queue length dependent since the policy still falls into the class of symmetric policies introduced by kelly as a result the mean response time and entire queue length distribution are affected by the service distribution through only its mean thus we can consider an m m ps system further the mean response time and entire queue length distribution are equivalent under all non size based service distributions in the m m queue thus to determine the optimal dynamic speed scaling scheme for an m gi ps queue we need only consider an m m fcfs queue the service rate control problem in the m m fcfs z ρ ρ sα β queue has been studied extensively in partic ular george and harrison provide an elegant solution to the problem of selecting the state dependent processing speeds the optimum occurs when ρ and to minimize a weighted sum of an arbitrary holding cost with a processing speed cost specifically the optimal state dz ds ρ ρ α sα ρ β dependent processing speeds can be framed as the solution to a stochastic dynamic program to which provides an efficient numerical solution in the remainder of this section which is solved when α sα ρ β the optimal speed can be solved for explicitly for some α for example when α sgs ρ β in general define g γ α σ t σ γ α σα γ σ with this notation the optimal static speed for a server which gates its clock is sgs αg γ α we call this policy the we will provide an overview of this numerical approach the core of this approach will form the basis of our derivation of bounds on the optimal speeds in section iv we will describe the algorithm of specialized to the case considered in this paper where the holding cost in state n is simply n further we will generalize the description to allow arbitrary arrival rates λ the solution starts with an estimate z of the minimal cost per unit time including both the occupancy cost and the energy cost as in the minimum cost of returning from state n to the empty system is given by the dynamic program gated static policy and denote the corresponding cost zgs v inf λ f n zl the following lemma bounds g the proof is deferred to appendix a lemma for α n a λ µs βt µs λ µs vn λ λ µs vn i α α where a is the set of available speeds we will usually assume γ α g γ α α γ α a with the substitution un λ vn vn this can be written as and the inequalities are reversed for α note that the first inequality becomes tight for γα and u sup z n λ f sun the second becomes tight for γα further when α n a βt ρ both become equalities giving g γ γ and and the conditions on α here have been corrected since the version presented at infocom two additional functions are defined first φ u sup ux ρ λf x βt x a second the minimum value of x which achieves this supre mum normalized to be dimensionless is ψ u β α min x ux ρ λf x βt φ u note that under is constant competitive i e in the worst case the total cost is within a constant of optimal this matches the asymptotic behavior of the bounds for α for large n this behavior can also be observed for general α lemma and theorem a bounds on cost φ u α u α α αγ ψ u u α αγ we start the analysis by providing bounds on z in this subsection and then using the bounds on z to bound n above given the estimate of z un satisfy and below sections iv b and iv c recall that zgs is the total cost under gated static z un φ un n z theorem max γα γα α α the optimal value of z can be found as the minimum value such that un n is an increasing sequence this allows z to z zgs γ g γ α γ γg γ α α be found by an efficient binary search after which un can in principle be found recursively the optimal speed in state n is then given by proof the optimal cost z is bounded above by the cost of the gated static policy which is simply n ψ u α n zgs γ g γ α γ γg γ α α this highlights the fact that γ ρ α provides the appro priate scaling of the workload information because the cost z normalized speed sβ α and variables un depend on λ µ two lower bounds can be obtained as follows in order to maintain stability the time average speed must satisfy e ρ but z e sα β e α β by jensen inequality and the convexity of α thus and β only through γ note that this forward approach advocated in is numerically unstable appendix b we suggest that a more e sα ρα z β β γα stable way to calculate un is to start with a guess for large n and work backwards errors in the initial guess decay exponentially as n decreases and are much smaller than the accumulated roundoff errors of the forward approach this for small loads this bound is quite loose another bound comes from considering the minimum cost of processing a single job of size x with no waiting time or processor sharing it is optimal to serve the job at a constant rate thus backward approach is made possible by the bounds we derive in section iv z ex min x sα x l iv bounds on optimal dynamic speed scaling in the prior section we presented the optimal designs for the cases of static gated static and dynamic speed scaling in the first two cases the optimal speeds were presented more or less explicitly however in the third case we presented only a recursive numerical algorithm for determining the optimal dynamic speed scaling even though this approach provides an efficient means to calculate n it is difficult to gain insight into system design in this section we provide results exhibiting the structure of the optimal dynamic speeds and the performance they achieve the main results of this section are summarized in table i the bounds on z for arbitrary α are essentially tight i e agree to leading order in the limits of small or large γ due the right hand side is minimized for β α α independent of x giving z ρβ αα α α thus z max γα γα α α the form of the bounds on z are complicated so it is useful to look at the particular case of α corollary for α gated static has cost within a factor of of optimal specifically max z zgs proof for α g γ γ hence gives γ to the complicated form of the general results we illustrate the bounds for the specific case of α to provide insight in zgs γ γ γ γ 19 particular it is easy to see the behavior of sn and z as a func tion of γ and n in the case of α this leads to interesting observations for example it illustrates a connection between the optimal stochastic policy and policies analyzed in the worst case model in particular bansal pruhs and stein showed that when nothing is known about future arrivals a policy that gives speeds of the form sn n α α which establishes the upper bound the lower bound follows from substituting α into 17 z max the ratio of zgs to the lower bound on z has a maximum value of at γ and hence gated static is within a factor of of the true optimal scheme table i bounds on total costs and speed as a function of the number n of jobs in the system for any α max γα γα α α z γ γg γ α α theorem g γ α γ n n σα γα γ α where σn satisfies σα α σn αγ n γ g γ α γ γg γ α α for α max z corollary 13 p n γ γ for α and n a lower bound on sn results from linear interpolation between max γ at n and γ at n it is perhaps surprising that such an idealized version of dynamic speed scaling provides such a small magnitude of improvement over a simplistic policy such as gated static in fact the bound of is very loose when γ is large or small further empirically the maximum ratios for typical α are below see fig thus there is little to be gained by unrolling the dynamic program gives a joint minimization over all sn u ρ min sα β n z sn sn ρ min sα β n z u l dynamic scaling in terms of mean cost however section vi shows that dynamic scaling dramatically improves robustness sn sn tti n ρ n remains bounded as the arrival rate λ grows specifically by an upper bound can be found by taking any possibly suboptimal choice of sn i for i and bounding the z e β optimal z taking si α for all i n gives e t λ λ µ β min n σα z γ l b upper bounds on the optimal dynamic speeds γ σ o γ σ γ we now move to providing upper bounds on the optimal dynamic speed scaling scheme theorem for all n and α since z γα from 17 equation follows with this establishes for n holds since otherwise it follows from the inequality σα n γn α α n γ α and the fact that n α n σα γα by specializing to the case when α we can provide un γ for all σ whence o γ σ γ some intuition for the upper bound on the speeds corollary for α n γ γ α α σ γ o γ σ γ proof factoring the difference of squares in the first term in particular for σ γ α of 21 and canceling with the denominator yields un n α α γ γ α γn un σ γ γ σ γ σ γ 27 which is concave in n proof as explained in can be rewritten as one term of 27 is increasing in σ and two are decreasing minimizing pairs of these terms gives upper bounds on un a first bound can be obtained by setting σ γ n which minimizes the sum of the first two terms and gives un min sα β n un z l u n by this gives a bound on the optimal speeds of theorem the scaled speed σn n α satisfies n γ σα α σ αγ n γ γg γ α α a second bound comes by minimizing the sum of the second and third terms when σ γ this gives proof note that un un thus by α un u n z α α γn un γ γ αγ α α n by this can be expressed in terms of n as which upon division by gives αγ n α α n α n z n γ α β the minimum of the right hand sides of and is a bound on sn n α α α n α αγ n z the result then follows from the fact that and the result follows from since z z γ γ n for α gs the above theorem can be expressed more n which follows from taking the square root of the first inequal explicitly as follows corollary for α and any n ity and rearranging factors c lower bounds on the optimal dynamic speeds n α γ n finally we prove lower bounds on the dynamic speed scaling scheme we begin by bounding the speed used when there is one job in the system the following result is an immediate consequence of corollary and proof for α 32 can be solved explicitly giving un n z since un by s corollary for α n γ n z 34 γ s γ α max β and substituting z from 18 gives the result observe that the bounds in like those in corollary are essentially tight for both large and small γ but loose for γ near especially the lower bound next we will prove a bound on s n for large n lemma for sufficiently large n there are two important observations about the above corollary first the corollary only applies when s ρ and hence after the mode of the distribution however it also proves that the mode occurs at n second the corollary only applies when n in this case we can simplify the upper bound on sn in and combine it with 33 to obtain s n α s proof rearrange as un n z un α α n α α when this form holds it is tight for large n and or large γ finally note that in the case when n the only bounds we have on the optimal speeds are where the inequality uses the fact that the un is non decreasing hence unbounded as n is unbounded whence un z for large n applying s n α un αγ α gives this result highlights the connection between the optimal stochastic policy and prior policies analyzed in the worst case model that we mentioned at the beginning of this section specifically combining with and shows that speeds chosen to perform well in the worst case are asymptot ically optimal for large n in the stochastic model however note that the probability of n being large is small next we can derive a tighter albeit implicit bound on the optimal speeds that s n is increasing in n the following lemma proves that an improved lower bound can be attained by interpolating linearly between max γ and γ lemma the sequence un is strictly concave increasing proof let p n be the proposition un un un un strict concavity of un is equivalent to there being no n for which p n holds since un is non decreasing and there exists an upper bound on un with gradient tending to it is sufficient to show that p n implies p n if so then any local non concavity would imply convexity from that point onwards in which case its long term gradient would be positive and bounded away from zero and hence un would eventually violate the upper bound 23 by u identity n un φ un φ u n with this p n is equivalent to φ un φ un un un 100 102 this implies un un and φ un φ un a absolute costs α b ratio of cost for gated static to optimal zgs z note that the first factor is positive since the second factor is positive since φ is convex there is a subgradient g defined at each point this gives shows that the gated static i e the upper bound has very close to the optimal cost in addition to comparing the total cost of the schemes it is important to contrast the mean response time and mean φ un φ un 1 g u un un 1 φ un 1 φ un un 1 un energy use figure shows the breakdown a reference load of ρ with delay aversion β 1 and power scaling α this and 36 imply that both of the factors of in crease when going from p n to p n 1 establishing p n 1 and the strict concavity of un since it is also non decreasing the result follows v comparing static and dynamic schemes to this point we have only provided analytic results we now use numerical experiments to contrast static and dynamic schemes in addition these experiments will illustrate the tightness of the bounds proven in section iv on the optimal dynamic speed scaling scheme we will start by contrasting the optimal speeds under each of the schemes figure compares the optimal dynamic speeds with the optimal static speeds note that the bounds on the dynamic speeds are quite tight especially when the number of jobs in the system n is large for reference the modes of the occupancy distributions are about 1 and close to the points at which the optimal speed matches the static speeds note also that the optimal rate grows only slowly for n much larger than the typical occupancy this is important since the range over which dvs is possible is limited although the speed of the optimal scheme differs signif icantly from that of gated static the actual costs are very similar as predicted by the remark after corollary this is shown in fig 3 the bounds on the optimal speed are also very tight both for large and small γ part a shows that the lower bound is loosest for intermediate γ where the weights given to power and response time are comparable part b was compared against changing ρ for fixed γ changing β for fixed ρ and changing α note γ 3 was chosen to maximize the ratio of zgs z the second scenario shows that when γ is held fixed but the load ρ is reduced and delay aversion is reduced commensurately the energy consumption becomes negligible vi robust power aware design we have seen both analytically and numerically that ide alized dynamic speed scaling only marginally reduces the cost compared to the simple gated static this raises the question of whether dynamic scaling is worth the complexity this section illustrates one reason robustness specifically dynamic schemes provide significantly better performance in the face of bursty traffic and mis estimation of workload we focus on robustness with respect to the load ρ the optimal speeds are sensitive to ρ but in reality this parameter must be estimated and will be time varying it is easy to see the problems mis estimation of ρ causes for static speed designs if the load is not known then the selected speed must be satisfactory for all possible anticipated loads consider the case that it is only known that ρ ρ ρ let z denote the expected cost per unit time if the arrival rate is but the speed was optimized for then the robust design problem is to select the speed ρt such that min max z ρ ρt ρl ρ ρ ρ the optimal design is to provision for the highest foreseen load i e maxρ ρ ρ z ρ ρt z ρ ρt however this is wasteful in the typical case that the load is less than ρ 3 1 occupancy n a γ 1 15 40 60 100 occupancy n b γ the fragility of static speed designs is illustrated in fig which shows that when speed is underprovisioned the server is unstable and when it is overprovisioned the design is wasteful optimal dynamic scaling is not immune to mis estimation of ρ since s n is highly dependent on ρ however because the speed adapts to the queue length dynamic scaling is more robust figure shows this improvement this robustness is improved further by the speed scaling fig rate vs n for α and different energy aware load γ scheme which we term linear that scales the server speed 80 25 60 40 15 20 10 15 20 25 35 40 design fig breakdown of e t and e sα for several scenarios in proportion to the queue length i e sn α n note fig cost at load β 1 α ρ 10 when speeds are designed for design ρ using that under this scaling the queue is equivalent to an m gi queue with homogeneous servers figure 5 shows that linear scaling provides significantly better robustness than the opti mal dynamic scheme indeed the optimal scheme is only and zgs e e β eρ ρ ρ β β β e optimal for designs with ρ even then its cost is only slightly lower than that of linear scaling the significant we can further relate zgs to zlin by eρ ρ ρ price that linear scaling pays is that it requires very high processing speed when the occupancy is high which may not be supported by the hardware we now compare the robustness analytically in the case of α first we will show that if ρ is known the cost of the zgs zlin from which follows β β e β eρ eρ β β β e linear scheme is exactly the same as the cost of the gated static scheme and thus within a factor of of optimal theorem then we will show that when the target load differs from the actual load the linear scheme significantly reduces the cost theorem in particular the linear scaling scheme has cost independent of the difference between the design and actual ρ in contrast the cost of gated static grows linearly in this difference as seen in fig 5 theorem when α zgs zlin thus zlin proof if the speed in state n is kn then this insensitivity to design load mirrors worst case analysis the optimum available scaling 14 which designs for ρ is o 1 worst case competitive 5 however fig 5 suggests that linear scaling is much better than designing for ρ tighter bounds are known for sn α 16 20 but those are still looser than theorem vii concluding remarks speed scaling is an important method for reducing energy consumption in computer communication systems intrinsi cally it trades off the mean response time and the mean energy n consumption and this paper provides insight into this tradeoff e n ρ e kn ρ k e ρ k ρk using a stochastic analysis and so the total cost is optimized for k β in this case e ρ ρ asymptotics for the optimal speed scaling scheme are provided these bounds are tight for small and large γ and provide a number of insights e g that the mean response time is γ2 2γ which is identical to the cost for gated static by corollary 3 this is within a factor of of z theorem consider a system designed for target load ρt that is operating at load ρ ρt e when α ρ2 ρ scaling and that the optimal dynamic speeds in the stochastic model match for large n dynamic speed scalings that have been shown to have good worst case performance surprisingly the bounds also illustrate that a simple scheme which gates the clock when the system is idle and uses a static rate otherwise provides performance within a factor of of the optimal dynamic speed scaling however the value of zlin β β ρ dynamic speed scaling is also illustrated dynamic speed scaling schemes provide significantly improved robustness to bursty traffic and mis estimation of workload parameters proof the optimal rates for the linear policy are sn n β independent of ρt thus its cost is always the optimal speed for gated static in this case is sn ρt β for n when operated at actual load ρ this gives e ρ e ρρt ρ β ρt ρ β β β longer optimal when robustness is considered a scheme that scales speeds linearly with n provides significantly improved robustness while increasing cost only slightly there are a number of related directions in which to extend this work for example we have only considered dynamic power consumption which can be modeled as a polynomial of the speed however the contribution of leakage power is growing and an important extension is to develop models of total power use that can be used for analysis also it will be very interesting to extend the analysis to scheduling policies beyond ps for example given that the speed can be reduced if there are fewer jobs in the system it is natural to suggest scheduling according to shortest remaining processing time first srpt w hich i s k nown t o m inimize t he n umber o f jobs in the system suppose you are about to go skiing for the first time in your life naturally you ask yourself whether to rent skis or to buy them renting skis costs say 30 whereas buying skis costs if you knew how many times you would go skiing in the future ignoring complicating factors such as inflation and changing models of skis then your choice would be clear if you knew you would go at least 10 times you would be financially better off by buying skis right from the beginning whereas if you knew you would go less than 10 times you would be better off renting skis every time alas the future is unclear and you must make a decision nonetheless although the ski rental problem is a very simple abstraction this basic paradigm arises in many ap plications in computer systems in these situations there is a system that can reside in either a low cost or a high cost state occasionally it is forced to be in the high cost state usually to perform some task a period between any two such points in time is called an idle period the system pays a per time unit cost to reside in the high cost state alternatively it can transition to the low cost state at a fixed one time cost if the idle period is long it is advantageous to transition to the low cost state immediately if the idle period is short it is better to stay in the high cost state an online algorithm which does not know the length of the idle period must balance these two possibilities this problem has been studied in the context of shared memory multiprocessors in which a thread is waiting for a locked piece of data and must decide whether to spin or block 11 researchers investigating research supported partially by nsf grants ccr and ccf and by onr award 1 ywork done while the author was a student at the department of computer science cornell university ithaca ny research supported partially by nsf grant ccr the interface between ip networks and connection oriented networks have discovered this same underlying problem in deciding whether to keep a connection open between bursts of packets that must be sent along the connection karlin kenyon and randall study the tcp acknowledgment problem and the related bahncard problem both of which are at heart ski rental problems 10 the problem also arises in cache coherency in deciding whether to update or invalidate data that has been changed in a processor s local cache an important application of the ski rental problem is in minimizing the power consumed by devices that can transition to a low power sleep state when idle the sleep state consumes less power however one incurs a fixed start up cost in making the transition to the high power active state in order to begin work when a new job arrives at the architectural level the technique of eliminating power to a functional component is called clock power gating at a higher level the powered down component might be a disk drive or even the whole system e g a laptop that hibernates the embedded systems community has invested a great deal of effort into devising policies governing the selection of power states during idle periods termed dynamic power management in their literature see for example for a survey these techniques have been critical to maximizing battery use in mobile systems while power is already a first class parameter in system design it will become increasingly important in the future since battery capacities are increasing at a much slower rate than power requirements most of the previous work on this problem has been concerned with two state systems which have an active state and a single sleep state this paper focuses on finding power down thresholds for systems that have more than one low power state an example of such a system is the advanced configuration and power interface acpi included in the bios on most newer computers which has five power states including a hibernation state and three levels of standby 1 previous work and new results for the two state problem an online algorithm consists of a single threshold tafter which time the algorithm will transition from the active to the sleep state the input to the problem is the length of the idle period and the cost of an algorithm is the total amount of energy it consumes over a single idle period typically an online algorithm is evaluated in terms of its competitive ratio the ratio of the cost of the online algorithm to the cost of the optimal offline algorithm maximized over all inputs when randomized algorithms are considered where the threshold tis chosen at random we look at the ratio of the expected cost of the online algorithm to the cost of the offline algorithm previous work has also addressed the two state problem when the idle period is generated by a known probability distribution in this case the online algorithm will choose a threshold which minimizes its expected cost where the expectation here is taken over the random choice of the idle period we call such algorithms probability based algorithms the best deterministic online algorithm will stay in the high power state until the total energy spent is equal to the cost to power up from the low power state it is known that this algorithm achieves the optimal deterministic competitive ratio of when one considers randomized if the idle period is generated by a known probability distribution then the algorithm that chooses of optimal furthermore this bound is tight since there is a distribution over the idle period lengths which e times larger than that incurred by the optimal offline algorithm note that in the context of power down systems it may not be the case that the power usage in the sleep state is zero or even that the start up cost in the active state is zero in these cases both the online and the offline algorithm will incur an identical additional cost thus the ratio of the online to the offline cost will decrease and the optimal competitive ratio will be strictly less than two however these additional costs do not change the optimal online or offline strategy in either the deterministic or the probability based case and the optimal competitive ratio that can be achieved for such systems can easily be determined as a function of all the parameters of the system is the active state and the system must transition to i e power up at the end of note that there can be costs to move from high power states to low power states and vice where mkis some limiting integer constant this generalization would be especially useful for engineers who have a large number of sleep state options available in the design phase but are required to implement at most a fixed number of states in the product that rolls out into the market from the perspective of worst case guarantees is which considers the special case where the cost to power down is zero and the algorithm only pays to move from low power states to higher power states note that this also includes the case where the transition costs are additive di j dj k di kfor i j k since the costs to power down can then be folded into the costs to power up gives natural generalizations of the algorithms for the two state case both for the case when the idle period length is unknown and when it is generated by a known probability distribution it is shown that when the e competitive thus matching the guarantees in the two state case there are two important directions left open by this work the first is based on the observation that systems in general do not have additive transition costs in many scenarios additional energy is spent in transitioning to lower power states furthermore there could be overhead in stopping at intermediate states resulting in non additive transition costs see 4 for an example the second point is that the known upper bounds are typically not optimal for the system under consideration that is while it is true that there exist systems for which the optimal competitive ratio that can be by any randomized algorithm it is possible to achieve a better competitive ratio for many systems for multi state systems the optimal competitive ratio that can be achieved will in general be a complicated function of all the parameters of the system the power consumption rates as well as transition costs for probability based algorithms the optimal competitive ratio will also depend on the probability distribution generating the length of the idle period while it may not be feasible to express the optimal competitive ratio as a function of all these parameters a system designer would in general like to design a power down strategy that obtains the best possible competitive ratio given the constraints of his or her particular system this paper establishes the following results whose competitive ratio is within an additive eof the best competitive ratio that can be og e where k is the number of states in the system and also outputs the competitive ratio of the algorithm works via a decision procedure which determines for a system and a constant pif there is a p competitive strategy for that system this decision procedure also allows us to obtain lower bounds on the competitive ratio achievable by deterministic algorithms for specific systems which in turn provides a lower bound on the competitive ratio achievable by deter ministic algorithms in general in particular we obtain a lower bound of the competitive ratio for deterministic algorithms this is the first lower 5 the above approach can be modified to solve the more general version where a bound of mis specified on the number of states allowed in final strategy we show how to extend the decision procedure to answer if there is a p competitive strategy for the system that uses at most mpower states experimental results show that there are significant performance gains to be made by estimating the distribution governing the length of an idle period based on recent history and using this estimate to drive a probability based strategy we give an algorithm that takes as input a description of a system and a probability distribution generating the idle period length and produces the optimal power down strategy naturally the running time of the algorithm will depend on the representation of the distribution in practice this is most likely is the number of bins in the histogram and k is the number of states one outcome of the proof is that it also establishes the optimality of the strategy given in for additive systems we then generalize this to find the best online algorithm subject to the restriction that at most mstates are used at the expense of an extra factor of min the running time for all systems this result gives a bound on the competitive ratio achieved by also serves as a bound on the ratio of the expected costs of the online and offline algorithms when the input is probabilistically generated in the remainder of this paper we use the terms schedule or strategy interchangeably to refer to the choices of states and threshold times for powering down the term algorithm will refer to a procedure that produces a schedule or strategy based on a particular system azar et al in 3 consider a related problem which they refer to as capital investment this problem is a different generalization of the ski rental problem than the power down problem considered here however a special case of their problem coincides with a special case of our problem specifically they give a competitive deterministic algorithm for the special case of the power down problem in which the cost to transition to each state is the same regardless of the state from which one is transitioning later damaschke in 5 improves the upper bound on the competitive ratio for this special case also in the context of capital investment to deterministic algorithms and ranomized lower bound for any deterministic algorithm which subsumes the lower bound of here 3 preliminaries first we will establish that we can assume without loss of generality that the power up transition j the cost to transition from sito sjis di j dj the cost to go from sjto siis since there is never any reason to transition to a higher power state unless the system is transitioning to the active state at the arrival of a new task any set of actions in the original system will incur the same cost in the new system thus in the sequel we assume that di 0 0for all i denote the state which attains the minimum the optimal state the none of the online strategies we present will make use of a state that is never used by the optimal offline strategy for any time t energy state 0 state 1 state 2 state 3 time figure 1 energy consumed by the optimal strategy as a function of idle period length as the idle period length gets longer it becomes more worthwhile to pay the extra cost to figure 1 shows the total energy consumed by optas a function of the length of the idle period there is a line for each state the y intercept is the transition cost to move to that state from the active state and the slope is the power consumption rate the energy consumed by the optimal strategy is the lower envelope of these lines since it will first we establish that we can assume that for all i j di j j recall that we are really using di jto denote j by first going to then down to sj this is a non trivial assumption that we will have to handle later consider the the same state as opt at every time t the optimal strategy which knows the length of the idle period in advance will just transition to the optimal state strategy however must follow the optimal strategy making each transition to a new state as the idle period gets longer this is the strategy proposed in and shown to be competitive for additive systems note that this strategy is the same as the competitive balance strategy for the two state case competitive strategy for any system 5 a near optimal deterministic algorithm in this section we turn our attention to obtaining a near optimal schedule for a particular system e the algorithm is based on a decision procedure which determines whether a competitive schedule exists competitive schedule we also output the resulting schedule energy energy time titime shows the transformed strategy which now has an eager transition the following lemma shows that the online strategy must eventually get to a sufficiently low power state lemma 3 allows us to limit our concern to just the transition points in any online schedule this is a contradiction is maximized is a transition point in the strategy a never and is the earliest point at which these two functions have the same value not consid is larger than the slope of and we now explore ways to restrict the space of schedules we need to consider in searching for a competitive that is also p competitive proof figure 2 shows a schematic of the proof the jumps in the online cost the dashed line are j so t t the procedure above can be repeated until all the transitions are eager these inequalities allow one to do a binary search using the line segments of now we repeatedly consider the left end point of the segment that is in the middle of the low and high segments and use the above inequalities to update the low or high segment and the corresponding end point accordingly until the end points of the low and high segments correspond respectively to the left and right end points of a segment of and this segment the binary search can be implemented in time logk where kis the number of segments i e number of states lemma 4 immediately gives an algorithm that is exponential in k the number of states and determines whether a p competitive strategy exists for the system this algorithm enumerates all subsequences of states and determines the p eager strategy for that subsequence by finding the eager transition to each state based on the eager transitions to the previous states as described in the proof of lemma 5 a p competitive strategy for the system exists if and only if one of these p eager the remainder of this section presents a way to remove the exponential dependence on k to be the contiguous subsequence hsi sji where siand sjare elements of ssuch that i j one can find transition times for the state sequence so that in the resulting schedule each for the sequence note that uniquely determines the transition times q eager over all its transitions up to and including the transition to state s observe that if there is p a strategy that consists entirely of early transitions is called a p early strategy competitive strategy and the strategy since eager transitions and ends in state s it is also p competitive we now argue that is an early strategy note that was chosen so that the transition to state sis p early we have to show that the remaining transitions of are also p early and the sequence of states in this schedule is given by consider the was an early transition so from lemma we can deduce that we only need to consider a specific early and eager schedule competitive strategy exists we can now define a decision procedure exists that takes a system and a constant pand outputs yes if a p competitive strategy exists for the system and no otherwise the procedure can be modified to also output a p competitive strategy if it exists we employ a dynamic given has a lower cost than competitive strategy exists one can quickly elicit the schedule by starting from state kand retracing the states that minimized the earliest transition time we use the procedure exists to do a bisection og e where a bound of mis specified on the number of states that can be used by the online the intuition is that function eis now required to return the earliest time when the 0 is non increasing as is the earliest time when the system p eagerly transitions from sjto sigiven that the transition to sjwas p eager and occurred at og e 6 a probability based algorithm karlin et al study the two state case when the length of the idle period is generated by a known probability distribution p although they examined the problem in the context of the spin block problem their problem is identical to our two state case they observed that the expected cost of the online strategy that makes the transition to the sleep state at time tis oc dt 3 where the power consumption rate in the active state is the power consumption rate in the sleep state and is the transition cost between the two states the online strategy then should select the transition time tthat minimizes this cost the multi state case presents two distinct challenges the first is to determine the optimal sequence of states through which an online strategy should transition throughout the course of the idle period then once this sequence has been determined the optimal transition times need to be determined our proof proceeds by establishing that the only transition times that need to be considered are the optimal transition times for two states systems suppose for example that we are considering a sequence of state transitions in which state siis followed by state sj let ti jdenote the optimal transition time from state sito sjif these were the only two states in the system that is if siwere the active state and sjwere the only sleep state note that ti jcan be determined by the expression above we establish that regardless of the rest of the sequence the optimal transition point from state sito sjis ti j we call the ti j s the pairwise optimal transition times lemmas and 8 establish that the pairwise optimal transition times happen in the right order that is for i k j ti ktk j if this is not the case then any subsequence that has sifollowed by skfollowed by sjcan not possibly be the best sequence of states note that the ti j s may not necessarily be unique in general we will select the earliest transition time that minimizes the cost for the two state system lemma then shows that as long as the pairwise optimal transition times are in the right order they give the globally optimal set of transition times for that subsequence our algorithm then uses this fact to find the optimal sequence of states by dynamic programming note that it is not necessary to exhaustively consider all possible subsequences 6 1 optimal transition times we can assume that for i j define i jto be the cost to transition from state qito state qj that is sup 0 0 the cost of the strategy that uses these transition times is l whose optimal cost is no greater than the optimal cost for ql be the sequence of thresholds that minimizes the cost of this sequence of states define the following quantities which means that it must be greater than or equal to at least one of these values this means that the strategy that transitions altogether can only improve the strategy save on the transition cost which is accounted for in the last term below the summations over the jindices in telescope to show that the two expressions are identical 6 2 the optimal state sequence we now present a simple polynomial time algorithm to obtain the optimal state sequence for a given were the only two states in the system the time complexity of determining a single ti jdepends on the representation of the probability distribution in practice this is most likely to be estimated by a finite histogram with bbins starting at time sampled at a uniform discrete interval of it follows that bin icorresponds to time i it is not difficult to generalize this for variable sized bins we will also assume that all transition equals values we can re write the expression for the cost of a two state system in equation 3 as respectively using the pre calculated values above the cost of transitioning from state sito state sjat time lis once the ti j s are found we sweep through them in non decreasing order keeping a running tab of the best sub schedules that we can achieve ending in each state siat each point in time when we encounter a ti j we check to see if transitioning from sito sjcan improve the current best sub schedule ending in sj and if it does update our data structure to reflect it a given strategy divides time into intervals where each interval is the period of time spent in a particular state the expected cost for a strategy given in equation 4 is obtained by summing over the expected cost we know the right hand side of this inequality is exactly cb u hence cb s cb t cb u since b was arbitrarily chosen it follows by summing that f u f s f t note that the function f x is a convex function when α and f a b f a f b it then immediately follows that e u e s e t on a job by job basis since ei u α and ei s ei t α xi s xi t xi s xi t how to compute mφ a for a given configuration φ we consider the problem of given an energy a and a fixed configuration φ finding the optimal schedule among those schedules with configuration φ and energy at most a actually we restate this problem as given a power pn and a fixed configuration φ finding the optimal schedule among those schedules with configuration φ and power pn on job n we define a group as a maximal substring of jobs where φ i is more precisely a a b is a group if a or φ a is not and for i a b it is the case that φ i is and b n or φ b is not a group is open if φ b is or b n and it is closed if φ b is lemma states how to compute the powers of the jobs in a group given the power of the last job of the group lemma if a a b are jobs in a group in the optimal schedule for energy level a then pi pb b i pn for i a b proof this is an immediate application of the third item in lemma it should now be clear how to compute the powers of the jobs a a b in an open group from lemma the power of the last job in the group is pn and the powers of the earlier jobs in the group are given by lemma to compute the powers of the jobs in a closed group is a bit more involved lemma establishes a relationship between the power of the jobs in a closed group and the length of the time period when the jobs in this group are run lemma if a a b are jobs in a closed group in the optimal schedule for energy level a b then i a pb b i pn α rb ra proof from the definition of closed group jobs a a b run back to back job a starts at time ra and job b completes at time rb thus b b b rb ra xi p b i p α where the last equality follows from lemma lemma gives an implicit definition of pb as a function of pn however it does not ap pear possible express pb as a simple closed form function of pn we next prove in lemma that b i a pb b i pn α is strictly increasing as a function of pb therefore one can determine pb from pn by binary search on pb we can then compute the powers of other jobs in this closed group using lemma b lemma when pn is fixed i a pb b i pn α decreases as pb increases proof for i a b as pb increases pb b i pn α increases thus pb b i pn α b decreases and so does i a pb b i pn α finally we show that continuously decreasing pn is equivalent to continuously decreasing the energy bound a in the sense that they will trace out the same schedules albeit at a different rate lemma in the optimal schedules the energy bound a is a strictly increasing continuous function of pn and similarly pn is a strictly increasing continuous function of a proof we first prove that there is a bijection between pn and a in the optimal schedules as well as in the optimal schedules restricted to a particular configuration the fact that a fixed pn is mapped into a unique energy should be obvious from our development to date that two different pn can not map to optimal schedules with the same energy follows from lemma since the function from pn to a is obviously continuous it then follows that the function from pn to a is either strictly increasing or strictly decreasing the fact that function is strictly increasing then follows from looking at the extreme points if a is very large then the optimal configuration is all and pn is large if a is very small then the optimal configuration is all and pn is small the results of lemmas and implies that if we know the configuration of the optimal schedule that uses energy a then we can compute the actual schedule since there are θ possible configurations then a simple algorithmic approach to compute the optimal schedule is to enumerate all these configurations compute the schedule that is uses energy a for each of these configurations and select the one with the smallest total flow time however some of these schedules are not feasible and need to be removed from consideration these schedules can be detected using the observations in section note that the fact that some of these schedules are not feasible does not contradict the results of lemmas and these lemmas assume that the configuration corresponds to the optimal schedule if the configuration does not correspond to the optimal schedule then the schedule obtained may be infeasible this algorithm runs in time θ poly n as there are θ possible configurations this algorithm is different from the one described in section how to recognize when the optimal configuration changes in the algorithm described in section the algorithm gradually decreases the amount of energy a or equivalently the power pn of job n eventually the configuration of the optimal schedule will change from a configuration φ to a configuration φ we now explain how to recognize this given a configuration φ that is no longer optimal for energy level a the formulas for computing the powers of the jobs in lemmas and will yield a schedule whose configuration is not equal to φ there are ways that this could happen one of the constraints is violated there is a last job b in some closed group with pb pb pn in a non degenerate case the configuration of the schedule obtained from lemmas and will be different from φ by exactly one constraint φ i the next configuration φ is obtained from φ by changing φ i as follows if φ i is it should be changed to if φ i is it should be changed to in a degenerate case all violated φ i need to be changed implementation details and time complexity to construct an efficient implementation of the algorithm described in section we need to change pn in a discrete fashion this can be accomplished using binary search to find the next value for pn when the optimal configuration changes suppose l is the range of possible values of pn divided by our desired accuracy then binary search needs to check o log l values of pn to determine the next value for pn when the optimal configuration changes the condition for determining whether the current configuration is optimal for a particular pn described in the section can be computed in linear time thus in time o n log l we can find the next configuration curve on the lower envelope there are only configuration curves on the lower envelope as the only possible transitions are from to or from to thus we get a running time of o log l if there are jobs with equal release dates then the only change that is required is that in the initial configuration all jobs with equal release dates are in one open group with speeds given by lemma mathematica and java animation program the animation program mentioned in the introduction is a variation of our algorithm to allow quick response time for animation the schedules for energy levels within the given range for the given accuracy are precomputed to simplify the implementation when the program computes the schedule for a particular energy level a it does not trace to lower envelop m from high energy to low energy instead it begins with a configuration φ where each φ i is then computes the optimal schedule in this configuration at energy level a if the resulting schedule violates the configuration φ it updates the configuration φ as described in the previous section and recomputes the optimal schedule for the new configuration the program repeats this process until the resulting schedule matches φ arbitrary work jobs we now consider the case of arbitrary work jobs we first show that applying our algorithmic approach in this case is problematic as the energy optimal schedule is not a smooth function of of the energy bound a in particular it is sufficient to consider an instance with two jobs where the job with the earlier release date has more work than the job with the later release date then consider what happens as the energy bound a decreases first becomes equal to then but srpt still gives priority to job at time and but eventually srpt will give priority to job at time and job is preempted by job thus when this happens the value of max increases discontinuously see figure this discontinuous change can have a ripple effect as it can disturb any schedule of other jobs lying between the previous value of max and the new value of max more energy less energy figure optimal configurations at different amounts of available energy when jobs have unit work left and arbitrary amount of work right to show that the makespan of the schedule with jobs changes discontinuously when the jobs switch priority we give the analysis of the situation let s be the optimal schedule under the restriction that job is given a higher priority than job let s be the optimal schedule under the restriction that job is given a higher priority than job it can be shown through an argument similar to that in lemma that s s and s s or equivalently x s x s and x s x s suppose a units of energy is available then α a p s x s p s x s wα w wα and s α α a p s x s p s x s wα αw wα s α wα αw wα α α the optimal schedule is either s or s and is equal to the one with the smaller flow time f s is smaller if a is large and f s is smaller if a is smaller f s s s s s s f s s s s s s the switch over occurs when f s f s or equivalent when s s s s when this occurs the makespan switches from cmax s s s to cmax s s s now we argue that in general cmax s cmax s suppose cmax s cmax s then from equality this implies that s s however from equalities and this is not true in general note that we don t have this problem when jobs have unit work because srpt behaves like fcfs in particular job priorities are fixed according to their arrival times thus jobs never switch priorities as the energy decreases for two unit size jobs the optimal schedule is always s and never s we can use the algorithm for equi work jobs to obtain an algorithm for arbitrary work jobs that is o approximate with respect to average response time given an additional factor of ǫ energy the fractional response time is the integral over times t of the sum over all jobs j released before time t of the fraction of the work of job j not completed by time t so if of the work of job j is completed by time t then job j contributes towards the sum at time t the fractional response time is a lower bound to the total response time which is the integral over all times t of the number of jobs released but not completed by time t by discretizing time we can write a convex program for the optimal fractional response time schedule as follows t t min wj wj u u rj n wj j t sα e u wj u j n u t the time t is some sufficiently late time that we may be sure all jobs finish by this time in the optimal schedule it is easy to see that t may be pseudo polynomially bounded the variable wj u is the amount of work done on job j at time u the objective function is just the definition of fractional flow time in a discrete setting the constraints in line express the fact that each job must be eventually finished the constraints in line define the speed su at time u the constraint in line is then the energy bound this convex program can be solved to arbitrary precision in polynomial time in the size of the program which is pseudo polynomial in the input size using the ellipsoid algorithm it is known that the response time of an optimal fractional response time schedule is within a factor of of the optimal response time schedule for a processor that is a factor of ǫ slower the proof in assumes a fixed speed processor but easily carries over to the case of a variable speed processor thus for arbitrary work jobs we can compute an o approximate schedule given an extra factor of ǫ α energy conclusions we believe that speed scaling problems particularly those without deadlines is a research area where there are many combinatorially interesting problems with real application probably the most obvious open question that arises from this paper is whether there is a polynomial time algorithm to compute the energy optimal schedule in the case of arbitrary work jobs more generally it would interesting to determine whether our algorithmic approach is applicable to other speed scaling problems very recently considers an online variation of the flow time scheduling problem that we consider here they consider a combined objective of the total flow time plus the energy used they essentially show that running at a power approximately equal to the number of released but unfinished jobs is o competitive acknowledgments we thank mahai anitescu for several helpful discussions the observation that our algorithm for equi work jobs can be applied to arbitrary work jobs arose from discussions with sandy irani and john augustine we thank aleksandar ivetic for creating the java applet illustrating the change of the optimal schedule as a function of energy enhanced intel speedstep technology for the intel pentium m processor white paper march enhanced intel speedstep technology for the intel pentium m processor information in this document is provided in connection with intel products no license express or implied by estoppel or otherwise to any intellectual property rights is granted by this document except as provided in intel s terms and conditions of sale for such products intel assumes no liability whatsoever and intel disclaims any express or implied warranty relating to sale and or use of intel products including liability or warranties relating to fitness for a particular purpose merchantability or infringement of any patent copyright or other intellectual property right intel products are not intended for use in medical life saving life sustaining applications intel may make changes to specifications and product descriptions at any time without notice designers must not rely on the absence or characteristics of any features or instructions marked reserved or undefined intel reserves these for future definition and shall have no responsibility whatsoever for conflicts or incompatibilities arising from future changes to them this document as well as the software described in it is furnished under license and may only be used or copied in accordance with the terms of the license the information in this manual is furnished for informational use only is subject to change without notice and should not be construed as a commitment by intel corporation intel corporation assumes no responsibility or liability for any errors or inaccuracies that may appear in this document or any software that may be provided in association with this document except as permitted by such license no part of this document may be reproduced stored in a retrieval system or transmitted in any form or by any means without the express written consent of intel corporation contact your local intel sales office or your distributor to obtain the latest specifications and before placing your product order copies of documents which have an ordering number and are referenced in this document or other intel literature may be obtained by calling or by visiting intel website at anypoint appchoice boardwatch bunnypeople cableport celeron chips ct media dialogic etherexpress etox flashfile icomp instantip intel intel centrino intel logo intel create share intel gigablade intel inbusiness intel inside intel inside logo intel netburst intel netmerge intel netstructure intel play intel play logo intel singledriver intel speedstep intel strataflash intel teamstation intel xeon intel xscale iplink itanium mcs mmx mmx logo optimizer logo overdrive paragon pc dads pc parents pdcharm pentium pentium ii xeon pentium iii xeon performance at your command remoteexpress smartdie sound mark storageexpress the computer inside the journey inside tokenexpress voicebrick vtune and xircom are trademarks or registered trademarks of intel corporation or its subsidiaries in the united states and other countries other names and brands may be claimed as the property of others copyright intel corporation enhanced intel speedstep technology for the intel pentium m processor contents enhanced intel speedstep technology for the intel pentium m processor introduction enhanced intel speedstep technology has revolutionized thermal and power management by giving application software greater control over the processor operating frequency and input voltage systems can easily manage power consumption dynamically this paper describes how this flexible power management scheme is implemented and how designers can best integrate this technology into their current or next generation board design today embedded systems are demanding greater performance at equivalent levels of power consumption legacy hardware support for backplanes board sizes and thermal solutions have forced design teams to place greater emphasis on power and thermal budgets intel has extended architectural innovation for saving power by implementing new features such as enhanced intel speedstep technology enhanced intel speedstep technology allows the processor performance and power consumption levels to be modified while a system is functioning this is accomplished via application software which changes the bus to core frequency ratio and the processor core voltage vcc a variety of inputs such as system power source processor thermal state or operating system policy are used to determine the proper operating state the software model behind enhanced intel speedstep technology has ultimate control over the frequency and voltage transitions this software model is a major step forward over previous implementations of intel speedstep technology legacy versions of intel speedstep technology required hardware support through the chipset enhanced intel speedstep technology has removed the chipset hardware requirement and only requires the support of the voltage regulator processor and operating system centralization of the control mechanism and software interface to the processor and reduced hardware overhead has reduced processor core unavailability time to µs from the previous generation unavailability of µs processor support enhanced intel speedstep technology is supported on current and future generations of intel pentium m processors the intel pentium m processor at ghz supports six frequency and voltage operating points table supported performance states for the intel pentium m processor at the top and bottom modes are commonly known as high frequency mode hfm and low frequency mode lfm these frequency and voltage operating points are stored within a read only processor model specific register msr this msr ensures bios will not allow transitions to invalid states above the hfm maximum or below the lfm minimum the other four operating points are stored within bios code as a drop in voltage table provided by intel to bios vendors enhanced intel speedstep technology for the intel pentium m processor thermal design power the intel power specification for components is known as thermal design power or tdp the tdp value along with the maximum junction temperature defines intel recommended design point for thermal solution capability for the intel pentium m processor which supports enhanced intel speedstep technology intel will specify the tdp for both the hfm and lfm operating states these tdp values are specified in the intel pentium m processor datasheet and also shown in the following table for the ghz and ghz parts table hfm and lfm thermal design power points for the intel pentium m processor at table hfm and lfm thermal design power points for the low voltage intel pentium m processor at between the hfm and lfm frequency voltage points the dissipated power will be approximately proportional to the square of the voltage due to ohm law equation ohm law where p power c capacitance v voltage and f frequency p enhanced intel speedstep technology for the intel pentium m processor figure power vs core voltage for the intel pentium m processor at for its six frequency voltage operating points not to scale hfm and lfm power values are tdp specifications power w core voltage v implementation all intel pentium m processors have support for enhanced intel speedstep technology this can be verified by checking the ecx feature bit in the cpuid register enhanced intel speedstep technology is enabled by setting the msr bit this bit should be written by the system bios upon boot processor frequency voltage transitions are initiated by writing a bit value to the register if a transition is already in progress transition to a new value will take effect subsequently reads of the register determine the last targeted operating point the current operating point can be read from register which is updated dynamically the bit encoding defining valid operating points is model specific and intel proprietary see your intel representative to obtain documentation outlining the required encoding by centralizing the implementation of enhanced intel speedstep technology to the processor software can perform a frequency voltage operating state transition by simply writing to one register enhanced intel speedstep technology for the intel pentium m processor table enhanced intel speedstep technology msrs software considerations as already noted the software model behind enhanced intel speedstep technology performs all management for the frequency and voltage transitions microsoft windows xp and the windows server family include complete native processor performance control to support this functionality the native support in windows xp and the windows server family consists of two components the kernel power policy manager and the processor driver the kernel power policy manager owns the decision making and the set of rules used to determine the appropriate frequency voltage operating state it may make decisions based on several inputs such as end user power policy processor utilization battery level or thermal conditions and events the processor driver is used to make actual state transitions on the kernel power policy manager behalf the driver does not initiate frequency voltage state transitions independent of the kernel power policy manager enhanced intel speedstep technology for the intel pentium m processor figure software model for windows xp and windows server get state change state msr read msr write end user power policy inputs are entered via the power options icon within the control panel based on the selected policy windows will adjust its decision making for power states accordingly table relationship between power policy scheme and processor dynamic throttling policy the processor dynamic throttle policy has been defined to operate in one of four states none adaptive constant and degrade none will ensure the processor is always at the highest performance state currently available barring any unique thermal conditions where hardware will force a lower state adaptive matches the performance state to current demand constant will run the processor in the lowest available frequency voltage state similar to the constant state degrade will always run the processor in the lowest available frequency voltage state furthermore this policy state will utilize linear stop clock throttling if the remaining battery capacity drops below a certain threshold apart from windows xp and the windows server family support also exists from intel and microsoft for enhanced intel speedstep technology transitions on legacy microsoft platforms legacy versions of the windows operating system do not include complete power policy manager and driver support thus intel has released an enhanced intel speedstep technology applet specifically to enable this functionality on the following versions of windows enhanced intel speedstep technology for the intel pentium m processor microsoft windows microsoft windows microsoft windows millennium edition me microsoft windows nt this applet is available from either intel or an intel oem see your intel representative for more information or to order the enhanced intel speedstep technology applet the enhanced intel speedstep technology applet provides both a power policy and a driver to enable dynamic transitions the end user policy is again established through the windows control panel with very similar power state operation figure software model for the enhanced intel speedstep technology applet on legacy windows platforms periodic get state set state requests load gui user preferences set state periodic get state i o i o notice the software model for legacy windows platforms must use system management mode which creates slightly more overhead on the system a detailed overview of system management mode smm may be found in the ia intel architecture software developer manual support for operating systems other than microsoft windows is available to varying degrees in the linux open source community linux operating systems with support for acpi may enable performance state transitions by manually writing to the appropriate file e g proc cpufreq or proc acpi processor linux kernel version and newer provides full acpi support and maps enhanced intel speedstep technology enhanced intel speedstep technology for the intel pentium m processor msrs to the correct object table see table although support may exist for manual performance state changes through acpi object tables automatic performance state change based on thermal conditions battery life or other user parameters is limited and dependent on operating system vendor more information may be found at intel instantly available technology website at acpi considerations acpi or advanced configuration and power interface has greatly simplified and standardized power system management acpi has introduced new power states on a system device and processor level this paper will introduce the processor power state levels p states and map them to how enhanced intel speedstep technology transitions are made other power states such as c states s states and d states are covered in the acpi specification at figure processor performance states p states discussed herein are sub states of processor p states are defined as frequency voltage operating states unlike other acpi defined power states such as etc the processor is actually executing instructions in all p states since they are substates of is defined as the maximum performance highest power consuming state which is also termed hfm on an intel pentium m processor px px are defined with incrementally lower performance and power dissipation enhanced intel speedstep technology for the intel pentium m processor table p states for the intel pentium m processor at acpi provides a standard interface for operating systems to communicate with hardware specific component capabilities are mapped into acpi tables table acpi object table definitions it is the responsibility of the bios to construct and fill in the acpi tables intel provides frequency voltage tables or a bios algorithm to fill the object table after the tables have been populated the operating system may force performance state changes by accessing the msrs via the corresponding acpi object tables non acpi implementations acpi is recommended for designing systems that utilize enhanced intel speedstep technology deeply embedded designs not utilizing the acpi interface may still incorporate frequency and voltage transitions by directly reading and writing to the corresponding msrs on the processor the chosen frequency and voltage combinations must match the valid combinations as presented in the processor datasheet any other combinations will force the processor to operate outside of specification and may cause component failure centralizing implementation for real time frequency voltage transitions to a processor msr has enabled deeply embedded operating systems and applications to easily change and control operating states enhanced intel speedstep technology for the intel pentium m processor voltage regulator support processors featuring enhanced intel speedstep technology provide multiple vid pins that are directly connected to voltage regulator these vid pins signal the desired voltage for the processor the pin output is mapped to a table as listed in the corresponding processor datasheet the corresponding voltage for the vid output is subsequently provided to the core processor plane by the voltage regulator voltage regulator designs for the intel pentium m processor have been designed to accept core voltage inputs from v to v as selected by six vid pins this wide range of voltages allows one regulator design to support all processors within the intel pentium m and intel celeron m families including low voltage and ultra low voltage versions additionally all processors in this family are both package and pin footprint compatible conclusion enhanced intel speedstep technology has revolutionized thermal and power management by centralizing hardware implementation to the processor by also giving software greater control over the transition mechanism performance state transitions may be dynamically controlled to manage increasingly tight power and thermal budgets references acpi advanced configuration and power interface ia intel architecture software developer manual volume system programming guide microsoft acpi power management intel pentium m processor datasheet windows native processor performance control instantly available technology acpi ap intel processor identification and the cpuid instruction notes please refer to the intel pentium m processor datasheet for the latest specifications all information included in the datasheet supersedes the material of this document joulesort a balanced energy efficiency benchmark suzanne rivoire stanford university mehul a shah hp labs parthasarathy ranganathan hp labs christos kozyrakis stanford university abstract the energy efficiency of computer systems is an important concern in a variety of contexts in data centers reducing energy use improves operating cost scalability reliability and other factors for mobile devices energy consumption directly affects functionality and usability we propose and motivate joulesort an external sort benchmark for evaluat ing the energy efficiency of a wide range of computer systems from clusters to handhelds we list the criteria challenges and pitfalls from our experience in creating a fair energy efficiency benchmark using a commercial sort we demon strate a joulesort system that is over as energy efficient as last year estimated winner this system is quite differ ent from those currently used in data centers it consists of a commodity mobile cpu and laptop drives connected by server style i o interfaces categories and subject descriptors h information systems database management systems general terms design experimentation measurement performance keywords benchmark energy efficiency power servers sort introduction in contexts ranging from large scale data centers to mobile devices energy use in computer systems is an important concern in data center environments energy efficiency affects a number of factors first power and cooling costs are signifi cant components of operational and up front costs today a typical data center with racks consuming total power costs to power and to cool per year with permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee sigmod june beijing china copyright acm of up front costs for cooling equipment these costs vary depending upon the installation but they are growing rapidly and have the potential eventually to outstrip the cost of hardware second energy use has implications for density reliability and scalability as data centers house more servers and consume more energy removing heat from the data center becomes increasingly difficult since the reliability of servers and disks decreases with increased temperature the power consumption of servers and other components limits the achievable density which in turn lim its scalability third energy use in data centers is starting to prompt environmental concerns of pollution and excessive load placed on local utilities energy related concerns are severe enough that companies like google are starting to build data centers close to electric plants in cold weather cli mates all these concerns have led to improvements in cooling infrastructure and in server power consumption for mobile devices battery capacity and energy use di rectly affect usability battery capacity determines how long devices last constrains form factors and limits functional ity since battery capacity is limited and improving slowly device architects have concentrated on extracting greater energy efficiency from the underlying components such as the processor the display and the wireless subsystems in isolation to drive energy efficiency improvements we need bench marks to assess their effectiveness unfortunately there has been no focus on a complete benchmark including a work load metric and guidelines to gauge the efficacy of energy optimizations from a whole system perspective some efforts are under way to establish benchmarks for energy efficiency in data centers but are incomplete other work has emphasized metrics such as the energy delay product or per formance per watt to capture energy efficiency for proces sors and servers without fixing a workload moreover while past emphasis on processor energy efficiency has led to improvements in overall power consumption there has been little focus on the i o subsystem which plays a significant role in total system power for many important workloads and systems in this paper we propose joulesort as a holistic bench mark to drive the design of energy efficient systems joule sort uses the same workload as the other external sort bench marks but its metric incorporates total energy which is a combination of power consumption and perfor mance the benchmark can be summarized as follows sort a fixed number of randomly permuted byte records with byte keys the sort must start with input in a file on non volatile store and finish with output in a file on non volatile store there are three scale categories for joulesort and records the winner in each category is the system with the minimum total energy use we choose sort as the workload for the same basic rea son that the terabyte sort minutesort pennysort and performance price sort benchmarks do it is simple to state and balances system component use sort stresses all core components of a system memory cpu and i o sort also exercises the os and filesystem sort is a portable workload it is applicable to a variety of systems from mobile devices to large server configurations another natural reason for choosing sort is that it represents sequen tial i o tasks in data management workloads joulesort is an i o centric benchmark that measures the energy efficiency of systems at peak use like previous sort benchmarks one of its goals is to gauge the end to end ef fectiveness of improvements in system components to do so joulesort allows us to compare the energy efficiencies of a variety of disparate system configurations because of the simplicity and portability of sort previous sort bench marks have been technology trend bellwethers for example foreshadowing the transition from supercomputers to clus ters similarly an important purpose of joulesort is to chart past trends and gain insight into future trends in energy ef ficiency beyond the benchmark definition our main contributions are twofold first we motivate and describe pitfalls sur rounding the creation of a fair energy efficiency benchmark we justify our fairest formulation which includes three scale factors that correspond naturally to the dominant classes of systems found today mobile desktop and server al though we support both daytona commercially supported and indy no holds barred categories for each scale we concentrate on daytona systems in this paper second we present the winning joulesort system that is over more efficient sortedrecs joule for than last year estimated winner sortedrecs joule for this system shows that a focus on energy effi ciency leads to a unique configuration that is hard to find pre assembled our winner balances a low power mobile processor with numerous laptop disks connected via server class pci e i o cards and uses a commercial sort nsort the rest of the paper is organized as follows in section we estimate the energy efficiency of past sort benchmark winners which suggests that existing sort benchmarks can not serve as surrogates for an energy efficiency benchmark section details the criteria and challenges in designing joulesort and lists issues and guidelines for proper energy measurement in section we measure the energy con sumption of unbalanced and balanced systems to motivate our choices in designing our winning system the balanced system shows that the i o subsystem is a significant part of total power section provides an in depth study of our joule sort system using nsort in particular we show that the most energy efficient cost effective and best performing configuration for this system is when the sort is cpu bound year figure estimated energy efficiency of previous winners of sort benchmarks we also find that both the choice of filesystem and in memory sorting algorithm affect energy efficiency section discusses the related work and section presents limitations and fu ture directions historical trends in this section we seek to understand if any of the exist ing sort benchmarks can serve as a surrogate for an energy efficiency benchmark to do so we first estimate the sort edrecs joule ratio a measure of energy efficiency of the past decade sort benchmark winners this analysis reveals that the energy efficiency of systems designed for pure per formance i e minutesort terabyte sort and datamation winners has improved slowly moreover systems designed for price performance i e pennysort winners are compar atively more energy efficient and their energy efficiency is growing rapidly however since our joulesort sys tem energy efficiency is well beyond what growth rates would predict for this year pennysort winner we conclude that existing sort benchmarks do not inherently provide an incentive to optimize for energy efficiency supporting the need for joulesort methodology figure shows the estimated sortedrecs joule metric for the past sort benchmark winners since we compute these metrics from the published performance records and our own estimates of power consumption since energy use was not reported we obtain the performance records and hardware configuration information from the sort bench mark website and the winners posted reports we estimate total energy during system use with a straight forward approach from the power management community since cpu memory and disk are usually the main power consuming system components we use individual estimates of these to compute total power for memory and disks we use the hp enterprise configurator power calcu lator to yield a fixed power of per disk and per dimm some of the sort benchmark reports only mention total memory capacity and not the number of dimms in those cases we assume a dimm size appropriate to the era of the report the maximum power specs for cpus usually quoted as thermal design power tdp are much higher than the peak numbers seen in common use thus we derate these power ratings by a factor although a bit con servative this approach allows reasonable approximations for a variety of systems when uncertain we assume the newest possible generation of the reported processor as of the sort benchmark record because a given cpu power consumption improves with shrinking feature sizes finally to account for power supplies inefficiencies which can vary widely and other components we scale total system power derived from component level estimates by for single node systems we use a higher factor for clusters to account for additional components such as networking management hardware and redundant power supplies our power estimates are intended to illuminate coarse his torical trends and are accurate enough to support the high level conclusions in this section we experimentally vali dated this approach against some server and desktop class systems and its accuracy was between and analysis although previous sort benchmark winners were not con figured with power consumption in mind they roughly re flect the power characteristics of desktop and higher end sys tems in their day thus from the data in figure we can in fer qualitative information about the relative improvements in performance price performance and energy efficiency in the last decade figure compares the energy efficiency of previous sort winners using the sortedrecs joule ratio and supports the following observations systems optimized for price performance i e pennysort winners clearly are more energy efficient than the other sort benchmark winners which were optimized for pure perfor mance there are two reasons for this effect first the price performance metric motivates system designers to use fewer components and thus less power second it provides incentive to use cheaper commodity components which for a given performance point traditionally have used less en ergy than expensive high performance components the energy efficiency of cost conscious systems has im proved faster than that of performance optimized systems which have hardly improved others have also observed a flat energy efficiency trend for cluster hardware much of the growth in the pennysort curve is from the last two indy winners which have made large leaps in energy efficiency in algorithmic improvements and a minimal hardware configuration played a role in this improvement but most importantly cpu design trends had finally swung toward energy efficiency the processor used in the pennysort winner has the clock frequency of its immediate prede cessor while only consuming the power overall the sort had better performance than the previous data point while using the power the pennysort win ner gputerasort increased energy efficiency by introduc ing a new system component the graphics processing unit gpu and utilizing it very effectively the chosen gpu is inexpensive and comparable in power consumption to the cpu but it provides better streaming memory bandwidth than the cpu this latest winner in particular shows the danger of rely ing on energy benchmarks that focus only on specific hard ware like cpu or disks rather than end to end efficiency such specific benchmarks would only drive and track im table this table shows the estimated yearly growth in pure performance price performance and energy efficiency of past winners provements of existing technologies and may fail to antici pate the use of potentially disruptive technologies since price performance winners are more energy efficient we next examine whether the most cost effective sort implies the best achievable energy efficient sort to do so we first estimate the growth rate of sort winners along multiple di mensions table shows the growth rate of past sort bench mark winners along three dimensions performance sort edrecs sec price performance sortedrecs and energy efficiency sortedrecs joule we separate the growth rates into two categories based on the benchmark optimization goal price or pure performance since the goal drives the system design for each category we calculate the growth rate as follows we choose the best system according to the metric in each year and fit the result with an exponential table shows that pennysort systems are improving al most at the pace of moore law along the performance and price performance dimensions the pure performance sys tems however are improving much more slowly as noted elsewhere more importantly our analysis shows much slower esti mated growth in energy efficiency than in the other two metrics for both benchmark categories given last year estimated pennysort winner provides srecs j our current joulesort winner at srecs j is nearly the expected value of srecs j for this year this result suggests that we need a benchmark focused on en ergy efficiency to promote development of the most energy efficient sorting systems and allow for disruptive technologies in energy efficiency irrespective of cost benchmark design in this section we detail the criteria and challenges in de signing an energy efficiency benchmark we describe some of the pitfalls of our initial specifications and how the bench mark has evolved we also specify rules of the benchmark with respect to both workload and energy measurement criteria although past studies have proposed energy efficiency met rics or power measurement techniques none provide a complete benchmark a workload a metric of comparison and rules for running the workload and mea suring energy consumption moreover these studies tradi tionally have focused on comparing existing systems rather than providing insight into future technology trends we set out to design an energy oriented benchmark that addresses these drawbacks with the criteria below in mind while achieving all these criteria simultaneously is hard we strive to encompass them as much as possible energy efficiency the benchmark should measure a sys tem bang for the buck where bang is work done and the cost reflects some measure of power use e g average power peak power total energy and energy delay to drive practical improvements in power consumption cost should reflect both a system performance and power use a sys tem that uses almost no power but takes forever to complete a task is not practical so average and peak power are poor choices thus there are two reasonable cost alternatives energy a product of execution time and power or energy delay a product of execution time and energy the former weighs performance and power equally while the latter pop ular in cpu centric benchmarks places more emphasis on performance since there are other sort benchmarks that emphasize performance we chose energy as the cost peak use a benchmark can consider system energy in three important modes idle peak use or a realistic combi nation of the two although minimizing idle mode power is useful evaluating this mode is straightforward real world workloads are often a combination but designing a broad benchmark that addresses a number of scenarios is difficult to impossible hence we chose to focus our bench mark on an important but simpler case energy efficiency during peak use energy efficiency at peak is the opposite extreme from idle and gives an upper bound on work that can be done for a given energy this operating point influ ences design and provisioning constraints for data centers as well as mobile devices in addition for some applications e g scientific computing near peak use can be the norm holistic and balanced a single component cannot accu rately reflect the overall performance and power character istics of a system therefore the workload should exercise all core components and stress them roughly equally the benchmark metrics should incorporate energy used by all core components inclusive and portable we want to assess the energy ef ficiencies of a wide variety of systems pdas laptops desk tops servers clusters etc thus the benchmark should include as many architectures as possible and be as unbi ased as possible it should allow innovations in hardware and software technology moreover the workload should be implementable and meaningful across these platforms history proof in order to track improvements over gen erations of systems and identify future profitable directions we want the benchmark specification to remain meaningful and comparable as technology evolves representative and simple the benchmark should be representative of an important class of workloads on the sys tems tested it should also be easy to set up execute and administer workload we begin with external sort as specified in the previous sort benchmarks as the workload because it covers most of our criteria the task is to sort a file containing randomly permuted byte records with byte keys the input file must be read from and the output file written to a non volatile store and all intermediate files must be deleted the output file must be newly created it cannot overwrite the input file this workload is representative because most platforms from large to small must manage an ever increasing sup ply of data to do so they all perform some type of i o centric tasks critical for their use for example large scale websites run parallel analyzes over voluminous log data across thousands of machines laptops and servers con tain various kinds of filesystems and databases cell phones pdas and cameras store retrieve and process multimedia data from flash memory with previous sort implementations on clusters super computers smps and pcs as evidence we believe sort is portable and inclusive it stresses i o memory and the cpu making it holistic and balanced moreover the fastest sorts tend to run most components at near peak utilization so sort is not an idle state benchmark finally this work load is relatively history proof while the parameters have changed over time the essential sorting task has been the same since the original datamationsort benchmark was proposed in metric after choosing the workload the next challenge is choos ing the metric by which to evaluate and compare different systems there are many ways to define a single metric that takes both power and performance into account we list some alternatives that we rejected describe why they are inappropriate and choose the one most consistent with the criteria presented in section fixed energy budget the most intuitive extension of minutesort and pennysort is to fix a budget for energy consumption and then com pare the number of records sorted by different systems while staying within that energy budget this approach has two drawbacks first the power consumption of current plat forms varies by several orders of magnitude less than for handhelds to over for servers and much more for clusters or supercomputers if the fixed energy budget is too small larger configurations can only sort for a frac tion of a second if the energy budget is more appropriate to larger configurations smaller configurations would run out of external storage to be fair and inclusive we would need multiple budgets and categories for different classes of systems second and more importantly from a practical bench marking perspective finding the number of records to fit into an energy budget is a non trivial task due to unavoid able measurement error there are inaccuracies in synchro nizing readings from a power meter to the actual runs and from the power meter itself for the one we used since energy is the product of power and time it is suscep tible to variation in both quantities so this choice is not simple fixed time budget similar to the minute and performance price sort we can fix a time budget e g one minute within which the goal is to sort as many records as possible the winners for the minute and performance price sorts are those with the min imum time and maximum sortedrecs respectively sim ilarly our first proposal for joulesort specified measuring energy and used sortedrecs joule as the ratio to maximize there are two problems with this approach which are illustrated by figure this figure shows the srecs j ra tio for varying input sizes n with our winning joulesort system we see that the ratio varies considerably with n there are two distinct regions records which 08 records sorted figure this figure shows the best measured en ergy efficiency of our winning system at vary ing input sizes corresponds to pass sorts and records which corresponds to pass sorts to get the best performance for pass sorts we stripe the input and output across disks using and use disks for temporary runs for pass sorts we stripe the input and output across disks see section for more system details with a fixed time budget approach the goals of our benchmark can be undermined in the following ways for both one and two pass sorts sort progress incentive first in any time budget ap proach there is no way to enforce continual progress sys tems will continue sorting only if the marginal cost of sort ing an additional record is lower than the cost of sleeping for the remaining time this tradeoff becomes problematic when an additional record moves the sort from pass to pass in the pass region of figure the sort is i o lim ited so it does not run twice as fast as a pass sort it goes fast enough however to provide about better efficiency than pass sorts if the system was designed to have a suf ficiently low sleep state power then with a minute budget the best approach would be to sort records which takes sec and sleep for the remaining sec re sulting in a best srecs j thus for some systems a fixed time budget defaults into assessing efficiency when no work is done violating our criteria sort complexity second even in the pass region total energy is a complex function of many performance factors that vary with n total i o memory accesses comparisons cpu utilization and effective parallelism figure shows that once the sort becomes cpu bound records the srecs j ratio trends slowly downward because total en ergy increases superlinearly with n the ratio for the largest sort is lower than the peak this decrease is in part because sorting work grows as o n lg n due to compar isons and the o notation hides constants and lower order overheads this effect implies that the metric is biased to ward systems that sort fewer records in the allotted time that is even if two fully utilized systems a and b have same true energy efficiency and a can sort twice as many records as b in a minute the sortedrecs joule ratio will favor b note since this effect is small our relative com parisons and conclusions in section remain valid our choice fixed input size the final option that we considered and settled upon was to fix the number of records sorted as in the terabyte sort benchmark and use total energy as the metric to min imize for the same fairness issues as in the fixed energy case we decided to have three scales for the input size and records similar to tpc h and declare win ners in each category for consistency henceforth we use mb gb and tb for and bytes respectively for a fixed input size minimum energy and maximum sorte drecs joule are equivalent metrics in this paper we prefer the latter because like an automobile mileage rating it highlights energy efficiency more clearly this approach has advantages and drawbacks but offers the best compromise given our criteria these scales cover a large spectrum and naturally divide the systems into classes we expect laptops desktops and servers moreover since energy is a product of power and time a fixed work ap proach is the simplest formulation that provides an incentive to optimize power consumption and performance both are important concerns for current computer systems one disadvantage is that as technologies improve scales must be added at the higher end and may need to be dep recated at the lower end for example if the performance of joulesort winners improves at the rate of moore law year a system which sorts a in sec to day would only take sec in years once all relevant systems require only a few seconds for a scale that scale becomes obsolete since even the best performing sorts are not improving with moore law we expect these scales to be relevant for at least years finally because compar ison across scales is misleading our approach is not fully history proof categories as with the other sort benchmarks we pro pose two categories for joulesort daytona for commer cially supported sorts and indy for no holds barred im plementations since daytona sorts are commercially sup ported the hardware components must be off the shelf and unmodified and run a commercially supported os as with the other sort benchmarks we expect entrants to report the cost of the system measuring energy there are a number of issues surrounding the proper ac counting of energy use specific proposals in the power management community for measuring energy are being de bated and are still untested in the large once these are agreed upon we plan to adopt the relevant portions for this benchmark as a start we propose guidelines for three areas the boundaries of the system to be measured envi ronmental constraints and energy measurement system boundaries our aim is to account for all en ergy consumed to power the physical system executing the sort all power is measured from the wall and includes any conversion losses from power supplies for both ac and dc systems power supplies are a critical component in deliv ering power and in the past have been notoriously ineffi cient some dc systems especially mobile devices can run from batteries and those batteries must eventually be recharged which also incurs conversion loss while the loss from recharging may be different from the loss from table the unbalanced systems measured in exploring energy efficiency tradeoffs for sort the adapter that powers a device directly for simplicity we allow measurements that include only adapters all hardware components used to sort the input records from start to finish idle or otherwise must be included in the energy measurement if some component is unused but cannot be powered down or physically separated from adja cent participating components then its power use must be included if there is any potential energy stored within the system e g in batteries the net change in potential energy must be no greater than zero joules with confidence or it must be included within the energy measurement environment the energy costs of cooling are important and cooling systems are variegated and operate at many lev els in a typical data center there are air conditioners blow ers and recirculators to direct and move air among aisles and heat sinks and fans to distribute and extract heat away from system components given recent trends in energy density future systems may even have liquid cooling it is diffi cult to incorporate anticipate and enforce rules for all such costs in a system level benchmark for simplicity we only include a part of this cost one that is easily measurable and associated with the system being measured we specify that a temperature between c should be maintained at the system inlets or within foot of the system if no inlet exists energy used by devices physically attached to the sorting hardware that remove heat to maintain this temper ature e g fans must be included energy use total energy is the product of average power over the sort execution and wall clock time as with the other sort benchmarks wall clock time is measured using an external software timer the easiest method to measure power for most systems will be to insert a digital power me ter between the system and the wall we intend to leverage the minimum power meter requirements from the spec power draft in particular the meter must report real power instead of apparent power since real power reflects the true energy consumed and charged for by utilities while we do not penalize for poor power factors a power factor measured anytime during the sort run should be re ported finally since energy measurements are often noisy a minimum of three consecutive energy readings must be re ported these will be averaged and the system with mean energy lower than all others including previous years with confidence will be declared the winner summary in summary the joulesort benchmark is as follows sort a fixed number of randomly permuted byte records with byte keys the sort must start with input in a file on non volatile store and finish with output in a file on non volatile store there are three scale categories for joulesort and records the total true energy consumed by the entire physical system executing the sort while maintaining an ambi ent temperature between should be reported the winner in each category is the system with the maximum sortedrecs joule i e minimum energy joulesort is a reasonable choice among many possible options for an energy oriented benchmark it is an i o centric system level energy efficiency benchmark that in corporates performance power and some cooling costs it is balanced portable representative and simple we can use it to compare different existing systems to evaluate the energy efficiency balance of components within a given system and to evaluate different algorithms that use these components these features allow us to chart past trends in energy efficiency and hopefully will help predict future trends a look at different systems in this section we measure the energy and performance of a sort workload on both unbalanced and balanced sort ing systems we analyze a variety of systems from laptops to servers that were readily available in our lab for the unbalanced systems the goal of these experiments is not to painstakingly tune these configurations rather we present results to explore the system hardware space with respect to power consumption and energy efficiency for sort after looking at unbalanced systems we present a balanced file server that is our default winner we use insights from these experiments to justify the approach for constructing our joulesort winner see section unbalanced systems configurations table shows the details of the unbal anced systems we evaluated spanning a reasonable spec trum of power consumption in servers and personal com puters we include a server an older low power blade and an a modern laptop we chose the laptop be cause it is designed for whole system energy conservation and and for comparison we turned off the laptop display for these experiments for we only used blade in an enclosure that holds and as per our rules report the power of the entire system sort workload we use ordinal technology commercial nsort software which was the terabyte sort daytona winner it uses asynchronous i o to overlap reading writ ing and sorting operations it performs both one and two pass sorts we tuned nsort parameters to get the best performing sort for each platform unless otherwise stated we use the radix in memory sort option table energy efficiency of unbalanced systems power measurement to measure the full system ac power consumption we used a digital power meter inter posed between the system and the wall outlet we sampled this power at a rate of once per second the meter used was brand electronics model which reports true power with accuracy in this paper we always re port the average power over several trials and the standard deviation in the average power results the joulesort results for our unbalanced systems are shown in table since disk space on these systems was limited we chose to run the benchmark at and a smaller dataset to allow fair comparison we see that the server is the fastest but the laptop is most energy efficient system uses over more power than but only provides better performance although disks can provide more sequential bandwidth was limited by its smartarray i o controller to mb in each pass sys tem the blade is not as bad as the results show because blade enclosures are most efficient only when fully popu lated the enclosure power without any blades was when we subtract this from the total power we get an upper bound of srecs j for for all these systems the standard deviation of total power during sort was at most the power factor pf for and were and respectively the cpus for all three systems were highly underutilized in particular attains an energy efficiency similar to that of last year estimated winner gputerasort by barely us ing its cores since the cpu is usually the highest power component these results suggest that building a system with more i o to complement the available processing capacity should provide better energy efficiencies balanced server in this section we present a balanced system that usually functions as a fileserver in our lab table shows the com ponents used during the sort and coarse breakdowns of total system power the main system is an hp proliant that includes a motherboard cpu low power laptop disk and a high throughput sas i o controller for the storage we use two disk trays one that holds the input and output files and the other which holds the temp disks each tray has disks and can hold a maximum of the disk trays and main system all have dual power supplies but for these experiments we powered them through one each for all our experiments the system has bit ubuntu linux and the xfs filesystem installed table shows that for a server of this kind the disks and their enclosures consume roughly the same power as the rest of the system when a tray is fully populated with disks table a balanced fileserver the idle power is w and with disks the idle power is w there clearly are inefficiencies when the tray is under utilized to estimate the power of the dimms we added two dimms and measured the system power with and without the dimms we found that the dimms use both during sort and at idle for this system we found the most energy efficient config uration by experimenting with a dataset by varying the number of disks used we found that even with the inef ficiencies the best performing setup uses disks split across two trays this effect happens because the i o con troller offers better bandwidth when data is shipped across its two channels a sort provides on av erage for each phase across the trays while only when the all disks are within a tray the average power of the system with only one tray is and with two trays is as a result with two trays the system attains a best srecs j instead of srecs j with one tray the tray disk setup is also when the sort becomes cpu bound when we reduce the system to disks the i o performance and cpu utilization drop and when we increase the system to disks the performance and uti lization remain the same in both cases total energy is higher than the disk point so this balanced cpu bound configuration is also the most energy efficient table shows the performance and energy characteristics of the disk setup for sorts this system takes nearly more power than but provides over the through put this system srecs j ratio beats the laptop and last year estimated winner even with a larger input ex periments similar to those for the dataset show that this setup provides just enough i o to keep the two cores fully utilized on both passes and uses the minimum energy for the scale thus at all scales the most energy efficient and best performing configuration for this system is when sort is cpu bound and balanced table winning system summary in conclusion from experimenting with these systems we learned cpu is wasted in unbalanced systems the most energy efficient server configuration is when the sys tem is cpu bound an unbalanced laptop is almost as energy efficient as a balanced server moreover current lap top drives use vs w less power than our server sata drives while offering around vs mb the bandwidth these observations suggest a reasonable ap proach for building the most energy efficient sorting system is to use mobile class cpus and disks and connect them via a high speed i o interconnect joulesort winner in this section we first describe our winning joulesort configuration and report its performance we then study this system through experiments that elucidate power and performance characteristics of this system winning configuration given limited time and budget our goal was to convinc ingly overtake the previous estimated winner rather than to try numerous combinations and construct an absolute op timal system as as result we decided to build a daytona system and solely use nsort as the software our design strategy for an energy efficient sort was to build a balanced sorting system out of low power components after esti mating the sorting efficiency of potential systems among a limited combination of modern low power processors and laptop disks we assembled the configuration in table this system uses a modern low power cpu with fre quency states and a tdp of for the highest state we use a motherboard that supports both a mobile cpu and multiple disk controllers to keep the cores busy few such boards exist because they target a niche market this one includes two pci e slots one channel and one channel to fill those slots we use controllers that hold and sata drives respectively finally our configuration uses low power laptop drives which support the sata in terface they offer an average ms seek time and their measured sequential bandwidth through xfs is around mb hitachi specs list an average for read and write and for active idle we use two dimms whose specs report for each finally the case comes with a power supply our optimal configuration uses disks because the pci e cards hold disks maximum and the i o performance of the motherboard controller with more than disk is poor the input and output files are striped across a disk array configured via and the remaining disks are inde pendent for the temporary runs for all experiments we use linux kernel and the xfs filesystem unless otherwise stated in the idle state at the lowest cpu frequency we measured w for this system table shows the performance of the system which at tains srecs j when averaged over consecutive runs the pure performance statistics are reported by nsort we configure it to use radix sort as its in memory sort algo rithm and use transfer sizes of for the input output array and for the temporary storage our system is faster than gputerasort and consumes an estimated less power the power use during sort is more than idle in the output pass the cpu is underutilized see ta ble max for cores and the bandwidth is lower than in the input pass because the output pass requires ran dom i os we pin the cpu to mhz which section shows is the most energy efficient frequency for the sort varying system size in these experiments we vary the system size disks and controllers and observe our system pure performance cost efficiency and energy efficiency we investigate these met rics using a dataset for the first two metrics we set the cpu to its highest frequency and report the metrics for the most cost effective and best performing configurations at each step we start with disks attached to the cheaper disk controller and at each step use the minimum cost hardware to support an additional disk thus we switch to the disk controller for configurations with disks and use both controllers combined for disks finally we add a disk directly to the motherboard for the disk con figuration figure shows the performance records sec and cost efficiency with increasing system size the disk config uration is both the best performing and most cost efficient point each additional disk on average increases system cost by about and improves performance by on average these marginal changes vary they are larger for small sys tem size and smaller for larger system sizes the disk point drops in cost efficiency because it includes the expen sive disk controller without a commensurate performance increase although the motherboard and controllers limit the system to disks we speculate that additional disks would not help since the first pass of the sort is cpu bound next we look at how energy efficiency varies with with system size at each step we add the minimum energy hard ware to support the added disk and report the most energy efficient setup we set the cpu frequency to at all points to get the best energy efficiency see section for convenience we had one extra os disk on the mother board from which we boot and which was unused in the sort for all but the last point the power measurements include this disk but this power is negligible at idle table performance of winning joulesort systems disks used disks used figure shows how performance price and perfor mance varies with system size sort idle disks used figure shows how power varies with system size figure shows idle power at the lowest frequency state versus average power during sort at mhz for the same system configurations with the system at idle and only the motherboard disk installed our measurements show that the disk controller uses and the disk one uses thus for points between disks we use only the disk controller between we use the only disk controller and for or more we use both figure shows jumps at these transitions the idle line indicates adding a disk increases power by during sorting adding a disk increases total power on average at sizes fewer than disks and on average for more than disks these increases reflect end to end utilization of the cpu disk controllers etc figure shows the energy efficiency with increasing num ber of disks used in the sort the curve is similar to the price performance curve in figure the average increase figure shows how energy efficiency varies with system size in energy at each step is while the average increase in performance is about the disk point again is a local minimum because it incurs the power of the larger controller without enough disks to take advantage of it the sort is cpu bound in the most energy efficient configuration there are two main points to take away from these exper iments first the similar shapes of these curves reflect that the base dollar and energy costs of the system are high com pared to the marginal dollar and energy cost of disks if we used server class disks that are similar in cost but consume the power of mobile disks we would see different cost and energy efficiency curves second for the components we chose the best performing most cost efficient and most energy efficient configurations are identical modulo the cpu frequency moreover in this best configuration the system is balanced with just enough i o bandwith to keep the cpu fully utilized for the first pass software matters next we vary the filesystem and in memory sort algo rithm to see how they affect energy efficiency the winning configuration uses the xfs filesystem and a radix sort figure examines the effect of changing the filesystem to reiserfs and the sort algorithm to merge sort at different cpu frequencies for a dataset as expected power consumption steadily increases with frequency in all cases the power consumptions of xfs with radix sort and merge sort are similar at all frequen cies reiserfs however consumes less power and also is less energy efficient all three configurations show improved energy efficiency from mhz to mhz and then level off or decrease this result indicates that the sorts are cpu bound at the lower frequencies reiserfs shows a im provement in performance between the lowest and highest on demand cpu freq mhz 6000 ing this sort cpu utilization was an average per core so we assign to the cpu subsystem similarly we discount the copying test for its cpu utilization and estimate that the i o subsystem uses these estimates combine to and almost match error the measured increase due to the disk sort thus our tests imply that about of the power increase during sort is from the i o subsystem and from the cpu subsystem we found similar proportions at smaller system sizes vary dimms and power supply since nsort uses only a fraction of the available mem ory for these experiments we ran experiments with only dimm power use and execution time were statistically in distinguishable from the dimm case during sort power figure shows how average power and energy ef ficiency vary with cpu frequency for a sort frequencies while xfs radix improves only and xfs merge improves only by reiserfs has worse energy efficiency mainly because it provides less sequential bandwidth and thus worse perfor mance than xfs although we tuned each configuration this result may be an artifact of our setup and not an in herent flaw of reiserfs similarly the merge sort also gives worse energy efficiency than radix entirely because its per formance is worse the graph also shows the power and energy efficiency of the linux on demand cpu frequency scaling policy which is within of the lowest execution time and of the lowest power for all three configurations for reiserfs the on demand policy offers the same efficiency as the best con figuration in summary these experiments show that the algorithms and underlying software used for sort affect en ergy efficiency mainly through performance approximate cpu vs i o breakdown we performed some micro benchmarks exercising the i o subsystem disks plus controllers and the cpu subsystem cpu plus memory separately to determine how much each contributes to the increase in power during sort the system at the disk point consumes more during sort than when the system is at idle with mhz cpu frequency this increase is nearly of the total power during sort our benchmarks suggest that the i o subsystem consumes a much greater fraction of this power increase than the cpu subsystem we first performed a test to help approximate the contri bution from the i o subsystem in this test we copied data from a disk array to another disk array which had the same average disk bandwidth as the disk sort we found that the system power increased by during this test the cpu utilization was out of max for cores next we performed an experiment to approximate the con tribution from the cpu subsystem we put a small input file on a ram disk and repeatedly sorted it this test pegged the cpu to utilization and the power increase was using the above values and assuming that cpu subsystem power increases linearly with cpu utilization we estimate its contribution during the disk sort as follows dur use is also within measurement error at idle we replaced the power supply with a one and found that the power consumption during sort and idle increased by this suggests that at load or less efficiencies of the two power supplies are similar note the power factors for the laptops and desktop sys tems in this paper including our winner are well below low power factors are problematic in data cen ters because power delivery mechanisms need to be over provisioned to carry additional current for loads with low power factors utilities often charge extra for this pro visioning similar to server class systems power supplies will need to provide power factor correction for systems like our winner to become a reality in data centers summary we describe the daytona joulesort system that is over as energy efficient as last year pennysort win ner the gputerasort for this system we show the most energy efficient sorting configuration is when the sort is cpu bound and balanced this configuration is also the best performing and most cost efficient it will be interesting to see how long this relationship holds we see that filesystem and in memory sort choice mainly affect energy efficiency through performance rather than power for this system in this paper we focused on building a balanced system with low power off the shelf components targeted for the scale unfortunately because of hardware limita tions and market availability we could not easily scale this system to the category in the future we expect sys tems in other classes to win the and categories but for completeness we report in table the best configu rations we encountered for those categories related work our related work falls into three categories we first dis cuss the history of sort benchmarks and large scale sorting techniques next we cover the previous work on metrics for evaluating energy efficiency finally we briefly discuss work on techniques for reducing energy consumption in systems sort benchmarks and techniques the original datamation sort benchmark was a pure per formance benchmark that measured the time to sort a mil lion records in the developers of alphasort recognized that the benchmark was losing its relevance be cause startup and shutdown would eventually dominate the time to sort such a small number of records they there fore proposed two variants minutesort and pennysort hop ing they would remain relevant as technology improved at the pace of moore law recognizing that pennysort was biased against large configurations by allowing too small a time budget researchers then proposed the performance price sort which is tied to the minutesort time budget the time budget approach undermines the goals of joulesort since the original datamation sort benchmark there have been many different implementations of external sort on a variety of platforms from desktops to supercomputers the sort benchmark website maintained by jim gray lists the winners and briefly surveys past trends energy benchmarks several different metrics have been proposed for evalu ating the energy efficiency of computer systems in gonzalez et al proposed the energy delay product as the metric of energy efficient microprocessor design alter natively the metric of performance per watt is also widely used to evaluate processors energy efficiency this met ric emphasizes performance less than the energy delay prod uct which is equivalent to performance squared per watt energy efficiency metrics tailored to data centers have also been proposed sun space watts and performance metric swap considers the rack space taken up by a hardware configuration along with its power and performance in an effort to promote data center compaction metrics based on exergy which is the energy converted into less efficient forms such as heat take into account every aspect of the data center from processors to the cooling infrastructure however these metrics are not applicable to the entire range of systems we want to evaluate with joulesort comparatively little work has been done on workloads for energy efficiency benchmarks in the embedded domain the eembc energybench benchmarks provide a physical in frastructure to evaluate a single processor energy efficiency on any of eembc existing mobile benchmark suites in the enterprise domain the spec power and performance committee is currently developing an energy benchmark suite for servers and the united states environmental pro tection agency energystar program is developing a way to rate the energy efficiency of servers and data centers energy efficiency there is a large body of prior work on energy efficiency for example at the component and system levels many studies have been devoted to algorithms for dynamically exploiting different power states in processors memory and disks in order to promote energy ef ficiency in clusters and data centers research has focused on energy efficient workload distribution and power budget ing e g other studies have focused at the application level including energy aware user interfaces and fidelity aware energy management conclusions in this section we summarize the limitations of joulesort speculate on future energy efficient systems and wrap up limitations joulesort does not address all possible energy related con cerns since joulesort focuses on data management tasks it misses some important energy relevant components for mul timedia applications joulesort omits displays which are an important component of total power for mobile devices gpus also consume significant p ower and a re u biquitous in desktop systems although we can use gpus to sort our benchmark does not require their use as a result it loses relevance for applications where these components are es sential there are other energy related concerns in data centers beyond system power that were difficult to incorporate at a high level cooling requires lowering ambient temper ature and extracting heat away from systems joule sort accounts only for part of the second delivering power to systems incurs losses at the rack and data center level which are ignored in joulesort moreover many systems are used as an ensemble in data centers with sophisti cated scheduling techniques to trade performance for lower energy among systems rather than at the component level as a system level benchmark joulesort may not identify the benefits o f uch methods greener systems we speculate on two emerging technologies that may im prove the energy efficiency of systems for the scale flash m emory a ppears to b e a p romising torage technology driven by the mobile device market per byte it is about cheaper than dram and provides sequential read and write bandwidth close to that of disks more importantly random read i os with flash a re f aster t han d isk and flash consumes less power than d isks the random reads allow interesting modifications t o t raditional pass sorting algorithms to date the largest cards at reasonable cost are we anticipate a system such as a laptop or low power embedded device that can leverage multiple flash devices as the next winner for the larger scales an intriguing option is a hybrid sys tem using a low power cpu laptop disks and a gpu the gputerasort has shown that gpus can provide much bet ter in memory sorting bandwidth than cpus using a motherboard that supports more i o controllers and a gpu we could scale our system to use more disks an interest ing question is whether these performance benefits might be offset by the recent trend in gpus to consume more power closing this paper proposes joulesort a simple balanced energy efficiency benchmark we present a complete benchmark a workload metric and guidelines and justify our choices we also present a winner that is over as efficient as last year estimated winner today this system is hard to find p re assembled i t c onsists o f a c ommodity mobile class cpu and laptop disks connected through server class pci e i o cards the details of joulesort already have undergone signifi cant changes since its inception since joulesort has not yet been tried in the wild we fully expect further revisions and fine tuning t o k eep i t f air a nd r elevant nevertheless we look forward to its use in guiding energy efficiency opti mizations in future systems speed scaling of processes with arbitrary speedup curves on a multiprocessor ho leung chan jeff edmonds kirk pruhs abstract we consider the setting of a multiprocessor where the speeds of the m proces sors can be individually scaled jobs arrive over time and have varying degrees of parallelizability a nonclairvoyant scheduler must assign the processes to processors and scale the speeds of the processors we consider the objective of energy plus flow time we assume that a processor running at speed uses power sα for some constant α for processes that may have side effects or that are not checkpointable we show an ω m α α bound on the competitive ratio of any randomized algorithm for checkpointable processes without side effects we give an o log m competitive al gorithm thus for processes that may have side effects or that are not checkpointable the achievable competitive ratio grows quickly with the number of processors but for checkpointable processes without side effects the achievable competitive ratio grows slowly with the number of processors we then show a lower bound of ω α m on the competitive ratio of any randomized algorithm for checkpointable processes without side effects introduction due to the power related issues of energy and temperature major chip manufacturers such as intel amd and ibm now produce chips with multiple cores processors and with dy namically scalable speeds and produce associated software such as intel speedstep and amd powernow that enables an operating system to manage power by scaling processor speed currently most multiprocessor chips have only a handful of processors but chip de signers are agreed upon the fact that chips with hundreds to thousands of processors will dominate the market in the next decade the founder of chip maker tilera asserted that a corollary to moore law will be that the number of cores processors will double every months according to the well known cube root rule a cmos based processor running at speed will have a dynamic power p of approximately in the algorithmic literature this is the university of hong kong york university supported in part by nserc canada university of pittsburgh supported in part by an ibm faculty award and by nsf grants cns ccf iis and ccf usually generalized to p sα thus in principle p processors running at speed p could do the work of one processor running at speed but at pα of the power but in spite of this chip makers waited until the power costs became prohibitive before switching to multiprocessor chips because of the technical difficulties in getting p speed p processors to come close to doing the work of one speed processor this is particularly true when one has many processors and few processes where these processes have widely varying degrees of parallelizability that is some processes may be considerably sped up when simultaneously run on multiple processors while some processes may not be sped up at all this could be because the underlying algorithm is inherently sequential in nature or because the process was not coded in a way to make it easily parallelizable to investigate this issue we adopt the following general model of parallelizability used in each process consists of a sequence of phases each phase consists of a positive real number that denotes the amount of work in that phase and a speedup function that specifies the rate at which work is processed in this phase as a function of the number of processors executing the process the speedup functions may be arbitrary other than we assume that they are nondecreasing a process doesn t run slower if it is given more processors and sublinear a process satisfies brent theorem that is increasing the number of processors doesn t increase the efficiency of computation the operating system needs a process assignment policy for determining at each time which processors if any a particular process is assigned to we assume that a process may be assigned to multiple processors in tandem with this the operating system will also need a speed scaling policy for setting the speed of each processor in order to be implementable in a real system the speed scaling and process assignment policies must be online since the system will not in general know about processes arriving in the future further to be implementable in a generic operating system these policies must be nonclairvoyant since in general the operating system does not know the size work of each process when the process is released to the operating system nor the degree to which that process is parallelizable so a nonclairvoyant algorithm only knows when processes have been released and finished in the past and which processes have been run on each processor at each time in the past the operating system has competing dual objectives as it both wants to optimize some schedule quality of service objective as well as some power related objective in this pa per we will consider the formal objective of minimizing a linear combination of total re sponse flow time the schedule objective and total energy used the power objective in the conclusion we will discuss the relationship between this energy objective and a temper ature objective this objective of flow plus energy has a natural interpretation suppose that the user specifies how much improvement in flow call this amount ρ is necessary to justify spending one unit of energy for example the user might specify that he is willing to spend erg of energy from the battery for a decrease of micro seconds in flow then the optimal schedule from this user perspective is the schedule that optimizes ρ times the energy used plus the total flow by changing the units of either energy or time one may assume without loss of generality that ρ so the problem we want to address here is how to design a nonclairvoyant process as signment policy and a speed scaling policy that will be competitive for the objective of flow plus energy the case of a single processor was considered in in the single processor case the parallelizability of the processes is not an issue if all the processes arrive at time then in the optimal schedule the power at time t is θ nt where nt is the number of active processes at time t the algorithm considered in runs at a speed of δ α for some constant δ the process assignment algorithm considered in is latest arrival processor sharing laps laps was proposed in in the context of running processes with arbitrary speedup functions on fixed speed processors and it was shown to be scalable i e ǫ speed o competitive for the objective of total flow time in this setting laps is parameterized by a constant β and shares the processing power evenly among the βnt most recently arriving processes note that the speed scaling policy and laps are both nonclairvoyant showed that by picking δ and β appropriately the resulting algorithm is α competitive for the objective of flow plus energy on a single speed scalable processor our results here we consider extending the results in to the setting of a multiprocessor with m processors it is straight forward to note that if all of the work is parallelizable then the multiprocessor setting is essentially equivalent to the uniprocessor setting to gain some intuition of the difficulty that varying speedup functions pose let us first consider an instance of one process that may either be sequential or parallelizable if an algorithm runs this process on few of the processors then the algorithm competitive ratio will be bad if the process is parallelizable and the optimal schedule runs the process on all of the processors note that if the algorithm wanted to be competitive on flow time it would have to run too fast to be competitive on energy if an algorithm runs this process on many of the processors then the algorithm competitive ratio will be bad if the process is sequential and the optimal schedule runs the process on few processors if the algorithm wanted to be competitive on energy it would have to run too slow to be competitive on flow time formalizing this argument we show in section a lower bound of ω m α α on the competitive ratio of any randomized nonclairvoyant algorithm against an oblivious adversary with an additional assumption that we will now discuss at first glance such a strong lower bound for such an easy instance might lead one to conclude that there is no way that the scheduler can be expected to guarantee reasonably competitive schedules but on further reflection one realizes that an underlying assumption in this lower bound is that only one copy of a process can be run if a process does not have side effects that is if the process doesn t change effect anything external to itself then this assumption is not generally valid one could run multiple copies of a process simultaneously with each copy being run on a different number of processors and halt computation when the first copy finishes for example in the instance in the previous paragraph one could be o competitive if the process didn t have side effects by running one copy on a single processor and running one copy on the rest of the processors generalizing this approach one can obtain a o log m competitive algorithm for instances consisting of processes that have no side effects and where the speed up function doesn t change unfortunately we show in section that such a result can not be obtained if processes can have multiple phases with different speed up functions we accomplish this by showing that the competitive ratio of any randomized nonclairvoyant algorithm that runs multiple independent copies of a process against an oblivious adversary is ω mω α contemplating this second lower bound it suggests that to be reasonably competitive the algorithm must be able to process work on all copies of a job at the maximum rate of work processing on any copy if a processes had small state so that the overhead of checkpointing isn t prohibitive one might reasonably approximate this by checkpointing saving the state of each copy periodically and then restarting each copy from the point of execution of the copy that made the most progress in section we formalize this intuition we give a process assignment algorithm multilaps which is a modification of laps we show that by combining multilaps with the natural speed scaling algorithm one obtains an o log m competitive algorithm if all copies process work at the rate of the fastest copy there are two steps in the analysis of multilaps the first step is to show that there is a worst case instance where every speedup function is parallel up to some number of processors and then is constant this shows that the worst case speedup functions for speed scalable processors are more varied than for fixed speed processors where it is sufficient to restrict attention to only parallelizable and sequential speedup functions the second step in the analysis of multilaps is a reduction to essentially the analysis of laps in a uniprocessor setting technically we need to analyze laps when some work is sequential that is it has the special property that it is processed at unit rate independent of the speed of the processor we then discuss how to generalize the analysis of laps in to allow sequential work using techniques from in section we then show a lower bound of ω α m on the competitive ratio of any nonclairvoyant randomized algorithm against an oblivious adversary for checkpointable processes without side effects in fact this lower holds even if the rate that a process is processed is the sum not the maximum of rate of the various copies thus in summary for processes that may have side effects or that are not checkpointable the achievable competitive ratio grows quickly with the number of processors but for check pointable processes without side effects the achievable competitive ratio grows slowly with the number of processors this shows the importance of being able to efficiently checkpoint multiple copies of a process in a setting of processes with varying degrees of parallelizability and individually speed scalable multiprocessors related results we start with some results in the literature about scheduling with the objective of total flow time on a single fixed speed processor it is well known that the online clairvoyant algorithm shortest remaining processing time srpt is optimal the competitive ratio of any deterministic nonclairvoyant algorithm is ω and the competitive ratio of every randomized algorithm against an oblivious adversary is ω log n a randomized version of the multi level feedback queue algorithm is o log n competitive the nonclair voyant algorithm shortest elapsed time first setf is scalable that is it is ǫ speed o competitive for any arbitrarily small but fixed ǫ setf shares the processor equally among all processes that have been run the least we now consider scheduling processes with arbitrary speedup functions on fixed speed processors for the objective of total flow time the algorithm round robin rr also called equipartition and processor sharing that shares the processors equally among all processes is ǫ speed o competitive as mentioned before laps is scalable we now consider speed scaling algorithms on a single processor for the objective of flow plus energy give efficient offline algorithms we now describe the results for online clairvoyant algorithms this setting was studied in a sequence of papers which culminated in the following result the scheduling algorithm that uses shortest remaining processing time srpt for process assignment and power equal to one more than the number of active processes for speed scaling is ǫ competitive for the objective of total flow plus energy on arbitrary work unit weight processes even if the power function is arbitrary so clairvoyant algorithms can be o competitive independent of the power function showed that nonclairvoyant algorithms can not be o competitive if the power function is growing too quickly the case of weighted flow time has also been studied the scheduling algorithm that uses highest density first hdf for process assignment and power equal to the fractional weight of the active processes for speed scaling is ǫ competitive for the objective of fractional weighted flow plus energy on arbitrary work arbitrary weight processes an o competitive algorithm for weighted flow plus energy can then be obtained using the known resource augmentation analysis of hdf extend some of the results for the unbounded speed model to a model where there is an upper bound on the speed of a processor there are many related scheduling problems with other objectives and or other assump tions about the processors and instance surveys can be found in formal problem definition and notations an instance consists of a collection j jn where job ji has a release arrival time ri and a sequence of phases jqi each phase is an ordered pair wq γq where i i i i i q is a positive real number that denotes the amount of work in the phase and γq is a function called the speedup function that maps a nonnegative real number to a nonnegative real number γq p represents the rate at which work is processed for phase q of job i when one copy of the job is run on p processors running at speed if these processors are running at speed then work is processed at a rate of sγq p a schedule specifies for each time and for each copy of a job a nonnegative real number specifying the number of processors assigned to the copy of the job and a nonnegative real speed we thus assume that if several processors are working on the same instance copy of a job then they must all run at the same speed but different copies can run at different speeds the number of processors assigned at any time can be at most m the number of processors note that formally a schedule does not specify an assignment of copies of jobs to processors a nonclairvoyant algorithm only knows when processes have been released and finished in the past and which processes have been run on each processor each time in the past in particular a nonclairvoyant algorithm does not know wq nor the current phase q nor the speedup function γq in this paper we consider several different models depending on how the processing on different copies interact assume multiple copies of job i are run with the speed and number of processors assigned to the k th copy being sk and pk in the independent processing model if copy k is running in a phase with speedup function γ then work is processed on this copy at rate skγ pk independent of the rate of processing on the other copies in the maximum processing model if each copy of job ji is in a phase with speedup function γ then each copy processes work at a rate of maxk skγ pk in the sum processing model if each copy of job ji is in a phase with speedup function γ then each copy processes work at a rate of k skγ pk note that as a consequence of these definitions in the maximum processing model and in the sum processing model it is the case that each copy of each job is always at the same point in its execution the completion time of a job ji denoted ci is the first point of time when all the work on some copy of the job has been processed note that in the language of scheduling we are assuming that preemption is allowed that is a job maybe be suspended and later restarted from the point of suspension a job is said to be active at time t if it has been released but has not completed i e ri t ci the response flow time of job ji is ci ri which is the length of the time interval during which the job is active let nt be the number of active jobs at time t another formulation of total flow time is ntdt when running at speed a processor consumes p sα units of energy per unit time where α is some fixed constant we call p the power function a phase of a job is parallelizable if its speedup function is γ p p increasing the number of processors allocated to a parallelizable phase by a factor of increases the rate of processing by a factor of a phase of a job is parallel up to q processors if γ p p for p q and γ p q for p q a speedup function γ is nondecreasing if and only if γ γ whenever a speedup function γ is sublinear if and only if γ γ whenever we assume all speedup functions γ in the input instance are nondecreasing and sublinear we further assume that all speedup functions satisfy γ p p for p this natural assumption means that when a job ji is assigned to a single processor and shares this processor with other jobs the rate that ji is processed is the fraction of the processor that ji receives times the speed of the processor let a be an algorithm and j an instance we denote the schedule output by a on j as a j we let fa j and ea j denote the total flow time and energy incurred in a j let costa j fa j ea j denote the cost we will use m as a short hand for multilaps let opt be the optimal algorithm that always minimizes total flow time plus energy a randomized algorithm a is c competitive or has competitive ratio c if for all instances j e costa j c costopt j lower bounds for single copy and non checkpointing algorithms in this section we show that the competitive ratio must grow quickly with the number of processors if only one copy of each job can be running lemma or if multiple copies are allowed but no checkpointing is allowed lemma we first start with a couple basic lemmas about optimal schedules that will be useful throughout the paper lemma consider a job with work w and with a single phase with a speedup function that is parallelizable up to q processors assume that the job is run on p q processors then the optimal speed is α for a cost of θ w assume that the job is run on p q processors then the optimal speed is α for a cost of θ α proof first consider that case that p q let be the speed of the processors the flow plus energy is then w psα w w sα this is minimized by setting α ps ps ps for a cost of θ w α p energy is then w psα w this is minimized by setting α giving a cost of qs qs θ α q α p lemma consider a job with work w with a single phase with a speedup function that is parallelizable up to q processors the optimal schedule uses p q processors run at speed α q α for a cost of θ w proof from the proof of lemma we know that if the algorithm allocates p q speed processors to this job the cost is minimized by when for a cost of θ w α this is minimized by making p as big as possible namely p q from the proof of lemma we know that if the algorithm allocates p q speed processors to this job the cost is minimized when giving a cost of θ α q this is minimized by making p α as small as possible namely p q thus in either case the optimal scheduling policy uses p q processors run at speed for a cost of θ w α lemma any randomized nonclairvoyant algorithm that only runs one copy of each job must be ω m α competitive against an oblivious adversary that must specify the input a priori proof applying yao technique we give a probability distribution over the input and show that every deterministic algorithm a will have an expected competitive ratio of ω m α α the instance will be selected uniformly at random from two possibilities the first possible instance consists of one job with α units of parallelizable work the second possible instance will consist of job with one unit of work that is parallel up to one processor by plugging these parameters into lemma one can see that the optimal cost is θ for both instances let p denote the number of processors used by the algorithm a by lemma the cost for the algorithm a is either ω m α or ω α depending on the instance both the maximum and the average of these two costs is minimized by balancing these two costs which is accomplished by setting p α this shows that the competitive ratio is ω m α α lemma in the independent processing model any randomized nonclairvoyant algorithm must be ω m α α competitive against an oblivious adversary that must specify the input a priori proof applying yao technique we give a probability distribution over the inputs with the property that every deterministic algorithm a will have expected competitive ratio ω m α the random instance consists of a single job with an infinitely large num ber of phases each phase will be randomly chosen to be one of the two job instances given in lemma that is each phase will either be parallelizable or parallel up to one processor and the optimal cost for each phase will be θ consider a particular copy of the job run by a because each phase is so small we can assume that the algorithm a allocates a fixed number of processors p running at a fixed speed for the duration of the phase by the proof of lemma no matter what the algorithm does the probability is at least a half that it occurs a cost of ω m α during this phase one can think of the phases as bernoulli trials with outcomes being cost ω m α with probability at least a half and smaller cost with probability at most a half applying a chernoff bound with high probability the algorithm has cost ω m α on nearly half of the stages while the optimal cost on each stage is by a union bound the probability that any copy has average cost per phase much less than ω m α is small analysis of multilaps in this section we assume that multiple copies of a job may be run simultaneously each copy of a job may be assigned a different number of processors but each processor running this copy must be run at the same speed we assume that at each moment in time the rate that work is processed on each copy of a job is the maximum of the rates of the different copies so all copies of a job are always at the same point of execution we give a nonclairvoyant algorithm multilaps for this setting and show that it is o log m competitive for flow time plus energy we now describe the algorithm laps from and the algorithm multilaps that we introduce here we then give some underlying motivation for the design of multilaps algorithm laps let δ and β be real constants at any time t the processor speed is δ na α where na is the number of active jobs at time t the processor processes the βna active jobs with the latest release times by splitting the processing equally among these jobs for our purposes in this paper we will take δ algorithm multilaps let β be a real number that parametrizes multilaps let µ consider any time t let na be the number of active jobs at t each of the βna active jobs with the latest release times will be run at this point in time call these jobs the late jobs for each late job ji a primary copy of ji is run on a group of pa µ m processors where each processor in this group is run at speed sa na α note that the µ m primary copies of the late jobs are equally sharing a µ fraction of the processors furthermore for each late job ji there are log pa secondary copies of ji run the jth j log pa secondary copy of ji is run on a group of processors where each processor in this group is run at speed α intuition behind the design of multilaps let us give a bit of intuition behind the design of multilaps if µ was and no secondary copies were run then multilaps would essentially be adopting the strategy of laps of sharing the processing power evenly among the latest arriving β fraction of the jobs laps is o competitive when all work is parallelizable up to the number of available processors however if a primary copy of a job is run on many processors the online algorithm may be wasting a lot of energy if this work is not highly parallelizable to account for this possibility multilaps runs the primary copy a little faster freeing up some processors to run secondary copies of the job on fewer processors and at a faster speed the number of processors running the secondary copies are geometrically decreasing by a factor of while the speeds are increasing by a factor of α thus each copy of a job is using approximately the same power intuitively one of the copies is running the late job on the right number of processors thus multilaps uses a factor of o log m more energy than optimal because of the log m different equi power copies of the job setting µ guarantees that that multilaps doesn t use more than m processors the rest of this section is devoted to proving the following theorem theorem in the maximum processing model multilaps is o log m competitive for total flow time plus energy overview of the proof of theorem we now give an overview of the structure of our proof of theorem in lemma we show how to reduce the analysis of multilaps on arbitrary instances to the analysis of multilaps on canonical instances we define an instance to be canonical if the speedup function for each job phase is parallel up to the number po of processors that opt uses on that phase and is constant there after the value of po may be different for each phase more specifically we show how to construct a canonical instance j from an arbitrary instance k such that the cost of multilaps on k is identical to the cost of multilaps on j and the optimal cost for j is at most the optimal cost for k we then define a variation of the uniprocessor setting that we call the sequential set ting in the sequential setting a job can have sequential phases which are phases that are processed at a unit rate independent of the computational resources assigned to the job we then show how to reduce the analysis of multilaps on canonical instances to the analysis of laps in the sequential setting more precisely from an arbitrary canonical instance j we show how to create an instance j for the sequential setting we show in lemma that the flow time for multilaps on j is identical to the flow time of laps on j and the energy used by multilaps on j is at most o log m times the energy used by laps on j we then need to relate the optimal schedule for j to the optimal schedule for j to accomplish this we classify each phase of a job in j as either saturated or unsaturated depend ing on the relationship between the speedup function and how many processors multilaps uses for this phase we consider two instances derived from j an instance jsat consisting of only the saturated phases in j and an instance juns consisting of only the unsaturated phases in j we then consider two instances derived from the instance j an instance j consisting of parallel phases in j and an instance j consisting of sequential phases in j the transformation of j to j transforms phases in jsat to phases in j and transforms phases in juns to phases in j it will be clear that the optimal cost for j is at least the optimal cost for jsat plus the optimal cost for juns we then show in lemma that the optimal cost for jsat is at least the optimal cost for j and in lemma that the optimal cost for juns is at least the optimal cost for j we then discuss how to generalize the analysis of laps in using techniques from to show that the cost of laps is at most a constant factor larger than the optimal cost for j plus the optimal cost for j this line of reasoning allows us to prove our theorem as follows costm k costm j o log m costlaps j o log m costopt j costopt j o log m costopt jsat costopt juns o log m costopt j o log m costopt k the first and final equalities follow from lemma the second equality follows from lemma the third equality follows from the analysis of laps in the sequential setting the fourth equality follows from lemma and lemma the fifth equality will be an obvious consequence of the definitions of jsat and juns we now execute the proof strategy that we have just outlined we first show that there is a worst case instance for multilaps that is canonical lemma let k be any input instance there is a canonical instance j such that fm j fm k em j em k and costopt j costopt k proof we construct j by modifying each job in k as follows consider an infinitesimally small phase of a job in k with work w and speedup function γ let po be the number of processors that opt allocates to this phase when scheduling k we modify this phase so that the new speedup function is γ p p γ po for p po and γ p γ po for p po note that multilaps may process this phase in several copies of this job assume that the i th copy is processed by pi processors of speed si due to the modification of speedup function the rate of processing for the i th copy changes from γ pi si to γ pi si if pi po then the rate of processing on the i th copy does not increase since γ is nondecreasing now consider a copy where pi po by the definition of γ the rate of processing γ pi si piγ po si since pi po and since γ is sublinear γ po γ pi plugging this back in we get po po pi that the rate of processing for copy is at most siγ pi so multilaps doesn t finish this phase in the modified instance before it can finish the phase in k we then decrease the work of this phase so that the time when this phase is first completed among all of the copies is identical to when it completes in multilaps k note that by construction the schedule of multilaps on this modified instance is identical to multilaps k while opt may get better performance due to the reduction of work finally we create j by multiplying both the work of this phase and the speedup function by the same factor of po o to make the final speed up function for this phase parallel up to po processors this change does not effect the schedules of either multilaps and opt definition of the sequential setting everything is defined identically as in subsection with the following two exceptions firstly there is only a single processor secondly job phases can be sequential which in the context of this paper means that work in this phase is processed at a rate of independent of the fraction of the processor assigned to the job and the speed of the processor so sequential work is processed at rate even if it is run at a speed much greater than or is not even run at all sequential work doesn t correspond to any realistic situation but is merely mathematical construct required for the proof definition of the transformation of a canonical instance j into the instance j in the sequential setting we transform each job in j into a job in j by modifying each phase of the original job at each point in time consider a phase and the copy in multilaps with the highest processing rate on this phase let γpo be the speedup function of the phase which is parallel up to po processors we say the phase is currently saturated m βna po and unsaturated otherwise note that µ m is the number of processors assigned to the primary copy in multilaps thus a phase is saturated if all copies in multilaps are processing in the parallel range of γpo and unsaturated otherwise consider the case that the phase is saturated the copy with the highest processing rate in multilaps is the one with pa µ m processors of speed sa na α giving a rate βna µ m α of m na α α na we modify this phase to be fully parallelizable and scale βna m βna down the work by a factor of α note that the processing rate of laps is na α so it a will complete the phase using the same time consider the case that the phase is unsaturated let r be the fastest rate that any copy in multilaps is processing work in this phase we modify this phase to be sequential and scale down the work by a factor of r by the definition of sequential the processing rate of laps on this phase is so it will complete the phase using the same time as multilaps we now show that the cost of multilaps on j is at most a log factor more than the cost of laps on j in the sequential setting lemma costm j flaps j o log m elaps j from this we can conclude that costm j o log m costlaps j proof by construction the flow time for multilaps j is identical to the flow time for laps j we now show that the energy used by multilaps j is at most a log factor more than the energy used by laps j the power in multilaps j is the sum of the powers in the m processors note that for each of the βna late jobs multilaps allocates pa processors of speed sa to a primary copy of this job recall that pa µ m and sa na α multilaps also runs log pa βna µ m secondary copies where the i th copy is run on processors of speed α hence the total power for multilaps j is βna µ m βna na µ m α α log pa i α α µ α na βna log pa laps j runs at speed α and hence power n since p m we conclude that e j a a a m o log m elaps j we now want to show a lower bound for opt j to state this lower bound we need to introduce some notation define jsat to be the instance obtained from j by removing all unsaturated phases in each job and directly concatenating the saturated phases define juns to be the instance obtained from j by removing all saturated phases and directly concatenating the unsaturated phases define j to be the instance obtained from j by removing all sequential phases in each job and directly concatenating the parallel phases define j to be the instance obtained from j by removing all parallel phases in each job and directly concatenating the sequential phases note that the transformation from j to j transforms a phase in jsat to a phase in j and transforms a phase in juns to a phase seq obviously opt can only gain by scheduling jsat and juns separately that is costopt j costopt jsat costopt juns we now want to show that costopt j o costopt jsat and costopt j o costopt juns lemma costopt j o costopt jsat proof we construct a schedule opt j from the schedule opt jsat phase by phase each phase in opt j will end no later than the corresponding phase in opt jsat and the schedule for opt j will use less energy than the schedule opt jsat consider a infinitesimal saturated phase in jsat let po and so be the number of processors and speed allocated by opt jsat to this phase by the definition of canonical the phase is parallelizable up to po processors thus opt jsat is processing at a rate of poso define opt j so that it runs at speed αposo on this phase since the transformation scales down the work by a factor of α opt j will complete the phase at the same time as opt jsat now we need to argue that at any point of time the power for opt jsat will be at least the power for opt jpar the power at this time in opt jsat is p j po j so j where the sum is over all jobs j it is processing and po j and so j are the number and speed of the processors allocated to j keeping r j po j so j fixed p is minimized by having all the so j to be the same fixed value so this gives r j po jso som and α p p α αm r m rα by our definition of opt j the power p in opt j can be bounded as follows α p j α m po j so j α α α rα p lemma costopt j o costopt juns proof consider a unsaturated phase in juns that is parallel up to po processors we gra ciously allow opt juns to schedule each phase in juns in isolation of the other phases this only improves opt juns consider a particular phase in juns with a speedup function that is parallel up to po processors and that has work w by lemma the total flow time plus energy incurred for opt juns is θ w opt j will allocate zero processors to the corresponding phase and process the phase at rate since the work in j is sequential hence opt j incurs no energy cost for this phase so to finish the proof we will show that the flow time for this phase in opt j is at most the cost of this phase in opt juns namely θ w recall that in the transformation from j to j this work is scaled down by the fastest rate that this phase is processed by any copy in multilaps consider the copy in multilaps that is processing in the parallel range with the most number of processors i e the copy with processors such that is maximized and at most po since the phase is unsaturated po by the definition of multilaps the processing rate of this copy is at least α α po α α o thus the work in this phase in j and the flow time for this phase in opt j is at most α o one can extend the analysis for laps in the uniprocessor setting to the sequential setting using the techniques used in we refer the reader to for full details and just give the underlying intuition here the analysis uses amortized local competitiveness that is it is shown that at every time plaps nlaps dφ dt c popt nopt where p denotes power n denotes the number of active jobs φ is the potential function and c is the desired competitive ratio so when plaps nlaps is large the processing of laps lowers the potential function φ enough to make the equation true now consider the sequential setting the difficulty that arises is that the processing that laps does on sequential jobs may not lower the potential function however if the number of sequential phases that laps is processing is very small then raising the speed of laps by a small amount will be enough so that the potential function decrease sufficiently quickly due to the processing on the non sequential jobs if the number of sequential phases is large at a particular time then the increase in flow time that laps is experiencing on these jobs is also experienced by the adversary at some point in time this increase in flow time experienced by the adversary pays for the increase in flow time for laps at this point of time note that by definition of laps the power used by laps is comparable to the increase in flow time experience by laps we can thus derive the following theorem theorem costlaps j o costopt j costopt j lower bound for checkpointable multiple copies we show here that even if the rate that the work is processed is the sum of the rate of the copies every randomized algorithm is poly log competitive theorem in the sum processing model the competitive ratio for every randomized nonclairvoyant algorithm is ω α m against an oblivious adversary that must specify the input a priori proof applying yao technique we give a probability distribution over the input and show that every deterministic algorithm a will have an expected competitive ratio of ω α m there are θ log m possible instances each selected with equal probability for each j log m instance jj will consist of one job with work wj α j and speedup function p where γq p is the speed up function that is parallelizable up to q processors by applying lemma opt jj allocates pj processors each of speed sj α resulting in a cost of θ now consider any deterministic nonclairvoyant algorithm a rounding the number of processors a copy is run on to a factor of two doesn t change the objective by more than a constant factor and there is no significant benefit from running two copies on an equal number of processors since the algorithm is nonclairvoyant it will gain no information about the identity of jj until some copy finishes since the power function is convex it is best for the algorithm to run each copy at constant speed thus we can assume that the algorithm runs log m copies of the job with copy i run on processors at at some constant speed si note that the algorithm can set si if it doesn t want to run a copy on that many processors the power of copy i is p pisiα and the total power for a is i i let r i j si denote the rate that the copy i is processing work on job jj because we are assuming that the work completed on a job is the sum of that completed by the groups working on it we have that rj i r i j is the rate that a completes work on job jj and maximize this time we bound this maximum denoted by t as follows min t j tj o log m j tj o log m i r i j wj j i o log m si wj o i j si log m o i si j i wj j i log m wj log m i j i α j j i log m α j o si α j log m i j i j i log m α j o si i log m o log m i α i i α α i p subject to p i pi the sum i pi is maximized by setting each pi to log m giving o p α o p α the total cost for a is fa ea t p t p t log m α ω α m recalling that the optimal cost is o the result follows conclusion in summary we have shown that for jobs that may have side effects or that are not check pointable the achievable competitive ratio grows quickly with the number of processors and for checkpointable jobs without side effects the achievable competitive ratio grows slowly with the number of processors there seem to be several interesting lines of research spawned by these results most obviously the upper and lower bounds on the competitive ratio for checkpointable jobs without side effects are not quite tight it is plausible that one could obtain tight upper and lower bounds by being more careful in the analyses one might also consider the situation where the power objective is temperature it is not clear how to best formalize this problem the most obvious approach is to include a constraint on temperature that is you can not exceed the threshold of the processor if the processor cools according to newton law then the temperature is approximately the maximum energy used over any time interval of a particular length where the length of the interval is determined by the cooling parameter of the device if the intervals are long then the temperature constraint is essentially an energy constraint but optimizing any reasonable scheduling objective subject to an energy constraint is known to be difficult acknowledgments we thank nikhil bansal tak wah lam lap kei lee and alberto marchetti spaccamela for helpful discussions delivering energy proportionality with non energy proportional systems optimizing the ensemble niraj tolia zhikui wang manish marwah cullen bash parthasarathy ranganathan xiaoyun zhu hp labs palo alto abstract with power having become a critical issue in the oper ation of data centers today there has been an increased push towards the vision of energy proportional comput ing in which no power is used by idle systems very low power is used by lightly loaded systems and proportion ately higher power at higher loads unfortunately given the state of the art of today hardware designing individ ual servers that exhibit this property remains an open chal lenge however even in the absence of redesigned hard ware we demonstrate how optimization based techniques can be used to build systems with off the shelf hardware that when viewed at the aggregate level approximate the behavior of energy proportional systems this paper ex plores the viability and tradeoffs of optimization based ap proaches using two different case studies first we show how different power saving mechanisms can be combined to deliver an aggregate system that is proportional in its use of server power second we show early results on deliver ing a proportional cooling system for these servers when compared to the power consumed at utilization re sults from our testbed show that optimization based sys tems can reduce the power consumed at utilization to for server power and for cooling power introduction with power having become a critical issue in the opera tion of data centers the concept of energy proportional computing or energy scaledown is drawing in creasing interest a system built according to this prin ciple would in theory use no power when not being uti lized with power consumption growing in proportion to utilization over the last few years a number of tech niques have been developed to make server processors more efficient including better manufacturing techniques and the use of dynamic voltage and frequency scaling dvfs for runtime power optimization while the savings are significant this has lead to cpus no longer being responsible for the majority of power con sumed in servers today instead subsystems that have not been optimized for power efficiency such as network cards hard drives graphics processors fans and power supplies have started dominating the power consumed by systems especially during periods of low utilization instead of waiting for all these different technologies to deliver better energy efficiency this paper advocates that energy proportional computing can be approximated by using software to control power usage at the ensem ble level an ensemble is defined as a logical collection of servers and could range from an enclosure of blades a single rack groups of racks to even an entire data center it was previously difficult to dynamically balance work loads to conserve power at the ensemble level for a num ber of reasons unnecessarily shutting down applications to simply restart them elsewhere is looked upon as a high risk high cost change because of the performance impact the risk to system stability and the cost of designing cus tom control software however the re emergence of vir tualization and the ability to live migrate entire vir tual machines vms consisting of oss and applications in a transparent and low overhead manner will enable a new category of systems that can react better to changes in workloads at the aggregate level by moving work loads off under utilized machines and then turning idle ma chines off it should now be possible to approximate at an ensemble level the behavior found in theoretical energy proportional systems however virtualization is not a magic bullet and a na ıve approach to consolidation can hurt application per formance if server resources are overbooked or lead to re duced power savings when compared to the maximum pos sible this paper therefore advocates a more rigorous ap proach in the optimization of ensemble systems including the use of performance modeling optimization and con trol theory finally instead of only looking at more tradi tional server components such as storage cpus and the network optimization based systems should also consider control of other components such as server fans and com puter room air conditioners cracs as cooling costs are rapidly becoming a limiting factor in the design and operation of data centers today we use two case studies to demonstrate that it is possi ble by applying optimizations at the ensemble layer to de the power consumed by servers and exhibit power usage behavior close to that of an energy proportional system second we demonstrate how a power and workload aware cooling controller can exhibit the same behavior for cool ing equipment such as server fans case studies no dvfs dvfs dvfs off average enclosure utilization it has been advocated that optimization based algorithms should be preferred over ad hoc heuristics in making sys tem runtime and management decisions we will therefore not stress this point further but through the use of two case studies show how optimization can be used to deliver energy proportionality at the ensemble layer it should be stressed that the focus of this section is not on the use of any particular algorithm but instead on how non energy proportional systems can be combined to approxi mate the behavior of an energy proportional system experimental setup to evaluate energy proportionality in the case studies we used an hp bladesystem enclosure with pro liant server blades and fans each blade was equipped with gb of ram and two amd he dual core processors each processor has p states a volt age and frequency setting corresponding to frequencies of and ghz the blades and the fans are equally divided among two rows on the front and back ends of the enclosure respectively each blade is cooled by multiple fans and each fan draws cool air in through the front of the blades the enclosure allows us to measure blade temperatures and the power consumed by different components we used xen with both the administrative domain and the vms using the para virtualized linux kernel the xen administrative domain is stored on local disks while the vms use a storage area network san we used vms configured with mb of ram a gb virtual hard drive and one virtual cpu we set each vm memory to a low value to allow us to evaluate a large configuration space for workload place ment in production environments the same effect could be achieved at runtime through the use of page sharing or ballooning we used gamut to experiment with different vm and physical server utilizations server energy proportionality we examined the energy proportionality in the enclosure layer using three different policies the first policy no each result presented above is an average of approximately readings over a minute interval figure enclosure power usage blades network dvfs uses no power saving features the second policy dvfs uses hardware based voltage and frequency scaling and is very similar to linux ondemand governor the third policy dvfs off uses dvfs plus a vm migra tion controller that consolidates virtual machines and turns idle machines off its algorithm uses the blade power models for the different p states and sensor readings from resource monitoring agents the optimization problem is similar to that in with constraints on the blade cpu and memory utilization to prevent overbooking examples of other constraints that could be modeled include network and storage utilization our experience has shown that al gorithms such as bin packing or simulated annealing work well in this scenario for this case study we varied each vm utilization in the range of of a single core and mea sured the power consumed by the enclosure the results are presented in figure where the x axis shows the en closure utilization by all vms as a percentage of total capacity of all the blades as each server has cores the utilization also corresponds to each vm utilization as a percentage of a single core the measured power shown on the y axis includes the power consumed by the blades as well as the networking san and management modules with no dvfs we notice a very small power range w between and utilization and the minimum power used is w or of the power consumed at utilization significantly away from the theoretical minimum of w once dvfs is enabled the power range increases but as seen in the figure the system is still not energy proportional and consumes w at utilization it is only when we look at the dvfs off policy that the system starts approximating energy propor tionality at the ensemble level the range of power used between and utilization is w and at utilization there is only a w difference or of the power consumed at utilization from the theoretical minimum of w speed rpm static fan speed reactive fan controller predictive fan controller average enclosure utilization figure fan power consumption and model note from the figure that at high utilizations above all three policies have the same power usage as they are all running near peak performance while the dvfs policy only shows a noticeable difference when it can switch to a lower p state the dvfs off policy starts showing benefit around utilization as it can start con solidating vms and turning machines off even at utilization zero power usage was not achieved with dvfs off for a number of reasons first at least one active server is needed to host all the vms second our enclosure contained two network switches two san switches two management modules and six power supplies most of these components are not energy proportional finally like all industry standard servers even off servers have an active management processor that is used for network management tasks such as remote kvm and power cycling cooling energy proportionality while we showed how energy proportionality could be achieved for server power in section cooling equip ment also consumes a significant part of the total power used by a data center in particular server fans can con sume between of total server power and at the data center level cooling can account for as much as of the total power consumed in this case study we therefore examine how intelligent fan control can be used to achieve better energy proportionality for server cooling resources we continue to use the experimental setup described in section and use the dvfs off pol icy presented in section for managing server power the objective of fan control is to provide enough cool air for the blades so that the server temperatures can be main tained below thresholds for thermal safety reasons in this paper we specifically evaluate two different fan controllers anomalous reading noticeable in figure is for the no dvfs and dvfs settings at and utilization levels while the processor reports being at the highest p state we recorded a sharp drop in power usage at these points we believe that the processor is using an internal power saving scheme that it disables at high utilizations each result presented above is an average of approximately readings over a minute interval figure fan power a reactive fan controller rfc and a predictive fan controller pfc the reactive fan controller rfc is a simple feedback controller that changes fan speeds based on the data gath ered from the hardware temperature sensors present on all the blades because of the complexity of sharing fans between blades the rfc synchronously changes the speed of all fans in each row there are two rows of blades and fans in the enclosure in response to the maximum observed temperature for that row when the maximum temperature is above the threshold the fan speeds are in creased to provide a larger volume of air flow to cool the servers and vice versa the rfc is similar to commercial fan controllers used in industry today the predictive fan controller pfc aims to minimize the total fan power consumption without violating temper ature constraints it uses temperature sensors in the blades as well as software sensors to monitor server utilization figure shows the power model for a fan in our enclosure where the fan power is a cubic function of the fan speed which closely matches the measured fan power note that the volume air flow rate is approximately proportional to the fan speed also note that each fan provides different levels of cooling resource i e cool air to each individual blade according to the location of the fan with respect to the blade in this regard each fan has a unique cooling effi ciency with respect to each blade this provides an oppor tunity to minimize the fan power consumption by explor ing the variation in cooling efficiency of different fans for different blades along with the time varying demands of the workloads we built a thermal model empirically that explicitly captures the cooling efficiency between each pair of blade and fan this thermal model together with the blade and the fan power models is used by the pfc to predict future server temperatures for any given fan speed and measured server utilization by representing this as a convex constrained optimization problem the pfc is able to use an off the shelf convex optimization solver to set the fan speeds to values that potentially minimize the aggregate power consumption by the fans while keeping the blade temperatures below their thresholds the results for the two controllers are presented in fig ure the figure also includes the power consumed by setting the fans to a static speed that independent of the workloads is guaranteed to keep temperatures below the threshold under normal ambient operating conditions when examining the rfc performance it is helpful to note the relationship between vm utilization and fan speed for a given row of fans the fan speed is directly controlled by the maximum cpu temperature in the row furthermore cpu temperature is a function of blade uti lization and blade ambient or inlet temperature blade uti lization however does not always directly correlate to vm utilization for example with each vm utilization set to one vcpu the system cannot fit more than vms per machine with an overall blade utilization of four cpus however with a reduced vm utilization of each blade can accommodate vms with an overall blade utilization of as a blade cpu temperature will be much higher with a utilization of vs it will require a greater amount of cooling and therefore use more power due to increased fan speeds similarly if a blade is located in an area of increased ambient temperature that blade could also drive fan speed higher if and when it be comes utilized even if the utilization levels are relatively low these factors are responsible for the rfc operating in two power bands approximately between w when the utilization ranges between and between w when the utilization ranges between even at utilization the rfc still uses of the peak power used at utilization in contrast due to its knowledge of the cooling efficien cies of different fans the demand levels of the individual blades and their ambient temperatures the pfc is able to set fan speeds individually and avoid the correlated behav ior exhibited by controllers like the rfc overall the pfc induces an approximately linear relationship between the fan power and the aggregate enclosure utilization and at utilization the pfc only consumes of the power it uses at utilization the use of model based opti mization also allows the pfc to perform significantly bet ter than the rfc when we compare the two controllers the pfc can reduce fan power usage by at both and utilization at utilization the pfc only consumes of the power used by the rfc at however the pfc and the rfc with zero load is un able to reduce its power usage to w this was not a de ficiency of our optimization based approach but was due to the fact that the fans were also responsible for cooling the blade enclosure networking and management mod ules we therefore had to lower bound the fan speeds to ensure that these non energy proportional components did not accidentally overheat assumptions even though we used a homogeneous set of machines in our cases studies our experience has shown that these al gorithms can be extended with different power and thermal models to control an ensemble composed of heterogeneous hardware further current generation of processors from both intel and amd support cpuid masking that al lows vms to migrate between processors from different families this work also assumes that it is possible to mi grate vm identities such as ip and network mac addresses with the vms while this is generally not a problem in a single data center migrating storage area network san identities can sometimes be problematic however vendor products such as hp virtual connect and emulex vir tual hba have introduced a layer of virtualization in the storage stack to solve this problem note that it might be difficult to adopt the optimization based solutions similar to those proposed in this paper to applications that depend on locally attached storage for their input data however the fact that such locally attached storage systems usually replicate data for relia bility and availability reasons might provide a possi ble solution in such a scheme the optimization algo rithms could be made aware of the dependencies between the vms and the locations of their datasets given this information the algorithms could find a suitable consoli dated mapping of vms to physical machines in order to make this scheme effective a higher degree of replication might be needed to give the algorithms more flexibility in making placement decisions this change essentially boils down to a tradeoff between the cost of increased storage capacity versus energy savings finally our approach approximates energy proportion ality by turning machines off concerns have been previ ously raised about reliability of both servers and disk drives due to an increased number of on off cycles however our conversations with blade system designers have shown that it should not affect server class machines or given the large number of on off cycles supported by disk drives their internal storage systems during their normal lifetime aggressive consolidation might also hurt application avail ability if the underlying hardware is unreliable as most applications tend to be distributed for fault tolerance in troducing application awareness into the consolidation al gorithm to prevent it from consolidating separate instances of the same application on the same physical machine will address this issue concluding remarks this paper has shown that it is possible to use optimization based techniques to approximate energy proportional be havior at the ensemble level even though our techniques result in some added complexity in the system in the form of models optimization routines and controllers we be lieve the power savings are significant e nough t o justify it however better instrumentation can help us get even closer to theoretical energy proportionality for example if temperature sensors which are relatively cheap to de ploy were installed in the networking and san switches it would have allowed our fan controller to have more com plete knowledge of the thermal condition inside the enclo sure so that more efficient fan speed optimization could be achieved in addition we have used cpu utilization in this case study as a proxy for application level performance to directly evaluate and manage application performance we will need sensors that measure application level metrics such as throughput and response time these sensors can be provided by parsing application logs or by monitoring user requests and responses while the controllers shown in this paper assumed a sin gle management domain further study needs to be done to show how they would work in a federated environment where information would need to be shared between con trollers at different management layers and possibly from different vendors we believe that some of the distributed management task force dmtf standards would help address at least some of these issues prior work exists that looked at data center level cool ing efficiency b y m anipulation o f c rac u nit ettings or by temperature aware workload placement how ever given that these studies only looked at total power consumption a more careful investigation is needed in the context of this paper for example the model based opti mization approach we used in the predictive fan controller may be applied to the crac unit control problem stud ied in to achieve energy proportionality for the cooling equipment at the data center level finally even though this paper demonstrated energy proportionality at the ensemble layer this does not pre clude the need for better energy efficiency for individual components such as disks memory and power supplies we believe that new hardware designs with finer levels of power control will help in designing energy efficient sys tems at both the single server and ensemble layer online strategies for dynamic power management in systems with multiple power saving states sandy irani sandeep shukla and rajesh gupta university of california at irvine online dynamic power management dpm strategies refer to strategies that attempt to make power mode related decisions based on information available at runtime in making such decisions these strategies do not depend upon information of future behavior of the system or any a priori knowledge of the input characteristics in this paper we present online strategies and evaluate them based on a measure called the competitive ratio that enables a quantitative analysis of the performance of online strategies all earlier approaches online or predictive have been limited to systems with two power saving states e g idle and shutdown the only earlier approaches that handled multiple power saving states were based on stochastic optimization this paper provides a theoretical basis for the analysis of dpm strategies for systems with multiple power down states without resorting to such complex approaches we show how a relatively simple online learning scheme can be used to improve the competitive ratio over deterministic strategies using the notion of probability based online dpm strategies experimental results show that the algorithm presented here attains the best competitive ratio in comparison with other known predictive dpm algorithms the other algorithms that come close to matching its performance in power suffer at least an additional wake up latency on average meanwhile the algorithms that have comparable latency to our methods use at least more power on average categories and subject descriptors c computer systems organization performance of systems general terms algorithms performance additional key words and phrases dynamic power management online algorithms introduction power management in embedded computing systems is achieved by actively changing the power consumption profile of the system by putting its components into power energy states sufficient to meeting functionality requirements for example an idling component such as a disk drive can be put into a slowdown or shutdown state of course bringing such a component back to the active this work was supported by nsf grant ccr src and darpa ito supported pads project under the pac c program in addition the first author is partially supported by nsf grant ccr and by onr award authors address information and computer science department university of california at irvine irvine ca email irani skshukla rgupta ics uci edu permission to make digital hard copy of all or part of this material without fee for personal or classroom use provided that the copies are not made or distributed for profit or commercial advan tage the acm copyright server notice the title of the publication and its date appear and notice is given that copying is by permission of the acm inc to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee qc acm acm transactions on embedded computing systems vol no august pages state may require additional energy and or latency to service an incoming task the input to the problem we consider here is the length of an upcoming idle period and the decision to be made is whether to transition to a lower power dissipation state while the system is idle there are several issues in coming to this decision intelligently for instance immediate shutdown that is shut down as soon as an idle period is detected may not save overall energy if the idle period is so short that the powering up costs are greater than the energy saved in the sleep state on the other hand waiting too long to power down may not achieve the best energy reductions possible thus there exists a need for effective and efficient decision procedures to manage power consumption dynamic power management dpm attempts to make such decisions usually under the control of the operating system at runtime based on the dynam ically changing system state functionality and timing requirements benini and de micheli benini et al chung et al hwang et al irani et al ramanathan et al shukla and gupta srivastava et al in a survey of dpm techniques in benini et al the authors classify dpm strategies into two main groups a predictive schemes and b stochastic optimum control schemes predictive schemes attempt to predict the timing of future input to the system and schedule shutdown usually to a single lower power state based on these predictions stochastic optimum control is a well researched area benini et al chung et al qiu and pedram qiu et al simunic et al the chief characteristic of these approaches is the construction and validation of a mathematical model of the system that lends itself to a formulation of a stochastic optimization problem then strategies to guide the system power profile are devised that achieve the most power savings in presence of the uncertainty related to system inputs while several useful and practical techniques have been developed using the predictive and stochastic optimum control schemes it is difficult to develop bounds on the quality of the results without extensive simulations and or model justification competitive analysis and its limitations we approach dpm as an inherently online problem in that an algorithm gov erning power management must make decisions about the expenditure of re sources before all the input to the system is available borodin and el yaniv specifically the algorithm does not learn the length of an idle period until the moment that it ends analytical solutions to such online problems are often best characterized in terms of a competitive ratio phillips and westbrook that compares the cost of an online algorithm to the optimal offline solu tion that knows the input in advance note that the offline algorithm is not a realistic algorithm since it knows the length of the idle period in advance it is just used as a point of comparison to quantify the performance loss due to the fact that a dpm strategy must make decisions with only partial information to be specific we say that an algorithm is c competitive if for any input the cost of the online algorithm is bounded by c times the cost of the optimal offline table i values for the power dissipation and start up energy for the ibm mobile hard drive algorithm for that input the offline algorithm has access to the entire input before committing to any decisions for dpm the cost of an algorithm is the total amount of energy consumed the competitive ratio cr of an algorithm is the infimum over all c such that the algorithm is c competitive competitive analysis borodin and el yaniv that is analysis to de termine bounds on the competitive ratio can be done either as a case analysis of the various adversarial scenarios karlin et al ramanathan or through theorem proving or automatic model checking shukla and gupta competitive analysis has proven to be a powerful tool in providing a guar antee on the performance of an algorithm for any input competitive analysis is valuable in situations where it is impractical to obtain and process information for predicting future inputs it also provides an assurance for the designers of the algorithm about the worst possible behavior of an online algorithm for example if an online strategy has a competitive ratio of which assures the designer that no matter what input sequence is provided to the strategy it will never cost the strategy more than twice the cost incurred by the best possible strategy there are two chief limitations of the competitive analysis approach to dpm that we seek to address in this paper the first is that the results tend to be overly pessimistic due to the fact that they examine worst case behavior thus there is a need to refine the cr bounds that take into account typical input behavior in many applications there is a structure in the input sequence that can be utilized to fine tune online strategies and improve their performance the second problem with our earlier work on cr based dpm strate gies karlin et al ramanathan et al is that the system model consists of devices with only two power saving states namely an idle state and a shutdown state this is a limitation of most predictive strategies as well benini et al in contrast most real systems consist of components with multiple power states there are two kinds of systems here systems with multiple shutdown states as well as systems with multiple operating states for instance table i shows the power states of a portable hard drive ibm that consists of three low power states in which a device can be when it is not processing any requests radio modems represent devices that can often be in multiple active states for instance transmitting data at various power levels to the radio transmitter we do not address the problem of choosing among different operating states in this paper for systems with more than one device our protocols can be applied to each device separately each device experiences periods of idle time that may or may not coincide with the idle periods experienced by other devices in the system a power management strategy can then be applied to a particular device during the idle periods experienced by that device there will be some dependence between performances for different devices in cases where a request requires more than one device to be satisfied in these situations a device that is in an active state may experience some latency because it has to wait for another device to transition from a low power state to the active state before either can begin work in this paper we focus on performance for a single device although this phenomenon would be an interesting direction for future work related work many strategies for dynamic management of power and energy have been pro posed and studied these include predictive strategies hwang et al srivastava et al stochastic modeling based strategies benini et al qiu and pedram session clustering and prediction strategies lu and de micheli online strategies ramanathan et al and adaptive learning based strategies chung et al a survey of dpm strategies that covers the classes of algorithms in predictive and stochastic optimum con trol categories can be found in benini et al lu et al present a quantitative comparison of various existing management strategies many predictive dynamic power management strategies benini et al chung et al hwang et al karlin et al lu and de micheli ramanathan et al srivastava et al use a sequence of past idle period lengths to predict the length of the next idle period these strategies typically describe their prediction for the next idle period with a single value given this prediction they transition to the power state that is optimal for the predicted idle period length in case the prediction is wrong they transition to the lowest power state if the idle period extends beyond a fixed threshold value for the sake of comparison with other approaches we call these predictive dpm schemes single value prediction svp schemes of particular interest is the work of chung et al that addresses multiple idle state systems using a prediction scheme based on adaptive learning trees their method has shown an impressively high hit ratio in its prediction in the past stochastic control based methods as in benini et al simunic et al chung et al qiu and pedram and qiu et al have been limited because they make assumptions about the characteristic probability distribution of the input job arrivals the service time distribution of the device and so on although the policies obtained are optimal given these assumptions there are no guarantees that the assumptions will always hold these problems have been addressed to some extent in chung et al and lu and de micheli recently the stochastic modeling approach has been extended so that the assumption that interarrival times are exponentially distributed can be removed glynn et al stochastic modeling has also been extended to systems which can be modeled by petri nets qiu et al one drawback of these approaches is that they require solving computationally expensive optimization problems to derive the optimal solutions if the usage patterns for the device change the algorithm has to be reoptimized to adjust to the new behavior in chung et al an adaptive technique has been proposed which tries to overcome this limitation the au thors use sliding windows to keep a long enough history of the input arrivals and estimate the parameters of the arrival markov processes however this method is complex because they have to solve the optimization problem to find the optimal strategy in a clock driven or event driven manner there are also a few existing results on competitive analysis of dpm strate gies in the online algorithms literature some of this work does not address dpm explicitly but the general framework applies to dpm as well an example of this is the now classical result described in phillips and westbrook that shows that is the best competitive ratio achievable by any deterministic online algorithm probabilistic analysis for online dpm algorithms in two state systems has been given in karlin et al and keshav et al they as sume that the distribution over the upcoming idle period is known and optimize the algorithm based on that distribution they give a method to determine the best online algorithm given a distribution and show that for any distribution the expected cost of this online algorithm is within a factor of e e of the expected cost of the optimal offline algorithm this result is tight in that there is a distribution for which the ratio is exactly e e although for some distributions the ratio may be less our contributions and paper organization in section we present a deterministic algorithm for dpm for multistate de vices and show that it is competitive this result which extends the earlier analysis described in phillips and westbrook is tight in the sense that there is no constant c such that there is a deterministic c competitive algo rithm that works for all multiple power down state devices however it may be possible to have a competitive ratio less than for a specific device depending on the parameters of the device e g number of states power dissipation rates start up costs and so on note that this deterministic algorithm focuses on a single idle period and does not depend on any history of previous idle periods thus it is nonadaptive in that it does not use any information about previous idle periods to tune its behavior as we have argued above competitive analysis often yields unduly pes simistic results indeed our goal is to devise a dpm strategy whose total energy expenditure is much lower than twice the optimal offline algorithm we show that by keeping track of a sequence of past idle periods we can improve the performance of the competitive deterministic strategy in practice by adapting the behavior of the strategy to patterns observed in the input sequence we do this in section by modeling the future input by a probability distribution that is learned from the recent history of the length of idle periods in doing so we avoid some of the problems with many of the previous strategies mentioned above one of the chief limitations of a single valued prediction svp approach is that it fails to capture uncertainty in the prediction for the upcoming idle period length for example if a very short idle period and a very long idle period are equally likely these methods are forced to pick a single prediction and pay a penalty in the likely event that the prediction is wrong using a probability distribution to model the length of the upcoming idle period allows for a much richer prediction so that the algorithm can optimize in a way that takes the nature of this additional information into account furthermore we make no assumptions about the form of the distribution governing idle period length which means that our method can automatically adapt to a variety of applications and also applies to nonstationary input arrivals in section we derive analytical bounds for an online algorithm for dpm that knows the probability distribution governing the length of the upcoming idle period section provides a systematic means of dynamically construct ing an estimate for the probability distribution based on online observations and combining it with the algorithm from the previous section to build an online power management strategy experimental results in section demonstrate the utility of our strategy in balancing power usage with latency system model consider a device that can be in one of the k power states denoted by sk the power consumption for state i is denoted by αi the states are ordered so that αi α j as long as i j thus state is the active state that is the highest power consumption state any strategy for an individual idle period can be described by a sequence of thresholds each of which is associated with a power consumption state as soon as the idle periods extend beyond a given threshold the device transitions to the associated power consumption state from the manufacturer specification we are also given the transition power pij and transition times tij to move from state si to sj usually the energy needed and time spent to go from a higher power state to a lower power state is negligible whereas the converse is not true thus we simplify the model by considering only the time and power necessary to power up the system fur thermore all of the algorithms considered in this paper have the property that they only transition to the active state when powering up and never transition to an intermediate higher powered state as a result we only need the time and total energy consumed in transitioning up from each state i to the active state state the total energy used in transitioning from state i to the active state is denoted by βi we note that in cases where the time and energy cost incurred in transi tioning to lower power consumption states is nonnegligible they can be easily incorporated by folding them into the corresponding power up parameters this can be done as long as the time and energy used in transitioning down is addi tive that is we require that for i j k the cost to go from i to j and then from j to k is the same as the cost of going from i directly down to k the input to the dpm algorithm is simply a sequence of idle periods for each idle period the dpm is notified when the idle period begins and then again when the idle period ends this is the only information required by our dpm in our experiments the idle periods are derived by our simulator which receives a time stamped sequence of requests for service with each request the simulator is told the time of its arrival and the length of time it will take to satisfy the request if the device is busy when a new request arrives it enters a queue and is served on a first come first serve basis in this case there is no idle period and the device remains active through the time that the request is finished this means that the number of idle periods is generally less than the number of requests serviced whenever a request terminates and there are no outstanding requests waiting in the system an idle period begins in these situations the dpm is invoked to determine which power consumption states the device should transition to and at what times if the device is not busy when a new request arrives it will immediately transition to the active state to serve the new request if it is not already there in the case where the device is not already in the active state the request can not be serviced immediately but will have to incur some latency in waiting for the transition to complete this delay will cause future idle periods to be shorter in fact if a request is delayed some idle periods may disappear thus we have an interesting situation where the behavior of the algorithm affects future inputs idle period lengths given to the algorithm note also that delaying servicing a request will tend to result in lower power usage for instance consider the extreme case where the power manager re mains in the deepest sleep state while it waits for all the requests to arrive and then processes them all consecutively this extreme case is not allowed in our model since we require that the strategy transition to the active state and begin to work on a request as soon as one appears however it illustrates the natural trade off that occurs between power consumption and latency our experimental results explore this trade off for the set of algorithms studied a more extensive discussion of this trade off is presented in ramanathan et al in the next two sections we present our analysis for the deterministic and probability based algorithms in this analysis we do not take into account the delay incurred in returning to the active state because the analysis focuses on an individual idle period these sections address the problem of minimizing total energy expenditure given that the length of the upcoming period is ar bitrary deterministic case or is governed by a fixed probability distribution probability based case the empirical evaluations incorporate the effect of start up latency as an entire sequence of requests for service arrive through time the deterministic online algorithm we now present our deterministic algorithm for online dpm the basic idea is that the online algorithm tries to mimic the behavior of the optimal offline algorithm at each point in time t the algorithm is in the state that the optimal algorithm would have chosen if the length of the idle period were exactly t to get the optimal cost we plot each line c αit βi this is the cost of spending the entire interval in state i as a function of t the length of the interval take the lower envelope of all of these lines let us call this function le t the optimal cost for an interval of length t is le t mini αit βi the fig energy consumption for each state for a four state system each state is represented by a line that indicates the energy used if an algorithm stays in that state as a function of the length of the idle period for each state the slope is the power dissipation rate and the y intercept is the energy required to power up from that state online algorithm called the lower envelope algorithm lea will also follow the function le it will remain in the state that realizes the minimum in le and will transition at the discontinuities of the curve that is lea will remain in state j as long as α j t β j mini αit βi for the current time t for j to k let t j be the solution to the equation α j t β j α j β j where t j is the time that lea will transition from state j to state j we will assume here that we have thrown out all the lines that do not appear on the lower envelope at some point this is equivalent to the assumption that tk tk see figure theorem the lower envelope algorithm is competitive the proof of theorem is shown in the appendix as emphasized earlier this algorithm does not take into account input pat terns the worst case scenario obtained via theorem shows that the energy cost resulting from the online decisions can be no worse than two times the energy cost of the optimal offline strategy which knows the input sequence in advance we show later that depending on request arrival patterns this worst case bound may not really happen and the empirical ratio of the online to offline costs may in fact be much lower as shown in qiu and pedram chung et al and benini et al input sequences are often interre lated and hence modeling of the input pattern and exploiting that knowledge in the design of the algorithm can help bridge the gap between the perfor mance of online strategy and that of the optimal offline strategy in the next section we discuss our probability based algorithm and show that if the proba bility distribution governing the length of the idle period is known before hand the worst case competitive ratio can be improved by with respect to the deterministic case moreover we show through experimental evaluation that this worst case bound is pathological in fact we can bring the energy cost of the online algorithm within of the optimal offline one the probability based online algorithm optimizing power based on a probability distribution let us assume that the length of the idle interval is generated by a fixed known distribution whose density function is π and devise a method to optimize power management decisions we first discuss systems with two states and then give our generalization to the multistate case let β be the start up energy of the sleep state and α the power dissipation of the active state suppose that the online algorithm uses τ as the threshold at which time it will transition from the active state to the sleep state if the system is still idle in this case the expected energy cost for the algorithm for a single idle period will be τ π t αt dt π t ατ β dt τ the best online algorithm will select a value for τ that minimizes this expres sion the offline algorithm that knows the actual length of an upcoming idle period will have an expected cost of β α π t αt dt β α π t β dt it is known for the two state case that the online algorithm can pick its thresh old τ so that the ratio of its expected cost to the expected cost of the optimal algorithm is at most e e karlin et al that is for any π and any α and β minτ τ π t α t dt π t τ ατ β dt e β α π t αt dt β α π t β dt e this is optimal in that for any α and β there is a distribution π such that this ratio is at least e e let us now consider the multistate case as in the previous section let t j be the solution to the equation α j t β j α j β j t j is the time that lea will transition from state j to state j we assume here that we have thrown out all the lines that do not appear on the lower envelope at some point this is equivalent to the assumption that tk tk for ease of notation we define to be and tk to be the cost expected energy consumption of the optimal offline algorithm is i ti ti π t αit βi dt now to determine the online algorithm we must determine k thresholds where the threshold τi is the time at which the online algorithm will transition from state i to state i in the spirit of the deterministic online algorithm for the multistate case we let τi be the same as the threshold which would be chosen if i and i were the only two states we call this algorithm the probability based lower envelope algorithm plea the proof of the following theorem appears in the appendix theorem for any distribution the expected cost of the probability based lower envelope algorithm is within a factor of e e of the expected cost for the optimal offline algorithm learning the probability distribution while we have proven the competitive bounds regardless of the chosen idle time probability distribution the practical problem of finding π t to guide the plea algorithm remains our approach is to learn π t online accordingly the algorithm that uses plea in conjunction with our scheme to learn the proba bility distribution is called the online probability based algorithm opba it works as follows a window size w is chosen in advance and is used throughout the execution of the algorithm the algorithm keeps track of the last w idle period lengths and summarizes this information in a histogram periodically the histogram is used to generate a new power management strategy the set of all possible idle period lengths is partitioned into n intervals where n is the number of bins in the histogram let ri be the left endpoint of the ith interval the ith bin has a counter that indicates the number of idle periods among that last w idle periods whose length fell in the range ri ri the bins are numbered from through n and rn the last w idle periods are held in a queue when a new idle period is com pleted the idle period at the head of the queue is deleted from the queue if this idle period falls in bin i then the counter for bin i is decremented the new idle period is added to the tail of the queue if this idle period length falls into bin j the counter for bin j is incremented thus the histogram always includes data for the last w idle periods our experimental results in section include a study experimenting with values for w the counter for bin i is denoted by ci the threshold for changing states is selected among n possibilities that is rn the lower end of each range we estimate the distribution π by the distribution that generates an idle period of length ri with probability ci w for each i n the sum of the counters is the window length w thus the threshold is taken to be arg min t c j r j αi αi c j rt αi αi βi βi rt w j w j t in order to be useful as a decision procedure this algorithm must be imple mented efficiently we have implemented the algorithm for finding the all k thresholds in time o kn where k is the number of states and n is the table ii a snapshot of the histogram used in opba note that the offline thresholds are and ms the number of bins per state is number of bins in the histogram two important factors determine the cost in time expenditure of implementing our method a the frequency with which the thresholds are updated and b the number of bins in the histogram these need to be minimized for policy implementation efficiency the algo rithm runs the o kn time algorithm every time the thresholds are updated the frequency with which the thresholds are updated is the subject of one set of experiments discussed in section note that the cost to implement our strategy is independent of w the number of idle periods tracked in the histogram minimization of the number of bins used in the histogram must be balanced with the fact that the finer grained the histogram the more accurate our choice of thresholds will be our experiments as discussed in the following sections show that a finer grained binning is more important in some ranges than it is in others a way of addressing this problem key to the success of the algorithm is to use the thresholds of the lower envelope algorithm to guide the selection of the bins recall that these thresholds are the tk defined in section we then choose a constant c number of bins per state the range from ti to ti is divided into c equal sized bins note that the running time of our algorithm depends linearly on the number of bins and hence depends linearly on c for this reason it is important to select a small value for c we experimented with values for c ranging from to and the results only varied by we use a value of c in all our results table ii shows a sample histogram from our experiments table iii this figure shows for each trace the percentage of idle periods for which the optimal algorithm chose to transition to each state experimental design data used in our experiments to demonstrate the utility of our probability based algorithm we use a mobile hard drive from ibm this drive has four power down states as shown in table i here the start up energy refers to the energy cost in transitioning from a state to the active state for application disk access data we used trace data from auspex file server archive from this data we collected the arrival times and lengths for requests for disk access for million disk accesses divided into multiple trace files corresponding to different hours of the day table iii gives some data on these traces the first column gives the number of requests the subsequent columns give information about the behavior of the optimal algorithm when run on each trace specifically they show for each state the percentage of idle periods in which the optimal algorithm transitions to that state in all the traces there is a high percentage of short sequences for which the optimal strategy is to stay in the active state shown in column the remaining percentages vary somewhat from trace to trace all of the results reported in this paper are an average of the results on each individual trace weighted by length algorithm test suite we compare opba and lea to several other predictive algorithms presented in the literature the algorithms come in two groups the algorithms in the first group use a series of thresholds that determine when the algorithm will transi tion from each state to the next lower power consumption state opba and lea fall into this group the second group is made up of single valued prediction file traces from the now project available at traces auspex html table iv predictions versus outcomes for the exponential decay algorithm the total number of idle periods is and the total number of requests is algorithms they use a single prediction for the length of the upcoming idle period and to transition immediately to the optimal state for that length they differ only in how they select a prediction for the length of the next idle period optimal offline algorithm opt this algorithm is assumed to know the length of the idle period in advance it selects the optimal power usage state for that idle period and then transitions to the active state before the new request arrives in order to service the incoming request just as it arrives last period last this is a single valued prediction algorithm that uses the last period as a predictor for the next idle period exponential decay exp this algorithm developed by hwang et al keeps a single prediction for the upcoming idle period after a new idle period ends the prediction is updated by taking a weighted average of the old pre diction and the new idle period length let p be the current prediction and l the length of the last idle period p is updated as follows p λp λ l where λ is a value in the range we use a value of for λ adaptive learning tree tree this method uses an adaptive learning tree to predict the value of the next period based on the sequence of recent idle period lengths just observed details of this method can be found in chung et al there are different possible versions of single valued prediction algorithms which are worth mentioning here in describing these variations we refer to the offline thresholds tk described in section the authors of the learning tree algorithm observe that there are many idle periods that are very short in order to avoid transitioning to a lower powered state for such short idle periods they keep their system in the active state until the first offline threshold has been passed only then do they transition to the predicted optimal state we ran all single valued prediction algorithms with and without this initial delay we found that the results were not significantly different between the two versions in this paper we only report results for the versions without this delay to see why the delay does not help significantly we give some statistics for the exponential decay algorithm when run without the delay in table iv an entry in row i column j indicates the total number of times over all traces that the online algorithm predicted that the optimal state for an upcoming idle period would be j and the actual optimal state was i having a delay until time helps only when the algorithm predicts that the optimal state is stand by idle or sleep and the actual optimal state is active the numer of times this happens is the sum of the values in row active columns stand by idle and sleep the degree of savings in latency is the largest for idle periods counted in entry active sleep less so for entry active stand by and even less for active idle given the values in the table it is clear why having a delay does not offer a significant saving in the total latency another feature that the authors of the learning tree algorithm employ is to transition to the active state after the threshold for the predicted optimal state is reached this is done in a hope that a job will arrive shortly there after and the system can avoid incurring any additional latency in powering up after the new job has arrived if no job arrives before the last threshold tk then the algorithm transitions to the lowest sleep state to summarize if there are k states and the prediction is that state i will be the optimal state for the upcoming idle period the algorithm will transition immediately to state i if a new request has not arrived by time ti the algorithm will tran sition back to the active state state finally if a request has not arrived by time tk the algorithm will transition to the deepest sleep state state k we call this version of single valued prediction algorithms the preemptive wake up version the alternative preemptive wake up is to transition immediately to state i if that is the predicted optimal state and then to transition directly to the deepest sleep state if a request has not arrived by time ti we call this version the non preemptive wake up version naturally the preemptive wake up version will use more power but will tend to incur less latency on average we report results for both versions of all the single valued predictive algorithms used in the study experimental results experimentation with window size figure shows the average energy consumed per request as the window size is varied note that beyond a certain threshold window size below which the predictions are not accurate anyway the variation of energy consumption with respect to the increase in window size is not large indicating that our method is fairly robust to choices in window size our method performs best with a relatively small value for the window size indicating that it is the most recent history that is the most relevant predicting upcoming idle period length it also indicates that the distribution over idle period lengths is not necessarily stable over time however the results get worse if the window size gets below showing that there need to be enough values to get a representative sample we selected a value of for the window size that is used for the remainder of the results presented experimentation with threshold update frequency figure shows the average energy consumed per idle period as the frequency of updating the thresholds is varied as one would expect as the interval between fig average energy consumed per request as a function of window size for the online probability based algorithm the thresholds are updated every ten requests fig average energy consumed per request as a function of frequency of update for the online probability based algorithm the window size is updates grows so does the power usage however there do not to seem to be large differences in the cost so we adopt a frequency of update of evaluation of algorithm performance table v shows the comparison of energy consumption and wake up latency ef fects across a number of predictive dpm algorithms the first column of num bers in table v is the average energy used per request for each of the algorithms the second column is the ratio of this figure to the average energy consumed per request by the optimal offline algorithm interestingly there were some traces where this ratio is less than although they always averaged out to be greater than over all the traces the reason it is possible for an algorithm to be better than the optimal offline algorithm on power consumption is because table v energy measured in joules and latency measured in milliseconds the optimal algorithm is always a forced wake up preemptively before a request arrives this means that the optimal algorithm incurs no additional latency due to waking up the disk drive recall that since there is a power latency trade off this will tend to penalize the optimal algorithm with respect to power usage the average latency per request is shown in the final column of the figure the latency is averaged over the number of requests not the number of idle periods these two numbers are different since some requests may arrive while the device is busy working on other jobs or powering up in these cases the incoming request would not correspond to an idle period as an example we refer back to the statistics for the exponential delay algorithm in table iv the total number of idle periods over all traces is whereas the total number of requests is note that the algorithm only experiences the full of transition time when a request arrives and the algorithm is in the sleep state refer to table i for the transition times for the various states for single value prediction algorithms without predictive wake up this only happens if the algorithm predicts that the sleep state will be the optimal state for the upcoming idle period or when the idle period actually lasts so long that the algorithm discovers that the sleep state was in fact the optimal state the number of times this happens is the sum of the numbers in entries which are either in column sleep or row sleep or both the algorithm only experiences the delay in transitioning from the stand by state to the active state for those idle periods counted in entries active stand by idle stand by and stand by stand by the final results in table v are also shown graphically in figure which plots the power usage middle column from table v against the average latency last column from table v the opba exhibits the lowest power consumption among all the online algorithms the other algorithms that come close to match ing its performance in power the nonpreemptive versions of last tree and exp all suffer at least an additional latency on average meanwhile the algorithms that have a lower average latency than opba lea and the pre emptive versions of last tree and exp all use at least more power on average thus opba is the most successful algorithm in balancing power usage as well as latency incurred finally we study the the effect of variable wake up time since many devices vary somewhat in the time it takes to transition from one state to another fig energy is measured in joules and latency is measured in milliseconds table vi latency as variation in transition time increases each transition time is multiplied by a number generated unformly at random from the range ε ε latency is measured in milliseconds we study the top performers for latency and power when random noise is added to the time to transition to the active state every time a transition is performed we multiply the time to transition by a number generated unformly at random from the range ε ε for different values of e the results are shown in table vi we did not include the power usage because it does not vary signif icantly for different values of ε all of the methods suffer a slight degradation in performance as e increases the variation seems to have a similar effect on all algorithms studied conclusion and future directions we have presented a deterministic online algorithm for dynamic power man agement on multistate devices and proved that it is competitive and that this bound is tight we improve upon this bound considerably by devising a probability based scheme to support this strategy in an online dpm frame work we provide a method to efficiently construct a probabilistic model for the length of the upcoming idle period based on online observations our experi ments show that the algorithm presented in this paper attains the best per formance in practice in comparison to other known predictive dpm algorithm the other algorithms that come close to matching its performance in power all suffer at least an additional wake up latency on average meanwhile the algorithms that have comparable latency to our methods all use at least more power on average our future plans include extension of this work into systems with multiple active as well as power down states our experimental framework is available on our for testing a range of online dpm algorithms using a java applet interface the interface allows users to upload their data and evaluate our algorithms and other known algo rithms that we have implemented in our simulation framework appendix proof of theorem first we establish that the worst case for the algorithm will always be just after a transition time consider the time t j γ for some j k and γ t j t j for any value of γ in the given range the optimal cost will be α j t j γ β j for any value of γ in the given range the online cost will be j αl tl tl α j γ β j l the ratio of these two will be maximized for γ now suppose that the interval ends just after t j for some j k using the cost for the online and offline determined above the ratio of the online cost to the offline cost will be j αl tl tl β j l α j t j β j thus it is sufficient to prove that j j αl tl tl α j t j l α j t j β j αl tl tl α j t j α j t j β j l each tl was chosen so that αl αl tl βl βl so we can substitute these values into inequality to get that β j β j α j t j β j collapsing the telescoping sum we get that β j α j t j β j website at uc irvine java applet based dpm strategy evaluation website using the fact that and that α j and t j the inequality holds proof of theorem consider a system in which there are only two states i and i both online and offline must pay at least αit for an interval of length t in addition each must pay at least βi for the start up cost these costs which are incurred by both algorithms regardless of their choices will only serve to decrease the competitive ratio in determining τi we disregard these additional costs consider the system where the power consumption rate in the on state is αi αi and is in the off state the energy required to transition from the on to the off state is βi βi we choose τi to the be the transition time for the optimal online policy in this system thus we choose τi to be arg min r τ t t dt r t dt the online cost for this new system is the above expression evaluated at τ τi oni τi π t t αi αi dt π t τi τi αi αi βi βi dt let ti be defined to be βi βi αi αi note that this is the same definition in the previous proof the point where the lines αit βi and αi βi meet the offline cost for the new system is offi ti π t t αi αi dt π t ti βi βi dt we are guaranteed that the ratio of the expected online to offline costs is at most e e karlin et al keshav et al since the ratio of oni to offi is at most e e for each i we know that k oni i offi e e we now prove that k i oni is exactly the expected cost for plea on the mul tilevel system we also prove that k offi is exactly the expected cost of the optimal algorithm for the multilevel system we rephrase oni by separating the integral into the intervals from τ j to τ j to simplify notation denotes and τk denotes oni j τ j τ j π t t αi αi dt k j i τ j τ j π t τi αi αi βi βi dt in the sum over all oni we group together all the contributions from each oni over the interval τ j τ j for j k note that this is the interval that the algorithm will spend in state j this value is i τ j τ j π t τi αi αi βi βi dt i j τ j τ j π t t αi αi dt thus we have that k k oni f j where i j r τ j i j τ j τ j π t t αi αi dt putting the summations inside the integrals and collapsing the telescoping sums the expression in becomes where π t cost t dt τ j j note that cost t β j τl τl αl t τl αl l j β j τl τl αl t τl αl l is exactly the energy expended by plea if the idle period t is in the range τ j τ j thus the expected cost for plea is j τ j τ j π t cost t dt i oni the proof that the expected offline cost is equal to k offi is the same as the proof for the online cost except that the integrals ar e separated into intervals according to the ti instead of the τi powernap eliminating server idle power david meisner brian t gold thomas f wenisch advanced computer architecture lab computer architecture lab the university of michigan carnegie mellon university abstract data center power consumption is growing to unprece dented levels the epa estimates u s data centers will con sume billion kilowatt hours annually by much of this energy is wasted in idle systems in typical deployments server utilization is below but idle servers still con sume of their peak power draw typical idle periods though frequent last seconds or less confounding simple energy conservation approaches in this paper we propose powernap an energy conservation approach where the entire system transitions rapidly be tween a high performance active state and a near zero power idle state in response to instantaneous load rather than requiring fine grained power performance states and complex load proportional operation from each system com ponent powernap instead calls for minimizing idle power and transition time which are simpler optimization goals based on the powernap concept we develop requirements and outline mechanisms to eliminate idle power waste in en terprise blade servers because powernap operates in low efficiency regions of current blade center power supplies we introduce the redundant array for inexpensive load shar ing rails a power provisioning approach that provides high conversion efficiency across the entire range of power nap power demands using utilization traces collected from enterprise scale commercial deployments we demon strate that together powernap and rails reduce average server power consumption by categories and subject descriptors c computer sys tem implementation servers general terms design measurement keywords power management servers introduction data center power consumption is undergoing alarming growth by u s data centers will consume bil permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee asplos march washington dc usa copyright qc acm lion kwh at a cost of billion per year unfortu nately much of this energy is wasted by systems that are idle at idle current servers still draw about of peak power in typical data centers average utilization is only low utilization is endemic to data center operation strict service level agreements force oper ators to provision for redundant operation under peak load idle energy waste is compounded by losses in the power delivery and cooling infrastructure which increase power consumption requirements by ideally we would like to simply turn idle systems off un fortunately a large fraction of servers exhibit frequent but brief bursts of activity moreover user demand often varies rapidly and or unpredictably making dynamic consol idation and system shutdown difficult our analysis shows that server workloads especially interactive services exhibit frequent idle periods of less than one second which cannot be exploited by existing mechanisms concern over idle energy waste has prompted calls for a fundamental redesign of each computer system component to consume energy in proportion to utilization proces sor dynamic frequency and voltage scaling dvfs exem plifies the energy proportional concept providing up to cu bic energy savings under reduced load unfortunately pro cessors account for an ever shrinking fraction of total server power only in current systems and control ling dvfs remains an active research topic other subsystems incur many fixed power overheads when active and do not yet offer energy proportional operation we propose an alternative energy conservation approach called powernap that is attuned to server utilization pat terns with powernap we design the entire system to tran sition rapidly between a high performance active state and a minimal power nap state in response to instantaneous load rather than requiring components that provide fine grain power performance trade offs powernap simplifies the sys tem designer task to focus on two optimization goals optimizing energy efficiency while napping and min imizing transition time into and out of the low power nap state based on the powernap concept we develop requirements and outline mechanisms to eliminate idle power waste in a high density blade server system whereas many mech anisms required by powernap can be adapted from mo utilization ibm sun google cpu fans i o disk memory other figure server utilization histogram real data centers are under utilized table enterprise data center utilization traces workload avg utilization description web web application servers it enterprise it infrastructure apps bile and handheld devices one critical subsystem of cur rent blade chassis falls short of meeting powernap energy efficiency requirements the power conversion system power nap reduces total ensemble power consumption when all blades are napping to only of the peak when all are ac tive power supplies are notoriously inefficient at low loads typically providing conversion efficiency below under load these losses undermines powernap energy efficiency directly improving power supply efficiency implies a sub stantial cost premium instead we introduce the redundant array for inexpensive load sharing rails a power pro visioning approach where power draw is shared over an ar ray of low capacity power supply units psus built with commodity components the key innovation of rails is to size individual power modules such that the power de livery solution operates at high efficiency across the entire range of powernap power demands in addition rails provides n redundancy graceful compute capacity degra dation in the face of multiple power module failures and reduced component costs relative to conventional enterprise class power systems through modeling and analysis of ac tual data center workload traces we demonstrate analysis of idle busy intervals in actual data centers we analyze utilization traces from production servers and data centers to determine the distribution of idle and active periods though interactive servers are typically over idle most idle intervals are under one second energy efficiency and response time bounds through queuing analysis we establish bounds on powernap energy efficiency and response time impact using our figure server power breakdown no single com ponent dominates total system power models we determine that powernap is effective if state transition time is below and incurs no overheads below furthermore we show that powernap pro vides greater energy efficiency and lower response time than solutions based on dvfs efficient powernap power provisioning with rails our analysis of commercial data center workload traces demonstrates that rails improves average power con version efficiency from to in powernap enabled servers understanding server utilization it has been well established in the research literature that the average server utilization of data centers is low often below in facilities that provide interactive services e g transaction processing file servers web average utilization is often even worse sometimes as low as figure depicts a histogram of utilization for two production workloads from enterprise scale commercial deployments table describes the workloads running on these servers we derive this data from utilization traces collected over many days aggregated over more than severs produc tion utilization traces were provided courtesy of hp labs the most striking feature of this data is that the servers spend the vast majority of time under utilization data center utilization is unlikely to increase for two reasons first data center operators must provision for peak rather than average load for interactive services peak utilization often exceeds average utilization by more than a factor of three second to provide redundancy in the event of failures operators usually deploy more systems than are actually needed though server consolidation can improve average utilization performance isolation redundancy and service robustness concerns often preclude consolidation of mission critical services low utilization creates an energy efficiency challenge be cause conventional servers are notoriously inefficient at low loads although power saving features like clock gating and busy period ms idle period ms figure busy and idle period cumulative distributions table fine grain utilization traces workload utilization avg interval description busy idle dynamic voltage and frequency scaling dvfs nearly elim inate processor power consumption in idle systems present day servers still dissipate about as much power when idle as when fully loaded processors often account for only a quarter of system power main memory and cool ing fans contribute larger fractions figure reproduces typical server power breakdowns for the ibm sun ultrasparc and a generic server specified by google respectively frequent brief utilization clearly eliminating server idle power waste is critical to im proving data center energy efficiency engineers have been successful in reducing idle power in mobile platforms such as cell phones and laptops however servers pose a funda mentally different challenge than these platforms the key observation underlying our work is that although servers have low utilization their activity occurs in frequent brief bursts as a result they appear to be under a constant light load to investigate the time scale of servers idle and busy peri ods we have instrumented a series of interactive and batch processing servers to collect utilization traces at gran ularity to our knowledge our study is the first to report server utilization data measured at such fine granularity we classify an interval as busy or idle based on how the os scheduler accounted the period in its utilization tracking the traces were collected over a period of a week from seven departmental it servers and a scientific computing cluster comprising over servers we present the mean idle and busy period lengths average utilization and a brief descrip tion of each trace in table figure shows the cumulative distribution for the busy and idle period lengths in each trace the key result of our traces is that the vast majority of idle periods are shorter than with mean lengths in the of milliseconds busy periods are even shorter typically only of milliseconds existing energy conservation techniques the rapid transitions and brief intervals of server activity make it difficult to conserve idle power with existing ap proaches the recent trend towards server consolidation is partly motivated by the high energy cost of idle sys tems by moving services to virtual machines several ser vices can be time multiplexed on a single physical server increasing average utilization consolidation allows the to tal number of physical servers to be reduced thereby re ducing idle inefficiency however server consolidation by itself does not close the gap between peak and average uti lization data centers still require sufficient capacity for peak demand which inevitably leaves some servers idle in the av erage case furthermore consolidation does not save energy automatically system administrators must actively consoli date services and remove unneeded systems although support for sleep states is widespread in handheld laptop and desktop machines these states are rarely used in current server systems unfortunately the high restart la tency typical of current sleep states renders them unaccept packet server operates at full performance to finish existing work system components nap while server is idle the nic detects the arrival of work server returns to full performance to finish work as quickly as possible figure powernap able for interactive services current laptops and desktops require several seconds to suspend using operating system interfaces e g acpi moreover unlike consumer devices servers cannot rely on the user to transition between power states they must have an autonomous mechanism that man ages state transitions recent server processors include cpu throttling solutions e g intel speedstep amd cool n quiet to reduce the large overhead of light loads these processors use dvfs to reduce their operating frequency linearly while gaining cubic power savings dvfs relies on operating system sup port to tune processor frequency to instantaneous load in linux the kernel continues lowering frequency until it ob serves idle time improving dvfs control algorithms remains an active research area nonetheless dvfs can be highly effective in reducing cpu power however as figure shows cpus account for a small portion of total system power energy proportional computing seeks to extend the suc cess of dvfs to the entire system in this scheme each sys tem component is redesigned to consume energy in propor tion to utilization in an energy proportional system explicit power management is unnecessary as power consumption varies naturally with utilization however as many compo nents incur fixed power overheads when active e g clock power on synchronous memory busses leakage power in cpus etc designing energy proportional subsystems re mains a research challenge energy proportional operation can be approximated with non energy proportional systems through dynamic virtual machine consolidation over a large server ensemble however such approaches do not address the performance isolation concerns of dynamic consolidation and operate at coarse time scales minutes hence they cannot exploit the brief idle periods found in servers powernap although servers spend most of their time idle conven tional energy conservation techniques are unable to exploit these brief idle periods hence we propose an approach to power management that enables the entire system to tran sition rapidly into and out of a low power state where all activity is suspended until new work arrives we call our ap proach powernap figure illustrates the powernap concept each time the server exhausts all pending work it transitions to the nap state in this state nearly all system components enter sleep modes which are already available in many components see section while in the nap state power consumption is low but no processing can occur system components that signal the arrival of new work expiration of a software timer or environmental changes remain partially powered when new work arrives the system wakes and transitions back to the active state when the work is complete the system returns to the nap state powernap is simpler than many other energy conservation schemes because it requires system components to support only two operating modes an active mode that provides maximum performance and a nap mode that minimizes power draw for many devices providing a low power nap mode is far easier than providing multiple active modes that trade performance for power savings any level of activity often implies fixed power overheads e g bus clock switch ing power distribution losses leakage power mechanical components etc we outline mechanisms required to im plement powernap in section powernap performance and power model to assess powernap potential we develop a queuing model that relates its key performance measures energy savings and response time penalty to workload parameters and powernap implementation characteristics we contrast powernap with a model of the upper bound energy savings possible with dvfs the goal of our model is threefold to gain insight into powernap behavior to derive re quirements for powernap implementations and to con trast powernap and dvfs work in queue active x suspend wake work in queue xx arrival arrival x x arrival time arrival arrival arrival time powernap b dvfs figure powernap and dvfs analytic models we model both powernap and dvfs under the assump tion that each seeks to minimize the energy required to serve the offered load hence both schemes provide iden tt f λe λt dt rtt t tt λe λtdt tical throughput matching the offered load but differ in re nap e s e i sponse time and energy consumption powernap model we model powernap as an m g queu ing system with arrival rate λ and a generalized service time distribution with known first and second moments e s and e figure a shows the work in the queue for three λe s λ e λtt λe s λe i the response time for an m g server with exceptional first service is due to welch job arrivals note that in this context work also includes time spent in the wake and suspend states average server λe s λe s i λe i λe i utilization is given by ρ λe s to model the effects of powernap suspend and wake transitions we extend the conventional m g model with an exceptional first service time we assume powernap transitions are symmetric with latency tt service of the first job in each busy period is delayed by an initial setup time i the setup time includes the wake transition and may include the remaining portion of a suspend transition as shown for the rightmost arrival in figure a hence for an arrival x time units from the start of the preceding idle period the initial setup time is given by i x if x tt tt if x tt the first and second moments e i and e are note that the first term of e r is the pollaczek khinchin formula for the expected queuing delay in a standard m g queue the second term is additional residual delay caused by the initial setup time i and the final term is the expected service time e s the second term vanishes when tt dvfs model rather than model a real dvfs frequency control algorithm we instead model the upper bound of en ergy savings possible with dvfs for each job arrival we scale instantaneous frequency f to stretch the job to fill any idle time until the next job arrival as illustrated in fig ure b which gives e f fmaxρ this scheme maxi mizes power savings but cannot be implemented in prac tice because it requires knowledge of future arrival times we base power savings estimates on the theoretical formu lation of processor dynamic power consumption pcp u cv we assume c and a are fixed and choose the op e i r iλe λxdx e λtt timal f for each job within the range fmin f fmax e t λ λ λxdx we impose a lower bound fmin fmax to prevent re sponse time from growing asymptotically when utilization is low we chose a factor of between fmin and fmax λtt based on the frequency range provided by a ghz amd t t athlon we assume voltage scales linearly with frequency λ t we compute average power as pavg pnap fnap pmax fnap where the fraction of time spent napping fnap is given by i e v vmax f fmax which is optimistic with respect to current dvfs implementations finally as dvfs only re duces the cpu contribution to system power we include a parameter fcp u to control the fraction of total system power affected by dvfs under these assumptions average power pavg is given by the ratio of the expected length of each nap period e n to the expected busy idle cycle length e c pavg pmax fcp u e f fmax dvfs dvfs dvfs fcpu fcpu fcpu powernap tt ms powernap tt ms powernap tt ms utilization power scaling utilization response time scaling figure powernap and dvfs power and response time scaling response time is given by e r e rbase l where rbase is the response time without dvfs analysis power savings figure a shows the average power as a fraction of peak required under powernap and dvfs as a function of utilization for dvfs we show power sav ings for three values of fcp u fcp u represents the upper bound if dvfs were applicable to all system power fcp u bound the typical range in cur rent servers for powernap we construct the graphs with e and e which are both esti mated from the observed busy period distribution in our web trace we assume pnap is of pmax we vary λ to adjust utilization and present results for three values of tt and we expect to be a conservative esti mate for achievable powernap transition time for transition times below transition time becomes negligible and the power savings from powernap varies linearly with utiliza tion for all workloads we discuss transition times further in section when fcp u is high dvfs clearly outperforms powernap as it provides cubic power savings while powernap sav ings are at best linear in utilization however for realistic values of fcp u and transition times in our expected range tt powernap savings rapidly overtake dvfs as transition time increases the break even point between dvfs and powernap shifts towards lower utilization even for a transition time of ms powernap can provide sub stantial energy savings when utilization is below table per workload energy savings response time in figure b we compare the response time impact of dvfs and powernap the vertical axis shows response time normalized to a system without power management i e that always operates at fmax for dvfs response time grows rapidly when the gap between job ar rivals is large and reaches the fmin floor below utiliza tion dvfs response time penalty is independent of fcp u and is bounded at by the ratio of fmax fmin for power nap the response time penalty is negligible if tt is small relative to average service time e s which we expect to be the common case i e most jobs last longer than however if tt is significant relative to e s the powernap response time penalty grows as utilization shrinks when utilization is high the server is rarely idle and few jobs are delayed by transitions as utilization drops the additional delay seen by each job converges to tt i e every job must wait for wake up per workload energy savings finally we report the en ergy savings under simulated powernap and dvfs schemes for our workload traces because these traces only contain busy and idle periods and not individual job arrivals we cannot estimate response time impact for each workload we perform a trace based simulation that assumes busy pe riods will start at the same time independent of the current powernap state i e new work still arrives during wake or suspend transitions we assume a powernap transition time of and nap power at of active power which we be lieve to be conservative estimates see section for dvfs we assume fcp u table shows the results of these simulations all workloads except mail and cluster hit the dvfs frequency floor and hence achieve a energy savings in all cases powernap achieves greater energy sav ings additionally we extracted the average arrival rate as suming a poisson arrival process and compared the results in table with the m g model of fnap derived above we found that for these traces the analytic model was within of our simulated results in all cases when arrivals are more deterministic e g backup than the exponential we assume the model slightly overestimates powernap savings for more variable arrival processes e g shell the model underestimates the energy savings implementation requirements based on the results of our analytic model we identify two key powernap implementation requirements fast transitions our model demonstrates that transition speed is the dominant factor in determining both the power savings potential and response time impact of powernap our results show that transition time must be less than one tenth of average busy period length although a transition speed is sufficient to obtain significant savings transitions are necessary for powernap overheads to become negligible to achieve these transition periods a powernap implementation must preserve volatile system state e g memory while napping mass storage devices transfer rates are insufficient to transfer multiple gb of memory state in milliseconds minimizing power draw in nap state given the low uti lization in most enterprise deployments servers will spend a majority of time in the nap state making powernap power requirements the key factor affecting average sys tem power hence it is critical to minimize the power draw of napping system components as a result of eliminating idle power powernap drastically increases the range be tween the minimum and maximum power demands on a blade chassis existing blade chassis power conversion sys tems are inefficient in the common case where all blades are napping hence to maximize powernap potential we must re architect the blade chassis power subsystem to increase its efficiency at low loads although powernap requires system wide modifications it demands only two states from each subsystem active and nap states hence implementing powernap is substantially simpler than developing energy proportional components because no computation occurs while napping many fixed table component power consumption power power draws such as clocks and leakage power can be conserved powernap mechanisms we outline the design of a powernap enabled blade server system and enumerate required implementation mecha nisms powernap requires nap support in all hardware sub systems that have non negligible idle power draws and soft ware firmware support to identify and maximize idle periods and manage state transitions hardware mechanisms most of the hardware mechanisms required by powernap already exist in components designed for mobile devices however few of these mechanisms are exploited in existing servers and some are omitted in current generation server class components for each hardware subsystem we identify existing mechanisms or outline requirements for new mech anisms necessary to implement powernap furthermore we provide estimates of power dissipation while napping and transition speed we summarize these estimates along with our sources in table our estimates for a typical blade are based on hp c series half height blade designs our powernap power estimate assumes a two cpu system with eight dram dimms processor acpi sleep state the acpi standard de fines the sleep state for processors that is intended to allow low latency transitions although the acpi standard does not specify power or performance requirements some implementations of are ideal for powernap for exam ple in intel mobile processor line preserves last level cache state and consumes only these processors require approximately µs for pll stabilization to transi tion from sleep back to active execution if is unavailable clock gating can provide substantial en ergy savings for example intel xeon series power requirements drop from to upon executing a halt instruction from this state resuming execution re quires only nanosecond scale delays dram self refresh dram is typically the second most power hungry system component when active however several recent dram specifications feature an operating mode called self refresh where the dram is isolated from the memory controller and autonomously refreshes dram content in this mode the memory bus clock and plls are disabled as are most of the dram interface circuitry self refresh saves more than an order of magnitude of power for example a sodimm designed for laptops with a peak power draw above uses only of power during self refresh transitions into and out of self refresh can be completed in less than a microsecond mass storage solid state disks solid state disks draw negligible power when idle and hence do not need to tran sition to a sleep state for powernap a recent sam sung ssd consumes only while idle network interface wake on lan the key responsibility powernap demands of the network interface card nic is to wake the system upon arrival of a packet existing nics already provide support for wake on lan to perform this function current implementations of wake on lan pro vide a mode to wake on any physical activity this mode forms a basis for powernap support current nics consume only while in this mode environmental monitoring service processors power nap transition management servers typically include additional circuitry for environmental monitoring remote management e g remote power on power capping power regulation and other functionality these components typ ically manage acpi state transitions and would coordinate powernap transitions a typical service processor draws less than when idle fans variable speed operation fans are a dominant power consumer in many recent servers modern servers employ variable speed fans where cooling capacity is con stantly tuned based on observed temperature or power draw fan power requirements typically grow cubically with aver age power thus powernap average power savings yield massive reductions in fan power requirements in most blade designs cooling systems are centralized in the blade chassis amortizing their energy cost over many blades because ther mal conduction progresses at drastically different timescales than powernap transition frequency chassis level fan con trol is independent of powernap state i e fans may con tinue operating during nap and may spin down during active operation depending on temperature conditions power provisioning rails powernap fundamentally al ters the range of currents over which a blade chassis must ef ficiently supply power in section we explain why conven tional power delivery schemes are unable to provide efficient ac to dc conversion over this range and present rails our power conversion solution software mechanisms for schemes like powernap the periodic timer interrupt used by legacy os kernels to track the passage of time and implement software timers poses a challenge as the timer interrupt is triggered every conventional os time keep ing precludes the use of powernap the periodic clock tick also poses a challenge for idle power conservation on lap tops and for virtualization platforms that consolidate hun dreds of os images on a single hardware platform hence the linux kernel has recently been enhanced to support tickless operation where the periodic timer interrupt is es chewed in favor of hardware timers for scheduling and time keeping powernap depends on a kernel that provides tickless operation powernap effectiveness increases with longer idle periods and less frequent state transitions some existing hardware devices e g legacy keyboard controllers require polling to detect input events current operating systems often per form maintenance tasks e g flushing disk buffers zeroing memory when the os detects significant idle periods these maintenance tasks may interact poorly with powernap and can induce additional state transitions however efforts are already underway e g as described in to redesign de vice drivers and improve background task scheduling rails ac to dc conversion losses in computer systems have re cently become a major concern leading to a variety of re search proposals product announcements e g hp blade system and standardization efforts to im prove power supply efficiency the concern is particularly acute in data centers where each watt wasted in the power delivery infrastructure implies even more loss in cooling because powernap power draw is substantially lower than the idle power in conventional servers powernap demands conversion efficiency over a wide power range from as few as to as much as in a fully populated enclo sure in this section we discuss why existing power solutions are inadequate for powernap and present rails our power solution rails provides high conversion efficiency across powernap power demand spectrum provides n redun dancy allows for graceful degradation of compute capacity when psus fail and minimizes costs by using commodity psus in an efficient arrangement power supply unit background poor efficiency at low loads although manufacturers of ten report only a single efficiency value most psus do not have a constant efficiency across electrical load a recent survey of server and desktop psus reported their efficiency across loads figure reproduces the range of efficien cies reported in that study though psus are often over red yellow green furthermore despite their name the specification does not require energy efficiency above across all loads rather only within the typical operating range of conventional systems this specified efficiency range is not wide enough for powernap single voltage supplies unlike desktop machines which require five different dc output voltages to support legacy components server psus typically provide only a single dc output voltage simplifying their design and improving reliability and efficiency although power load nap benefits from this feature a single output voltage does not directly address inefficiency at low loads figure power supply efficiency efficient at their optimal operating point usually near load efficiency drops off rapidly below load some times dipping below i e in for out we divide the operating efficiency of power supplies into three zones based on electrical load above load the psus operate in the green zone where their efficiency is at or above in the yellow zone psu efficiency begins to drop but typically exceeds however in the red zone below efficiency drops off precipitously two factors cause servers to frequently operate in the yel low or red efficiency zones first servers are highly con figurable which leads to a large range of power require ments the same server model might be sold with only one or as many as disks installed and the amount of installed dram might vary by a factor of furthermore peripher als may be added after the system is assembled to simplify ordering upgrades testing and safety certification manu facturers typically install a power supply rated to exceed the power requirements of the most extreme configuration sec ond servers are often configured with redundant power supplies i e twice as many as are required for a worst case configuration the redundant supplies typically share the electrical load to minimize psu temperature and to ensure current flow remains uninterrupted if a psu fails however the epri study concluded that this load sharing arrange ment often shifts psus from yellow zone to red zone operation recent efficiency improvements a variety of recent ini tiatives seek to improve server power efficiency certification the epa energy star program has defined the certification standard to incen tivize psu manufacturers to improve efficiency at low loads the incentive program is primarily targeted at the low peak power desktop psu market supplies require considerably higher design complexity than con ventional psus which may pose a barrier to widespread adoption in the reliability conscious server psu market dc distribution recent research has called for dis tributing dc power among data center racks eliminating ac to dc conversion efficiency concerns at the blade en closure level however the efficiency advantages of dc distribution are unclear and deploying dc power will require multi industry coordination dynamic load sharing blade enclosures create a fur ther opportunity to improve efficiency through dynamic load sharing hp dynamic power saver feature in the hp blade center employs up to six high efficiency psus in a single enclosure and dy namically varies the number of psus that are engaged ensuring that all active supplies operate in their green zone while maintaining redundancy although hp so lution is ideal for the idle and peak power range of the c class blades it requires expensive psus and provides insufficient granularity for powernap while all these solutions improve efficiency for their target markets none achieve all our goals of efficiency for power nap redundancy and low cost rails design we introduce a new power delivery solution tuned for powernap the redundant array for inexpensive load shar ing rails the central idea of our scheme is to load share over multiple inexpensive small psus to provide the efficiency and reliability of larger more expensive units through intelligent sizing and load sharing we ensure that active psus operate in their efficiency sweet spots our scheme provides efficiency and enterprise class redun dancy with commodity components rails targets three key objectives efficiency across the entire powernap dynamic power range n reliability and graceful degradation of compute capacity under multiple psu failure and minimal cost figure illustrates rails as in conventional blade en closures power is provided by multiple psus connected in loadn maximum output w figure rails psu design parallel a conventional load sharing control circuit contin uously monitors and controls the psus to ensure load is di vided evenly among them as in dynamic smart power rails disables and electrically isolates psus that are not necessary to supply the load however our key departure from prior designs is in the granularity of the individual psus we select psus from the economic sweet spot of the high sales volume market for low wattage commodity sup plies we choose a power supply granularity to satisfy two criteria a single supply must be operating in its green zone when all blades are napping this criterion establishes an upper bound on the psu capacity based on the minimum chassis power draw when all blades are napping subject to this bound we size psus to match the incremental power draw of activating a blade thus as each blade awakens one additional psu is brought on line because of intelligent sizing each of these psus will operate in their optimal efficiency region whereas current blade servers use multi kilowatt psus a typical rails psu might supply rails meets its cost goals by incorporating high volume commodity components although the form factor of com modity psus may prove awkward for rack mount blade en closures precluding the use of off the shelf psus the power density of high sales volume psus differs little from high end server supplies hence with appropriate mechanical modifications it is possible to pack rails psus in roughly the same physical volume as conventional blade enclosure power systems rails meets its reliability goals by providing fine grain degradation of the system peak power capacity as psus fail in any n design the first psu failure does not af fect compute capacity however in conventional blade en closures a subsequent failure may force shutdown of several possibly all blades multiple failure tolerance typically re quires redundancy which is expensive in contrast in rails where psu capacity is matched to the active power figure power supply pricing draw of a single blade the second and subsequent failures each require the shutdown of only one blade evaluation we evaluate the power efficiency and cost of powernap with four power supply designs commodity supplies commod ity high efficiency supplies dynamic load sharing dynamic and rails rails we evalu ate all four designs in the context of a powernap enabled blade system similar to hp blade center we as sume a fully populated chassis with half height blades each blade consumes at peak at idle without powernap and in powernap see table we as sume the blade enclosure draws we neglect any vari ation in chassis power as a function of the number of active blades the non rails systems employ psus sufficient to provide n redundancy the rails design uses psus we assume the average efficiency char acteristic from figure for commodity psus cost server components are sold in relatively low vol umes compared to desktop or embedded products and thus command premium prices some internet companies e g google have eschewed enterprise servers and instead as semble systems from commodity components to avoid these premiums psus present another opportunity to capitalize on low cost commodity components because desktop atx psus are sold in massive volumes their constituent compo nents are cheap a moderately sized supply can be obtained at extremely low cost figure shows a survey of psu prices in watts per dollar for a wide range of psus across market segments price per watt increases rapidly with power deliv ery capacity this rise can be attributed to the proportional increase in required size for power components such as in ductors and capacitors also the price of discrete power components grows with size and maximum current rating presently the market sweet spot is around supplies both and blade server psus are substantially more ex table relative psu density microatx atx custom blade density normalized w vol pensive than commodity parts because rails uses com modity psus with small maximum outputs it takes advan tage of psu market economics making rails far cheaper than proprietary blade psus power density in data centers rack space is at a premium and hence the physical volume occupied by a blade en closure is a key concern rails drastically increases the number of distinct psus in the enclosure but each psu is individually smaller to confirm the feasibility of rails we have compared the highest power density available in commodity psus which conform to one of several stan dard form factors with that of psus designed for blade cen ters which may have arbitrary dimensions table com pares the power density of two commodity form factors with the power density of hp psus we report density in terms of watts per unit volume normalized to the volume of one atx power supply the highly compact microatx form factor exhibits the worst power density these units have been optimized for small dimensions but are employed in small form factor devices that do not require high peak power though they are not designed for density commod ity atx supplies are only less dense than enterprise class supplies furthermore as rails requires only a single output voltage eliminating the need for many of a standard atx psu components we conclude that rails psus fit within blade enclosure volumetric constraints power savings and energy efficiency to evaluate each power system we calculate expected power draw and con version efficiency across blade ensemble utilizations as noted in section low average utilization manifests as brief bursts of activity where a subset of blades draw near peak power the efficiency of each power delivery solution de pends on how long blades are active and how many are simultaneously active for each utilization we construct a probability mass function for the number of simultaneously active blades assuming utilization across blades is uncorre lated hence the number of active blades follows a bino mial distribution from the distribution of active blades we compute an expected power draw and determine conversion losses from the power supply efficiency versus load curve we obtain efficiency curves from the energy star bronze specification for psus and for commodity psus figure compares the relative efficiency of powernap un der each power delivery solution using commodity com modity or high efficiency psus results in the low est efficiency as powernap low power draw will operate these power supplies in the red zone rails rails utilization figure power delivery solution comparison and dynamic load sharing dynamic both improve psu performance because they increase average psu load rails outperforms all of the other options because its fine grain sizing best matches powernap requirements conclusion we presented powernap a method for eliminating idle power in servers by quickly transitioning in and out of an ultra low power state we have constructed an analytic model to demonstrate that for typical server workloads powernap far exceeds dvfs power savings potential with better response time because of powernap unique power requirements we introduced rails a novel power delivery system that improves power conversion efficiency provides graceful degradation in the event of psu failures and re duces costs to conclude we present a projection of the effectiveness of powernap with rails in real commercial deployments we construct our projections using the commercial high density server utilization traces described in table ta ble presents the power requirements energy conversion ef ficiency and total power costs for three server configurations an unmodified modern blade center such as the hp a powernap enabled system with large conventional psus powernap and powernap with rails the power costs include the estimated purchase price of the power delivery system conventional high wattage psus or rails year power costs assuming california commercial rate of cents kwh and a cooling burden of per of it equipment powernap yields a striking reduction in average power rela tive to blade of nearly for web servers improving the power system with rails shaves another our total power cost estimates demonstrate the true value of power nap with rails our solution provides power cost reduc tions of nearly for web servers and for enter prise it power aware speed scaling in processor sharing systems adam wierman computer science department california institute of technology lachlan l h andrew centre for advanced internet architectures swinburne university of technology australia ao tang school of ece cornell university abstract energy use of computer communications systems has quickly become a vital design consideration one effective method for reducing energy consumption is dynamic speed scaling which adapts the processing speed to the current load this paper studies how to optimally scale speed to balance mean response time and mean energy consumption under processor sharing scheduling both bounds and asymptotics for the optimal speed scaling scheme are provided these results show that a simple scheme that halts when the system is idle and uses a static rate while the system is busy provides nearly the same performance as the optimal dynamic speed scaling however the results also highlight that dynamic speed scaling provide at least one key benefit significantly improved robustness to bursty traffic and mis estimation of workload parameters introduction power management is increasingly important in computer communications systems not only is the energy consumption of the internet becoming a significant fraction of the energy consumption of developed countries but cooling is also becoming a major concern consequently there is an important tradeoff in modern system design between reducing energy use and maintaining good performance there is an extensive literature on power management reviewed in a common technique which is the focus of the current paper is dynamic speed scaling this dynamically reduces the processing speed at times of low workload since processing more slowly uses less energy per operation this is now common in many chip designs in particular speed scaling has been proposed for many network devices such as switch fabrics tcp offload engines and ofdm modulation clocks this paper studies the efficacy of dynamic speed scaling analytically the goal is twofold i to elucidate the structure of the optimal speed scaling scheme e g how should the speed depend on the current workload ii to compare the performance of dynamic speed scaling designs with that of designs that use static processing speeds e g how much improvement does dynamic speed scaling provide there are many analytic studies of speed scaling designs beginning with yao et al the focus has been on either the goal of minimizing the total energy used in order to complete arriving jobs by their deadlines e g or the goal of minimizing the average response time of jobs i e the time between their arrival and their completion of service given a set energy heat budget e g web settings typically have neither job completion deadlines nor fixed energy budgets instead the goal is to optimize a tradeoff between energy consumption and mean response time this model is the focus of the current paper in particular the performance metric considered is e t e e βt where t is the response time of a job e is the expected energy expended on that job and βt controls the relative cost of delay this performance metric has attracted attention recently the related analytic work falls into two categories worst case analyses and stochastic analyses the former provide specific simple speed scalings guaranteed to be within a constant factor of the optimal performance regardless of the workload e g in contrast stochastic re sults have focused on service rate control in the m m model under first come first served fcfs scheduling which can be solved numerically using dynamic programming one such approach is reviewed in section iii c unfortunately the structural insight obtained from stochastic models has been limited our work extends the stochastic analysis of dynamic speed scaling we focus on the m gi queue under processor sharing ps scheduling which serves all jobs currently in the system at equal rates we focus on ps because it is a tractable model of current scheduling policies in cpus web servers routers etc based on the model section ii and the speed scaling we consider section iii our analysis makes three main contributions we provide bounds on the performance of dynamic speed scaling section iv a surprisingly these bounds show that even an idealized version of dynamic speed scaling im proves performance only marginally compared to a simple scheme where the server uses a static speed when busy and speed when idle at most a factor of for typical pa rameters and often less see section v counterintuitively these bounds also show that the power optimized response time remains bounded as the load grows we provide bounds and asymptotics for the speeds used by the optimal dynamic speed scaling scheme sections iv b and iv c these results provide insight into how the speeds scale with the arriving load the queue length and the relative cost of energy further they uncover a connection between the optimal stochastic policy and results from the worst case community section iv we illustrate through analytic results and numerical experi ments that though dynamic speed scaling provides limited performance gains it dramatically improves robustness to mis estimation of workload parameters and bursty traffic section vi model and notation in order to study the performance of dynamic speed scaling we focus on a simple model an m gi ps queue with controllable service rates dependent on the queue length in this model jobs arrive to the server as a poisson process with rate λ have intrinsic sizes with mean µ and depart at rate snµ when there are n jobs in the system under static schemes the constant service rate is denoted by define the load as ρ λ µ and note that this ρ is not the fraction of time the server is busy the performance metric we consider is e t e e βt where t is the response time of a job and e is the energy expended on a job it is often convenient to work with the expected cost per unit time instead of per job by little law this can be written as z e n λe f βt where n is the number of jobs in the system and f determines the power used when running at speed the remaining piece of the model is to define the form of f prior literature with the notable exception of has typically assumed that f is convex and often that f is a polynomial specifically a cubic that is because the dynamic power of cmos is proportional to v where v is the supply voltage and f is the clock frequency operating at a higher frequency requires dynamic voltage scaling dvs to a higher voltage nominally with v f yielding a cubic relationship to validate the polynomial form of f we consider data from real nm chips in fig the voltage versus speed data comes from the intel pxa pentium m pro cessor and the tcp offload engine studied in specifically the nbb trace at c in fig interestingly the dynamic power use of real chips is well modeled by a polynomial scaling of speed to power but this polynomial is far from cubic in fact it is closer to quadratic indicating that the voltage is scaled down less aggressively than linearly with speed as a result we will model the power used by running log freq fig dynamic power for an intel pxa a tcp offload engine and a pentium m the slopes of the fitted lines are and respectively the scope of this paper and we leave the question of including both leakage and dynamic power for future work power aware speed selection when provisioning processing speed in a power aware manner there are three natural thresholds in the capability of the server static provisioning the server uses a constant static speed which is determined based on workload charac teristics so as to balance energy use and response time gated static provisioning the server gates its clock setting if no jobs are present and if jobs are present it works at a constant rate chosen to balance energy use and response time dynamic speed scaling the server adapts its speed to the current number of requests present in the system the goal of this paper is to understand how to choose optimal speeds in each of these scenarios and to contrast the relative merits of each scheme clearly the expected cost is reduced each time the server is allowed to adjust its speed more dynamically this must be traded against the costs of switching such as a delay of up to tens of microseconds to change speeds the important question is what is the magnitude of improvement at each level for our comparison at speed by λ f βt sα β we will use idealized versions of each scheme in particular in each case we will assume that the server can be run at any desired speed in and ignore switching costs where α and β takes the role of βt but has dimension time α the cost per unit time then becomes sα thus in particular the dynamic speed scaling is a significant idealization of what is possible in practice however our results will suggest that it provides very little performance z e n β improvement over an ideally tuned gated static scheme in this section we will derive expressions for the optimal we will often focus on the case of α to provide intuition clearly this is an idealized model since in reality only a few discrete speeds can be used the impact of the workload parameters ρ β and α can often be captured using one simple parameter γ ρ α which is a dimensionless measure thus we will state our results in terms of γ to simplify their form also it will often be convenient to use the the dimensionless unit of speed α though we focus on dynamic power in this paper it should be noted that leakage power is increasingly important it represents of the power use of current and near future chips however analytic models for leakage are much less understood and so including leakage in our analysis is beyond speeds in cases i and ii for case iii we will describe a numerical approach for calculating the optimal speeds which is due to george and harrison though this numerical approach is efficient it provides little structural insight into the structure of the dynamic speeds or the overall performance providing such results will be the focus of section iv the optimal static speed the simplest system to manage power is one which selects an optimal speed and then always runs the processor at that speed this case which we call pure static is the least power aware scenario we consider and will be used simply as a benchmark for comparison even when the speed is static the optimal design can be power aware since the optimal speed can be chosen so that it trades off the cost of response time and energy appropriately in particular we can write the cost per unit time as z ρ β then differentiating and solving for the minimizer gives that the optimum occurs when ρ and sα ρ βρ α the optimal static speed for a gated system the next simplest system is when the processor is allowed two states halted or processing we model this situation with a server that runs at a constant rate except when there are no jobs in the system at which point it sets using zero dynamic power to determine the optimal static speed we proceed as we did in the previous section if the server can gate its clock the energy cost is only incurred during the fraction of time the server is busy ρ the cost per unit time then becomes optimal dynamic speed scaling a popular alternative to static power management is to allow the speed to adjust dynamically to the number of requests in the system the task of designing an optimal dynamic speed scaling scheme in our model can be viewed as a stochastic control problem we start with the following observation which simplifies the problem dramatically an m gi ps system is well known to be insensitive to the job size distribution this still holds when the service rate is queue length dependent since the policy still falls into the class of symmetric policies introduced by kelly as a result the mean response time and entire queue length distribution are affected by the service distribution through only its mean thus we can consider an m m ps system further the mean response time and entire queue length distribution are equivalent under all non size based service distributions in the m m queue thus to determine the optimal dynamic speed scaling scheme for an m gi ps queue we need only consider an m m fcfs queue the service rate control problem in the m m fcfs z ρ ρ sα β queue has been studied extensively in partic ular george and harrison provide an elegant solution to the problem of selecting the state dependent processing speeds the optimum occurs when ρ and to minimize a weighted sum of an arbitrary holding cost with a processing speed cost specifically the optimal state dz ds ρ ρ α sα ρ β dependent processing speeds can be framed as the solution to a stochastic dynamic program to which provides an efficient numerical solution in the remainder of this section which is solved when α sα ρ β the optimal speed can be solved for explicitly for some α for example when α sgs ρ β in general define g γ α σ t σ γ α σα γ σ with this notation the optimal static speed for a server which gates its clock is sgs αg γ α we call this policy the we will provide an overview of this numerical approach the core of this approach will form the basis of our derivation of bounds on the optimal speeds in section iv we will describe the algorithm of specialized to the case considered in this paper where the holding cost in state n is simply n further we will generalize the description to allow arbitrary arrival rates λ the solution starts with an estimate z of the minimal cost per unit time including both the occupancy cost and the energy cost as in the minimum cost of returning from state n to the empty system is given by the dynamic program gated static policy and denote the corresponding cost zgs v inf λ f n zl the following lemma bounds g the proof is deferred to appendix a lemma for α n a λ µs βt µs λ µs vn λ λ µs vn i α α where a is the set of available speeds we will usually assume γ α g γ α α γ α a with the substitution un λ vn vn this can be written as and the inequalities are reversed for α note that the first inequality becomes tight for γα and u sup z n λ f sun the second becomes tight for γα further when α n a βt ρ both become equalities giving g γ γ and and the conditions on α here have been corrected since the version presented at infocom two additional functions are defined first φ u sup ux ρ λf x βt x a second the minimum value of x which achieves this supre mum normalized to be dimensionless is ψ u β α min x ux ρ λf x βt φ u note that under is constant competitive i e in the worst case the total cost is within a constant of optimal this matches the asymptotic behavior of the bounds for α for large n this behavior can also be observed for general α lemma and theorem bounds on cost φ u α u α α αγ ψ u u α αγ we start the analysis by providing bounds on z in this subsection and then using the bounds on z to bound n above given the estimate of z un satisfy and below sections iv b and iv c recall that zgs is the total cost under gated static z un φ un n z theorem max γα γα α α the optimal value of z can be found as the minimum value such that un n is an increasing sequence this allows z to z zgs γ g γ α γ γg γ α α be found by an efficient binary search after which un can in principle be found recursively the optimal speed in state n is then given by proof the optimal cost z is bounded above by the cost of the gated static policy which is simply n ψ u α n zgs γ g γ α γ γg γ α α this highlights the fact that γ ρ α provides the appro priate scaling of the workload information because the cost z normalized speed sβ α and variables un depend on λ µ two lower bounds can be obtained as follows in order to maintain stability the time average speed must satisfy e ρ but z e sα β e α β by jensen inequality and the convexity of α thus and β only through γ note that this forward approach advocated in is numerically unstable appendix b we suggest that a more e sα ρα z β β γα stable way to calculate un is to start with a guess for large n and work backwards errors in the initial guess decay exponentially as n decreases and are much smaller than the accumulated roundoff errors of the forward approach this for small loads this bound is quite loose another bound comes from considering the minimum cost of processing a single job of size x with no waiting time or processor sharing it is optimal to serve the job at a constant rate thus backward approach is made possible by the bounds we derive in section iv z ex min x sα x l bounds on optimal dynamic speed scaling in the prior section we presented the optimal designs for the cases of static gated static and dynamic speed scaling in the first two cases the optimal speeds were presented more or less explicitly however in the third case we presented only a recursive numerical algorithm for determining the optimal dynamic speed scaling even though this approach provides an efficient means to calculate n it is difficult to gain insight into system design in this section we provide results exhibiting the structure of the optimal dynamic speeds and the performance they achieve the main results of this section are summarized in table i the bounds on z for arbitrary α are essentially tight i e agree to leading order in the limits of small or large γ due the right hand side is minimized for β α α independent of x giving z ρβ αα α α thus z max γα γα α α the form of the bounds on z are complicated so it is useful to look at the particular case of α corollary for α gated static has cost within a factor of of optimal specifically max z zgs proof for α g γ γ hence gives γ to the complicated form of the general results we illustrate the bounds for the specific case of α to provide insight in zgs γ γ γ γ particular it is easy to see the behavior of sn and z as a func tion of γ and n in the case of α this leads to interesting observations for example it illustrates a connection between the optimal stochastic policy and policies analyzed in the worst case model in particular bansal pruhs and stein showed that when nothing is known about future arrivals a policy that gives speeds of the form sn n α α which establishes the upper bound the lower bound follows from substituting α into z max the ratio of zgs to the lower bound on z has a maximum value of at γ and hence gated static is within a factor of of the true optimal scheme table i bounds on total costs and speed as a function of the number n of jobs in the system for any α max γα γα α α z γ γg γ α α theorem g γ α γ n n σα γα γ α where σn satisfies σα α σn αγ n γ g γ α γ γg γ α α for α max z corollary p n γ γ for α and n a lower bound on sn results from linear interpolation between max γ at n and γ at n it is perhaps surprising that such an idealized version of dynamic speed scaling provides such a small magnitude of improvement over a simplistic policy such as gated static in fact the bound of is very loose when γ is large or small further empirically the maximum ratios for typical α are below see fig thus there is little to be gained by unrolling the dynamic program gives a joint minimization over all sn u ρ min sα β n z sn sn ρ min sα β n z u l dynamic scaling in terms of mean cost however section vi shows that dynamic scaling dramatically improves robustness sn sn tti n ρ n remains bounded as the arrival rate λ grows specifically by an upper bound can be found by taking any possibly suboptimal choice of sn i for i and bounding the z e β optimal z taking si α for all i n gives e t λ λ µ β min n σα z γ l upper bounds on the optimal dynamic speeds γ σ o γ σ γ we now move to providing upper bounds on the optimal dynamic speed scaling scheme theorem for all n and α since z γα from equation follows with this establishes for n holds since otherwise it follows from the inequality σα n γn α α n γ α and the fact that n α n σα γα by specializing to the case when α we can provide un γ for all σ whence o γ σ γ some intuition for the upper bound on the speeds corollary for α n γ γ α α σ γ o γ σ γ proof factoring the difference of squares in the first term in particular for σ γ α of and canceling with the denominator yields un n α α γ γ α γn un σ γ γ σ γ σ γ which is concave in n proof as explained in can be rewritten as one term of is increasing in σ and two are decreasing minimizing pairs of these terms gives upper bounds on un a first bound can be obtained by setting σ γ n which minimizes the sum of the first two terms and gives un min sα β n un z l u n by this gives a bound on the optimal speeds of theorem the scaled speed σn n α satisfies n γ σα α σ αγ n γ γg γ α α a second bound comes by minimizing the sum of the second and third terms when σ γ this gives proof note that un un thus by α un u n z α α γn un γ γ αγ α α n by this can be expressed in terms of n as which upon division by gives αγ n α α n α n z n γ α β the minimum of the right hand sides of and is a bound on sn n α α α n α αγ n z the result then follows from the fact that and the result follows from since z z γ γ n for α gs the above theorem can be expressed more n which follows from taking the square root of the first inequal explicitly as follows corollary for α and any n ity and rearranging factors lower bounds on the optimal dynamic speeds n α γ n finally we prove lower bounds on the dynamic speed scaling scheme we begin by bounding the speed used when there is one job in the system the following result is an immediate consequence of corollary and proof for α can be solved explicitly giving un n z since un by corollary for α n γ n z γ γ α max β and substituting z from gives the result observe that the bounds in like those in corollary are essentially tight for both large and small γ but loose for γ near especially the lower bound next we will prove a bound on n for large n lemma for sufficiently large n there are two important observations about the above corollary first the corollary only applies when ρ and hence after the mode of the distribution however it also proves that the mode occurs at n second the corollary only applies when n in this case we can simplify the upper bound on sn in and combine it with to obtain n α proof rearrange as un n z un α α n α α when this form holds it is tight for large n and or large γ finally note that in the case when n the only bounds we have on the optimal speeds are where the inequality uses the fact that the un is non decreasing hence unbounded as n is unbounded whence un z for large n applying n α un αγ α gives this result highlights the connection between the optimal stochastic policy and prior policies analyzed in the worst case model that we mentioned at the beginning of this section specifically combining with and shows that speeds chosen to perform well in the worst case are asymptot ically optimal for large n in the stochastic model however note that the probability of n being large is small next we can derive a tighter albeit implicit bound on the optimal speeds that n is increasing in n the following lemma proves that an improved lower bound can be attained by interpolating linearly between max γ and γ lemma the sequence un is strictly concave increasing proof let p n be the proposition un un un un strict concavity of un is equivalent to there being no n for which p n holds since un is non decreasing and there exists an upper bound on un with gradient tending to it is sufficient to show that p n implies p n if so then any local non concavity would imply convexity from that point onwards in which case its long term gradient would be positive and bounded away from zero and hence un would eventually violate the upper bound by u identity n un φ un φ u n with this p n is equivalent to φ un φ un un un 102 this implies un un and φ un φ un absolute costs α ratio of cost for gated static to optimal zgs z note that the first factor is positive since the second factor is positive since φ is convex there is a subgradient g defined at each point this gives shows that the gated static i e the upper bound has very close to the optimal cost in addition to comparing the total cost of the schemes it is important to contrast the mean response time and mean φ un φ un g u un un φ un φ un un un energy use figure shows the breakdown a reference load of ρ with delay aversion β and power scaling α this and imply that both of the factors of in crease when going from p n to p n establishing p n and the strict concavity of un since it is also non decreasing the result follows comparing static and dynamic schemes to this point we have only provided analytic results we now use numerical experiments to contrast static and dynamic schemes in addition these experiments will illustrate the tightness of the bounds proven in section iv on the optimal dynamic speed scaling scheme we will start by contrasting the optimal speeds under each of the schemes figure compares the optimal dynamic speeds with the optimal static speeds note that the bounds on the dynamic speeds are quite tight especially when the number of jobs in the system n is large for reference the modes of the occupancy distributions are about and close to the points at which the optimal speed matches the static speeds note also that the optimal rate grows only slowly for n much larger than the typical occupancy this is important since the range over which dvs is possible is limited although the speed of the optimal scheme differs signif icantly from that of gated static the actual costs are very similar as predicted by the remark after corollary this is shown in fig the bounds on the optimal speed are also very tight both for large and small γ part a shows that the lower bound is loosest for intermediate γ where the weights given to power and response time are comparable part b was compared against changing ρ for fixed γ changing β for fixed ρ and changing α note γ was chosen to maximize the ratio of zgs z the second scenario shows that when γ is held fixed but the load ρ is reduced and delay aversion is reduced commensurately the energy consumption becomes negligible robust power aware design we have seen both analytically and numerically that ide alized dynamic speed scaling only marginally reduces the cost compared to the simple gated static this raises the question of whether dynamic scaling is worth the complexity this section illustrates one reason robustness specifically dynamic schemes provide significantly better performance in the face of bursty traffic and mis estimation of workload we focus on robustness with respect to the load ρ the optimal speeds are sensitive to ρ but in reality this parameter must be estimated and will be time varying it is easy to see the problems mis estimation of ρ causes for static speed designs if the load is not known then the selected speed must be satisfactory for all possible anticipated loads consider the case that it is only known that ρ ρ ρ let z denote the expected cost per unit time if the arrival rate is but the speed was optimized for then the robust design problem is to select the speed ρt such that min max z ρ ρt ρl ρ ρ ρ the optimal design is to provision for the highest foreseen load i e maxρ ρ ρ z ρ ρt z ρ ρt however this is wasteful in the typical case that the load is less than ρ occupancy n γ occupancy n γ the fragility of static speed designs is illustrated in fig which shows that when speed is underprovisioned the server is unstable and when it is overprovisioned the design is wasteful optimal dynamic scaling is not immune to mis estimation of ρ since n is highly dependent on ρ however because the speed adapts to the queue length dynamic scaling is more robust figure shows this improvement this robustness is improved further by the speed scaling fig rate vs n for α and different energy aware load γ scheme which we term linear that scales the server speed design fig breakdown of e t and e sα for several scenarios in proportion to the queue length i e sn α n note fig cost at load β α ρ when speeds are designed for design ρ using that under this scaling the queue is equivalent to an m gi queue with homogeneous servers figure shows that linear scaling provides significantly better robustness than the opti mal dynamic scheme indeed the optimal scheme is only and zgs e e β eρ ρ ρ β β β e optimal for designs with ρ even then its cost is only slightly lower than that of linear scaling the significant we can further relate zgs to zlin by eρ ρ ρ price that linear scaling pays is that it requires very high processing speed when the occupancy is high which may not be supported by the hardware we now compare the robustness analytically in the case of α first we will show that if ρ is known the cost of the zgs zlin from which follows β β e β eρ eρ β β β e linear scheme is exactly the same as the cost of the gated static scheme and thus within a factor of of optimal theorem then we will show that when the target load differs from the actual load the linear scheme significantly reduces the cost theorem in particular the linear scaling scheme has cost independent of the difference between the design and actual ρ in contrast the cost of gated static grows linearly in this difference as seen in fig theorem when α zgs zlin thus zlin proof if the speed in state n is kn then this insensitivity to design load mirrors worst case analysis the optimum available scaling which designs for ρ is o worst case competitive however fig suggests that linear scaling is much better than designing for ρ tighter bounds are known for sn α but those are still looser than theorem concluding remarks speed scaling is an important method for reducing energy consumption in computer communication systems intrinsi cally it trades off the mean response time and the mean energy n consumption and this paper provides insight into this tradeoff e n ρ e kn ρ k e ρ k ρk using a stochastic analysis and so the total cost is optimized for k β in this case e ρ ρ asymptotics for the optimal speed scaling scheme are provided these bounds are tight for small and large γ and provide a number of insights e g that the mean response time is 2γ which is identical to the cost for gated static by corollary this is within a factor of of z theorem consider a system designed for target load ρt that is operating at load ρ ρt e when α ρ scaling and that the optimal dynamic speeds in the stochastic model match for large n dynamic speed scalings that have been shown to have good worst case performance surprisingly the bounds also illustrate that a simple scheme which gates the clock when the system is idle and uses a static rate otherwise provides performance within a factor of of the optimal dynamic speed scaling however the value of zlin β β ρ dynamic speed scaling is also illustrated dynamic speed scaling schemes provide significantly improved robustness to bursty traffic and mis estimation of workload parameters proof the optimal rates for the linear policy are sn n β independent of ρt thus its cost is always the optimal speed for gated static in this case is sn ρt β for n when operated at actual load ρ this gives e ρ e ρρt ρ β ρt ρ β β β longer optimal when robustness is considered a scheme that scales speeds linearly with n provides significantly improved robustness while increasing cost only slightly there are a number of related directions in which to extend this work for example we have only considered dynamic power consumption which can be modeled as a polynomial of the speed however the contribution of leakage power is growing and an important extension is to develop models of total power use that can be used for analysis also it will be very interesting to extend the analysis to scheduling policies beyond ps for example given that the speed can be reduced if there are fewer jobs in the system it is natural to suggest scheduling according to shortest remaining processing time first srpt w hich i k nown t o m inimize t he n umber o f jobs in the system optimal power down strategies john augustine school of information and computer science univ of california at irvine irvine ca sandy irani school of information and computer science univ of california at irvine irvine ca chaitanya swamyy center for the mathematics of information caltech pasadena ca abstract we consider the problem of selecting threshold times to transition a device to low power sleep states during an idle period the two state case in which there is a single active and a single sleep state is a continuous version of the ski rental problem we consider a generalized version in which there is more than one sleep state each with its own power consumption rate and transition costs we give an algorithm that given a system produces a deterministic strategy whose competitive ratio is arbitrarily close to optimal we also give an algorithm to produce the optimal online strategy given a system and a probability distribution that generates the length of the idle period we also give a simple algorithm that achieves a competitive ratio of p any system introduction suppose you are about to go skiing for the first time in your life naturally you ask yourself whether to rent skis or to buy them renting skis costs say whereas buying skis costs if you knew how many times you would go skiing in the future ignoring complicating factors such as inflation and changing models of skis then your choice would be clear if you knew you would go at least times you would be financially better off by buying skis right from the beginning whereas if you knew you would go less than times you would be better off renting skis every time alas the future is unclear and you must make a decision nonetheless although the ski rental problem is a very simple abstraction this basic paradigm arises in many ap plications in computer systems in these situations there is a system that can reside in either a low cost or a high cost state occasionally it is forced to be in the high cost state usually to perform some task a period between any two such points in time is called an idle period the system pays a per time unit cost to reside in the high cost state alternatively it can transition to the low cost state at a fixed one time cost if the idle period is long it is advantageous to transition to the low cost state immediately if the idle period is short it is better to stay in the high cost state an online algorithm which does not know the length of the idle period must balance these two possibilities this problem has been studied in the context of shared memory multiprocessors in which a thread is waiting for a locked piece of data and must decide whether to spin or block researchers investigating research supported partially by nsf grants ccr and ccf and by onr award ywork done while the author was a student at the department of computer science cornell university ithaca ny research supported partially by nsf grant ccr the interface between ip networks and connection oriented networks have discovered this same underlying problem in deciding whether to keep a connection open between bursts of packets that must be sent along the connection karlin kenyon and randall study the tcp acknowledgment problem and the related bahncard problem both of which are at heart ski rental problems the problem also arises in cache coherency in deciding whether to update or invalidate data that has been changed in a processor local cache an important application of the ski rental problem is in minimizing the power consumed by devices that can transition to a low power sleep state when idle the sleep state consumes less power however one incurs a fixed start up cost in making the transition to the high power active state in order to begin work when a new job arrives at the architectural level the technique of eliminating power to a functional component is called clock power gating at a higher level the powered down component might be a disk drive or even the whole system e g a laptop that hibernates the embedded systems community has invested a great deal of effort into devising policies governing the selection of power states during idle periods termed dynamic power management in their literature see for example for a survey these techniques have been critical to maximizing battery use in mobile systems while power is already a first class parameter in system design it will become increasingly important in the future since battery capacities are increasing at a much slower rate than power requirements most of the previous work on this problem has been concerned with two state systems which have an active state and a single sleep state this paper focuses on finding power down thresholds for systems that have more than one low power state an example of such a system is the advanced configuration and power interface acpi included in the bios on most newer computers which has five power states including a hibernation state and three levels of standby previous work and new results for the two state problem an online algorithm consists of a single threshold tafter which time the algorithm will transition from the active to the sleep state the input to the problem is the length of the idle period and the cost of an algorithm is the total amount of energy it consumes over a single idle period typically an online algorithm is evaluated in terms of its competitive ratio the ratio of the cost of the online algorithm to the cost of the optimal offline algorithm maximized over all inputs when randomized algorithms are considered where the threshold tis chosen at random we look at the ratio of the expected cost of the online algorithm to the cost of the offline algorithm previous work has also addressed the two state problem when the idle period is generated by a known probability distribution in this case the online algorithm will choose a threshold which minimizes its expected cost where the expectation here is taken over the random choice of the idle period we call such algorithms probability based algorithms the best deterministic online algorithm will stay in the high power state until the total energy spent is equal to the cost to power up from the low power state it is known that this algorithm achieves the optimal deterministic competitive ratio of when one considers randomized online algorithms the best competitive ratio achievable improves to e e if the idle period is generated by a known probability distribution then the algorithm that chooses tso as to minimize the expected cost is always within a factor of e e of optimal furthermore this bound is tight since there is a distribution over the idle period lengths which will force any online algorithm to incur an expected cost that is a factor e e times larger than that incurred by the optimal offline algorithm note that in the context of power down systems it may not be the case that the power usage in the sleep state is zero or even that the start up cost in the active state is zero in these cases both the online and the offline algorithm will incur an identical additional cost thus the ratio of the online to the offline cost will decrease and the optimal competitive ratio will be strictly less than two however these additional costs do not change the optimal online or offline strategy in either the deterministic or the probability based case and the optimal competitive ratio that can be achieved for such systems can easily be determined as a function of all the parameters of the system we denote the problem that involves powering down through ksleep states pd k a formal description of the problem is as follows we are given a sequence of k states s ski there is also a vector of power consumption rates k h ki where iis the power consumption rate of the system in state si we assume as a convention that the states are ordered so that i jfor jk so the active state and the system must transition to i e power up at the end of the idle period there is an associated transition cost di jto move from state sito sj a system is described by a pair d note that there can be costs to move from high power states to low power states and vice versa however the only power up costs that are of interest are the costs to transition from a particular state sito the active state since the only reason to transition to a higher power state is when a new task arrives a schedule or strategy a sa ta consists of a sequence of na states sathat is a subsequence of s and a sequence of transition times ta where obvious we will omit the subscript a we require that s t we use a t to denote the cost of the schedule produced by strategy afor an idle period of length t we also consider a generalization of pd k that we call pd k m wherein we require that nam where mkis some limiting integer constant this generalization would be especially useful for engineers who have a large number of sleep state options available in the design phase but are required to implement at most a fixed number of states in the product that rolls out into the market the only previous work that examines the multiple state problem pd k from the perspective of worst case guarantees is which considers the special case where the cost to power down is zero and the algorithm only pays to move from low power states to higher power states note that this also includes the case where the transition costs are additive di j dj k di kfor i j k since the costs to power down can then be folded into the costs to power up gives natural generalizations of the algorithms for the two state case both for the case when the idle period length is unknown and when it is generated by a known probability distribution it is shown that when the transition costs are additive the generalized deterministic algorithm is competitive and the probability based algorithm is e e competitive thus matching the guarantees in the two state case there are two important directions left open by this work the first is based on the observation that systems in general do not have additive transition costs in many scenarios additional energy is spent in transitioning to lower power states furthermore there could be overhead in stopping at intermediate states resulting in non additive transition costs see for an example the second point is that the known upper bounds are typically not optimal for the system under consideration that is while it is true that there exist systems for which the optimal competitive ratio that can be achieved by any deterministic algorithm is and e e by any randomized algorithm it is possible to achieve a better competitive ratio for many systems for multi state systems the optimal competitive ratio that can be achieved will in general be a complicated function of all the parameters of the system the power consumption rates as well as transition costs for probability based algorithms the optimal competitive ratio will also depend on the probability distribution generating the length of the idle period while it may not be feasible to express the optimal competitive ratio as a function of all these parameters a system designer would in general like to design a power down strategy that obtains the best possible competitive ratio given the constraints of his or her particular system this paper establishes the following results we give an algorithm that takes as input an instance of pd k that is described by k d and an error parameter e and produces a power down strategy a a whose competitive ratio is within an additive eof the best competitive ratio that can be achieved for that system the algorithm runs in time o k logk log e where k is the number of states in the system and also outputs the competitive ratio of the algorithm works via a decision procedure which determines for a system and a constant pif there is a p competitive strategy for that system this decision procedure also allows us to obtain lower bounds on the competitive ratio achievable by deterministic algorithms for specific systems which in turn provides a lower bound on the competitive ratio achievable by deter ministic algorithms in general in particular we obtain a lower bound of the competitive ratio for deterministic algorithms this is the first lower bound known that is greater than independently damaschke has given a lower bound of the above approach can be modified to solve the more general version where a bound of mis specified on the number of states allowed in final strategy we show how to extend the decision procedure to answer if there is a p competitive strategy for the system that uses at most mpower states experimental results show that there are significant performance gains to be made by estimating the distribution governing the length of an idle period based on recent history and using this estimate to drive a probability based strategy we give an algorithm that takes as input a description of a system and a probability distribution generating the idle period length and produces the optimal power down strategy naturally the running time of the algorithm will depend on the representation of the distribution in practice this is most likely to be a histogram our algorithm runs in time o k logk b where bis the number of bins in the histogram and k is the number of states one outcome of the proof is that it also establishes the optimality of the strategy given in for additive systems we then generalize this to find the best online algorithm subject to the restriction that at most mstates are used at the expense of an extra factor of min the running time we give a simple deterministic strategy that achieves a competitive ratio of p all systems this result gives a bound on the competitive ratio achieved by the optimal strategies generated by our algorithms note that p also serves as a bound on the ratio of the expected costs of the online and offline algorithms when the input is probabilistically generated in the remainder of this paper we use the terms schedule or strategy interchangeably to refer to the choices of states and threshold times for powering down the term algorithm will refer to a procedure that produces a schedule or strategy based on a particular system azar et al in consider a related problem which they refer to as capital investment this problem is a different generalization of the ski rental problem than the power down problem considered here however a special case of their problem coincides with a special case of our problem specifically they give a p competitive deterministic algorithm for the special case of the power down problem in which the cost to transition to each state is the same regardless of the state from which one is transitioning later damaschke in improves the upper bound on the competitive ratio for this special case also in the context of capital investment to deterministic algorithms and ranomized algorithms in addition damaschke gives a bound for any deterministic algorithm which subsumes the lower bound of here preliminaries first we will establish that we can assume without loss of generality that the power up transition costs are zero if this is not the case for some system d we can define a new system such that for any i j the cost to transition from sito sjis di j dj the cost to go from sjto siis since there is never any reason to transition to a higher power state unless the system is transitioning to the active state at the arrival of a new task any set of actions in the original system will incur the same cost in the new system thus in the sequel we assume that di all i let d i denote i then opt t mini d i it let s t denote the state which attains the minimum the optimal state the optimal strategy is to transition to state s t at time and stay there through time t we assume that the optimal strategy will actually use every state i e range s t sk none of the online strategies we present will make use of a state that is never used by the optimal offline strategy for any time t energy state state state state b b figure energy consumed by the optimal strategy as a function of idle period length note that opt t is piecewise linear and s t is non decreasing with t as the idle period length gets longer it becomes more worthwhile to pay the extra cost to transition to a lower power state let bidenote the first time instant at which state sibecomes the optimal state so b d i i bi d i ibi bi we have b b b k figure shows the total energy consumed k i k i by optas a function of the length of the idle period there is a line for each state the y intercept is the transition cost to move to that state from the active state and the slope is the power consumption rate the energy consumed by the optimal strategy is the lower envelope of these lines since it will pick the single state which minimizes the cost for a given idle period length thus for t bi bi i opt t d i it j bj bj i t bi j we compare our online strategy with opt t and want to get a strategy awhich minimizes the com petitive ratio ca sup t topt t where a t denotes the total power consumption of aby time t a simple p competitive strategy first we establish that we can assume that for all i j di j j recall that we are really using di jto denote di j dj jto denote j dj thus the assumption that di j jreally amounts to assuming that di j di j if this were not the case we could just transition from state sito state sj by first going to then down to sj let us for the moment assume that for some d i d i for all i k this is a non trivial assumption that we will have to handle later consider the strategy which always stays in state s t the same state as opt at every time t the optimal strategy which knows the length of the idle period in advance will just transition to the optimal state strategy however must follow the optimal strategy making each transition to a new state as the idle period gets longer this is the strategy proposed in and shown to be competitive for additive systems note that this strategy is the same as the competitive balance strategy for the two state case for t bi bi the online cost is a t pj j bj bj dj j i t bi in comparing this cost to the optimal cost in equation observe that both terms have an additive i t bi which means that the ratio a t will be maximized at t bi to bound the cost of ain terms of opt we use the fact i that opt bi d i and opt bi j bj bj both of which come from equation i a bi j bj bj dj j j i i x j bj bj xd j j j i opt bi d i i j j opt b opt b i i this holds for any timplying a competitive ratio of now suppose the assumption d i d i does not hold we consider a new offline strategy opt that only uses a subset of states s for which the property does hold and is a approximation of opt i e opt t opt t we now view our problem as specified by just the states in s and execute strategy aas specified above emulating opt instead of opt we get that a t opt t opt t setting p we get a competitive ratio of p we determine opt as follows let s fskginitially consider the states in sin reverse order let sibe the last state added to s we find the largest j j is t d j d i we add sjto s and continue until no such jexists note that s since d opt will execute the optimal offline strategy assuming that only the states in s are available consider i js t si sj s and no is in s for i r j we have opt t opt t for t bi bi and t bj bj for rs t i r jand time t b b opt t min d i it d j jt and opt t d r t jwas chosen to be the largest value less than isuch that d j d i which means that d r d i furthermore since i we have that opt t d i it d r t opt t and opt is a approximation to opt theorem there is a p competitive strategy for any system a near optimal deterministic algorithm in this section we turn our attention to obtaining a near optimal schedule for a particular system more pre cisely given a system d with state sequence sfor which the optimal online schedule has competitive ra tio p we give an algorithm that returns a p e competitive online schedule in time o k e the algorithm is based on a decision procedure which determines whether a competitive schedule exists for a given value of p theorem establishes an upper bound of p on the optimal competitive ratio so we perform a bisection search in the range p to find the smallest psuch that there exists a p competitive schedule we also output the resulting schedule energy energy ttime titime figure energy consumed by the online and optimal strategy as a function of idle period length the solid line is popt t the dashed line is the online cost tis the first transition time that is not eager t shows the transformed strategy which now has an eager transition the following lemma shows that the online strategy must eventually get to a sufficiently low power state lemma allows us to limit our concern to just the transition points in any online schedule lemma if a s t is a p competitive strategy and is the last state in s then p k proof for the sake of contradiction assume that p k for to be p competitive the function t must lie entirely below popt t however the last line of popt t has slope p kand will therefore intersect the last line of t which has a larger slope after which time t will exceed popt t this is a contradiction lemma if a schedule ahas finite competitive ratio then the earliest time t which a t is maximized is a transition point in the strategy a proof let p maxt t consider the functions a t and popt t the function a t never exceeds popt t and is the earliest point at which these two functions have the same value not consid ering the origin for the sake of contradiction assume that t is not a transition point in a so we can find some small e that a t is linear in t e t e since a t is strictly less than popt t in the interval t e t and a t popt t it must be the case that the slope of a t is larger than the slope of popt t in this interval this gives a contradiction because a t has constant slope over t e e and popt t is a continuous function with decreasing slope which means that a t popt t for t t we now explore ways to restrict the space of schedules we need to consider in searching for a p competitive schedule for a strategy a s t we say that a transition at time t tis p eager or just eager if pis clear from the context if a t popt t we say that ais a p eager strategy if a t popt t for every t t note that by lemmas and a p eager strategy that ends at state such that p kis p competitive lemma if is a p competitive strategy then there exists an eager strategy that is also p competitive proof figure shows a schematic of the proof the jumps in the online cost the dashed line are transition costs the solid line is popt t the figure shows a transition time tat which the online cost is less than popt t the idea is that we can slide such a transition time earlier until it hits the function popt t consider the earliest transition time twhich is not eager suppose that atransitions from state sito state sjat time t let t tbe the time of the immediately preceding transition if there is no such transition time then set t the function popt t a t is continuous in the interval t t since adoes not have any transitions in this open interval and popt t t is time t and is strictly greater than di jat time tefor a small enough e let tbe the earliest time after t such that popt t t di j so t t consider the strategy a that is identical to aexcept that the transition from sito sjis moved earlier from tto t we need to argue that a is p competitive clearly a t a t for t t t and a t popt t also a t a t since a transitions earlier to the low power state sjand hence uses less total energy and since the strategies behave the same after time t a will continue to have a lower cost at all times t t to see that a t popt t over the interval t t note that a t is linear over this interval since a remains in state sj also popt t is a piecewise linear concave function since its slope is non increasing over time thus since the points t t and t t both lie on or below this curve the straight line connecting them lies under the curve popt t the procedure above can be repeated until all the transitions are eager lemma suppose a strategy makes a p eager transition to state siat time tiand next makes a transition to state sj using the function popt t one can compute the earliest p eager transition time t to state sj in time o logk proof define the line l t it popt ti iti di j t is the smallest t tisuch that popt t l t if there is no such t then a p eager transition from sito sjdoes not exist since popt t is concave we have that if l t popt t or if l t popt t and the slope of popt t is less than or equal to i then t t otherwise t these inequalities allow one to do a binary search using the line segments of popt t to determine tif it exists let be the optimal state i e state of opt t at time ti consider the line segments of popt t corresponding to states and sk recall that b and bkare respectively the left end points of these segments these are the first time instants at which and skbecome the optimal states respectively using the above inequalities if we determine that t bk then t is simply the point of intersection if it exists of l t with the segment of popt t corresponding to sk otherwise we have a low segment with end point b and a high segment with end point bk now we repeatedly consider the left end point of the segment that is in the middle of the low and high segments and use the above inequalities to update the low or high segment and the corresponding end point accordingly until the end points of the low and high segments correspond respectively to the left and right end points of a segment of popt t when this happens we can compute by finding the intersection point if it exists of l t and this segment the binary search can be implemented in time logk where kis the number of segments i e number of states lemma immediately gives an algorithm that is exponential in k the number of states and determines whether a p competitive strategy exists for the system this algorithm enumerates all subsequences of states and determines the p eager strategy for that subsequence by finding the eager transition to each state based on the eager transitions to the previous states as described in the proof of lemma a p competitive strategy for the system exists if and only if one of these p eager strategies is p competitive i e ends at a state swith sp k the remainder of this section presents a way to remove the exponential dependence on k let s skibe a sequence of states that form a system define ssi sj to be the contiguous subsequence hsi sji where siand sjare elements of ssuch that i j let sbe the set of subse quences of ss sthat include ssuch that for each one can find transition times for the state sequence so that in the resulting schedule each transition up to and including the transition to state sis a p eager transition for a state q we will use qto denote this p eager transition time to qfor the sequence note that uniquely determines the transition times q we define the earliest transition time e p of state sfor the given system as e p that is e p is the earliest time at which any online strategy can transition to state swhile remaining p eager over all its transitions up to and including the transition to state observe that if there is p competitive strategy that uses state then by lemma there is such a p eager strategy so and e p is well defined we call a transition to state sp early or simply early if it happens at time e p a strategy that consists entirely of early transitions is called a p early strategy lemma if there is a p competitive strategy then there is an eager and early p competitive strategy proof let sbe the last state in s consider the sequence ssuch that e p and the strategy that uses only the states in transitioning to state q at time q i e since is p competitive it must be that sp kand since by definition has all p eager transitions and ends in state it is also p competitive we now argue that is an early strategy note that was chosen so that the transition to state sis p early we have to show that the remaining transitions of are also p early suppose not consider the latest transition that is not p early suppose this happens for state r so t r e r p let r be the state just after rin sequence let rbe the sequence for which i r e r p t t the earliest time that a p eager schedule can transition to state rand the sequence of states in this schedule is given by consider the hybrid strategy that uses the states in followed by the states in that appear after r with the transition times being i qfor q and qfor q ri strategy transitions to state rat time t and strategy transitions to state rat time t t both of these transitions are eager transitions both strategies are in state rat time t and make the same state transitions thereafter thus for any t t t t t t in particular both strategies transition to r the state after r at time ri e r p t using the equation above we have that t t t t we will show that t t which implies in particular that t t so in the transition to r is no longer p eager arguing as in lemma this means that we can shift the transition to r to get an eager transition at an earlier time but this contradicts the assumption that the transition to state r at time t was an early transition we now prove that t t the transitions to state rin schedules and are eager transitions so both the points t t and t t lie on the popt t curve since t popt t for all t the the slope of popt t at time t is at least r the slope of t at time t and strictly greater since the gap between popt t and t must accommodate the transition cost from state rto r at time t the concavity of popt t implies that its slope is greater than rover the interval t t popt t so t popt t popt t r t t t where the last inequality follows since stays in state rin the interval t t from lemma we can deduce that we only need to consider a specific early and eager schedule the one that is determined by the e p values to determine if a p competitive strategy exists we can now define a decision procedure exists that takes a system and a constant pand outputs yes if a p competitive strategy exists for the system and no otherwise the procedure can be modified to also output a p competitive strategy if it exists we employ a dynamic programming approach to calculate e si p for i k we always start with the high power state and hence e p suppose we have computed e sj p for all j i let tjbe the earliest time at which the system p eagerly transitions from sjto sigiven t figure the solid line is popt the dashed line is the schedule from lemma and the dashed dotted line is the point labeled pis t t and p is t t the idea is to show that at time t has a lower cost than that the transition to sjis p eager and occurs at time e sj p if such a transition is not possible then we assign tj we can compute tjin o logk time as described in lemma then e si p minj itj determining each e si p requires examining jdifferent possibilities so finding all the early transition times for all states takes time o k by lemma we know that if e si p is finite for some state siwhere ip k we know that a p competitive strategy exists one can quickly elicit the schedule by starting from state kand retracing the states that minimized the earliest transition time we use the procedure exists to do a bisection search in the interval and find a p competitive strategy where pp e the total time taken is o k e we now turn our attention to adapting this dynamic programming technique to solve pd k m where a bound of mis specified on the number of states that can be used by the online algorithm we introduce a new parameter bmodifying our function to e si p b where i m the intuition is that function eis now required to return the earliest time when the system can transition to state siwhile staying entirely below popt t and using at most b states from si the base case is e p b for all intuitively e si p b is determined by the best state sjprior to sisuch that at most b states were used to reach sj notice that for any given state siand fixed p e sj p b is non increasing as bincreases therefore as above we can write e si p b minj it j where t is the earliest time when the system p eagerly transitions from sjto sigiven that the transition to sjwas p eager and occurred at e sj p b the running time increases by a factor of mnow and is o k logk log e a probability based algorithm karlin et al study the two state case when the length of the idle period is generated by a known probability distribution p although they examined the problem in the context of the spin block problem their problem is identical to our two state case they observed that the expected cost of the online strategy that makes the transition to the sleep state at time tis oc p t dt p t t t dt where the power consumption rate in the active state is the power consumption rate in the sleep state and is the transition cost between the two states the online strategy then should select the transition time tthat minimizes this cost the multi state case presents two distinct challenges the first is to determine the optimal sequence of states through which an online strategy should transition throughout the course of the idle period then once this sequence has been determined the optimal transition times need to be determined our proof proceeds by establishing that the only transition times that need to be considered are the optimal transition times for two states systems suppose for example that we are considering a sequence of state transitions in which state siis followed by state sj let ti jdenote the optimal transition time from state sito sjif these were the only two states in the system that is if siwere the active state and sjwere the only sleep state note that ti jcan be determined by the expression above we establish that regardless of the rest of the sequence the optimal transition point from state sito sjis ti j we call the ti j the pairwise optimal transition times lemmas and establish that the pairwise optimal transition times happen in the right order that is for i k j ti ktk j if this is not the case then any subsequence that has sifollowed by skfollowed by sjcan not possibly be the best sequence of states note that the ti j may not necessarily be unique in general we will select the earliest transition time that minimizes the cost for the two state system lemma then shows that as long as the pairwise optimal transition times are in the right order they give the globally optimal set of transition times for that subsequence our algorithm then uses this fact to find the optimal sequence of states by dynamic programming note that it is not necessary to exhaustively consider all possible subsequences optimal transition times consider a particular subsequence of l states sa sal in order to avoid the double subscripts through out this subsection we will rename our subsequence q ql since the strategy must start in state we can assume that for i j define i jto be the cost to transition from state qito state qj that is i j dai aj furthermore we will refer to the power consumption rate of state qias i that is i ai we will consider the strategy that transitions through the states in the subsequence q ql sup pose that we use transition time tito transition from state qi to state qi it will be convenient for notation to define tl and the cost of the strategy that uses these transition times is cost t tl l tj x p t j t tj dt xzoc p t j tj tj j j dt j tj j tj the goal is to pick the t tlso as to minimize the above cost this is the optimal cost for the subse quence ql for each i f lg let i i i i i lemma suppose that there is an i jsuch that i j then there is a a strict subsequence of ql whose optimal cost is no greater than the optimal cost for ql proof consider the first jsuch that j j let t tj tj t l be the sequence of thresholds that minimizes the cost of this sequence of states define the following quantities fj j cost t tj tj tj t j tl fj j cost t tj tj tj tj t l fj j cost t t j t j tj t j t l we will show that fj jis greater than or equal to a weighted average of fj j and fj jwhich means that it must be greater than or equal to at least one of these values this means that the strategy that transitions from state qj state qj and then immediately transitions to state qjat either time tj or t jis at least as good as the original strategy since j j j j j j skipping state j altogether can only improve the strategy below we have an expression for fj j fj jwhich can be derived from the definition for the cost in equation under fj jthe transition from state qj qj is moved forward from time t j to time t j any time spent in the interval t j tj happens at the higher power rate of j of j this is accounted for in the first two terms of the sum however idle times ending in the interval t j tj save on the transition cost which is accounted for in the last term below fj j fj j z tj tj p t t tj j j dt z oc tj p t t j t j j j dt t j j j p t dt dividing by j j this becomes t tj t fj j fj jzj zoczj j j tj p t t tj dt t j p t tj tj dt t j j p t dt below we use the definition of cost in equation to get an expression for fj j fj j note that in fj j the transition from state qj to state qis moved back from time t jto time tj thus fj j will spend j jmore power than fj j for any time spent in the interval t j tj furthermore fj j will have an additional transition cost of j jfor those intervals that end in the period t j tj t zjzoc fj j fj j t j p t t tj j j dt t j p t tj tj j j dt t j j jp t dt dividing by j j this becomes t tj t fj j fj j zj zoczj j j t j p t t tj dt tj p t tj tj dt tj p t dt j comparing equations and the expressions are almost identical except for the in the last term t since j jand r j t j p t dt we have that fj j fj j j j fj j fj j j j let w j j and w j j note that both w and w at least rearranging we get that w f w f f w w j j w w j j j j now suppose that we consider only the two state system consisting of state qi and state qi we will let tidenote the optimal threshold time if these are the only two states in the system we have that tiis the time tthat minimizes p t i tdt zocp t i t i t t i i dt note that the value of tthat results in the minimum above may not be unique in this case we take tto be the smallest value which achieves the minimum also note that by subtracting the term ocp t itdt which is independent of t and dividing by i iin the above definition it can be seen that ti argmintf i t where t f t p t tdt oc t p t t dt note that for a two state system whose active state and sleep states has power consumption rates of and respectively and whose transition cost is f t denotes the expected power consumed by an online strategy that transitions to the sleep state at time t we will show that for a particular subsequence of states if we minimize the cost over all choices for the thresholds the resulting thresholds are those obtained by the pair wise optimization above first however we must establish that the tivalues have the correct ordering lemma if i i then ti ti proof intuitively iis the ratio of the additional power cost of being in state qiinstead of state qi over the transition costs between the two states it stands to reason that the larger this cost the sooner one would want to transition from state qi to state qi we will formalize this argument using a proof by contradiction suppose that we have ti ti and i i the proof will make use of the definition of f t given above tiis the smallest value for twhich attains the minimum of f i t since ti ti we know that f i ti f i ti by the definition of ti we have that f i ti f i ti thus it should be the case that f i ti f i ti f i ti f i ti using the definition of f t above for any t t f t f t z p t tt dt oc t p t t t dt z p t dt the quantity inside the square braces above is non negative this implies that the quantity f t f t is non decreasing in this however contradicts inequality and the fact that i i finally we prove the main lemma which states that the transition times are simultaneously optimized at the pairwise optimal transition points lemma for a given subsequence of states ql if ti tifor all i f lg then the minimum total cost is achieved for cost t tl proof the basic idea is that we can interpret cost t tl ocp t ltdtas the sum of the power consumed in ltwo state systems where the ithsystem for i l has states whose power consump tion rates are i i and and the cost to transition between the two is i i note that ocp t ltdt is a constant independent of the choice of ti after rescaling one can write this expression as a linear combination of the f i ti terms since timinimizes f i t and the tivalues have the right ordering this implies that cost t tl is minimized by setting ti tifor i l we will establish below that we can rewrite as follows cost t t zocp t tdt ll ltioc z i p t i i tdt z p t i i ti i i dt so by rescaling we get that cost t tl zocp t ltdt x i if i ti we want to choose t tlto minimize this expression since t tland each ti argmintf i t it follows that the minimum is attained by setting ti tifor each i to complete the proof we show the equivalence of and it suffices to show that and integrate the same expression over each interval ti ti for i l the integrand in over the interval ti ti is i p t i t ti j tj tj j j j and the integrand in is i l p t x j j tj j j x j j l t j j i the summations over the jindices in telescope to show that the two expressions are identical the optimal state sequence we now present a simple polynomial time algorithm to obtain the optimal state sequence for a given system first for each pair i j jk let ti jdenote the optimal transition point if siand sjwere the only two states in the system the time complexity of determining a single ti jdepends on the representation of the probability distribution in practice this is most likely to be estimated by a finite histogram with bbins starting at time sampled at a uniform discrete interval of it follows that bin icorresponds to time i it is not difficult to generalize this for variable sized bins we will also assume that all transition times occur at some i the height of bin iis h i and this implies that the probability that the idle time tequals h i io iis given by ph i in algorithm we calculate acc i and acct i values which are t dtand t dtand we then use them to evaluate ti jvalues we can re write the expression for the cost of a two state system in equation as t i oc p t tdt j t p t tdt i j t i j z p t dt bobo we also denote t dtand t dtas totaland totaltrespectively using the pre calculated values above the cost of transitioning from state sito state sjat time lis i acct l il jl i j total acc l j totalt acct l once the ti j are found we sweep through them in non decreasing order keeping a running tab of the best sub schedules that we can achieve ending in each state siat each point in time when we encounter a ti j we check to see if transitioning from sito sjcan improve the current best sub schedule ending in sj and if it does update our data structure to reflect it a given strategy divides time into intervals where each interval is the period of time spent in a particular state the expected cost for a strategy given in equation is obtained by summing over the expected cost green it toward a science of power management krishna kant intel corp a formal understanding of energy and computation tradeoffs will lead to significantly more energy efficient hardware and software designs omputers originally evolved with the goal of automating repeti tive calculations and ever since have focused on doing the job faster and cheaper conse quently the dominant resources in most computational settings have been processing time and memory occupancy beginning in the research ers have exploited computational complexity theory to achieve remark able successes in devising faster algorithms proving bounds on computational speed and defining classes of problems of equivalent dif ficulty today this huge vibrant field offers benefits that extend far beyond theoretical computer science into the design of systems that directly solve socially relevant problems with the explosive growth of internet enabled gadgets of all types it energy consumption and sus tainability impact are expected to continue climbing well into the future although the problem is well recognized and aggressive efforts to address it are under way in both industry and academia much of the effort tends to be empirically driven just as the formal notions of computational complexity have immensely benefited hardware and software design a formal treat ment of energy power and thermal issues should also lead to significant advances in how such systems are designed in the future the vision is of a well developed body of knowledge that can guide various power perfor mance tradeoffs in the design tuning and operation of it devices rang ing from nanowatt level embedded devices to large server farms consum ing tens of megawatts of power although power and energy are often used interchangeably there are important distinctions transactional applications are better described in terms of throughput and average power whereas completion time and energy consumed are more meaning ful for nontransactional applications power consumption can often be increased over short periods to accelerate computation or accumu late more data and thereby minimize overall energy consumption energy versus computation our current understanding of energy issues is rudimentary for example is there a notion of energy or power complexity of an algorithm distinct from computational com plexity certainly a program energy consumption strongly depends on the number of instructions executed and the number of accesses to the memory hierarchy these are the same factors that determine a pro gram completion time also an instruction that takes fewer clock cycles to execute generally also con sumes less energy yet the correspondence between completion time and energy con sumption is not one to one for example the order of certain opera tions and bit representation of data can alter the energy consumed without necessarily changing the completion time at a coarser level of detail how ever computation and power energy complexity may differ because of various power management actions available in modern computing hard ware for example many devices provide the capability to transition to one or more lower power modes when idle if the transition latencies into and out of lower power modes are negligible energy consumption can be lowered simply by exploiting these states unfortunately the transition laten cies are rarely negligible and thus the use of low power modes impedes per formance to minimize this impact it is necessary to alter the workload so that many small idle periods can ieee published by the ieee computer society september what is needed is a hierarchy of models with varying abstraction levels in addition the models should reflect the physical and logical struc ture of computing systems in terms of multiple subsystems operating at multiple granularities of control col lectively such models will form the beginnings of the science of power management scipm a comprehensive scipm should let researchers address many other facets of energy consumption as well as the need for reducing energy con sumption grows so does the number of control knobs hardware design ers are developing new mechanisms for reducing energy consumption most of which have performance implications for example deeper sleep states and more speed states may be added to various platform components the crucial question is whether formal techniques can indicate how many states are needed and what their essential characteristics should be further since a cross product of power states of all components quickly becomes unwieldy a formal model is needed to establish relation ships between various states not only in terms of their number and char acteristics but also in terms of their simultaneous usage power management relates to more than just coordinated power state management as semiconduc tor technology begins to reach limits of reliable operation computation models will need to change to account for this unreliability scipm thus must address three attributes together per formance energy and reliability it may be necessary to invent new hardware architectures that slow down error propagation which would raise a new set of issues in model ing this three way tradeoff can we develop a science to start examining such tradeoffs before the products need to deal with it another benefit of a formal approach is the exploration of more computer energy efficient semantically equiva lent software transformations for example automated reordering of operations to allow optimum use of available power management functions finally scipm should ideally help quantify the capabilities and limitations of energy efficiency mechanisms at lower levels of detail including hardware architecture cir cuit design semiconductor processes and materials physics scipm workshop with these goals in mind the national science foundation spon sored a scipm workshop in arlington virginia in april cs vt edu the workshop generated tremendous interest among research ers across the us facing various data center related issues a signifi cant number of theoretical computer scientists also participated the attendees were divided into f ive groups to focus the deliberations the physicals group examined issues related to physical infra structure which often represents a substantial part of data center cost and could easily consume more than percent of the energy these include cooling from room level down to server fans electrical conversion and distribution from substation to board level voltage regulators dis plays and so on the hardware and architecture group was tasked with addressing issues ranging from new materials and processes to new architectures and power management knobs the software and middleware group was concerned not only with sophis ticated multilevel and multidomain energy management but also with energy efficient software design the storage group considered the energy impact of the increasingly data intensive nature of computing and various nonvolatile storage technolo gies currently under development finally the networking group was asked to explore network related energy management issues ranging from platform level interconnects to data center fabrics to the internet routing infrastructure each group was seeded with a few theoretical computer scientists to blend domain knowledge with abstractions the scipm deliberations sup ported a more scientific investigation of power thermal issues and develop ment of good metrics and models at least two groups expressed the need for a big o notation for power several groups emphasized the importance of multidisciplinary education so that future computer scientists are better prepared to deal with physicals type energy issues in particular and more broadly the numerous sustainability challenges inherent in it equipment and infrastructure design finally workshop participants recognized the need to tap into the enormous potential of it to tackle the remaining percent of the problem that is use it solutions to significantly reduce the percent of emissions not attributed to it itself itandenergy pdf formal understanding of energy and computation tradeoffs will open up new areas of investigation and lead to significantly more energy efficient hardware and software designs while preserving the continued improvement in performance that the emerging applications demand a formal approach will hopefully lead to energy management solutions that are not only effective but also easy to implement and validate traditionally energy consumption and supply sides have been fairly well isolated however with a greater role played by variable energy sources such as renewable and harvested energy it is imperative to design sys tems that are not just energy efficient in proceedings of the acm international symposium on computer architecture san diego ca june power provisioning for a warehouse sized computer xiaobo fan wolf dietrich weber luiz andré barroso google inc amphitheatre pkwy mountain view ca xiaobo wolf luiz google com abstract large scale internet services require a computing infrastructure that can be appropriately described as a warehouse sized computing system the cost of building datacenter facilities capable of de livering a given power capacity to such a computer can rival the re curring energy consumption costs themselves therefore there are strong economic incentives to operate facilities as close as possible to maximum capacity so that the non recurring facility costs can be best amortized that is difficult to achieve in practice because of uncertainties in equipment power ratings and because power con sumption tends to vary significantly with the actual computing ac tivity effective power provisioning strategies are needed to deter mine how much computing equipment can be safely and efficiently hosted within a given power budget in this paper we present the aggregate power usage character istics of large collections of servers up to thousand for dif ferent classes of applications over a period of approximately six months those observations allow us to evaluate opportunities for maximizing the use of the deployed power capacity of datacenters and assess the risks of over subscribing it we find that even in well tuned applications there is a noticeable gap between achieved and theoretical aggregate peak power usage at the cluster level thousands of servers the gap grows to almost in whole datacenters this headroom can be used to deploy additional com pute equipment within the same power budget with minimal risk of exceeding it we use our modeling framework to estimate the potential of power management schemes to reduce peak power and energy usage we find that the opportunities for power and energy savings are significant but greater at the cluster level thousands of servers than at the rack level tens finally we argue that systems need to be power efficient across the activity range and not only at peak performance levels categories and subject descriptors c computer systems or ganization general system architectures c computer sys tems organization performance of systems design studies mea surement techniques modeling techniques general terms measurement experimentation keywords power modeling power provisioning energy efficiency permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee isca june san diego california usa copyright acm introduction with the onset of large scale internet services and the massively parallel computing infrastructure that is required to support them the job of a computer architect has expanded to include the design of warehouse sized computing systems made up of thousands of computing nodes their associated storage hierarchy and intercon nection infrastructure power and energy are first order con cerns in the design of these new computers as the cost of powering server systems has been steadily rising with higher performing sys tems while the cost of hardware has remained relatively stable barroso argued that if these trends were to continue the cost of the energy consumed by a server during its lifetime could surpass the cost of the equipment itself by comparison another energy related cost factor has yet to receive significant attention the cost of building a datacenter facility capable of providing power to a group of servers typical datacenter building costs fall between and per deployed watt of peak critical power power for computing equip ment only excluding cooling and other ancillary loads while electricity costs in the u s are approximately watt year less than that in areas where large datacenters tend to be deployed un like energy costs that vary with actual usage the cost of building a datacenter is fixed for a given peak power delivery capacity con sequently the more under utilized a facility the more expensive it becomes as a fraction the total cost of ownership for exam ple if a facility operates at of its peak capacity on average the cost of building the facility will still be higher than all elec tricity expenses for ten years of maximizing usage of the available power budget is also important for existing facilities since it can allow the computing infrastructure to grow or to en able upgrades without requiring the acquisition of new datacenter capacity which can take years if it involves new construction the incentive to fully utilize the power budget of a datacenter is offset by the business risk of exceeding its maximum capacity which could result in outages or costly violations of service agree ments determining the right deployment and power management strate gies requires understanding the simultaneous power usage charac teristics of groups of hundreds or thousands of machines over time this is complicated by three important factors the rated maxi mum power or nameplate value of computing equipment is usu ally overly conservative and therefore of limited usefulness actual consumed power of servers varies significantly with the amount of activity making it hard to predict different applications exercise large scale systems differently consequently only the monitoring typical tier datacenter costs of watt of crit ical power and a energy overhead for cooling and conversion losses of real large scale workloads can yield insight into the aggregate load at the datacenter level in this paper we present the power usage characteristics of three large scale workloads as well as a workload mix from an actual datacenter each using up to several thousand servers over a period of about six months we focus on critical power and examine how power usage varies over time and over different aggregation levels from individual racks to an entire cluster we use a light weight yet accurate power estimation methodology that is based on real time activity information and the baseline server hardware config uration the model lets us also estimate the potential power and energy savings of power management techniques such as power capping and cpu voltage frequency scaling to our knowledge this is the first power usage study of very large scale systems running real live workloads and the first reported use of power modeling for power provisioning some of our other key findings and contributions are the gap between the maximum power actually used by large groups of machines and their aggregate theoretical peak us age can be as large as in datacenters suggesting a sig nificant opportunity to host additional machines under the same power budget this gap is smaller but still significant when well tuned large workloads are considered power capping using dynamic power management can en able additional machines to be hosted but is more useful as a safety mechanism to prevent overload situations we observe time intervals when large groups of machines are operating near peak power levels suggesting that power gaps and power management techniques might be more easily ex ploited at the datacenter level than at the rack level cpu voltage frequency scaling a technique targeted at en ergy management has the potential to be moderately effec tive at reducing peak power consumption once large groups of machines are considered we evaluate the benefits of building systems that are power efficient across the activity range instead of simply at peak power or performance levels datacenter power provisioning it is useful to present a typical datacenter power distribution hier archy since our analysis uses some of those concepts even though the exact power distribution architecture can vary significantly from site to site figure shows a typical tier datacenter facility with a total capacity of mw the rough capacity of the different compo nents is shown on the left side a medium voltage feed top from a substation is first transformed down to v it is common to have an uninterruptible power supply ups and generator combination to provide back up power should the main power fail the ups is responsible for conditioning power and providing short term back up while the generator provides longer term back up an auto matic transfer switch ats switches between the generator and the mains and supplies the rest of the hierarchy from here power is supplied via two independent routes in order to assure a degree of fault tolerance each side has its own ups that supplies a se ries of power distribution units pdus each pdu is paired with a static transfer switch sts to route power from both sides and assure an uninterrupted supply should one side fail the pdus are figure simplified datacenter power distribution hierarchy which individual circuits emerge circuits power a rack worth of computing equipment or a fraction of a rack power deployment decisions are generally made at three levels rack pdu and facility or datacenter here we consider a rack as a collection of computing equipment that is housed in a standard wide and tall enclosure depending on the types of servers a rack can contain between and computing nodes and is fed by a small number of circuits between and racks are aggregated into a pdu enforcement of power limits can be physical or contractual in nature physical enforcement means that overloading of electrical circuits will cause circuit breakers to trip and result in outages contractual enforcement is in the form of economic penalties for exceeding the negotiated load power and or energy physical lim its are generally used at the lower levels of the power distribution system while contractual limits show up at the higher levels at the circuit level breakers protect individual circuits and this limits the power that can be drawn out of that circuit enforcement at the circuit level is straightforward because circuits are typically not shared between users as we move higher up in the power distribu tion system larger power units are more likely to be shared between multiple different users the datacenter operator must provide the maximum rated load for each branch circuit up to the contractual limits and assure that the higher levels of the power distribution system can sustain that load violating one of these contracts can have steep penalties because the user may be liable for the outage of another user sharing the power distribution infrastructure since the operator typically does not know about the characteristics of the load and the user does not know the details of the power distri bution infrastructure both tend to be very conservative in assuring that the load stays far below the actual circuit breaker limits if the operator and the user are the same entity the margin between expected load and actual power capacity can be reduced because load and infrastructure can be matched to one another rated on the order of kw each they further transform the voltage to v in the us and provide additional condition ing and monitoring equipment as well as distribution panels from fact the national electrical code article a limits the load to of the ampacity of the branch circuit inefficient use of the power budget the power budget available at a given aggregation level is often underutilized in practice sometimes by large amounts some of the important contributing factors to underutilization are staged deployment a facility is rarely fully populated upon initial commissioning but tends to be sized to accomodate business demand growth therefore the gap between de ployed and used power tends to be larger in new facilities fragmentation power usage can be left stranded simply be cause the addition of one more unit a server rack or pdu might exceed that level limit for example a cir cuit may support only four servers which would guar antee a underutilization of that circuit if a datacenter is designed such that the pdu level peak capacity exactly matches the sum of the peak capacities of all of its circuits such underutilization percolates up the power delivery chain and become truly wasted at the datacenter level conservative equipment ratings nameplate ratings in com puting equipment datasheets often reflect the maximum rat ing of the power supply instead of the actual peak power draw of the specific equipment as a result nameplate values tend to drastically overestimate achievable power draw variable load typical server systems consume variable lev els of power depending on their activity for example a typ ical low end server system consumes less than half its actual peak power when it is idle even in the absence of any so phisticated power management techniques such variability transforms the power provisioning problem into an activity prediction problem statistical effects it is increasingly unlikely that large groups of systems will be at their peak activity therefore power lev els simultaneously as the size of the group increases load variation and statistical effects are the main dynamic sources of inefficiency in power deployment and therefore we will focus on those effects for the remainder of this paper other consumers of power our paper focuses on critical power and therefore does not di rectly account for datacenter level power conversion losses and the power used for the cooling infrastructure however in modern well designed facilities both conversion losses and cooling over heads can be approximately modeled as a fixed tax over the critical power less modern facilities might have a relatively flat cooling power usage that does not react to changes in the heat load in ei ther case the variations in the critical load will accurately capture the dynamic power effects in the facility and with the aid of some calibration can be used to estimate the total power draw power estimation one of the difficulties of studying power provisioning strategies is the lack of power usage data from large scale deployments in particular most facilities lack on line power monitoring and data collection systems that are needed for such studies we circumvent this problem by deploying an indirect power estimation framework that is flexible low overhead and yet accurate in predicting power usage at moderate time intervals in this section we describe our framework and present some validation data supporting its accu racy we begin by looking at the power usage profile of a typical server and how nameplate ratings relate to the actual power draw of machines table component peak power breakdown for a typical server nameplate vs actual peak power a server is typically tagged with a nameplate rating that is meant to indicate the maximum power draw of that machine the main purpose of this label is to inform the user of the power infrastruc ture required to safely supply power to the machine as such it is a conservative number that is guaranteed not to be reached it is typ ically estimated by the equipment manufacturer simply by adding up the worst case power draw of all components in a fully config ured system table shows the power draw breakdown for a server built out of a motherboard with cpus an ide disk drive slots of dram and pci expansion slots using the maximum power draw taken from the component datasheets we arrive at a total dc draw of w assuming a power supply efficiency of we arrive at a total nameplate power of w when we actually measure the power consumption of this server using our most power intensive benchmarks we instead only reach a maximum of which is less than of the nameplate value we refer to this measured rating as the actual peak power as this example illustrates actual peak power is a much more accurate es timate of a system peak consumption therefore we choose to use it instead of nameplate ratings in our subsequent analysis the breakdown shown in table does nevertheless reflect the power consumption breakdown in a typical server cpus and mem ory dominate total power with disk power becoming significant only in systems with several disk drives miscellaneous items such as fans and the motherboard components round out the picture estimating server power usage our power model uses cpu utilization as the main signal of machine level activity for each family of machines with similar hardware configuration we run a suite of benchmarks that includes some of our most representative workloads as well as a few micro benchmarks under variable loads we measure total system power against cpu utilization and try to find a curve that approximates the aggregate behavior figure shows our measurements alongside a linear model and an empirical non linear model that more closely fits our observations the horizontal axis shows the cpu utilization reported by the os as an average across all cpus u a calibra tion parameter r that minimizes the squared error is chosen a value of in this case for each class of machines deployed one set of calibration experiments is needed to produce the corresponding model an approach similar to mantis the error bars in figure give a visual indication that such mod els can be reasonably accurate in estimating total power usage of individual machines of greater interest to this study however is the accuracy of this methodology in estimating the dynamic power usage of groups of machines figure shows how the model com pares to the actual measured power drawn at the pdu level a few pbusy pidle cpu utilization figure model fitting at the machine level hundred servers in one of our production facilities note that ex cept for a fixed offset the model tracks the dynamic power usage behavior extremely well in fact once the offset is removed the error stays below across the usage spectrum and over a large number of pdu level validation experiments the fixed offset is due to other loads connected to the pdus that are not captured by our model most notably network switching equipment we have found that networking switches operate on a very narrow dynamic therefore a simple inventory of such equipment or a facility level calibration step is sufficient for power estimation we were rather surprised to find that this single activity level signal cpu utilization produces very accurate results especially when larger numbers of machines are considered the observa tion can be explained by noting that cpu and memory are in fact the main contributors to the dynamic power and other components either have very small dynamic range or their activity levels cor relate well with cpu activity therefore we found it unnecessary so far to use more complex models and additional activity signals such as hardware performance counters this modeling methodology has proved very useful in informing our own power provisioning plans the data collection infrastructure in order to gather machine utilization information from thou sands of servers we use a distributed collection infrastructure as shown in figure at the bottom layer collector jobs gather pe riodic data on cpu utilization from all our servers the collec tors write the raw data into a central data repository in the analy sis layer different jobs combine cpu activity with the appropriate models for each machine class derive the corresponding power es timates and store them in a data repository in time series format analysis programs are typically built using google mapreduce framework power usage characterization here we present a baseline characterization of the power usage of three large scale workloads and an actual whole datacenter based on six months of power monitoring observations show that ethernet switch power consumption can vary by less than across the activity spectrum component measurements show that the dynamic power range is less than for disks and negligible for motherboards 00 hour of the day figure modeled vs measured power at the pdu level figure collection storage and analysis architecture workloads we have selected three workloads that are representative of dif ferent types of large scale services below we briefly describe the characteristics of these workloads that are relevant to this study websearch this represents a service with high request through put and a very large data processing requirements for each request we measure machines that are deployed in google web search services overall activity level is generally strongly correlated with time of day given the online nature of the system webmail this represents a more disk i o intensive internet ser vice we measure servers running gmail a web based email prod uct with sophisticated searching functionality machines in this ser vice tend to be configured with a larger number of disk drives and each request involves a relatively small number of servers like websearch activity level is correlated with time of day mapreduce this is a cluster that is mostly dedicated to running large offline batch jobs of the kind that are amenable to the mapre duce style of computation the cluster is shared by several users and jobs typically involve processing terabytes of data using hundreds or thousands of machines since this is not an online ser vice usage patterns are more varied and less correlated with time of day datacenter setup for the results in this section we picked a sample of approxi mately five thousand servers running each of the workloads above in each case the sets of servers selected are running well tuned workloads and typically at high activity levels therefore we be lieve they are representative of the more efficient datacenter level workloads in terms of usage of the available power budget the main results are shown as cumulative distribution functions cdfs of the time that a group of machines spends at or below a given fraction of their aggregate peak power see for example fig ure for each machine we derive the average power over minute intervals using the power model described earlier the ag gregate power for each group of machines during an interval makes up a rack power value which is normalized to their actual peak i e the sum of the maximum achieavable peak power con sumption of all machines in the group the cumulative distribution of these rack power values is the curve labeled rack in the graph the pdu curve represents a similar aggregation but now group ing sets of racks or about machines finally the cluster curve shows the cdf for all machines approximately ma chines cdf power results let take a closer look at the power cdf for websearch figure the rack cdf starts at around of normalized power in dicating that at no time does any one rack consume less than of its actual peak this is likely close to the idle power of the ma chines in the rack the curve rises steeply with the largest fraction of the cdf i e the most time spent in the range of actual peak power the curve intercepts the top of the graph at of the peak power indicating that there are some time inter vals where all machines in a given rack are operating very close to their actual peak power the right graph of figure zooms in on the upper part of the cdf to make the intercepts with the top of the graph clearer looking at the pdu and cluster curves we see that they tend to have progressively higher minimum power and lower maximum power the larger the group of machines is the less likely it is that all of them are simultaneously operating near the extreme minimum or maximum of power draw for websearch some racks are reaching of actual peak power for some time interval whereas the entire cluster never goes over it is strik ing to see that groups of many hundreds of machines pdu level can spend nearly of the time within of their aggregate peak power the corresponding cdfs for webmail are shown in figure the shape of these is similar to that of websearch with two no table differences the dynamic range of the power draw is much narrower and the maximum power draw is lower webmail ma chines tend to have more disks per machine and disk power draw does not vary significantly with changes in activity levels hence a larger fraction of the power draw of these machines is fixed and the dynamic range is reduced the max power draw is also lower in terestingly we see a maximum of about of peak actual power at the rack level and at the cluster level an even higher gap than websearch the curves for mapreduce figure show a larger difference between the rack pdu and cluster graphs than both websearch and webmail this indicates that the power draw across different racks is much less uniform likely a result of its less time dependent activity characteristics this behavior leads to a much more notice able averaging effect at the cluster level while the racks top out at very close to of peak actual power the cluster never goes above about these results are significant for machine deployment planning if we use the maximum power draw of individual machines to pro vision the datacenter we will be stranding some capacity for web search about more machines could be safely deployed within the same power budget the corresponding numbers for webmail and mapreduce are even higher at and the impact of diversity figure presents the power cdf when all the machines running the three workloads are deployed in a hypothetical combined cluster this might be representative of a datacenter level behavior where multiple high activity services are hosted note that the dynamic range of the mix is narrower than that of any individual workload and that the highest power value achieved of actual peak is also lower than even that of the lowest individual workload webmail at this is caused by the fact that power consumption peaks are less correlated across workloads than within them it is an important argument for mix ing diverse workloads at a datacenter in order to smooth out the peaks that individual workloads might present using the highest power of the mix to drive deployment would allow more ma chines to be deployed to this datacenter an actual datacenter so far we have looked only at large well tuned workloads in a fully deployed environment in a real datacen ter there will be additional workloads that are less well tuned still in development or simply not highly loaded for example ma chines can be assigned to a service that is not yet fully deployed or might be in various stages of being repaired or upgraded etc figure shows the power cdf for one such datacenter we note the same trends as seen in the workload mix only much more pro nounced overall dynamic range is very narrow and the highest power consumption is only of actual peak power us ing this number to guide deployment would present the opportunity to host a sizable more machines at this datacenter value of power capping one of the features that stands out in the power cdf curves pre sented in the previous section is that the cdf curve intercepts the line at a relatively flat slope indicating that there are few time intervals in which close to the highest power is drawn by the ma chines if we could somehow remove those few intervals we might be able to further increase the number of machines hosted within a given power budget power capping techniques accomplish that by setting a value below the actual peak power and preventing that number from being exceeded through some type of control loop there are numerous ways to implement this but they generally consist of a power monitoring system possibly such as ours or one based on direct power sensing and a power throttling mech anism power throttling generally works best when there is a set of jobs with loose service level guarantees or low priority that can be forced to reduce consumption when the datacenter is approaching the power cap value power consumption can be reduced simply by descheduling tasks or by using any available component level power management knobs such as cpu voltage frequency scaling note that the power sensing throttling mechanisms needed for power capping are likely needed anyway even if we do not intend to cap power but simply want to take advantage of the power usage gaps shown in the cdf graphs in those cases it is required to normalized power full distribution normalized power zoomed view figure websearch cdf of power usage normalized to actual peak normalized power full distribution normalized power zoomed view figure webmail cdf of power usage normalized to actual peak 98 96 normalized power full distribution normalized power zoomed view figure mapreduce cdf of power usage normalized to actual peak 98 96 normalized power full distribution 78 86 88 94 normalized power zoomed view figure cdf of websearch webmail mapreduce and the mixture of all at the cluster level 98 96 normalized power full distribution 65 85 normalized power zoomed view figure cdf of a real datacenter insure against poorly characterized workloads or unexpected load spikes table presents the gains that could be achieved with such a scheme for each workload we show the potential for increased machine deployment given an allowance of or of time spent in power capping mode we also include the no power capping numbers for comparison we have excluded websearch and web mail by themselves from power capping because given their on line nature they might not have much opportunity for power reduc tion at peak load overall the additional gains in machine deployment are notice able but relatively modest generally captures most of the ben efits with only little additional gains for of capping time the best case is mapreduce which shows an increase from in po tential increased machine deployment without power capping to with capping of the time notably mixing the workloads diminishes the relative gains because the different workloads are already decreasing the likelihood of a simultaneous power spike in all machines the table also shows the number and length of power capping intervals that would be incurred for each workload this informa tion gives some insight into how often the power capping system would be triggered which in turn is useful for deciding on what kind of mechanism to use fewer longer intervals are probably more desirable because there is always some loss upon entering and leaving the power capping interval perhaps the biggest advantage dynamic power capping is that it can relax the requirement to accurately characterize workloads prior to deployment and provide a safety valve for cases where workload behavior changes unexpectedly average versus peak power another interesting observation that can be derived from our data is the difference between the average and observed peak power draw of a workload or mix of workloads while peak power draw is the most important quantity for guiding the deployment of ma chines to a datacenter average power is what determines the power bill we mentioned load dependent power variations as one of the factors leading to inefficient use of the power budget in an earlier section we are now able to quantify it table shows the ratio of average power to observed peak power over the half year interval for the different workloads and mixes of workloads the ratios reflect the different dynamic ranges for the different workloads websearch has the highest dynamic range and lowest average to peak ratio at mapreduce is somewhat higher and webmail has the highest ratio at close to the two table impact of power capping table average and observed peak power normalized to ac tual peak at the cluster level mixed workloads also show higher ratios with for the mix of the three tuned workloads and for the real datacenter we see that a mix of diverse workloads generally reduces the difference between average and peak power another argument in favor of this type of deployment note that even for this best case on the order of of the power budget remains stranded simply because of the difference between average and peak power which further increases the relative weight of power provisioning costs over the cost of energy two power savings approaches in the previous section we used the power modeling infrastruc ture to analyze actual power consumption of various workloads here we take the same activity data from our machines over the six month time period and use the model to simulate the potential for power and energy saving of two schemes cpu voltage frequency scaling cpu voltage and frequency scaling dvs for short is a useful technique for managing energy consumption that has recently been made available to server class processors here we ask our power model to predict how much energy savings and peak power reduc tions could have been achieved had we used power management techniques based on dvs in the workloads analyzed in the previ ous section for simplicity and for the purpose of exploring the limits of the benefit we use an oracle style policy for each machine and each data collection interval if the cpu utilization is below a certain threshold we simulate dvs activation by the cpu com are various cpus in the market today that are capable of such power reductions through dvs ponent of the total power while leaving the power consumption of the remaining components unchanged without detailed application characterization we cannot deter mine how system performance might be affected by dvs therefore we simulate three cpu utilization thresholds for triggering dvs we pick as a conservative threshold to exam ine how much benefit can be achieved with almost no performance impact we use as a very aggressive threshold for the scenario where performance can be degraded significantly or the application has substantial amount of performance slack figure shows the impact of cpu dvs at the cluster level on our three workloads and on the real datacenter dvs has a more significant potential impact on energy than peak power with sav ings of over when using the more aggressive threshold in two out of four cases we expected this since in periods of cluster wide peak activity it is unlikely that many servers will be below the dvs trigger threshold it is still surprising that there are cases where dvs can deliver a moderate but noticeable reduction in maximum observed power this is particularly the case for the real datacenter where the workload mix enables peak power reductions between among the three workloads websearch has the highest reduc tion in both peak power and energy websearch is the most compute intensive workload therefore the cpu consumes a larger percent age of the total machine power allowing dvs to produce larger re ductions relative to total power dvs achieves the least energy sav ings for webmail which has the narrowest dynamic power range and relatively high average energy usage webmail is generally de ployed on machines with more disks and therefore the cpu is a smaller contributor to the total power resulting in a correspond ingly smaller impact of dvs mapreduce shows the least reduc tion in peak power since it also tends to use machines with more disks while achieving even higher peak power usage than webmail these two factors create the most difficult scenario for dvs it is also worth noting that due to our somewhat coarse data collection interval min the dvs upside is somewhat under estimated here the switching time of the current dvs technology can accommodate a sub second interval so bigger savings might be possible using finer grained triggers improving non peak power efficiency power efficiency of computing equipment is almost invariably measured when running the system under maximum load gen erally when performance per watt is presented as a rating it is peak power reduction b energy saving figure impact of cpu dvs at datacenter level implicitly understood that the system was exercised to maximum performance and upon reaching that the power consumption was measured however as the analysis in the previous section showed the reality is that machines operate away from peak activity a good fraction of the time therefore it is important to conserve power across the activity spectrum and not just at peak activity figure shows the power consumption at idle no activity as a fraction of peak power from five of the server configurations we de ploy idle power is significantly lower than the actual peak power but generally never below of peak ideally we would like our systems to consume no power when idle and for power to increase roughly proportionally with increased activity a behavior similar to the curves in figure but where pidle is near zero arguably sys tems with this behavior would be equally power efficient regardless of activity level to assess the benefits of such behavioral change we altered our model so that idle power for every machine was set to of the actual peak power all other model parameters in cluding actual peak power remained the same as before the results shown in figure reveal that the gains can be quite substantial the maximum cluster level peak power was re duced between for our three workloads with correspond ing energy savings of in a real datacenter however the observed maximum power consumption dropped over while less than half the energy was used the fact that such dramatic gains are possible without any changes to peak power consumption strongly suggest that system and component designers should strive to achieve such behavior in real servers it is important to note that the machines in our study especially the ones running the three workloads were rarely fully idle there fore inactive power modes such as sleep or standby modes are unlikely to achieve the same level of savings power provisioning strategies from the results in the previous sections we can draw some con clusions about strategies for maximizing the amount of compute equipment that can be deployed at a datacenter with a given power capacity first of all it is important to understand the actual power draw of the machines to be deployed nameplate power figures are so conservative as to be useless for the deployment process accurate power measurements of the machines are needed in the actual con figurations to be deployed and running benchmarks that maximize overall power draw figure idle power as fraction of peak power in server configurations figure power and energy savings achievable by reducing idle power consumption to of peak the characterization of application power draw at different lev els of deployment granularity allows us to judge the potential for safely over subscribing pieces of the power distribution hierarchy over subscription at the rack level is not safe in both websearch and mapreduce individual racks approach very close to peak actual power during some time intervals webmail has a little room for over subscription at the rack level at at the pdu level more potential for over subscription exists at the cluster level there is a noticeable difference between observed and actual peak power allowing for the deployment of between more machines for individual applications the headroom increases when appli cations are mixed together indicating that it is desirable to do so mixing also leads to a narrowing of average to peak power which is desirable from a utilization of infrastructure standpoint finally we have shown that in a real cluster the deployment of less well tuned applications and other conditions leading to poorly utilized machines can drive the headroom close to once again this is using peak actual power to guide deployment the more common practice of using nameplate power further inflates these numbers leading to headroom for more machines to be deployed a dynamic power management scheme to cap the peak power draw at some pre determined value has two advantages first of all it can act as a safety valve protecting the power distribution hierarchy against overdraw it thus allows for aggressive deploy ment of machines even in the face of poorly characterized appli cations or unexpected load spikes secondly it enables additional over subscription of the available power capping power for even a small fraction of overall time can deliver noticeable additional gains in machine deployment while dynamic voltage frequency scaling may not produce much reduction of peak power draw at the rack level there is a notice able reduction at the cluster level depending on application peak power reductions of up to are seen for aggressive schemes growing up to for the real datacenter workload mix even the least aggressive scheme netted an reduction in peak power for the real datacenter mix related work to overcome the conservativeness of nameplate power the in dustry starts to provide coarse grained power calculators based on customized component and activity level selections models that estimate power usage based on activity metrics have been stud ied by a number of researchers contreras and martonosi use hardware event counters to derive power estimates that are accu rate at sub second time intervals our approach is more similar to that of economou et al which is based on coarser activity metrics such as cpu load and i o activity an even coarser model ing scheme is presented by bohrer et al in their study of energy management techniques our results further validate the usefulness of relatively simple low overhead power modeling techniques there is growing number of studies of power management tech niques and power energy aware scheduling policies at the single system level felter et al study power shifting a technique to reduce peak power with minimal performance impact that is based on dynamically re allocating power to the most performance criti cal components carrera et al instead focus on the disk subsys tem proposing energy management strategies that include the use of multi speed disks and combinations of server class and laptop drives at the cluster or datacenter level chase et al treat energy as a resource to be scheduled by a hosting center management infrastructure and propose a scheme that can reduce energy us age by while still meeting a specified level of service moore et al present a framework for measurement and analysis of datacenter level workloads with a measurement infrastructure that has some similarities to ours they use a synthetic workload to evaluate their framework running on two moderate sized clusters our paper is not concerned with any specific power management scheme we believe that a wide range of techniques could be effec tive at the datacenter level given the large time constants involved the studies of femal and freeh and of ranganathan et al are probably the most closely related to ours ranganathan et al note that more efficient power management solutions can be reached by managing power at the rack or ensemble level than at individual blades while we agree with that assertion our results seem to contradict their observation that synchronized power us age spikes never happen in practice our data indicates that such spikes at the rack level do happen suggesting that the kind of power management solutions they proposed might be more appropriate for much larger groups of machines we speculate that the much larger scale of our workloads and how well they are tuned are partly responsible for this discrepancy in observed behavior fe mal and freeh deal directly with the issue of power over subscription in small clusters tens of servers and propose a dy namic control scheme based on dynamic cpu dvs to reduce peak consumption although our estimates appear to be more modest than theirs we agree that cpu dvs can have an impact on cluster level peak power savings finally some researchers focus on the cooling infrastructure and temperature management as previously stated our paper does not deal with the energy or power used for the cooling infras tructure these are important research areas that are complemen tary to our work conclusions some of the most interesting computing systems being built to day look more like a warehouse than a refrigerator power provi sioning decisions for such systems can have a dramatic economic impact as the cost of building large datacenters could surpass the cost of energy for the lifetime of the facility since new datacenter construction can take tens of months intelligent power provision ing also has a large strategic impact as it may allow an existing facility to accommodate the business growth within a given power budget in this paper we study how power usage varies over time and as the number of machines increases from individual racks to clusters of up to five thousand servers by using multiple production work loads we are also able to quantify how power usage patterns are affected by workload choice the understanding of power usage dynamics can inform the choice of power management and provi sioning policies as well as quantify the potential impact of power and energy reduction opportunities to our knowledge this is the first power usage study at the scale of datacenter workloads and the first reported use of model based power monitoring techniques for power provisioning in real production systems we echo commonly held beliefs that nameplate ratings are of little use in power provisioning as they tend to grossly overesti mate actual maximum usage using a more realistic peak power definition we were able to quantify the gaps between maximum achieved and maximum theoretical power consumption of groups of machines these gaps would allow hosting between and more computing equipment for individual well tuned appli cations and as much as in a real datacenter running a mix of applications through careful over subscription of the datacenter power budget we find that power capping mechanisms can en able us to capitalize on those opportunities by acting as a safety net against the risks of over subscription and are themselves able to provide additional albeit modest power savings we note how ever that over subscribing power at the rack level is quite risky given that large internet services are capable of driving hundreds of servers to high activity levels simultaneously the more easily exploitable over subscription opportunities lie at the facility level thousands of servers we also find that cpu dynamic voltage frequency scaling might yield moderate energy savings up to although it has a more limited peak power savings potential it is still surprising that a technique usually dismissed for peak power management can have a noticeable impact at the datacenter level finally we argue that component and system designers should consider power efficiency n o t i mply a t p e ak p e rformance levels but across the activity range as even machines used in well tuned large scale workloads will spend a significant fraction of their oper ational lives below peak activity levels we show that peak power consumption at the datacenter level could be reduced by up to and energy usage could be halved if systems were designed so that lower activity levels meant correspondingly lower power usage pro files reducing network energy consumption via sleeping and rate adaptation sergiu nedevschi lucian popa gianluca iannaccone sylvia ratnasamy david wetherall abstract we present the design and evaluation of two forms of power management schemes that reduce the energy consumption of networks the first is based on putting network components to sleep during idle times reducing energy consumed in the absence of packets the second is based on adapting the rate of network operation to the offered workload reducing the energy consumed when actively processing packets for real world traffic workloads and topologies and us ing power constants drawn from existing network equip ment we show that even simple schemes for sleeping or rate adaptation can offer substantial savings for in stance our practical algorithms stand to halve energy consumption for lightly utilized networks we show that these savings approach the maximum achiev able by any algorithms using the same power manage ment primitives moreover this energy can be saved with out noticeably increasing loss and with a small and con trolled increase in latency finally we show that both sleeping and rate adaptation are valuable de pending primarily on the power profile of network equipment and the utilization of the network itself introduction in this paper we consider power management for networks from a perspective that has recently begun to receive attention the conservation of energy for operating and environmental reasons energy consump tion in network exchanges is rising as higher capacity network equipment becomes more power hungry and requires greater amounts of cooling combined with rising energy costs this has made the cost of powering network exchanges a substantial and growing fraction of the total cost of ownership up to half by some estimates various studies now estimate the power usage of the us network infrastructure at between and twh year or year at a rate of kwh depending on what is included public concern about carbon footprints is also rising and stands to affect network equipment much as it has computers university of california berkeley intel research berkeley university of washingon intel research seattle via standards such as energystar in fact energystar standard proposals for discuss slower operation of network links to conserve energy when idle a new ieee task force was launched in early to focus on this issue for ethernet fortunately there is an opportunity for substantial re ductions in the energy consumption of existing networks due to two factors first networks are provisioned for worst case or busy hour load and this load typically exceeds their long term utilization by a wide margin for example measurements reveal backbone utilizations under and up to hour long idle times at access points in enterprise wireless networks second the energy consumption of network equipment remains sub stantial even when the network is idle the implication of these factors is that most of the energy consumed in networks is wasted our work is an initial exploration of how overall network energy consumption might be reduced without adversely affecting network performance this will require two steps first network equipment ranging from routers to switches and nics will need power man agement primitives at the hardware level by analogy power management in computers has evolved around hardware support for sleep and performance states the former e g c states in intel processors reduce idle con sumption by powering off sub components to different extents while the latter e g speedstep p states in intel processors tradeoff performance for power via operating frequency second network protocols will need to make use of the hardware primitives to best effect again by analogy with computers power management preferences control how the system switches between the available states to save energy with minimal impact on users of these two steps our focus is on the network protocols admittedly these protocols build on hardware support for power management that is in its infancy for networking equipment yet the necessary support will readily be deployed in networks where it proves valuable with forms such as sleeping and rapid rate selection for ethernet already under development for comparison computer power management compat ible with the acpi standard has gone from scarce to widely deployed over the past five to ten years and is now expanding into the server market thus our goal is to learn what magnitude of energy savings a protocol using feasible hardware primitives might offer what per formance tradeoff comes with these savings and which of the feasible kinds of hardware primitives would max imize benefits we hope that our research can positively influence the hardware support offered by industry the hardware support we assume from network equip ment is in the form of performance and sleep states performance states help to save power when routers are active while sleep states help to save power when routers are idle the performance states we assume dynamically change the rate of links and their associated interfaces the sleep states we assume quickly power off network interfaces when they are idle we develop two approaches to save energy with these primitives the first puts network interfaces to sleep during short idle periods to make this effective we introduce small amounts of buffering much as aps do for sleeping clients this collects packets into small bursts and thereby creates gaps long enough to profitably sleep potential concerns are that buffering will add too much delay across the network and that bursts will exacerbate loss our algorithms arrange for routers and switches to sleep in a manner that ensures the buffering delay penalty is paid only once not per link and that routers clear bursts so as to not amplify loss noticeably the result is a novel scheme that differs from schemes in that all network elements are able to sleep when not utilized yet added delay is bounded the second approach adapts the rate of individual links based on the utilization and queuing delay of the link we then evaluate these approaches using real world network topologies and traffic workloads from abilene and intel we find that rate adaptation and sleeping have the potential to deliver substantial energy savings for typical networks the simple schemes we develop are able to capture most of this energy saving potential our schemes do not noticeably degrade network performance and both sleeping and rate adaptation are valuable depending primarily on the utilization of the network and equipment power profiles approach this section describes the high level model for power consumption that motivates our rate adaptation and sleeping solutions as well as the methodology by which we evaluate these solutions power model overview active and idle power a network element is active when it is actively processing incoming or outgoing traf fic and idle when it is powered on but does not process traffic given these modes the energy consumption for a network element is e pata piti where pa pi denote the power consumption in active and idle modes respectively and ta ti the times spent in each mode reducing power through sleep and performance states sleep states lower power consumption by putting sub components of the overall system to sleep when there is no work to process thus sleeping reduces the power consumed when idle i e it reduces the piti term of eqn by reducing the pi to some sleep mode power draw ps where ps pi performance states reduce power consumption by lowering the rate at which work is processed as we elaborate on in later sections some portion of both active and idle power consumption depends on the frequency and voltage at which work is processed hence performance states that scale frequency and or voltage reduce both the pa and pi power draws resulting in an overall reduction in energy consumption we also assume a penalty for transitioning between power states for simplicity we measure this penalty in time typically milliseconds treating it as a period in which the router can do no useful work we use this as a simple switching model that lumps all penalties ignor ing other effects that may be associated with switches such as a transient increase in power consumption thus there is also a cost for switching between states networks with rate adaptation and sleeping support in a network context the sleeping and rate adaptation decisions one router makes fundamentally impacts and is impacted by the decisions of its neighboring routers moreover as we see later in the paper the strategies by which each is best exploited are very different intuitively this is because sleep mode savings are best exploited by maximizing idle times which implies processing work as quickly as possible while performance scaling is best exploited by processing work as slowly as possible which reduces idle times hence to avoid complex interactions we consider that the whole network or at least well defined components of it run in either rate adaptation or sleep mode we develop the specifics of our sleeping schemes in section and our rate adaptation schemes in section note that our solutions are deliberately constructed to apply broadly to the networking infrastructure from end host nics to switches and ip routers etc so that they may be applied wherever they prove to be the most valuable they are not tied to ip layer protocols methodology the overall energy savings we can expect will depend on the extent to which our power management algorithms can successful exploit opportunities to sleep or rate adapt as well as the power profile of network equipment i e relative magnitudes of pa pi and ps to clearly separate the effect of each we evaluate sleep solutions in terms of the fraction of time for which network elements can sleep and rate adaptation solutions in terms of the reduction in the average rate at which the network operates in this way we assess each solution with the appropriate baseline we then evaluate how these metrics translate into overall network energy savings for different equipment power profiles and hence compare the relative merits of sleeping and rate adaptation section for both sleep and rate adaptation we calibrate the savings achieved by our practical solutions by comparing to the maximum savings achievable by optimal but not necessarily practical solutions in network environments where packet arrival rates can be highly non uniform allowing network elements to transition between operating rates or sleep active modes with corresponding transition times can introduce additional packet delay or even loss that would have not otherwise occurred our goal is to explore solutions that usefully navigate the tradeoff between potential energy savings and performance in terms of performance we measure the average and percentile of the end to end packet delay and loss in the absence of network equipment with hardware support for power management we base our evaluations on packet level simulation with real world network topologies and traffic workloads the key factors on which power savings then depend beyond the details of the solutions themselves are the technology constants of the sleep and performance states and the characteristics of the network in particular the utilization of links determines the relative magnitudes of tactive and tidle as well as the opportunities for profitably exploiting sleep and performance states we give simple models for technology constants in the following sections to cap ture the effect of the network on power savings we drive our simulation with two realistic network topologies and traffic workloads abilene and intel that are summa rized below we use as our packet level simulator abilene we use abilene as a test case because of the ready availability of detailed topology and traffic infor mation the information from provides us with the link connectivity weights to compute routes latencies and capacities for abilene router level topology we use measured abilene traffic matrices tms available in the community to generate realistic workloads over this topology unless otherwise stated we use as our default a traffic matrix whose link utilization levels reflect the average link utilization over the entire day this corresponds to a link utilization on average with bottleneck links experiencing about utilization we linearly scale tms to study performance with increasing utilization up to a maximum average network utilization of as beyond this some links reach very high utilizations finally while the tms specify the minute average rate observed for each ingress egress pair we still require a packet level traffic generation model that creates this rate in keeping with previous studies we generate traffic as a mix of pareto flows and for some results we use constant bit rate cbr traffic as per standard practice we set router queue sizes equal to the bandwidth delay product in the network we use the bandwidth of the bottleneck link and a delay of intel as an additional real world dataset we collected topology and traffic information for the global intel enterprise network this network connects intel sites worldwide from small remote offices to large multi building sites with thousands of users it comprises approximately routers and over links with capacities ranging from to to simulate realistic traffic we collected unsampled netflow records from the core routers the records exported by each router every minute contain per flow information that allows us to recreate the traffic sourced by ingress nodes putting network elements to sleep in this section we discuss power management algorithms that exploit sleep states to reduce power consumption during idle times model and assumptions background a well established technique as used by microprocessors and mobiles is to reduce idle power by putting hardware sub components to sleep for example modern intel processors such as the core duo have a succession of sleep states called c states that offer increasingly reduced power at the cost of increasingly high latencies to enter and exit these states we assume similar sleep states made available for network equip ment for the purpose of this study we ignore the options afforded by multiple sleep states and assume as an initial simplification that we have a single sleep state model we model a network sleep state as character ized by three features or parameters the first is the power draw in sleep mode ps which we assume to be a small fraction of the idle mode power draw pi the second characterizing parameter of a sleep state is the time δ it takes to transition in and out of sleep states higher values of δ raise the bar on when the network element can profitably enter sleep mode and hence δ critically affects potential savings while network interface cards can make physical layer transitions in as low as transition times that involve restoring state at higher layers memory operating system are likely to be higher we thus evaluate our solutions over a wide range values of transition times finally network equipment must support a mechanism for invoking and exiting sleep states the option that makes the fewest assumptions about the sophistication of hardware support is timer driven sleeping in which the network element enters and exits sleep at well defined times prior to entering sleep the network element specifies the time in the future at which it will exit sleep and all packets that arrive at a sleeping interface are lost the second possibility described in is for routers to wake up automatically on sensing incoming traffic on their input ports to achieve this wake on arrival woa the circuitry that senses packets on a line is left powered on even in sleep mode while support for woa is not common in either computers or interfaces today this is a form of hardware support that might prove desirable for future network equipment and is currently under discussion in the ieee task force note that even with wake on arrival bits arriving during the transition period δ are effectively lost to handle this the authors in propose the use of dummy packets to rouse a sleeping neighbor a node a that wishes to wake b first sends b a dummy packet and then waits for time δ before transmitting the actual data traffic the solutions we develop in this paper apply seamlessly to either timer driven or woa based hardware measuring savings and performance in this section we measure savings in terms of the percentage of time network elements spend asleep and performance in terms of the average and percentile of the end to end packet delay and loss we assume that individual line cards in a network element can be independently put to sleep this allows for more opportunities to sleep than if one were to require that a router sleep in its entirety as the latter is only possible when there is no incoming traf fic at any of the incoming interfaces correspondingly our energy savings are with respect to interface cards which typically represent a major portion of the overall consumption of a network device that said one could in addition put the route processor and switch fabric to sleep at times when all line cards are asleep approaches and potential savings for interfaces that support wake on arrival one ap proach to exploiting sleep states is that of opportunistic sleeping in which link interfaces sleep when idle i e a router is awakened by an incoming dummy packet and after forwarding it on returns to sleep if no subsequent figure packets within a burst are organized by destination packet arrives for some time while very simple such an approach can result in frequent transitions which limits savings for higher transition times and or higher link speeds for example with a link even under low utilization and packet sizes of the average packet inter arrival time is very small thus while opportunistic sleeping might be effective in lans with high idle times for fast links this technique is only effective for very low transition times δ we quantify this shortly in addition opportunistic sleep is only possible with the more sophisticated hardware support of wake on arrival to create greater opportunities for sleep we consider a novel approach that allows us to explicitly control the tradeoff between network performance and energy savings our approach is to shape traffic into small bursts at the edges of the network edge devices then transmit packets in bunches and routers within the network wake up to process a burst of packets and then sleep until the next burst arrives the intent is to provide sufficient bunching to create opportunities for sleep if the load is low yet not add excessive delay this is a radical approach in the sense that much other work seeks to avoid bursts rather than create them e g token buckets for qos congestion avoidance buffering at routers as our measurements of loss and delay show our schemes avoid the pitfalls associated with bursts because we introduce only a bounded and small amount of burstiness and a router never enters sleep until it has cleared all bursts it has built up more precisely we introduce a buffer interval b that controls the tradeoff between savings and performance an ingress router buffers incoming traffic for up to b ms and once every b ms forwards buffered traffic in a burst to ensure that bursts created at the ingress are retained as they traverse through the network an ingress router arranges packets within the burst such that all packets destined for the same egress router are contiguous within the burst see figure the above buffer and burst approach b b creates alternating periods of contiguous activity and sleep leading to fewer transitions and amortizing the transition penalty δ over multiple packets this improvement comes at the cost of an added end to end delay of up sj si idle bound woa pareto woa cbr optb b si sj si sj b figure examples of burst synchronization to b ms note that because only ingress routers buffer traffic the additional delay due to buffering is only incurred once along the entire ingress to egress path as importantly this approach unlike opportunistic sleep can be used by interfaces that support only timer driven sleep a router that receives a burst from upstream router at time knows that the next start of burst will arrive at time b and can hence sleep between bursts the question then is how significant are the savings this approach enables for reasonable additional delay we note that the best possible savings would occur if a router received the incoming bursts from all ingress routers close in time such that it processes all incoming bursts and returns to sleep thus incurring exactly one sleep wake transition per b ms this might appear pos sible by having ingress routers coordinate the times at which they transmit bursts such that bursts from different ingresses arrive close in time at intermediate routers for example consider the scenario in figure a where ingress routers and are scheduled to transmit traffic at times and respectively if instead were to schedule its burst for time instead then bursts from and would align in time at thus reducing the number of distinct burst times and sleep to wake transitions at downstream routers and unfortunately the example in figure b suggests this is unachievable for general topologies here si and sj represent the arrival times of incoming bursts to nodes and respectively and we see that the topology makes it impossible to find times si and sj that could simulta neously align the bursts downstream from and we thus use a brute force strategy to evaluate the maximum achievable coordination for a given topology and traffic workload we consider the start of burst time for traffic from each ingress i to egress j denoted sij and perform an exhaustive search of all sij to find a set of start times that minimizes the number of transitions across all the interfaces in the network we call this scheme optb b clearly such an algorithm is not practical and we use it merely as an optimistic bound on what might be achievable were nodes to coordinate in shaping traffic under a buffer and burst approach we compare the sleep time achieved by optb b to the upper bound on sleep as given by µ where µ is the average utilization figure time asleep using optb b and opportunistic sleeping and compared to the upper bound µ network utilization this upper bound is not achievable by any algorithm since unlike optb b it does not take into account the overhead δ due to sleep wake transi tions nonetheless it serves to capture the loss in savings due to δ and the inability to achieve perfect coordination any traffic shaping incurs some additional complexity and hence a valid question is whether we need any traffic shaping or whether opportunistic sleeping that does not require shaping is enough we therefore also compare optb b to opportunistic sleeping based on wake on arrival woa for this naive woa we assume optimistically that an interface knows the precise arrival time of the subsequent packet and returns to sleep only for inter packet arrival periods greater than δ because the performance of opportunistic woa depends greatly on the inter arrival times of packets we evaluate woa for two types of traffic constant bit rate cbr and pareto for each of the above bounds figure plots the percentage of time asleep under increasing utilization in abilene we use a buffer period of b and assume a conservative transition time δ of comparing the savings from optb b to the utilization bound we see that a traffic shaping approach based on buffer and burst can achieve much of the potential for exploiting sleep as expected even at very low utiliza tion woa with cbr traffic can rarely sleep perhaps more surprising is that even with bursty traffic woa performs relatively poorly these results suggest that even assuming hardware woa traffic shaping offers a significant improvement over opportunistic sleep a practical algorithm we consider a very simple buffer and burst scheme called practb b in which each ingress router sends its bursts destined for the various egresses one after the other in a single train of bursts at routers close to the ingress this appears as a single burst which then disperses as it traverses through the network practb b bounds the number of bursts and corre spondingly the number of transitions seen by any router r in an interval of b ms to at most ir the total number of ingress routers that send traffic through r in practice our results show that the number of bursts seen by r in time bms is significantly smaller than this bound average utilization average utilization average utilization a average delay practb b b delay ile figure time asleep using cbr and pareto traffic practb b is simple it requires no inter router co ordination as the time at which bursts are transmitted is decided independently by each ingress router for net works supporting wake on arrival the implementation is trivial the only additional feature is the implementation of buffer and burst at the ingress nodes for networks that employ timer driven sleeping packet bursts would need to include a marker denoting the end of burst and notifying the router of when it should expect the next burst on that interface evaluation we evaluate the savings vs performance tradeoff achieved by practb b algorithm and the impact of equipment and network parameters on the same savings vs performance using practb b we compare the sleep time achieved by practb b to that achievable by optb b in terms of performance we compare the end to end packet delay and loss in a net work using practb b to that of a network that never sleeps as today as this shows the overall performance penalty due to our sleep protocols figure plots the sleep time with increasing utilization on the abilene network using a buffering interval b we plot the percentage sleep time under both cbr and pareto traffic workloads we see that even a scheme as simple as practb b can create and exploit significant opportunities for sleep and approaches the savings achieved by the significantly more complex optb b as with opportunistic sleeping we see that practb b savings with cbr traffic are lower than for the more bursty pareto workloads but that this reduction is significantly smaller in the case of practb b than with opportunistic sleeping recall figure that pareto traffic improves savings is to be expected as burstier traffic only enhances our bunching strategy figures a and b plot the corresponding average and percentile of the end to end delay as expected we see that the additional delay in both cases is propor tional to the buffering interval b note that this is the end to end delay reinforcing that the buffering delay b is incurred once for the entire end to end path figure the impact on delay of practb b we see that for higher b the delay grows slightly faster with utilization e g compare the absolute increase in delay for b to because this situation is more prone to larger bursts overlapping at intermediate routers however this effect is relatively small even in a situation combining larger b and larger utilizations and is negligible for smaller b and or more typical utilizations we see that both average and maximum delays in crease abruptly beyond network utilizations exceeding this occurs when certain links approach full utilization and queuing delays increase recall that the utilization on the horizontal axis is the average network wide utilization however this increase occurs even in the default network scenario and is thus not caused by practb b traffic shaping finally our measurements revealed that practb b introduced no additional packet loss rela tive to the default network scenario until we approach utilizations that come close to saturating some links for example in a network scenario losses greater than occur at utilization without any buffering they occur at utilization with b and at utilization with b as networks do not typically operate with links close to saturation point we do not expect this additional loss to be a problem in practice in summary the above results suggest that practb b can yield significant savings with a very small and controllable impact on network delay and loss for example at a utilization of a buffering time of just b allows the network to spend over of its time in sleep mode for under added delay impact of hardware characteristics we now evaluate how the transition time δ affects the performance of practb b figure a plots the sleep time achieved by practb b for a range of transition times and compares this to the ideal case of having instantaneous transitions as expected the ability to sleep degrades drastically with increasing δ this observation holds across various buffer intervals b as illustrated in figure b that plots the sleep time achieved at typical utilization for ideal b for example ethernet links dissipate between when operating between compared to between second operating at a lower frequency also allows the use of dynamic voltage scaling dvs that reduces the operating voltage this allows power to scale cubically and hence energy con sumption quadratically with operating frequency average utilization impact of δ transition time ms impact of b dvs and frequency scaling are already common in microprocessors for these reasons we assume the application of these techniques to network links and associated equipment i e linecards figure the impact of hardware constants on sleep time link utilization figure time asleep per link increasing transition times and different values of b these findings reinforce our intuition that hardware support featuring low power sleep states and quick transitions preferably between these states are essential to effectively save energy impact of network topology we now evaluate practb b for the intel enterprise network the routing structure of the intel network is strictly hierarchical with a relatively small number of nodes that connect to the wide area because of this we find a wide variation in link utilization far more than on the abilene network over of links have utilizations below while a small number of links can see significantly higher utilizations of between correspond ingly the opportunity for sleep also varies greatly across links this is shown in figure each point in the scatter plot corresponds to a single link and we look at sleep times for two transition times and we see that the dominant trends in sleep time vs utilization remains and that higher δ yields lower savings rate adaptation in networks this section explores the use of performance states to reduce network energy consumption model and assumptions background in general operating a device at a lower frequency can enable dramatic reductions in energy consumption for two reasons first simply operating more slowly offers some fairly substantial savings transceivers while the use of dvs has been demon strated in prototype linecards it is not currently sup ported in commercial equipment and hence we investi gate savings under two different scenarios equipment that supports only frequency scaling and equipment that supports both frequency and voltage scaling model we assume individual links can switch perfor mance states independently and with independent rates for transmission and reception on interfaces hence the savings we obtain apply directly to the consumption at the links and interface cards of a network element although in practice one could also scale the rate of operation of the switch fabric and or route processor we assume that each network interface supports n performance states corresponding to link rates rn with ri ri and rn rmax the default maximum link rate and we investigate the effect that the granularity and distribution linear vs exponential of these rates has on the potential energy savings the final defining characteristic of performance states is the transition time denoted δ during which packet transmission is stalled as the link transitions between successive rates we explore performance for a range of transition times δ from to milliseconds measuring savings and performance as in the case of sleep we re interested in solutions that reduce the rate at which links operate without significantly affecting performance in this section we use the percentage reduction in average link rate as an indicative measure of energy savings and relate this to overall energy savings in section where we take into account the power profile of equipment including whether it supports dvs or not in terms of performance we again measure the average and percentile of the end to end packet delay and packet loss an optimal strategy our initial interest is to understand the extent to which performance states can help if used to best effect for a dvs processor it has been shown that the most energy efficient way to execute c cycles within a given time interval t is to maintain a constant clock speed of figure an illustration of delay constrained service curves and the service curve minimizing energy c t in the context of a network link this translates into sending packets at a constant rate equal to the average arrival rate however under non uniform traffic this can result in arbitrary delays and hence we instead look for an optimal schedule of rates i e the set of rates at which the link should operate at different points in time that minimize energy while respecting a specified constraint on the additional delay incurred at the link more precisely given a packet arrival curve ac we look for a service curve sc that minimizes energy con sumption while respecting a given upper bound d on the per packet queuing delay the delay parameter d thus serves to tradeoff savings for increased delay figure a shows an example arrival curve and the associated latest departure curve ac d which is simply the arrival curve shifted in time by the delay bound d to meet the delay constraint the service curve sc must lie within the area between the arrival and latest departure curves in the context of wireless links proves that if the energy can be expressed as a convex monotonically increasing function of the transmission rate then the minimal energy service curve is the shortest euclidean distance in the arrival space bytes time between the arrival and shifted arrival curves in the scenario where we assume dvs support the energy consumption is a convex monotonically increas ing function of link rate and thus this result applies to our context as well where only frequency scaling is to calibrate practical protocols we will evaluate the savings achieved by applying the above per link solution at all links in the network and call this approach link optra one issue in doing so is that the service curves at the different links are inter dependent i e the service curve for a link l depends in turn on the service curves at other links since the latter in turn determine the arrival curve at l we address this by applying the per link optimal algorithm iteratively across all links until the service and arrival curves at the different links converge a practical algorithm building on the insight offered by the per link opti mal algorithm we develop a simple approach called practra practical rate adaptation that seeks to nav igate the tradeoff between savings and delay constraints a practical approach differs from the optimum in that it does not have knowledge of future packet arrivals it can only choose among a fixed set of available rates rn and iii at every rate switch it incurs a penalty δ during which it cannot send packets while knowledge of the future arrival rate is unavail able we can use the history of packet arrivals to predict the future arrival rate we denote this predicted arrival rate as rˆf and estimate it with an exponentially weighted moving average ewma of the measured history of past arrivals similarly we can use the current link buffer size q and rate ri to estimate the potential queuing delay so as to avoid violating the delay constraint with these substitutes we define a technique inspired by the per link optimal algorithm in practra packets are serviced at a constant rate until we intersect one of the two bounding curves presented earlier figure the arrival curve ac and the latest departure curve ac d thus we avoid increasing the operating rate ri unless not doing so would violate the delay constraint this leads to the following condition for rate increases a link operating at rate ri with current queue size q supported any service curve between the arrival and shifted arrival curves would achieve the same energy increases its rate to ri iff q i d or δrˆf q d δ i savings and therefore the service curve with the shortest euclidean distance would be optimal in this case too in summary for both frequency scaling and dvs the shortest distance service curve would achieve the highest possible energy savings fig illustrates an example of such a minimal energy service curve intuitively this is the set of lowest constant rates obeying the delay constraint note that this per link optimal strategy is not suited to practical implementation since it assumes perfect knowledge of the future arrival curve link rates of infinite granularity and ignores switching overheads nonetheless it is useful as an estimate of the potential savings by which the first term checks whether the delay bound d would be violated were we to maintain the current link rate the second constraint ensures that the service curve does not get too close to the delay constrained curve which would prevent us from attempting a rate increase in the future without violating the delay bound that is we need to allow enough time for a link that increases its rate to subsequently process packets that arrived during the transition time estimated by δrˆf and its already accumulated queue note that we cannot use delay constraints d smaller than the transition time δ similarly the condition under which we allow a rate decrease is as follows a link operating at rate ri with current queue size q decreases its rate to ri iff q and rˆf ri first we only attempt to switch to a lower rate when the queue at the link is empty q intuitively this corre sponds to an intersection between the arrival curve and the service curve however we don t always switch to a lower rate ri when a queue empties as doing so would prevent the algorithm from operating in a desired steady state mode with zero queuing delay instead the desirable steady state is one where ri rf ri and we want to avoid oscillating between rates which uniform r uniform r average utilization would lead to larger average delay for example a link that sees a constant arrival rate of might oscillate between and incurring queuing figure average rate of operation the average is weighted by the time spent at a particular rate delays at instead of remaining at with low average queuing delay we thus use the additional condition rˆf ri to steer our algorithm toward the desired steady state and ensure that switching to a lower rate does not immediately lead to larger queues in addition to the above conditions we further dis courage oscillations by enforcing a minimum time kδ between consecutive switches intuitively this is because rate switching should not occur on timescales smaller than the transition time δ in our experiments we found k to be a reasonable value for this parameter average utilization average delay average utilization delay ile note that unlike the link optra algorithm the above decision process does not guarantee that the delay constraints will be met since it is based on estimated rather than true arrival rates similarly practra cannot guarantee that the rates used by the links match those used by the link optimal algorithm in section after discussing how power scales with rate we use simulation to compare our approximate algorithm to the optimal under realistic network conditions we leave it to future work to analytically bound the inaccuracy due to our approximations finally we observe that the above rate adaptation is simple in that it requires no coordination across different nodes and is amenable to implementation in high speed equipment this is because the above decision making need not be performed on a per packet basis evaluation we evaluate the savings vs performance tradeoff achieved by our practical rate adaptation algorithm and the impact of equipment and network parameters on the same we first evaluate the percentage reduction in av erage link rate achieved by practra for the abilene network for comparison we consider the rate reduction due to link optra and the upper bound on rate reduc tion as determined by the average link utilization in this case since we found the reduction from link optra was virtually indistinguishable from the utilization bound we only show the latter here for clarity we re figure average and percentile delay achieved by practra for various available rates port on the energy savings due to link optra in the following section to measure performance we measure the end to end packet delay and loss in a network using practra to that in a network with no rate adaptation impact of hardware characteristics the main con stants affecting the performance of practra are the granularity and distribution of available rates and δ the time to transition between successive rates we investigate the reduction in rate for three different dis tributions of rates rn i rates uniformly dis tributed between to ii rates uniformly distributed between to and iii expo nentially distributed rates we consider the latter case since physical layer technologies for these rates already exist making these likely candidates for early rate adaptation technologies figure plots the average rate reduction under increas ing utilizations with a per link delay constraint d δ and a transition time δ we see that for uniformly distributed rates practra operates links at a rate that approaches the average link utilization with uniformly distributed rates this reduction drops but not significantly however for exponentially distributed rates the algorithm performs poorly indicating that support for uniformly distributed rates is essential uniform transition time ms utiliz utiliz transition time ms exponential utilization bound link utilization rate reduction time max delay percentile figure rate reduction per link figure impact of switch time δ for practra figure plots the corresponding average and percentile of the end to end delay we see that for all the scenarios the increase in average delay due to practra is very small in the worst case and less than for the scenario using uniform rates the increase in maximum delay is also reasonable at most with practra relative to with no adaptation perhaps surprisingly the lowest additional delay occurs in the case of uniformly distributed rates we found this occurs because there are fewer rate transitions with corresponding transition delays in this case fi nally we found that practra introduced no additional packet loss for the range of utilizations considered next we look at the impact of transition times δ figures a and b plot the average rate reduction and percentile of delay under increasing δ for different network utilizations for each test we set the delay constraint as d δ and we assume uniform rates as would be expected we see that larger δ lead to reduced savings and higher delay on the whole we see that in this scenario both savings and performance remain attractive for transition times as high as in summary these results suggest that rate adaptation as implemented by practra has the potential to offers significant energy savings with little impact on packet loss or delay in all our tests we found practra to have minimal effects on the average delay and loss and hence from here on we measure the performance impact only in terms of the percentile in packet delay impact of network topology we now evaluate practra applied to the intel enterprise network fig ure plots the rate reduction across links each point in the scatter plot corresponds to a single link and we look at rate reduction for two rate distribution policies uni formly distributed rates and exponentially distributed rates since these are per link results we see significant variations in rate reduction for the same utilization due to specifics of traffic across various links we also notice that the dominant trend in reduction remains similar to that seen in the abilene network figure overall energy savings in the previous sections we evaluated power management solutions based on their ability to increase sleep times section or operate at reduced rates section in this section we translate these to overall energy savings and hence compare the relative merits of rate adaptation vs sleeping for this we develop an analytical model of power consumption under different operating modes our model derives from measurements of existing networking equipment at the same time we construct the model to be sufficiently general that we may study the potential impact of future more energy efficient hardware power model recall from section that the total energy consump tion of a network element operating in the absence of any power saving modes can be approximated as e pata piti we start by considering the power consumption when actively processing packets pa typically a portion of this power draw is static in the sense that it does not depend on the operating frequency e g refresh power in memory blocks leakage currents and so forth while the dominant portion of power draw does scale with operating frequency correspondingly we set pa r c f r intuitively c can be viewed as that portion of power draw that cannot be eliminated through rate adaptation while f r reflects the rate dependent portion of energy consumption to reflect the relative proportions of c and f r we set c to be relatively small between and of the maximum active power pa rn to study the effect of just frequency scaling alone we set f r o r and set f r o to evaluate dynamic voltage scal ing dvs in evaluating dvs we need to consider an additional constraint namely that in practice there is a minimum rate threshold below which scaling the link rate offers no further reduction in voltage we thus define a maximum scaling factor λ and limit our choice of avail able operating rates to lie between rn λ rn for scenar ios that assume voltage scaling while current transistor technology allows scaling up to factors as high as current processors typically use λ and hence we investigate both values as potential rate scaling limits empirical measurements further reveal that the idle mode power draw pi varies with operating frequency in a manner similar to the active mode power draw but with lower absolute value correspondingly we model the idle mode power draw as pi r c βf r intuitively the parameter β represents the relative magnitudes of routine work incurred even in the absence of packets to the work incurred when actively processing packets while measurements from existing equipment suggest values of β as high as for network interface cards and router linecards we would like to cap ture the potential for future energy efficient equipment and hence consider a wide range of β between energy savings from rate adaptation with the above definitions of pa and pi we can now evaluate the overall energy savings due to rate adaptation the total energy consumption is now given by pa rk ta rk pi rk ti rk rk our evaluation in section yields the values of ta rk and ti rk for different rk and test scenarios while eqns and allow us to model pa rk and pi rk for different c β and f r we first evaluate the energy savings using rate adaptation under frequency scaling f r o r and dvs f r o for these tests we set c and β to middle of the range values of and respectively we examine the effect of varying c and β in the next section figure a plots the energy savings for our practical practra and optimal link optra rate adaptation algorithms assuming only frequency scaling we see that in this case the relative energy savings for the different algorithms as well as the impact of the different rate distributions is similar to our previous results fig that measured savings in terms of the average reduction in link rates overall we see that significant savings are possible even in the case of frequency scaling alone figure b repeats the above test assuming voltage scaling for two different values of λ the maximum rate scaling factor allowed by dvs in this case we see that the use of dvs significantly changes the savings curve the more aggressive voltage scaling allows for larger savings that can be maintained over a wide range of utilizations moreover we see that once again the savings from our practical algorithm practra approach those from the optimal algorithm finally as expected increasing the range of supported rates λ results in additional energy savings energy savings from sleeping to model the energy savings with sleeping we need to pin down the relative magnitudes of the sleep mode power draw ps relative to that when idle pi we do so by introducing a parameter γ and set ps γpi rn where γ while the value of γ will depend on the hardware characteristics of the network element in question empirical data suggest that sleep mode power is typically a very small fraction of the idle mode power consumption for network interfaces for rfm radios for pc cards and less than for dram memory in our evaluation we consider values of γ between and with this the energy consumption of an element that spends time ts in sleep is given by e pa rn ta pi rn ti ts psts our evaluation from section estimated ts for different scenarios figure c plots the corresponding overall energy savings for different values of γ for our practb b algorithm we assume a transition time δ and a buffering interval b again our results confirm that sleeping offers good overall energy savings and that as expected energy savings are directly proportional to γ comparison sleep vs rate adaptation we now compare the savings from sleeping vs rate adaptation by varying the two defining axes of our power model c the percentage of power that does not scale with frequency and β that determines the relative magnitudes of idle to active power draws we consider two end of the range values for each c and c and β and β combining the two gives us four test cases that span the spectrum of hardware power profiles c and β captures the case where the static portion of power consumption that cannot be rate scaled away is low and idle mode power is significantly lower than active mode power c and β the static portion of power consumption is low and idle mode power is almost comparable to active mode power c and β the static portion of power consumption is high idle mode power is significantly lower than that in active mode c and β the static portion of power consumption is high idle mode power is almost comparable to active mode power average utilization practra freq scaling link_optra practra practra average utilization practra dvs average utilization practb b figure total energy saving with sleeping and rate adaptation we evaluate energy savings for each of the above scenarios for the case where the hardware supports dvs and when the hardware only supports frequency scaling with dvs f r o figures plots the overall energy savings for practra and practb b for the different test scenarios these tests assume uniformly distributed rates and a sleep power ps rn in each case for both sleep and rate adaptation we consider hardware parameters that reflect the best and worst case savings for the algorithm in question for utilization utilization practra these parameters are λ the range for voltage scaling and δ the transition time for the best case results these are λ and δ for the worst case λ δ the parameter for practb b is the transition time δ which we set as δ best case and δ worst case the conclusion we draw from figure is that in each scenario there is a boundary utilization below which sleeping offers greater savings and above which rate adaptation is preferable comparing across graphs we see that the boundary utilization depends primarily on the values of c and β and only secondarily on the a c β utilization c c β b c β rate adaptation sleeping utilization d c β transition time and other hardware parameters of the algorithm for example the boundary utilization for c and β varies between approximately while at c β this boundary utilization lies between and we also evaluated savings under different traffic characteristics cbr pareto and found that the burstiness of traffic has a more secondary effect on the boundary utilization for further insight on what determines the boundary utilization we consider the scenario of a single idealized link the sleep mode energy consumption of such an idealized link can be viewed as esleep pa rmax µt ps µ t similarly the idealized link with rate adaptation is one that runs with an average rate of µrmax for an energy consumption of erate pa µrmax t figure comparison of energy savings between sleep and rate adaptation support for dynamic voltage scaling figure represents the boundary utilization for this idealized link as a function of c in this idealized scenario the dominant parameter is c because the link is never idle and therefore β has only a small indirect effect on ps the gray zone in the figure represents the spread in boundary utilization obtained by varying β between and with frequency scaling alone f r o r fig ures plots the overall energy savings for practra and practb b for the different test scenarios in the more pessimistic scenario where voltage scaling is not supported due to lack of space we only plot the com parison for the first two test scenarios where c at c the savings show a similar scaling trend but with significantly poorer performance for rate adaptation utilization figure sleeping vs rate adaptation and hence add little additional information utilization a c β utilization b c β the primary observation is that the savings from rate adaptation are significantly lower than in the previous case with dvs and in this case sleeping outperforms rate adaptation more frequently we also see that unlike the dvs case network utilization impacts energy savings in a similar manner for both sleeping and rate adaptation i e the overall slope of the savings vs utilization curves is similar with both sleeping and rate adaptation while they were dramatically different with dvs see fig once again we obtain insight on this by studying the the highly simplified case of a single idealized link for this idealized scenario with f r o r we find that the boundary condition that determines whether to use sleep or rate adaptation is in fact independent of network utilization instead one can show that sleep is superior to rate adaptation if the following inequality holds figure energy savings of sleep vs rate adaptation β frequency scaling alone and os techniques to extend battery lifetimes in mobiles perhaps the first to draw attention to the problem of saving overall energy in the network was an early position paper by gupta et al they use data from the us department of commerce to detail the growth in network energy consumption and argue the case for energy saving network protocols including the possi bility of wake on arrival in wired routers in follow on work they evaluate the application of opportunistic sleeping in a campus lan environment other recent work looks at powering down redundant access points aps in enterprise wireless networks the authors propose that a central server collect ap γβ c γ β otherwise rate adaptation is superior connectivity and utilization information to determine which aps can be safely powered down this approach is less applicable to wired networks that exhibit much less redundancy in practice network utilization does play a role as our results clearly indicate because the various practical constraints due to delay bounds and transition times prevent our algorithms from fully exploiting all opportunities to sleep or change rates in summary we find that both sleeping and rate adaptation are useful with the tradeoff between them depending primarily on the power profile of hardware ca pabilities and network utilization results such as those presented here can guide operators in deciding how to best run their networks for example an operator might choose to run the network with rate adaptation during the day and sleeping at night based on where the boundary utilization intersects diurnal behavior or identify com ponents of the network with consistently low or high utilization to be run with sleeping or rate adaptation related work there is a large body of work on power management in contexts complementary to ours this includes power provisioning and load balancing in data centers sleeping has also been explored in the context of to save client power e g see the standard itself includes two schemes power save poll and automatic power save delivery by which access points may buffer packets so that clients may sleep for short intervals in some sense our proposal for bunching traffic to improve sleep opportunities can be viewed as extending this idea deep into the network finally the ieee energy efficient ethernet task force has recently started to explore both sleeping and rate adaptation for energy savings some initial studies consider individual links and are based on synthetic traffic and infinite buffers in the domain of sensor networks there have been numerous efforts to design energy efficient protocols approaches investigated include putting nodes to sleep using tdma like techniques to coordinate transmis sion and idle times e g fps and distributed algorithms for sleeping e g s mac this context differs from ours in many ways conclusion we have argued that power management states that slow down links and put components to sleep stand to save much of the present energy expenditure of networks at a high level this is apparent from the facts that while network energy consumption is growing networks continue to operate at low average utilizations we present the design and evaluation of simple power man agement algorithms that exploit these states for energy conservation and show that with the right hardware support there is the potential for saving much energy with a small and bounded impact on performance e g a few milliseconds of delay we hope these preliminary results will encourage the development of hardware support for power saving as well as algorithms that use them more effectively to realize greater savings peak shaving through resource buffering amotz bar noy matthew p johnson ou liu the graduate center of the city university of new york abstract we introduce and solve a new problem inspired by energy pricing schemes in which a client is billed for peak usage at each timeslot the system meets an energy demand through a combination of a new request an unreliable amount of free source energy e g solar or wind power and previously received energy the added piece of infrastructure is the battery which can store sur plus energy for future use more generally the demands could represent required amounts of energy water or any other tenable resource which can be obtained in advance and held until needed in a feasible solution each demand must be supplied on time through a combination of newly requested energy energy with drawn from the battery and free source the goal is to minimize the maximum request in the online version of this problem the algorithm must determine each request without knowledge of future demands or free source availability with the goal of maximizing the amount by which the peak is reduced we give efficient optimal algorithms for the offline problem with and without a bounded battery we also show how to find the optimal offline battery size given the requirement that the final battery level equals the initial battery level finally we give effi cient hn competitive algorithms assuming the peak effective demand is revealed in advance and provide matching lower bounds introduction there is increasing interest in saving fuel costs by use of renewable energy sources such as wind and solar power although such sources are highly desirable and the power they provide is in a sense free the typical disadvantage is unreliability availability depends e g on weather conditions it is not dispatchable on demand many companies seek to build efficient systems to gather such energy when available and store it perhaps in modified form for future use on the other hand power companies charge some high consumption clients not just for the total amount of power consumed but also for how quickly they consume it within the billing period typically a month the client is charged for the amount of energy used usage charge in kwh and for the maximum amount requested over time peak charge in kw if demands are given as a sequence dn then the total bill is of the form i di maxi di for some constants i e a weighted sum of the total usage and the maximum usage in practice the dis crete timeslots may be minute averages this means that a client who powers a this work was supported by grants from the national science foundation and the new york state office of science technology and academic research piece of machinery for one hour and then uses no more energy for the rest of the month would be charged more than a client who uses a total of spread evenly over the course of the month since the per unit cost for peak charges may be on the order of times the per unit cost for total usage this difference can be significant this suggests a second use for the battery to store purchased energy for future use indeed at least one start up company is currently marketing such a battery based system intended to reduce peak energy charges in such a system a battery is placed between the power company and a high consumption client site in order to smooth power requests and shave the peak the client site will charge to the battery when demand is low and discharge when demand is high spikes in the demand curve can thus be rendered consistent with a relatively flat level of supplied power the result is a lower cost for the client and a more manageable request curve for the provider we may generalize this problem of minimaxing the request to any resource which is tenable in the sense that it may be obtained early and stored until needed for ex ample companies frequently face shortages of popular products plentiful supply of xboxes would be possible only if microsoft made millions of consoles in advance and stored them without releasing them or if it built vast production lines that only ran for a few weeks both economically unwise strategies a recent news story asserted a producer could smooth the product production curve by increasing production and warehousing supply until future sales but when should the producer charge and dis charge in some domains there may also be an unpredictable level of volunteer help a third application is the scheduling of jobs composed of generic work units that may be done in advance although the problem is very general we will use the language of energy and batteries for concreteness in the online version of our problem the essential choice faced at each timeslot is whether and by how much to invest in the future or to cash in a prior investment the investment in our setting is a request for more energy than is needed at the time if the algorithm only asks for the minimum required then it is vulnerable to spikes in demand if it asks for much more energy than it needs then the greater request could itself introduce a new higher peak the strictness of the problem lies in the fact that the cost is not cumulative we want every request to be low background experimental work applying variations of these online algorithms to set tings lacking provable guarantees was recently presented the present paper focuses on settings that allow guaranteed competitiveness there is a wide literature on commodity production storage warehousing and supply chain management see e g more specifically there are a num ber of inventory problems based on the economic lot sizing model in which de mand levels for a product vary over a discrete finite time horizon and are known in advance a feasible solution in these problems must obtain sufficient supply through production sometimes construed as ordering or through other methods in order to meet each of the demands on time while observing certain constraints the nature of solution quality varies by formulation one such inventory problem is single item lot sizing in which sufficient supplies must be ordered to satisfy each demand while minimizing the total cost of ordering charges and holding charges the ordering charge consists of a fixed charge per or der plus a charge linear in order size the holding charge for inventory is per unit and per timeslot there is a tradeoff between these incentives since fixed ordering charges encourage large orders while holding charges discourage them wagner whitin showed in that this problem can be solved in polynomial time under the as sumption of non speculative costs in which case orders should always be placed as late as possible the problem can be solved in linear time such speculative behavior however is the very motivation of our problem there are many lot sizing variations including constant capacity models that limit the amount ordered per timeslot see and references therein our offline problem differs in that our objective is minimizing this constant capacity for orders subject to a bound on inventory size and we have no inventory charge another related inventory problem is capacity and subcontracting with inventory csi which incorporates trade offs between production costs subcontracting costs holding costs and the cost for maximum per unit timeslot production capacity the goal in that problem is to choose a production capacity and a feasible production sub contracting schedule that together minimize total cost whereas in our problem choosing a production capacity subject to storage constraints is the essential task in the minimax work scheduling problem the goal is to minimize the maxi mum amount of work done in any timeslot over a finite time horizon our online prob lem is related to a previously studied special case in which jobs with deadlines are assigned online in that problem all work must be done by deadline but cannot be be gun until assigned subject to these restrictions the goal is to minimize the maximum work done in any timeslot while the optimization goal is the same our online prob lem differs in two respects first each job for us is due immediately when assigned second we are allowed to do work request and store energy in advance one online algorithm for the jobs by deadlines problem is the α policy at each timeslot the amount of work done is α times the maximum per unit timeslot amount of work that opt would have done when running on the partial input received so far one of our online algorithms adopts a similar strategy contributions we introduce a novel scheduling problem and solve several versions optimally with efficient combinatorial algorithms we solve the offline problem for two kinds of batteries unbounded battery in o n time and bounded in o separately we show how to find the optimal offline battery size for the setting in which the final battery level must equal the initial battery level this is the smallest battery size that achieves the optimal peak the online problem we study is very strict a meta strategy in many online problems is to balance expensive periods with cheap ones so that the overall cost stays low the difficulty in our problem lies in its non cumulative nature we optimize for the max not for the average we show that several versions of the online problem have no algorithm with non trivial competitive ratio i e better than n or ω n given advanced knowledge of the peak demand d however we give hn competitive algorithms for batteries bounded and unbounded our fastest algorithm has o per slot running time hn is the optimal competitive ratio for both battery settings examples although there is no constant ratio competitive algorithm for unbounded n our intended application in fact presumes a fixed time horizon if the billing period is one month and peak charges are computed as minute averages then for this setting hn is approximately if we assume that the battery can fully recharge at night so that each day can be treated as a separate time period then for a hour daytime time horizon hn is approximately model and preliminaries definition at each timeslot i di is the demand ri is the request bi is the battery charge level at the start of the timeslot and fi is the amount of free source available by dˆi we indicate the effective demand di fi we sometimes refer to the sequence over time of one of these value types as a curve e g the demand curve d is the maximum effective demand maxi dˆi and r is the maximum request maxi ri the problem instance comprises the demands the free source curve battery size b initial charge and required final charge bn in the offline case the problem solution consists of the request curve definition let overflow be the situation in which ri fi di b bi i e there is not enough room in the battery for the amount we want to charge let underflow be the situation in which di ri fi bi i e there is not enough energy in the battery for the amount we want to discharge call an algorithm feasible if underflow never occurs the goal of the problem is to minimize r for competitiveness measures this is construed as maximizing d r while maintaining feasibility in the absence of overflow underflow the battery level at timeslot i is simply bi bi ri fi di it is forbidden for bi to ever fall below that is the request ri the free source fi and the battery level bi must sum to at least the demand di at each timeslot i notice that effective demand can be negative which means that the battery may be charged capacity allowing even if the request is we assume d however is strictly positive otherwise the problem instance is essentially trivial we use the following to simplify the problem statement observation if effective demands may be negative then free source energy need not be explicitly considered as such we set aside the notion of free source and for the remainder of the paper simplifying notation allow demand di to be negative in the energy application battery capacity is measured in kwh while instantaneous request is measured in kw by discretizing we assume wlog that battery level demand and request values are expressed in common units peak charges are based linearly on the max request which is what we optimize for the battery can have a maximum capac ity b or be unbounded the problem may be online offline or in between we consider the setting in which the peak demand d is revealed in advance perhaps predicted from historical information threshold algorithms for a particular snapshot di ri bi demand di must be sup plied through a combination of the request ri and a change in battery bi bi this means that there are only three possible modes for each timeslot request exactly the demand free request more than this and charge the difference or request less and dis charge the difference we refer to our algorithms as threshold algorithms let tn be a sequence of values then the following algorithm uses these as request thresholds intuitively the algorithm amounts to the rule at each timeslot i request an amount as near to ti as the battery constraints will allow our offline algorithms are constant threshold algorithms with a fixed t our online algorithms compute ti dynamically for each timeslot i a constant threshold algorithm is specifiable by a single number in the online set ting predicting the exact optimal threshold from historical data suffices to solve the online algorithm optimally a small overestimate of the threshold will merely raise the peak cost correspondingly higher unfortunately however examples can be found in which even a small underestimate eventually depletes the battery before peak demand and thus produce no cost savings at all the offline problem can be solved approximately within additive error e through binary search for the minimum feasible constant threshold value t simply search the range d for the largest value t for which the threshold algorithm has no underflow in time o n log d if the optimal peak reduction is r t then the algorithm peak reduction will be at least r t e it is straightforward to give a linear programming formulation of the offline problem it can also be solved by generalized parametric max flow our interest here however is in efficient combinatorial optimal algorithms indeed our combinatorial offline algorithms are significantly faster than these general techniques and lead naturally to our competitive online algorithms online algorithms based on such general techniques would be intractable for fine grain timeslots offline problem we now find optimal algorithms for both battery settings for unbounded we assume the battery starts empty for bounded we assume the battery starts with amount b for both the final battery level is unspecified we show below that these assumptions are made wlog the two offline threshold functions shown in table use the following definition definition let µ j j dt be the mean demand of the prefix region j and let µˆ k j kµ j be the maximum mean among of the prefix regions up to k let ρ i j t i dt j i be the density of the region i j and ρˆ k i j kρ i j be the maximum density among all subregions of k alg battery threshold ti run time unbounded bounded µˆ n o n ρˆ n o table threshold functions used for offline algorithm settings bounded capacity changes the character of the offline problem it suffices however to find the peak request made by the optimal algorithm ropt clearly ropt d b since the ideal case is that a width one peak is reduced by size b of course the peak region might be wider theorem algorithm a threshold ti µˆ n for unbounded battery and algo rithm b threshold ti ρˆ n for bounded battery are optimal feasible and run in times o n and o respectively proof first let the battery be unbounded for any region j the best we can hope for is that requests for all demands dj can be spread evenly over the first j timeslots therefore the optimal threshold cannot be lower than the maximum µ j which is algorithm a threshold for feasibility it suffices to show that after each time j the battery level is nonnegative but by time j the total input to the system will be j µˆ n j µ j j dj which is the total output to the system up to that point for complexity just note that µ j j µ j dj so the sequence of µ values and their max can be computed in linear time now let the battery be bounded over the course of any region i j the best that can be hoped for is that the peak request will be reduced to b j i less than the average di in the region i e ρ i j so no threshold lower than algorithm b is possible for feasibility it suffices to show that the battery level will be nonnegative after each time j suppose j is the first time underflow occurs let i be the last timeslot prior to j with a full battery then there is no underflow or overflow in i j and so for each t i j the discharge at t is bt bt dt t possibly negative meaning a charge and the so the total net discharge over i j is j dt j i t total net discharge greater than b implies t j t i j i dt which contradicts the definition of t the densest region can be found in o with n separate linear time passes each of which finds the densest region beginning in some position i since ρ i j can be computed in constant time from ρ i j battery level boundary conditions we assumed above that the battery starts empty for the unbounded offline algorithm and starts full for the bounded offline algorithm with the final battery level left inde terminate for both settings a more general offline problem may require that and bn i e the battery begins and ends at some charge levels specified by parameters and we argue here that these requirements are not significant algo rithmically since by pre and postprocessing we can reduce to the default cases for both the unbounded and bounded versions first consider the unbounded setting in which the initial battery level is in order to enforce that and bn run the usual optimal algorithm on the sequence dn dn recall that negative demands are allowed then bn will be at least larger than dn to correct for any surplus manually delete a total of bn from the final requests for the bounded setting the default case is b and bn indeterminate to support b and bn modify the demand sequence as above except with b as the first demand and then do similar postprocessing as in the unbounded case to deal with any final surplus optimal battery size a large component of the fixed initial cost of the system will depend on battery ca pacity a related problem therefore is finding the optimal battery size b for a given demand curve di given that the battery starts and ends at the same level β which can be seen as an amount borrowed and repaid the optimal peak request possible will be n di µ n and the goal is to find the smallest b and β that achieve peak µ n a completely flat request curve is possible given a sufficiently large battery this can be done in o n since we will have bn ri µ for all i and di ri there must be no overflow let dii di µ i e the amount by which di is above average positive means discharge negative means charge then the minimum possible β is the maximum prefix sum of the di curve which will be at least it could happen that the battery level will at some point rise above however consider the example d for which µ di and β the needed capacity b can be computed as β plus the maximum prefix sum of the negation of the di curve which will also be at least in the example we have b β although b is computed using β we emphasize that the computed β is the mini mum possible regardless of b and the computed b is the minimum possible regardless of β online problem we consider two natural choices of objective function for the online problem one op tion is to compare the peak requests so that if alg is the peak request of the on line algorithm alg and op t is that of the optimal offline algorithm opt then a c competitive algorithm for c must satisfy alg c for every demand sequence although this may be the most natural candidate we argue that for many settings it is uninteresting if the peak demand is a factor k larger than the battery capacity for exam ple then the trivial online algorithm that does not use the battery would be k k competitive if we drop the assumption of a small battery then no reasonable competi tive factor is possible in general even if d is revealed in advance proposition with peak demand d revealed in advance and finite time horizon n no online algorithm for the problem of minimizing peak request can have competitive ratio better than n if the battery begins with some strictly positive charge or better than ω n if the battery begins empty proof for part suppose that the battery begins with initial charge d consider the demand curve d where the last demand is either d or alg must dis charge d at time since op t when dn thus alg battery is empty at time if alg requests nothing between times and n and dn d then we have op t d n and alg d if alg requests some α during any of those timeslots and dn then we have op t and alg α this yields a lower bound of n for part suppose the battery begins empty which is a disadvantage for both alg and opt consider the demand curve d in which case op t d n if an algorithm is c competitive for some c then in each of the first n timeslots of this demand curve alg can charge at most amount cd n now suppose that the only nonzero demand of value d arrives possibly earlier at some timeslot k n following k demands of zero during which alg can charge at most k cd n in this case we have op t d k and alg d k cd n which yields the competitive ratio c d k cd n k c n k n kc n solving for c and then choosing k n we have k n c ω n n k n thus establishing the lower bound n instead we compare the peak shaving amount or savings i e d r for a given input let opt be the peak shaving of the optimal algorithm and let alg be the peak shaving of the online algorithm then an online algorithm is c competitive for c if c o p t for every problem instance for this setting we obtain the online algorithms described below the online algorithms see def are shown in table definition let t opt be the optimal threshold used by the appropriate optimal algo rithm when run on the first i timeslots at time i during the online computation let si be the index of the most recent time prior to i with bsi b or in the unbounded setting if the peak is not known a lower bound of n can be obtained also for the latter case alg battery threshold ti per slot time both d t opt hn o n both d d ρ si i hn si o table threshold functions used for online algorithms lower bounds for d r since the competitiveness of the online algorithms holds for arbitrary initial battery level in obtaining lower bounds on competitiveness we assume particular initial battery levels proposition with peak demand d unknown and finite time horizon n there is no online algorithm with any constant competitive ratio for unbounded battery even with n or with competitive ratio better than n for bounded battery proof for part assume and suppose then if alg requests and we have d then op t d and alg if alg requests a for some a and we have a then op t a and alg for part let b and assume alg is c competitive consider the demand curve b then opt clearly discharges b at time decreasing the peak by b for alg to be c competitive it must discharge at least b in the first slot now consider curve b at time opt discharges b decreasing the peak by b so at time alg must discharge at least b at time alg already had to discharge b similarly at time i for b ib nb alg must discharge b total c n b nb b c the trivial algorithm that simply discharges amount b n at each of the n timeslots and never charges is n competitive since op t b and so matches the lower bound for the bounded case proposition with peak demand d known in advance and finite time horizon n no online algorithm can have competitive ratio better than hn if the battery begins nonempty or competitive ratio better than hn if the battery begins empty regardless of whether the battery is bounded or not proof first assume the battery has initial charge b the capacity is either at least b or unbounded suppose alg is c competitive consider the curve d with d b then opt clearly discharges b at time decreasing the peak by b for alg to be c competitive it must discharge at least b now consider curve d d at times and opt discharges b decreasing the peak by b at time alg will have to discharge at least b b similarly at time i on d d d d alg must discharge b c n b b since we discharge at each timeslot and never charge we must have b hn b and so it follows that c hn now let the battery start empty assume the battery capacity is at least d or is unbounded repeat the argument as above except now with a zero demand inserted at the start of the demand curves which gives both alg and opt an opportunity to charge then for each time i n alg must discharge at least d since opt may discharge and so save d in which case it would have initially charged d i alg is then required to discharge hn d during the last n timeslots obviously it could not have charged more than d during the first timeslot in fact it must charge less than this on the sequence d opt charges d at time and discharges it at time saving d alg must discharge d at time in order to be c competitive on this sequence and so reduce the peak d by d therefore at time alg cannot charge more than d d therefore we must have d d hn d c which implies that c hn bounded battery our first online algorithm bases its threshold at time i on a computation of the optimal offline threshold t opt for the demands di the second bases its threshold at time i on ρ si i see defs and assuming the algorithms are feasible i e no battery underflow occurs it is not difficult to show that they are competitive theorem algorithms a and b are hn competitive if they are feasible and have per timeslot running times of o n and o respectively proof first observe that ρˆ i ρ si i implies d ρˆ i d ρ si i implies t a t b for all i therefore it suffices to prove competitiveness for algorithm a since t opt i is the lowest possible threshold up to time i d t opt i is the highest possible peak shaving as of time i since the algorithm always saves a hn fraction of this it is hn competitive by construction since µ i can be found in constant time from µ i algorithm b is constant time per slot similarly algorithm a is recalling the proof of theorem linear per slot we now show that indeed both algorithms are feasible using the following lemma which allows us to limit our attention to a certain family of demand sequences lemma if there is a demand sequence dn in which underflow occurs for algorithm a or b then there is also a demand sequence for the same algorithm in which underflow continues to the end i e bn and no overflow ever occurs i e one in which the battery level decreases monotonically from full to empty proof the battery is initialized to full b over the course of running one of the algorithms on a particular problem instance the battery level will fall and rise and may return to full charge multiple times suppose underflow were to occur at some time t i e bt and let be the most recent time before t when the battery was full we now construct a demand sequence with the two desired properties for both algorithms first if then also considering region when defining the thresh old ti for algorithm a or b can only raise the threshold over what it would be if only region t were considered therefore shifting the region leftward from t to ti t will only lower the thresholds used which therefore preserves the underflow second since any underflow that occurs in region ti can be extended to the end of sequence by setting each demand after time ti to d we can assume wlog that ti n theorem algorithms a and b are feasible proof for a proof by contradiction we can restrict ourselves by lemma to regions that begin with a full battery underflow at the end and have no overflow in the middle for such a region the change in battery level is well behaved bi bi di ti which allows us to sum the net discharge and prove it is bounded by b we now show that it is impossible for the battery to fall below at time n by upperbounding the net discharge over this region let bi bi bi di ti be the amount of energy the battery discharges at step i bi will be negative when the battery charges we will show that bi b let ba and bb refer to the change in battery levels for the corresponding algorithms because as we observed above t a t b we have i i ba di t a bb di t b therefore it suffices to prove the feasibility result for algorithm b and so we drop the superscripts expanding the definition of that algorithm threshold we have bi di ti di d h d ρ i di d hn i d i k dk b by summing eq for each i we obtain bi di d i dk b i i i hn d i d i b i i di d k hn d i k d i h i therefore it suffices to show that i di d i k hn dk i which is equivalent to d nd nd d i k i n i k n iff hndi n dk i nd hn i with the following derivation i k n i n n n n n n n k n n dk dk dk d k dk h d h d we can rewrite eq replacing the parenthesized expression as n hi n hn in fact this holds with equality see eq unbounded battery and boundary conditions both online algorithms modified to call appropriate subroutines also work for the un bounded battery setting the algorithms are feasible in this setting since ρ i t opt i still holds where t opt i is now the optimal threshold for the unbounded battery setting recall that offline algorithm a can greedily run in linear total time the algorithm is hn competitive by construction as before corollary algorithms a and b are feasible in the unbounded battery setting proof the proof is similar to that of theorem except that which may be is plugged in for all occurrences of b resulting in a modified ρ and overflow is no longer a concern we also note that the correctness and competitiveness of both algorithms with mi nor preprocessing holds for the setting of arbitrary initial battery levels in the case of algorithm a each computation of t opt is computed for the chosen as described in section and the online algorithm again provides a fraction hn of this savings although in the bounded setting increasing for the t opt computation by modify ing may raise the peak demand in the offline algorithm input the d value for the online algorithm is not changed indeed examining the proof of theorem we note that the upperbound on the di is only used for i see eq conclusion in this paper we formulated a novel peak shaving problem and gave efficient optimal offline a lgorithms a nd o ptimally c ompetitive o nline a lgorithms i n w ork i n progress we are testing our online algorithms on actual client data from gaia see for preliminary results there are several interesting extensions to the theoretical problem that we plan to address such as adapting the algorithms to inefficient batteries that lose a percentage of charge instantly or over time or batteries with a charging speed limit we could also optimize for a moving average of demands rather than a single peak finally online algorithms could also be granted additional predictions about the future optimal power allocation in server farms abstract anshul gandhi carnegie mellon university pittsburgh pa usa rajarshi das ibm research hawthorne ny usa mor harchol balter carnegie mellon university pittsburgh pa usa charles lefurgy ibm research austin tx usa improve server farm performance by a factor of typically server farms today consume more than of the total electricity in the u s at a cost of nearly billion given the rising cost of energy many industries are now seeking solutions for how to best make use of their available power an important question which arises in this context is how to distribute available power among servers in a server farm so as to get maximum performance by giving more power to a server one can get higher server frequency speed hence it is commonly believed that for a given power budget performance can be maximized by operating servers at their highest power levels however it is also conceivable that one might prefer to run servers at their lowest power levels which allows more servers to be turned on for a given power budget to fully understand the effect of power allocation on performance in a server farm with a fixed power budget we introduce a queueing theo retic model which allows us to predict the optimal power allocation in a variety of scenarios results are verified via extensive experiments on an ibm bladecenter we find that the optimal power allocation varies for differ ent scenarios in particular it is not always optimal to run servers at their maximum power levels there are scenar ios where it might be optimal to run servers at their lowest power levels or at some intermediate power levels our anal ysis shows that the optimal power allocation is non obvious and depends on many factors such as the power to frequency relationship in the processors the arrival rate of jobs the maximum server frequency the lowest attainable server fre quency and the server farm configuration furthermore our theoretical model allows us to explore more general settings than we can implement including arbitrarily large server farms and different power to frequency curves importantly we show that the optimal power allocation can significantly research supported by nsf sma pdos grant ccr and a ibm faculty award permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee sigmetrics performance june seattle wa usa copyright acm and as much as a factor of in some cases categories and subject descriptors c performance of systems modeling techniques general terms theory experimentation measurement performance introduction servers today consume ten times more power than they did ten years ago recent articles estimate that a high performance server requires more than of energy cost per year given the large number of servers in use today the worldwide expenditure on enterprise power and cooling of these servers is estimated to be in excess of billion power consumption is particularly pronounced in cpu intensive server farms composed of tens to thousands of servers all sharing workload and power supply we consider server farms where each incoming job can be routed to any server i e it has no affinity for a particular server server farms usually have a fixed peak power budget this is because large power consumers operating server farms are often billed by power suppliers in part based on their peak power requirements the peak power budget of a server farm also determines its cooling and power delivery infras tructure costs hence companies are interested in maxi mizing the performance at a server farm given a fixed peak power budget the power allocation problem we consider is how to dis tribute available power among servers in a server farm so as to minimize mean response time every server running a given workload has a minimum level of power consumption b needed to operate the processor at the lowest allowable frequency and a maximum level of power consumption c needed to operate the processor at the highest allowable fre quency by varying the power allocated to a server within the range of b to c watts one can proportionately vary the server frequency see fig hence one might expect that running servers at their highest power levels of c watts which we refer to as powmax is the optimal power allocation scheme to minimize response time since we are constrained by a power budget there are only a limited number of servers that we can operate at the highest power level the rest of the servers remain turned off thus powmax corresponds to having few fast servers in sharp contrast is powmin which we define as operating servers at their lowest power levels of b watts since we spend less power on each server powmin corresponds to having many slow servers of course there might be scenarios where we neither operate our servers at the highest power levels nor at the lowest power levels but we operate them at some intermediate power levels we refer to such power allocation schemes as powmed understanding power allocation in a server farm is intrin sically difficult for many reasons first there is no single allocation scheme which is optimal in all scenarios for ex ample it is commonly believed that powmax is the optimal power allocation scheme however as we show later powmin and powmed can sometimes outperform powmax by almost a factor of second it turns out that the optimal power allocation depends on a very long list of ex ternal factors such as the outside arrival rate whether an open or closed workload configuration is used the power to frequency relationship how power translates to server frequency inherent in the technology the minimum power consumption of a server b watts the maximum power that a server can use c watts and many other factors it is sim ply impossible to examine all these factors via experiments to fully understand the effect of power allocation on mean response time in a server farm with a fixed power budget we introduce a queueing theoretic model which allows us to predict the optimal power allocation in a variety of scenarios we then verify our results via extensive experiments on an ibm bladecenter prior work in power management has been motivated by the idea of managing power at the global data center level rather than at the more localized single server level while power management in server farms often deals with various issues such as reducing cooling costs minimizing idle power wastage and minimizing average power consumption we are more interested in the problem of allocating peak power among servers in a server farm to maximize perfor mance notable prior work dealing with peak power alloca tion in a server farm includes raghavendra et al femal et al and chase et al among others raghavendra et al present a power management solution that co ordinates different individual approaches to simultaneously minimize average power peak power and idle power wastage femal et al allocate peak power so as to maximize throughput in a data center while simultaneously attempt ing to satisfy certain operating constraints such as load balancing the available power among the servers chase et al present an auction based architecture for improving the energy efficiency of data centers while achieving some quality of service specifications we differ from the above work in that we specifically deal with minimizing mean re sponse time for a given peak power budget and understand ing all the factors that affect it our contributions as we have stated the optimal power allocation scheme depends on many factors perhaps the most important of these is the specific relationship between the power allocated to a server and its frequency speed henceforth referred to as the power to frequency relationship there are sev eral mechanisms within processors that control the power to frequency relationship these can be categorized into dfs dynamic frequency scaling dvfs dynamic volt age and frequency scaling and dvfs dfs section dis cusses these mechanisms in more detail the functional form of the power to frequency relationship for a server de pends on many factors such as the workload used maxi mum server power maximum server frequency and the volt age and frequency scaling mechanism used dfs dvfs or dvfs dfs unfortunately the functional form of the server level power to frequency relationship is only recently beginning to be studied see the papers and is still not well understood our first contribution is the inves tigation of how power allocation affects server frequency in a single server using dfs dvfs and dvfs dfs for various workloads in particular in section we derive a functional form for the power to frequency relationship based on our measured values see figs a and b our second contribution is the development of a queue ing theoretic model which predicts the mean response time for a server farm as a function of many factors including the power to frequency relationship arrival rate peak power budget etc the queueing model also allows us to determine the optimal power allocation for every possible configuration of the above factors see section our third contribution is the experimental implementa tion of our schemes powmax powmin and powmed on an ibm bladecenter and the measurement of their response time for various workloads and voltage and frequency scal ing mechanisms see sections and importantly our experiments show that using the optimal power allocation scheme can significantly reduce mean response time some times by as much as a factor of to be more concrete we show a subset of our results in fig which assumes a cpu bound workload in an open loop setting fig a depicts one possible scenario using dfs where powmax is optimal by contrast fig b depicts a scenario using dvfs where powmin is optimal for high arrival rates lastly fig c depicts a scenario using dvfs dfs where powmed is op timal for high arrival rates we experiment with different workloads while section presents experimental results for a cpu bound workload linpack section repeats all the experiments under the stream memory bound workload the webbench web work load and other workloads in all cases experimental results are in excellent agreement with our theoretical predictions section summarizes our work finally section discusses future applications of our model to more complex situations such as workloads with varying arrival rates servers with idle low power states and power management at the sub system level such as the storage subsystem experimental framework experimental setup our experimental setup consists of a server farm with up to fourteen ibm bladecenter blade servers featuring two ghz dual core intel woodcrest xeon processors and gb memory per blade all residing in a single chassis we installed and configured apache as an application server on each of the blade servers to process transactional requests to generate http requests for the apache web servers we employ an additional blade server on the same chassis as the workload generator to reduce the effects of network la tency the workload generator uses the web server perfor mance benchmarking tool httperf in the open server low arrival rate high arrival rate low arrival rate high arrival rate powmax is best b powmin is best c powmed is best at high arrival rates at high arrival rates figure subset of our results showing that no single power allocation scheme is optimal fig a depicts a scenario using dfs where powmax is optimal fig b depicts a scenario using dvfs where powmin is optimal at high arrival rates whereas powmax is optimal at low arrival rates fig c depicts a scenario using dvfs dfs where powmed is optimal at high arrival rates whereas powmax is optimal at low arrival rates farm configuration and wbox in the closed server farm configuration we modified and extended httperf and wbox to allow for multiple servers and to specify the routing prob ability among the servers we measure and allocate power to the servers using ibm amester software amester along with additional scripts collects all relevant data for our ex periments voltage and frequency scaling mechanisms processors today are commonly equipped with mecha nisms to reduce power consumption at the expense of re duced server frequency common examples of these mecha nisms are intel speedstep technology and amd cool n quiet technology the power to frequency relationship in such servers depends on the specific voltage and frequency scaling mechanism used most mechanisms fall under the following three categories dynamic frequency scaling dfs a k a clock throttling or t states is a technique to manage power by running the processor at a less than maximum clock fre quency under dfs the intel ghz woodcrest xeon processors we use allow for operating points which corre spond to effective frequencies of and of the maximum server fre quency dynamic voltage and frequency scaling dvfs a k a p states is a more efficient power savings mech anism that reduces server frequency by reducing the pro cessor voltage and frequency under dvfs our processors allow for operating points which correspond to effective frequencies of and of the maxi mum server frequency dvfs dfs attempts to leverage both dvfs and dfs by applying dfs on the lowest performance state available in dvfs under dvfs dfs our processors allow for operating points which correspond to effective frequencies of 88 and of the maximum server frequency power consumption within a single server when allocating power to a server there is a minimum level of power consumption b needed to operate the proces sor at the lowest allowable frequency and a maximum level of power consumption c needed to operate the processor at the highest allowable frequency of course the specific values of b and c depend on the application that the server is running formally we define the following notation baseline power b watts the minimum power consumed by a fully utilized server over the allowable range of proces sor frequency speed at baseline sb hertz the speed or frequency of a fully utilized server running at b watts maximum power c watts the maximum power con sumed by a fully utilized server over the allowable range of processor frequency speed at maximum power sc hertz the speed or fre quency of a fully utilized server running at c watts power to frequency an integral part of determining the optimal power allo cation is to understand the power to frequency relationship this relationship differs for dfs dvfs and dvfs dfs and also differs based on the workload in use unfortunately the functional form of the power to frequency relationship is not well studied in the literature the servers we use sup port all three voltage and frequency scaling mechanisms and therefore can be used to study the power to frequency rela tionships in this section we present our measurements de picted in figs a and b showing the functional relation ship between power allocated to a server and its frequency for the linpack workload we generalize the func tional form of the power to frequency relationship to other workloads in section see figs and throughout we assume a homogeneous server farm we use the tools developed in to limit the maxi mum power allocated to each server limiting the maximum power allocated to a server is usually referred to as capping the power allocated to a server we run linpack jobs back to back to ensure that the server is always occupied by the workload and that the server is running at the specified power cap value hence the power values we observe will be the peak power values for the specified workload recall from section that the voltage and frequency scaling mech anisms have certain discrete performance points in terms of frequency at which the server can operate at each of these performance points the server consumes a certain amount of power for a given workload by quickly dithering between available performance states we can ensure that the server never consumes more than the set power cap value in this way we also get the best performance from the server for the given power cap value note that when we say power we mean the system level power which includes the power consumed by the processor and all other components within the server fig a illustrates the power to frequency curves obtained for linpack using dfs and dvfs from the figure we see that the power to frequency relationship for both dfs and dvfs is almost linear it may seem surprising that the power to frequency relationship for dvfs looks like a lin ear plot this is opposite to what is widely suggested in the literature for the processor power to frequency relation ship which is cubic the reason why the server power to frequency relationship is linear can be explained by two interrelated factors first manufacturers usually settle on a limited number of allowed voltage levels or performance states which results in a less than ideal relationship be tween power and frequency in practice second dvfs is not applied on many components at the system level for example power consumption in memory remains propor tional to the number of references to memory per unit time which is only linearly related to the frequency of the pro cessor thus the power to frequency curve for both dfs and dvfs can be approximated as a linear function using the terminology introduced in section we approximate the server speed or frequency ghz as a function of the power allocated to it p watts as sb α p b where the coefficient α units of ghz per watt is the slope of the power to frequency curve specifically our experi ments show that α ghz w for dvfs and α ghz w for dfs also from our experiments we find that b and c for both dvfs and dfs how ever sb ghz for dvfs and sb ghz for dfs the maximum speed in both cases is sc ghz which is simply the maximum speed of the processor we use note that the specific values of these parameters change depend ing on the workload in use section discusses our measured parameter values for different workloads for dvfs dfs we expect the power to frequency rela tionship to be piecewise linear since it is a combination of dvfs and dfs experimentally we see from fig b that the power to frequency relationship is in fact piecewise lin ear ghz ghz and then ghz ghz though we could use a piecewise linear fit for dvfs dfs we choose to approximate it using a cubic curve for the fol lowing reasons using a cubic fit demonstrates how we can extend our results to non linear power to frequency relationships power to frequency relationship to be cubic especially for the processor by using a cubic model for dvfs dfs we wish to analyze the optimal power allocation policy for those settings approximating dvfs dfs using a cubic fit gives the fol lowing relationship between the speed of a server and the power allocated to it sb αl p b specifically our experiments show that αl ghz w also from our experiments we found that b c sb ghz and sc ghz for dvfs dfs theoretical results the optimal power allocation depends on a large num ber of factors including the power to frequency relationship just discussed in fig the arrival rate the minimum and maximum power consumption levels b and c respectively whether the server farm has an open loop configuration or a closed loop configuration etc in order to investigate the effects of all these factors on the optimal power allocation we develop a queueing the oretic model which predicts the mean response time as a function of all the above factors see section we then produce theorems that determine the optimal power allo cation for every possible configuration of the above factors including open loop configuration see theorems and in section and closed loop configuration see theorems and in section queueing model figure illustration of our k server farm model fig illustrates our queueing model for a server farm with k servers we assume that there is a fixed power bud get p which can be split a mong the k servers allocating pi power to server i where i pi p the correspond as previously mentioned several papers consider the ing server speeds are denoted by sk each server dfs and dvfs b dvfs dfs figure power to frequency curves for dfs dvfs and dvfs dfs for the cpu bound linpack workload fig a illustrates our measurements for dfs and dvfs in both these mechanisms we see that the server frequency is linearly related to the power allocated to the server fig b illustrates our measurements for dvfs dfs where the power to frequency curve is better approximated by a cubic relationship i receives a fraction qi of the total workload coming in to the server farm corresponding to any vector of power allo cation k there exists an optimal workload allo cation vector qk we derive the optimal workload allocation for each power allocation and use that vector qk both in theory and in the actual experiments the details of how we obtain the optimal qk are deferred to the appendix our model assumes that the jobs at a server are scheduled using the processor sharing ps scheduling discipline un der ps when there are n jobs at a server they each receive nth of the server capacity ps is identical to round robin with quantums as in linux when the quantum size ap proaches zero a job response time t is the time from when the job arrives until it has completed service includ ing waiting time we aim to minimize mean response time e t we will analyze our server farm model under both an open loop configuration see section and a closed loop con figuration see section an open loop configuration is one in which jobs arrive from outside the system and leave the system after they complete service we assume that the arrival process is poisson with average rate λ jobs sec sometimes it will be convenient to instead express λ in units of ghz this conversion is easily achievable since an average job has size e s gigacycles in the theorems pre sented in the paper λ is in the units of ghz however in the timal power allocation is non trivial computing e t for a given allocation is easy hence we omit showing the mean response time in each case and refer the reader to the ap pendix due to lack of space we defer all proofs to the appendix however we present the intuition behind the theorems in each case recall from section that each fully utilized server has a minimum power consumption of b watts and maximum power consumption of c watts to illustrate our results clearly we shall assume through out this section and the appendix that the power budget is such that powmax allows us to run n servers each at power c and powmin allows us to run m servers each at power b this is equivalent to saying p m b n c where m and n are less than or equal to k obviously m n theorems for open loop configurations theorem derives the optimal power allocation in an open loop configuration for a linear power to frequency relation ship as is the case for dfs and dvfs in such cases the server frequency varies with the power allocated to it as si sb α i b the theorem says that if the speed at baseline sb is sufficiently low then powmax is optimal by contrast if sb is high then powmin is optimal for high arrival rates and powmax is optimal for low arrival rates if i is the speed of server i when run at power pi then the to jobs sec likewise while it is common for us to express the speed of the server in ghz we sometimes switch to jobs sec in the appendix when convenient a closed loop configuration is one in which there are always a fixed number of users n also referred to as the multi programming level theorem given an open k server farm configuration with a linear power to frequency relationship given by eq and power budget the following power allocation mini mizes e t if sb α p c p who each submit one job to the server once a user job is b n p n n k c p if λ λ in all of the theorems that follow we find the optimal power allocation k for a k server farm which minimizes the mean response time e t given the fixed peak power budget p k pi while deriving the op where λlow α p corollary for dfs powmax is optimal for dvfs powmax is optimal at low arrival rates and powmin is op timal at high arrival rates intuition for a linear power to frequency relationship we have from eq that the speed of a server si varies with the power allocated to it pi as si sb α pi b by eqs and the following power allocation mini mizes e t for low n based on the asymptotic approxima tions in from this equation it follows that the frequency per watt for a single server si can be written as pi si sb αb α pi pi hence maximizing the frequency per watt depends on whether sb αb or sb αb if sb αb maximizing is equivalent n c pn n k corollary for a closed loop server farm configura tion with low n powmax is optimal for dfs dvfs and dvfs dfs to maximizing p pi i which is achieved by powmax alterna intuition when n is sufficiently low there are very few tively if sb αb we want to minimize i which is achieved by powmin however the above argument still does not take into account the mean arrival rate λ if λ is sufficiently low there are very few jobs in the server farm hence few fast servers or powmax is optimal the corollary follows by simply plugging in the values of sb α and b for dfs and dvfs from section theorem derives the optimal power allocation for non linear power to frequency relationships such as the cubic relationship in the case of dvfs dfs in such cases the server frequency varies with the power allocated to it as si sb αl i b the theorem says that if the arrival rate is sufficiently low then powmax is optimal however if the arrival rate is high powmed is optimal although the orem specifies a cubic power to frequency relationship we conjecture that similar results hold for more general power to frequency curves where server frequency varies as the n th jobs in the system hence few fast servers are optimal since there aren t enough jobs to utilize the servers leaving servers idle thus powmax is optimal this is similar to the case of low arrival rate that we considered for an open loop server farm configuration in theorems and when n is high the optimal power allocation is non trivial from here on we assume n is large enough to keep all servers busy so that asymptotic bounds apply see in our experiments we find that n suffices theorem says that for high n if the speed at baseline sb is sufficiently low then powmax is optimal by contrast if sb is high then powmin is optimal theorem given a closed k server farm configuration with a linear power to frequency relationship given by eq the following power allocation minimizes e t for high n based on the asymptotic approximations in if sb α n c p if sb α m b p b theorem given an open k server farm configuration m m k with a cubic power to frequency relationship given by eq and power budget the following power allocation mini mizes e t n c pn n k if λ λllow l pl pl l k if λ λllow corollary for dfs powmax is optimal for high n for dvfs powmin is optimal for high n intuition for a closed queueing system with zero think time the mean response time is inversely proportional to i the throughput of the system hence to minimize the mean b corollary for dvfs dfs powmax is optimal at low arrival rates and powmed is optimal at high arrival rates intuition when the arrival rate is sufficiently low there are very few jobs in the system hence powmax is optimal however for higher arrival rates we allocate to each server the amount of power that maximizes its frequency per watt ratio for the cubic power to frequency relationship which has a downwards concave curve see fig b we find that the optimal power allocation value for each server derived via calculus lies between the maximum c and the minimum b hence powmed is optimal theorems for closed loop configurations we now move on to closed loop configurations where the a server farm configuration all servers are busy hence the throughput is the sum of the server speeds it can be easily shown that the throughput of the system under powmin sb m exceeds the throughput of the system under powmax sc n when sb αb hence the result theorem deals with the case of high n for a non linear power to frequency relationship the theorem says that if the speed at baseline sb is sufficiently low then powmax is optimal by contrast if sb is high then powmed is optimal theorem given a closed k server farm configuration with a cubic power to frequency relationship given by eq the following power allocation minimizes e t for high n based on the asymptotic approximations in if sb sl n c pn n k if sb sl l b x pl l k where l p sl msc αl x and x is the non will rely on asymptotic operational laws see which approximate the performance of the system for very high n and very low n see appendix theorem says that for a closed server farm configuration with sufficiently low value of n powmax is optimal theorem given a closed k server farm configuration with a linear or cubic power to frequency relationship given negative real solution of the equation b αj sb corollary for dvfs dfs for high n powmed is optimal if sb is high else powmax is optimal intuition as in the case of theorem we wish to maximize the throughput of the system when we turn on a new dfs b dvfs figure open loop experimental results for mean response time as a function of the arrival rate using dfs and dvfs for linpack in fig a powmax outperforms powmin for all arrival rates under dfs by as much as a factor of by contrast in fig b for dvfs at lower arrival rates powmax outperforms powmin by up to while at higher arrival rates powmin outperforms powmax by up to server at b units of power the increase in throughput of the system is sb ghz for low values of sb this increase is small hence for low values of sb we wish to turn on as few servers as possible thus powmax is optimal however when sb is high it pays to turn servers on once a server is on the initial steep increase in frequency per watt afforded by a cubic power to frequency relationship advocates running the server at more than the minimal power b the exact optimal powmed power value b x in the theorem is close to the knee of the cubic power to frequency curve experimental results in this section we test our theoretical results from sec tion on an ibm bladecenter using the experimental setup discussed in section we shall first present our experimen tal results for the open server farm configuration and then move on to the closed server farm configuration for the experiments in this section we use the intel linpack workload which is cpu bound we defer experimental re sults for other workloads to section as noted in section the baseline power level and the maximum power level for both dfs and dvfs are b and c respectively for dvfs dfs b and c in each of our experiments we try to fix the power budget to be an integer multiple of b and c as in eq open server farm configuration fig a plots the mean response time as a function of the arrival rate for dfs with a power budget of in this case powmax represented by the dashed line de notes running servers at c and turning off all other servers powmin represented by the solid line de notes running servers at b and turning off all other servers clearly powmax outperforms powmin throughout the range of arrival rates this is in agreement with the predictions of theorem note from fig a that the im provement in mean response time afforded by powmax over mean arrival rate jobs sec figure open loop experimental results for mean response time as a function of the arrival rate using dvfs dfs for linpack at lower arrival rates powmax outperforms powmed by up to while at higher arrival rates powmed outperforms powmax by up to note that powmin is worse than both powmed and powmax throughout the range of arrival rates powmin is huge ranging from a factor of at low arrival rates load ρ to as much as a factor of at high arrival rates load ρ this is because the power to frequency relationship for dfs is steep see fig a hence running servers at maximum power levels affords a huge gain in server frequency arrival rates higher than jobs sec cause our systems to overload under powmin because sb is very low for dfs hence we only go as high as jobs sec fig b plots the mean response time as a function of the arrival rate for dvfs with a power budget of powmax represented by the dashed line again denotes run ning servers at c and turning off all other servers multi programming level n multi programming level n dfs b dvfs figure closed loop experimental results for mean response time as a function of number of jobs n in the system using dfs and dvfs for linpack in fig a for dfs powmax outperforms powmin for all values of n by almost a factor of throughout by contrast in fig b for dvfs at lower values of n powmax is slightly better than powmin while at higher values of n powmin outperforms powmax by almost powmin represented by the solid line denotes running servers at b and turning off all other servers we see that when the arrival rate is low powmax produces lower mean response times than powmin in particular when the arrival rate is jobs sec powmax affords a im provement in mean response time over powmin however at higher arrival rates powmin outperforms powmax as predicted by theorem in particular when the arrival rate is job sec powmin affords a improvement in mean response time over powmax under dvfs we can afford arrival rates up to job sec before overloading the system to summarize under dvfs we see that powmin can be preferable to powmax this is due to the flatness of the power to frequency curve for dvfs see fig a and powmin x powmed x powmax x agrees perfectly with theorem fig plots the mean response time as a function of the arrival rate for dvfs dfs with a power budget of in this case powmax represented by the dashed line denotes running servers at c and turning off all other servers powmed represented by the solid line denotes running servers at b c and turning off all other servers we see that when the arrival rate is low powmax produces lower mean response times than powmed however at higher arrival rates powmed outperforms pow max exactly as predicted by theorem for the sake of completion we also plot powmin dotted line in fig note that powmin is worse than both powmed and pow max throughout the range of arrival rates note that we use the value of b c as the optimal power allocated to each server in powmed for our experiments as this value is close to the theoretical optimum predicted by theorem which is around for the range of arrival rates we use and also helps to keep the power budget at closed server farm configuration we now turn to our experimental results for closed server farm configurations fig a plots the mean response time as a function of the multi programming level mpl n for dfs with a power budget of p in this case multi programming level n figure closed loop experimental results for mean response time as a function of number of jobs n in the system using dvfs dfs for linpack at lower values of n powmed is slightly better than powmax while at higher values of n powmed out performs powmax by as much as note that powmin is worse than both powmed and powmax for all values of n powmax represented by the dashed line denotes running servers at c and turning off all other servers powmin represented by the solid line denotes running servers at b and turning off all other servers clearly powmax outperforms powmin throughout the range of n by almost a factor of throughout the range this is in agreement with the predictions of theorem fig b plots the mean response time as a function of the multi programming level for dvfs with a power bud get of powmax represented by the dashed line again denotes running servers at c and turning off all other servers powmin represented by the solid line dfs and dvfs b dvfs dfs figure power to frequency curves for dfs dvfs and dvfs dfs for the cpu bound daxpy workload fig a illustrates our measurements for dfs and dvfs in both these mechanisms we see that the server frequency is linearly related to the power allocated to the server fig b illustrates our measurements for dvfs dfs where the power to frequency curve is better approximated by a cubic relationship mean arrival rate jobs sec mean arrival rate jobs sec 80 mean arrival rate jobs sec a dfs b dvfs c dvfs dfs figure open loop experimental results for mean response time as a function of the arrival rate using dfs dvfs and dvfs dfs for the cpu bound daxpy workload in fig a for dfs powmax outperforms powmin throughout the range of arrival rates by as much as a factor of in fig b for dvfs powmax outperforms powmin throughout the range of arrival rates by around in fig c for dvfs dfs powmax outperforms both powmed and powmin throughout the range of arrival rates while at lower arrival rates powmax only slightly outperforms powmed at higher arrival rates the improvement is around 60 denotes running servers at b and turning off all other servers we see that when n is high powmin pro duces lower mean response times than powmax this is in agreement with the predictions of theorem in particular when n powmin affords a improvement in mean response time over powmax however when n is low pow max produces slightly lower response times than powmin this is in agreement with theorem fig plots the mean response time as a function of the multi programming level for dvfs dfs with a power bud get of in this case powmax represented by the dashed line denotes running servers at c and turning off all other servers powmed represented by predictions of theorem in particular when n powmed affords a improvement in mean response time over powmax however when n is low powmed produces only slightly lower response times than powmax note that throughout the range of n powmin is outperformed by both powmax and powmed other workloads thus far we have presented experimental results for a cpu bound workload linpack in this section we present ex perimental results for other workloads our experimental results agree with our theoretical predictions even in the the solid line denotes running servers at b c case of non cpu bound workloads and turning off all other servers powmin represented by the dotted line denotes running servers at we see that when n is high powmed produces lower mean re sponse times than powmax this is in agreement with the we fully discuss experimental results for two workloads daxpy and stream in this section and summarize our results for other workloads at the end of the section due to lack of space we only show results for open loop configura tions daxpy daxpy is a cpu bound workload which we have sized to be cache resident this means daxpy uses a lot of processor and cache but rarely uses the server mem ory and disk subsystems hence the power to frequency relationship for daxpy is similar to that of cpu bound linpack except that daxpy peak power consumption tends to be lower than that of linpack since daxpy does not use a lot of memory or disk figs a and b present our results for the power to frequency relationship for daxpy the functional form of the power to frequency relationship under dfs and dvfs in fig a is clearly linear however the power to frequency relationship under dvfs dfs in fig b is better approx imated by a cubic relationship these trends are similar to the power to frequency relationship for linpack seen in fig figs a b and c present our power allocation results for daxpy under dfs dvfs and dvfs dfs respec tively for dfs in fig a powmax outperforms powmin throughout the range of arrival rates by as much as a fac tor of this is in agreement with theorem note that we use as the power allocated to each server under powmin to keep the power budget same for powmin and powmax for dvfs in fig b powmax outperforms powmin throughout the range of arrival rates by around this is in contrast to linpack where powmin out performs powmax at high arrival rates the reason why powmax outperforms powmin for daxpy is the lower value of sb ghz for daxpy as compared to sb ghz for linpack since sb α for daxpy under dvfs theorem rightly predicts powmax to be op timal finally in fig c for dvfs dfs powmax out performs both powmed and powmin throughout the range of arrival rates again this is in contrast to linpack where powmed outperforms powmax at high arrival rates the reason why powmax outperforms powmed for daxpy is the higher value of αl ghz w for daxpy as compared to αl ghz w for linpack this is in agreement with the predictions of theorem for high values of αl intuitively for a cubic power to frequency re lationship we have from eq sb αl b as αl increases we get more server frequency for every watt of power added to the server thus at high αl we allocate as much power as possible to every server implying powmax stream stream is a memory bound workload which does not use a lot of processor cycles hence the power consumption at a given server frequency for stream is usually lower than cpu bound linpack and daxpy figs a and b present our results for the power to frequency relationship for stream surprisingly the functional form of the power to frequency relationship under dfs dvfs and dvfs dfs is closer to a cubic relation ship than to a linear one in particular the gain in server frequency per watt at higher power allocations is much lower than the gain in frequency per watt at lower power alloca tions we argue this observation as follows at extremely low server frequencies the bottleneck for stream perfor mance is the cpu thus every extra watt of power added to the system would be used up by the cpu to improve its frequency however at higher server frequencies the bottle neck for stream performance is the memory subsystem since stream is memory bound thus every extra watt of power added to the system would mainly be used up by the memory subsystem and the improvement in processor frequency would be minimal figs a b and c present our power allocation re sults for stream under dfs dvfs and dvfs dfs re spectively due to the downwards concave nature of the power to frequency curves for stream studied in fig theorem says that powmax should be optimal at low ar rival rates and powmed should be optimal at high arrival rates however for the values of αl in fig we find that the threshold point λlow below which powmax is optimal is quite high hence powmax is optimal in fig c in figs a and b powmax and powmed produce similar response times gzip and gzip and are common software applications used for data compression in unix systems these cpu bound com pression applications use sophisticated algorithms to reduce the size of a given file we use gzip and to compress a file of uncompressed size mb for gzip we find that powmax is optimal for all of dfs dvfs and dvfs dfs these results are similar to the results for daxpy for the results are similar to those of linpack in particular at low arrival rates powmax is optimal for high arrival rates powmax is optimal for dfs powmin is optimal for dvfs and powmed is optimal for dvfs dfs webbench webbench is a benchmark program used to measure web server performance by sending multiple file requests to a server for webbench we find the power to frequency relationship for dfs dvfs and dvfs dfs to be cubic this is similar to the power to frequency relationships ob served for stream since webbench is more memory and disk intensive as theory predicts see theorem we find powmax to be optimal at low arrival rates and powmed to be optimal at high arrival rates for dfs dvfs and dvfs dfs summary in this paper we consider the problem of allocating an available power budget among servers in a server farm to minimize mean response time the amount of power allo cated to a server determines its speed in accordance to some power to frequency relationship hence we begin by mea suring the power to frequency relationship within a single server we experimentally find that the power to frequency relationship within a server for a given workload can be ei ther linear or cubic interestingly we see that the relation ship is linear for dfs and dvfs when the workload is cpu bound but cubic when it is more memory bound by con trast the relationship for dvfs dfs is always cubic in our experiments given the power to frequency relationship we can view the problem of finding the optimal power allocation in terms of determining the optimal frequencies of servers in the server farm to minimize mean response time however there are several factors apart from the server frequencies that affect the mean response time for a server farm these include the arrival rate the maximum speed of a server the total power budget whether the server farm has an open or closed configuration etc to fully understand the effects of these a dfs and dvfs b dvfs dfs figure power to frequency curves for dfs dvfs and dvfs dfs for the memory bound stream workload fig a illustrates our measurements for dfs and dvfs while fig b illustrates our measurements for dvfs dfs in all the three mechanisms the power to frequency curves are downwards concave depicting a cubic relationship between power allocated to a server and its frequency a dfs b dvfs c dvfs dfs figure open loop experimental results for mean response time as a function of the arrival rate using dfs dvfs and dvfs dfs for the memory bound stream workload in figs a and b for dfs and dvfs respectively powmed and powmax produce similar response times in fig c however for dvfs dfs powmax outperforms powmed by as much as at high arrival rates in all three cases powmin is worse than both powmed and powmax factors on mean response time we develop a queueing theo retic model see section that allows us to predict mean response time as a function of the above factors we then produce theorems see sections and that determine the optimal power allocation for every possible configuration of the above factors to verify our theoretical predictions we conduct extensive experiments on an ibm bladecenter for a range of work loads using dfs dvfs and dvfs dfs see section and in every case we find that the experimental results are in excellent agreement with our theoretical predictions discussion and future work there are many extensions to this work that we are ex ploring but are beyond the scope of this paper first of all the arrival rate into our server farm may vary dynamically over time in order to adjust to a dynamically varying arrival rate we may need to adjust the power alloca tion accordingly the theorems in this paper already tell us the optimal power allocation for any given arrival rate we are now working on incorporating the effects of switching costs into our model second while we have considered turning servers on or off today technology allows for servers which are sleep ing halt state or deep c states these sleeping servers consume less power than servers that are on and can more quickly be moved into the on state than servers that are turned off we are looking at ways to extend our theorems to allow for servers with sleep states third while this paper deals with power management at the server level measuring and allocating power to the server as a whole our techniques can be extended to deal with individual subsystems within a server such as power allocation within the storage subsystem we are looking at extending our implementation to individual components within a server references lesswatts org race to idle power management race to idle php intel nehalem pdf u s environmental protection agency epa report on server and data center energy efficiency national electrical contractors association data centers meeting today demand jeffrey s chase darrell c anderson prachi n thakar and amin m vahdat managing energy and server resources in hosting centers in in proceedings of the eighteenth acm symposium on operating systems principles sosp pages intel corp intel duo mobile processor datasheet table 32012001 pdf m elnozahy m kistler and r rajamony energy conservation policies for web servers in usits xiaobo fan wolf dietrich weber and luiz andre barroso power provisioning for a warehouse sized computer pages wes felter karthick rajamani tom keller and cosmin rusu a performance conserving approach for reducing peak power consumption in server systems in ics proceedings of the annual international conference on supercomputing pages new york ny usa acm mark e femal and vincent w freeh boosting data center performance through non uniform power allocation in icac proceedings of the second international conference on automatic computing pages washington dc m s floyd s ghiasi t w keller k rajamani f l rawson j c rubio and m s ware system power management support in the ibm microprocessor ibm journal of research and development anshul gandhi mor harchol balter rajarshi das and charles lefurgy optimal power allocation in server farms technical report cmu cs intel corp intel math kernel library david mosberger and tai jin httperf a tool for measuring web server performance acm sigmetrics performance evaluation review 1998 vivek pandey w jiang y zhou and r bianchini dma aware memory energy management hpca the international symposium on high performance computer architecture pages feb ramya raghavendra parthasarathy ranganathan vanish talwar zhikui wang and xiaoyun zhu no power struggles coordinated multi level power management for the data center in asplos xiii proceedings of the international conference on architectural support for programming languages and operating systems pages k rajamani h hanson j c rubio s ghiasi and f l rawson online power and performance estimation for dynamic power management research report rc july salvatore sanfilippo wbox http testing tool version x wang and m chen cluster level feedback power control for performance optimization ieee international symposium on high performance computer architecture hpca february zhikui wang xiaoyun zhu cliff mccarthy partha ranganathan and vanish talwar feedback control algorithms for power management of servers in third international workshop on feedback control implementation and design in computing systems and networks febid annapolis md june appendix proofs of open loop configura tion theorems section in this appendix we provide brief sketches of the proofs of each theorem from the paper all theorems again assume eq theorem given an open k server farm configuration with a linear power to frequency relationship and power bud get p the following power allocation setting minimizes e t if sb α n c p if sb α n c pn n k if λ λlow na eng htm raj jain the art of computer systems performance analysis techniques for experimental design measurement simulation and modeling pages wiley radim kolar web bench http home tiscali cz webbench html kleinrock l queueing systems volume where λlow α p proof we provide a complete proof for the case of k in lemma we then use this result to prove the theorem for the case of arbitrary k by contradiction lemma given an open server farm configuration with linear power to speed relationship and power budget the following power allocation minimizes e t if sb α min c p p wiley interscience new york min c p p if λ λlow charles lefurgy xiaorui wang and malcolm ware power capping a prelude to power shifting cluster if sb α b b if b c λ λ min c p min c p otherwise computing november j d mccalpin stream sustainable memory bandwidth in high performance computers where λlow sc α c b c b p sb sb α p proof lemma the proof is trivial for the cases p and thus assume let be split among the two servers as the goal is to find the optimal power allocation that min imizes e t given the constraint two servers say servers i and j such that b pi c and b pj c we invoke lemma on servers i and j with total power pl pi pj lemma tells us that the optimal power allocation for servers i and j is pi min c pl and p j l i it can also be shown see that reducing the mean response time of the server system consisting assuming a linear power to frequency relationship we have the following relationship for the speed of server i si as a function of the power allocated to it pi si sb α pi b where α sb and b are as defined in sections and we now wish to derive e t for a server farm with speeds and recall from section that we have a ps schedul ing discipline and a poisson arrival process with some mean λ it is well known that for a single m g ps queue with speed poisson arrival rate λ and general job size e t is as follows e t m g ps λ by exploiting poisson splitting in our model we have that the mean response time of jobs in the server farm with split ting parameter q where q is the fraction of jobs sent to the server and server speeds and is of servers i and j reduces the mean response time of the whole system thus we have shown that the mean response time of the system under π can reduced thus powmax is optimal the case of sb α is more complex we want to show that the servers that are turned on should either all run at power c or all run at power b depending on λ we start with an arbitrary power allocation and show that we can repeatedly apply lemma to two servers at a time to end up in a power allocation where we have some servers at power b some servers at power c and at most one server at some intermediate power at this point we optimize over all power allocation settings of the above type and find that e t is minimized in the particular power allocation where either n servers are at power c for low λ or m servers are at power b for high λ for details see theorem given an open k server farm configuration with a cubic power to frequency relationship as in eq and power budget p the following power allocation setting e t q q λq λ q minimizes e t n c pn n k if λ λllow l pl pl l k if λ λllow given the stability constraints i from eq we see that e t is a function of and q however using eqs and we can express in terms of as α p we now derive the optimal value of q which we call q to do this we fix the speeds and in eq and set the derivative of e t w r t q to be this yields b proof we follow the same process as in the proof of thm staring with the case of two servers we note that eq is still valid in the case of a cubic power to frequency relationship however eq now takes the form sb p sb where αl sb and b are as defined in section we can q λ now express e t from eq in terms of just by substituting for q and from eqs and hence λ we can differentiate e t w r t to yield the optimal using q in the expression for e t from eq gives us which translates to we find that for the server case powmax is optimal at low arrival rates whereas powmed e t λ is optimal at high arrival rates note that powmed for two at this point we can express e t in terms of just by substituting for from eq hence we can differentiate e t w r t to yield the optimal which translates via eq to we find that for sb α powmax minimizes otherwise we now return to the proof of thm when there are k servers we want to show that we should either run n servers at power c or run l servers at power pl depending b on λ we start with an arbitrary power allocation and show e t where powmax for servers refers to min c p and p for sb α we find that powmax minimizes e t for λ λlow whereas powmin minimizes e t for λ λ low powmin for servers refers to b p b if p b c and min c p min c p otherwise we now return to the proof of thm we first consider the case sb α the proof proceeds by contradiction we claim that powmax is optimal thus assume we have a power allocation π k which is not the same as powmax since π is not powmax there must exist at least that we can repeatedly apply the two server result discussed above to pairs of servers at a time to end up in a power allocation where we have some v servers at power c and some w servers at power p vc at this point we optimize over all power allocation settings of the above type and find that e t is minimized in the particular power allocation where either n servers are at power c for low λ or l servers are at power pl for high λ for details see proofs of closed loop config uration theorems section for closed loop configurations provides well known where α sb and b are as defined in section thus we have asymptotic bounds for the mean response time both in the case where n the number of jobs in the system is very high and in the case where n is very low these bounds provide excellent approximations for e t here n is considered j si i j sb α pi b i j high if it significantly exceeds the number of servers that are on and n is considered low if it is close to we will use these approximations from to derive the optimal power allocations in theorem low n and in theorems and high n j sb αb α pi i j sb αb αp hence if sb α e t is minimized by maximizing j thus theorem given a closed k server farm configuration b powmin is optimal for sb α when sb α e t is with a linear or cubic power to frequency relationship the following power allocation minimizes e t for low n based on the asymptotic approximations in n c pn n k proof we start by assuming the number of jobs in the system n is low from we have that minimized by minimizing j thus powmax is optimal for sb α theorem given a closed k server farm configuration with a cubic power to frequency relationship the following power allocation minimizes e t for high n based on the asymptotic approximations in if sb sl n c pn n k if sb sl l b x pl l k q where l p sl msc αl x and x is the non i without loss of generality assume sk αj sb proof as in the proof for theorem minimizing e t this implies sk thus e t given by is equivalent to maximizing j si where j k is the eq is minimized by setting if or qa if sa for some integer a if we want to minimize e t thus powmax is optimal number of servers that are turned on for a cubic power to frequency relationship given by eq we have l in this case for the case of sa for some integer si sb α pi b a we want to minimize e t for i a this i is achieved by maximizing si clearly by setting a pc si where αl sb and b are as defined in section thus we have is maximized by i c for i a hence for low n powmax is optimal theorem given a closed k server farm configuration i si i sb αl pi b j with a linear power to frequency relationship the following power allocation minimizes e t for high n based on the asymptotic approximations in if sb α n c p jsb αl pi b i since the sum of pi is a constant the sum of their cube if sb α m b p roots attains its maximum value when all the pi are equal b m m k this follows from the generalized mean inequality thus proof assuming that n is high from we have that we wish to maximize j sb pj b by looking at without loss of generality assume sj response time we find that powmax is optimal for low values of sb whereas powmed is optimal for high values of where j k denotes the number of servers that are turned sb hence the result on thus e t n from eq since n is a constant minimizing e t is now equivalent to minimizing which attains the lowest value of when however itself attains the lowest value of when s2 s2 and so on thus to minimize e t we must set qj r say since j q we have e t n n r n thus minimizing e t is equivalent to maximizing j si for a linear power to frequency relationship given by eq we have si sb α pi b somniloquy augmenting network interfaces to reduce pc energy usage yuvraj agarwal steve hodges ranveer chandra james scott paramvir bahl rajesh gupta microsoft research university of california san diego shodges ranveer jws bahl microsoft com abstract reducing the energy consumption of pcs is becoming in creasingly important with rising energy costs and environmen tal concerns sleep states such as suspend to ram save energy but are often not appropriate because ongoing network ing tasks such as accepting remote desktop logins or perform ing background ﬁle transfers must be supported in this paper we present somniloquy an architecture that augments network interfaces to allow pcs in to be responsive to network traf ﬁc we show that many applications such as remote desktop and voip can be supported without application speciﬁc code in the augmented network interface by using application level wakeup triggers a further class of applications such as in stant messaging and peer to peer ﬁle sharing can be supported with modest processing and memory resources in the network interface experiments using our prototype somniloquy imple mentation a usb based network interface demonstrates en ergy savings of to in most commonly occuring sce narios this translates to signiﬁcant cost savings for pc users introduction many personal computers pcs remain switched on for much or all of the time even when a user is not present despite the existence of low power modes such as sleep or suspend to ram acpi state and hibernate acpi state the resulting electricity usage wastes money and has a negative impact on the environment pcs are left on for a variety of reasons see section including ensuring remote access to local ﬁles main taining the reachability of users via incoming email in stant messaging im or voice over ip voip clients ﬁle sharing and content distribution and so on unfortu nately these are all incompatible with current power saving schemes such as and in which the pc does not respond to remote network events existing solutions for sleep mode responsiveness such as wake on lan tion servers or conﬁgure network hardware a few initial proposals suggest the use of network proxies to perform lightweight protocol functionality such as re sponding to arps however such a system too requires signiﬁcant modiﬁcations to the network infrastructure and to the best of our knowledge such a prototype has not been described in published form see section for a full discussion in this paper we present a system called som that supports continuous operation of many network facing applications even while a pc is asleep somniloquy provides functionality that is not present in existing wake up systems in particular it allows a pc to sleep while continuing to run some applications such as bittorrent and large web downloads in the background in existing systems these applications would stop when the pc sleeps somniloquy achieves the above functionality by em bedding a low power secondary processor in the pc network interface this processor runs an embedded op erating system and impersonates the sleeping pc to other hosts on the network many applications can be sup ported either with or without application speciﬁc code stubs on the secondary processor applications sim ply requiring the pc to be woken up on an event can be supported without stubs while other applications require stubs but in return support greater levels of functionality during the sleep state we have prototyped somniloquy using a usb based low power network interface our system works for desktops and laptops over wired and wireless networks and is incrementally deployable on systems with an existing network interface it does not require any changes to the operating system to network hardware e g routers or to remote application servers we have implemented support for applications including remote desktop access ssh telnet voip im web downloads wol have not proven successful in the wild for a number of reasons such as the need to modify applica the act or habit of talking in one sleep and bittorrent our system can also be extended to sup port other applications we have evaluated somniloquy in various settings and in our testbed section a pc in somniloquy mode consumes to less power than a pc in idle state for commonly occurring scenarios this translates to energy savings of to we make the following contributions in this paper we present a new architecture to signiﬁcantly re duce the energy consumption of a pc while main taining network presence this is accomplished without changes in the network infrastructure we show that several applications bittorrent web downloads im remote desktop etc can consume much less energy this is achieved with out modifying the remote application servers we present and empirically validate a model to pre dict the energy savings of somniloquy for various applications we demonstrate the feasibility of somniloquy via a prototype using commodity hardware this proto type is incrementally deployable and saves signiﬁ cant energy in a number of scenarios motivation prior studies have shown that that users often leave their computer powered on even when they are largely idle a study by roberson et al shows that in ofﬁces of desktop pcs remain powered on outside work hours and only use sleep mode in home envi ronments roth et al show that average residential computer is on of the time but is not being actively used for more than half the time to uncover the reasons why people do not use sleep mode we conducted an informal survey we passed it among our contacts who in turn circulated it further we had respondents from various parts of the world of which worked in the it sector of the respon dents left at least one machine at home on all of the time and of the respondents left at least one work ma chine on even when no one was using it among the people who left their home machine pow ered on did so for remote access for quick availability and for applications running in the back ground of which ﬁle sharing downloading and im e mail were most popular in the ofﬁce envi ronment of respondents left their machines on for remote access and did so to support applications running in the background of which e mail and im were most popular although this survey should not be regarded as repre figure somniloquy augments the pc network inter face with a low power secondary processor that runs an embedded os and networking stack network port filters and lightweight versions of certain applications stubs shading indicates elements introduced by somniloquy pcs don t go to sleep even when they are unused sec ond signiﬁcant energy savings can be achieved if only a few applications remote reachability ﬁle sharing ﬁle downloads instant messaging e mail can be handled when the pc is asleep the somniloquy architecture our primary aims during the development of somnilo quy were to allow an unattended pc to be in low power state while still being available and active for network facing applications as if the pc were fully on to do so without changing the user experience of the pc or requiring modiﬁcation to the network infras tructure or remote application servers we accomplish these goals by augmenting the pc network interface hardware with an always on low power embedded cpu as shown in figure this sec ondary processor has a relatively small amount of mem ory and ﬂash storage which consumes much less power than if it were sharing the larger disk and memory of the host processor it runs an embedded operating system with a full tcp ip networking stack such as embedded linux or windows ce the ﬂash storage is used as a temporary buffer to store data before the data is trans ferred in a larger chunk to the pc a larger ﬂash on the secondary processor allows the pc to sleep longer sec tion this architecture has a couple of useful prop erties first it does not require any changes to the host operating system and second it can be incrementally de ployed on existing pcs using a peripheral network inter face section sentative of all users and is not statistically signiﬁcant it does highlight two important points first a number of prototype had mb dram and gb of ﬂash the software components of somniloquy and their in teractions are illustrated in figure the high level oper ation of somniloquy is as follows when the host pc is powered on the secondary processor does nothing the network stack on the host processor communicates di rectly with the network interface hardware when the pc initiates sleep the somniloquy daemon on the host pro cessor captures the sleep event and transfers the network state to the secondary processor this state includes the arp table entries ip address dhcp lease details and associated ssid for wireless networks i e mac and ip layer information it also includes details of what events the host should be woken on and application speciﬁc de tails such as ongoing ﬁle downloads that should continue during sleep following the transfer of this information to the secondary processor the host pc enters sleep although the host processor is asleep power to the network interface and the secondary processor is main tained to maintain transparent reachability to the host while it is asleep the secondary processor imper sonates the host by using the same mac and ip ad dresses host name dhcp details and for wireless the same ssid it also handles trafﬁc at the link and network layers such as arp requests and pings thereby main taining basic presence on the network new incoming connection requests for the host processor are now re ceived and handled by the network stack running on the secondary processor in this way the pc transition into sleep is transparent to remote hosts on the network to ensure that the host pc is reachable by various ap plications a process on the secondary processor mon itors incoming packets this process watches for pat terns such as requests on speciﬁc port numbers which should trigger wake up of the host processor although this simple architecture supports several ap plications with minimal complexity somniloquy can get much greater energy savings for some applications by not waking up the host processor for simple tasks for example to send instant messenger presence updates to perform these tasks on the secondary processor we re quire the application writer to add a small amount of application speciﬁc code stubs on the host and sec ondary processor in the rest of this section we describe in more detail how we handle various applications with and without application stubs somniloquy without application stubs the somniloquy daemon on the host processor speci ﬁes packet ﬁlters i e patterns on incoming packets on which the secondary processor should wake up the host processor from sleep state the somniloquy daemon cre ates ﬁlters at various layers of the network stack at the link layer and network layer the secondary processor can figure somniloquy software components on the host pc and the secondary processor and their interactions be told to wake the computer when it detects a particular packet analogously to the magic packets used by wake on lan though not requiring the mac address to be known by the remote host see further discussion in sec tion trigger conditions at the transport layer may also be speciﬁed for example wake on tcp port for telnet requests similarly somniloquy also supports wake ups on patterns in the application payload although the host pc will wake up within a few sec onds it will not receive the packet that triggered the wake up one way to solve this problem is to buffer the packet on the secondary processor and replay it on the network stack of the host processor once it has woken up however since the time to wake up is just a few sec onds most sources can be relied upon to retry the con nection request for example any protocol using tcp as the transport layer will automatically retransmit the initial syn packet even udp based applications that are designed for internet use are designed to cope with packet loss using automatic retransmissions this simple packet ﬁlter based approach to trigger ing wake ups has the advantage that application speciﬁc code does not need to be executed on the secondary pro cessor nonetheless it is sufﬁcient to support many ap plications that get triggered on remote connection re quests such as remote ﬁle access remote desktop access telnet and ssh requests to name a few application specific extensions several applications maintain active state on the pc even when it is idle and hence prevent a pc from going to sleep for example a movie download client on a home pc e g from netﬂix will require the host pc to be awake for a few hours while downloading the movie an instant messenger im client will require the pc to be on in order for the user to stay online reachable to their contacts somniloquy provides a way for these applications to consume signiﬁcantly less power by performing lightweight operations on the secondary processor it can opportunistically put the host processor to sleep for example the secondary processor can send and re ceive presence updates to from the im server while the host processor is asleep during a large download the secondary processor can download portions of the ﬁle putting the host processor to sleep in the meantime the key to supporting these applications is the use of stubs that run on the host and the secondary proces sor we have implemented stubs for three popular ap plications im msn aol icq bittorrent and web download here we will describe the general guidelines for writing these stubs and describe the speciﬁc imple mentations for the three applications in section writing application stubs when designing an appli cation stub the ﬁrst step is to understand the subset of the application functionality that needs to run when the pc is asleep this is implemented as a stub on the secondary processor for example for an im stub the functionality to send and receive presence updates is essential to main tain im reachability however the stub need not include any ui related code such as opening a chat window we note that it is not feasible for the stub to reuse the entire original application code from the host pc the application code might depend on drivers display disk etc that are absent on the secondary processor further more running the entire application might overload the secondary processor therefore only the essential com ponents of the application are implemented as part of the application stub another step in designing application stubs is to de cide when to wake up the host processor triggers can be user deﬁned for example waking up on an incoming call from a speciﬁc im contact triggers may also occur when the secondary processor resources are insufﬁ cient for example when the ﬂash is full or more cpu re sources are needed in all of these cases the stub wakes up the host processor to interface with the application on the host pc and the somniloquy daemon the application stub needs to have a component on the host processor this compo nent registers two callback functions with the somnilo quy daemon one that is called just before the pc goes to sleep and the other just after it has woken up the ﬁrst function transfers the application state to the stub on the secondary processor and also sets the trigger condi tions on which to wake the host processor these val ues depend on the application being handled by the stub the second callback function which is called when the host resumes from sleep checks the event that caused the wakeup whether it was caused by a trigger con dition on the secondary processor or due to user activ ity it handles these events differently if the wakeup was caused by user activity the stub transfers state from the secondary processor and disables it however if the wakeup was caused by a trigger condition on the sec ondary processor the application stub handles it as de ﬁned by the user for example for an incoming voip call the stub engages the incoming call functionality of the voip application having determined what functionality needs to be sup ported by the application stub and host based callbacks and what state must pass between them the ﬁnal step is to implement this we have used two manual approaches to doing this for the download stub we built all the functionality ourselves based on detailed knowledge of the application protocols and for the bittorrent and im stubs we trimmed down existing application code to re duce memory and cpu footprint an alternative could be to automatically learn protocol behavior to build these application stubs however we believe that this is an extremely difﬁcult problem there are parts of the ap plication that are difﬁcult to infer and any inaccuracy in the application stub will make it unusable for exam ple knowledge of how bittorrent hashes the ﬁle blocks is necessary for the stub to successfully share a ﬁle with peers we are unaware of any automatic tool that can learn such application behavior therefore we believe that the best although perhaps not the most elegant approach to building these stubs is to modify applica tion source code and remove functionality that is not re quired by the secondary processor in the future with a greater incentive to save energy we expect that appli cation developers will compete for energy consumption and hence provide stubs for their applications using the guidelines described in this section we realize that partial application stubs might be cre ated using tools such as the generic application level protocol analyzer and discoverer which auto matically learn the behavior and message formats for a range of protocols as part of future work we plan to explore how the knowledge of the protocol can be aug mented with application speciﬁc behavior to ease the de velopment of application stubs when to use application stubs not all applications are conducive to low power operation via application stubs a cpu intensive application such as a compi lation job will be very slow on the secondary processor since it has a less powerful cpu and low memory simi larly an i o intensive application such as a disk indexer will need to read the disk very often and will therefore need the pc to be awake download and ﬁle sharing ap plications are an interesting exception because portions of a ﬁle can be transferred by the secondary processor whilst the host sleeps we will discuss this approach in more detail in section even for an application stub that saves energy for a given application it is not always useful to ofﬂoad the ap plication to the secondary processor when the host pc is going to sleep several other applications may also want to run their application stubs on the secondary processor this might overload the cpu of the weaker low power secondary processor in this case it might be beneﬁcial to keep the host pc awake one way to solve this problem is to modify the som niloquy daemon to predict the cpu utilization of the stubs for all applications that are willing to be ofﬂoaded to the secondary processor however making this pre diction is extremely difﬁcult there might be little cor relation between the cpu utilization of the application on the host pc and the stub on the secondary proces sor because of different processor architectures and varying application demands instead we take a sys tems approach we monitor the cpu utilization of the secondary processor if it remains at more than continuously seconds we wake up the pc and re sume all applications on the host processor if the cpu utilization of these applications decreases by more than on the host processor we repeat the same procedure ofﬂoad to the secondary processor and stay there if cpu utilization is less than in our somniloquy de ployment the need to move applications arose when run ning multiple application stubs on the secondary proces sor such as two concurrent mbps web downloads and two concurrent bittorrent downloads of section incremental deployment we realize that somnil oquy may never be universally deployed and that get ting software vendors to try for incremental deployment when it would previously have been awake for applica tions without stubs this proportion is largely dependent on the actions of a remote user how frequently a re mote ssh session is initiated for example and for how long on the other hand for applications with stubs the secondary processor may regularly wake up the host to perform some task or other we quantify the energy sav ings for an application with different wake up intervals in section more formally suppose the host is woken up once ev ery tsleep seconds whereupon it stays awake for tawake seconds tawake includes the time it takes to transfer data between the pc and the secondary processor also assume that d is sum of the time to wake up the host plus the time to transition to sleep suppose pa is the power consumption of the pc when it is awake in w ps is power consumed in sleep mode in w and pe is power consumed by the secondary embed ded processor in w the energy e consumed during somniloquy operation is given by esomniloquy epcinsleepmode epcinawakemode esecondaryprocessor tsleep ps tawake d pa tawake d tsleep pe joules in the absence of somniloquy the amount of energy consumed by the host pc in the same time is ehost pa tawake tsleep joules therefore the ratio of energy consumed by somniloquy compared to the host pc being always on is given by esomniloquy tsleep pe ps tawake pa pe d pa ps requires a low effort mechanism to ensure that their ehost pa tawake tsleep somniloquy enhanced software is compatible with ma chines and platforms that do not have somniloquy sup port the somniloquy daemon queries the os to de termine the presence of a secondary processor and the supported application stubs applications then need to query the somniloquy daemon and invoke the applica tion stubs only if the os supports somniloquy and the corresponding stubs are implemented on the secondary processor quantifying energy savings the amount of energy saved through adoption of som niloquy is quite easy to predict it depends on the relative power consumption of the awake and sleep states and the proportion of time that a machine can be kept asleep typically as we show in section pe and ps are two orders of magnitude less than pa for a desktop computer and d is around seconds to wake up the host and put it back to sleep therefore for most energy savings we would want tawake to be much less than tsleep i e if tawake tsleep then the ratio esomniloquy ehost is approximately pe ps pa we will present the approximate energy savings for different applications in section of course somniloquy could save more energy by dis abling the secondary processor when the pc is awake this would require the pc to enable the secondary pro cessor before going to sleep and disable it when the pc has woken up we were unable to fully implement this functionality in our prototype but we expect this to be a minor ﬁx in a production system discussion security a common requirement of corporate it de partments is that all pcs should be up to date with the latest os and application patches somniloquy can en sure that this constraint is met even when pcs are asleep this is achieved using a port based trigger to wake up the host pc when the sms systems management server contacts the host pc to install updates somniloquy ensures that the secondary processor is secure by patching its os whenever security updates become available also it prevents attackers from re placing the secondary processor by requiring that it be a physically part of the pc as part of the network in terface in some cases however the functionality that somniloquy provides could be misused to conduct at tacks that spuriously wake up the pc and waste energy this kind of denial of service attack would be particu larly effective for mobile devices where a drained bat tery might result one way to address this issue is to disable port triggers and instead exclusively use appli cation stubs which ensure that only authenticated remote hosts are allowed to trigger wakeup another concern is that application stubs and hence the use of extra code increases the pc attack surface to mitigate the impact of this vulnerability we use a few techniques first the secondary processor only listens on ports that have been opened by applications on the host pc second we require the pc and the secondary processor to be on the same administrative domain we also note that modern processors have additional security features built in for example an execute disable bit used by some applications to prevent executing ar bitrary code and preventing buffer overﬂows we realize that a low power processor may not currently support this advanced functionality although we expect that in the future low power chips will also be available with these features alternative design with the increasing prevalence of multi core pcs one idea to alleviate the need for the additional secondary processor introduced by somnilo quy would be to use one of the cores of the host cpu in stead running just one core at the lowest possible clock frequency would minimize energy consumption and ob viate the need for a separate low power processor in the nic however it turns out that such an approach is not use ful without signiﬁcant modiﬁcation to today pc archi tecture our measurements see section show that the power consumption of a multi core pc with only one core active running at the lowest permissible clock speed is still approximately times that of our low power sec ondary processor even with all other peripherals in their lowest power modes e g disk spun down this is be cause of the lack of truly ﬁne grained power control of pc components such as the northbridge southbridge memory buses parts of the storage hierarchy and various peripherals even if ﬁne grained control were available the base power consumption of individual components nic hard drive is signiﬁcant see table one way to reduce this base power draw would be to have a sep arate and relatively simple core with a small amount of associated memory running from a separate power do main so that it can function without powering on other components such an architecture is very similar to som niloquy and most of our design principles can easily be adopted prototype implementation we have prototyped somniloquy using gumstix a low power modular embedded processor platform manufac tured by gumstix inc that support a wide variety of pe ripherals hardware and software overview an important goal when prototyping somniloquy was to have it work with existing unmodiﬁed desktops and lap tops and for both wired and wireless networks further more we required the platform to be low power have a small form factor and be well supported for develop ment the gumstix platform served all these design re quirements well the speciﬁc components we use for somniloquy include a connex processor board an etherstix network interface card nic for wired eth ernet a wiﬁstix nic for wi fi and a thumbstix com bined usb interface breakout board the connex employs a low power mhz xscale pro cessor with mb of non volatile ﬂash and mb of ram the etherstix provides a wired eth ernet interface plus an sd memory slot to which we have attached a sd card the thumbstix provides a usb connector serial connections and general purpose input and output gpio connections from the xscale to enable somniloquy we needed mechanisms to wake up the host pc and also to detect its state awake or in to achieve this we added a custom de signed circuit board that incorporates a single chip the from ftdi the is a usb to serial converter chip supporting functionality such as sending a resume signal to the host and detecting the state of the host both over the usb bus this board is attached to the computer via a second usb port and to the thumb stix module and thence to the xscale processor via a two wire serial interface plus two gpio lines one gpio line is connected to the ring indi cator input to wake up the computer the second gpio figure block diagram of the somniloquy prototype system wired version the figure shows various components of the gumstix and the usb interfaces to the host laptop line is connected to the sleep output which can be polled by the gumstix to detect whether the host pc is active or in as mentioned above and shown in figure the com puter is connected to the secondary processor via two usb connections one of these provides power and two way communications between the two processors it is conﬁgured to appear as a point to point network inter face usbnet over which the gumstix and the host computer communicate using tcp ip the second usb interface provides sleep and wake up signaling and a se rial port for debugging purposes the use of two usb interfaces is not a fundamental requirement it is simply for ease of prototyping since we use standard usb ports for interfacing with the host and for sleep signaling our prototype works on any recent desktop or laptop that supports usb we run an embedded distribution of linux on the gumstix that supports a full tcp ip stack dhcp conﬁgurable routing tables a conﬁgurable ﬁrewall ssh and serial port com munication this provides a ﬂexible prototyping plat form for somniloquy with very low power operation we have implemented the somniloquy host software on windows vista the somniloquy daemon detects transition to sleep state and before this is allowed to occur we transfer the network state mac address ip address and in the case of the wireless prototype the ssid of the ap and other information about the wakeup triggers as discussed in section figure photograph of the gumstix based somniloquy prototype wired version three different prototypes we have prototyped three different somniloquy designs to explore different aspects of operation the ﬁrst uses the gumstix as an augmented ethernet interface as de scribed in section however in our prototype this has some performance limitations so we have also imple mented a second design which uses the gumstix in co operation with an existing high speed ethernet interface finally we have a wi fi version all three prototypes are described in further detail below augmented network interface we call this imple mentation the wired version the architecture is shown in figure with a photograph of the prototype shown in figure in this prototype we disable the nic of the host and conﬁgure the pc to use the usbnet in terface usb connection between the gumstix and the host as its only nic the gumstix is connected to the network using its ethernet connection to enable the host pc to be on the network we set up a transparent layer software bridge between the usbnet interface to the host and the ethernet interface of the gumstix this bridge is active when the host is awake when the host transitions to sleep the gumstix disables the bridge and resets the mac address of its ethernet interface to that of the us bnet interface of the host the gumstix thus appears to the rest of the network as the host itself since it has the same network parameters ip mac address when the host wakes up the gumstix resets its mac address to its original value and starts bridging trafﬁc to the host again although our wired prototype hardware sup ports a mbps ethernet interface we are limited to a throughput of mbps due to the bandwidth supported by the usbnet interface driver there is also a slight over head of bridging trafﬁc on the gumstix although this limits bandwidth to the host signiﬁcantly in our proto type we note that in a ﬁnal integrated version this over head of bridging can be avoided by allowing both the host and the low power secondary processor to access the nic directly using existing network interface somniloquy can coexist with an existing nic on such systems the over head of bridging is avoided by using the existing ether net interface on the host pc for data transfer when it is awake with the gumstix using its own ethernet interface while still impersonating the host pc when the host is asleep we have built this version where the gumstix does not perform layer bridging and call it the wired prototype using wi fi we have also implemented a wireless version of somniloquy we were unable to implement a one nic version since the marvell b g chipset present on the wiﬁstix does not currently sup port layer bridging we have however implemented a wireless version applications without stubs we have implemented a ﬂexible packet ﬁlter on the gum stix using the bsd raw socket interface to support appli cations that do not require stubs e g rdp ssh telnet and smb connections every application in this class provides a regular expression matched against incoming packets to decide whether to trigger host wakeup for example handling incoming remote desktop requests re quires the host to be woken up when the gumstix receives a tcp packet with destination port we note that waking up the host computer is not enough the incoming connection request must somehow be conveyed to the host we accomplish this by using the iptables ﬁrewall on the gumstix to ﬁlter any re sponse to tcp or udp packets that the gumstix does not handle itself thus trigger packets are not acknowledged by the gumstix and the remote client sends retries af ter the host has resumed one of the retries will reach it since it is still using the same ip and mac addresses and it will respond directly using port based ﬁltering we have implemented wake up triggers for four appli cations remote desktop requests rdp remote secure shell ssh ﬁle access requests smb and voice over ip calls sip voip applications using stubs to demonstrate how modest application stubs can enable signiﬁcant sleep mode operation in somniloquy we have also implemented application stubs for three applications that were popular in our informal survey background web download peer to peer content distribution using bittorrent and instant messaging for all these appli cations we did not have to modify the operating system or the existing applications on the pc which were only available to us in binaries to capture the state of the application for the respective stub we wrote wrappers around the binaries background web downloads we developed the web download stub for wget which works as follows when the host pc transitions to sleep the status of ac tive downloads is sent to the stub running on the gum stix the status includes the download url the offset of how much download has taken place the buffer space available and the credentials if required for the down load most popular web servers e g iis and apache allow these byte ranges to be speciﬁed using the http accept ranges primitives the web download stub then resumes the downloads from the respective off sets of the ﬁles and stores the data on the ﬂash storage of the gumstix if the ﬂash memory ﬁlls up before the downloads complete the stub wakes up the host pc and transfers the downloaded ﬁles from ﬂash storage to the host pc thereby freeing up space the host pc then goes back to sleep while the stub continues the downloads at the end of a download the gumstix wakes up the host pc and transfers the remaining part of the ﬁle the download stub consumes signiﬁcantly less energy to download a ﬁle than keeping the pc awake to down load it the overhead is a slight increase in latency we can quantify the savings and overhead using the model described in section if ﬂash storage is f mb and the download bandwidth is b mbps then the host pc is woken up every f b seconds and it is awake for f t seconds where t is the transfer rate between the host and the gumstix therefore using the formula in sec tion somniloquy gives most energy savings at low b and high t we empirically validate this observation in section when t is of the same order as b somniloquy might not save much energy this can hap pen if the nic supports very high rates e g gbps while the secondary processor can only support lower data rates up to mbps or if the transfer rate t is limited however we anticipate the download stub to be primarily used in scenarios where the download speeds are limited by the last mile connection of at most a few tens of mbps here this stub is nearly always beneﬁcial bittorrent for the bittorrent stub we customized a console based client ctorrent to run on the gumstix with a low cpu utilization and memory footprint prior to suspending to the host computer transfers the tor rent ﬁle and the portion of the ﬁle that has already been downloaded to the gumstix the bittorrent stub on the gumstix then resumes download of the torrent ﬁle and stores it temporarily on the sd ﬂash memory of the gum stix when the download completes the stub wakes up the host and transfers the ﬁle when only downloading content the energy saved by using this stub is similar to that of the web download stub i e frequency of waking up the pc and the duration for which it is woken up depends on the download band width b the transfer speed t and the ﬂash size f how ever when uploading sharing which is key to altruis tic applications the energy savings are much more the same ﬁle chunk can be uploaded to many peers and hence the pc can sleep for much longer implying more energy savings using the formula in section instant messaging for the im stub we used a console only im client called finch that supports many im protocols such as msn aol icq etc on the pc we used the corresponding gui version of the im client to ensure our goal of a low memory and cpu footprint we customized ﬁnch to include only the features salient to our aim of waking up the host processor when an in coming chat message arrives this only requires authen tication presence updates and notiﬁcations we disabled other functionality the host processor transfers over the authentication credentials for relevant im accounts be fore going to the gumstix then logs into the rele vant im servers and when an incoming message arrives it triggers wakeup the energy saved by the im stub is thus similar to applications that are handled using packet ﬁlters e g ssh rdp where the duration for which a host can sleep depends on the frequency of occurrence of wake up triggers system evaluation we present the beneﬁts of somniloquy in four steps first we show that gumstix consumes much less power than a pc by proﬁling standalone desktops laptops and the gumstix in different power states second we mea sure the energy saved and latency introduced by som niloquy when used on an idle host processor third we show how somniloquy affects the performance of vari ous applications with and without application stubs fi nally we quantify somniloquy energy savings mon etary and environmental cost for an enterprise and bat tery lifetime increase for laptops methodology to measure the power consumption of laptops and desktop pcs we used a commercially avail able mains power meter watts up to measure the power consumption of the standalone gumstix we built a usb extension cable with a mω sense resis tor which was inserted in series with the v supply line and we used this cable to connect the gumstix to the computer we calculated the power draw of the gumstix by measuring the voltage drop across the sense resistor all power numbers presented in this section are averaged across at least ﬁve runs table power consumption and suspend resume time for two desktops under various operating condi tions in all cases the processor is idle and the hard disk is spun down the power consumed by other peripherals such as displays is not included table power consumption and battery lifetime of three laptops under various operating conditions and the time to change power states microbenchmarks power latency desktops table presents the average power consump tion for two dell desktop machines an intel dual core ghz optiplex with gb ram run ning windows vista and a ghz pentium dimen sion with mb ram running windows xp the display is turned off in these experiments and only the essential system processes are left running the power consumption of the desktop in is two orders of mag nitude less than when it is awake this is consistent with prior published data on the power consumption of mod ern pcs we use the term base power to indicate the lowest power mode that a pc can be in and still be re sponsive to network trafﬁc without using somniloquy to get this number we further scaled down the cpu to the lowest permissible frequency on these desktops fur thermore we disabled the multi core functionality using the system bios to effectively use only one core and veriﬁed that the system was actually doing so by using a processor id utility supplied by intel the time taken for the desktops to resume from and reconnect to the network is of the order of a few seconds table table power consumption for the gumstix platform in various states of operation laptops table presents the average power con sumption of three popular laptops a lenovo tablet pc with gb ram running windows vista a toshiba laptop with gb ram running windows xp and a lenovo laptop with gb ram running windows vista for all power measurements the processor is set to the lowest speed and is idle the hard disk is spun down and the wireless network interface is powered on the base power is between w and w resulting in a bat tery lifetime of around to hours with the batteries that are present on these laptops using the sleep state can dramatically extend the battery lifetime to between and hours for the laptops we tested although the laptop is unreachable in this state gumstix table shows the average power con sumed by the gumstix with both etherstix and wiﬁstix in various states of operation the gumstix has a base power of approximately mw when no network in terface is present row a gumstix with an active net work interface typically consumes approximately mw rows and however with an associated wi fi interface in power save mode it consumes only mw row the power consumption of the gumstix when its network interface is active and the downloaded data is being written to ﬂash is around mw row broadcast and unicast storms continuous trafﬁc increase the power consumption by a few hundred milli importantly the power consumption of the gum stix is approximately one tenth that of an awake laptop in the lowest power state and approximately times less than an idle desktop fi broadcasts are sent at mbps while unicasts are sent at mbps in our setup consequently a unicast storm consumes more power than a broadcast storm figure power consumption and state transitions for our desktop testbed somniloquy in operation we now report the power consumption of somniloquy in operation for these measurements we use two testbed systems a desktop dell optiplex with gb ram running windows vista with the wired prototype of somniloquy and a laptop lenovo tablet pc run ning windows vista with the wireless version of somniloquy thus our tests span both ethernet and wi fi networks and both the integrated single network in terface and the higher performance versions which uses the existing internal network interface the test trafﬁc is generated using a standard desktop machine running on the same wireless or wired lan subnet as the testbed machine figure shows the power consumption of our desktop testbed initially the desktop host processor is awake and uses the gumstix for bridging and the whole sys tem draws w of power at time a a state change to is initiated by the user this request completes at time b after which the power draw of the system is approximately w i e less this power is split between the gumstix the dram of the pc and other power chain elements in the pc subsequently at time c the gumstix which has been actively monitoring the network interface wakes up the host in response to a net work event this request completes at time d when the host system has fully resumed as the ﬁgures illustrate this resume event takes about seconds we do not show the laptop ﬁgure for space reasons the trace looks very similar with a starting power of w with the screen on which drops to w if the screen is turned off a power draw of w when using somniloquy less than the screen off case and a resume time of seconds application performance as described earlier there are two classes of applications that are supported by somniloquy ﬁrst a large class of applications that do not require application stubs and second a smaller class of applications that can be sup figure application layer latency for three somniloquy testbeds and four application types ported using application stubs running on the gumstix we performed a number of experiments to evaluate the performance of both these classes of applications applications without stubs we now quantify the end to end latency as perceived by users incurred by the applications that are handled by somniloquy without using application stubs for these experiments we use the same two testbeds as above with the addition of a third testbed based on the wired prototype using same desktop machine as the wired case providing a direct comparison between the and cases in each case the latency reported is the mean over ﬁve test runs figure reports the time taken to satisfy an incoming application layer request for four sample applications for each application we show the latency for awake operation i e when the host is on and directly responds to the request and when the host is in and somnilo quy prototype receives the incoming packet and triggers wake up of the host the four applications we tested were remote desktop access rdp here we used a stop watch to measure the latency between initiating a remote desktop session to the host and the remote desktop be ing displayed a stopwatch was used to ensure that true user perceived latency was measured the gumstix was conﬁgured to wakeup the main processor on detecting tcp trafﬁc on port the rdp port remote directory listing smb a directory listing from the somniloquy testbed was requested by the tester machine via windows ﬁle sharing which is based on the smb protocol the time between the request being ini tiated and the listing being returned was measured using a simple script the secondary processor was conﬁgured to initiate wake up on detection of trafﬁc on either of the tcp ports used by smb i e ports and remote file copy smb the smb protocol was used again but this time to transfer a mb ﬁle from the somniloquy testbed to the tester machine voip call sip a voice over ip call was placed to a user who had been running a sip client on the som niloquy laptop before it had entered on receipt of the incoming call the sip server responded with a tcp connection to the testbed causing the gumstix to trig ger wakeup a similar procedure was used in once again the latencies were measured using a stopwatch to measure true user perceived delay as figure shows somniloquy adds between latency in all cases as described in section earlier part of this latency is attributed to resuming from i e for the desktop and for the laptop and is in dependent of somniloquy further latency is due to the delay for tcp to retransmit the request and for the host to respond to the request which may take longer since it has just resumed note that the wired proto type shows higher latency than the wired proto type this is purely an artifact of our prototype caused by the overhead of mac bridging and largely the slower speed of the usbnet ip link between the gumstix and the host the latter is particularly obvious in the ﬁle copy test where the ﬁle copy time with the wired case is much faster than for wired although the wired speed is still faster than wireless while somniloquy does result in additional application layer latency these delays are acceptable for real usage including voip in exchange for the substantial ben eﬁt of power savings applications requiring stubs in this section we present evaluations for applications that require stub support on the gumstix primarily look ing at the overhead in terms of memory consumption and processing capabilities that they impose on the gum stix we have implemented application stubs for three common applications background downloads using the http protocol ﬁle sharing using bittorrent and maintaining presence on im networks as described in section to study the overhead of im clients we run the cor responding application stub using up to three different im protocols simultaneously msn messenger aol messenger and icq chat table shows the processor utilization and memory footprint of the wired pro totype when running these im clients since the behav ior of the im stub is such that it maintains presence of the user on various networks and on receipt of an appro priate trigger im from someone wakes up the host the latency values are similar to those of the voip application table processor and memory utilization for the im stub for various configurations total memory for the gumstix is mb table processor and memory utilization for the bit torent stub for various configurations total memory for the gumstix is mb as reported in figure for our wired prototype the additional latency for the im stub when using som niloquy is around seven seconds to evaluate the overhead of ﬁle sharing using the bittorrent stub on the gumstix we initiated downloads using a torrent from a remote into the gb sd card of the wired gumstix we varied the mem ory cache available to the stub while conducting a single download and then tested two simultaneous downloads the results in table show that the memory footprint of the stub increases proportionally to the cache size as ex pected while the processor utilization remains constant when there are two simultaneous downloads each in stance of the stub uses memory proportional to its speci ﬁed mb cache finally to evaluate the web download stub on the gumstix we initiate download of a large mb ﬁle from a local web server we varied the throughput of the downloads and measured the processor utilization and the memory consumption of the gumstix and exper imented with two simultaneous downloads as shown in table the processor utilization increases as the down load rate increases although the memory footprint for each download remains constant the above results show that using application stubs we can support fairly complex tasks and applications in cluding background web downloads and ﬁle shar table processor and memory utilization for the web download stub for various configurations total memory for the gumstix is mb ing using relatively modest resources on the gumstix it is important to note that the power consumption of the gumstix did not exceed w in all of these experiments energy savings using somniloquy in addition to evaluating the operating performance of our somniloquy prototypes it also important to assess the higher level goal of this work namely the impact on pc energy consumption in this section we present some data which demonstrates the potential of somniloquy to reduce both desktop and laptop energy usage in general terms we also verify the energy saving model presented in section which allows the speciﬁc savings in a given application scenario to be calculated unless other wise noted we are using the wired version of our prototype for the desktop energy measurements and the wireless version for the laptop energy measure ments reducing desktop energy consumption our testbed desktop pc consumes w in normal op eration and w in with somniloquy somniloquy therefore saves around w on this basis if somnilo quy were to be deployed in an environment where a pc is actively used for an average of hours each week i e of the time this would result in kwh of savings per computer in a year assuming kg and us this means an annual saving of kg of to put it in perspective the av erage us residents annual emissions are metric tonnes as compared to a worldwide average of met ric tonnes per and us per computer we page html epa html htm figure power consumption and the resulting estimated battery lifetime of a lenovo using somniloquy the lifetime is calculated using the standard watt hour battery of the laptop believe this is signiﬁcantly higher than the bill of ma terials cost of the components required to implement a commoditized somniloquy enabled network card in this case deployments of somniloquy enabled devices would pay for themselves within a year desktop energy savings for real workloads we now estimate the energy savings enabled by somnil oquy under realistic workloads we use the data provided by relating to the use patterns of twenty two distinct desktop pcs each of which is classiﬁed as being either idle active sleep or turned off we then compute the energy consumed by each of the pcs with and without somniloquy using the formula of section for ease of exposition we bin the data into three different categories pcs that are idle for of the time machines idle for of the time machines and ﬁnally those that are idle for of the time machines the average energy savings for these twenty two pcs when using somniloquy is as compared to normal oper ation without somniloquy the average energy savings for the pcs in the individual categories are and respectively as expected the most energy sav ings are for the pcs with larger idle times since they have more opportunity to use somniloquy increasing laptop battery lifetime figure shows the average power consumption of the laptop testbed when operating normally i e no power saving mechanisms with standard power saving mech anisms in place the baseline power when somniloquy wireless is operational and in the standard mode without the gumstix attached somniloquy adds a relatively low overhead of mw to mode result ing in a total power consumption which is close to just figure comparing the analytical results with the mea sured values for the web download stub the flash stor age available on the gumstix is set to mb unless stated otherwise w as compared to the w of the idle laptop this means that when the laptop needs to be attached to the network and available for remote applications but is oth erwise idle it can be put into somniloquy mode to enable an order of magnitude decrease in power consumption and a resulting increase in battery lifetime from hours to hours using the standard watt hour battery energy savings for specific applications the basic analysis of energy consumption and battery lifetime presented above is very generic for a given us age scenario it should be possible to use the energy sav ing model presented in section to predict savings much more accurately in order to validate this model we ran experiments downloading content from a remote web server and measured both energy consumption and latency so as to compare them with their corresponding analytical values note that we only measure the energy consumption for the duration of the application the web download stub was chosen since it was rela tively easy to change the duty cycle of the host i e the duration for which the host can sleep tsleep after which it needs to be woken up to transfer data from the gumstix tawake as discussed in section tsleep depends on the download bandwidth and the amount of ﬂash storage on the gumstix while tawake depends on the amount of ﬂash storage on the gumstix and the transfer rate between the gumstix and the host we downloaded a mb ﬁle at various link bandwidths ranging from kbps to mbps and used two different ﬂash storage sizes at the gumstix mb and mb effectively varying tsleep from approximately seconds down to seconds we measured the power consumed during the download using the methodology described in the begin ning of this section in figure we present the measured energy savings and the corresponding predicted values using our model for four different data points as we can see from the ﬁgure the predicted energy savings and the increased latency closely match the measured values within the values do not exactly match since the actual measured power values vary over time and the time taken to suspend and resume also varies across runs we used a ﬁxed value for these in the formula figure also illustrates that increasing the bandwidth from kbps to mbps reduces the energy savings from to and increases the latency from to although a larger amount of ﬂash storage improves the energy saving and latency as explained earlier this is due to the limited transfer speed of the usbnet inter face in our prototype mbps because of which the pc is awake for longer periods of time while transfer ring the data from the gumstix tawake seconds to transfer mb of data in figure we have also plot ted an ideal case mbps ideal where the host can read the ﬂash storage of the gumstix directly for the ideal case the duration for which the host needs to stay awake to transfer data from the gumstix reduces considerably tawake seconds this improves energy savings to and limits the increase in latency when using som niloquy to less than related work there have been several proposals to reduce the en ergy consumption of desktop pcs and laptops prior work can largely be grouped in three categories re ducing the active power consumption of devices when awake reducing the power con sumption of the network infrastructure e g routers and switches and opportunistically putting the devices to sleep somniloquy falls in the third category since a machine in sleep state consumes signiﬁcantly less power than in lowest power active state ver iﬁed by us in section signiﬁcant energy savings are possible by putting the machine to sleep whenever pos sible for opportunistic sleep systems the biggest challenge is to ensure connectivity when the host is asleep prior techniques to solve this problem either use advanced functionality in the nic or use extra network in terfaces we now compare and contrast somnil oquy to both these classes of work among schemes that do not use an extra net work interface the most well known are wake on lan wol and its wireless equivalent wake on wlan wowlan in both these schemes the nic parses in coming packets when the host is asleep it wakes up the host pc whenever an incoming magic packet is re ceived according to the speciﬁcation the magic packet payload must include characters of a wakeup pattern that is set by the host pc followed by copies of the nic mac address in wowlan the only dif ference is that this packet is sent over the wireless lan although most modern nics implement wol function ality few deployed systems actually use this function ality due to four main reasons first the remote host must know that the pc is asleep and that it must wake it up before pursuing application functionality second the remote host must have a way of sending a packet to the sleeping pc through any ﬁrewalls nat boxes which typically do not allow incoming connections without spe cial conﬁguration third the remote host must know the mac address of the sleeping pc fourth wowlan does not work when laptops change their subnet because of mobility in contrast somniloquy does not require the extra conﬁguration of ﬁrewalls nat boxes and is trans parent to remote application servers it can handle mo bility across subnets since the secondary processor can re associate with services such as dynamic dns to redi rect a permanent host name to the pc new ip address and re log in to servers such as im servers in addition to these differences somniloquy also allows applications to be ofﬂoaded to the low power processor there is no such concept in wol which instead wakes up the host when any pattern is matched intel recently announced its remote wake chipset technology rwt that claims to extend wol on new motherboards by allowing voip calls to wake up a system although its general applicability to other appli cations is not known the details of this technology are not published in contrast somniloquy goes beyond just wol or rwt it allows low power operation for various applications other than voip furthermore somniloquy does not require modiﬁcations to application end points or servers rwt requires applications to ﬁrst contact a server which then sends a special packet to the pc to signal a wake up another approach is to use additional low power network interfaces to maintain connectivity to the pc that is asleep this approach has been proposed for use with mobile devices for example wake on wireless wakes up the host pc on receiving a special packet on the low power network interface turducken uses several tiers of network interfaces and processors with different power characteristics and wakes up the upper tier when the lower tier cannot handle a task in con trast to these schemes somniloquy requires only a single network interface and presents the paradigm of a single pc to users rather than a multi tiered system preserv ing the current user experience and therefore requiring less training to use somniloquy also gives the impres sion to remote application servers that a device remains awake all the time even though it is actually asleep since the same mac and ip addresses are used this level of transparency is not provided either by wake on wireless or turducken finally we have gone into more detail than previous work on ways of supporting applications that require interactions among the secondary and the host processor to perform ofﬂoad such as im bittor rent and web downloads to reduce the power consumed by desktop pcs some early proposals have suggested the use of proxies on the subnet that function on behalf of the desktop pc when it is asleep the proxy monitors incoming pack ets for the pc and wakes it up using wol when the pc needs to handle the packet we are not aware of any pub lished prototype implementations of such systems re cently sabhanatarajan et al propose a smart nic that can act as proxy for a host to save power how ever the authors focus primarily on the design of a high speed packet classiﬁer for such an interface in compar ison somniloquy has much wider applicability than the above schemes it can be used in homes and small ofﬁces where it might be infeasible to deploy a dedicated server to handle processing for another pc a contemporaneous effort to somniloquy is the idea of a network connection proxy ncp which is a network entity that maintains the presence of a sleep ing pc in the authors deﬁne the requirements of an ncp and propose modiﬁcations to the socket layer similar to split tcp for keeping tcp connections alive through a pc sleep transitions in the authors ex tend these apis to support other protocols as well som niloquy is similar in spirit to ncp and ncp socket apis can reduce somniloquy overhead when waking up from sleep section furthermore to the best of our knowledge somniloquy is the ﬁrst published proto type of any proxying system we note that the concept of adding more process ing to the network interface is not new existing prod ucts ofﬂoad processing to the nic to improve perfor mance tcp ofﬂoad and remote manageability in tel amt somniloquy uses a similar ofﬂoading paradigm but to conserve energy instead of improving performance or manageability conclusions we have presented somniloquy a system that augments network interfaces to allow pcs to be put into low power sleep states opportunistically without sacriﬁcing func tionality somniloquy enables several new energy sav ing opportunities first pcs can be put to sleep while maintaining network reachability without special net work infrastructure as needed by previous solutions e g wol second some applications can be run in sleep mode thereby requiring much less power in this paper we have shown the feasibility for three such applications to be run in sleep mode bittorrent instant messaging and web downloads somniloquy achieves these energy savings without re quiring any modiﬁcations to network to remote appli cation servers or to the user experience of the pc fur thermore somniloquy can be incrementally deployed on legacy network interfaces and does not rely on changes to the cpu scheduler or the memory manager to imple ment this functionality thus it is compatible with a wide class of machines and operating systems our prototype implementation based on a usb pe ripheral includes support for waking up the pc on net work events such as incoming ﬁle copy requests voip calls instant messages and remote desktop connections and we have also demonstrated that ﬁle sharing content distribution systems e g bittorrent web downloads can run in the augmented network interface allowing for ﬁle downloads to progress without the pc being awake our tests show power savings of are possible for desktop pcs left on when idle or for laptops for pcs that are left idle most of the time this translates to energy savings of to the electricity savings made are such that deploying a productized version of somniloquy could pay for itself within a year optimality fairness and robustness in speed scaling designs lachlan l h andrew centre for advanced internet architectures swinburne university of technology australia minghong lin adam wierman computer science department california institute of technology abstract system design must strike a balance between energy and per formance by carefully selecting the speed at which the sys tem will run in this work we examine fundamental trade offs incurred when designing a speed scaler to minimize a weighted sum of expected response time and energy use per job we prove that a popular dynamic speed scaling algo rithm is competitive for this objective and that no nat ural speed scaler can improve on this further we prove that energy proportional speed scaling works well across two common scheduling policies shortest remaining process ing time srpt and processor sharing ps third we show that under srpt and ps gated static speed scaling is nearly optimal when the mean workload is known but that dynamic speed scaling provides robustness against uncertain workloads finally we prove that speed scaling magnifies unfairness notably srpt bias against large jobs and the bias against short jobs in non preemptive policies however ps remains fair under speed scaling together these results show that the speed scalers studied here can achieve any two but only two of optimality fairness and robustness introduction computer systems must make a fundamental tradeoff be tween performance and energy usage the days of faster is better are gone energy usage can no longer be ignored in designs all the way from chips to mobile devices to data centers the importance of energy has led designs at all levels of systems to move toward speed scaling once a technique used primarily at the chip level speed scaling designs adapt the speed of the system so as to balance energy and perfor mance measures speed scaling designs can be highly so phisticated adapting the speed at all times to the current state dynamic speed scaling or very simple running at a static speed that is chosen to balance energy and per formance except when idle gated static speed scaling the growing adoption of speed scaling designs for systems from chips to disks to data centers has spurred analytic re search into the topic the analytic study of the speed scal ing problem began with yao et al in since permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee copyright acm x xxxxx xx x xx xx 00 three main performance objectives balancing energy and de lay have been considered i minimize the total energy used in order to meet job deadlines e g ii minimize the average response time given an energy power budget e g and iii minimize a linear combination of expected response time and energy usage per job in this work we focus on the third objective this objective captures how much reduction in response time is necessary to justify using an extra joule of energy and naturally applies to settings where there is a known monetary cost to extra delay e g many web applications fundamentally a speed scaling algorithm must make two decisions at each time i a scheduling policy must decide which job to service and ii a speed scaler must decide how fast to run the server it has been noted by prior work e g that an optimal speed scaling algorithm will use shortest remaining processing time srpt scheduling however in real systems it is often impossible to imple ment srpt since it requires exact knowledge of remaining sizes instead typical system designs often use scheduling that is closer to processor sharing ps e g web servers operating systems and routers in this work we focus on the design of speed scalers for both srpt and ps the study of speed scaling algorithms for these two policies is not new there has been significant prior work which we discuss in sections and studying speed scaling for srpt and for ps interestingly the prior work for srpt is entirely done using a worst case framework while the prior work for ps is done in a stochastic environment the m gi queue despite the considerable literature studying speed scaling there are many fundamental issues in the design of speed scaling algorithms that are not yet understood this paper provides new insights into four of these issues can a speed scaling algorithm be optimal what struc ture do near optimal algorithms have how does speed scaling interact with scheduling how important is the sophistication of the speed scaler what are the drawbacks of speed scaling to address these questions we study both ps and srpt scheduling under both dynamic and gated static speed scal ing algorithms our work provides i new results for dy namic speed scaling with srpt scheduling in the worst case model ii the first results for dynamic speed scaling with ps scheduling in the worst case model iii the first results for dynamic speed scaling with srpt scheduling in the stochas tic model iv the first results for gated static speed scaling with srpt in the stochastic model and v the first results identifying unfairness in speed scaling designs table sum marizes these these results lead to important new insights into issues i iv above we describe these insights informally here and provide pointers to the results in the body of the paper with respect to issue i our results show that energy proportional speed scaling provides near optimal perfor mance specifically we consider the algorithm which uses srpt scheduling and chooses sn the speed to run at given n jobs to satisfy p sn nβ where p is the power needed to run at speed and β is the cost of energy we prove that this algorithm is ε competitive under general p corollary this provides a tight analysis of an algorithm with a considerable literature e g see sec tion for a discussion it also gives analytic justification for a common heuristic applied by system designers e g further we show that no natural speed scaling algorithm definition can be better than competitive theorem which implies that no online energy proportional speed scaler can match the offline optimal with respect to issue ii our results uncover two new insights first we prove that at least with respect to ps and srpt speed scaling can be decoupled from the scheduler that is energy proportional speed scaling performs well for both srpt and ps and another policy laps studied in specifically we show that ps scheduling with speeds such that p sn n which are optimally competitive under srpt is again o competitive theorem further we show that using the speeds optimal for an m gi ps queue to control instead an m gi srpt queue leads to nearly optimal performance section second our results show that scheduling is not as important once energy is considered specifically ps is o competitive for the linear combina tion of energy and response time however when just mean response time is considered ps is ω competitive for in stances with ν jobs similarly we see in the stochastic environment that the performance under srpt and ps is almost indistinguishable e g figure together the in sights into issue ii provide a significant simplification of the design of speed scaling systems they suggest that practi tioners can separate two seemingly coupled design decisions and deal with each individually with respect to issue iii our results add support to an insight suggested by prior work prior work has shown that the optimal gated static speed scaling algorithm per forms nearly as well as the optimal dynamic speed scaling al gorithm in the m gi ps setting our results show that the same holds for srpt section thus sophistication does not provide significant performance improvements in speed scaling designs however sophistication provides improved robustness section to support this analytically we pro vide worst case guarantees on the near optimal stochastic speed scalers for ps and srpt corollary note that it is rare to be able to provide such guarantees for stochastic control policies the insights related to issue iii have an interesting practical implication instead of designing opti mal speeds it may be better to design optimally robust speeds since the main function of dynamic speed scaling is to provide robustness this represents a significant shift in approach for stochastic speed scaling design with respect to issue iv our results uncover one unin tended drawback of dynamic speed scaling speed scaling can magnify unfairness unfairness in speed scaling designs has not been identified previously but in retrospect the intuition behind it is clear if a job size is correlated with the oc cupancy of the system while it is in service then dynamic speed scaling will lead to differential service rates across job sizes and thus unfairness we prove that speed scaling mag nifies unfairness under srpt theorem and all non preemptive policies e g fcfs proposition in contrast ps is fair even with dynamic speed scaling proposition combining these results with our insights related to issue ii we see that designers can decouple the scheduler and the speed scaler when considering performance but should be wary about the interaction when considering fairness our results highlight the balancing act a speed scaling al gorithm must perform in order to achieve the three desirable properties we have discussed near optimal performance ro bustness and fairness it is possible to be near optimal and robust using srpt scheduling and dynamic speed scaling but this creates unfairness srpt can be fair and still near optimal if gated static speed scaling is used but this is not robust on the other hand dynamic speed scaling with ps can be fair and robust but in the worst case pays a signif icant performance penalty though in stochastic settings is near optimal thus the policies considered in this paper can achieve any two of near optimal fair and robust but not all three finally it is important to note that the analytic approach of this paper is distinctive it is unusual to treat both stochastic and worst case models in one paper and further many results depend on a combination of worst case and stochastic techniques which leads to insights that could not have been attained by focusing on one model alone model and notation we consider the joint problem of speed scaling and scheduling in a single server queue to minimize a linear com bination of expected response time also called sojourn time or flow time denoted by t and energy usage per job de noted e z e t e e β by little law this may be more conveniently expressed as λz e n e p β where n is the number of jobs in the system and p λ is the power expended before defining the speed scaling algorithms we need some notation let n t be the number of jobs in the system at time t and t be the speed that the system is running at at time t further define p as the power needed to run at speed then the energy used by time t is t measurements have shown that p can take on a variety of forms depending on the system being studied however in many applications a low order polynomial form provides a good approximation i e p ksα with α for example for dynamic power in cmos chips α is a good approximation however this polynomial form is not always appropriate wireless and other communication over an additive white gaussian noise channel have an expo nential power function while interference limited com munications has unbounded power at finite rate some of our results assume a polynomial form to make the analysis tractable and particularly α provides a simple example which we use for many of our numerical experiments other results hold for general even non convex and discontinuous power functions additionally we occasionally limit our re sults to regular power functions which are differentiable on strictly concave non negative and at speed table summary of the speed scaling schemes in this paper now we can define a speed scaling algorithm a speed scaling algorithm π σ is a pair of a scheduling dis cipline π that defines the order in which jobs are processed i under a given algorithm a as za i tj e i and a speed scaling rule σ that defines the speed as a func tion of system state in terms of the power function p in this paper we consider speed scaling rules where the speed is a function of the number of jobs in the system i e sn is the speed when the occupancy is n the scheduling algorithms π we consider are online and so are not aware of a job j until it arrives at time r j at which point π learns the size of the job xj we con sider a preempt resume model that is the scheduler may preempt a job and later restart it from the point it was in terrupted without any overhead the policies that we focus on are shortest remaining processing time srpt which preemptively serves the job with the least remaining work processor sharing ps which shares the service rate evenly among the jobs in the system at all times and first come first served fcfs which serves jobs in order of arrival the speed scaling rules sn we consider can be gated static which runs at a constant speed while the system is non idle and sleeps while the system is idle i e sn or more generally dynamic sn g n for some func tion g n note that the speed is simply the rate at which work is completed i e a job of size x served at speed will complete in time x to avoid confusion we occasionally write sπ as the speed under policy π when the occupancy is n we analyze the performance of speed scaling algorithms in two different models one worst case and one stochastic notation for the worst case model in the worst case model we consider finite arbitrary maybe adversarial instances of arriving jobs a problem instance consists of ν jobs with the jth job having arrival time re lease time r j and size work xj our objective is again a linear combination of response time and energy usage let i be the total energy used to complete instance i and tj be the response time of job j the completion time minus the release time the analog of is to replace the ensemble average by the sample average giving the cost of an instance that for some other objectives it is better to base the speed on the unfinished work instead ν j β in this model we compare the cost of speed scaling algo rithms to the cost of the optimal offline algorithm opt in particular we study the competitive ratio defined as cr sup za i zo i i where zo i is the optimal cost achievable on i notation for the stochastic model in the stochastic model we consider an m gi or some times gi gi queue with arrival rate λ let x denote a random job size with c d f f x c c d f f x and continu ous p d f f x let ρ λe x denote the load of arriving jobs note that ρ is not the utilization of the system and that many dynamic speed scaling algorithms are stable for all ρ when the power function is p sα it is natural to use a scaled load γ ρ βα which jointly characterizes the impact of ρ and β see denote the response time of a job of size x by t x we consider the performance metric where the expectations are averages per job in this model the goal is to optimize this cost for a specific workload ρ define the competitive ratio in the m gi model as cr sup za zo f λ where zo is the average cost of the optimal offline algorithm dynamic speed scaling we start by studying the most sophisticated speed scaling algorithms those that dynamically adjust the speed as a function of the queue length in this section we investigate the structure of the optimal speed scaling algorithm in two ways i we study near optimal speed scaling rules in the case of both srpt and ps scheduling ii we study each of these algorithms in both the worst case model and the stochastic model worst case analysis there has been significant work studying speed scaling in the worst case model following yao et al seminal pa per most of it focusing on srpt a promising algorithm that has emerged is srpt p n and there has been a significant stream of papers providing upper bounds on the competitive ratio of this algorithm for objective for unit size jobs in and for general jobs with p sα in a major breakthrough was made in which shows the competitiveness of srp t p n for general p our contribution to this literature is twofold first we tightly characterize the competitive ratio of srpt p nβ specifically we prove that srpt p nβ is exactly competitive under general power functions see theorem and corollary second we prove that no natural speed scaling algorithm can be better than competitive natural speed scaling algorithms include al gorithms which have speeds that grow faster slower or pro portional to p nβ or that use a scheduler that works on exactly one job between arrival departure events see defini tion thus the class of natural algorithms includes energy proportional designs for all schedulers and srpt scheduling for any sn we conjecture that this result can be extended tiable it does not increase running condition when φ is differentiable za t dφ czo t dt where za t and zo t are the cost z t under and op t respectively given these conditions the competitiveness follows from in tegrating which gives za za φ φ czo srpt analysis we now state and prove our results for srpt theorem for any regular power function p srpt p nβ has a competitive ratio of exactly the proof of the upper bound is a modification of the anal ysis in that accounts more carefully for some boundary cases it uses the potential function n q t in contrast to this stream of work studying srpt there has been no analysis of speed scaling under ps we prove that ps p nβ is o competitive and in particular is competitive for typical α i e α this builds on which studies laps another policy blind to job sizes laps p nβ is also o competitive for p sα with fixed α however for both ps and laps the competitive ratio is unbounded for large α which proves holds for all blind policies but note that α in most computer systems today e g disks chips and servers thus asymptotics in α are less important than the perfor mance for small α the results in this section highlight important insights about fundamental issues in speed scaling design first the competitive ratio results highlight that energy proportional speed scaling p sn nβ is nearly optimal which provides analytic justification of a common design heuristic e g second note that energy proportional speed scaling works well for ps and srpt and laps this suggests a designer may decouple the choice of a speed scaler from the choice of a scheduler choices that initially seem very intertwined though we have seen this decoupling only for ps srpt and laps we conjecture that it holds more generally third scheduling seems much less important in the speed scaling model than in the standard constant speed model for an instance of ν jobs ps is ω competitive for mean re sponse time in the constant speed model but is o competitive in the speed scaling model again we conjecture that this holds more generally than for just ps amortized competitive analysis the proofs of the results described above use a technique termed amortized local competitive analysis the technique works as follows to show that an algorithm is c competitive with an op timal algorithm op t for a performance metric z z t dt it is sufficient to find a potential function φ r r such that for any instance of the problem boundary condition φ before the first job is re leased and φ after the last job is finished jump condition at any point where φ is not differen for some non decreasing with i for i where n q t max na q t no q t with na q t and no q t the number of unfinished jobs at time t with remaining size at least q under the scheme under investigation and the optimal offline scheme respectively the following technical lemma is the key step of the proof and is proven in appendix a lemma let η and φ be given by with i η p p iβ let srp t sn with sn p nβ p ηnβ then at points where φ is differentiable na p sa β dφ η no p so β dt using the above lemma we can now prove theorem proof of theorem to show that the competitive ra tio of srpt p nβ is at most we show that φ given by and is a valid potential function the boundary conditions are satisfied since φ when there are no jobs in the system also φ is differentiable except when a job arrives or departs when a job arrives the change in na q equals that in no q for all q and so φ is unchanged when a job is completed n q is unchanged for all q and so φ is again unchanged the running condition is established by lemma with η to prove the lower bound on the competitive ratio con sider periodic unit work arrivals at rate λ sn for some n as the number of jobs that arrive grows large the optimal schedule runs at rate λ and maintains a queue of at most one packet the one in service giving a cost per job of at most p λ β λ in order to run at speed λ the schedule srpt p nβ requires n p λ β jobs in the queue giving a cost per job of p λ p λ λβ the competitive ratio is thus at least λ as λ becomes large this tends to since a regular p is unbounded theorem can easily be extended to non negative power functions by applying the same argument as used in corollary let ε for any non negative and un bounded p there exists a p such that emulating srpt p nβ yields a ε competitive algorithm this emulation involves avoiding speeds where p is not convex instead emulating such speeds by switching between a higher and lower speed on the convex hull of p corollary shows that srpt p nβ does not match the performance of the offline optimal this motivates con sidering other algorithms however we now show that no natural algorithm can do better consider a type b natural on ir n sn also sat isfies let to denote the time average speed for all φ for sufficiently long instances we need φsn to prevent an unbounded queue forming by jensen inequal ity the average cost per job satisfies z g p β g φsn p φsn β since φ can be arbitrarily close to the cost can be arbitrarily close to n p sn β whence holds for a type c natural a p sn n for large n thus for types a c such that for all n definition a speed scaling algorithm a is natural if p sn x it runs at speed sn when it has n unfinished jobs and for n nβ ε convex p one of the following holds the scheduler is work conserving and works on a single job between arrival departure events or g p β is convex for some g with g sn n or the speeds sn satisfy p sn ω n or the speeds sn satisfy p sn o n we now show that this condition precludes having a com petitive ratio of ε in the case of batch arrivals ib ν for ib ν the cost of any algorithm which does not serve the jobs one at a time can be lowered by reassigning the service instants so that it does so without loss of generality ν note that natural algorithms include all algorithms that use the optimal scheduler srpt and all algorithms whose z i b ν n n p sn βsn speeds grow faster than slower than or proportional to n α α f nβ α sα α p n theorem for any ε there is a regular power func n α α nβ tion pε such that any natural algorithm a on pε has com petitive ratio larger than ε the unique local minimum of α α α occurs at α this gives a minimum cost of this theorem highlights that if an algorithm does have a smaller competitive ratio than srpt p nβ it will not zo ib ν ν n n α α use natural scheduling or speed scaling though the result only applies to natural algorithms we conjecture that in fact it holds for all speed scaling algorithms and thus the competitive ratio of srpt p nβ is minimal proof consider the case when p sα with α yet to be determined we show that for large α the competitive ratio is at least ε by considering two cases instance ib ν is a batch arrival of ν jobs of size at time with no future α α α α for sn nβ α α more generally the optimum is βn snp sn p sn moreover for α ε the minimum subject to occurs when xn ε hence the competitive ratio for the batch case subject to satisfies arrivals and instance ir b λ is a batch of b jobs at time followed by a long train of periodic arrivals of jobs of size at times k λ for k n crbatch ν n ν n n α α α α α n α α α fix an ε and consider a speed scaling which can attain a competitive ratio of ε for all instances ir for ir λ with large λ the optimal algorithm will run at speed exceeding λ for a finite time until the occupancy is one after α α ε α ε that it will run at speed λ so that no queue forms for long trains this leads to a cost per job of p λ β λ first consider a type d natural for sufficiently large λ n ksα for all sn λ where k β between arrivals at least unit of work must be done at speed at least λ in order for not to fall behind the cost per unit work is at least ksα sα β and so the total cost of performing this unit is at least k β λα β for large λ this is at least twice the cost per job under the optimal scheme p λ β λ β it remains to consider natural algorithms of types a c consider a type a natural on the instance ir n sn for some n it will initially process exactly one job at speed sn which it will finish at time sn from this time a new arrival will occur whenever a job completes and so the algo rithm runs at speed sn with occupancy n until the last ar rival so the average cost per job tends to n p sn β sn on large instances leading to a competitive ratio of n for any ε the product of the last two factors tends to ε as α and hence there is an α α ε for which their product exceeds ε similarly for all α there is a sufficiently large ν that the first factor exceeds ε for this α and ν crbatch so for p sα ε if the competitive ratio is smaller than ε in the periodic case it must be larger than in the batch case theorem is pessimistic but note that the proof focuses on p sα for large α thus it is likely possible to design natural algorithms that can outperform srpt p nβ for α which is typical for computer systems today this is an interesting topic for future research ps analysis we now state and prove our bound on the competitive ratio of ps theorem if p sα then ps p nβ is p sn β crperiodic ε max α α competitive in particular ps is competitive for α in the typical range of for general α they theorem is proven using amortized local competitive α sn α min n σα γα γ α ness let η and γ η β function is then defined as the potential β sn α σ γ n α σ γ σ γ na t α α φ γ α max qa ji t qo ji t i where qπ j t is the remaining work on job j at time t under scheme π and j na t is an ordering of the jobs in increas proof bounds and are shown in addi tionally the concavity of sn follows from results in to prove note that when ρ the optimal speeds are those optimal for batch arrivals which satisfy by then it is straightforward from the dp that increases ing order of release time r r r jna t note that this is a scaling of the potential function that was used in to analyze laps as a result to prove theo rem we can use the corresponding results in to verify the boundary and jump conditions all that remains is the running condition which follows from the technical lemma below the proof is provided in appendix b lemma let φ be given by and a be the discipline ps sn with sn nβ ηnβ then under a at points where φ is differentiable monotonically with load ρ which gives interestingly the bounds in proposition are tight for large n and have a form similar to the form of the worst case speeds for srpt and ps in theorems and in contrast to the large body of work studying the opti mal speeds under ps scheduling there is no work charac terizing the optimal speeds under srpt scheduling this is not unexpected since the analysis of srpt in the static speed setting is significantly more involved than that of ps thus instead of analytically determining the optimal speeds for srpt we are left to use a heuristic approach a a α dφ o o α note that the speeds suggested by the worst case results n β c n dt β for srpt and ps theorems and are the same and the optimal speeds for a batch arrival are given by for both where c η max α α stochastic analysis we now study optimal dynamic speed scaling in the stochastic setting in contrast to the worst case results in the stochastic setting it is possible to optimize the algorithm for the expected workload in a real application it is clear that incorporating knowledge about the workload into the design can lead to improved performance of course the policies motivated by this and the fact that matches the asymptotic form of the stochastic results for ps in propo sition we propose to use the optimal ps speeds in the case of srpt to evaluate the performance of this heuristic we use simu lation experiments figure that compare the performance of this speed scaling algorithm to the following lower bound proposition in a gi gi queue with p sα drawback is that there is always uncertainty about workload information either due to time varying workloads measure zo max γα γα α α λ ment noise or simply model inaccuracies we discuss ro bustness to these factors in section and in the current section assume that exact workload information is known to the speed scaler and that the model is accurate in this setting there has been a substantial amount of work studying the m gi ps model this work is in the context of operations management and so focuses on operating costs rather than energy but the model struc ture is equivalent this series of work formulates the deter mination of the optimal speeds as a stochastic dynamic pro gramming dp problem and provides numeric techniques for determining the optimal speeds as well as proving that the optimal speeds are monotonic in the queue length the optimal speeds have been characterized as follows re call that γ ρ α proposition consider an m gi ps queue with controllable service rates sn let p sα the optimal dynamic speeds are concave and satisfy the dynamic program given in for α and any n they satisfy γ n γ n min γ work actually studies the m m fcfs queue but since the m gi ps queue with controllable service rates is this was proven in in the context of the m gi ps but the proof can easily be seen to hold more generally simulation experiments also allow us to study other inter esting topics such as i a comparison of the performance of the worst case schemes for srpt and ps with the stochastic schemes and ii a comparison of the performance of srpt and ps in the speed scaling model in these experiments the optimal speeds for ps in the stochastic model are found using the numeric algorithm for solving the dp described in and then these speeds are also used for srpt due to limited space we describe the results from only one of many settings we investigated figure shows that the optimal speeds from the dp dp have a similar form to the speeds motivated by the worst case results p nβ inv differing by γ for high queue occupancies figure shows how the total cost de pends on the choice of speeds and scheduler at low loads all schemes are indistinguishable at higher loads the per formance of the ps inv scheme degrades significantly but the srpt inv scheme maintains fairly good performance note though that if p sα for α the performance of srpt inv degrades significantly too in contrast the dp based schemes benefit significantly from having the slightly higher speeds chosen to optimize rather than minimize the competitive ratio finally the srpt dp scheme per forms nearly optimally which justifies the heuristic of using a symmetric discipline it has the same occupancy distri bution and mean delay as an m m fcfs queue the range of minimization was misstated as σ a b 80 n figure comparison of srpt and ps scheduling under both sn p nβ and speeds optimized for an m gi ps system using pareto job sizes and p the optimal speeds for ps in the case of however the ps dp scheme performs nearly as well as srpt dp to gether these observations suggest that it is important to figure comparison of sn p nβ with speeds dp optimized for an m gi system with γ and p gated static speed satisfies β de n r ds figure validation of the heavy traffic approx imation by simula tion using pareto job sizes with e x p optimize the speed scaler but not necessarily the scheduler gated static speed scaling section studied a sophisticated form of speed scaling where the speed can depend on the current occupancy this scheme can perform nearly optimally however its complex ity and overheads may be prohibitive this is in contrast to the simplest non trivial form gated static speed scaling where sn for some constant speed sgs this re quires minimal hardware to support e g a cmos chip may have a constant clock speed but and it with the gating sig where r ρ is the utilization and p sp p note that if p is convex then p is increasing and if p is bounded away from then p is unbounded under ps e n ρ ρ and so de n ds n ρ by the optimal speeds satisfy βe n r rp unfortunately in the case of srpt things are not as easy for it is well known e g that nal to set the speed to r r x dt λ r x τ df τ x gated static speed scaling can be arbitrarily bad in the worst case since jobs can arrive faster than sgs thus we e t x t λ r t τ df τ λ r x τ df τ df x study gated static speed scaling only in the stochastic model where the constant speed sgs can depend on the load we study the gated static speed scaling under srpt and ps scheduling the optimal gated static speed under ps has been derived in but the optimal speed under srpt has not been studied previously our results highlight two practical insights first we show that gated static speed scaling can provide nearly the same cost as the optimal dynamic policy in the stochastic model thus the simplest policy can nearly match the performance of the most sophisticated policy second we show that the performance of gated static under ps and srpt is not too different thus scheduling is much less important to optimize than in systems in which the speed is fixed in advance this reinforces what we observed for dynamic speed scaling optimal gated static speeds we now derive the optimal speed sgs which minimizes the expected cost of gated static in the stochastic model under both srpt and ps first note that since the power cost is constant at p sgs whenever the server is running the optimal speed is arg min βe t p pr n λ the complexity of this equation rules out calculating the speeds analytically so instead we use simpler forms for e n that are exact in asymptotically heavy or light traffic a heavy traffic approximation we state the heavy traffic results for distributions whose c c d f f has lower and upper matuszewska indices m and m intuitively f x as x for some so the matuszewska index can be thought of x be the fraction of work coming from jobs of size at most x the following was proven in proposition for an m gi under srpt with speed e n θ h ρ as ρ where e ρ g ρ if m proposition motivates the following heavy traffic approxi mation for the case when the speed is e n ch ρ where c is a constant dependent on the job size distribu tion for job sizes which are pareto a or more gener ally regularly varying with a it is known that c π a sin π a figure shows in the second term pr n ρ and so multiplying that in this case the heavy traffic results are accurate even by λ and setting the derivative to gives that the optimal that the peak around γ in fig b is most likely due to the looseness of the lower bound for quite low loads given approximation we can now return to equation and calculate the optimal speed for gated static srpt define h r g r g r a a b a b figure comparison for gated static ps using and srpt using with p a utilization given pareto job sizes b dependence of speed on the job size distribution for pareto a theorem suppose approximation holds with equality if m then for the optimal gated static speed βe n r rh r rp r if m then for the optimal gated static speed figure comparison of ps and srpt with gated static speeds and versus the dynamic speeds optimal for an m gi ps job sizes are dis tributed as pareto and p beyond heavy traffic let us next briefly consider the light traffic regime as ρ there is seldom more than one job in the system and srpt and ps have nearly indistinguishable e n so in this case it is appropriate to use speeds given by given the light and heavy traffic approximations we have just described it remains to decide the speed in the inter mediate regime we propose setting srp t p s srp t ht βe n r log r p sgs min sgs sgs where sp s satisfies and ssrp t ht is given by with proof for brevity we only prove the second claim if m then there is a c ce x such that gs gs e n estimated by to see why is reasonable we first show that often tends to the optimal speed as ρ e n c log for speed now de n c ρ c ρ proposition if m or both m and arbi trarily small jobs are possible i e for all x there is a y x with f y then produces the optimal scaling as ρ proof for ρ also r and e n r by e n ρ g and rh r by l hospital rule whence ρ log ρ and the factor in brackets is dominated by its second term in heavy traffic substituting this into gives the result to evaluate the speeds derived for heavy traffic fig ure b illustrates the gated static speeds derived for srpt and ps for p and ρ and varying job size dis tribution this suggests that the srpt speeds are nearly independent of the job size distribution note that the ver tical axis does not start from moreover the speeds of srpt and ps differ significantly in this setting since the also becomes β p from this is the optimal speed at which to server a batch of a single job since as ρ the system almost certainly has a single job when it is non empty this is an appropriate speed although tends to the optimal speeds over estimates e n for small ρ and so ssrp t ht is higher than optimal for small loads conversely for a given speed the delay is less under srpt than ps and so the optimal speed under srpt will be lower than that under ps hence ssrp t ht sp s in the large ρ regime where the former gs gs speeds under srpt are approximately minimal the speeds must be larger than γ while the ps speeds are γ theorem assumes that the system is in heavy traffic to understand when this holds first note that if there is a maximum allowable speed smax then the heavy traffic regime is valid as ρ smax in the case when there is no maximum allowable speed the following applies proposition if p is unbounded as and m m then as ρ induces the heavy traffic regime ρ we omit the proof of this result due to space concerns fig ure a illustrates the effect of raising the load on the uti lization in the case of p and pareto job sizes becomes accurate thus the min operation in selects the appropriate form in each regime gated static vs dynamic speed scaling now that we have derived the optimal gated static speeds we can contrast the performance of gated static with that of dynamic speed scaling this is a comparison of the most and least sophisticated forms of speed scaling as figure shows the performance in terms of mean delay plus mean energy of a well tuned gated static system is almost indistinguishable from that of the optimal dynamic speeds moreover there is little difference between the cost under ps gated and srpt gated again highlighting that the importance of scheduling in the speed scaling model is considerably less than in standard queueing models design 15 30 design 60 30 srpt ps a b c figure impact of misestimation of γ under ps and srpt cost when γ but sn were chosen for designed γ job sizes are pareto and p in addition to observing numerically that the gated static schemes are near optimal it is possible to provide some ana lytic support for this fact as well in it was proven that ps gated is within a factor of of ps dp when p combining this result with the competitive ratio results in this paper we have corollary consider p the optimal ps and srpt gated static designs are o competitive in an figure comparison of ps and srpt with linear speeds sn n β and with dynamic speeds optimal for ps job sizes are pareto and p by proposition sn nβ α α since α this implies sn p nβ further implies that sdp o α for any fixed ρ and β and is bounded for finite n hence the speeds sdp are of the form given in lemmas and for some finite η which may depend on π and the constant ρ from which it follows that is constant com petitive for α proposition implies sdp p nβ proof let π p s srp t and sπ be the optimal whence srpt sn is competitive gs gated static speed for π and sdp be the optimal speeds corollary highlights that sdp designed for a given ρ n which solve the dp for the m gi ps queue then leads to a speed scaler that is robust however the cost still degrades significantly when ρ is mispredicted badly as z π sπ p s sp s p s sdp shown in figure we now consider a different form of robustness if the ar the last two inequalities follow from and theorem as z p s sdp z p s p nβ in an m gi with known ρ rivals are known to be well approximated by a poisson pro cess but ρ is unknown is it possible to choose speeds that are close to optimal for all ρ it was shown in that using linear speeds sn n β gives near optimal performance robustness and speed scaling section shows that near optimal performance can be ob tained using the simplest form of speed scaling running at a static speed when not idle why then do cpu manufactur ers design chips with multiple speeds the reason is that the optimal gated static design depends intimately on the load ρ this cannot be known exactly in advance especially since workloads typically vary over time so an important prop erty of a speed scaling design is robustness to uncertainty in the workload ρ and f and to model inaccuracies figure illustrates that if a gated static design is used performance degrades dramatically when ρ is mispredicted if the static speed is chosen and the load is lower than ex pected excess energy will be used underestimating the load is even worse if the system has static speed and ρ then the cost is unbounded in contrast figure illustrates simulation experiments which show that dynamic speed scaling srpt dp is sig nificantly more robust to misprediction of the workload in fact we can prove this analytically by providing worst case guarantees for the srpt dp and ps dp let sdp denote the speeds used for srpt dp and ps dp note that the corollary below is distinctive in that it provides worst case guarantees for a stochastic control policy corollary consider p sα with α and algorithm which chooses speeds optimal for ps scheduling in an m gi queue with load ρ if uses either ps or srpt scheduling then is o competitive in the worst case model proof the proof applies lemmas and from the worst case model to the speeds from the stochastic model when p and ps scheduling is used this scheme per forms considerably better than using sn p nβ despite the fact that it also uses no knowledge of the workload given the decoupling of scheduling and speed scaling suggested by the results in section this motivates using the same linear speed scaling for srpt figure illustrates that this lin ear speed scaling provides near optimal performance under srpt too the robustness of this speed scaling is illustrated in figure however despite being more robust in the sense of this paragraph the linear scaling is not robust to inaccu racies in the model specifically it is not o competitive in general nor even for the case of batch arrivals fairness and speed scaling to this point we have seen that speed scaling has many benefits however we show in this section that dynamic speed scaling has an undesirable consequence magnifying unfair ness fairness is an important concern for system design in many applications and the importance of fairness when con sidering energy efficiency was recently raised in how ever unfairness under speed scaling designs has not previ ously been identified in retrospect though it is not a sur prising byproduct of speed scaling if there is some job type that is always served when the queue length is long short it will receive better worse performance than it would have in a system with a static speed to see that this magnifies un fairness rather than being independent of other biases note that the scheduler has greatest flexibility to select which job to serve when the queue is long and so jobs served at that time are likely to be those that already get better service in this section we prove that this service rate differen tial can lead to unfairness in a rigorous sense under srpt and non preemptive policies e g fcfs however under ps speed scaling does not lead to unfairness defining fairness the fairness of scheduling policies has recently received a lot of attention in computer systems modeling which has led to a variety of fairness measures e g and the analysis of nearly all common scheduling policies e g 30 40 50 x f x refer to the survey for more details here we compare fairness not between individual jobs but vs job size vs cdf of job size between classes of jobs where a class consists of all jobs of a given size since this paper focuses on delay we compare e t x across x for this purpose fairness when has been defined in prior work as follows definition a policy π is fair if for all x figure slowdown of large jobs under ps and srpt under pareto job sizes γ sn p n and p the intuition behind theorem is the following an infinitely sized job under srpt will receive almost all of e t π x x t p s x x its service while the system is empty of smaller jobs thus it receives service during the idle periods of the rest of the system further if ssrp t sp s then the busy periods will this metric is motivated by the fact that i ps is intuitively fair since it shares the server evenly among all jobs at all times ii the slowdown a k a stretch of ps is constant i e e t x x ρ iii e t x θ x so nor malizing by x when comparing the performance of different be longer under srpt and so the slowdown of the largest job will be strictly greater under srpt this intuition also provides an outline of the proof proof by lemma in appendix c t π x x π ρ a in each case job sizes is appropriate additional support is provided by lemma completes the proof by showing p s the fact that minπ maxx e t π x x ρ using this definition it is interesting to note that the class of large jobs is always treated fairly under all work conserving policies i e limx e t x x ρ even under policies such as srpt that seem biased against large jobs in contrast all non preemptive policies e g fcfs have been shown to be unfair to small jobs the foregoing applies when the following propo sition shows that ps still maintains a constant slowdown in the speed scaling environment and so definition is still a natural notion of fairness proposition 15 consider an m gi queue with a symmetric scheduling discipline e g ps with controllable service rates then e t x x e t e x the proof follows from using little law for jobs with size in x x e but is omitted for brevity speed scaling magnifies unfairness now that we have a natural criterion for fairness we prove that speed scaling creates magnifies unfairness under srpt and non preemptive policies such as fcfs srpt we first prove that srpt treats the largest jobs unfairly in a speed scaling system recall that the largest jobs are always treated fairly in the case of a static speed let π be the time average speed under policy π and let π denote running policy π on a system with a permanent customer in addition to the stochastic load theorem consider a gi gi queue with control lable service rates and unbounded interarrival times let srp t it considers the average speed between renewal instants in which both queues are empty which it maps to renewal periods it then uses lemma which shows that a busy period is longer under srpt than ps to show that less work is done on the permanent customer in the renewal period under srpt than under ps figure shows that unfairness under srpt can be consid erable with large jobs suffering a significant increase in slow down as compared to ps however in this case only around 10 of the jobs are worse off than under ps note that this setting has a moderate load which means that srpt with static speeds would be fair to all job sizes figure was generated by running a simulation to steady state and then injecting a job of size x into the system and measuring its response time this was repeated until the confidence intervals shown on figure a for srpt were tight around the estimate theorem proves that srpt cannot use dynamic speeds and provide fairness to large jobs however by using gated static speed scaling srpt can provide fairness e g further as figure illustrates gated static speed scaling provides nearly optimal cost so it is possible to be fair and near optimal using srpt scheduling but to be fair robust ness must be sacrificed non preemptive policies the magnification of unfairness by speed scaling also oc curs for all non preemptive policies in the static speed setting all non preemptive policies are unfair to small jobs since the response time must include at least the residual of the job size distribution if the server srp t n sn be weakly monotone increasing and satisfy is busy i e p s ρ and srp t ρ then t p s x t srp t x e t x x ρe x x which grows unboundedly as x however if we condi lim x x a lim x x tion on the arrival of a job to an empty system i e the work in system at arrival w then non preemptive that the conditions p s ρ and srp t ρ are equivalent to the stability conditions for ssrp t and sp s policies are fair in the sense that the slowdown is con stant t x w x speed scaling magnifies unfair ness under non preemptive policies in the following sense t x w x can now differ dramatically across job sizes proposition consider a non preemptive gi gi speed scaling queue with mean inter arrival time λ and speeds sn monotonically approaching as n then with probability main results of the work corollary providing worst case guarantees for policies designed in the stochastic model and theorem identifying unfairness in expected performance under dynamic speed scaling with srpt further the com bination of stochastic and worst case analysis adds support to many of the other insights of the paper e g the decou pling of scheduling and speed scaling lim t x w and lim t x w the results in this paper motivate many interesting topics x x x x for future work foremost it will be interesting to see if the lower bound of competitive for natural speed scaling algo the intuition behind this result is that small jobs receive their whole service while alone in the system whereas large jobs have a large queue build up behind them and there fore get served at a faster speed thus the service rate of large and small jobs differs magnifying the unfairness of non preemptive policies proof first the limit as x follows immediately from noting that as x shrinks the probability of another ar rival before completion goes to to prove the limit as x let a x be such that rithms can be extended to all algorithms additionally it is important to understand the range of applicability of the insights that speed scaling can be decoupled from schedul ing with little performance loss and that scheduling is less important when energy is added to the objective further the study of fairness in the context of speed scaling was only touched on briefly in this paper and there are many ques tions left to answer finally it is important to address all of the issues studied in this paper in the context of other per formance objectives e g when temperature is considered or a x a x si x si when more general combinations of energy and response time are considered and let e be arbitrary this a x can be thought of as the number of arrivals before x work is completed if jobs arrived periodically with interarrival time λ since speeds are non decreasing the time to finish the job can be bounded above by the time to reach speed si plus the time it would take to finish the whole job at speed si further we can use the law of large numbers to bound the time to reach speed si as x this gives t x w a x e since si are non decreasing and a x θ x it follows that the right hand side inside the brackets approaches as x conversely a lower bound on the time to finish the job is given by the time to finish it at maximum speed p r t x w w p together and establish the result in general speed scaling based on the occupancy n may magnify unfairness in any policy for which n t is correlated with the size of the job being processed at time t note that gated static scaling does not magnify unfairness regard less of the scheduling discipline since all jobs are processed at the same speed concluding remarks this paper has studied several fundamental questions about the design of speed scaling algorithms the focus has been on understanding the structure of the optimal algo rithm the interaction between speed scaling and scheduling and the impact of the sophistication of the speed scaler this has led to a number of new insights which are summarized in the introduction the analytic approach of this paper is distinctive in that it considers both worst case and stochastic models this com bination of techniques is fundamental in obtaining two of the speed scaling with an arbitrary power function nikhil bansal ho leung chan kirk pruhs what matters most to the computer design ers at google is not speed but power low power because data centers can consume as much electricity as a city dr eric schmidt ceo of google abstract all of the theoretical speed scaling research to date has assumed that the power function which expresses the power consumption p as a function of the processor speed is of the form p sα where α is some constant motivated in part by technological advances we initiate a study of speed scaling with arbitrary power functions we consider the problem of minimizing the total flow plus energy our main result is a ǫ competitive algorithm for this problem that holds for essentially any power function we also give a ǫ competitive algorithm for the objective of fractional weighted flow plus energy even for power functions of the form sα it was not previously known how to obtain competitiveness independent of α for these problems we also introduce a model of allowable speeds that generalizes all known models in the literature introduction energy consumption has become a key issue in the design of microprocessors major chip manufacturers such as intel amd and ibm now produce chips with dynamically scalable speeds and produce associated software that enables an operating system to manage power by scaling processor speed within the last few years there has been a significant amount of research on the scheduling problems that arise in this setting generally these problems have dual objectives as one wants both to optimize some schedule quality of service objective for example total flow and some power related objective for example the total energy used ibm t j watson research p o box yorktown heights ny max planck institut fu r informatik this paper was done when the author was in university of pittsburgh computer science department university of pittsburgh supported in part by an ibm faculty award and by nsf grants cns ccf iis and ccf scheduling algorithms for these problems have two components a job selection policy that determines which job to run and a speed scaling policy to determine the speed at which the processor is run all of the theoretical speed scaling research to date has assumed that the power function which expresses the power consumption p as a function of the processor speed is of the form p sα where α is some constant let us call this the traditional model the traditional model was motived by the fact that in cmos based processors the well known cube root rule states that the speed is approximately the cube root of the power so historically p was a reasonable assumption in the literature one finds different variations on this traditional model based on which speeds are allowable most of the literature assumes the unbounded speed model in which a processor can be run at any real speed in the range some of the literature assumes the bounded speed model in which the allowable speeds lie in some real interval t some of the literature on offline algorithms assumes the discrete speeds model in which there are a finite number of allowable speeds our main contribution in this paper is to initiate theoretical investigations into speed scaling problems with more general power functions and develop algo rithmic analysis techniques for this setting for an ex planation of the historical technological motivation for the traditional model and the current technological mo tivations for considering more general power functions see section a secondary contribution is to intro duce a model for allowable speeds that generalizes all of various models found in the literature we will consider the objective of minimizing a linear combination of total possibly weighted flow and total energy used our third contribution is to improve on the known results for this important fundamental problem optimizing a linear combination of energy and total flow has the following natural interpretation suppose that the user specifies how much improvement in flow call this amount ρ is necessary to justify spending one unit of energy for example the user might specify that he is willing to spend erg of energy from the battery for a decrease of micro seconds in flow then the optimal schedule from this user perspective is the schedule that optimizes ρ times the energy used plus the total flow by changing the units of either energy or time one may assume without loss of generality that ρ weighted flow generalizes both total flow and total average stretch which is another common qos measure the stretch slowdown of a job is the flow divided by the work of the job when the user is aware of the size of a job say if the user knows that he she is downloading a video file instead of a text file then perhaps slowdown is a more appropriate measure of the happiness of a user than flow many server is say completed at time t only contributes to the increase in fractional flow at time t they then showed that the natural algorithm proposed in is competitive for total flow plus energy for unit work jobs for the more general setting where jobs have arbi trary sizes and arbitrary weights and the objective is weighted flow plus energy bansal pruhs and stein considered the algorithm that uses highest density first hdf for job selection and always runs at a power equal to the fractional weight of the unfinished jobs systems such as operating systems and databases have they showed that this algorithm is o α competitive mechanisms that allow the user or the system to give different priorities to different jobs for example unix has the nice command in a speed scaling setting the for fractional weighted flow plus energy using an amor tized local competitiveness argument using the known resource augmentation analysis of hdf they then weight of a job is indicative of the flow versus energy showed how to obtain an o competitive algo trade off for this job the user may be willing to spend more energy to reduce the flow of a higher priority job than for a lower priority job the literature on flow energy in the traditional model let us start with results in the unbounded speed model pruhs uthaisombut and woeginger gave an efficient offline algorithm to find the schedule that minimizes average flow subject to a constraint on the amount of energy used in the case that jobs have unit work this algorithm can also be used to find optimal schedules when the objective is a linear combination of total flow and energy used they observed that in any locally optimal schedule essentially each job i is run at a power proportional to the number of jobs that would be delayed if job i was delayed albers and fujiwara proposed the natural online speed scaling algorithm that always runs at a power equal to the number of unfinished jobs which is lower bound to the number of jobs that would be delayed if the selected job was delayed they did not actually analyze this natural algorithm but rather analyzed a batched variation in which jobs that are released while the current batch is running are ignored until the current batch finishes they showed that for unit work jobs that this batched algorithm is o competitive by reasoning directly about the optimal schedule this gave a competitive ratio of about when the cube root rule holds they also gave an efficient offline dynamic programming algorithm bansal pruhs and stein considered the algorithm that runs at a power equal to the unfinished work which is in general a bit less than the number of unfinished jobs for unit work jobs they showed that for unit work jobs this algorithm is competitive with respect to the objective of fractional flow plus energy using an amortized local competitiveness argument a job that rithm for integral weighted flow plus energy the com petitive ratio was a bit less than when the cube root rule holds recently lam et al improved the competitive ratio for total flow plus energy for arbitrary work and unit weight jobs they considered the job selection algo rithm shortest remaining processing time srpt and the speed scaling algorithm of running at a power pro portional to the number of unfinished jobs and proved that this is o α competitive for flow time plus en ergy when the cube root rule holds this competitive ratio was about their improvement came by rea soning directly about integral flow time instead of argu ing first about fractional flow time speed scaling pa pers prior to used potential functions related to the fractional amount of unfinished work or weight the main reason for this was that such a potential func tion varies continuously with time which substantially simplifies the proofs as one only needs to consider in stantaneous states of the online and offline algorithms the main technical contribution of was the intro duction of a different form of continuously varying po tential function that depends on the integral number of unfinished jobs bansal et al extended the results of for the unbounded speed model to the bounded speed model the speed scaling algorithm was to run at the minimum of the speed recommended by the speed scaling algorithm in the unbounded speed model and the maximum speed of the processor the contribution there was to develop the algorithmic analysis techniques necessary to analyze this algorithm the results for the bounded speed model in were improved in again by reasoning directly about integral flow again showed competitive ratios of o α when the cube root rule holds the obtained competitive ratio was our results we assume that the allowable sα our new potential function not only allows us to speeds are a countable collection of disjoint subintervals break the barrier of α and it allows us to obtain o of we assume that all the intervals except possi bly the rightmost interval are closed on both ends the rightmost interval may be open on the right if the power p approaches infinity as the speed approaches the rightmost endpoint of that interval we assume that p is non negative and p is continuous and differentiable on all but countably many points we assume that ei ther there is a maximum allowable speed t or that the limit inferior of p as approaches infinity is not zero if this condition doesn t hold then then the opti mal speed scaling policy is to run at infinite speed let us call this the general model we give two main results in the general model theorem consider the scheduling algorithm that uses shortest remaining processing time srpt for job selection and power equal to one more than the number of unfinished jobs for speed scaling in the general model this scheduling algorithm is ǫ competitive for the objective of total flow plus energy on arbitrary work unit weight jobs theorem consider the scheduling algorithm that uses highest density first hdf for job selection and power equal to the fractional weight of the unfinished jobs for speed scaling in the general model this schedul ing algorithm is ǫ competitive for the objective of fractional weighted flow plus energy on arbitrary work arbitrary weight jobs we establish these results through an amortized local competitiveness argument as in our potential function is based on the integral number of unfinished jobs however our potential function is quite different and more general than the potential function considered in while theorem deals with integral flow theorem only holds for fractional weighted flow obtaining a competitive ratio independent of α for the objective of integral weighted flow plus energy is ruled out since resource augmentation is required to achieve o competitiveness for the objective of weighted flow on a fixed speed processor let us consider what these theorems say in the traditional model theorem slightly improves the best known competitive ratios when the cube root rule holds in both the unbounded and bounded speed models the potential functions used in all previous papers are specifically tailored toward the function p sα moreover these potential functions can competitiveness for general power functions technological motivations the power used by a processor can be partitioned into dynamic power the power used by switching when doing computations and static leakage power which is the power lost in absence of any switching historically say up until years ago the static power used by a processor was negligible com pared to the dynamic power dynamic power is roughly proportional to sv where v is the voltage however as the minimum voltage required to drive the micropro cessor at a desired speed is approximately proportional to the frequency this leads to the well known cube root rule that the speed is roughly proportional to the cube root of the power p or equivalently p the power is proportional to the speed cubed however currently the static power used by the common processors manufactured by intel and amd is now comparable to the dynamic power the reason for this is that to increase processor speeds transistor sizes and desired transistor switching delays must de crease to obtain a smaller switching delay the thresh old voltage for the transistor must be decreased un fortunately subthreshold leakage current increases ex ponentially as threshold voltage decreases this suggests modeling the speed to power function as some thing like p sα c where there is some range of speeds smin smax that the processor can run at here α comes from the cube root rule for dynamic power and c is a constant specifying the static power loss there is however good motivation for considering more general speed to power functions we will state three more examples here example in general the higher speed that one can scale a core or a processor the greater the static power because the threshold voltage must be less it has been proposed that it might be advantageous to build a processor consisting of different circuitry for running jobs at different speeds speed ranges each circuitry would have different speed ranges different static powers due to the different threshold voltages and different power functions thus the speed scaling algorithm might be able to choose among a collection of power functions of the form pi aisα ci where each pi is applicable over a collection si si si ki of speeds note that in this context that we can not scale our units so that the multiplicative not be used to show competitive ratios of o α for constant is always in this setting the power function arbitrary work jobs so while the competitive ratios were o competitive for a fixed α they were not o competitive for a general power function of the form of might be of the form p min aisα ci i si example subthreshold leakage current power is not independent of temperature the temperature in turn depends on the speed that the processor is run some discussion of how leakage current power is related to temperature can be found in but it seems that this issue is perhaps not so well understood but in any case there appears to be no reason to presume that the resulting power function would be of the form sα example another motivation for considering gen eral speed to power functions is at the data center level the opening quote indicates the importance of power management at the data center level a nice investi gation by google researchers into the energy that a data center could save from speed scaling can be found in ultimately what a data center operator might care about is the cost of the energy used not the actual amount of energy used these can be quite different because normally the contract that the data center has with the electrical suppliers can have drastic increases in cost if various power thresholds are exceeded so here the relevant function that one would care about is the speed to cost say measured in dollars per second function not the speed to power function there is no reason why these contracts need to have a cost that rises as sα preliminaries an instance consists of n jobs where job i has a release time ri and a positive work yi in some cases each job may have a positive integer weight wi an online scheduler is not aware of job i until time ri and at time ri learns yi and weight wi for each time a scheduler specifies a job to be run and a speed at which the processor is run we assume that preemption is allowed that is a job may be suspended and later restarted from the point of suspension a job i completes once yi units of work have been performed on i the speed is the rate at which work is completed a job with work y run at a constant speed completes in y seconds the flow fi of a job i is its completion time minus its release time the φ t such that the following two conditions hold boundary condition φ is zero before any job is released and φ is non negative after all jobs are finished general condition at any time t dφ t ga t dt c gopt t to prove the general condition it suffices to show that φ does not increase when a job arrives or is completed by a or opt and equation is true during any period of time without job arrival or completion by integrating equation we can see that a is c competitive for h for more information see flow plus energy our goal in this section is to prove theorem that in the general model srpt plus the natural speed scaling algorithm is ǫ competitive for the objective of total flow plus energy we start by showing that this algorithm is competitive if p also satisfies the following additional special conditions p is defined and continuous and differentiable at all speeds in p p is strictly increasing p is strictly convex that is for a b and for all p is the case that pp a p p b p pa p b p is unbounded that is for all c the exists a speed such that p c then in section we show how to extend the analysis to the case when these special conditions do not hold at the cost of an arbitrarily small increase in the competitive ratio definition of online algorithm a algorithm a n n total flow is i fi the weighted flow is i wi fi let us quickly review amortized local competitive ness analysis consider an objective g let ga t be schedules the unfinished job with the least remaining unfinished work and runs at speed st where t t the increase in the objective in the schedule for algo rithm a at time t so when g is total flow plus energy ga t is p st nt where st is the speed for a at t p na if na a if nt a a a time t and nt is the number of unfinished jobs for a at where nt is the number of unfinished jobs for a at time t this is because energy is power integrated over time and total flow is the number of unfinished jobs integrated over time let opt be the offline adversary that optimizes g and a be some algorithm to prove a is c competitive it suffices to give a potential function time t as p and p is strictly increasing continuous and unbounded p i exists and is unique for all integers i thus a is well defined definition of potential function φ let opt be the offline adversary that minimizes total flow plus energy we can assume without loss of generality that opt runs srpt at any time t let nt be the number the first equality is obtained by integration by parts and of unfinished jobs in opt let nt q and nt q be the second equality is obtained by letting y g x a o the number of unfinished jobs with remaining size at least q in a and opt respectively at time t let nt q max nt q nt q we define the the potential function φ t f nt q dq where f is defined by f i f i f i p p i and p x is the derivative of p x note that p p i simply means substituting x p i into p x since p x is differentiable p x is well defined hence f is well defined for all integer i as both p x and p i are increasing we have f i f i f j f j if i j we denote i f i f i for simplicity we now wish to establish a crucial technical lemma lemma which relies on the well known young inequality theorem young inequality let g be a real valued continuous and strictly increasing function on c with c if g and a b such that a c and b g g c then g x dx g x dx ab g where g is the inverse function of g lemma let p be a strictly increasing strictly convex continuous and differentiable function let i sa so be any real then p p i sa so sa p i p p i p so i the lemma follows by adding sap p i to both sides we are now ready to proceed with our amortized local competitiveness argument for the boundary condition we observe that before any job is released and after all jobs are finished nt q for all q so φ for the general condition we observe that when a job is released nt q is not changed for all q so φ is unchanged when a job is completed by a or opt nt q is changed only at the single point of q which does not affect the integration so φ is also unchanged it remains to show at any time t during a time interval without job arrival or completion nt p st d φ t nt p st a a dt o o the rest of this proof considers any such time t and proves equation we omit t from the superscript and the parameter for convenience first observe that if na then n q for all q and d φ thus equation trivially holds henceforth we assume na let qa be the remaining size of the job with the shortest remaining size in a qa exists as na note that a is processing a job of size qa define qo similarly for opt qo if no consider an infinitesimal interval of time t t dt the processing of a causes na q to decrease by for q qa sadt qa similarly the processing of opt causes no q to decrease by for q qo sodt qo denote the change in φ during this time interval as dφ we consider three cases depending on whether no na no na or no na case no na we show that dφ as follows at time t no q is greater than na q for q in qo sodt qo thus at time t dt n q is still at least n q for q in o a proof since p is strictly increasing and strictly convex p and p x is strictly increasing thus by young inequality with g x p x a so and b p p i we have sop p i so p p i qo sodt qo it means that n q remains zero in this interval and φ is not increased due to the processing of opt the processing of a only decreases φ or leaves φ unchanged hence dφ thus implies that d φ and na p sa d φ na na so holds g x dx p g x dx g p i case no na we show that either dφ na no sa so dt or dφ na no sa so dt as follows consider subcases depending on whether p so xg x lg i g p i xd g x qa is smaller than equal to or bigger than qo assume qa qo as na q decreases by for q in p so g p w p i p so g p i p i i g y dy qa sadt qa by the definition of φ the change in φ due to a is f na qa no qa f na qa no qa sadt na qa no qa sadt since qa qo we have no qa no qo no so the change in φ due to a is na no sadt where the last inequality follows from sa p na p na no similarly na no sa so as n q decreases by for q in q dt q by p p na no sa so setting i na no o o o o the definition of φ the change in φ due to opt in lemma we have that is at most f na qo no qo f na qo no qo sodt na qo no qo sodt since p p na no sa so q q we have n q n q thus sa p na no p p na no a o a o a a na qo no qo na qa no qo as i j for i j the change in φ due to opt is at most na qa no qo sodt na no sodt adding up the change in φ due to a and opt we obtain that dφ na no sa so dt p so na no p so na no where the last inequality follows from sa p na p na no thus d φ so assume qa qo if sa so n q na q no q decreases by for q in qa sadt qa sodt hence the change in φ is f na qa no qa f na qa no qa sa so dt na no sa so dt else if sa so n q na q no q increases by for q in qa sodt qa sadt hence the change in φ is f na qa no qa f na qa no qa so sa dt na sa so dt assume qa qo as na q decreases by for q in qa sadt qa the change in φ due to a is f na qa no qa f na qa no qa sadt na qa no qa sadt since qa qo we have no qa no qo thus na qa no qa na qa no qo as i j for i j the change in φ due to a is at most na qa no qo sadt na no sadt on the other hand no q decreases by for q in qo sodt qo so the change in φ due to opt is at most f na qo no qo f na qo no qo sodt na qo no qo sodt since qa qo na qo na qa na and the change in φ due to opt is at most na no sodt summing the change in φ due to a and opt dφ na no sa so dt combining the above subcases it implies that if no na then either d φ na sa so or dt d φ na sa so dt note that na no sa so p p na no sa so setting i na no in lemma we have that p p na no sa so sa p na no p p na no p so na no p so na no in both cases it follows that d na p sa dt φ na na so p so no so equation is true if no na case no na we show that dφ or dφ na no sa so dt as follows again we consider subcases depending whether qa qo qa qo or qa qo assume qa qo then n q remains zero for q qa sadt qa and q qo sodt qo thus n q is not changed for any q and d φ assume qa qo if sa so n q remains zero for q qa sadt qa which also contains qo sodt qo thus n q is unchanged for any q and d φ else if sa so n q increases by for q qa sodt qa sadt the change in φ is f na qa no qo f na qa no qo sa so dt na no sa so dt if qa qo it is identical to subcase of the case no na so dφ na no sa so dt it implies that d φ or d φ na no sa so dt similar argument as before shows that equation is true if no na this completes the analysis when p satisfies the special conditions that we imposed at the start of this section removing the special conditions on p con sider an arbitrary power function p in the general model we explain how to modify p so that it meets the special conditions without significantly raising the com petitive ratio if p is not increasing one can make p undefined on those speeds where there is a greater speed that consumes less power similarly we can make p convex by eliminating any points in a subinterval a b time t is st p wt a a of speeds where the line segment between a p a and b p b lies below the curve p we now argue that the online algorithm a with the new power function can simulate running a speed a b in the old power function algorithm a can simulate any speed a b by alternately running at speeds a and b specifically assume pa p b for some p we can run at speed a for a p fraction of the time and run at speed b for a p fraction of the time it gives an effective speed of and the energy usage is pp a p p b if the slowest speed on which p is defined is not then can shift p down by redefining where wt is the total fractional weight of all unfinished jobs for a at time t definition of the potential function γ let opt be the offline adversary that minimizes fractional weighted flow plus energy at any time t let wt be the total fractional weight of all unfinished jobs in opt for a job j its inverse density is defined to be the ratio of its original size divided by its original weight at any time t let wt m denote the total fractional weight of all unfinished jobs with inverse density at least m in a at time t define wt m similarly for that of opt let wt m max wt m wt m we define p as p p p and p for then it is easy to see that if a is c competitive with respect to this new p then a is also be c competitive with respect to the original p if p is defined at a and a γ t o h wt m dm b but not at any point in the interval a b then in terpolate p linearly between a and b once again the online algorithm can simulate running a speed a b in the new power function by alternately running at speeds a and b p stays convex by the convexity of the original p one can find a power function between p where h is the function such that for any real w d h w p p w dw since both p x and p are increasing p p w is an increasing of function in w furthermore h w and p max ǫ ǫp that is defined on the same domain w of speeds is continuous differentiable strictly increas ing and strictly convex the competitive ratio of a for the old power function will be within ǫ of the com petitive ratio of a for the new power function if p is bounded where t is the maximum speed then the nat ural interpretation of the algorithm is to run at speed min p na t we can follow the line of reason ing from we first note that at all times it must be the case that na no p t then the anal ysis essentially then goes through as in the unbounded case in particular in the case that no na we arrive at the same inequality of dφ sa p na no p p na no na no if sa p na then we have that sa p na no as before else if sa t we still have that sa p na no fractional weighted flow plus energy our goal in this section is to prove theorem that can be written as h w p p y dy we now start the amortized local competitiveness analysis for the boundary condition we observe that before any job is released and after all jobs are completed wt m for all m so γ for the general condition when a job is released wt m is unchanged for all m so γ is unchanged when a job is processed by a or opt the fractional weight of the job decreases continuously to zero so γ is continuous and does not decrease due to the completion of a job it remains to show that at any time t during a time interval without job arrival or completion wt p st d γ t wt p st a a dt o o the rest of this proof considers any such time t and proves equation we omit t from the superscript and the parameter for convenience then in the general model hdf plus the natural speed scaling algorithm is ǫ competitive for the objective of fractional weighted flow plus energy we start by showing using an amortized local competitiveness argument that this algorithm is competitive if p also satisfies the special conditions from the last section definition of online algorithm a the algorithm always runs the job of highest density the density of a job is its weight divided by its work the speed st at d γ d h w m dm dt dt d h w m d w m dm d w m dt p p w m d w m dm dt where the second equality follows from chain rule let ma and mo denote the minimum inverse density of an unfinished job in a and opt respectively let ma resp mo be if a resp opt has no unfinished job note that ma and mo are the density of the highest density job in a and opt respectively since case wo wa for m mo wo m is decreas ing at a rate of so mo which causes γ to increase at a rate at most mo p p w m so dm for mo a is running hdf at speed sa wa m is decreasing at the rate of sa ma for all m ma and wa m remains unchanged for m m similarly w m is m mo w m max wa m wo m wa wo so p p w m p p and mo p p w m so dm p p by a o mo o decreasing at the rate of so mo for m mo and remains unchanged for m mo we consider three cases depending on wo wa wo wa or wo wa case wo wa we show that d γ in this case note that for any m mo wo m wo wa wa m thus for any m mo w m max wa m wo m remains zero and does not change it means that d w m for m mo for any m mo wo m remains unchanged so d w m for m mo therefore d γ lemma with i sa we obtain that d γ p so so we have wa p sa d γ so wo p so hence proving equa tion this completes the analysis when the special con ditions hold the proof can be extended to the general model in the same way as was done in the proof of the orem dt dt r p p w m d w m dm we have wa p sa d γ wo p so so equation is true case wo wa the decrease in wa m causes a de crease in γ and the decrease in wo m causes an in crease in γ we analyze these two effects separately and bound them appropriately for any m ma wa m is decreasing at a rate of sa ma which causes γ to ma decrease at the rate of p p w m sa dm ma for m ma w m wa m wo m wa wo so p p w m p p wa wo thus ma p p w m sa dm ma ma r p p w w sa dm a o ma p wa wo sa for m mo wo m is decreasing at a rate of so mo which causes γ to increase at a rate at most mo p p w m so dm for m m mo o w m max wa m wo m wa wo so p p w m p p wa wo thus mo p p w m so dm mo mo r p p w w so dm a o mo p wa wo so summing the above two terms we have d γ p wa wo sa so by lemma it is at most sa p wa wo p p wa wo p so wa wo since sa p wa p wa wo d γ p so wa wo we have wa p sa d γ p so wa wo p so wo hence proving equation speed scaling to manage energy and temperature nikhil bansal and tracy kimbrel ibm t j watson research center and kirk pruhs university of pittsburgh abstract speed scaling is a power management technique that involves dynamically changing the speed of a processor we study policies for setting the speed of the processor for both of the goals of minimizing the energy used and the maximum temperature attained the theoretical study of speed scaling policies to manage energy was initiated in a seminal paper by yao et al and we adopt their setting we assume that the power required to run at speed is p sα for some constant α we assume a collection of tasks each with a release time a deadline and an arbitrary amount of work that must be done between the release time and the deadline yao et al gave an offline greedy algorithm yds to compute the minimum energy schedule they further proposed two online algorithms average rate avr and optimal available oa and showed that avr is competitive with respect to energy we provide a tight αα bound on the competitive ratio of oa with respect to energy we initiate the study of speed scaling to manage temperature we assume that the environment has a fixed ambient temperature and that the device cools according to newton law of cooling we observe that the maximum temperature can be approximated within a factor of two by the maximum energy used over any interval of length b where b is the cooling parameter of the device we define a speed scaling policy to be cooling oblivious if it is simultaneously constant competitive with respect to temperature for all cooling parameters we then observe that cooling oblivious algorithms are also constant competitive with respect to energy maximum speed and maximum power we show that yds is a cooling oblivious algorithm in contrast we show that the online algorithms oa and avr are not cooling oblivious we then propose a new online al gorithm that we call bkp we show that bkp is cooling oblivious we further show that bkp is e competitive with respect to the maximum speed and that no deterministic online algorithm can have a better competitive ratio bkp also has a lower competitive ratio for energy than oa for α k pruhs was supported in part by nsf grants ccr cns cns ccf ccf and iis authors addresses n bansal and t kimbrel ibm t j watson research center po box york town heights ny e mail nikhil kimbrel us ibm com k pruhs department of computer science south bouquet street sennott square building room university of pittsburgh pittsburgh pa e mail permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or direct commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation copyrights for components of this work owned by others than acm must be honored abstracting with credit is permitted to copy otherwise to republish to post on servers to redistribute to lists or to use any component of this work in other works requires prior specific permission and or a fee permissions may be requested from publications dept acm inc penn plaza suite new york ny usa fax or qc acm doi finally we show that the optimal temperature schedule can be computed offline in polynomial time using the ellipsoid algorithm categories and subject descriptors c performance of systems performance attributes f analysis of algorithms and problem complexity nonnumerical algorithms and problems sequencing and scheduling general terms algorithms performance additional key words and phrases speed scaling voltage scaling power management acm reference format bansal n kimbrel t and pruhs k speed scaling to manage energy and temper ature j acm article march pages doi 1206038 introduction motivation the energy consumption rate of computing devices has in creased exponentially for several decades since the early power densities in microprocessors have doubled every three years skadron et al this increased power usage poses two types of difficulties energy consumption as energy is power integrated over time supplying the required energy may become prohibitively expensive or even technologically in feasible this is a particular difficulty in devices that rely on batteries for energy and will will become even more critical since battery capacities are increasing at a much slower rate than energy consumption temperature the energy used in computing devices is in large part converted into heat cooling costs are rising exponentially along with energy consumption and threaten the computer industry ability to deploy new systems skadron et al in fact in may intel publicly acknowledged that it had hit a thermal wall on its microprocessor line intel scrapped the development of its tejas and jayhawk chips in order to rush to the marketplace a more power efficient chip technology designers said the escalating heat problems were so severe that they threatened to cause chips to fracture markoff apple was unable to develop a laptop due to inadequate heat management in the ibm powerpc chips and in the summer of apple announced that it was switching to cooler intel chips apple these factors have resulted in power becoming a first class design constraint for modern computing devices mudge there is an extensive literature on power management in computing devices overviews have been given by brooks et al mudge and tiwari et al both in academic research and practice voltage frequency speed scaling is the dominant technique for power management speed scaling involves dynamically changing the speed of the processor current microprocessors from amd intel and transmeta allow the speed of the microprocessor to be set dynamically some modern processors are able to sense their own temperatures such a device can be slowed down or shut down so that its temperature will stay below its thermal threshold skadron et al there is an inherent tradeoff between power reduction and performance in gen eral when more power is available better performance can be achieved as a result it is generally proposed that power reduction techniques be preferentially applied during times when performance is less critical it is likely that in the future these policies will necessarily incorporate information provided by applications and high level resource managers in operating systems ellis this will require policies to determine how essential performance is at any given time and how to apply a particular power reduction technique our goal here is to make a formal study of a particular power reduction technique namely speed scaling in a specific setting namely scheduling tasks with deadlines to manage either temperature or energy background the starting point for our investigations is the seminal paper by yao et al in which the authors proposed formulating speed scaling problems as scheduling problems that is the setting is a collection of tasks and a schedule specifies not only which task to run at each time but also the speed at which to run the selected task each task i has a release time ri at which it enters the system a deadline di and an amount of work wi that must be performed between times ri and di to complete the task in some settings for example the playing of a video or other multimedia presen tation there may be natural deadlines for the various tasks imposed by the applica tion in other settings the system may impose deadlines to better manage tasks or insure a certain quality of service to each task buttazzo yao et al assumed that tasks can be preempted that is the device can suspend the execution of a task and later resume the task from the point of suspension preemption is a necessary feature to obtain reasonable performance in a system with tasks with widely varying work yao et al assumed the existence of a function p that specifies the power used when the device is run at speed they assumed that p sα for some α the key fact about such a function is that it is strictly convex that is the slower a task is run the less energy is used to complete that task this is a generalization of the well known cube root rule for cmos devices which states that the speed is roughly proportional to the cube root of the power p or equivalently p cmos is likely to remain the dominant technology for the near term future power in cmos devices has three components switching loss leakage loss and short circuit loss brooks et al mudge switching loss is the energy consumption due to charging and discharging gates the switching loss is roughly proportional to sv where is the speed clock frequency and v is the voltage v and are not independent there is a minimum voltage required to drive the microprocessor at a desired frequency and this minimum voltage is approximately proportional to the frequency brooks et al hence one can conclude that switching loss is roughly proportional to the cube of the speed currently switching loss is responsible for the majority of the energy used by computing devices mudge yao et al then studied the problem of minimizing the total energy used subject to the deadline feasibility constraints this is always possible under the assumption that the processor can run at any speed they gave an optimal offline greedy polynomial time algorithm which we call yds the yds schedule is si multaneously optimal for all strictly convex speed to power functions the yds schedule can also be seen to minimize the maximum speed and hence the maxi mum power yao et al also proposed two simple online algorithms in the online version of the problem the scheduler learns about a task only at its release time at this time the scheduler also learns the exact work requirement and the deadline of the task the online algorithm average rate avr runs each task i at speed wi di ri the online algorithm optimal available oa schedules the unfinished work optimally say using yds under the assumption that no more tasks will arrive yao et al state a lower bound of αα on the competitive ratio for avr and oa they prove using a rather complicated spectral analysis that the competitive ratio of avr is at most they also show that an on line algorithm cannot in general construct an optimal energy schedule even for an instance that contains only two tasks our contributions yao et al did not explicitly prove that the yds algorithm produces the most energy efficient feasible schedule to the best of our knowledge no such proof has appeared in the literature we show in section that the correctness of yds is an elegant consequence of the well known kkt optimality conditions for convex programs this illustrates the utility of the kkt optimality conditions in power management problems in section we extend the results yao et al on online algorithms for energy minimization we give explicit instances that show that the competitive ratios of avr and oa are at least αα we then provide a tight αα bound on the competitive ratio of oa using a potential function argument we then turn our attention to speed scaling to manage temperature to our knowl edge this is the first theoretical investigation of this area we first need to model the cooling behavior of a device cooling is a complex phenomenon that cannot be captured completely accurately by any simple model sergent and krum for tractability we require a simple first order approximation our key assumptions are that heat is lost via conduction and the ambient temperature of the environment surrounding the device is constant this is likely a reasonable first order approxi mation in some but certainly not all settings then we appeal to newton law of cooling which states that the rate of cooling is proportional to the difference in tem perature between the object and the ambient environmental temperature without loss of generality we may assume that the temperature scale is translated so that the ambient temperature is zero if we assume that the net change in temperature is the sum of the decrease due to cooling as described above and an increase proportional to the electrical power applied to the device a first order approximation for rate of change t t t of the temperature t t at time t is then given by the equation t t t ap t bt t where p t is the supplied power at time t and a and b are constants sergent and krum crusoe we call b the cooling parameter of the device we then consider the relationship between temperature and energy temperature and energy are physical variables with quite different properties if the processor in a mobile device exceeds its energy bound then the battery is exhausted if a proces sor exceeds it thermal threshold it is destroyed power management schemes for conserving energy focus on reducing cumulative power while power management schemes for reducing temperature must focus more on instantaneous power power management schemes designed to conserve energy may not perform well when the goal is to reduce temperature in fact many low power techniques are reported to have little or no effect on temperature skadron et al temperature aware design is therefore an area of study distinct from albeit related to energy aware design skadron et al in section we consider the relationship between temperature and energy one consequence of newton law is that an un powered device cools by a constant fraction every time units this leads us to observe that the maximum temperature is within a factor of two of a times the maximum energy used over any interval of length if b then no energy is ever dissipated from the device and the maximum temperature is the final temperature which is a times the energy used thus when b the temperature minimization problem is equivalent to the energy minimization problem in the limit as the cooling parameter b approaches the maximum temperature is essentially determined by the maximum energy over an infinitesimal interval and thus the minimization problem intuitively becomes equivalent to the problem of minimizing the maximum power or equivalently minimizing the maximum speed the energy minimization problem when the speed to power parameter α is is also equivalent to minimizing the maximum power because of this exponential cooling it seems difficult to reason about tempera ture however the above observation about approximating temperature by energy used over some interval makes reasoning about approximate temperature much easier than reasoning about exact temperature this observation also motivates us to define what we call a cooling oblivious speed scaling algorithm which is an algorithm that is simultaneously o approximate for minimizing the maximum temperature for all cooling parameters b thus a cooling oblivious algorithm is o approximate for total energy further if the schedule produced by a cooling oblivious algorithm does not depend on the value of α and the cooling parameter b as is the case for all the algorithms that we consider then the algorithm is also o approximate for minimizing the maximum speed in section we show that while the yds schedule may not be optimal for temper ature it is cooling oblivious more precisely we show that yds is approximate with respect to temperature for all cooling parameters b this constructively shows that there are schedules that are o approximate with respect to both of the dual criteria of temperature and energy we then turn to online speed scaling to minimize the maximum temperature that a device ever reaches again subject to the constraint that all tasks finish by their deadlines in section we show that online algorithms oa and avr proposed by yao et al in the context of energy management are not o competitive with respect to temperature that is these algorithms are not cooling oblivious recall that both oa and avr are o competitive with respect to energy this demonstratively illustrates the observation from practice that power management techniques that are effective for managing energy may not be effective for tem perature one intuitive speed scaling algorithm to manage temperature is to run at the minimum constant speed that will allow all tasks to finish by their deadline surprisingly we show that this algorithm is also not o competitive with respect to temperature we propose a new online speed scaling algorithm that we call bkp in section we show that bkp is cooling oblivious that is bkp is simultane ously o competitive for total energy maximum temperature maximum power and maximum speed we show that the competitive ratio for bkp with respect to energy is at most α α α exp α note that for α this competitive ratio is at most exp α the competitive ratio of bkp is better than the competitive ratios of oa and avr for α we show that the competitive ratio of bkp with respect to temperature is at most exp α α α α we show that bkp is e competitive with respect to maximum speed or equivalently that bkp is exp α competitive with respect to maximum power we further show that bkp is optimally competitive with respect to both maximum speed and maximum power that is no deterministic algorithm can have better competitive ratios as a consequence of this one can conclude that if a deterministic online algorithm is o kα competitive with respect to energy then the value of the constant k has to be at least e thus the competitive ratio of bkp with respect to energy is optimal up to a multiplicative constant for deterministic online algorithms we finally turn our attention to offline temperature management we show in section that this problem can be posed as a convex optimization problem convex optimization problems can be solved arbitrarily precisely in polynomial time using the ellipsoid algorithm if one can compute a separating hyperplane for a violated constraint in polynomial time to accomplish this for our temperature problem we show that the key subproblem is determining the maximum work that can be accomplished during a fixed time period with a fixed starting and a fixed ending temperature we show how to use techniques from calculus of variations to solve this subproblem as a consequence of this we reveal some structure of the optimal temperature schedule during any maximal time period which contains no release time or deadline the temperature curve is either an euler lagrange curve or rises to the thermal threshold along an euler lagrange curve stays at the thermal threshold for some amount of time and then falls along an euler lagrange curve related research a naive implementation of yds runs in time o this can be improved to o if the intervals have a tree structure li et al recently li et al gave an o log n implementation for the general case for hard real time tasks with fixed priorities yun and kim show that it is np hard to compute a minimum energy schedule they also give a fully polynomial time approximation scheme for the problem kwon and kim give a polynomial time algorithm for the case of a processor with discrete speeds li and yao give an algorithm with running time o d n log n where d is the number of speeds irani et al study online speed scaling algorithms to minimize energy usage for a device that also has a sleep state they give an offline polynomial time approximate algorithm irani et al also give an online algorithm a that uses as a subroutine an algorithm b for pure speed scaling if b is additive and monotone as avr oa and bkp are then a is max α r competitive where r is the competitive ratio of b thus our analysis of oa and bkp improve the best known competitive ratio for this problem a survey on algorithmic problems in power management was given by irani and pruhs definitions in this section we recap the definitions introduced so far and introduce some definitions that we will use throughout the article we also make some observations about these definitions a problem instance consists of n tasks task i has a release time ri a deadline di ri and work wi in the online version of the problem the scheduler learns about a task only at its release time at this time the scheduler also learns the exact work requirement and the deadline of the task we assume that time is continuous a schedule specifies for each time a task to be run and a speed at which to run the task the speed is the amount of work performed on the task per unit time a task with work w run at a constant speed thus takes w time to complete more generally the work done on a task during a time period is the integral over that time period of the speed at which the task is run a schedule is feasible if for each task i work at least wi is done on task i during ri di note that the times at which work is performed on task i do not have to be contiguous if a task is run at speed then the power is p sα for some constant α the energy used during a time period is the integral of the power over that time period in the energy version of our problem the objective is to minimize the total energy e s used by the schedule s we now turn to temperature we assume without loss of generality that the initial temperature is we assume that the temperature t t at time t is then given by the cooling equation t t t ap t bt t where p t is the power at time t and a and b are nonnegative constants if t is a temperature function then t t will always refer to the derivative of t with respect to time we make observations about this cooling equation note that by rescaling temperature or energy one could assume that a thus a will not play much of a role in our analysis and the key parameter is the cooling parameter b note that the temperature function will be continuous even if the power function is not if we want to maintain a constant temperature tz then as t t will equal it is sufficient to run at power btz a once temperature tz is reached solving the cooling equation for p t yields p t t t t bt t a thus one can specify a power function and hence a speed function by specifying a temperature function by eq the energy used during an interval x y is y y t t t bt t dt x x a r y t b r y t y t x b r y since p sα the work done during a time interval x y is r y f t t t bt t α in the temperature version of our problem the objective is to minimize the maximum temperature t s reached during the schedule s let c ln call a time interval of length c a c interval as we will see the problem of minimizing the maximum temperature is related to the problem of minimizing the maximum energy c s used in any c interval during the schedule s in the maximum speed version of our problem the objective function is to min imize the maximum speed reached during the schedule in the maximum power version of our problem the objective function is to minimize the maximum power reached during the schedule if a is a scheduling algorithm then a i denotes the schedule output by a on input i e a i will denote the energy of a i and t a i the maximum temperature for convenience we will use opt i to represent an optimal schedule for the objective under consideration e opt i will denote the optimal energy and t opt i the optimal temperature when i is clearly understood from context we may drop it from the notation a schedule is r competitive or r approximate for a particular objective func tion if the value of that objective function on the schedule is at most r times the value of the objective function on an optimal schedule an online scheduling algorithm a is r competitive or has competitive ratio r if a i is r competitive for all in stances an offline scheduling algorithm a is r approximate or has approximation ratio r if a i is r approximate for all instances an online algorithm a is cooling oblivious if a is o competitive with respect to temperature for all cooling parameters b we now define the algorithms that we consider in this paper along with related concepts we start with the offline speed scaling algorithm yds proposed by yao et al let w denote the work that has release time at least and has deadline at most the intensity i of the time interval is defined to be w algorithm yds the algorithm repeats the following steps until all jobs are scheduled let be the maximum intensity time interval the processor will run at speed i during and schedule all the jobs comprising w always running the released unfinished task with the earliest deadline then the instance is modified as if the times didn t exist that is all deadlines di are reduced to max di and all release times ri are reduced to max ri it is easy to see that the yds algorithm is optimal with respect to maximum speed and hence maximum power by noting that the maximum speed of any schedule must be at least the intensity of the maximum intensity interval found by yds at the very beginning i e when no jobs have been scheduled and the intensity of the maximum intensity interval and hence the speed at which yds schedules jobs only decreases as yds proceeds note that the yds schedule has the property that each task is run at a fixed speed however this speed may be different for different tasks let y t be the speed of yds at time t we now define the online speed scaling algorithms oa and avr proposed by yao et al algorithm oa maintain the invariant that at all times t the task with the earliest deadline is run at speed maxt w t t where w t is the unfinished work that has deadline within the next t units of time an alternative description is that oa schedule for the future is always the optimal energy yds schedule based on the current state algorithm avr maintain the invariant that at all times t the earliest deadline task is run at speed i j t wi where j t is the collection of tasks i with ri t di intuitively avr runs each task at the optimal speed under the assumption that it is the only task in the system we now turn to defining our proposed online speed scaling algorithm bansal et al for t let w t denote amount of work that has release time at least and deadline at most and that has already arrived by time t we define three more terms q t p t and v t which will be useful in the description and analysis of our new online algorithm bkp let q t be the maximum intensity of an interval containing t that is q t max i such that t intuitively q t can be viewed as yds speed at time t note that this statement is not exactly true but q t is an upper bound on yds speed at time t let p t be defined by p t max w t such that t t t intuitively p t is the online algorithm estimate of the speed at which yds would work at time t based on the knowledge of tasks that have arrived by time t let v t be defined by v t max w t et e t t t t t t t e t t t we are now ready to define the online algorithm bkp algorithm bkp at time t work at speed e v t on the unfinished task with the earliest deadline note that w t p t and v t may be computed by an online algorithm at time t it is a matter of taste whether we define bkp to run at speed e v t or e p t all of our results hold for both variations the following lemma which we use frequently relates v t p t and q t lemma for all instances i and for all times t v t p t q t proof the speed v t is equivalent to a restricted variant of p t where instead of considering the maximum over all such that t we require that and to be related such that t e t thus v t p t finally it is obvious that p t q t since w t w for any t properties of the yds schedule energy we show that the energy optimality of the yds schedule follows as a direct consequence of the well known kkt optimality conditions for convex programs theorem yds is optimal with respect to energy proof we start by stating the kkt conditions next we show how to express the energy problem as a convex program and then show the result of applying the kkt conditions to this convex program consider a convex program min x fi x i n assume that this program is strictly feasible that is there is some point x where fi x for i n assume that the fi are all differentiable let λi i n be a variable lagrangian multiplier associated with the function fi x then the necessary and sufficient kkt conditions for solutions x and λ to be feasible primal and dual solutions are boyd and vandenberghe fi x i n λi i n λi fi x i n n x λi fi x i to state the energy minimization problem as a convex program we break time into intervals tm at release times and deadlines of the tasks note that because the set of available jobs does not change over any such interval and because of the convexity of the speed to power function we may assume that the processor runs at constant speed throughout any such interval let j i be the tasks that can feasibly be executed during the time interval ii ti ti and j j be intervals during which task j can be feasibly executed we introduce a variable wi j for j j i that represents the work done on task j during time ti ti our interval indexed program is then min e wi j α w j i j j wi j j n i j j i ti ti ti ti e wi j i m j j i it is easy to verify that the program is convex we now apply the kkt conditions to this program we associate a dual variable δ j with inequality j in line a dual variable β with the inequality in line and a dual variable γi j with inequality i j in line we now evaluate line of the kkt conditions for our convex program we have e j δ j w j i j j wi j β m j j i wi j α t e i m n ti ti i i γi j wi j considering the component of this equation corresponding to the variable e we have β or equivalently β considering the component corresponding to the variable wi j we have δj βα f k j i wi k α t γi j consider a wi j such that wi j we know that by complementary slackness eq that it must be the case that γi j hence δ j α f k j i wi k α t hence the interpretation of the dual variable δ j is α times the speed at which the processor runs during interval i raised to the power of α this quantity and hence the speed of the processor must be the same for each i such that wi j that is during each interval i in which task j is run now consider a wi j such that wi j rearranging eq we find that γi j α f k j i wi k α t δ j since γi j is nonnegative the processor runs at least as fast during interval i as during the intervals where task j is run thus we can conclude that necessary conditions for a primal feasible solution to be optimal are for each task j the processor runs at the same speed say j in all intervals i in which task j is run the processor runs at speed no less than j during intervals i such that j j i and task j is not run the yds schedule clearly has these properties we can use eqs and to find a dual feasible solution satisfying the kkt conditions and thus we have found optimal primal and dual solutions the yds schedule corresponding to the primal solution is thus optimal the relationship between energy and temperature we show in theorem that the maximum temperature t s of a schedule s is within a factor of two of a c s recall the parameter a in the cooling equation and our definition of a c interval where c ln and that c s is the maximum energy expended over any c interval further recall that we assume that the initial temperature is zero we show in theorem that the energy optimal yds schedule is approximate with respect to temperature for all cooling parameters b that is yds is cooling oblivious given theorem to prove theorem it is sufficient to show that yds is approximate with respect to the objective of the maximum energy expended over any c interval theorem for any schedule s and for any cooling parameter b a c s t s c s proof if b then a c s t s a e s and the result holds so assume from now on that b we rewrite our cooling equation dt t ap t bt t as d exp bt t t dt a exp bt p t integrating this equation over a c interval that ends at some time we get t t bt t t c bt cb a r bt p t dt note t x for x we first show that t s c s suppose that the temperature t s is achieved at time we simplify eq as follows since exp bt is increasing in t we have that c exp bt p t dt exp bt c p t dt thus t t t t c cb a r p t dt as t c t t s it follows that t s cb a r p t dt a c s and as cb ln it follows that t s a c s c s exp cb we now show that t s a c s let c bea c interval where c s energy is used again we start with eq using the fact that the temperature at any time is nonnegative as the environmental temperature is and hence in particular that t c and that exp bt is an increasing function of t it follows that t t bt a r bt p t dt a bt cb r p t dt thus exp cb a c s t s t t exp cb a c s a c s note that yds is not optimal for minimizing the maximum temperature nor for minimizing the maximum total energy in any c interval the fact that yds is not optimal for temperature can be seen on single task instances where the optimal temperature schedule must run at a speed that follows some non constant euler lagrange temperature curve see theorem for more details that yds is not optimal for minimizing the maximum energy used in any c interval can be seen from the following instance without loss of generality we can normalize so that c there are two tasks with work each both arriving at time with deadlines and respectively if the first task is run at speed from time to and the second at speed from to the maximum energy in any interval of length is yds runs the first task at speed from time to and the second at speed from to the energy used from time to for instance is larger than note that this holds for any speed to power function of the form p sα with α energy in a c interval for the rest of this section we only consider the objective of minimizing the maximum energy in any c interval this will culminate in lemma which states that the yds schedule is approximate with respect to this objective we first require some preliminary definitions and observations let cy i denote a c interval in yds i that uses energy c yds i a maximum energy c interval yds i let e be a small positive constant that we will define precisely later let the speed be defined as ec yds i c α we call a task in i slow if it runs at a speed strictly less than in yds i this notion is well defined because each task runs at constant speed in the yds schedule the rest of the tasks are called fast let t denote the speed at time t in yds i define an island to be a maximal interval of time where t we now give some simple but useful properties of the schedule yds i claim let g be an island yds schedules within g exactly those tasks k such that rk dk proof it is trivial that all such tasks must be executed wholly in g to see that only such tasks are executed in g observe that if a task can feasibly be executed outside g and is partially or wholly executed in g we would have a contradiction to the energy optimality of yds i since work could be shifted from an interval of higher speed to an interval of lower speed claim for any island g of length no more than c in any instance i c opt i is at least the energy consumed in g by yds proof follows from claim and the energy optimality of yds since opt must execute in g at least the tasks executed by yds in g we now show that most of the energy in cy i is contained in fast tasks lemma let h i denote the set of islands that intersect cy i and let e h i denote the energy consumed under yds in the islands h i then we have that e h i e c yds i proof consider the times in cy i where t those periods are contained in h i so c yds i e h i is at most the energy used by the yds i during the times t in the c interval cy i when t thus c yds i e h i csα which is at most ec yds i by the definition of the claimed inequality then follows we now show that yds is approximate with respect to minimizing the maxi mum energy used over any c interval lemma for any instance i c opt i min f e c yds i e c yds i choosing e it follows that c opt i c yds i proof consider an island g of i and let g be the length of g as the yds schedule for i runs at speed at least during g the total energy consumed by yds is at least g sα by claim all the tasks in i that yds runs in g must also be run in g in any feasible schedule so it must be the case that the total energy consumed by any feasible schedule for i during g has to also be at least g sα if g c then by a simple averaging argument for any feasible schedule there is some c interval that is totally contained in g with the property that the energy used during this c interval is at least g sα i g cl in turn this is at least csα which by the definition of α equals e c yds i thus c opt i e c yds i if g c if all the islands have length no more than c then consider the islands that intersect cy i if some such island g has energy at least e c yds i the result follows by claim now suppose that all islands that intersect cy i have energy less than e c yds i by lemma we know that in yds i the total energy during the islands intersecting cy i is at least e c yds i as at most two islands can lie partially in cy i at least e c yds i energy is in islands that are totally contained inside cy i and hence the result follows by claim we can now conclude that yds is cooling oblivious theorem the energy optimal algorithm yds is a approximation with respect to maximum temperature proof by lemma we know that c opt i c yds i by the orem we know that t opt i c opt i and c yds i combining these three inequalities gives that t opt i t yds i the oa avr and constant temperature algorithms in this section we consider the online algorithms avr and oa proposed by yao et al and the class of constant temperature algorithms recall the definitions of avr and oa in section we show that both avr and oa have a competitive ratio of at least αα with respect to energy we then show that oa is in fact exactly αα competitive we show that both avr and oa are not o competitive with respect to temperature we also show that another natural algorithm that we call the constant temperature algorithm is not o competitive with respect to temperature under a natural definition of competitiveness for this class of algorithms energy before giving an explicit lower bound instance for the competi tive ratio of oa and avr with respect to energy we need the following technical lemma lemma if α and x y then x y α xα αxα proof setting t y x and dividing through by xα the inequality above is equivalent to t α αt or t α αt which we need to show holds when t note that the left hand side is when t now differentiating the left hand side with respect to t gives α t α α which is always positive when t thus t α αt is increasing and thus positive when t lemma the competitive ratio of avr and oa with respect to energy is at least αα proof the instance is defined as follows all tasks have the same deadline n for i n a task of work n i α arrives at time i observe that for instances with a common deadline as is the case here avr and oa behave identically the optimal energy algorithm yds completes the task that arrives at time i by time i running at speed n i α during the time interval i i the resulting energy usage for yds is then n n i α α n n i we now analyze the energy usage of oa and avr let i be the speed of avr during the time interval i i then for i n i α j i n j j n j α r j i α α n i α αn α the first equality above is by the definition of avr then the energy used by avr is n e avr i i i n α n i α αn α α i n α n i n i n α α α α n α i n i n i n α αn α n f α α i n i i n i αα hn αg α α hn g the first equality above is by the definition of yds the first inequality comes from the lower bound on i from eq the second inequality comes from applying lemma with x n i α and y n α choosing n large enough the competitive ratio can be made arbitrarily close to αα we now turn to the main result of this section that the competitive ratio of oa with respect to energy is exactly αα before we begin we need the following algebraic fact lemma let q r δ and α then q δ α q αr α δ qα q αr proof we need to show that q δ α q αr q δ α α δ qα q αr or equivalently that q αr q δ α qα q δ α α δ since q δ α qα it suffices to show that q q δ α qα q δ α α δ substituting δ zq the left hand side of the above can be written qα z α qα z α α z thus it will be enough to show that for z z α z α α z differentiating this with respect to z we get α z α α z z α α α z α α z z α α z z α where the last inequality holds since α and z thus the maximum of this expression is attained at z where it has value this implies the result theorem the algorithm optimum available is αα competitive with re spect to energy proof let soa t denote the speed at which oa works at time t and sopt t denote the speed that the optimal algorithm yds works at time t at any time t either a task arrives or finishes or else an infinitesimal interval of time dt elapses and oa consumes soa t αdt units of energy we will define a potential function φ t that satisfies the following properties the potential function φ t does not increase as a result of any of the following events the arrival of a task the completion of a task by oa the completion of a task by opt at any time t between arrivals t α dφ t ααs t α oa dt opt the potential function φ t has value before any tasks arrive and also has value after the last deadline integrating eq over time and using the other two stated properties we can conclude that e oa i αα e opt i for more information on the potential function method see cormen et al before we can define the potential function we need to introduce some notation let t denote the speed at which oa would be working at time t if no new tasks were to arrive after the current time since oa simply computes the yds schedule based on the current knowledge of tasks the speeds t are computed as follows let woa t t t denote the unfinished work under oa that is currently available with deadlines in t t t we will refer to woa t t t t t t as the density of interval t t t consider a sequence of times defined inductively as follows let always denote the current time let ti i denote the smallest time such that woa ti ti ti ti max woa ti t t t t ti t t ti thus is the smallest time when the density of tasks with deadlines between and is maximized and so on it is easy to see that for all i t ti for ti t ti we will call the interval ti ti a critical interval and denote it by ii note that these intervals are those scheduled in successive steps of the yds algorithm assuming that ties are broken by choosing the smallest interval with maximum intensity we now note that ti is a nonincreasing sequence if ti ti then this contradicts that ti is the critical density for time ti since in this case woa ti ti ti ti woa ti ti ti ti ti analogously let wopt t t t denote the unfinished work under the optimal offline algorithm at the current time that has deadline in t t t we define the potential function φ t as follows φ t α ti α woa ti ti αwopt ti ti i note that by the definition of oa is always working at speed if no new task arrives the algorithm continues to work at the same speed until the current critical interval finishes when the current critical interval finishes the algorithm enters the next critical interval the indices of the critical deadlines shift by one and the new speed is that which was previously also note that the potential is continuous as a critical interval finishes and we move to the next one this follows because as the current critical interval finishes approaches and both woa and wopt approach since both algorithms have to finish this work by time thus the contribution of the first term approaches as the current interval is about to finish we now consider the various cases as time progresses and prove that the potential function satisfies the properties claimed above it is easy to see that at any time oa is running some task if and only if the optimal energy algorithm yds is running some task and thus we may assume that all times in our arguments that each of oa and yds is running some task working case we first consider the case that no task arrives and no task is completed during the next dt units of time thus each ti remains fixed including during the dt time units we have to show that or equivalently t α ααs opt t α dφ t dt α ααsopt t α d α t α w dt i ti ti αw opt ti ti as oa works woa is decreasing at rate and woa ti ti remains fixed for all i let k be the smallest index such that wopt tk tk assuming the optimal energy schedule yds always executes the task with the earliest deadline we have that wopt tk tk decreases at rate sopt and wopt ti ti is fixed for i k thus evaluating the left hand side of eq we see that it is equivalent to α ααsopt α αs α tk α since tk eq would be implied by α α α ααsopt α let z sopt by substitution eq is equivalent to α zα αα for z let u z be the polynomial on the left hand side of inequality note that u αα and u differentiating u z with respect to z we ut z α α zα α zα solving for ut z we get the unique value z α because α ut z for z α and ut z for z α u z is maximized at z α and u α hence u z is nonpositive for nonnegative z thus we have established inequality arrival case consider the arrival of a task of work x with deadline t let i be such that ti t ti we must show that the change in potential caused by this arrival is nonpositive first we consider the simplest case when the critical intervals are unchanged that is only the values of the critical densities change hence the only effect the new task has is to increase the density ti of the interval ii ti ti to woa ti ti x ti ti and the quantity woa ti ti αwopt ti ti decreases by α x thus the change in the potential function is then f woa ti ti x α f woa ti ti α substituting q woa ti ti δ x and r wopt ti ti and rearranging we can write α q δ α q αr α δ qα q αr φ which is nonpositive by lemma ti ti α we now consider the more interesting case when the arrival of a task might change the critical intervals while this new task may radically change the structure of the critical intervals we show that we can think of this change as a sequence of smaller changes where each smaller change affects only two critical intervals moreover each change is essentially equivalent to that in the previous case where the structure of the critical intervals remains unchanged to explain how to accomplish this imagine the work of the new task increasing starting from for some amount of work x t x one of the following three events must occur the interval ii remains a critical interval and its density becomes equal to that of ii in particular x t is such that ti woa ti ti x t ti ti the interval ii splits into two critical intervals iit ti t t and iitt t t ti for some ti t t ti since x t is the smallest such work the densities of iit and iitt are identical and equal to woa ti ti x t ti ti a sequence ii of critical intervals merge into one new critical interval we can think of this event as a sequence of pairwise merges each of which combines with to form a new interval in this case it must be that x t for each of these events we can imagine the original task of work x as consisting of two tasks such that both arrive at the same time and have the same deadline t but one has work x t and the other has work x x t we can then first analyze the change in potential as the result of the arrival of the work x and then repeat this procedure recursively for x potential for a task of work x x t thus we only need to consider the change in that causes one of the three events described above for the first type of event the only change in the potential function is due to the change of density of ii this change is identical to eq with x replaced by x t and again the non positivity of φ follows by lemma for the second type of event the change in the potential function is only due to ii being replaced by iit and iitt since the densities of iit and iitt are identical these intervals can still be considered together as far as the potential function is concerned hence the change in the potential function is again given by eq with x replaced by x t and again the non positivity of φ follows by lemma for the third type of event the change in the potential function is only due to and being replaced by since the densities of and are all identical these intervals can still be considered together as far as the potential function is concerned hence the change in the potential function is again given by eq with x replaced by x t and again the nonpositivity of φ follows by lemma we can repeat this process until we have used up all of the x work in the arriving task temperature we start this subsection by showing that avr and oa are not o competitive with respect to temperature in the reasonable case that the thermal threshold tmax of the device is known the most obvious temperature management strategy is to run at a speed that leaves the temperature fixed at tmax in particular whenever there is work to do the algorithm works at a speed that maintains the temperature tmax and otherwise it cools according to newton law of cooling we call such a strategy o competitive if on any instance i on which this constant temperature algorithm misses a deadline every feasible schedule reaches a temperature of q tmax at some point in time we then show that staying at the thermal threshold is not an o competitive strategy lemma the online algorithms avr and oa are not o competitive with respect to temperature more precisely the competitive ratios of these algorithms must depend on either the number of tasks or the cooling rate b proof we use a variation of an instance from yao et al choose an arbitrarily large integer n and consider an instance with n tasks where task i is released at time ri ic has work wi c and deadline di nc for i n again note that since all tasks have a common deadline avr and oa behave identically the yds schedule runs tasks at a constant speed of and thus uses total energy n and energy c in any c interval using theorem it is sufficient to show that there is some c interval where the energy used by oa and avr is ω c as avr runs task i at speed n i during the interval ic nc during the c interval c n cn avr and oa run at a speed of hn g log n where hn is the nth harmonic number and thus the energy used during this c interval is q c logα n theorem the speed scaling algorithm that runs at such a speed that the temperature remains constant at the thermal threshold tmax is not o competitive proof suppose at time a task with work x which will be specified later and deadline e arrives we will consider the behavior as e goes to suppose the temperature at time is we choose x such that it is equal to the maximum work that the adversary can get done by time e while keeping the temperature below tmax k for some constant k using eq from section for this maximum work and substituting α we get x g btmax the crucial fact is that the maximum work that the adversary can do depends on e as on the other hand the constant temperature algorithm at temperature tmax has power p btmax a and hence speed btmax a and work g btmax a which depends linearly on e thus for any constant k the ratio of the work completed by the adversary to the work completed by the constant temperature algorithm goes to infinity as e goes to the bkp algorithm recall the definition of our newly introduced algorithm bkp in section we will show that it is cooling oblivious that is bkp is o competitive with respect to energy maximum power speed and temperature we analyze bkp separately for each of these objectives preliminaries recall that at any time t bkp works at speed e v t on the unfinished job with the earliest deadline where v t is defined by eq in this section we first prove that bkp always produces a feasible schedule we then give four inequalities that will allow us to relate bkp to the optimal schedule theorem the bkp algorithm always outputs a feasible schedule proof assume for the sake of contradiction that bkp misses some deadline for some problem instance of these infeasible instances consider one with the fewest number of tasks and let d denote the first deadline that is missed in this instance we claim that on this instance bkp always works on tasks with deadline no more than d during the interval d to see this first observe that if bkp is idle at some time t during d then we can replace the instance by a smaller one by removing all jobs that finish before t similarly as bkp always works on the job with the earliest deadline if it worked on a task with deadline greater than d at some time t d then bkp must have finished all work with deadline no more than than d that arrived by time t thus one could obtain another infeasible instance with fewer tasks by considering only those tasks released after the time t this implies that if bkp misses the deadline at time d it must be that the total work done by bkp during d is strictly less than w d the total work in the instance with deadline no more than d our proof will be to show that this cannot happen by choosing t t d in the definition of v t it follows trivially that v t w t et e d d e d t thus the work done by bkp during the time period d is r d e v t dt r d w t et e d d dt we now expand the right hand side of this inequality let b x denote the rate at which work arrives at time x thus if no work arrives at time x then b x if w units of work arrives at time x then b x is w times the dirac delta function thus for example r b b x dx is just the work that arrives during the interval a b r d w t et e d d dt r d fr t b x dx dt r d fr x e d e b x r d fr x e d e d b x ln r d d x dx d x e d e w d the second equality follows by interchanging the integrals and observing that for each x d the work b x contributes to w t et e d d d t if and only if x et e d t or equivalently that t x x e d e thus bkp does at least w d work during the interval d which is the contradiction we need we now state two inequalities from hardy et al that are critical in our further analysis of bkp fact hardy s inequality theorem hardy et al if it is the case that α f x and f x r x f t dt then r f f x α dx f α α r f α x dx x α the following fact was first proved by hardy and littlewood and later simplified by gabriel it can also be found in hardy et al theorem and fact suppose that f x is nonnegative and integrable in a finite interval a and that f x is the rearrangement of f x in decreasing order let m x m x f r x f t dt max y x x y y suppose y is any increasing function of y defined for y then r a m x dx r a f r x f t dt dx roughly if we think of f x as the work arriving at time x then the definition of m x resembles the way we define r x in our algorithm this allows us to argue about the function m in terms of f we use these two facts to prove the following two lemmas lemma shows how to relate the work in an arbitrary schedule which will be the optimal schedule in our arguments to the speed q t that upper bounds the speed v t used in the definition of bkp lemma then shows that the energy used by running at speed q t is less than some constant times the energy used by an arbitrary schedule lemma let z t be the speed at time t for some feasible schedule z then q t r z t dt max t proof since z is feasible for any times and we have that r z t dt w thus q t i t t r z t dt max t max t lemma let q t and y t be functions such that q t r y t dt max t then it must be the case that r q t αdt α α α y t αdt t proof we split q t into two parts let l t r t y x dx t t similarly let max t such that v t r y x dx t t max t t such that q t αdt it suffices to show that both by the quantity α α rrt y t αdt t l t αdt and t v t αdt are upper bounded denote the rearrangement of y t in nonincreasing order then by fact it follows that r l t αdt r f r t α y x dx dt t t t x now using fact with f x y x r f r t α y x dx dt f α α r y x αdx t t x α x the desired bound on rt l t αdt follows by eqs and and observing that α α α r y x αdx α α α y x αdx x as y is a rearrangement of y the analysis of t v t αdt is similar energy in this section we show that the bkp algorithm is o α competitive with respect to energy theorem the bkp algorithm is α α exp α competitive with respect to energy α proof we first note that e bkp r e v t αdt r e q t αdt exp α r q t αdt the first inequality follows by the definition of bkp the second inequality follows by lemma setting y t to be the speed at which yds works at time t by lemma it is the case that q t r y t dt finally max t r q t αdt α α α y t αdt t α α α e opt the first inequality is by lemma the last equality follows since yds is the optimal energy schedule maximum speed and maximum power it is not hard to see that our online algorithm bkp is e competitive with respect to the maximum speed or equivalently exp α competitive for maximum power we show this formally in lemma we then show in lemma that this is the best possible competitive ratio for the maximum speed consider an online deterministic algorithm a with the property that the schedule produced by a is identical for all values of α we call such an algorithm α independent note all online algorithms considered in this article bkp oa and avr are α independent given any fixed schedule produced by an α independent algorithm a we can choose α large enough such that the total energy for this schedule is essentially determined by the maximum power used by a thus the lower bound of e for maximum speed implies that for any arbitrarily small constant e there is some α large enough such that a cannot be e e α competitive with respect to total energy when power varies as speed raised to α this implies that restricted to the class of α independent algorithms the base of the exponent in the competitive ratio of bkp cannot be improved lemma the online algorithm bkp is e competitive with respect to maxi mum speed proof yds is the optimum offline algorithm with respect to maximum speed the maximum speed at which yds ever works is exactly equal to maxt q t at any time the bkp algorithm works at speed at most e v t at time t which is at most e q t by lemma lemma for every deterministic online algorithm a that maintains dead line feasibility there is some input that causes a at some time to run e times faster than the maximum speed of yds proof assume a has a competitive ratio r we show that it must be the case that r e let e be an arbitrarily small constant let a x ln x the adversary adopts the following strategy it releases work at the rate of a x until some time t e and then after time t no more work is released the value of t depends on the behavior of the online algorithm all work has deadline the adversary will make t e unless a works at too great a speed at some time before e so assume for the moment that t e the work that is released between time f and time g is r g dx g let i t w t denote the intensity of interval restricted to the work that has arrived by time t note that for t e w t ln e ln t in particular if t e then w t for some time t e and some k t by eq i t k k ln e ln t k given a fixed t we will be interested in the value of k t that maximizes t k let k t denote this value of k to determine the value of k t we differentiate i t k with respect to k we get d i t k ln t dk k ln e k k ln e setting i t t k and solving for k we have k et e also note that i t t k is positive when k et e and negative when k et e e also note that et e is always less than t and is nonnegative when t e we can then conclude that if t e e then k t et e and if t e e then k t recall that the yds schedule is optimal with respect to maximum speed suppose the adversary stops bringing in more work at time t then the first interval chosen by yds on this instance will be k t and hence t k t is the maximum speed that yds will run if t e e then k t and yds will run at a constant speed of i t k t i t ln t ln e during the time interval thus for each time x e e a cannot work at a greater speed than r ln x if it did so then a would not be r competitive in the case that adversary stops bringing in work at time x if t e e e then k t et e and yds will run at maximum speed of i t k t i t et e e t ln e thus for each time x e e e a cannot work at a greater speed than r x ln e if it did so then a would not be r competitive in the case that t x if t e then yds will run at a maximum speed of i e k e ee ln e thus during the time period e a cannot work faster than r e if it did so then a would not be r competitive in the case that t e now consider the case that t e by the arguments above the most work that a can get done is e e ln t r ln e dt r e e e dt r e t ln e e dt ee ln e r f r ln e r r f e f r now as e approaches the term e e r approaches thus the maximum work that a can get done approaches r which must be at least w thus we conclude that r cannot be less than e temperature we show in theorem that bkp is o competitive with respect to temperature theorem the online algorithm bkp is exp α α α competitive with respect to temperature for all cooling parameters b satisfying b proof let x be an arbitrary c interval as x is arbitrary by theorem it is sufficient to show that bkp uses at most a factor of exp α α times as much energy as c opt during the interval x here we use zαt speed at time t of a fixed arbitrary schedule opt that uses energy at most c opt in every c interval let xk respectively xk denote the kth c interval immediately to the left respectively r ight of x that i s the left endpoint of xk the leftmost point of x let the interval z be defined to be x x is kc units to x as in the proof of theorem we will assume that bkp runs a t spe ed e q t even if there is no work to do thus we are left to show that rt x q t αdt α α α c opt since opt is feasible we have by lemma q t r z t dt max t we decompose z t as follows let t z t if t z and at all other times let t z t t for all t let t t max r x dx t x and q t r z x dx max t x note that q t t t since z t t t for all each t by convexity of the speed to power function p s it follows that q t α t t α t α t α and thus rt x q t αdt t x t αdt t x t αdt we first upper bound q t αdt in fact we will upper bound q t αdt note that t is identically at all points not in z moreover as z is an interval of length by the definition of c opt it follows that now we have rt z r t αdt opt f α α r t t αdt α t αdt f α α r f α α t z t αdt α c opt the first inequality follows from lemma the equality follows from the defi nition of the final inequality follows from eq we now bound the term t x t αdt by the definition of c opt and the con vexity of the function sα for α any c interval contains at most c c opt c α amount of work in opt our next step is to upper bound t for any t x in particular we claim that for any and such that t x and t q t r z x dx c c α max t opt x to see this the crucial observation is that t for t z as z x x if xk for k then the interval t can contain at most k α k c c opt c work from similarly if x then the interval t can contain at most k c c opt c α work from thus any interval containing t and with k c kc can contain at most k c c opt c α work from the bound on t follows as the integral is just the work from over the time interval thus rt x t αdt rt x f c opt α α dt rt x c opt c dt c opt combining eqs and we have that for any c interval x rt x q t αdt rt x t α t α dt α α α c opt this accomplishes our goal computing the optimal offline temperature schedule in this section we consider the offline problem of speed scaling to minimize the maximum temperature we show how to solve this problem in polynomial time with arbitrary precision using the ellipsoid algorithm for basic information on the use of the ellipsoid algorithm to solve convex problems see for example nestorov we assume a constant tmax that is the thermal threshold for the device the problem is then to determine whether there is a schedule that is feasible and maintains the invariant that the temperature stays below tmax by binary search we can then solve the problem of minimizing tmax before giving the convex program we need to make a few definitions let maxw tx ty tx ty be the maximum work that can be done starting at time tx at temperature tx and ending at time ty at temperature ty subject to the temperature constraint t tmax throughout the interval tx ty strictly speaking maxw is a function of tmax but we suppress this in the notation as we assume throughout that tmax is fixed and given a priori in order to understand maxw we will first need to understand the unconstrained problem umaxw tx ty tx ty defined as the maximum possible work that can be done during the interval tx ty subject to the boundary constraints that t tx tx and t ty ty in particular the temper ature at any time is allowed to exceed tmax in lemma we prove the intuitive fact that maxw and umaxw are well defined if and only if it is possible to cool as quickly as specified lemma suppose that tx and ty are at most tmax each of the quanties maxw tx ty tx ty and umaxw tx ty tx ty are well defined if and only if ty tx exp b tx ty proof we wish to show that if ty tx exp b tx ty then there is no feasi ble solution consider the case that the power is zero throughout the interval tx ty then throughout this interval it is the case that t t t bt t solving this differ ential equation we get t t tx exp b tx t for t tx ty so in particular if ty tx exp b tx ty then there is no feasible solution to either the constrained problem or unconstrained problem since the power must be nonnegative we now wish to show that if ty tx exp b tx ty then there is a feasible solution for the constrained problem and hence trivially for the unconstrained problem if ty tx exp b tx ty then running with power equal to zero is a feasible solution so assume that ty tx exp b tx ty let tz be some time just after tx one feasible solution for the constrained problem is for the temperature to rise to tmax at time tz then stay at temperature tmax until the time tv that solves ty tmax exp b tv ty and then run with power equal to zero until time ty we are now ready to give the convex program we divide time into intervals demarcated by the list tm of all release times and deadlines we introduce a variable ti that represents t ti the temperature at time ti let j i be the set of tasks j that can feasibly be executed during the time interval ti ti that is r j ti and d j ti we introduce a variable wi j for j j i that represents the work done on task j during ti ti we can then express our problem as a mathematical program cp in a relatively straightforward way p j i j j i wi j j n j j i wi j maxw ti ti ti ti i m ti exp b ti ti ti i m ti tmax i m ti i m wi j i m j n constraint ensures that enough work is done to finish each job constraint 40 ensures that it is feasible to complete the claimed work within an interval by lemma constraint ensures that the quantity maxw ti ti ti ti is well defined lemma the mathematical program c p is convex that is the feasible region described in c p is convex proof to see that the feasible region is convex let t and tˆ be the temperature curves corresponding to two feasible solutions to this problem let t t tˆ the speed curve corresponding to t is s t t bt a α t t tˆ t bt btˆ α then since x α is a concave function for α s s sˆ that is the average of the two underlying feasible solutions is feasible to apply the ellipsoid algorithm one needs to give a procedure to determine whether an arbitrary point is feasible and if not to determine a separating hyper plane in particular if g is a violated constraint then the separating hyperplane for us will be the hyperplane whose normal is the gradient of g evaluated at the cur rent point the only constraints for which this is not straightforward are the maxw constraints so we assume for the rest of this section that all constraints other than the maxw constraints are not violated by the current point as this is the only interesting case our goal is then to explain how to take the gradient of the maxw function so that we may compute a separating hyperplane to better understand maxw we first need to understand the unconstrained function umaxw the unconstrained maximum work problem we consider the un constrained problem umaxw ti ti ti ti where the times and temperatures are arbitrary other than that we require that ti ti exp b ti ti so that a feasible solution exists let umaxt t umaxt ti ti ti ti t denote the temperature as a function of the time t that solves umaxw ti ti ti ti that is umaxt t is the temperature curve t that maximizes the quantity r ti p t αdt r ti f t t t bt t α dt ti ti a subject to the constraints that p t t ti ti and t ti ti this problem falls under the rubric of calculus of variations we refer the reader to smith for the basics on calculus of variations for notational simplicity we usually translate time so that since temperature is always a function of time we will drop t in future references to temperature functions let f be the functional f t t bt and let f a f b t t bt t α be the partial derivative of f with respect to t and let ft t α t t bt α be the partial derivative of f with respect to t t any weak extremum t must satisfy the euler lagrange equation see e g smith page d ft dt ft t we call a temperature function t that satisfies the euler lagrange equation an euler lagrange curve we have that d f α t t bt t tt bt t dt t t α α thus the euler lagrange equation gives that b t t bt α t t bt t tt bt t α α α α eliminating common factors and multiplying by t t bt α gives bt t t tt using the standard laplace transform technique we find that the solution to the above differential equation is umaxt t c exp bt d exp btα α where the constants c and d are determined by the boundary conditions alterna tively one can verify the correctness of this solution by plugging it back into the differential equation we now compute the values of c and d setting t and t in eq we get c d setting t and t in eq we get c exp d exp α using c d we have d exp d exp α or d exp exp exp α and therefore c exp exp exp α this gives a complete description of the curve umaxt in particular the tem perature values at two times completely determine the curve umaxt that passes through these points since exp we can conclude that d and hence c d we must check that our curve umaxt is indeed a maximum and not a minimum this is easily seen by substituting d for c in eq we have umaxt exp bt d exp btα α exp bt since d and the term in parentheses are both non positive their product is non negative and thus umaxt is at least as great as the nopower curve t exp bt note that umaxt is well defined for all times t not just for t we will argue about properties of umaxt on this larger domain we now turn our attention to evaluating the work umaxw done by the curve umaxt differentiating eq gives umaxtt t t bc exp bt bdα exp btα α adding eq to b times eq the term bc exp bt cancels out leaving just t t bt bd α exp btα α by eq the power function corresponding to the temperature curve umaxt is then hence by eq t t bt a bd a α exp btα α umaxw r f t t bt α r f bd α a α exp α we now state several intuitive but technical properties of the umaxt when the property is not obvious we will give some explanation why the property holds in fact we observe that it is obvious from eq for umaxt that the temperature approaches zero as time goes to infinity lemma observes that umaxt has a unique maximum and characterizes where this maximum occurs lemma observes that if then the temperature will always stay above lemma observes that the maximum temperature is a nondecreasing function of and finally lemma observes that the maximum temperature will exceed tmax for sufficiently large fact for any and the curve umaxt t approaches as t approaches infinity lemma let umaxt and umaxt be the two curves that intersect at distinct points ta ta and tb tb where ta tb max min let umaxt ta ta tb tb then t t t for all t ta proof follows immediately from the uniqueness of the maximum determined by two points lemma consider the curve umaxt umaxt ti ti ti ti the curve umaxt has at most one point where umaxtt the derivative of umaxt with respect to time t is if umaxtt x for some time x then umaxtt t for all t x if umaxtt then the maximum of umaxt is at t if umaxtt then the maximum of umaxt is at t if umaxtt and umaxtt then the maximum of umaxt is at the unique point tx where umaxtt tx proof if d it follows from eq that t t bc exp bt exp bt and hence that umaxtt everywhere and it is easy to see that the above claims hold now assume that d using c d and multiplying by exp bt b eq can be rewritten as exp bt umaxtt dα b d α exp bt α note that since exp bt b for all t the sign of umaxtt is the same as the sign of the right hand side of the above equation which is a strictly decreasing function of t in particular observe that this implies that umaxtt cannot go from negative to positive all the above claims are then simple consequences of this observation lemma consider two points and let denote the constant temperature curve between and let denote a temperature curve such that and t t for t then at least as much work is completed by following as by following proof consider a temperature curve t such that t t apply ing eq the energy used during by following t is b t a t dt under the temperature constraint t t this integral is maximized with t t throughout and thus the total energy of is at most the total energy of by convexity for a given energy budget working at constant power maximizes the work done that is the maximum work that can be done by any curve l with energy at most e is e α and is achieved by staying at constant power as stays at constant temperature and hence constant power it follows that the work completed by following the curve is no more than that of lemma let and be fixed and consider the class of unconstrained curves umaxt for in particular for let and denote the curves umaxt t and u max t t respectively then if we have that for all times t t t if then for all t such that t t t in particular this implies the following the maximum temperature reached by the curve umaxt in the interval is a nondecreasing function of if then umaxtt is a nondecreasing function of if then umaxtt is a nonincreasing function of proof we first consider the case that as it must be true that either t t for all t or one of these temperature curves is always larger than the other since by lemma if they intersect at two points they are the same by the mean value theorem there must be a time t where t by lemma this implies that x for all x t in particular since t this implies that hence t t it is then obvious that the maximum temperature must be a nondecreasing function of the ending time and the derivative at time must be a nondecreasing function of the ending time we now consider the case that instead of it will be more convenient to work with the curve umaxt which is just the curve translated to the right by time units now since either and are identical or one these temperature curves is always larger than the other if t t then this would in particular mean that but by lemma it is clear that t for all t t2 thus it must be the case that t t for all t t2 t2 it is then obvious that the maximum temperature must be a nondecreasing function of the ending time and the derivative at the ending time must be a nonincreasing function of the ending time lemma consider the temperature curve umaxt as tmax and tmax are fixed and is varied then there is a finite time t such that the maximum temperature reached by the curve umaxt is at least tmax for all t proof consider the curve u umaxt tmax as u t approaches as t approaches infinity there exists a finite time tˆ such that u tˆ consider the curve umaxt tˆ as u and share the points and tˆ u and must be identical thus umaxt tˆ attains a maximum temperature of at least tmax and hence by lemma the maximum temperature reached by umaxt is at least tmax for all tˆ the temperature constrained maximum work problem we now turn our attention to maxw and maxt again requiring that exp so that a feasible solution exists also we now require that both and are at most tmax that is we assume the existence of a temperature constraint t tmax it is known that when such a global constraint is added the solution can be decomposed into subcurves where each subcurve is either an euler lagrange curve corresponding to some unconstrained problem or else follows the boundary smith page we are fortunate in our case that the portion of maxt for which maxt tmax is a single line segment this is an immediate consequence of lemma we now know that either maxt umaxt or maxt consists of three parts an euler lagrange curve up to tmax a line segment at tmax and an euler lagrange curve down to note that either or both of the euler lagrange curves may have length zero the euler lagrange curve up to tmax is of length zero if and only if tmax the euler lagrange curve down from tmax is of length zero if and only if tmax before we characterize the curve maxt in lemma we define some notation let t denote the supremum of values of for which the curve umaxt does not exceed temperature tmax at any time the time t is well defined and finite by lemma note that t is a function of and define γ to be the time unique by lemma at which the maximum temperature is attained on the curve umaxt t define β t γ we now provide alternate characterizations of γ and β and show that these points occur where the derivative of umaxt is zero lemma 9 γ is the largest value of for which the maximum temperature attained by the curve umaxt tmax during the interval is no more than tmax similarly β is the largest value of for which the maximum temperature attained by the curve umaxt tmax in is no more than tmax proof by lemma umaxt t umaxt γ tmax call this curve t t let γ t γ and consider the curve t umaxt γ tmax we know that γ t γ t tmax so using lemma again we know that γ γ tmax the proof for β is similar lemma 10 umaxt γ tmax t γ and umaxt tmax β t proof again using umaxt t umaxt γ tmax we see that the derivative must be at t γ since the maximum is attained there the argument for β is similar lemma 11 consider the curve maxt let γ and β be defined as above if γ β then maxt umaxt if γ β then the curve maxt travels along the curve umaxt γ tmax then stays at tmax until time β and finally travels along the curve umaxt β tmax proof assume first that t γ β by lemma the maximum tem perature reached by umaxt during is a nondecreasing function of hence by the definition of t at no time can the function umaxt exceed tmax therefore maxt umaxt now consider the case that γ β let tx and ty denote the first and last times respectively at which maxt equals tmax we will show that tx γ and ty β by definition of tx and ty the curve maxt restricted to tx is identical to the curve umaxt tx tmax and the curve maxt restricted to ty is identical to the curve umaxt tmax ty if tx γ then by lemma 7 and lemma 9 this contradicts the definition of γ a similar argument shows that ty β now suppose for contradiction that tx γ since ty β γ it follows that the point γ tmax lies on maxt hence for the non zero length time interval tx γ it must be the case that maxt tmax this contradicts the work optimality of umaxt t0 γ tmax and hence the work optimality of maxt t0 again in a similar manner we can obtain a contradiction from the assumption that ty t1 β we now explain how to explicitly compute γ and β consider the curve u umaxt t0 γ tmax recall from lemma 10 that u t γ setting u t γ in eq gives c dα exp bγ α plugging in the values of c and d from eqs and we have that γ is the unique solution of the equation t0 exp exp exp α α exp bγ α f t0 exp α exp exp α by multiplying through by exp exp α and aggregating like terms this is equivalent to t0 exp bγ α α α tmax αtmax exp bγ α similarly the curve u umaxt tmax β satisfies u t we can see from eq that u t is equivalent to c dα α plugging in the values of c and d by eqs and we have that β is the unique solution of the equation t0 exp exp exp α f t0 exp f α exp exp α α by multiplying through by exp exp α and aggregating like terms this is equivalent to tmax exp bβα α α exp bβ α computing a separating hyperplane we are now finally ready to explain how to compute a separating hyperplane for a violated maxw constraint consider an arbitrary point where each ti takes the value tˆi and each wi j takes the value wˆ i j assume that the i th maxw constraint is violated given the values of ti ti tˆi and tˆi we can compute the values γ and β by binary search using the eqs and note that the left hand sides of these equations are monotone functions of γ and β respectively first consider the case that ti ti γ β then we know the maximum temperature constraint is not relevant here now eq gives that maxw d a α where by eq b α α exp b ti ti α d ti exp b ti ti ti exp b ti ti exp b ti ti α α we can then determine whether the i th maxw constraint is violated if this constraint is violated to compute a separating hyperplane let g be a function of ti ti and wi j for j j i defined as g j j i wi j umaxw ti ti ti ti the i th maxw constraint is then equivalent to g a separating hyperplane is then the plane whose normal is the gradient of g evaluated at the current point note that one can easily differentiate g with respect to all variables now consider the case that ti ti γ β maxw is given by the work done by the euler lagrange curve between ti ti and ti γ tmax plus the work done on the constant temperature curve at tmax between time ti γ and time ti β plus the work done by the euler lagrange curve from ti β tmax to ti ti using eq to compute the work done by a constant temperature curve and eq 50 to compute the work done by the two euler lagrange curves we have maxw a α b α α exp bγ α ti ti γ β f btmax α a where by eq α exp bβ α and d ti exp bγ tmax exp bγ exp bγ α α d tmax exp bβ ti exp bβ exp bβα α we can determine whether this maxw constraint is violated if this constraint is violated to compute a separating hyperplane let g be a function of ti ti and wi j for j j i defined as g j j i wi j maxw ti ti ti ti the i th maxw constraint is then equivalent to g a separating hyperplane is then the plane whose normal is the gradient of g evaluated at the current point computing the gradient of g though tedious is straightforward with the possible exceptions of differentiating γ with respect to ti and differentiating β with respect to ti these partial derivatives can be computed using the eqs and that define γ and β respectively consider eq where t0 is replaced by ti namely ti exp bγ α α α tmax αtmax exp bγ α differentiating this equation with respect to ti yields exp f bαγ ti f bα exp f bαγ dγ α α α d ti αtmaxb exp f bγ dγ solving for dγ i yields α α d ti dγ d ti α exp bγ αb tmax ti exp bγ consider eq where t1 is replaced by ti namely tmax exp bβα α α ti αti exp bβ α differentiating eq with respect to ti yields tmax bα exp f bβα dβ α α α d ti α exp f bβ αbti exp f bβ dβ solving for dβ yields d ti α α α d ti dβ α α α exp bβ α d ti αb tmax exp αbβ α ti exp bβ α conclusion in this article we have initiated the theoretical study of speed scaling to manage temperature we assumed a fixed ambient temperature and that the device cools according to newton s law of cooling we have observed that the maximum tem perature is within a factor of two of the energy used over an interval with length inversely proportional to the cooling parameter in newton s law we have identi fied the concept of a cooling oblivious algorithm as an algorithm that is simulta neously o competitive with respect to temperature for all cooling parameters and observed that cooling oblivious algorithms are also o competitive with re spect to energy and maximum power we showed that the optimal energy schedule yds is cooling oblivious and introduced the first known online cooling oblivious algorithm bkp further we have shown that bkp is optimally competitive with respect to maximum power we believe that speed scaling to manage energy and temperature is an area deserving further research attention this area is both academically interesting and has practical applications the most obvious way to proceed is to consider speed scaling versions of other scheduling problems we can take essentially any scheduling problem and consider an energy management version or a temperature management version we would then get a dual criteria optimization problem in which the first objective is the original quality of service objective for the scheduling problem and the second objective is either energy or temperature by solving many such problems the hope would be that it would be possible to build up a better understanding of speed scaling and to build an algorithmic toolkit of analysis and design techniques that are useful for speed scaling problems srcmap energy proportional storage using dynamic consolidation akshat verma ricardo koller luis useche raju rangaswami ibm research india florida international university luis raju cs fiu edu abstract we investigate the problem of creating an energy pro portional storage system through power aware dynamic storage consolidation our proposal sample replicate consolidate mapping srcmap is a storage virtual ization layer optimization that enables energy propor tionality for dynamic i o workloads by consolidating the cumulative workload on a subset of physical vol umes proportional to the i o workload intensity instead of migrating data across physical volumes dynamically or replicating entire volumes both of which are pro hibitively expensive srcmap samples a subset of blocks from each data volume that constitutes its working set and replicates these on other physical volumes dur ing a given consolidation interval srcmap activates a minimal set of physical volumes to serve the workload and spins down the remaining volumes redirecting their workload to replicas on active volumes we present both theoretical and experimental evidence to establish the effectiveness of srcmap in minimizing the power con sumption of enterprise storage systems introduction energy management has emerged as one of the most significant challenges faced by data center operators the current power density of data centers is estimated to be in the range of w sq ft and growing at the rate of per year barroso and ho lzle have made the case for energy proportional computing based on the observation that servers in data centers to day operate at well below peak load levels on an aver age a popular technique for delivering energy pro portional behavior in servers is consolidation using vir tualization these techniques a utilize heterogeneity to select the most power efficient servers at any given time b utilize low overhead live virtual machine vm migration to vary the number of active servers in response to workload variation and c pro vide fine grained control over power consumption by al lowing the number of active servers to be increased or decreased one at a time storage consumes roughly of the power within computing equipment at data centers depending on the load level consuming a greater fraction of the power when server load is lower energy proportion ality for the storage subsystem thus represents a critical gap in the energy efficiency of future data centers in this paper we the investigate the following fundamental question can we use a storage virtualization layer to design a practical energy proportional storage system storage virtualization solutions e g emc invista hp svsp ibm svc netapp v series provide a unified view of disparate storage controllers thus simplifying management similar to server vir tualization storage virtualization provides a transparent i o redirection layer that can be used to consolidate frag mented storage resource utilization similar to server workloads storage workloads exhibit significant varia tion in workload intensity motivating dynamic consoli dation however unlike the relatively inexpensive vm migration migrating a logical volume from one de vice to another can be prohibitively expensive a key fac tor disrupting storage consolidation solutions our proposal sample replicate consolidate map ping srcmap is a storage virtualization layer op timization that makes storage systems energy propor tional the srcmap architecture leverages storage vir tualization to redirect the i o workload without any changes in the hosts or storage controllers srcmap ties together disparate ideas from server and storage power management namely caching replication transparent live migration and write off loading to minimize the power drawn by storage devices in a data center it con tinuously targets energy proportionality by dynamically increasing or decreasing the number of active physical volumes in a data center in response to variation in i o workload intensity srcmap is based on the following observations in production workloads detailed in i the active data set in storage volumes is small ii this active data set is stable and iii there is substantial variation in work load intensity both within and across storage volumes thus instead of creating full replicas of data volumes srcmap creates partial replicas that contain the working sets of data volumes the small replica size allows cre ating multiple copies on one or more target volumes or analogously allowing one target volume to host replicas of multiple source volumes additional space is reserved on each partial replica to offload writes to volumes that are spun down srcmap enables a high degree of flexibility in spin ning down volumes because it activates either the pri mary volume or exactly one working set replica of each volume at any time based on the aggregate workload intensity srcmap changes the set of active volumes in the granularity of hours rather than minutes to address the reliability concerns related to the limited number of disk spin up cycles it selects active replica targets that allow spinning down the maximum number of volumes while serving the aggregate storage workload the vir tualization layer remaps the virtual to physical volume mapping as required thereby replacing expensive data migration operations with background data synchroniza tion operations srcmap is able to create close to n power performance levels on a storage subsystem with n volumes enabling storage energy consumption pro portional to the i o workload intensity in the rest of this paper we propose design goals for energy proportional storage systems and examine exist ing solutions analyze storage workload characteris tics that motivate design choices provide de tailed system design algorithms and optimizations and and evaluate for energy proportionality we conclude with a fairly positive view on srcmap meet ing its energy proportionality goals and some directions for future work on energy proportional storage in this section we identify the goals for a practical and effective energy proportional storage system we also examine existing work on energy aware storage and the extent to which they deliver on these goals design goals fine grained energy proportionality energy pro portional storage systems are uniquely characterized by multiple performance power levels true energy propor tionality requires that for a system with a peak power of ppeak for a workload intensity ρmax the power drawn for a workload intensity ρi would be ppeak ρi low space overhead replication based strategies could achieve energy proportionality trivially by repli cating each volume on all the other n volumes this would require n copies of each volume representing an unacceptable space overhead a practical energy propor table comparison of power management tech niques indicates the goal is partially addressed tional system should incur minimum space overhead for example additional space is often available reliability disk drives are designed to survive a lim ited number of spin up cycles energy conservation based on spinning down the disk must ensure that the additional number of spin up cycles induced during the disks expected lifetime is significantly lesser than the manufacturer specified maximum spin up cycles workload shift adaptation the popularity of data changes even if slowly over time power management for storage systems that rely on caching popular data over long intervals should address any shift in popular ity while ensuring energy proportionality heterogeneity support a data center is typically composed of several substantially different storage sys tems e g with variable numbers and types of drives an ideal energy proportional storage system should ac count for the differences in their performance power ra tios to provide the best performance at each host level examining existing solutions it has been shown that the idleness in storage workload is quite low for typical server workloads we ex amine several classes of related work that represent ap proaches to increase this idleness for power minimization and evaluate the extent to which they address our design goals we next discuss each of them and summarize their relative strengths in table singly redundant schemes the central idea used by these schemes is spinning down disks with redundant data during periods of low i o load ri mac uses memory level and on disk redundancy to reduce passive spin ups in systems enabling the spinning down of one out of the n disks in the array the diverted accesses technique generalizes this approach to find the best redundancy configuration for energy performance and reliability for all raid levels greenan et al propose generic techniques for manag ing power aware erasure coded storage systems the above techniques aim to support two energy levels and do not address fine grained energy proportionality geared raids paraid is a gear shifting mech anism each disk spun down represents a gear shift for a parity based raid to implement n gears in a n disk array with used storage x paraid requires o x log n space even if we ignore the space required for storing parity information diskgroup is a mod ification of raid that enables a subset of the disks in a mirror group to be activated as necessary both techniques incur large space overhead further they do not address heterogeneous storage systems composed of multiple volumes with varying i o workload intensities caching systems this class of work is mostly based on caching popular data on additional storage to spin down primary data drives maid an archival storage system optionally uses additional cache disks for replicating popular data to increase idle periods on the remaining disks pdc does not use additional disks but rather suggests migrating data between disks accord ing to popularity always keeping the most popular data on a few active disks exces uses a low end flash device for caching popular data and buffering writes to increase idle periods of disk drives lee et al sug gest augmenting raid systems with an ssd for a simi lar purpose a dedicated storage cache does not provide fine grained energy proportionality the storage system is able to save energy only when the i o load is low and can be served from the cache further these techniques do not account for the reliability impact of frequent disk spin up operations write offloading write off loading is an energy sav ing technique based on redirecting writes to alternate locations the authors of write offloading demonstrate that idle periods at a one minute granularity can be sig nificantly increased by off loading writes to a different volume the reliability impact due to frequent spin up cycles on a disk is a potential concern which the au thors acknowledge but leave as an open problem in con trast srcmap increases the idle periods substantially by off loading popular data reads in addition to the writes and thus more comprehensively addressing this impor tant concern another important question not addressed in the write off loading work is with multiple volumes which active volume should be treated as a write off loading target for each spun down volume srcmap addresses this question clearly with a formal process for identifying the set of active disks during each interval other techniques there are orthogonal classes of work that can either be used in conjunction with sr cmap or that address other target environments hiber nator uses drpm to create a multi tier hierar chy of futuristic multi speed disks the speed for each disk is set and data migrated across tiers as the workload changes pergamum is an archival storage system de signed to be energy efficient with techniques for reduc ing inter disk dependencies and staggering rebuild oper ations gurumurthi et al propose intra disk par allelism on high capacity drives to improve disk band table summary statistics of one week i o work load traces obtained from three different volumes mail web vm homes 500 hour figure variability in i o workload intensity width without increasing power consumption fi nally ganesh et al propose log structured striped writ ing on a disk array to increase the predictability of ac tive inactive spindles storage workload characteristics in this section we characterize the nature of i o access on servers using workloads from three production sys tems specifically looking for properties that help us in our goal of energy proportional storage the systems in clude an email server mail workload a virtual machine monitor running two web servers web vm workload and a file server homes workload the mail workload serves user inboxes for the entire computer science department at fiu the homes workload is that of a nfs server that serves the home directories for our re search group at fiu activities represent those of a typical researcher consisting of software development testing and experimentation the use of graph plotting software and technical document preparation finally the web vm workload is collected from a virtualized system that hosts two cs department web servers one hosting the depart ment s online course management system and the other hosting the department s web based email access portal in each system we collected i o traces downstream of an active page cache for a duration of three weeks average weekly statistics related to these workloads are summarized in table the first thing to note is that the weekly working sets unique accesses during a week is a small percentage of the total volume size this trend is consistent across all volumes and leads to our first observation observation the active data set for storage volumes is typically a small fraction of total used storage dynamic consolidation utilizes variability in i o workload intensity to increase or decrease the number of i m ii m iii m i h ii h iii h i w ii w iii w days days days days days days 60min figure overlap in daily working sets for the mail m homes h and web vm w workloads i reads and writes against working set ii reads against work ing set and iii reads against working set recently of floaded writes and recent missed reads active devices figure depicts large variability in i o workload intensity for each of the three workloads over time with as much as orders of magnitude between the lowest and highest workload intensity levels across time this highlights the potential of energy savings if the storage systems can be made energy proportional observation there is a significant variability in i o workload intensity on storage volumes based on our first two observations we hypothe size that there is room for powering down physical vol umes that are substantially under utilized by replicating a small active working set on other volumes which have the spare bandwidth to serve accesses to the powered down volumes this motivates sample and replicate in srcmap energy conservation is possible provided the corresponding working set replicas can serve most re quests to each powered down volume this would be true if working sets are largely stable we investigate the stability of the volume working sets in fig for three progressive definitions of the working set in the first scenario we compute the classical work ing set based on the last few days of access history in the second scenario we additionally assume that writes can be offloaded and mark all writes as hits in the third scenario we further expand the working set to include re cent writes and past missed reads for each scenario we compute the working set hits and misses for the follow ing day s workload and study the hit ratio with change in the length of history used to compute the working set we observe that the hit ratio progressively increases both across the scenarios and as we increase the history length leading us to conclude that data usage exhibits high tem poral locality and that the working set after including re cent accesses is fairly stable this leads to our third ob servation also observed earlier by leung et al observation data usage is highly skewed with more than of the working set consisting of some really popular data and recently accessed data the first three observations are the pillars behind the sample replicate and consolidate approach whereby we sample each volume for its working set replicate interval length figure distribution of read idle times these working sets on other volumes and consolidate i o workloads on proportionately fewer volumes dur ing periods of low load before designing a new system based on the above observations we study the suitabil ity of a simpler write offloading technique for building energy proportional storage systems write off loading is based on the observation that i o workloads are write dominated and simply off loading writes to a different volume can cause volumes to be idle for a substantial fraction for workloads in the original study of time while write off loading increases the fraction of idle time of volumes the distribution of idle time du rations due to write off loading raises an orthogonal but important concern if these idle time durations are short saving power requires frequent spinning down up of the volumes which degrades reliability of the disk drives figure depicts the read idle time distributions of the three workloads it is interesting to note that idle time durations for the homes and mail workloads are all less than or equal to minutes and for the web vm the ma jority are less than or equal to minutes are all are less than minutes observation the read idle time distribution periods of writes alone with no intervening read operations of i o workloads is dominated by small durations typically less than five minutes this observation implies that exploiting all read idleness for saving power will necessitate spinning up the disk at least times a day in the case of homes and mail and at least times in the case of web vm this can be a significant hurdle to reliability of the disk drives which typically have limited spin up cycles it is therefore important to develop new techniques that can substantially increase average read idle time durations background and rationale storage virtualization managers simplify storage man agement by enabling a uniform view of disparate stor age resources in a data center they export a storage controller interface allowing users to create logical vol umes or virtual disks vdisks and mount these on hosts the physical volumes managed by the physical storage controllers are available to the virtualization manager as managed disks mdisks entirely transparently to the hosts which only view the logical vdisk volumes a use ful property of the virtualization layer is the complete flexibility in allocation of mdisk extents to vdisks applying server consolidation principles to storage consolidation using virtualization would activate only the most energy efficient mdisks required to serve the ag gregate workload during any period t data from the other mdisks chosen to be spun down would first need to be migrated to active mdisks to effect the change while data migration is an expensive operation the ease with which virtual to physical mappings can be reconfigured provides an alternative approach a na ıve strategy fol lowing this approach could replicate data for each vdisk on all the mdisks and adapt to workload variations by dynamically changing the virtual to physical mappings to use only the selected mdisks during t unfortunately this strategy requires n times additional space for a n vdisk storage system an unacceptable space overhead srcmap intelligently uses the storage virtualization layer as an i o indirection mechanism to deliver a practi cally feasible energy proportional solution since it op erates at the storage virtualization manager it does not alter the basic redundancy based reliability properties of the underlying physical volumes which is determined by the respective physical volume e g raid controllers to maintain the redundancy level srcmap ensures that a volume is replicated on target volumes at the same raid level while we detail srcmap s design and al gorithms in subsequent sections and here we list the rationale behind srcmap s design decisions these design decisions together help to satisfy the design goals for an ideal energy proportional storage system multiple replica targets fine grained energy propor tionality requires the flexibility to increase or decrease the number of active physical volumes one at a time techniques that activate a fixed secondary device for each data volume during periods of low activity cannot provide the flexibility necessary to deactivate an arbi trary fraction of the physical volumes in srcmap we achieve this fine grained control by creating a primary mdisk for each vdisk and replicating only the working set of each vdisk on multiple secondary mdisks this ensures that a every volume can be offloaded to one of multiple targets and b each target can serve the i o workload for multiple vdisks during peak load each vdisk maps to its primary mdisk and all mdisks are ac tive however during periods of low activity srcmap selects a proportionately small subset of mdisks that can support the aggregate i o workload for all vdisks sampling creating multiple full replicas of vdisks is impractical drawing from observation sr cmap substantially reduces the space overhead of main taining multiple replicas by sampling only the working set for each vdisk and replicating it since the working set is typically small the space overhead is low ordered replica placement while sampling helps to reduce replica sizes substantially creating multiple replicas for each sample still induces space overhead in srcmap we observe that all replicas are not created equal for instance it is more beneficial to replicate a lightly loaded volume than a heavily loaded one which is likely to be active anyway similarly a large working set has greater space overhead srcmap chooses to create fewer replicas aiming to keep it active if possible as we shall formally demonstrate carefully ordering the replica placement helps to minimize the number of active disks for fine grained energy proportionality dynamic source to target mapping and dual data synchronization from observation we know that workloads can vary substantially over a period of time hence it is not possible to pre determine which volumes need to be active target replica selection for any volume being powered down therefore needs to be a dynamic decision and also needs to take into account that some volumes have more replicas or target choices than others we use two distinct mechanisms for updat ing the replica working sets the active replica lies in the data path and is immediately synchronized in the case of a read miss this ensures that the active replica contin uously adapts with change in workload popularity the secondary replicas on the other hand use a lazy incre mental data synchronization in the background between the primary replica and any secondary replicas present on active mdisks this ensures that switching between replicas requires minimal data copying and can be per formed fairly quickly coarse grained power cycling in contrast to most existing solutions that rely on fine grained disk power mode switching srcmap implements coarse grained consolidation intervals of the order of hours during each of which the set of active mdisks chosen by sr cmap does not change this ensures normal disk life times are realized by adhering to the disk power cycle specification contained within manufacturer data sheets design overview srcmap is built in a modular fashion to directly inter face with storage virtualization managers or be integrated into one as shown in figure the overall architecture supports the following distinct flows of control the replica generation flow flow a identifies the working set for each vdisk and replicates it on multiple mdisks this flow is orchestrated by the replica place ment controller and is triggered once when srcmap initialization reconfiguration time trigger load monitor the load monitor resides in the storage virtualization manager and records access to data on any of the vdisks exported by the virtualization layer it provides two inter faces for use by srcmap long term workload data in terface invoked by the replica placement controller and predicted short term workload data interface invoked by the active disk manager figure srcmap integrated into a storage vir tualization manager arrows depict control flow dashed solid boxes denote existing new components is initialized and whenever a configuration change e g addition of a new workload or new disks takes place once a trigger is generated the replica placement con troller obtains a historical workload trace from the load monitor and computes the working set and the long term workload intensity for each volume vdisk the work ing set is then replicated on one or more physical vol umes mdisks the blocks that constitute the working set for the vdisk and the target physical volumes where these are replicated are managed using a common data structure called the replica disk map rdm the active disk identification flow flow b identifies for a period t the active mdisks and activated repli cas for each inactive mdisk the flow is triggered at the beginning of the consolidation interval t e g every hours and orchestrated by the active disk manager in this flow the active disk manager queries the load monitor for expected workload intensity of each vdisk in the period t it then uses the workload information along with the placement of working set replicas on tar get mdisks to compute the set of active primary mdisks and a active secondary replica mdisk for each inactive primary mdisk it then directs the consistency manager to ensure that the data on any selected active primary or active secondary replica is current once consistency checks are made it updates the virtual to physical map ping to redirect the workload to the appropriate mdisk the i o redirection flow flow c is an extension of the i o processing in the storage virtualization manager and utilizes the built in virtual to physical re mapping support to direct requests to primaries or active repli cas further this flow ensures that the working set of each vdisk is kept up to date to ensure this whenever a request to a block not available in the active replica is made a replica miss event is generated on a replica miss the replica manager spin ups the primary mdisk to fetch the required block further it adds this new block to the working set of the vdisk in the rdm we next describe the key components of srcmap replica placement controller the replica placement controller orchestrates the pro cess of sampling identifying working sets for each vdisk and replicating on one or more target mdisks we use a conservative definition of working set that in cludes all the blocks that were accessed during a fixed duration configured as the minimum duration beyond which the hit ratio on the working set saturates conse quently we use days for mail days for homes and days for web vm workload fig the blocks that capture the working set for each vdisk and the mdisks where it is replicated are stored in the rdm the details of the parameters and methodology used within replica placement are described in section active disk manager the active disk manager orchestrates the consolidate step in srcmap the module takes as input the work load intensity for each vdisk and identifies if the primary mdisk can be spun down by redirecting the workload to one of the secondary mdisks hosting its replica once the target set of active mdisks and replicas are identified the active disk manager synchronizes the identified ac tive primaries or active secondary replicas and updates the virtual to physical mapping of the storage virtualiza tion manager so that i o requests to a vdisk could be redirected accordingly the active disk manager uses a consistency manager for the synchronization operation details of the algorithm used by active disk manager for selecting active mdisks are described in section consistency manager the consistency manager ensures that the primary mdisk and the replicas are consistent before an mdisk is spun down and a new replica activated the new active replica is made consistent with the previous one in order to ensure that the overhead during the re synchronization is minimal an incremental point in time pit relation ship e g flash copy in ibm svc is maintained between the active data either the primary mdisk or one of the active replicas and all other copies of the same data a go to sync operation is performed periodi cally between the active data and all its copies on active mdisks this ensures that when an mdisk is spun up or down the amount of data to be synchronized is small replica manager the replica manager ensures that the replica data set for a vdisk is able to mimic the working set of the vdisk over time if a data block unavailable at the active replica of the vdisk is read causing a replica miss the replica manager copies the block to the replica space assigned to v pi workingsetn v n mn w n primary data replica space the active replica and adds the block to the replica meta vdisks target mdisks data accordingly finally the replica manager uses a least recently used lru policy to evict an older block in case the replica space assigned to a replica is filled up if the active data set changes drastically there may be a large number of replica misses all these replica misses can be handled by a single spin up of the pri mary mdisk once all the data in the new working set is touched the primary mdisk can be spun down as the active replica is now up to date the continuous updat ing of the replica metadata enables srcmap to meet the goal of workload shift adaptation without re running the expensive replica generation flow the replica gener ation flow needs to re run only when a disruptive change occurs such as addition of a new workload or a new vol ume or new disks to a volume algorithms and optimizations figure replica placement model active mdisks at time t the replica placement algorithm consists of i creat ing an initial ordering of vdisks in terms of cost benefit tradeoff ii a bipartite graph creation that reflects this ordering iii iteratively creating one source target map ping respecting the current order and iv re calibration of edge weights to ensure the ordering property holds for the next iteration of source target mapping initial vdisk ordering the initial vdisk ordering creates a sorted order amongst vdisks based on their cost benefit tradeoff for each vdisk vi we compute the probability pi that its primary mdisk mi would be spun down as smin p rmin w f mmin p in this section we present details about the algorithms i employed by srcmap we first present the long term w si p p ri ρi mi replica placement methodology and subsequently the short term active disk identification method replica placement algorithm the replica placement controller creates one or more replicas of the working set of each vdisk on the available replica space on the target mdisks we use the insight that all replicas are not created equal and have distinct associated costs and benefits the space cost of creating the replica is lower if the vdisk has a smaller working set similarly the benefit of creating a replica is higher if the vdisk i has a stable working set lower misses if the primary mdisk is switched off ii has a small average load making it easy to find spare bandwidth for it on any target mdisk and iii is hosted on a less power efficient primary mdisk hence the goal of both replica placement and active disk identification is to ensure that we create more replicas for vdisks that have a favorable cost benefit ratio the goal of the replica placement is to ensure that if the active disk manager decides to spin down the primary mdisk of a vdisk it should be able to find at least one active target mdisk that hosts its replica captured in the following ordering property definition ordering property for any two vdisks vi and vj if vi is more likely to require a replica target than vj at any time t during active disk identification then vi is more likely than vj to find a replica target amongst where the wk are tunable weights w si is the size of the working set of vi p p ri is the performance power ratio ratio between the peak io bandwidth and peak power for the primary mdisk mi of vi ρi is the average long term i o workload intensity measured in iops for vi and mi is the number of read misses in the working set of vi normalized by the number of spindles used by its primary mdisk mi the corresponding min subscript terms represent the minimum values across all the vdisks and provide normalization the probability formulation is based on the dual rationale that it is relatively easier to find a target mdisk for a smaller workload and switch ing off relatively more power hungry disks saves more power further we assign a higher probability for spin ning down mdisks that host more stable working sets by accounting for the number of times a read request can not be served from the replicated working set thereby necessitating the spinning up of the primary mdisk bipartite graph creation replica placement creates a bipartite graph g v m with each vdisk as a source node vi its primary mdisk as a target node mi and the edge weights e vi mj rep resenting the cost benefit trade off of placing a replica of vi on mj fig the nodes in the bipartite graph are sorted using pi disks with larger pi are at the top we initialize the edge weights wi j pi for each edge e vi mj source target pair initially there are no pi vk vk inactive mdisks workload redirection mk mk active mdisks vn mn figure active disk identification replica assignments made to any target mdisk the replica placement algorithm iterates through the follow ing two steps until all the available replica space on the target mdisks have been assigned to source vdisk repli cas in each iteration exactly one target mdisk s replica space is assigned source target mapping the goal of the replica placement method is to achieve a source target mapping that achieves the ordering prop erty to achieve this goal the algorithm takes the top most target mdisk mi whose replica space is not yet assigned and selects the set of highest weight incident edges such that the combined replica size of the source nodes in this set fills up the replica space available in mi e g the working sets of and vn are replicated in the replica space of in fig when the replica space on a target mdisk is filled up we mark the target mdisk as assigned one may observe that this procedure always gives preference to source nodes with a larger pi once an mdisk finds a replica the likelihood of it requiring another replica decreases and we factor this using a re calibration of edge weights which is detailed next re calibration of edge weights we observe that the initial assignments of weights en sure the ordering property however once the work ing set of a vdisk vi has been replicated on a set of tar get mdisks ti mleast mleast is the mdisk with the least pi in ti s t pi pleast the probability that vi would require a new target mdisk during active disk identification is the probability that both mi and mleast would be spun down hence to preserve the or dering property we re calibrate the edge weights of all outgoing edges of any primary mdisks si assigned to target mdisks tj as k wi k pjpi once the weights are recomputed we iterate from the source target mapping step until all the replicas have been assigned to target mdisks one may observe that the re calibration succeeds in achieving the ordering property because we start assigning the replica space for the top most target mdisks first this allows us to in crease the weights of source nodes monotonically as we figure active replica identification algorithm place more replicas of its working set we formally prove the following result in the appendix theorem the replica placement algorithm ensures ordering property active disk identification we now describe the methodology employed to identify the set of active mdisks and replicas at any given time for ease of exposition we define the probability pi of a primary mdisk mi equal to the probability pi of its vdisk vi active disk identification consists of i active mdisk selection we first estimate the expected aggregate workload to the storage subsystem in the next interval we use the workload to a vdisk in the previ ous interval as the predicted workload in the next interval for the vdisk the aggregate workload is then estimated as sum of the predicted workloads for all vdisks in the storage system this aggregate workload is then used to identify the minimum subset of mdisks ordered by re verse of pi such that the aggregate bandwidth of these mdisks exceeds the expected aggregate load ii active replica identification this step elaborated shortly identifies one of the many possible replicas on an active mdisk for each inactive mdisk to serve the workload redirected from the inactive mdisk iii iterate if the active replica identification step suc ceeds in finding an active replica for all the inactive mdisks the algorithm terminates else the number of active mdisks are increased by and the algorithm re peats the active replica identification step one may note that since the number of active disks are based on the maximum predicted load in a consoli dation interval a sudden increase in load may lead to an increase in response times if performance degradation beyond user defined acceptable levels persists beyond a user defined interval e g mins the active disk iden tification is repeated for the new load active replica identification fig depicts the high level goal of active replica identification which is to have the primary mdisks for vdisks with larger pi spun down and their workload directed to few mdisks with smaller pi to do so it must identify an active replica for each inactive primary mdisk on one of the active mdisks the algorithm uses two insights i the replica placement process creates more replicas for vdisks with a higher probability of be ing spun down pi and ii primary mdisks with larger pi are likely to be spun down for a longer time to utilize the first insight we first allow primary mdisks with small pi which are marked as inactive to find an active replica as they have fewer choices avail able to utilize the second insight we force inactive pri mary mdisks with large pi to use a replica on active mdisks with small pi for example in fig vdisk vk has the first choice of finding an active mdisk that hosts its replica and in this case it is able to select the first active mdisk mk as a result inactive mdisks with larger pi are mapped to active mdisks with the smaller pi e g is mapped to mn since an mdisk with the smallest pi is likely to remain active most of the time this ensures that there is little to no need to switch active replicas frequently for the inactive disks the details of this methodology are described in fig key optimizations to basic srcmap we augment the basic srcmap algorithm to increase its practical usability and effectiveness as follows sub volume creation srcmap redirects the workload for any primary mdisk that is spun down to exactly one target mdisk hence a target mdisk mj for a primary mdisk mi needs to support the combined load of the vdisks vi and vj in order to be selected with this requirement the sr cmap consolidation process may incur a fragmentation of the available i o bandwidth across all volumes to elaborate consider an example scenario with iden tical mdisks each with capacity c and input load of c δ note that even though this load can be served using mdisks there is no single mdisk can support the input load of vdisks to avoid such a scenario srcmap sub divides each mdisk into nsv sub volumes and identifies the working set for each sub volume separately the sub replicas working sets of a sub volume are then placed independently of each other on target mdisks with this optimization srcmap is able to subdivide the least amount of load that can be mi grated thereby dealing with the fragmentation problem in a straightforward manner this optimization requires a complementary modifi cation to thereplica placement algorithm the source target mapping step is modified to ensure that sub replicas belonging to the same source vdisk are not co located on a target mdisk scratch space for writes and missed reads srcmap incorporates the basic write off loading mech anism as proposed by narayanan et al the current implementation of srcmap uses an additional alloca tion of write scratch space with each sub replica to ab sorb new writes to the corresponding portion of the data volume a future optimization is to use a single write scratch space within each target mdisk rather than one per sub replica within the target mdisk so that the over head for absorbing writes can be minimized a key difference from write off loading however is that on a read miss for a spun down volume srcmap additionally offloads the data read to dynamically learn the working set this helps srcmap achieve the goal of workload shift adaptation with change in working set while write off loading uses the inter read miss dura tions exclusively for spin down operations srcmap tar gets capturing entire working sets including both reads and writes in replica locations to prolong read miss du rations to the order of hours and thus places more impor tance on learning changes in the working set evaluation in this section we evaluate srcmap using a prototype implementation of srcmap based storage virtualization manager and an energy simulator seeded by the proto type we investigate the following questions what degree of proportionality in energy consump tion and i o load can be achieved using srcmap how does srcmap impact reliability what is the impact of storage consolidation on the i o performance how sensitive are the energy savings to the amount of over provisioned space what is the overhead associated with implementing an srcmap indirection optimization workload the workloads used consist of i o requests to eight independent data volumes each mapped to an independent disk drive in practice volumes will likely comprise of more than one disk but resource restrictions did not allow us to create a more expansive testbed we argue that relative energy consumption results still hold despite this approximation these volumes support a mix of production web servers from the fiu cs department data center end user homes data and our lab s subver sion svn and wiki servers as detailed in table workload i o statistics were obtained by running blk trace on each volume observe that there is a wide variance in their load intensity values creating opportu nities for consolidation across volumes storage testbed for experimental evaluation we set up a single machine intel pentium ht mem a table workload and storage system details traces figure logical view of experimental setup ory connected to disks via two sata ii controllers a and b the cumulative merged workload trace is played back using btreplay with each volume s trace played back to the corresponding disk all the disks share one power supply p that is dedicated only for the experimental drives the machine connects to another power supply the power supply p is connected to a watts up pro power meter which allows us to measure power consumption at a one second granularity with a resolution of an overhead of is intro duced by the power supply itself which we deduct from all our power measurements experimental setup we describe the experimental setup used in our evaluation study in fig we im plemented an srcmap module with its algorithms for replica placement and active disk identification during any consolidation interval an overall experimental run consists of using the monitored data to identify the consolidation candidates for each interval and create the virtual to physical mapping modify the original traces to reflect the mapping and replaying it and power and response time reporting at each consolida tion event the workload modifier generates the neces sary additional i o to synchronize data across the sub volumes affected due to active replica changes we evaluate srcmap using two different sets of ex periments i prototype runs and ii simulated runs the prototype runs evaluate srcmap against a real storage system and enable realistic measurements of power con sumption and impact to i o performance via the report ing module in a prototype run the modified i o work b table experimental settings a estimated disk iops capacity levels b storage system power con sumption in watts as the number of disks in active mode are varied from to all disks consumed ap proximately the same power when active the disks not in active mode consume standby power which was found to be the same across all disks load is replayed on the actual testbed using btreplay the simulator runs operate similarly on a simulated testbed wherein a power model instantiated with power measurements from the testbed is used for reporting the power numbers the advantage with the simulator is the ability to carry out longer duration experiments in sim ulated time as opposed to real time allowing us to ex plore the parameter space efficiently further one may use it to simulate various types of storage testbeds to study the performance under various load conditions in particular we use the simulator runs to evaluate energy proportionality by simulating the testbed with different values of disk iops capacity estimates we also simulate alternate power management techniques e g caching replication for a comparative evaluation all experiments with the prototype and the simula tor were performed with the following configuration pa rameters the consolidation interval was chosen to be hours for all experiments to restrict the worst case spin up cycles for the disk drives to an acceptable value two minute disk timeouts were used for inactive disks active disks within a consolidation interval remain continuously active working sets and replicas were created based on a three week workload history and we report results for a subsequent hour duration for brevity the consoli dation is based on an estimate of the disk iops capacity which varies for each volume we computed an estimate of the disk iops using a synthetic random i o workload for each volume separately level we use iops estimation levels through to a simulate storage testbeds at different load factors and b study the sen sitivity of srcmap with the volume iops estimation the per volume sustainable iops at each of these load levels is provided in table a the power consumption of the storage system with varying number of disks in active mode is presented in table b prototype results for the prototype evaluation we took the most dy namic hour period consolidation intervals from the 3 hour figure power and active disks time line 95 85 hours and played back i o traces for the work loads described earlier in real time we report actual power consumption and the i o response time which includes queuing and service time distribution for sr cmap when compared to a baseline configuration where 0 102 response time msec all disks are continuously active power consumption was measured every second and disk active standby state information was polled every seconds we used dif ferent iops levels when a very conservative low estimate of the disk iops capacity is made and when a reasonably aggressive high estimate is made we study the power savings due to srcmap in fig ure even using a conservative estimate of disk iops we are able to spin down approximately disks on an average leading to an average savings of using an aggressive estimate of disk iops sr cmap is able to spin down disks saving for all periods other than the period in the hr period it uses disks leading to a power savings of the spikes in the power consumption re late to planned and unplanned due to read misses vol ume activations which are few in number it is impor tant to note that substantial power is used in maintaining standby states and within the dynamic range the power savings due to srcmap are even higher we next investigate any performance penalty incurred due to consolidation fig upper depicts the cumula tive probability density function cdf of response times for three different configurations baseline on no consolidation and all disks always active srcmap us ing and the accuracy of the cdfs for and suffer from a reporting artifact that the cdfs include the latencies for the synchronization i os themselves which we were not able to filter out we throttle the synchro nization i os to one every to reduce their interfer ence with foreground operations first we observed that less than 0 003 of the re quests incurred a spin up hit due to read misses result ing in latencies of greater than seconds in both the and configurations not shown this implies that the working set dynamically updated with missed reads and offloaded writes is a fairly at capturing the active data for these workloads second we observe that for re sponse times greater than baseline on demon figure impact of consolidation on response time strates better performance than and upper plot for both and less than of requests incur la tencies greater than less than of requests in cur latencies greater than having more disks at its disposal shows slightly better response times than for response times lower than a reverse trend is observed wherein the srcmap configurations do better than baseline on we conjectured that this is due to the influence of the low latency writes during synchro nization operations to further delineate the influence of synchronization i os we performed two additional runs in the first run we disable all synchronization i os and in the second we disable all foreground i os lower plot the cdfs of only the synchronization operations which show a bi modal distribution with low latency writes absorbed by the disk buffer and reads with latencies greater than indicate that synchronization reads are con tributing towards the increased latencies in and for the upper plot the cdf without synchronization w o synch is much closer to baseline on with a decrease of approximately in the number of request with la tencies greater than intelligent scheduling of syn chronization i os is an important area of future work to further reduce the impact on foreground i o operations simulator results we conducted several experiments with simulated testbeds hosting disks of capacities to for brevity we report our observations for disk capacity levels and expanding to other levels only when required comparative evaluation we first demonstrate the basic energy proportionality achieved by srcmap in its most conservative config uration and three alternate solutions caching caching and replication caching is a scheme that uses additional physical volume as a cache if the ag gregate load observed is less than the iops capacity of srcmap baseline on load iops modified load iops power watts 0 0 0 8 20 hour figure power consumption remap operations and aggregate load across time for a single day 0 0 6 0 6 the cache volume the workload is redirected to the cache volume if the load is higher the original physical vol umes are used caching uses cache volumes in a sim ilar manner replication identifies pairs of physical vol umes with similar bandwidths and creates replica pairs where all the data on one volume is replicated on the other if the aggregate load to a pair is less than the iops capacity of one volume only one in the pair is kept ac tive else both volumes are kept active figure evaluates power consumption of all four so lutions by simulating the power consumed as volumes are spun up down over hour consolidation intervals it also presents the average load measured in iops within each consolidation interval in the case of sr cmap read misses are indicated by instantaneous power spikes which require activating an additional disk drive to avoid clutter we do not show the spikes due to read misses for the cache configurations we observe that each of solutions demonstrate varying degrees of energy proportionality across the intervals srcmap uni formly consumes the least amount of power across all in tervals and its power consumption is proportional to load replication also demonstrates good energy proportional ity but at a higher power consumption on an average the caching configurations are the least energy proportional with only two effective energy levels to work with we also observe that srcmap remaps i e changes the active replica for a minimal number of volumes ei ther 0 1 or during each consolidation interval in fact we found that for all durations the number of volumes be ing remapped equaled the change in the number of active physical volumes indicating that the number of synchro nization operations are kept to the minimum finally in our system with eight volumes caching 1 caching 2 and replication use and additional space respectively while as we shall show later sr cmap is able to deliver almost all its energy savings with just additional space next we investigate how srcmap modifies per volume activity and power consumption with an aggres sive configuration a configuration that demonstrated figure load and power consumption for each disk y ranges for all loads is 1 130 iops in log arithmic scale y ranges for power is 0 19 w interesting consolidation dynamics over the 2 hour consolidation intervals each row in figure is specific to one of the eight volumes through the left and center columns show the original and srcmap modified load iops for each volume the modified load were consolidated on disks and by srcmap note that disks and are continuously in standby mode is continuously in active mode throughout the hour duration while the remaining disks switched states more than once of these and were maintained in standby mode by srcmap but were spun up one or more times due to read misses to their replica volumes while was made active by srcmap for two of the consolidation intervals only we note that the number of spin up cycles did not ex ceed 6 for any physical volume during the hour pe riod thus not sacrificing reliability due to the reliability aware design of srcmap volumes marked as active consume power even when there is idleness over shorter sub interval durations for the right column power con sumption for each disk in either active mode or spun down is shown with spikes representing spin ups due to read misses in the volume s active replica further even if the working set changes drastically during an interval it only leads to a single spin up that services a large num ber of misses for example served approximately 104 misses in the single spin up it had to incur figure omitted due to lack of space we also note that summing up power consumption of individual volumes cannot be used to compute total power as per table b sensitivity with space overhead we evaluated the sensitivity of srcmap energy savings with the amount of over provisioned space to store vol ume working sets figure depicts the average power consumption of the entire storage system i e all eight volumes across a hour interval as the amount of over provisioned space is varied as a percentage of the total 25 15 20 25 overprovisioned space 45 30 25 0 10 20 30 40 50 60 90 load factor figure sensitivity to over provisioned space figure energy proportionality with load storage space for the load level we observe that sr cmap is able to deliver most of its energy savings with pressed as n s r for a storage virtualization man 10 space over provisioning and all savings with 20 hence we conclude that srcmap can deliver power sav ings with minimal replica space energy proportionality our next experiment evaluates the degree of energy pro portionality to the total load on the storage system de livered by srcmap for this experiment we examined the power consumption within each 2 hour consolida tion interval across the hour duration for each of the five load estimation levels through giving us 60 data points further we created a few higher load lev els below to study energy proportionality at high load as well each data point is characterized by an average power consumption value and a load factor value which is the observed average iops load as a percentage of the estimated iops capacity based on the load estima tion level across all the volumes figure presents the power consumption at each load factor even though the load factor is a continuous variable power consumption levels in srcmap are discrete one may note that sr cmap can only vary one volume at a time and hence the different power performance levels in srcmap differ by one physical volume we do observe that srcmap is able to achieve close to n level proportionality for a system with n volumes demonstrating a step wise lin ear increase in power levels with increasing load 7 3 resource overhead of srcmap the primary resource overhead in srcmap is the mem ory used by the replica metadata map of the replica manager this memory overhead depends on the size of the replica space maintained on each volume for storing both working sets and off loaded writes we maintain a per block map entry which consists of 5 bytes to point to the current active replica 4 additional bytes keep what replicas contain the last data version and 4 more bytes are used to handle the i os absorbed in the replica space write buffer making a total of bytes for each entry in the map if n is the number of volumes of size s with r space to store replicas then the worst case memory consumption is approximately equal to the map size ex ager that manages 10 volumes of total size each with a replica space allocation of 10 over provisioning the memory overhead is only 3 2gb eas ily affordable for a high end storage virtualization man ager conclusions and future work in this work we have proposed and evaluated srcmap a storage virtualization solution for energy proportional storage srcmap establishes the feasibility of an energy proportional storage system with fully flexible dynamic storage consolidation along the lines of server consoli dation where any virtual machine can be migrated to any physical server in the cluster srcmap is able to meet all the desired goals of fine grained energy proportionality low space overhead reliability workload shift adapta tion and heterogeneity support our work opens up several new directions for further research some of the most important modeling and op timization solutions that will improve a system like sr cmap are i new models that capture the performance impact of storage consolidation ii investigating the use of workload correlation between logical volumes dur ing consolidation and iii optimizing the scheduling of replica synchronization to minimize impact on fore ground i o chapter pointers and arrays a pointer is a variable that contains the address of a variable pointers are much used in c partly because they are sometimes the only way to express a computation and partly because they usually lead to more compact and efficient code than can be obtained in other ways pointers and arrays are closely related this chapter also explores this relationship and shows how to exploit it pointers have been lumped with the goto statement as a marvelous way to create impossible to understand programs this is certainly true when they are used carelessly and it is easy to create pointers that point somewhere unexpected with discipline however pointers can also be used to achieve clarity and simplicity this is the aspect that we will try to illustrate the main change in ansi c is to make explicit the rules about how pointers can be manipulated in effect mandating what good programmers already practice and good compilers already enforce in addition the type void pointer to void replaces char as the proper type for a generic pointer pointers and addresses let us begin with a simplified picture of how memory is organized a typical machine has an array of consecutively numbered or addressed memory cells that may be manipulated individually or in contiguous groups one common situation is that any byte can be a char a pair of one byte cells can be treated as a short integer and four adjacent bytes form a long a pointer is a group of cells often two or four that can hold an address so if c is a char and p is a pointer that points to it we could represent the situation this way the unary operator gives the address of an object so the statement p c assigns the address of c to the variable p and p is said to point to c the operator only applies to objects in memory variables and array elements it cannot be applied to expressions constants or register variables the unary operator is the indirection or dereferencing operator when applied to a pointer it accesses the object the pointer points to suppose that x and y are integers and ip is a pointer to int this artificial sequence shows how to declare a pointer and how to use and int x y z int ip ip is a pointer to int ip x ip now points to x y ip y is now ip x is now ip z ip now points to z the declaration of x y and z are what we ve seen all along the declaration of the pointer ip int ip is intended as a mnemonic it says that the expression ip is an int the syntax of the declaration for a variable mimics the syntax of expressions in which the variable might appear this reasoning applies to function declarations as well for example double dp atof char says that in an expression dp and atof have values of double and that the argument of atof is a pointer to char you should also note the implication that a pointer is constrained to point to a particular kind of object every pointer points to a specific data type there is one exception a pointer to void is used to hold any type of pointer but cannot be dereferenced itself we ll come back to it in section if ip points to the integer x then ip can occur in any context where x could so ip ip increments ip by the unary operators and bind more tightly than arithmetic operators so the assignment y ip takes whatever ip points at adds and assigns the result to y while ip increments what ip points to as do ip and ip the parentheses are necessary in this last example without them the expression would increment ip instead of what it points to because unary operators like and associate right to left finally since pointers are variables they can be used without dereferencing for example if iq is another pointer to int iq ip copies the contents of ip into iq thus making iq point to whatever ip pointed to pointers and function arguments since c passes arguments to functions by value there is no direct way for the called function to alter a variable in the calling function for instance a sorting routine might exchange two out of order arguments with a function called swap it is not enough to write swap a b where the swap function is defined as void swap int x int y wrong int temp temp x x y y temp because of call by value swap can t affect the arguments a and b in the routine that called it the function above swaps copies of a and b the way to obtain the desired effect is for the calling program to pass pointers to the values to be changed swap a b since the operator produces the address of a variable a is a pointer to a in swap itself the parameters are declared as pointers and the operands are accessed indirectly through them void swap int px int py interchange px and py int temp temp px px py py temp pictorially pointer arguments enable a function to access and change objects in the function that called it as an example consider a function getint that performs free format input conversion by breaking a stream of characters into integer values one integer per call getint has to return the value it found and also signal end of file when there is no more input these values have to be passed back by separate paths for no matter what value is used for eof that could also be the value of an input integer one solution is to have getint return the end of file status as its function value while using a pointer argument to store the converted integer back in the calling function this is the scheme used by scanf as well see section the following loop fills an array with integers by calls to getint int n array size getint int for n n size getint array n eof n each call sets array n to the next integer found in the input and increments n notice that it is essential to pass the address of array n to getint otherwise there is no way for getint to communicate the converted integer back to the caller our version of getint returns eof for end of file zero if the next input is not a number and a positive value if the input contains a valid number include ctype h int getch void void ungetch int getint get next integer from input into pn int getint int pn int c sign while isspace c getch skip white space if isdigit c c eof c c ungetch c it is not a number return sign c if c c c getch for pn isdigit c c getch pn pn c pn sign if c eof ungetch c return c throughout getint pn is used as an ordinary int variable we have also used getch and ungetch described in section so the one extra character that must be read can be pushed back onto the input exercise as written getint treats a or not followed by a digit as a valid representation of zero fix it to push such a character back on the input exercise write getfloat the floating point analog of getint what type does getfloat return as its function value pointers and arrays in c there is a strong relationship between pointers and arrays strong enough that pointers and arrays should be discussed simultaneously any operation that can be achieved by array subscripting can also be done with pointers the pointer version will in general be faster but at least to the uninitiated somewhat harder to understand the declaration int a defines an array of size that is a block of consecutive objects named a a a the notation a i refers to the i th element of the array if pa is a pointer to an integer declared as int pa then the assignment pa a sets pa to point to element zero of a that is pa contains the address of a now the assignment x pa will copy the contents of a into x if pa points to a particular element of an array then by definition pa points to the next element pa i points i elements after pa and pa i points i elements before thus if pa points to a pa refers to the contents of a pa i is the address of a i and pa i is the contents of a i these remarks are true regardless of the type or size of the variables in the array a the meaning of adding to a pointer and by extension all pointer arithmetic is that pa points to the next object and pa i points to the i th object beyond pa the correspondence between indexing and pointer arithmetic is very close by definition the value of a variable or expression of type array is the address of element zero of the array thus after the assignment pa a pa and a have identical values since the name of an array is a synonym for the location of the initial element the assignment pa a can also be written as pa a rather more surprising at first sight is the fact that a reference to a i can also be written as a i in evaluating a i c converts it to a i immediately the two forms are equivalent applying the operator to both parts of this equivalence it follows that a i and a i are also identical a i is the address of the i th element beyond a as the other side of this coin if pa is a pointer expressions might use it with a subscript pa i is identical to pa i in short an array and index expression is equivalent to one written as a pointer and offset there is one difference between an array name and a pointer that must be kept in mind a pointer is a variable so pa a and pa are legal but an array name is not a variable constructions like a pa and a are illegal when an array name is passed to a function what is passed is the location of the initial element within the called function this argument is a local variable and so an array name parameter is a pointer that is a variable containing an address we can use this fact to write another version of strlen which computes the length of a string strlen return length of string int strlen char int n for n n return n since is a pointer incrementing it is perfectly legal has no effect on the character string in the function that called strlen but merely increments strlen private copy of the pointer that means that calls like strlen hello world string constant strlen array char array strlen ptr char ptr all work as formal parameters in a function definition char and char are equivalent we prefer the latter because it says more explicitly that the variable is a pointer when an array name is passed to a function the function can at its convenience believe that it has been handed either an array or a pointer and manipulate it accordingly it can even use both notations if it seems appropriate and clear it is possible to pass part of an array to a function by passing a pointer to the beginning of the subarray for example if a is an array f a and f a both pass to the function f the address of the subarray that starts at a within f the parameter declaration can read f int arr or f int arr so as far as f is concerned the fact that the parameter refers to part of a larger array is of no consequence if one is sure that the elements exist it is also possible to index backwards in an array p p and so on are syntactically legal and refer to the elements that immediately precede p of course it is illegal to refer to objects that are not within the array bounds address arithmetic if p is a pointer to some element of an array then p increments p to point to the next element and p i increments it to point i elements beyond where it currently does these and similar constructions are the simples forms of pointer or address arithmetic c is consistent and regular in its approach to address arithmetic its integration of pointers arrays and address arithmetic is one of the strengths of the language let us illustrate by writing a rudimentary storage allocator there are two routines the first alloc n returns a pointer to n consecutive character positions which can be used by the caller of alloc for storing characters the second afree p releases the storage thus acquired so it can be re used later the routines are rudimentary because the calls to afree must be made in the opposite order to the calls made on alloc that is the storage managed by alloc and afree is a stack or last in first out the standard library provides analogous functions called malloc and free that have no such restrictions in section we will show how they can be implemented the easiest implementation is to have alloc hand out pieces of a large character array that we will call allocbuf this array is private to alloc and afree since they deal in pointers not array indices no other routine need know the name of the array which can be declared static in the source file containing alloc and afree and thus be invisible outside it in practical implementations the array may well not even have a name it might instead be obtained by calling malloc or by asking the operating system for a pointer to some unnamed block of storage the other information needed is how much of allocbuf has been used we use a pointer called allocp that points to the next free element when alloc is asked for n characters it checks to see if there is enough room left in allocbuf if so alloc returns the current value of allocp i e the beginning of the free block then increments it by n to point to the next free area if there is no room alloc returns zero afree p merely sets allocp to p if p is inside allocbuf define allocsize size of available space static char allocbuf allocsize storage for alloc static char allocp allocbuf next free position char alloc int n return pointer to n characters if allocbuf allocsize allocp n it fits allocp n return allocp n old p else not enough room return void afree char p free storage pointed to by p if p allocbuf p allocbuf allocsize allocp p in general a pointer can be initialized just as any other variable can though normally the only meaningful values are zero or an expression involving the address of previously defined data of appropriate type the declaration static char allocp allocbuf defines allocp to be a character pointer and initializes it to point to the beginning of allocbuf which is the next free position when the program starts this could also have been written static char allocp allocbuf since the array name is the address of the zeroth element the test if allocbuf allocsize allocp n it fits checks if there enough room to satisfy a request for n characters if there is the new value of allocp would be at most one beyond the end of allocbuf if the request can be satisfied alloc returns a pointer to the beginning of a block of characters notice the declaration of the function itself if not alloc must return some signal that there is no space left c guarantees that zero is never a valid address for data so a return value of zero can be used to signal an abnormal event in this case no space pointers and integers are not interchangeable zero is the sole exception the constant zero may be assigned to a pointer and a pointer may be compared with the constant zero the symbolic constant null is often used in place of zero as a mnemonic to indicate more clearly that this is a special value for a pointer null is defined in stdio h we will use null henceforth tests like if allocbuf allocsize allocp n it fits and if p allocbuf p allocbuf allocsize show several important facets of pointer arithmetic first pointers may be compared under certain circumstances if p and q point to members of the same array then relations like etc work properly for example p q is true if p points to an earlier element of the array than q does any pointer can be meaningfully compared for equality or inequality with zero but the behavior is undefined for arithmetic or comparisons with pointers that do not point to members of the same array there is one exception the address of the first element past the end of an array can be used in pointer arithmetic second we have already observed that a pointer and an integer may be added or subtracted the construction p n means the address of the n th object beyond the one p currently points to this is true regardless of the kind of object p points to n is scaled according to the size of the objects p points to which is determined by the declaration of p if an int is four bytes for example the int will be scaled by four pointer subtraction is also valid if p and q point to elements of the same array and p q then q p is the number of elements from p to q inclusive this fact can be used to write yet another version of strlen strlen return length of string int strlen char char p while p p return p in its declaration p is initialized to that is to point to the first character of the string in the while loop each character in turn is examined until the at the end is seen because p points to characters p advances p to the next character each time and p gives the number of characters advanced over that is the string length the number of characters in the string could be too large to store in an int the header stddef h defines a type that is large enough to hold the signed difference of two pointer values if we were being cautious however we would use for the return value of strlen to match the standard library version is the unsigned integer type returned by the sizeof operator pointer arithmetic is consistent if we had been dealing with floats which occupy more storage that chars and if p were a pointer to float p would advance to the next float thus we could write another version of alloc that maintains floats instead of chars merely by changing char to float throughout alloc and afree all the pointer manipulations automatically take into account the size of the objects pointed to the valid pointer operations are assignment of pointers of the same type adding or subtracting a pointer and an integer subtracting or comparing two pointers to members of the same array and assigning or comparing to zero all other pointer arithmetic is illegal it is not legal to add two pointers or to multiply or divide or shift or mask them or to add float or double to them or even except for void to assign a pointer of one type to a pointer of another type without a cast character pointers and functions a string constant written as i am a string is an array of characters in the internal representation the array is terminated with the null character so that programs can find the end the length in storage is thus one more than the number of characters between the double quotes perhaps the most common occurrence of string constants is as arguments to functions as in printf hello world n when a character string like this appears in a program access to it is through a character pointer printf receives a pointer to the beginning of the character array that is a string constant is accessed by a pointer to its first element string constants need not be function arguments if pmessage is declared as char pmessage then the statement pmessage now is the time assigns to pmessage a pointer to the character array this is not a string copy only pointers are involved c does not provide any operators for processing an entire string of characters as a unit there is an important difference between these definitions char amessage now is the time an array char pmessage now is the time a pointer amessage is an array just big enough to hold the sequence of characters and that initializes it individual characters within the array may be changed but amessage will always refer to the same storage on the other hand pmessage is a pointer initialized to point to a string constant the pointer may subsequently be modified to point elsewhere but the result is undefined if you try to modify the string contents we will illustrate more aspects of pointers and arrays by studying versions of two useful functions adapted from the standard library the first function is strcpy t which copies the string t to the string it would be nice just to say t but this copies the pointer not the characters to copy the characters we need a loop the array version first strcpy copy t to array subscript version void strcpy char char t int i i while i t i i for contrast here is a version of strcpy with pointers strcpy copy t to pointer version void strcpy char char t int i i while t t because arguments are passed by value strcpy can use the parameters and t in any way it pleases here they are conveniently initialized pointers which are marched along the arrays a character at a time until the that terminates t has been copied into in practice strcpy would not be written as we showed it above experienced c programmers would prefer strcpy copy t to pointer version void strcpy char char t while t this moves the increment of and t into the test part of the loop the value of t is the character that t pointed to before t was incremented the postfix doesn t change t until after this character has been fetched in the same way the character is stored into the old position before is incremented this character is also the value that is compared against to control the loop the net effect is that characters are copied from t to up and including the terminating as the final abbreviation observe that a comparison against is redundant since the question is merely whether the expression is zero so the function would likely be written as strcpy copy t to pointer version void strcpy char char t while t although this may seem cryptic at first sight the notational convenience is considerable and the idiom should be mastered because you will see it frequently in c programs the strcpy in the standard library string h returns the target string as its function value the second routine that we will examine is strcmp t which compares the character strings and t and returns negative zero or positive if is lexicographically less than equal to or greater than t the value is obtained by subtracting the characters at the first position where and t disagree strcmp return if t if t if t int strcmp char char t int i for i i t i i if i return return i t i the pointer version of strcmp strcmp return if t if t if t int strcmp char char t for t t if return return t since and are either prefix or postfix operators other combinations of and and occur although less frequently for example p decrements p before fetching the character that p points to in fact the pair of expressions p val push val onto stack val p pop top of stack into val are the standard idiom for pushing and popping a stack see section the header string h contains declarations for the functions mentioned in this section plus a variety of other string handling functions from the standard library exercise write a pointer version of the function strcat that we showed in chapter strcat t copies the string t to the end of exercise write the function strend t which returns if the string t occurs at the end of the string and zero otherwise exercise write versions of the library functions strncpy strncat and strncmp which operate on at most the first n characters of their argument strings for example strncpy t n copies at most n characters of t to full descriptions are in appendix b exercise rewrite appropriate programs from earlier chapters and exercises with pointers instead of array indexing good possibilities include getline chapters and atoi itoa and their variants chapters and reverse chapter and strindex and getop chapter pointer arrays pointers to pointers since pointers are variables themselves they can be stored in arrays just as other variables can let us illustrate by writing a program that will sort a set of text lines into alphabetic order a stripped down version of the unix program sort in chapter we presented a shell sort function that would sort an array of integers and in chapter we improved on it with a quicksort the same algorithms will work except that now we have to deal with lines of text which are of different lengths and which unlike integers can t be compared or moved in a single operation we need a data representation that will cope efficiently and conveniently with variable length text lines this is where the array of pointers enters if the lines to be sorted are stored end to end in one long character array then each line can be accessed by a pointer to its first character the pointers themselves can bee stored in an array two lines can be compared by passing their pointers to strcmp when two out of order lines have to be exchanged the pointers in the pointer array are exchanged not the text lines themselves this eliminates the twin problems of complicated storage management and high overhead that would go with moving the lines themselves the sorting process has three steps read all the lines of input sort them print them in order as usual it best to divide the program into functions that match this natural division with the main routine controlling the other functions let us defer the sorting step for a moment and concentrate on the data structure and the input and output the input routine has to collect and save the characters of each line and build an array of pointers to the lines it will also have to count the number of input lines since that information is needed for sorting and printing since the input function can only cope with a finite number of input lines it can return some illegal count like if too much input is presented the output routine only has to print the lines in the order in which they appear in the array of pointers include stdio h include string h define maxlines max lines to be sorted char lineptr maxlines pointers to text lines int readlines char lineptr int nlines void writelines char lineptr int nlines void qsort char lineptr int left int right sort input lines main int nlines number of input lines read if nlines readlines lineptr maxlines qsort lineptr nlines writelines lineptr nlines return else printf error input too big to sort n return define maxlen max length of any input line int getline char int char alloc int readlines read input lines int readlines char lineptr int maxlines int len nlines char p line maxlen nlines while len getline line maxlen if nlines maxlines p alloc len null return else line len delete newline strcpy p line lineptr nlines p return nlines writelines write output lines void writelines char lineptr int nlines int i for i i nlines i printf n lineptr i the function getline is from section the main new thing is the declaration for lineptr char lineptr maxlines says that lineptr is an array of maxlines elements each element of which is a pointer to a char that is lineptr i is a character pointer and lineptr i is the character it points to the first character of the i th saved text line since lineptr is itself the name of an array it can be treated as a pointer in the same manner as in our earlier examples and writelines can be written instead as writelines write output lines void writelines char lineptr int nlines while nlines printf n lineptr initially lineptr points to the first line each element advances it to the next line pointer while nlines is counted down with input and output under control we can proceed to sorting the quicksort from chapter needs minor changes the declarations have to be modified and the comparison operation must be done by calling strcmp the algorithm remains the same which gives us some confidence that it will still work qsort sort v left v right into increasing order void qsort char v int left int right int i last void swap char v int i int j if left right do nothing if array contains return fewer than two elements swap v left left right last left for i left i right i if strcmp v i v left swap v last i swap v left last qsort v left last qsort v last right similarly the swap routine needs only trivial changes swap interchange v i and v j void swap char v int i int j char temp temp v i v i v j v j temp since any individual element of v alias lineptr is a character pointer temp must be also so one can be copied to the other exercise rewrite readlines to store lines in an array supplied by main rather than calling alloc to maintain storage how much faster is the program multi dimensional arrays c provides rectangular multi dimensional arrays although in practice they are much less used than arrays of pointers in this section we will show some of their properties consider the problem of date conversion from day of the month to day of the year and vice versa for example march is the day of a non leap year and the day of a leap year let us define two functions to do the conversions converts the month and day into the day of the year and converts the day of the year into the month and day since this latter function computes two values the month and day arguments will be pointers m d sets m to and d to february these functions both need the same information a table of the number of days in each month thirty days hath september since the number of days per month differs for leap years and non leap years it easier to separate them into two rows of a two dimensional array than to keep track of what happens to february during computation the array and the functions for performing the transformations are as follows static char daytab 29 30 31 set day of year from month day int int year int month int day int i leap leap year year year for i i month i day daytab leap i return day set month day from day of year void int year int yearday int pmonth int pday int i leap leap year year year for i yearday daytab leap i i yearday daytab leap i pmonth i pday yearday recall that the arithmetic value of a logical expression such as the one for leap is either zero false or one true so it can be used as a subscript of the array daytab the array daytab has to be external to both and so they can both use it we made it char to illustrate a legitimate use of char for storing small non character integers daytab is the first two dimensional array we have dealt with in c a two dimensional array is really a one dimensional array each of whose elements is an array hence subscripts are written as daytab i j row col rather than daytab i j wrong other than this notational distinction a two dimensional array can be treated in much the same way as in other languages elements are stored by rows so the rightmost subscript or column varies fastest as elements are accessed in storage order an array is initialized by a list of initializers in braces each row of a two dimensional array is initialized by a corresponding sub list we started the array daytab with a column of zero so that month numbers can run from the natural to instead of to since space is not at a premium here this is clearer than adjusting the indices if a two dimensional array is to be passed to a function the parameter declaration in the function must include the number of columns the number of rows is irrelevant since what is passed is as before a pointer to an array of rows where each row is an array of ints in this particular case it is a pointer to objects that are arrays of ints thus if the array daytab is to be passed to a function f the declaration of f would be f int daytab it could also be f int daytab since the number of rows is irrelevant or it could be f int daytab which says that the parameter is a pointer to an array of integers the parentheses are necessary since brackets have higher precedence than without parentheses the declaration int daytab is an array of pointers to integers more generally only the first dimension subscript of an array is free all the others have to be specified section has a further discussion of complicated declarations exercise there is no error checking in or remedy this defect initialization of pointer arrays consider the problem of writing a function n which returns a pointer to a character string containing the name of the n th month this is an ideal application for an internal static array contains a private array of character strings and returns a pointer to the proper one when called this section shows how that array of names is initialized the syntax is similar to previous initializations return name of n th month char int n static char name illegal month january february march april may june july august september october november december return n n name name n the declaration of name which is an array of character pointers is the same as lineptr in the sorting example the initializer is a list of character strings each is assigned to the corresponding position in the array the characters of the i th string are placed somewhere and a pointer to them is stored in name i since the size of the array name is not specified the compiler counts the initializers and fills in the correct number pointers vs multi dimensional arrays newcomers to c are sometimes confused about the difference between a two dimensional array and an array of pointers such as name in the example above given the definitions int a int b then a and b are both syntactically legal references to a single int but a is a true two dimensional array int sized locations have been set aside and the conventional rectangular subscript calculation row col is used to find the element a row col for b however the definition only allocates pointers and does not initialize them initialization must be done explicitly either statically or with code assuming that each element of b does point to a twenty element array then there will be ints set aside plus ten cells for the pointers the important advantage of the pointer array is that the rows of the array may be of different lengths that is each element of b need not point to a twenty element vector some may point to two elements some to fifty and some to none at all although we have phrased this discussion in terms of integers by far the most frequent use of arrays of pointers is to store character strings of diverse lengths as in the function compare the declaration and picture for an array of pointers char name illegal month jan feb mar with those for a two dimensional array char aname illegal month jan feb mar exercise rewrite the routines and with pointers instead of indexing command line arguments in environments that support c there is a way to pass command line arguments or parameters to a program when it begins executing when main is called it is called with two arguments the first conventionally called argc for argument count is the number of command line arguments the program was invoked with the second argv for argument vector is a pointer to an array of character strings that contain the arguments one per string we customarily use multiple levels of pointers to manipulate these character strings the simplest illustration is the program echo which echoes its command line arguments on a single line separated by blanks that is the command echo hello world prints the output hello world by convention argv is the name by which the program was invoked so argc is at least if argc is there are no command line arguments after the program name in the example above argc is and argv argv and argv are echo hello and world respectively the first optional argument is argv and the last is argv argc additionally the standard requires that argv argc be a null pointer the first version of echo treats argv as an array of character pointers include stdio h echo command line arguments version main int argc char argv int i for i i argc i printf argv i i argc printf n return since argv is a pointer to an array of pointers we can manipulate the pointer rather than index the array this next variant is based on incrementing argv which is a pointer to pointer to char while argc is counted down include stdio h echo command line arguments version main int argc char argv while argc printf argv argc printf n return since argv is a pointer to the beginning of the array of argument strings incrementing it by argv makes it point at the original argv instead of argv each successive increment moves it along to the next argument argv is then the pointer to that argument at the same time argc is decremented when it becomes zero there are no arguments left to print alternatively we could write the printf statement as printf argc argv this shows that the format argument of printf can be an expression too as a second example let us make some enhancements to the pattern finding program from section if you recall we wired the search pattern deep into the program an obviously unsatisfactory arrangement following the lead of the unix program grep let us enhance the program so the pattern to be matched is specified by the first argument on the command line include stdio h include string h define maxline int getline char line int max find print lines that match pattern from arg main int argc char argv char line maxline int found if argc printf usage find pattern n else while getline line maxline if strstr line argv null printf line found return found the standard library function strstr t returns a pointer to the first occurrence of the string t in the string or null if there is none it is declared in string h the model can now be elaborated to illustrate further pointer constructions suppose we want to allow two optional arguments one says print all the lines except those that match the pattern the second says precede each printed line by its line number a common convention for c programs on unix systems is that an argument that begins with a minus sign introduces an optional flag or parameter if we choose x for except to signal the inversion and n number to request line numbering then the command find x npattern will print each line that doesn t match the pattern preceded by its line number optional arguments should be permitted in any order and the rest of the program should be independent of the number of arguments that we present furthermore it is convenient for users if option arguments can be combined as in find nx pattern here is the program include stdio h include string h define maxline int getline char line int max find print lines that match pattern from arg main int argc char argv char line maxline long lineno int c except number found while argc argv while c argv switch c case x except break case n number break default printf find illegal option c n c argc found break if argc printf usage find x n pattern n else while getline line maxline lineno if strstr line argv null except if number printf ld lineno printf line found return found argc is decremented and argv is incremented before each optional argument at the end of the loop if there are no errors argc tells how many arguments remain unprocessed and argv points to the first of these thus argc should be and argv should point at the pattern notice that argv is a pointer to an argument string so argv is its first character an alternate valid form would be argv because binds tighter than and the parentheses are necessary without them the expression would be taken as argv in fact that is what we have used in the inner loop where the task is to walk along a specific argument string in the inner loop the expression argv increments the pointer argv it is rare that one uses pointer expressions more complicated than these in such cases breaking them into two or three steps will be more intuitive exercise write the program expr which evaluates a reverse polish expression from the command line where each operator or operand is a separate argument for example expr evaluates exercise modify the program entab and detab written as exercises in chapter to accept a list of tab stops as arguments use the default tab settings if there are no arguments exercise extend entab and detab to accept the shorthand entab m n to mean tab stops every n columns starting at column m choose convenient for the user default behavior exercise write the program tail which prints the last n lines of its input by default n is set to let us say but it can be changed by an optional argument so that tail n prints the last n lines the program should behave rationally no matter how unreasonable the input or the value of n write the program so it makes the best use of available storage lines should be stored as in the sorting program of section not in a two dimensional array of fixed size pointers to functions in c a function itself is not a variable but it is possible to define pointers to functions which can be assigned placed in arrays passed to functions returned by functions and so on we will illustrate this by modifying the sorting procedure written earlier in this chapter so that if the optional argument n is given it will sort the input lines numerically instead of lexicographically a sort often consists of three parts a comparison that determines the ordering of any pair of objects an exchange that reverses their order and a sorting algorithm that makes comparisons and exchanges until the objects are in order the sorting algorithm is independent of the comparison and exchange operations so by passing different comparison and exchange functions to it we can arrange to sort by different criteria this is the approach taken in our new sort lexicographic comparison of two lines is done by strcmp as before we will also need a routine numcmp that compares two lines on the basis of numeric value and returns the same kind of condition indication as strcmp does these functions are declared ahead of main and a pointer to the appropriate one is passed to qsort we have skimped on error processing for arguments so as to concentrate on the main issues include stdio h include string h define maxlines max lines to be sorted char lineptr maxlines pointers to text lines int readlines char lineptr int nlines void writelines char lineptr int nlines void qsort void lineptr int left int right int comp void void int numcmp char char sort input lines main int argc char argv int nlines number of input lines read int numeric if numeric sort if argc strcmp argv n numeric if nlines readlines lineptr maxlines qsort void lineptr nlines int void void numeric numcmp strcmp writelines lineptr nlines return else printf input too big to sort n return in the call to qsort strcmp and numcmp are addresses of functions since they are known to be functions the is not necessary in the same way that it is not needed before an array name we have written qsort so it can process any data type not just character strings as indicated by the function prototype qsort expects an array of pointers two integers and a function with two pointer arguments the generic pointer type void is used for the pointer arguments any pointer can be cast to void and back again without loss of information so we can call qsort by casting arguments to void the elaborate cast of the function argument casts the arguments of the comparison function these will generally have no effect on actual representation but assure the compiler that all is well qsort sort v left v right into increasing order void qsort void v int left int right int comp void void int i last void swap void v int int if left right do nothing if array contains return fewer than two elements swap v left left right last left for i left i right i if comp v i v left swap v last i swap v left last qsort v left last comp qsort v last right comp the declarations should be studied with some care the fourth parameter of qsort is int comp void void which says that comp is a pointer to a function that has two void arguments and returns an int the use of comp in the line if comp v i v left is consistent with the declaration comp is a pointer to a function comp is the function and comp v i v left is the call to it the parentheses are needed so the components are correctly associated without them int comp void void wrong says that comp is a function returning a pointer to an int which is very different we have already shown strcmp which compares two strings here is numcmp which compares two strings on a leading numeric value computed by calling atof include stdlib h numcmp compare and numerically int numcmp char char double atof atof if return else if return else return the swap function which exchanges two pointers is identical to what we presented earlier in the chapter except that the declarations are changed to void void swap void v int i int j void temp temp v i v i v j v j temp a variety of other options can be added to the sorting program some make challenging exercises exercise modify the sort program to handle a r flag which indicates sorting in reverse decreasing order be sure that r works with n exercise add the option f to fold upper and lower case together so that case distinctions are not made during sorting for example a and a compare equal exercise add the d directory order option which makes comparisons only on letters numbers and blanks make sure it works in conjunction with f exercise add a field searching capability so sorting may bee done on fields within lines each field sorted according to an independent set of options the index for this book was sorted with df for the index category and n for the page numbers complicated declarations c is sometimes castigated for the syntax of its declarations particularly ones that involve pointers to functions the syntax is an attempt to make the declaration and the use agree it works well for simple cases but it can be confusing for the harder ones because declarations cannot be read left to right and because parentheses are over used the difference between int f f function returning pointer to int and int pf pf pointer to function returning int illustrates the problem is a prefix operator and it has lower precedence than so parentheses are necessary to force the proper association although truly complicated declarations rarely arise in practice it is important to know how to understand them and if necessary how to create them one good way to synthesize declarations is in small steps with typedef which is discussed in section as an alternative in this section we will present a pair of programs that convert from valid c to a word description and back again the word description reads left to right the first dcl is the more complex it converts a c declaration into a word description as in these examples char argv argv pointer to char int daytab daytab pointer to array of int int daytab daytab array of pointer to int void comp comp function returning pointer to void void comp comp pointer to function returning void char x x function returning pointer to array of pointer to function returning char char x x array of pointer to function returning pointer to array of char dcl is based on the grammar that specifies a declarator which is spelled out precisely in appendix a section this is a simplified form dcl optional direct dcl direct dcl name dcl direct dcl direct dcl optional size in words a dcl is a direct dcl perhaps preceded by a direct dcl is a name or a parenthesized dcl or a direct dcl followed by parentheses or a direct dcl followed by brackets with an optional size this grammar can be used to parse functions for instance consider this declarator pfa pfa will be identified as a name and thus as a direct dcl then pfa is also a direct dcl then pfa is recognized as a dcl so pfa is a direct dcl then pfa is a direct dcl and thus a dcl we can also illustrate the parse with a tree like this where direct dcl has been abbreviated to dir dcl the heart of the dcl program is a pair of functions dcl and dirdcl that parse a declaration according to this grammar because the grammar is recursively defined the functions call each other recursively as they recognize pieces of a declaration the program is called a recursive descent parser dcl parse a declarator void dcl void int ns for ns gettoken count ns dirdcl while ns strcat out pointer to dirdcl parse a direct declarator void dirdcl void int type if tokentype dcl dcl if tokentype printf error missing n else if tokentype name variable name strcpy name token else printf error expected name or dcl n while type gettoken parens type brackets if type parens strcat out function returning else strcat out array strcat out token strcat out of since the programs are intended to be illustrative not bullet proof there are significant restrictions on dcl it can only handle a simple data type line char or int it does not handle argument types in functions or qualifiers like const spurious blanks confuse it it doesn t do much error recovery so invalid declarations will also confuse it these improvements are left as exercises here are the global variables and the main routine include stdio h include string h include ctype h define maxtoken enum name parens brackets void dcl void void dirdcl void int gettoken void int tokentype type of last token char token maxtoken last token string char name maxtoken identifier name char datatype maxtoken data type char int etc char out main convert declaration to words while gettoken eof token on line strcpy datatype token is the datatype out dcl parse rest of line if tokentype n printf syntax error n printf n name out datatype return the function gettoken skips blanks and tabs then finds the next token in the input a token is a name a pair of parentheses a pair of brackets perhaps including a number or any other single character int gettoken void return next token int c getch void void ungetch int char p token while c getch c t if c if c getch strcpy token return tokentype parens else ungetch c return tokentype else if c for p c p getch p return tokentype brackets else if isalpha c for p c isalnum c getch p c p ungetch c return tokentype name else return tokentype c getch and ungetch are discussed in chapter going in the other direction is easier especially if we do not worry about generating redundant parentheses the program undcl converts a word description like x is a function returning a pointer to an array of pointers to functions returning char which we will express as x char to char x the abbreviated input syntax lets us reuse the gettoken function undcl also uses the same external variables as dcl does undcl convert word descriptions to declarations main int type char temp maxtoken while gettoken eof strcpy out token while type gettoken n if type parens type brackets strcat out token else if type sprintf temp out strcpy out temp else if type name sprintf temp token out strcpy out temp else printf invalid input at n token return exercise make dcl recover from input errors exercise modify undcl so that it does not add redundant parentheses to declarations exercise expand dcl to handle declarations with function argument types qualifiers like const and so on back to chapter index chapter back to chapter index chapter chapter structures a structure is a collection of one or more variables possibly of different types grouped together under a single name for convenient handling structures are called records in some languages notably pascal structures help to organize complicated data particularly in large programs because they permit a group of related variables to be treated as a unit instead of as separate entities one traditional example of a structure is the payroll record an employee is described by a set of attributes such as name address social security number salary etc some of these in turn could be structures a name has several components as does an address and even a salary another example more typical for c comes from graphics a point is a pair of coordinate a rectangle is a pair of points and so on the main change made by the ansi standard is to define structure assignment structures may be copied and assigned to passed to functions and returned by functions this has been supported by most compilers for many years but the properties are now precisely defined automatic structures and arrays may now also be initialized basics of structures let us create a few structures suitable for graphics the basic object is a point which we will assume has an x coordinate and a y coordinate both integers the two components can be placed in a structure declared like this struct point int x int y the keyword struct introduces a structure declaration which is a list of declarations enclosed in braces an optional name called a structure tag may follow the word struct as with point here the tag names this kind of structure and can be used subsequently as a shorthand for the part of the declaration in braces the variables named in a structure are called members a structure member or tag and an ordinary i e non member variable can have the same name without conflict since they can always be distinguished by context furthermore the same member names may occur in different structures although as a matter of style one would normally use the same names only for closely related objects a struct declaration defines a type the right brace that terminates the list of members may be followed by a list of variables just as for any basic type that is struct x y z is syntactically analogous to int x y z in the sense that each statement declares x y and z to be variables of the named type and causes space to be set aside for them a structure declaration that is not followed by a list of variables reserves no storage it merely describes a template or shape of a structure if the declaration is tagged however the tag can be used later in definitions of instances of the structure for example given the declaration of point above struct point pt defines a variable pt which is a structure of type struct point a structure can be initialized by following its definition with a list of initializers each a constant expression for the members struct maxpt an automatic structure may also be initialized by assignment or by calling a function that returns a structure of the right type a member of a particular structure is referred to in an expression by a construction of the form structure name member the structure member operator connects the structure name and the member name to print the coordinates of the point pt for instance printf d d pt x pt y or to compute the distance from the origin to pt double dist sqrt double dist sqrt double pt x pt x double pt y pt y structures can be nested one representation of a rectangle is a pair of points that denote the diagonally opposite corners struct rect struct point struct point the rect structure contains two point structures if we declare screen as struct rect screen then screen x refers to the x coordinate of the member of screen structures and functions the only legal operations on a structure are copying it or assigning to it as a unit taking its address with and accessing its members copy and assignment include passing arguments to functions and returning values from functions as well structures may not be compared a structure may be initialized by a list of constant member values an automatic structure may also be initialized by an assignment let us investigate structures by writing some functions to manipulate points and rectangles there are at least three possible approaches pass components separately pass an entire structure or pass a pointer to it each has its good points and bad points the first function makepoint will take two integers and return a point structure makepoint make a point from x and y components struct point makepoint int x int y struct point temp temp x x temp y y return temp notice that there is no conflict between the argument name and the member with the same name indeed the re use of the names stresses the relationship makepoint can now be used to initialize any structure dynamically or to provide structure arguments to a function struct rect screen struct point middle struct point makepoint int int screen makepoint screen makepoint xmax ymax middle makepoint screen x screen x screen y screen y the next step is a set of functions to do arithmetic on points for instance addpoints add two points struct addpoint struct point struct point x x y y return here both the arguments and the return value are structures we incremented the components in rather than using an explicit temporary variable to emphasize that structure parameters are passed by value like any others as another example the function ptinrect tests whether a point is inside a rectangle where we have adopted the convention that a rectangle includes its left and bottom sides but not its top and right sides ptinrect return if p in r if not int ptinrect struct point p struct rect r return p x r x p x r x p y r y p y r y this assumes that the rectangle is presented in a standard form where the coordinates are less than the coordinates the following function returns a rectangle guaranteed to be in canonical form define min a b a b a b define max a b a b a b canonrect canonicalize coordinates of rectangle struct rect canonrect struct rect r struct rect temp temp x min r x r x temp y min r y r y temp x max r x r x temp y max r y r y return temp if a large structure is to be passed to a function it is generally more efficient to pass a pointer than to copy the whole structure structure pointers are just like pointers to ordinary variables the declaration struct point pp says that pp is a pointer to a structure of type struct point if pp points to a point structure pp is the structure and pp x and pp y are the members to use pp we might write for example struct point origin pp pp origin printf origin is d d n pp x pp y the parentheses are necessary in pp x because the precedence of the structure member operator is higher then the expression pp x means pp x which is illegal here because x is not a pointer pointers to structures are so frequently used that an alternative notation is provided as a shorthand if p is a pointer to a structure then p member of structure refers to the particular member so we could write instead printf origin is d d n pp x pp y both and associate from left to right so if we have struct rect r rp r then these four expressions are equivalent r x rp x r x rp x the structure operators and together with for function calls and for subscripts are at the top of the precedence hierarchy and thus bind very tightly for example given the declaration struct int len char str p then p len increments len not p because the implied parenthesization is p len parentheses can be used to alter binding p len increments p before accessing len and p len increments p afterward this last set of parentheses is unnecessary in the same way p str fetches whatever str points to p str increments str after accessing whatever it points to just like p str increments whatever str points to and p str increments p after accessing whatever str points to arrays of structures consider writing a program to count the occurrences of each c keyword we need an array of character strings to hold the names and an array of integers for the counts one possibility is to use two parallel arrays keyword and keycount as in char keyword nkeys int keycount nkeys but the very fact that the arrays are parallel suggests a different organization an array of structures each keyword is a pair char word int cout and there is an array of pairs the structure declaration struct key char word int count keytab nkeys declares a structure type key defines an array keytab of structures of this type and sets aside storage for them each element of the array is a structure this could also be written struct key char word int count struct key keytab nkeys since the structure keytab contains a constant set of names it is easiest to make it an external variable and initialize it once and for all when it is defined the structure initialization is analogous to earlier ones the definition is followed by a list of initializers enclosed in braces struct key char word int count keytab auto break case char const continue default unsigned void volatile while the initializers are listed in pairs corresponding to the structure members it would be more precise to enclose the initializers for each row or structure in braces as in auto break case but inner braces are not necessary when the initializers are simple variables or character strings and when all are present as usual the number of entries in the array keytab will be computed if the initializers are present and the is left empty the keyword counting program begins with the definition of keytab the main routine reads the input by repeatedly calling a function getword that fetches one word at a time each word is looked up in keytab with a version of the binary search function that we wrote in chapter the list of keywords must be sorted in increasing order in the table include stdio h include ctype h include string h define maxword int getword char int int binsearch char struct key int count c keywords main int n char word maxword while getword word maxword eof if isalpha word if n binsearch word keytab nkeys keytab n count for n n nkeys n if keytab n count printf n keytab n count keytab n word return binsearch find word in tab tab n int binsearch char word struct key tab int n int cond int low high mid low high n while low high mid low high if cond strcmp word tab mid word high mid else if cond low mid else return mid return we will show the function getword in a moment for now it suffices to say that each call to getword finds a word which is copied into the array named as its first argument the quantity nkeys is the number of keywords in keytab although we could count this by hand it a lot easier and safer to do it by machine especially if the list is subject to change one possibility would be to terminate the list of initializers with a null pointer then loop along keytab until the end is found but this is more than is needed since the size of the array is completely determined at compile time the size of the array is the size of one entry times the number of entries so the number of entries is just size of keytab size of struct key c provides a compile time unary operator called sizeof that can be used to compute the size of any object the expressions sizeof object and sizeof type name yield an integer equal to the size of the specified object or type in bytes strictly sizeof produces an unsigned integer value whose type is defined in the header stddef h an object can be a variable or array or structure a type name can be the name of a basic type like int or double or a derived type like a structure or a pointer in our case the number of keywords is the size of the array divided by the size of one element this computation is used in a define statement to set the value of nkeys define nkeys sizeof keytab sizeof struct key another way to write this is to divide the array size by the size of a specific element define nkeys sizeof keytab sizeof keytab this has the advantage that it does not need to be changed if the type changes a sizeof can not be used in a if line because the preprocessor does not parse type names but the expression in the define is not evaluated by the preprocessor so the code here is legal now for the function getword we have written a more general getword than is necessary for this program but it is not complicated getword fetches the next word from the input where a word is either a string of letters and digits beginning with a letter or a single non white space character the function value is the first character of the word or eof for end of file or the character itself if it is not alphabetic getword get next word or character from input int getword char word int lim int c getch void void ungetch int char w word while isspace c getch if c eof w c if isalpha c w return c for lim w if isalnum w getch ungetch w break w return word getword uses the getch and ungetch that we wrote in chapter when the collection of an alphanumeric token stops getword has gone one character too far the call to ungetch pushes that character back on the input for the next call getword also uses isspace to skip whitespace isalpha to identify letters and isalnum to identify letters and digits all are from the standard header ctype h exercise our version of getword does not properly handle underscores string constants comments or preprocessor control lines write a better version pointers to structures to illustrate some of the considerations involved with pointers to and arrays of structures let us write the keyword counting program again this time using pointers instead of array indices the external declaration of keytab need not change but main and binsearch do need modification include stdio h include ctype h include string h define maxword int getword char int struct key binsearch char struct key int count c keywords pointer version main char word maxword struct key p while getword word maxword eof if isalpha word if p binsearch word keytab nkeys null p count for p keytab p keytab nkeys p if p count printf n p count p word return binsearch find word in tab tab n struct key binsearch char word struck key tab int n int cond struct key low tab struct key high tab n struct key mid while low high mid low high low if cond strcmp word mid word high mid else if cond low mid else return mid return null there are several things worthy of note here first the declaration of binsearch must indicate that it returns a pointer to struct key instead of an integer this is declared both in the function prototype and in binsearch if binsearch finds the word it returns a pointer to it if it fails it returns null second the elements of keytab are now accessed by pointers this requires significant changes in binsearch the initializers for low and high are now pointers to the beginning and just past the end of the table the computation of the middle element can no longer be simply mid low high wrong because the addition of pointers is illegal subtraction is legal however so high low is the number of elements and thus mid low high low sets mid to the element halfway between low and high the most important change is to adjust the algorithm to make sure that it does not generate an illegal pointer or attempt to access an element outside the array the problem is that tab and tab n are both outside the limits of the array tab the former is strictly illegal and it is illegal to dereference the latter the language definition does guarantee however that pointer arithmetic that involves the first element beyond the end of an array that is tab n will work correctly in main we wrote for p keytab p keytab nkeys p if p is a pointer to a structure arithmetic on p takes into account the size of the structure so p increments p by the correct amount to get the next element of the array of structures and the test stops the loop at the right time don t assume however that the size of a structure is the sum of the sizes of its members because of alignment requirements for different objects there may be unnamed holes in a structure thus for instance if a char is one byte and an int four bytes the structure struct char c int i might well require eight bytes not five the sizeof operator returns the proper value finally an aside on program format when a function returns a complicated type like a structure pointer as in struct key binsearch char word struct key tab int n the function name can be hard to see and to find with a text editor accordingly an alternate style is sometimes used struct key binsearch char word struct key tab int n this is a matter of personal taste pick the form you like and hold to it self referential structures suppose we want to handle the more general problem of counting the occurrences of all the words in some input since the list of words isn t known in advance we can t conveniently sort it and use a binary search yet we can t do a linear search for each word as it arrives to see if it already been seen the program would take too long more precisely its running time is likely to grow quadratically with the number of input words how can we organize the data to copy efficiently with a list or arbitrary words one solution is to keep the set of words seen so far sorted at all times by placing each word into its proper position in the order as it arrives this shouldn t be done by shifting words in a linear array though that also takes too long instead we will use a data structure called a binary tree the tree contains one node per distinct word each node contains a pointer to the text of the word a count of the number of occurrences a pointer to the left child node a pointer to the right child node no node may have more than two children it might have only zero or one the nodes are maintained so that at any node the left subtree contains only words that are lexicographically less than the word at the node and the right subtree contains only words that are greater this is the tree for the sentence now is the time for all good men to come to the aid of their party as built by inserting each word as it is encountered to find out whether a new word is already in the tree start at the root and compare the new word to the word stored at that node if they match the question is answered affirmatively if the new record is less than the tree word continue searching at the left child otherwise at the right child if there is no child in the required direction the new word is not in the tree and in fact the empty slot is the proper place to add the new word this process is recursive since the search from any node uses a search from one of its children accordingly recursive routines for insertion and printing will be most natural going back to the description of a node it is most conveniently represented as a structure with four components struct tnode the tree node char word points to the text int count number of occurrences struct tnode left left child struct tnode right right child this recursive declaration of a node might look chancy but it correct it is illegal for a structure to contain an instance of itself but struct tnode left declares left to be a pointer to a tnode not a tnode itself occasionally one needs a variation of self referential structures two structures that refer to each other the way to handle this is struct t struct p p points to an struct struct t q q points to a t the code for the whole program is surprisingly small given a handful of supporting routines like getword that we have already written the main routine reads words with getword and installs them in the tree with addtree include stdio h include ctype h include string h define maxword struct tnode addtree struct tnode char void treeprint struct tnode int getword char int word frequency count main struct tnode root char word maxword root null while getword word maxword eof if isalpha word root addtree root word treeprint root return the function addtree is recursive a word is presented by main to the top level the root of the tree at each stage that word is compared to the word already stored at the node and is percolated down to either the left or right subtree by a recursive call to adtree eventually the word either matches something already in the tree in which case the count is incremented or a null pointer is encountered indicating that a node must be created and added to the tree if a new node is created addtree returns a pointer to it which is installed in the parent node struct tnode talloc void char strdup char addtree add a node with w at or below p struct treenode addtree struct tnode p char w int cond if p null a new word has arrived p talloc make a new node p word strdup w p count p left p right null else if cond strcmp w p word p count repeated word else if cond less than into left subtree p left addtree p left w else greater than into right subtree p right addtree p right w return p storage for the new node is fetched by a routine talloc which returns a pointer to a free space suitable for holding a tree node and the new word is copied into a hidden space by strdup we will discuss these routines in a moment the count is initialized and the two children are made null this part of the code is executed only at the leaves of the tree when a new node is being added we have unwisely omitted error checking on the values returned by strdup and talloc treeprint prints the tree in sorted order at each node it prints the left subtree all the words less than this word then the word itself then the right subtree all the words greater if you feel shaky about how recursion works simulate treeprint as it operates on the tree shown above treeprint in order print of tree p void treeprint struct tnode p if p null treeprint p left printf n p count p word treeprint p right a practical note if the tree becomes unbalanced because the words don t arrive in random order the running time of the program can grow too much as a worst case if the words are already in order this program does an expensive simulation of linear search there are generalizations of the binary tree that do not suffer from this worst case behavior but we will not describe them here before leaving this example it is also worth a brief digression on a problem related to storage allocators clearly it desirable that there be only one storage allocator in a program even though it allocates different kinds of objects but if one allocator is to process requests for say pointers to chars and pointers to struct tnodes two questions arise first how does it meet the requirement of most real machines that objects of certain types must satisfy alignment restrictions for example integers often must be located at even addresses second what declarations can cope with the fact that an allocator must necessarily return different kinds of pointers alignment requirements can generally be satisfied easily at the cost of some wasted space by ensuring that the allocator always returns a pointer that meets all alignment restrictions the alloc of chapter does not guarantee any particular alignment so we will use the standard library function malloc which does in chapter we will show one way to implement malloc the question of the type declaration for a function like malloc is a vexing one for any language that takes its type checking seriously in c the proper method is to declare that malloc returns a pointer to void then explicitly coerce the pointer into the desired type with a cast malloc and related routines are declared in the standard header stdlib h thus talloc can be written as include stdlib h talloc make a tnode struct tnode talloc void return struct tnode malloc sizeof struct tnode strdup merely copies the string given by its argument into a safe place obtained by a call on malloc char strdup char make a duplicate of char p p char malloc strlen for if p null strcpy p return p malloc returns null if no space is available strdup passes that value on leaving error handling to its caller storage obtained by calling malloc may be freed for re use by calling free see chapters and exercise write a program that reads a c program and prints in alphabetical order each group of variable names that are identical in the first characters but different somewhere thereafter don t count words within strings and comments make a parameter that can be set from the command line exercise write a cross referencer that prints a list of all words in a document and for each word a list of the line numbers on which it occurs remove noise words like the and and so on exercise write a program that prints the distinct words in its input sorted into decreasing order of frequency of occurrence precede each word by its count table lookup in this section we will write the innards of a table lookup package to illustrate more aspects of structures this code is typical of what might be found in the symbol table management routines of a macro processor or a compiler for example consider the define statement when a line like define in is encountered the name in and the replacement text are stored in a table later when the name in appears in a statement like state in it must be replaced by there are two routines that manipulate the names and replacement texts install t records the name and the replacement text t in a table and t are just character strings lookup searches for in the table and returns a pointer to the place where it was found or null if it wasn t there the algorithm is a hash search the incoming name is converted into a small non negative integer which is then used to index into an array of pointers an array element points to the beginning of a linked list of blocks describing names that have that hash value it is null if no names have hashed to that value a block in the list is a structure containing pointers to the name the replacement text and the next block in the list a null next pointer marks the end of the list struct nlist table entry struct nlist next next entry in chain char name defined name char defn replacement text the pointer array is just define hashsize static struct nlist hashtab hashsize pointer table the hashing function which is used by both lookup and install adds each character value in the string to a scrambled combination of the previous ones and returns the remainder modulo the array size this is not the best possible hash function but it is short and effective hash form hash value for string unsigned hash char unsigned hashval for hashval hashval 31 hashval return hashval hashsize unsigned arithmetic ensures that the hash value is non negative the hashing process produces a starting index in the array hashtab if the string is to be found anywhere it will be in the list of blocks beginning there the search is performed by lookup if lookup finds the entry already present it returns a pointer to it if not it returns null lookup look for in hashtab struct nlist lookup char struct nlist np for np hashtab hash np null np np next if strcmp np name return np found return null not found the for loop in lookup is the standard idiom for walking along a linked list for ptr head ptr null ptr ptr next install uses lookup to determine whether the name being installed is already present if so the new definition will supersede the old one otherwise a new entry is created install returns null if for any reason there is no room for a new entry struct nlist lookup char char strdup char install put name defn in hashtab struct nlist install char name char defn struct nlist np unsigned hashval if np lookup name null not found np struct nlist malloc sizeof np if np null np name strdup name null return null hashval hash name np next hashtab hashval hashtab hashval np else already there free void np defn free previous defn if np defn strdup defn null return null return np exercise write a function undef that will remove a name and definition from the table maintained by lookup and install exercise implement a simple version of the define processor i e no arguments suitable for use with c programs based on the routines of this section you may also find getch and ungetch helpful typedef c provides a facility called typedef for creating new data type names for example the declaration typedef int length makes the name length a synonym for int the type length can be used in declarations casts etc in exactly the same ways that the int type can be length len maxlen length lengths similarly the declaration typedef char string makes string a synonym for char or character pointer which may then be used in declarations and casts string p lineptr maxlines alloc int int strcmp string string p string malloc notice that the type being declared in a typedef appears in the position of a variable name not right after the word typedef syntactically typedef is like the storage classes extern static etc we have used capitalized names for typedefs to make them stand out as a more complicated example we could make typedefs for the tree nodes shown earlier in this chapter typedef struct tnode treeptr typedef struct tnode the tree node char word points to the text int count number of occurrences struct tnode left left child struct tnode right right child treenode this creates two new type keywords called treenode a structure and treeptr a pointer to the structure then the routine talloc could become treeptr talloc void return treeptr malloc sizeof treenode it must be emphasized that a typedef declaration does not create a new type in any sense it merely adds a new name for some existing type nor are there any new semantics variables declared this way have exactly the same properties as variables whose declarations are spelled out explicitly in effect typedef is like define except that since it is interpreted by the compiler it can cope with textual substitutions that are beyond the capabilities of the preprocessor for example typedef int pfi char char creates the type pfi for pointer to function of two char arguments returning int which can be used in contexts like pfi strcmp numcmp in the sort program of chapter besides purely aesthetic issues there are two main reasons for using typedefs the first is to parameterize a program against portability problems if typedefs are used for data types that may be machine dependent only the typedefs need change when the program is moved one common situation is to use typedef names for various integer quantities then make an appropriate set of choices of short int and long for each host machine types like and from the standard library are examples the second purpose of typedefs is to provide better documentation for a program a type called treeptr may be easier to understand than one declared only as a pointer to a complicated structure unions a union is a variable that may hold at different times objects of different types and sizes with the compiler keeping track of size and alignment requirements unions provide a way to manipulate different kinds of data in a single area of storage without embedding any machine dependent information in the program they are analogous to variant records in pascal as an example such as might be found in a compiler symbol table manager suppose that a constant may be an int a float or a character pointer the value of a particular constant must be stored in a variable of the proper type yet it is most convenient for table management if the value occupies the same amount of storage and is stored in the same place regardless of its type this is the purpose of a union a single variable that can legitimately hold any of one of several types the syntax is based on structures union int ival float fval char sval u the variable u will be large enough to hold the largest of the three types the specific size is implementation dependent any of these types may be assigned to u and then used in expressions so long as the usage is consistent the type retrieved must be the type most recently stored it is the programmer responsibility to keep track of which type is currently stored in a union the results are implementation dependent if something is stored as one type and extracted as another syntactically members of a union are accessed as union name member or union pointer member just as for structures if the variable utype is used to keep track of the current type stored in u then one might see code such as if utype int printf d n u ival if utype float printf f n u fval if utype string printf n u sval else printf bad type d in utype n utype unions may occur within structures and arrays and vice versa the notation for accessing a member of a union in a structure or vice versa is identical to that for nested structures for example in the structure array defined by struct char name int flags int utype union int ival float fval char sval u symtab nsym the member ival is referred to as symtab i u ival and the first character of the string sval by either of symtab i u sval symtab i u sval in effect a union is a structure in which all members have offset zero from the base the structure is big enough to hold the widest member and the alignment is appropriate for all of the types in the union the same operations are permitted on unions as on structures assignment to or copying as a unit taking the address and accessing a member a union may only be initialized with a value of the type of its first member thus union u described above can only be initialized with an integer value the storage allocator in chapter shows how a union can be used to force a variable to be aligned on a particular kind of storage boundary bit fields when storage space is at a premium it may be necessary to pack several objects into a single machine word one common use is a set of single bit flags in applications like compiler symbol tables externally imposed data formats such as interfaces to hardware devices also often require the ability to get at pieces of a word imagine a fragment of a compiler that manipulates a symbol table each identifier in a program has certain information associated with it for example whether or not it is a keyword whether or not it is external and or static and so on the most compact way to encode such information is a set of one bit flags in a single char or int the usual way this is done is to define a set of masks corresponding to the relevant bit positions as in define keyword define extrenal or define static enum keyword external static the numbers must be powers of two then accessing the bits becomes a matter of bit fiddling with the shifting masking and complementing operators that were described in chapter certain idioms appear frequently flags external static turns on the external and static bits in flags while flags external static turns them off and if flags external static is true if both bits are off although these idioms are readily mastered as an alternative c offers the capability of defining and accessing fields within a word directly rather than by bitwise logical operators a bit field or field for short is a set of adjacent bits within a single implementation defined storage unit that we will call a word for example the symbol table defines above could be replaced by the definition of three fields struct unsigned int unsigned int unsigned int flags this defines a variable table called flags that contains three bit fields the number following the colon represents the field width in bits the fields are declared unsigned int to ensure that they are unsigned quantities individual fields are referenced in the same way as other structure members flags flags etc fields behave like small integers and may participate in arithmetic expressions just like other integers thus the previous examples may be written more naturally as flags flags to turn the bits on flags flags to turn them off and if flags flags to test them almost everything about fields is implementation dependent whether a field may overlap a word boundary is implementation defined fields need not be names unnamed fields a colon and width only are used for padding the special width may be used to force alignment at the next word boundary fields are assigned left to right on some machines and right to left on others this means that although fields are useful for maintaining internally defined data structures the question of which end comes first has to be carefully considered when picking apart externally defined data programs that depend on such things are not portable fields may be declared only as ints for portability specify signed or unsigned explicitly they are not arrays and they do not have addresses so the operator cannot be applied on them whitepaper nvidia next generation cudatm compute architecture fermitm table of contents a brief history of gpu computing the architecture nvidia next generation cuda compute and graphics architecture code named fermi a quick refresher on cuda hardware execution an overview of the fermi architecture third generation streaming multiprocessor high performance cuda cores load store units four special function units designed for double precision 9 dual warp scheduler kb configurable shared memory and cache summary table second generation parallel thread execution isa unified address space enables full c support optimized for opencl and directcompute ieee bit floating point precision improved conditional performance through predication memory subsystem innovations nvidia parallel datacachetm with configurable and unified cache first gpu with ecc memory support fast atomic memory operations gigathreadtm thread scheduler faster application context switching concurrent kernel execution introducing nvidia nexus conclusion a brief history of gpu computing the graphics processing unit gpu first invented by nvidia in is the most pervasive parallel processor to date fueled by the insatiable desire for life like real time graphics the gpu has evolved into a processor with unprecedented floating point performance and programmability today gpus greatly outpace cpus in arithmetic throughput and memory bandwidth making them the ideal processor to accelerate a variety of data parallel applications efforts to exploit the gpu for non graphical applications have been underway since by using high level shading languages such as directx opengl and cg various data parallel algorithms have been ported to the gpu problems such as protein folding stock options pricing sql queries and mri reconstruction achieved remarkable performance speedups on the gpu these early efforts that used graphics apis for general purpose computing were known as gpgpu programs while the gpgpu model demonstrated great speedups it faced several drawbacks first it required the programmer to possess intimate knowledge of graphics apis and gpu architecture second problems had to be expressed in terms of vertex coordinates textures and shader programs greatly increasing program complexity third basic programming features such as random reads and writes to memory were not supported greatly restricting the programming model lastly the lack of double precision support until recently meant some scientific applications could not be run on the gpu to address these problems nvidia introduced two key technologies the unified graphics and compute architecture first introduced in geforce quadro fx and tesla gpus and cuda a software and hardware architecture that enabled the gpu to be programmed with a variety of high level programming languages together these two technologies represented a new way of using the gpu instead of programming dedicated graphics units with graphics apis the programmer could now write c programs with cuda extensions and target a general purpose massively parallel processor we called this new way of gpu programming gpu computing it signified broader application support wider programming language support and a clear separation from the early gpgpu model of programming the architecture nvidia geforce was the product that gave birth to the new gpu computing model introduced in november the based geforce brought several key innovations to gpu computing was the first gpu to support c allowing programmers to use the power of the gpu without having to learn a new programming language was the first gpu to replace the separate vertex and pixel pipelines with a single unified processor that executed vertex geometry pixel and computing programs was the first gpu to utilize a scalar thread processor eliminating the need for programmers to manually manage vector registers introduced the single instruction multiple thread simt execution model where multiple independent threads execute concurrently using a single instruction introduced shared memory and barrier synchronization for inter thread communication in june nvidia introduced a major revision to the architecture the second generation unified architecture first introduced in the geforce gtx quadro fx and tesla gpus increased the number of streaming processor cores subsequently referred to as cuda cores from to each processor register file was doubled in size allowing a greater number of threads to execute on chip at any given time hardware memory access coalescing was added to improve memory access efficiency double precision floating point support was also added to address the needs of scientific and high performance computing hpc applications when designing each new generation gpu it has always been the philosophy at nvidia to improve both existing application performance and gpu programmability while faster application performance brings immediate benefits it is the gpu relentless advancement in programmability that has allowed it to evolve into the most versatile parallel processor of our time it was with this mindset that we set out to develop the successor to the architecture nvidia next generation cuda compute and graphics architecture code named fermi the fermi architecture is the most significant leap forward in gpu architecture since the original was our initial vision of what a unified graphics and computing parallel processor should look like extended the performance and functionality of with fermi we have taken all we have learned from the two prior processors and all the applications that were written for them and employed a completely new approach to design to create the world first computational gpu when we started laying the groundwork for fermi we gathered extensive user feedback on gpu computing since the introduction of and and focused on the following key areas for improvement improve double precision performance while single precision floating point performance was on the order of ten times the performance of desktop cpus some gpu computing applications desired more double precision performance as well ecc support ecc allows gpu computing users to safely deploy large numbers of gpus in datacenter installations and also ensure data sensitive applications like medical imaging and financial options pricing are protected from memory errors true cache hierarchy some parallel algorithms were unable to use the gpu shared memory and users requested a true cache architecture to aid them more shared memory many cuda programmers requested more than kb of sm shared memory to speed up their applications faster context switching users requested faster context switches between application programs and faster graphics and compute interoperation faster atomic operations users requested faster read modify write atomic operations for their parallel algorithms with these requests in mind the fermi team designed a processor that greatly increases raw compute horsepower and through architectural innovations also offers dramatically increased programmability and compute efficiency the key architectural highlights of fermi are third generation streaming multiprocessor sm o cuda cores per sm over o the peak double precision floating point performance over o dual warp scheduler simultaneously schedules and dispatches instructions from two independent warps o kb of ram with a configurable partitioning of shared memory and cache second generation parallel thread execution isa o unified address space with full c support o optimized for opencl and directcompute o full ieee bit and bit precision o full bit integer path with bit extensions o memory access instructions to support transition to bit addressing o improved performance through predication improved memory subsystem o nvidia parallel datacachetm hierarchy with configurable and unified caches o first gpu with ecc memory support o greatly improved atomic memory operation performance nvidia gigathreadtm engine o faster application context switching o concurrent kernel execution o out of order thread block execution o dual overlapped memory transfer engines a quick refresher on cuda cuda is the hardware and software architecture that enables nvidia gpus to execute programs written with c c fortran opencl directcompute and other languages a cuda program calls parallel kernels a kernel executes in parallel across a set of parallel threads the programmer or compiler organizes these threads in thread blocks and grids of thread blocks the gpu instantiates a kernel program on a grid of parallel thread blocks each thread within a thread block executes an instance of the kernel and has a thread id within its thread block program counter registers per thread private memory inputs and output results a thread block is a set of concurrently executing threads that can cooperate among themselves through barrier synchronization and shared memory a thread block has a block id within its grid cuda hierarchy of threads blocks and grids with corresponding per thread private per block shared and per application global memory spaces a grid is an array of thread blocks that execute the same kernel read inputs from global memory write results to global memory and synchronize between dependent kernel calls in the cuda parallel programming model each thread has a per thread private memory space used for register spills function calls and c automatic array variables each thread block has a per block shared memory space used for inter thread communication data sharing and result sharing in parallel algorithms grids of thread blocks share results in global memory space after kernel wide global synchronization hardware execution cuda hierarchy of threads maps to a hierarchy of processors on the gpu a gpu executes one or more kernel grids a streaming multiprocessor sm executes one or more thread blocks and cuda cores and other execution units in the sm execute threads the sm executes threads in groups of threads called a warp while programmers can generally ignore warp execution for functional correctness and think of programming one thread they can greatly improve performance by having threads in a warp execute the same code path and access memory in nearby addresses an overview of the fermi architecture the first fermi based gpu implemented with billion transistors features up to cuda cores a cuda core executes a floating point or integer instruction per clock for a thread the cuda cores are organized in sms of cores each the gpu has six bit memory partitions for a bit memory interface supporting up to a total of gb of dram memory a host interface connects the gpu to the cpu via pci express the gigathread global scheduler distributes thread blocks to sm thread schedulers fermi s sm are positioned around a common cache each sm is a vertical rectangular strip that contain an orange portion scheduler and dispatch a green portion execution units and light blue portions register file and cache third generation streaming multiprocessor the third generation sm introduces several architectural innovations that make it not only the most powerful sm yet built but also the most programmable and efficient high performance cuda cores each sm features cuda processors a fourfold increase over prior sm designs each cuda processor has a fully pipelined integer arithmetic logic unit alu and floating result queue point unit fpu prior gpus used ieee floating point arithmetic the fermi architecture implements the new ieee floating point standard providing the fused multiply add fma instruction for both single and double precision arithmetic fma improves over a multiply add mad instruction by doing the multiplication and addition with a single final rounding step with no loss of precision in the addition fma is more accurate than performing the operations separately implemented double precision fma interconnect network fermi streaming multiprocessor sm in the integer alu was limited to bit precision for multiply operations as a result multi instruction emulation sequences were required for integer arithmetic in fermi the newly designed integer alu supports full bit precision for all instructions consistent with standard programming language requirements the integer alu is also optimized to efficiently support bit and extended precision operations various instructions are supported including boolean shift move compare convert bit field extract bit reverse insert and population count load store units each sm has load store units allowing source and destination addresses to be calculated for sixteen threads per clock supporting units load and store the data at each address to cache or dram four special function units special function units sfus execute transcendental instructions such as sin cosine reciprocal and square root each sfu executes one instruction per thread per clock a warp executes over eight clocks the sfu pipeline is decoupled from the dispatch unit allowing the dispatch unit to issue to other execution units while the sfu is occupied designed for double precision double precision arithmetic is at the heart of hpc applications such as linear algebra numerical simulation and quantum chemistry the fermi architecture has been specifically designed to offer unprecedented performance in double precision up to double precision fused multiply add operations can be performed per sm per clock a dramatic improvement over the architecture double precision application performance architecture fermi architecture double precision matrix multiply double precision tri diagonal solver early performance evaluations show fermi performing up to 2x faster than in double precision applications dual warp scheduler the sm schedules threads in groups of parallel threads called warps each sm features two warp schedulers and two instruction dispatch units allowing two warps to be issued and executed concurrently fermi s dual warp scheduler selects two warps and issues one instruction from each warp to a group of sixteen cores sixteen load store units or four sfus because warps execute independently fermi s scheduler does not need to check for dependencies from within the instruction stream using this elegant model of dual issue fermi achieves near peak hardware performance most instructions can be dual issued two integer instructions two floating instructions or a mix of integer floating point load store and sfu instructions can be issued concurrently double precision instructions do not support dual dispatch with any other operation kb configurable shared memory and cache one of the key architectural innovations that greatly improved both the programmability and performance of gpu applications is on chip shared memory shared memory enables threads within the same thread block to cooperate facilitates extensive reuse of on chip data and greatly reduces off chip traffic shared memory is a key enabler for many high performance cuda applications and have kb of shared memory per sm in the fermi architecture each sm has kb of on chip memory that can be configured as kb of shared memory with kb of cache or as kb of shared memory with kb of cache for existing applications that make extensive use of shared memory tripling the amount of shared memory yields significant performance improvements especially for problems that are bandwidth constrained for existing applications that use shared memory as software managed cache code can be streamlined to take advantage of the hardware caching system while still having access to at least kb of shared memory for explicit thread cooperation best of all applications that do not use shared memory automatically benefit from the cache allowing high performance cuda programs to be built with minimum time and effort summary table gpu fermi transistors million billion billion cuda cores double precision floating point capability none 30 fma ops clock fma ops clock single precision floating point capability mad ops clock mad ops clock fma ops clock warp schedulers per sm special function units sfus sm shared memory per sm kb kb configurable kb or kb cache per sm none none configurable kb or kb cache per sm none none kb ecc memory support no no yes concurrent kernels no no up to load store address width bit bit bit second generation parallel thread execution isa fermi is the first architecture to support the new parallel thread execution ptx instruction set ptx is a low level virtual machine and isa designed to support the operations of a parallel thread processor at program install time ptx instructions are translated to machine instructions by the gpu driver the primary goals of ptx are d provide a stable isa that spans multiple gpu generations d achieve full gpu performance in compiled applications d provide a machine independent isa for c c fortran and other compiler targets d provide a code distribution isa for application and middleware developers d provide a common isa for optimizing code generators and translators which map ptx to specific target machines d facilitate hand coding of libraries and performance kernels d provide a scalable programming model that spans gpu sizes from a few cores to many parallel cores ptx 0 introduces several new features that greatly improve gpu programmability accuracy and performance these include full ieee bit floating point precision unified address space for all variables and pointers bit addressing and new instructions for opencl and directcompute most importantly ptx 0 was specifically designed to provide full support for the c programming language unified address space enables full c support fermi and the ptx 0 isa implement a unified address space that unifies the three separate address spaces thread private local block shared and global for load and store operations in ptx 1 0 load store instructions were specific to one of the three address spaces programs could load or store values in a specific target address space known at compile time it was difficult to fully implement c and c pointers since a pointer s target address space may not be known at compile time and may only be determined dynamically at run time with ptx 0 a unified address space unifies all three address spaces into a single continuous address space a single set of unified load store instructions operate on this address space augmenting the three separate sets of load store instructions for local shared and global memory the bit unified address space supports a terabyte of addressable memory and the load store isa supports bit addressing for future growth the implementation of a unified address space enables fermi to support true c programs in c all variables and functions reside in objects which are passed via pointers ptx 0 makes it possible to use unified pointers to pass objects in any memory space and fermi s hardware address translation unit automatically maps pointer references to the correct memory space fermi and the ptx 0 isa also add support for c virtual functions function pointers and new and delete operators for dynamic object allocation and de allocation c exception handling operations try and catch are also supported optimized for opencl and directcompute opencl and directcompute are closely related to the cuda programming model sharing the key abstractions of threads thread blocks grids of thread blocks barrier synchronization per block shared memory global memory and atomic operations fermi a third generation cuda architecture is by nature well optimized for these apis in addition fermi offers hardware support for opencl and directcompute surface instructions with format conversion allowing graphics and compute programs to easily operate on the same data the ptx 0 isa also adds support for the directcompute instructions population count append and bit reverse ieee bit floating point precision single precision floating point instructions now support subnormal numbers by default in hardware as well as all four ieee rounding modes nearest zero positive infinity and negative infinity subnormal numbers are small numbers that lie between zero and the smallest normalized number of a given floating point number system prior generation gpus flushed subnormal operands and results to zero incurring a loss of accuracy cpus typically perform subnormal calculations in exception handling software taking thousands of cycles fermi s floating point units handle subnormal numbers in hardware allowing values to gradually underflow to zero with no performance penalty a frequently used sequence of operations in computer graphics linear algebra and scientific applications is to multiply two numbers adding the product to a third number for example d a b c prior generation gpus accelerated this function with the multiply add mad instruction that allowed both operations to be performed in a single clock the mad instruction performs a multiplication with truncation followed by an addition with round to nearest even fermi implements the new fused multiply add fma instruction for both bit single precision and bit double precision floating point numbers supported fma only in double precision that improves upon multiply add by retaining full precision in the intermediate stage the increase in precision benefits a number of algorithms such as rendering fine intersecting geometry greater precision in iterative mathematical calculations and fast exactly rounded division and square root operations improved conditional performance through predication in the fermi isa the native hardware predication support used for divergent thread management is now available at the instruction level predication enables short conditional code segments to execute efficiently with no branch instruction overhead memory subsystem innovations nvidia parallel datacachetm with configurable and unified cache working with hundreds of gpu computing applications from various industries we learned that while shared memory benefits many problems it is not appropriate for all problems some algorithms map naturally to shared memory others require a cache while others require a combination of both the optimal memory hierarchy should offer the benefits of both shared memory and cache and allow the programmer a choice over its partitioning the fermi memory hierarchy adapts to both types of program behavior adding a true cache hierarchy for load store operations presented significant challenges traditional gpu architectures support a read only load path for texture operations and a write only export path for pixel data output however this approach is poorly suited to executing general purpose c or c thread programs that expect reads and writes to be ordered as one example spilling a register operand to memory and then reading it back creates a read after write hazard if the read and write paths are separate it may be necessary to explicitly flush the entire write export path before it is safe to issue the read and any caches on the read path would not be coherent with respect to the write data the fermi architecture addresses this challenge by implementing a single unified memory request path for loads and stores with an cache per sm multiprocessor and unified cache that services all operations load store and texture the per sm cache is configurable to support both shared memory and caching of local and global memory operations the kb memory can be configured as either kb of shared memory with kb of cache or kb of shared memory with kb of cache when configured with kb of shared memory programs that make extensive use of shared memory such as electrodynamic simulations can perform up to three times faster for programs whose memory accesses are not known beforehand the kb cache configuration offers greatly improved performance over direct access to dram 400 250 150 0 radix sort using shared memory architecture fermi architecture in either configuration the cache also helps by caching temporary register spills of complex programs prior generation gpus spilled registers directly to dram increasing access latency with the cache performance scales gracefully with increased temporary register usage fermi features a kb unified cache that when using kb of shared memory on fermi radix sort executes 4 7x faster than physx fluid collision for convex shapes 250 150 services all load store and texture requests the provides efficient high speed data sharing across the gpu algorithms for which data addresses are not known beforehand such as physics solvers raytracing and sparse matrix multiplication especially benefit from the cache hierarchy filter and convolution kernels that require multiple sms to read the same data also benefit 0 architecture fermi architecture physics algorithms such as fluid simulations especially benefit from fermi s caches for convex shape collisions fermi is 7x faster than first gpu with ecc memory support fermi is the first gpu to support error correcting code ecc based protection of data in memory ecc was requested by gpu computing users to enhance data integrity in high performance computing environments ecc is a highly desired feature in areas such as medical imaging and large scale cluster computing naturally occurring radiation can cause a bit stored in memory to be altered resulting in a soft error ecc technology detects and corrects single bit soft errors before they affect the system because the probability of such radiation induced errors increase linearly with the number of installed systems ecc is an essential requirement in large cluster installations fermi supports single error correct double error detect secded ecc codes that correct any single bit error in hardware as the data is accessed in addition secded ecc ensures that all double bit errors and many multi bit errors are also be detected and reported so that the program can be re run rather than being allowed to continue executing with bad data fermi s register files shared memories caches cache and dram memory are ecc protected making it not only the most powerful gpu for hpc applications but also the most reliable in addition fermi supports industry standards for checking of data during transmission from chip to chip all nvidia gpus include support for the pci express standard for crc check with retry at the data link layer fermi also supports the similar standard for crc check with retry aka edc during transmission of data across the memory bus fast atomic memory operations atomic memory operations are important in parallel programming allowing concurrent threads to correctly perform read modify write operations on shared data structures atomic operations such as add min max and compare and swap are atomic in the sense that the read modify and write operations are performed without interruption by other threads atomic memory operations are widely used for parallel sorting reduction operations and building data structures in parallel without locks that serialize thread execution thanks to a combination of more atomic units in hardware and the addition of the cache atomic operations performance is up to faster in fermi compared to the generation gigathreadtm thread scheduler one of the most important technologies of the fermi architecture is its two level distributed thread scheduler at the chip level a global work distribution engine schedules thread blocks to various sms while at the sm level each warp scheduler distributes warps of threads to its execution units the first generation gigathread engine introduced in managed up to 288 threads in realtime the fermi architecture improves on this foundation by providing not only greater thread throughput but dramatically faster context switching concurrent kernel execution and improved thread block scheduling faster application context switching like cpus gpus support multitasking through the use of context switching where each program receives a time slice of the processor s resources the fermi pipeline is optimized to reduce the cost of an application context switch to below microseconds a significant improvement over last generation gpus besides improved performance this allows developers to create applications that take greater advantage of frequent kernel to kernel communication such as fine grained interoperation between graphics and physx applications concurrent kernel execution fermi supports concurrent kernel execution where different kernels of the same application context can execute on the gpu at the same time concurrent kernel execution allows programs that execute a number of small kernels to utilize the whole gpu for example a physx program may invoke a fluids solver and a rigid body solver which if executed sequentially would use only half of the available thread processors on the fermi architecture different kernels of the same cuda context can execute concurrently allowing maximum utilization of gpu resources kernels from different application contexts can still run sequentially with great efficiency thanks to the improved context switching performance serial kernel execution concurrent kernel execution introducing nvidia nexus nvidia nexus is the first development environment designed specifically to support massively parallel cuda c opencl and directcompute applications it bridges the productivity gap between cpu and gpu code by bringing parallel aware hardware source code debugging and performance analysis directly into microsoft visual studio the most widely used integrated application development environment under microsoft windows nexus allows visual studio developers to write and debug gpu source code using exactly the same tools and interfaces that are used when writing and debugging cpu code including source and data breakpoints and memory inspection furthermore nexus extends visual studio functionality by offering tools to manage massive parallelism such as the ability to focus and debug on a single thread out of the thousands of threads running parallel and the ability to simply and efficiently visualize the results computed by all parallel threads nexus is the perfect environment to develop co processing applications that take advantage of both the cpu and gpu it captures performance events and information across both processors and presents the information to the developer on a single correlated timeline this allows developers to see how their application behaves and performs on the entire system rather than through a narrow view that is focused on a particular subsystem or processor nvidia nexus integrated development environment conclusion for sixteen years nvidia has dedicated itself to building the world s fastest graphics processors while was a pioneering architecture in gpu computing and a major refinement their designs were nevertheless deeply rooted in the world of graphics the fermi architecture represents a new direction for nvidia far from being merely the successor to fermi is the outcome of a radical rethinking of the role purpose and capability of the gpu rather than taking the simple route of adding execution units the fermi team has tackled some of the toughest problems of gpu computing the importance of data locality is recognized through fermi s two level cache hierarchy and its combined load store memory path double precision performance is elevated to supercomputing levels while atomic operations execute up to twenty times faster lastly fermi s comprehensive ecc support strongly demonstrates our commitment to the high performance computing market on the software side the architecture brings forward support for c the world s most ubiquitous object orientated programming language and nexus the world s first integrated development environment designed for massively parallel gpu computing applications with its combination of ground breaking performance functionality and programmability the fermi architecture represents the next revolution in gpu computing notice all information provided in this white paper including commentary opinion nvidia design specifications reference boards files drawings diagnostics lists and other documents together and separately materials are being provided as is nvidia makes no warranties expressed implied statutory or otherwise with respect to materials and expressly disclaims all implied warranties of noninfringement merchantability and fitness for a particular purpose information furnished is believed to be accurate and reliable however nvidia corporation assumes no responsibility for the consequences of use of such information or for any infringement of patents or other rights of third parties that may result from its use no license is granted by implication or otherwise under any patent or patent rights of nvidia corporation specifications mentioned in this publication are subject to change without notice this publication supersedes and replaces all information previously supplied nvidia corporation products are not authorized for use as critical components in life support devices or systems without express written approval of nvidia corporation trademarks nvidia the nvidia logo cuda fermi and geforce are trademarks or registered trademarks of nvidia corporation in the united states and other countries other company and product names may be trademarks of the respective companies with which they are associated copyright nvidia corporation all rights reserved two or more common taken targets and at least one of those targets is correlated with branch history leading up to the branch then convert the indirect branch to a tree where one or more indirect branches are preceded by conditional branches to those targets apply this peeling procedure to the common target of an indirect branch that correlates to branch history the purpose of this rule is to reduce the total number of mispredictions by enhancing the predictability of branches even at the expense of adding more branches the added branches must be predictable for this to be worthwhile one reason for such predictability is a strong correlation with preceding branch history that is the direc tions taken on preceding branches are a good indicator of the direction of the branch under consideration example shows a simple example of the correlation between a target of a preceding conditional branch and a target of an indirect branch example indirect branch with two favored targets function int n rand random integer to if n n will be half the times n updates branch history to predict taken indirect branches with multiple taken targets may have lower prediction rates switch n case break common target correlated with branch history that is forward taken case break uncommon case break uncommon default common target correlation can be difficult to determine analytically for a compiler and for an assembly language programmer it may be fruitful to evaluate performance with and without peeling to get the best performance from a coding effort an example of peeling out the most favored target of an indirect branch with corre lated branch history is shown in example example a peeling technique to reduce indirect branch misprediction function int n rand random integer to if n then n n will be half the times if n then peel out the most common target with correlated branch history switch n case break uncommon case break uncommon default make the favored target in the fall through path loop unrolling benefits of unrolling loops are unrolling amortizes the branch overhead since it eliminates branches and some of the code to manage induction variables unrolling allows one to aggressively schedule or pipeline the loop to hide latencies this is useful if you have enough free registers to keep variables live as you stretch out the dependence chain to expose the critical path unrolling exposes the code to various other optimizations such as removal of redundant loads common subexpression elimination and so on the pentium processor can correctly predict the exit branch for an inner loop that has or fewer iterations if that number of iterations is predictable and there are no conditional branches in the loop so if the loop body size is not excessive and the probable number of iterations is known unroll inner loops until they have a maximum of iterations with the pentium m processor do not unroll loops having more than iterations the potential costs of unrolling loops are excessive unrolling or unrolling of very large loops can lead to increased code size this can be harmful if the unrolled loop no longer fits in the trace cache tc unrolling loops whose bodies contain branches increases demand on btb capacity if the number of iterations of the unrolled loop is or fewer the branch predictor should be able to correctly predict branches in the loop body that alternate direction assembly compiler coding rule h impact m generality unroll small loops until the overhead of the branch and induction variable accounts generally for less than of the execution time of the loop assembly compiler coding rule h impact m generality avoid unrolling loops excessively this may thrash the trace cache or instruction cache assembly compiler coding rule m impact m generality unroll loops that are frequently executed and have a predictable number of iterations to reduce the number of iterations to or fewer do this unless it increases code size so that the working set no longer fits in the trace or instruction cache if the loop body contains more than one conditional branch then unroll so that the number of iterations is conditional branches example shows how unrolling enables other optimizations example loop unrolling in this example the loop that executes times assigns x to every even numbered element and y to every odd numbered element by unrolling the loop you can make assignments more efficiently removing one branch in the loop body compiler support for branch prediction compilers generate code that improves the efficiency of branch prediction in the pentium pentium m intel core duo processors and processors based on intel core microarchitecture the intel c compiler accomplishes this by keeping code and data on separate pages using conditional move instructions to eliminate branches generating code consistent with the static branch prediction algorithm inlining where appropriate unrolling if the number of iterations is predictable with profile guided optimization the compiler can lay out basic blocks to eliminate branches for the most frequently executed paths of a function or at least improve their predictability branch prediction need not be a concern at the source level for more information see intel c compiler documentation fetch and decode optimization intel core microarchitecture provides several mechanisms to increase front end throughput techniques to take advantage of some of these features are discussed below optimizing for micro fusion an instruction that operates on a register and a memory operand decodes into more ops than its corresponding register register version replacing the equivalent work of the former instruction using the register register version usually require a sequence of two instructions the latter sequence is likely to result in reduced fetch bandwidth assembly compiler coding rule ml impact m generality for improving fetch decode throughput give preference to memory flavor of an instruction over the register only flavor of the same instruction if such instruction can benefit from micro fusion the following examples are some of the types of micro fusions that can be handled by all decoders all stores to memory including store immediate stores execute internally as two separate ops store address and store data all read modify load op instructions between register and memory for example addps oword ptr rsp fadd double ptr rdi rsi xor rax qword ptr rbp all instructions of the form load and jump for example jmp rdi ret cmp and test with immediate operand and memory an intel instruction with rip relative addressing is not micro fused in the following cases when an additional immediate is needed for example cmp rip mov rip when an rip is needed for control flow purposes for example jmp rip in these cases intel core microarchitecture provides a op flow from decoder resulting in a slight loss of decode bandwidth since op flow must be steered to decoder from the decoder with which it was aligned rip addressing may be common in accessing global data since it will not benefit from micro fusion compiler may consider accessing global data with other means of memory addressing optimizing for macro fusion macro fusion merges two instructions to a single op intel core microarchitecture performs this hardware optimization under limited circumstances the first instruction of the macro fused pair must be a cmp or test instruction this instruction can be reg reg reg imm or a micro fused reg mem comparison the second instruction adjacent in the instruction stream should be a conditional branch since these pairs are common ingredient in basic iterative programming sequences macro fusion improves performance even on un recompiled binaries all of the decoders can decode one macro fused pair per cycle with up to three other instruc tions resulting in a peak decode bandwidth of instructions per cycle each macro fused instruction executes with a single dispatch this process reduces latency which in this case shows up as a cycle removed from branch mispredict penalty software also gain all other fusion benefits increased rename and retire bandwidth more storage for instructions in flight and power savings from repre senting more work in fewer bits the following list details when you can use macro fusion cmp or test can be fused when comparing reg reg for example cmp eax ecx jz label reg imm for example cmp eax jz label reg mem for example cmp eax ecx jz label mem reg for example cmp eax ecx jz label test can fused with all conditional jumps cmp can be fused with only the following conditional jumps in intel core microar chitecture these conditional jumps check carry flag cf or zero flag zf jump the list of macro fusion capable conditional jumps are ja or jnbe jae or jnb or jnc je or jz jna or jbe jnae or jc or jb jne or jnz cmp and test can not be fused when comparing mem imm e g cmp eax jz label macro fusion is not supported in bit mode for intel core microarchitecture intel microarchitecture nehalem supports the following enhancements in macrofusion cmp can be fused with the following conditional jumps that was not supported in intel core microarchitecture jl or jnge jge or jnl jle or jng jg or jnle macro fusion is support in bit mode assembly compiler coding rule m impact ml generality employ macro fusion where possible using instruction pairs that support macro fusion prefer test over cmp if possible use unsigned variables and unsigned jumps when possible try to logically verify that a variable is non negative at the time of comparison avoid cmp or test of mem imm flavor when possible however do not add other instructions to avoid using the mem imm flavor example macro fusion unsigned iteration count without macro fusion with macro fusion c code for i i i a for unsigned i i i a disassembly for int i i i mov dword ptr i jmp first loop mov eax dword ptr i add eax mov dword ptr i eax first cmp dword ptr i jge end a mov eax dword ptr a addqq eax mov dword ptr a eax jmp loop end for unsigned int i i i mov dword ptr i jmp first loop mov eax dword ptr i add eax mov dword ptr i eax first cmp eax jae end a mov eax dword ptr a add eax mov dword ptr a eax jmp loop end notes signed iteration count inhibits macro fusion unsigned iteration count is compatible with macro fusion cmp mem imm jge inhibit macro fusion cmp reg imm jae permits macro fusion example macro fusion if statement without macro fusion with macro fusion c code a if a a else a unsigned a if a a else a disassembly int a mov dword ptr a if a cmp dword ptr a jge dec a mov eax dword ptr a add eax mov dword ptr a eax else jmp end a dec mov eax dword ptr a sub eax mov dword ptr a eax end unsigned int a mov dword ptr a if a mov eax dword ptr a cmp eax jae dec a add eax mov dword ptr a eax else jmp end a dec sub eax mov dword ptr a eax end notes signed iteration count inhibits macro fusion unsigned iteration count is compatible with macro fusion cmp mem imm jge inhibit macro fusion assembly compiler coding rule m impact ml generality software can enable macro fusion when it can be logically determined that a variable is non negative at the time of comparison use test appropriately to enable macro fusion when comparing a variable with example macro fusion signed variable without macro fusion with macro fusion test ecx ecx test ecx ecx jle outsidetheif jle outsidetheif cmp ecx cmp ecx jge outsidetheif jae outsidetheif if block code if block code outsidetheif outsidetheif for either signed or unsigned variable a cmp a and test a a produce the same result as far as the flags are concerned since test can be macro fused more often software can use test a a to replace cmp a for the purpose of enabling macro fusion example macro fusion signed comparison c code without macro fusion with macro fusion if a cmp a test a a jne lbl jne lbl lbl lbl if a cmp a test a a jl lbl jl lbl lbl lbl length changing prefixes lcp the length of an instruction can be up to bytes in length some prefixes can dynamically change the length of an instruction that the decoder must recognize typically the pre decode unit will estimate the length of an instruction in the byte stream assuming the absence of lcp when the predecoder encounters an lcp in the fetch line it must use a slower length decoding algorithm with the slower length decoding algorithm the predecoder decodes the fetch in cycles instead of the usual cycle normal queuing throughout of the machine pipeline generally cannot hide lcp penalties the prefixes that can dynamically change the length of a instruction include operand size prefix address size prefix the instruction mov dx is subject to lcp stalls in processors based on intel core microarchitecture and in intel core duo and intel core solo processors instructions that contain as part of their fixed encoding but do not require lcp to change the immediate size are not subject to lcp stalls the rex prefix in bit mode can change the size of two classes of instruction but does not cause an lcp penalty if the lcp stall happens in a tight loop it can cause significant performance degrada tion when decoding is not a bottleneck as in floating point heavy code isolated lcp stalls usually do not cause performance degradation assembly compiler coding rule mh impact mh generality favor generating code using or values instead of values if is needed load equivalent into a register and use the word value in the register instead double lcp stalls instructions that are subject to lcp stalls and cross a byte fetch line boundary can cause the lcp stall to trigger twice the following alignment situations can cause lcp stalls to trigger twice an instruction is encoded with a modr m and sib byte and the fetch line boundary crossing is between the modr m and the sib bytes an instruction starts at offset of a fetch line references a memory location using register and immediate byte offset addressing mode the first stall is for the fetch line and the stall is for the fetch line a double lcp stall causes a decode penalty of cycles the following examples cause lcp stall once regardless of their fetch line location of the first byte of the instruction add dx add word ptr edx add word ptr edx add word ptr the following instructions cause a double lcp stall when starting at offset of a fetch line add word ptr edx esi add word ptr edx add word ptr edx esi to avoid double lcp stalls do not use instructions subject to lcp stalls that use sib byte encoding or addressing mode with byte displacement false lcp stalls false lcp stalls have the same characteristics as lcp stalls but occur on instructions that do not have any value false lcp stalls occur when a instructions with lcp that are encoded using the opcodes and b are located at offset of a fetch line these instructions are not neg div idiv mul and imul false lcp experiences delay because the instruction length decoder can not determine the length of the instruction before the next fetch line which holds the exact opcode of the instruction in its modr m byte the following techniques can help avoid false lcp stalls upcast all short operations from the group of instructions to long using the full bit version ensure that the opcode never starts at offset of a fetch line assembly compiler coding rule m impact ml generality ensure instructions using opcode byte does not start at offset of a fetch line and avoid using these instruction to operate on bit data upcast short data to bits example avoiding false lcp delays with group instructions a sequence causing delay in the decoder alternate sequence to avoid delay neg word ptr a movsx eax word ptr a neg eax mov word ptr a ax optimizing the loop stream detector lsd loops that fit the following criteria are detected by the lsd and replayed from the instruction queue to feed the decoder in intel core microarchitecture must be less than or equal to four byte fetches must be less than or equal to instructions can contain no more than four taken branches and none of them can be a ret should usually have more than iterations loop stream detector in intel microarchitecture nehalem is improved by caching decoded micro operations in the instruction decoder queue idq see section to feed the rename alloc stage the size of the lsd is increased to micro ops many calculation intensive loops searches and software string moves match these characteristics these loops exceed the bpu prediction capacity and always termi nate in a branch misprediction assembly compiler coding rule mh impact mh generality break up a loop long sequence of instructions into loops of shorter instruction blocks of no more than the size of lsd assembly compiler coding rule mh impact m generality avoid unrolling loops containing lcp stalls if the unrolled block exceeds the size of lsd scheduling rules for the pentium processor decoder processors based on intel netburst microarchitecture have a single decoder that can decode instructions at the maximum rate of one instruction per clock complex instructions must enlist the help of the microcode rom because ops are delivered from the trace cache in the common cases decoding rules and code alignment are not required scheduling rules for the pentium m processor decoder the pentium m processor has three decoders but the decoding rules to supply ops at high bandwidth are less stringent than those of the pentium iii processor this provides an opportunity to build a front end tracker in the compiler and try to schedule instructions correctly the decoder limitations are the first decoder is capable of decoding one macroinstruction made up of four or fewer ops in each clock cycle it can handle any number of bytes up to the maximum of multiple prefix instructions require additional cycles the two additional decoders can each decode one macroinstruction per clock cycle assuming the instruction is one op up to seven bytes in length instructions composed of more than four ops take multiple cycles to decode assembly compiler coding rule m impact m generality avoid putting explicit references to esp in a sequence of stack operations pop push call ret other decoding guidelines assembly compiler coding rule ml impact l generality use simple instructions that are less than eight bytes in length assembly compiler coding rule m impact mh generality avoid using prefixes to change the size of immediate and displacement long instructions more than seven bytes limit the number of decoded instructions per cycle on the pentium m processor each prefix adds one byte to the length of instruction possibly limiting the decoder throughput in addition multiple prefixes can only be decoded by the first decoder these prefixes also incur a delay when decoded if multiple prefixes or a prefix that changes the size of an immediate or displacement cannot be avoided schedule them behind instructions that stall the pipe for some other reason optimizing the execution core the superscalar out of order execution core in recent generations of microarchi tectures contain multiple execution hardware resources that can execute multiple ops in parallel these resources generally ensure that ops execute efficiently and proceed with fixed latencies general guidelines to make use of the available paral lelism are follow the rules see section to maximize useful decode bandwidth and front end throughput these rules include favouring single op instructions and taking advantage of micro fusion stack pointer tracker and macro fusion maximize rename bandwidth guidelines are discussed in this section and include properly dealing with partial registers rob read ports and instructions which causes side effects on flags scheduling recommendations on sequences of instructions so that multiple dependency chains are alive in the reservation station rs simultaneously thus ensuring that your code utilizes maximum parallelism avoid hazards minimize delays that may occur in the execution core allowing the dispatched ops to make progress and be ready for retirement quickly instruction selection some execution units are not pipelined this means that ops cannot be dispatched in consecutive cycles and the throughput is less than one per cycle it is generally a good starting point to select instructions by considering the number of ops associated with each instruction favoring in the order of single op instruc tions simple instruction with less then ops and last instruction requiring microse quencer rom ops which are executed out of the microsequencer involve extra overhead assembly compiler coding rule m impact h generality favor single micro operation instructions also favor instruction with shorter latencies a compiler may be already doing a good job on instruction selection if so user inter vention usually is not necessary assembly compiler coding rule m impact l generality avoid prefixes especially multiple non prefixed opcodes assembly compiler coding rule m impact l generality do not use many segment registers on the pentium m processor there is only one level of renaming of segment regis ters assembly compiler coding rule ml impact m generality avoid using complex instructions for example enter leave or loop that have more than four µops and require multiple cycles to decode use sequences of simple instructions instead complex instructions may save architectural registers but incur a penalty of µops to set up parameters for the microsequencer rom in intel netburst microarchitecture theoretically arranging instructions sequence to match the template applies to processors based on intel core microarchitecture however with macro fusion and micro fusion capabilities in the front end attempts to schedule instruction sequences using the template will likely provide diminishing returns instead software should follow these additional decoder guidelines if you need to use multiple op non microsequenced instructions try to separate by a few single op instructions the following instructions are examples of multiple op instruction not requiring micro sequencer adc sbb cmovcc read modify write instructions if a series of multiple op instructions cannot be separated try breaking the series into a different equivalent instruction sequence for example a series of read modify write instructions may go faster if sequenced as a series of read modify store instructions this strategy could improve performance even if the new code sequence is larger than the original one use of the inc and dec instructions the inc and dec instructions modify only a subset of the bits in the flag register this creates a dependence on all previous writes of the flag register this is especially problematic when these instructions are on the critical path because they are used to change an address for a load on which many other instructions depend assembly compiler coding rule m impact h generality inc and dec instructions should be replaced with add or sub instructions because add and sub overwrite all flags whereas inc and dec do not therefore creating false dependencies on earlier instructions that set the flags integer divide typically an integer divide is preceded by a cwd or cdq instruction depending on the operand size divide instructions use dx ax or edx eax for the dividend the cwd or cdq instructions sign extend ax or eax into dx or edx respectively these instructions have denser encoding than a shift and move would be but they generate the same number of micro ops if ax or eax is known to be positive replace these instructions with xor dx dx or xor edx edx using lea in some cases with processor based on intel netburst microarchitecture the lea instruction or a sequence of lea add sub and shift instructions can replace constant multiply instructions the lea instruction can also be used as a multiple operand addition instruction for example lea ecx eax ebx a using lea in this way may avoid register usage by not tying up registers for operands of arithmetic instructions this use may also save code space if the lea instruction uses a shift by a constant amount then the latency of the sequence of µops is shorter if adds are used instead of a shift and the lea instruction may be replaced with an appropriate sequence of µops this however increases the total number of µops leading to a trade off assembly compiler coding rule ml impact l generality if an lea instruction using the scaled index is on the critical path a sequence with adds may be better if code density and bandwidth out of the trace cache are the critical factor then use the lea instruction using shift and rotate the shift and rotate instructions have a longer latency on processor with a cpuid signature corresponding to family and model encoding of or the latency of a sequence of adds will be shorter for left shifts of three or less fixed and variable shifts have the same latency the rotate by immediate and rotate by register instructions are more expensive than a shift the rotate by instruction has the same latency as a shift assembly compiler coding rule ml impact l generality avoid rotate by register or rotate by immediate instructions if possible replace with a rotate by instruction address calculations for computing addresses use the addressing modes rather than general purpose computations internally memory reference instructions can have four operands relocatable load time constant immediate constant base register scaled index register in the segmented model a segment register may constitute an additional operand in the linear address calculation in many cases several integer instructions can be eliminated by fully using the operands of memory references clearing registers and dependency breaking idioms code sequences that modifies partial register can experience some delay in its dependency chain but can be avoided by using dependency breaking idioms in processors based on intel core microarchitecture a number of instructions can help clear execution dependency when software uses these instruction to clear register content to zero the instructions include xor reg reg sub reg reg xorps pd xmmreg xmmreg pxor xmmreg xmmreg subps pd xmmreg xmmreg psubb w d q xmmreg xmmreg in intel core solo and intel core duo processors the xor sub xorps or pxor instructions can be used to clear execution dependencies on the zero evaluation of the destination register the pentium processor provides special support for xor sub and pxor opera tions when executed within the same register this recognizes that clearing a register does not depend on the old value of the register the xorps and xorpd instructions do not have this special support they cannot be used to break dependence chains assembly compiler coding rule m impact ml generality use dependency breaking idiom instructions to set a register to or to break a false dependence chain resulting from re use of registers in contexts where the condition codes must be preserved move into the register instead this requires more code space than using xor and sub but avoids setting the condition codes example of using pxor to break dependency idiom on a xmm register when performing negation on the elements of an array int a b c for int i i i c i a i b i example clearing register to break dependency while negating array elements negation x x xor without breaking dependency negation x x using pxor reg reg breaks dependency lea eax a lea eax a lea ecx b lea ecx b lea edi c lea edi c xor edx edx xor edx edx movdqa allone lp lp movdqa eax edx movdqa eax edx paddd ecx edx paddd ecx edx pxor pxor psubd psubd movdqa edi edx movdqa edi edx add edx add edx cmp edx cmp edx jl lp jl lp assembly compiler coding rule m impact mh generality break dependences on portions of registers between instructions by operating on bit registers instead of partial registers for moves this can be accomplished with bit moves or by using movzx on pentium m processors the movsx and movzx instructions both take a single op whether they move from a register or memory on pentium processors the movsx takes an additional op this is likely to cause less delay than the partial register update problem mentioned above but the performance gain may vary if the additional op is a critical problem movsx can sometimes be used as alternative sometimes sign extended semantics can be maintained by zero extending oper ands for example the c code in the following statements does not need sign exten sion nor does it need prefixes for operand size overrides static short int a b if a b code for comparing these bit operands might be movzw eax a movzw ebx b cmp eax ebx these circumstances tend to be common however the technique will not work if the compare is for greater than less than greater than or equal and so on or if the values in eax or ebx are to be used in another operation where sign extension is required assembly compiler coding rule m impact m generality try to use zero extension or operate on bit operands instead of using moves with sign extension the trace cache can be packed more tightly when instructions with operands that can only be represented as bits are not adjacent assembly compiler coding rule ml impact l generality avoid placing instructions that use bit immediates which cannot be encoded as sign extended bit immediates near each other try to schedule µops that have no immediate immediately before or after µops with bit immediates compares use test when comparing a value in a register with zero test essentially ands operands together without writing to a destination register test is preferred over and because and produces an extra result register test is better than cmp because the instruction size is smaller use test when comparing the result of a logical and with an immediate constant for equality or inequality if the register is eax for cases such as if avar the test instruction can also be used to detect rollover of modulo of a power of for example the c code if avar can be implemented using test eax jnz afterif using the test instruction between the instruction that may modify part of the flag register and the instruction that uses the flag register can also help prevent partial flag register stall assembly compiler coding rule ml impact m generality use the test instruction instead of and when the result of the logical and is not used this saves µops in execution use a test if a register with itself instead of a cmp of the register to zero this saves the need to encode the zero and saves encoding space avoid comparing a constant to a memory operand it is preferable to load the memory operand and compare the constant to a register often a produced value must be compared with zero and then used in a branch because most intel architecture instructions set the condition codes as part of their execution the compare instruction may be eliminated thus the operation can be tested directly by a jcc instruction the notable exceptions are mov and lea in these cases use test assembly compiler coding rule ml impact m generality eliminate unnecessary compare with zero instructions by using the appropriate conditional jump instruction when the flags are already set by a preceding arithmetic instruction if necessary use a test instruction instead of a compare be certain that any code transformations made do not introduce problems with overflow using nops code generators generate a no operation nop to align instructions examples of nops of different lengths in bit mode are shown below byte xchg eax eax byte nop byte lea reg reg bit displacement byte nop dword ptr eax bit displacement byte nop dword ptr eax eax bit displacement byte lea reg reg bit displacement byte nop dword ptr eax bit displacement byte nop dword ptr eax eax bit displacement byte nop word ptr eax eax bit displacement these are all true nops having no effect on the state of the machine except to advance the eip because nops require hardware resources to decode and execute use the fewest number to achieve the desired padding the one byte nop xchg eax eax has special hardware support although it still consumes a µop and its accompanying resources the dependence upon the old value of eax is removed this µop can be executed at the earliest possible opportunity reducing the number of outstanding instructions and is the lowest cost nop the other nops have no special hardware support their input and output registers are interpreted by the hardware therefore a code generator should arrange to use the register containing the oldest value as input so that the nop will dispatch and release rs resources at the earliest possible opportunity try to observe the following nop generation priority select the smallest number of nops and pseudo nops to provide the desired padding select nops that are least likely to execute on slower execution unit clusters select the register arguments of nops to reduce dependencies mixing simd data types previous microarchitectures before intel core microarchitecture do not have explicit restrictions on mixing integer and floating point fp operations on xmm registers for intel core microarchitecture mixing integer and floating point opera tions on the content of an xmm register can degrade performance software should avoid mixed use of integer fp operation on xmm registers specifically use simd integer operations to feed simd integer operations use pxor for idiom use simd floating point operations to feed simd floating point operations use xorps for idiom when floating point operations are bitwise equivalent use ps data type instead of pd data type movaps and movapd do the same thing but movaps takes one less byte to encode the instruction spill scheduling the spill scheduling algorithm used by a code generator will be impacted by the memory subsystem a spill scheduling algorithm is an algorithm that selects what values to spill to memory when there are too many live values to fit in registers consider the code in example where it is necessary to spill either a b or c example spill scheduling code loop c b a a for modern microarchitectures using dependence depth information in spill sched uling is even more important than in previous processors the loop carried depen dence in a makes it especially important that a not be spilled not only would a store load be placed in the dependence chain but there would also be a data not ready stall of the load costing further cycles assembly compiler coding rule h impact mh generality for small loops placing loop invariants in memory is better than spilling loop carried dependencies a possibly counter intuitive result is that in such a situation it is better to put loop invariants in memory than in registers since loop invariants never have a load blocked by store data that is not ready avoiding stalls in execution core although the design of the execution core is optimized to make common cases executes quickly a op may encounter various hazards delays or stalls while making forward progress from the front end to the rob and rs the significant cases are rob read port stalls partial register reference stalls partial updates to xmm register stalls partial flag register reference stalls rob read port stalls as a op is renamed it determines whether its source operands have executed and been written to the reorder buffer rob or whether they will be captured in flight in the rs or in the bypass network typically the great majority of source operands are found to be in flight during renaming those that have been written back to the rob are read through a set of read ports since the intel core microarchitecture is optimized for the common case where the operands are in flight it does not provide a full set of read ports to enable all renamed ops to read all sources from the rob in the same cycle when not all sources can be read a op can stall in the rename stage until it can get access to enough rob read ports to complete renaming the op this stall is usually short lived typically a op will complete renaming in the next cycle but it appears to the application as a loss of rename bandwidth some of the software visible situations that can cause rob read port stalls include registers that have become cold and require a rob read port because execution units are doing other independent calculations constants inside registers pointer and index registers in rare cases rob read port stalls may lead to more significant performance degra dations there are a couple of heuristics that can help prevent over subscribing the rob read ports keep common register usage clustered together multiple references to the same written back register can be folded inside the out of order execution core keep dependency chains intact this practice ensures that the registers will not have been written back when the new micro ops are written to the rs these two scheduling heuristics may conflict with other more common scheduling heuristics to reduce demand on the rob read port use these two heuristics only if both the following situations are met short latency operations indications of actual rob read port stalls can be confirmed by measurements of the performance event the relevant event is see appendix a of the intel and ia architectures software developer manual volume if the code has a long dependency chain these two heuristics should not be used because they can cause the rs to fill causing damage that outweighs the positive effects of reducing demands on the rob read port bypass between execution domains floating point fp loads have an extra cycle of latency moves between fp and simd stacks have another additional cycle of latency example addps pand addps the overall latency for the above calculation is cycles cycles for each addps instruction cycle for the pand instruction cycle to bypass between the addps floating point domain to the pand integer domain cycle to move the data from the pand integer to the second floating point addps domain to avoid this penalty you should organize code to minimize domain changes some times you cannot avoid bypasses account for bypass cycles when counting the overall latency of your code if your calculation is latency bound you can execute more instructions in parallel or break dependency chains to reduce total latency code that has many bypass domains and is completely latency bound may run slower on the intel core microarchitecture than it did on previous microarchitectures partial register stalls general purpose registers can be accessed in granularities of bytes words double words bit mode also supports quadword granularity referencing a portion of a register is referred to as a partial register reference a partial register stall happens when an instruction refers to a register portions of which were previously modified by other instructions for example partial register stalls occurs with a read to ax while previous instructions stored al and ah or a read to eax while previous instruction modified ax the delay of a partial register stall is small in processors based on intel core and netburst microarchitectures and in pentium m processor with cpuid signature family model intel core solo and intel core duo processors pentium m processors cpuid signature with family model and the family incur a large penalty note that in intel architecture an update to the lower bits of a bit integer register is architecturally defined to zero extend the upper bits while this action may be logically viewed as a bit update it is really a bit update and therefore does not cause a partial stall referencing partial registers frequently produces code sequences with either false or real dependencies example demonstrates a series of false and real dependen cies caused by referencing partial registers if instructions and in example are changed to use a movzx instruction instead of a mov then the dependences of instruction on and transitively before it and instruction on are broken this creates two independent chains of computation instead of one serial one example dependencies caused by referencing partial registers add ah bh add al instruction has a false dependency on mov bl al depends on but the dependence is real mov ah ch instruction has a false dependency on sar eax this wipes out the al ah ax part so the result really doesn t depend on them programatically but the processor must deal with real dependency on al ah ax mov al bl instruction has a real dependency on add ah instruction has a false dependency on imul dl instruction has a false dependency on because al is implicitly used mov al instruction has a false dependency on and a real dependency on imul cx implicitly uses ax and writes to dx hence a real dependency example illustrates the use of movzx to avoid a partial register stall when packing three byte values into a register example avoiding partial register stalls in integer code a sequence causing partial register stall alternate sequence using movzx to avoid delay mov al byte ptr a movzx eax byte ptr a shl eax shl eax mov ax word ptr a movzx ecx word ptr a movd eax or eax ecx ret movd eax ret partial xmm register stalls partial register stalls can also apply to xmm registers the following sse and instructions update only part of the destination register movl hpd xmm movl hps xmm movss sd between registers using these instructions creates a dependency chain between the unmodified part of the register and the modified part of the register this dependency chain can cause performance loss example illustrates the use of movzx to avoid a partial register stall when packing three byte values into a register follow these recommendations to avoid stalls from partial updates to xmm registers avoid using instructions which update only part of the xmm register if a bit load is needed use the movsd or movq instruction if bit loads are required to the same register from non continuous locations use movsd movhpd instead of movlpd movhpd when copying the xmm register use the following instructions for full register copy even if you only want to copy some of the source register data movaps movapd movdqa example avoiding partial register stalls in simd code using movlpd for memory transactions and movsd between register copies causing partial register stall using movsd for memory and movapd between register copies avoid delay mov edx x mov edx x mov ecx count mov ecx count movlpd movsd movlpd movsd align align example avoiding partial register stalls in simd code contd using movlpd for memory transactions and movsd between register copies causing partial register stall using movsd for memory and movapd between register copies avoid delay lp movlpd edx addsd movsd subsd edx mulsd movsd edx add edx dec ecx jnz lp lp movsd edx addsd movapd subsd edx mulsd movsd edx add edx dec ecx jnz lp partial flag register stalls a partial flag register stall occurs when an instruction modifies a part of the flag register and the following instruction is dependent on the outcome of the flags this happens most often with shift instructions sar sal shr shl the flags are not modified in the case of a zero shift count but the shift count is usually known only at execution time the front end stalls until the instruction is retired other instructions that can modify some part of the flag register include various rotate instructions stc and std an example of assembly with a partial flag register stall and alternative code without the stall is shown in example in processors based on intel core microarchitecture shift immediate by is handled by special hardware such that it does not experience partial flag stall example avoiding partial flag register stalls a sequence with partial flag register stall alternate sequence without partial flag register stall xor eax eax or eax eax mov ecx a mov ecx a sar ecx sar ecx setz al test ecx ecx no partial register stall setz al but flag stall as sar may no partial reg or flag stall change the flags test always updates all the flags floating point simd operands in intel netburst microarchitecture in processors based on intel netburst microarchitecture the latency of mmx or simd floating point register to register moves is significant this can have implications for register allocation moves that write a portion of a register can introduce unwanted dependences the movsd reg reg instruction writes only the bottom bits of a register not all bits this introduces a dependence on the preceding instruction that produces the upper bits even if those bits are not longer wanted the dependence inhibits register renaming and thereby reduces parallelism use movapd as an alternative it writes all bits even though this instruction has a longer latency the ops for movapd use a different execution port and this port is more likely to be free the change can impact performance there may be excep tional cases where the latency matters more than the dependence or the execution port assembly compiler coding rule m impact ml generality avoid introducing dependences with partial floating point register writes e g from the movsd instruction use the movapd instruction instead the movsd xmmreg mem instruction writes all bits and breaks a dependence the movupd from memory instruction performs two bit loads but requires addi tional µops to adjust the address and combine the loads into a single register this same functionality can be obtained using movsd mem movsd mem unpcklpd which uses fewer µops and can be packed into the trace cache more effectively the latter alternative has been found to provide a several percent performance improvement in some cases its encoding requires more instruction bytes but this is seldom an issue for the pentium processor the store version of movupd is complex and slow so much so that the sequence with two movsd and a unpckhpd should always be used assembly compiler coding rule ml impact l generality instead of using movupd mem for a unaligned bit load use movsd mem movsd mem unpcklpd if the additional register is not available then use movsd mem movhpd mem assembly compiler coding rule m impact ml generality instead of using movupd mem for a store use movsd mem unpckhpd movsd mem instead vectorization this section provides a brief summary of optimization issues related to vectorization there is more detail in the chapters that follow vectorization is a program transformation that allows special hardware to perform the same operation on multiple data elements at the same time successive processor generations have provided vector support through the mmx technology streaming simd extensions sse streaming simd extensions streaming simd extensions and supplemental streaming simd extensions vectorization is a special case of simd a term defined in flynn architecture taxonomy to denote a single instruction stream capable of operating on multiple data elements in parallel the number of elements which can be operated on in parallel range from four single precision floating point data elements in streaming simd extensions and two double precision floating point data elements in streaming simd extensions to sixteen byte operations in a bit register in streaming simd extensions thus vector length ranges from to depending on the instruction extensions used and on the data type the intel c compiler supports vectorization in three ways the compiler may be able to generate simd code without intervention from the user the can user insert pragmas to help the compiler realize that it can vectorize the code the user can write simd code explicitly using intrinsics and c classes to help enable the compiler to generate simd code avoid global pointers and global variables these issues may be less troublesome if all modules are compiled simulta neously and whole program optimization is used user source coding rule h impact m generality use the smallest possible floating point or simd data type to enable more parallelism with the use of a longer simd vector for example use single precision instead of double precision where possible user source coding rule m impact ml generality arrange the nesting of loops so that the innermost nesting level is free of inter iteration dependencies especially avoid the case where the store of data in an earlier iteration happens lexically after the load of that data in a future iteration something which is called a lexically backward dependence the integer part of the simd instruction set extensions cover bit bit and bit operands not all simd operations are supported for bits meaning that some source code will not be able to be vectorized at all unless smaller operands are used user source coding rule m impact ml generality avoid the use of conditional branches inside loops and consider using sse instructions to eliminate branches user source coding rule m impact ml generality keep induction loop variable expressions simple optimization of partially vectorizable code frequently a program contains a mixture of vectorizable code and some routines that are non vectorizable a common situation of partially vectorizable code involves a loop structure which include mixtures of vectorized code and unvectorizable code this situation is depicted in figure packed simd instruction unpacking unvectorizable code serial routine packing packed simd instruction figure generic program flow of partially vectorized code it generally consists of five stages within the loop prolog unpacking vectorized data structure into individual elements calling a non vectorizable routine to process each element serially packing individual result into vectorized data structure epilog this section discusses techniques that can reduce the cost and bottleneck associated with the packing unpacking stages in these partially vectorize code example shows a reference code template that is representative of partially vectorizable coding situations that also experience performance issues the unvec torizable portion of code is represented generically by a sequence of calling a serial function named foo multiple times this generic example is referred to as shuffle with store forwarding because the problem generally involves an unpacking stage that shuffles data elements between register and memory followed by a packing stage that can experience store forwarding issue there are more than one useful techniques that can reduce the store forwarding bottleneck between the serialized portion and the packing stage the following sub sections presents alternate techniques to deal with the packing unpacking and parameter passing to serialized function calls example reference code template for partially vectorizable program example reference code template for partially vectorizable program contd alternate packing techniques the packing method implemented in the reference code of example will experi ence delay as it assembles doubleword result from memory into an xmm register due to store forwarding restrictions three alternate techniques for packing using different simd instruction to assemble contents in xmm registers are shown in example all three techniques avoid store forwarding delay by satisfying the restrictions on data sizes between a preceding store and subsequent load operations example three alternate packing methods for avoiding store forwarding difficulty packing method packing method packing method movd ebp movd ebp movd ebp movd ebp movd ebp movd ebp movd ebp movd ebp movd ebp movd ebp movd ebp movd ebp punpckldq psllq movlhps punpckldq orps psllq punpckldq psllq movlhps orps orps simplifying result passing in example individual results were passed to the packing stage by storing to contiguous memory locations instead of using memory spills to pass four results result passing may be accomplished by using either one or more registers using registers to simplify result passing and reduce memory spills can improve perfor mance by varying degrees depending on the register pressure at runtime example shows the coding sequence that uses four extra xmm registers to reduce all memory spills of passing results back to the parent routine however soft ware must observe the following conditions when using this technique there is no register shortage if the loop does not have many stores or loads but has many computations this technique does not help performance this technique adds work to the computa tional units while the store and loads ports are idle example using four registers to reduce memory spills and simplify result passing mov eax ebp mov ebp eax call foo movd eax mov eax ebp mov ebp eax call foo movd eax mov eax ebp mov ebp eax call foo movd eax mov eax ebp mov ebp eax call foo movd eax stack optimization in example an input parameter was copied in turn onto the stack and passed to the non vectorizable routine for processing the parameter passing from consecu tive memory locations can be simplified by a technique shown in example example stack optimization technique to simplify parameter passing call foo mov ebp eax add ebp call foo mov ebp eax example stack optimization technique to simplify parameter passing contd add ebp call foo mov ebp eax add ebp call foo stack optimization can only be used when the serial operations are function calls the function foo is declared as int foo int a the parameter is passed on the stack the order of operation on the components is from last to first note the call to foo and the advance of edp when passing the vector elements to foo one by one from last to first tuning considerations tuning considerations for situations represented by looping of example include applying one of more of the following combinations choose an alternate packing technique consider a technique to simply result passing consider the stack optimization technique to simplify parameter passing minimizing the average number of cycles to execute one iteration of the loop minimizing the per iteration cost of the unpacking and packing operations the speed improvement by using the techniques discussed in this section will vary depending on the choice of combinations implemented and characteristics of the non vectorizable routine for example if the routine foo is short representative of tight short loops the per iteration cost of unpacking packing tend to be smaller than situations where the non vectorizable code contain longer operation or many dependencies this is because many iterations of short tight loop can be in flight in the execution core so the per iteration cost of packing and unpacking is only partially exposed and appear to cause very little performance degradation evaluation of the per iteration cost of packing unpacking should be carried out in a methodical manner over a selected number of test cases where each case may implement some combination of the techniques discussed in this section the per iteration cost can be estimated by evaluating the average cycles to execute one iteration of the test case evaluating the average cycles to execute one iteration of a base line loop sequence of non vectorizable code example shows the base line code sequence that can be used to estimate the average cost of a loop that executes non vectorizable routines example base line code sequence to estimate loop overhead push ebp mov ebp esp sub ebp mov ebp edi call foo mov ebp edi call foo mov ebp edi call foo mov ebp edi call foo add ebp pop ebp ret the average per iteration cost of packing unpacking can be derived from measuring the execution times of a large number of iterations by cycles to run testcase cycles to run equivalent baseline sequence iteration count for example using a simple function that returns an input parameter representative of tight short loops the per iteration cost of packing unpacking may range from slightly more than cycles the shuffle with store forwarding case example to cycles accomplished by several test cases across test cases consisting of one of the alternate packing methods no result simplification simplification of either or results no stack optimization or with stack optimization the average per iter ation cost of packing unpacking is about cycles generally speaking packing method and see example tend to be more robust than packing method the optimal choice of simplifying or results will be affected by register pressure of the runtime and other relevant microarchitectural conditions note that the numeric discussion of per iteration cost of packing packing is illustra tive only it will vary with test cases using a different base line code sequence and will generally increase if the non vectorizable routine requires longer time to execute because the number of loop iterations that can reside in flight in the execution core decreases optimizing memory accesses this section discusses guidelines for optimizing code and data memory accesses the most important recommendations are execute load and store operations within available execution bandwidth enable forward progress of speculative execution enable store forwarding to proceed align data paying attention to data layout and stack alignment place code and data on separate pages enhance data locality use prefetching and cacheability control instructions enhance code locality and align branch targets take advantage of write combining alignment and forwarding problems are among the most common sources of large delays on processors based on intel netburst microarchitecture load and store execution bandwidth typically loads and stores are the most frequent operations in a workload up to of the instructions in a workload carrying load or store intent are not uncommon each generation of microarchitecture provides multiple buffers to support executing load and store operations while there are instructions in flight software can maximize memory performance by not exceeding the issue or buffering limitations of the machine in the intel core microarchitecture only stores and loads may be in flight at once since only one load can issue per cycle algorithms which operate on two arrays are constrained to one operation every other cycle unless you use programming tricks to reduce the amount of memory usage intel netburst microarchitecture has the same number of store buffers slightly more load buffers and similar throughput of issuing load operations intel core duo and intel core solo processors have less buffers nevertheless the general heuristic applies to all of them enhance speculative execution and memory disambiguation prior to intel core microarchitecture when code contains both stores and loads the loads cannot be issued before the address of the store is resolved this rule ensures correct handling of load dependencies on preceding stores the intel core microarchitecture contains a mechanism that allows some loads to be issued early speculatively the processor later checks if the load address overlaps with a store if the addresses do overlap then the processor re executes the instruc tions example illustrates a situation that the compiler cannot be sure that ptr array does not change during the loop therefore the compiler cannot keep ptr array in a register as an invariant and must read it again in every iteration although this situation can be fixed in software by a rewriting the code to require the address of the pointer is invariant memory disambiguation provides performance gain without rewriting the code example loads blocked by stores of unknown address c code assembly sequence struct aa aa array void aa ptr dword index aa thisptr while ptr array index thisptr ptr array index null mov dword ptr eax mov edx dword ptr edi sub ecx cmp dword ptr ecx edx esi lea eax ecx edx jne alignment alignment of data concerns all kinds of variables dynamically allocated variables members of a data structure global or local variables parameters passed on the stack misaligned data access can incur significant performance penalties this is particu larly true for cache line splits the size of a cache line is bytes in the pentium and other recent intel processors including processors based on intel core microarchi tecture an access to data unaligned on byte boundary leads to two memory accesses and requires several µops to be executed instead of one accesses that span byte boundaries are likely to incur a large performance penalty the cost of each stall generally are greater on machines with longer pipelines double precision floating point operands that are eight byte aligned have better performance than operands that are not eight byte aligned since they are less likely to incur penalties for cache and mob splits floating point operation on a memory operands require that the operand be loaded from memory this incurs an additional µop which can have a minor negative impact on front end bandwidth additionally memory operands may cause a data cache miss causing a penalty assembly compiler coding rule h impact h generality align data on natural operand size address boundaries if the data will be accessed with vector instruction loads and stores align the data on byte boundaries for best performance align data as follows align bit data at any address align bit data to be contained within an aligned byte word align bit data so that its base address is a multiple of four align bit data so that its base address is a multiple of eight align bit data so that its base address is a multiple of sixteen align bit data so that its base address is a multiple of sixteen a byte or greater data structure or array should be aligned so that its base address is a multiple of sorting data in decreasing size order is one heuristic for assisting with natural alignment as long as byte boundaries and cache lines are never crossed natural alignment is not strictly necessary though it is an easy way to enforce this example shows the type of code that can cause a cache line split the code loads the addresses of two dword arrays is not a byte aligned address so a byte access at this address will get bytes from the cache line this address is contained in and bytes from the cache line that starts at on processors with byte cache lines a similar cache line split will occur every iter ations example code that causes cache line split mov esi mov edi blockmove mov eax dword ptr esi mov ebx dword ptr esi mov dword ptr edi eax mov dword ptr edi ebx add esi add edi sub edx jnz blockmove figure illustrates the situation of accessing a data element that span across cache line boundaries figure cache line split in accessing elements in a array alignment of code is less important for processors based on intel netburst microar chitecture alignment of branch targets to maximize bandwidth of fetching cached instructions is an issue only when not executing out of the trace cache alignment of code can be an issue for the pentium m intel core duo and intel core duo processors alignment of branch targets will improve decoder throughput store forwarding the processor memory system only sends stores to memory including cache after store retirement however store data can be forwarded from a store to a subsequent load from the same address to give a much shorter store load latency there are two kinds of requirements for store forwarding if these requirements are violated store forwarding cannot occur and the load must get its data from the cache so the store must write its data back to the cache first this incurs a penalty that is largely related to pipeline depth of the underlying micro architecture the first requirement pertains to the size and alignment of the store forwarding data this restriction is likely to have high impact on overall application performance typi cally a performance penalty due to violating this restriction can be prevented the store to load forwarding restrictions vary from one microarchitecture to another several examples of coding pitfalls that cause store forwarding stalls and solutions to these pitfalls are discussed in detail in section store to load forwarding restriction on size and alignment the second requirement is the availability of data discussed in section store forwarding restriction on data avail ability a good practice is to eliminate redundant load operations it may be possible to keep a temporary scalar variable in a register and never write it to memory generally such a variable must not be accessible using indirect pointers moving a variable to a register eliminates all loads and stores of that variable and eliminates potential problems associated with store forwarding however it also increases register pressure load instructions tend to start chains of computation since the out of order engine is based on data dependence load instructions play a significant role in the engine ability to execute at a high rate eliminating loads should be given a high priority if a variable does not change between the time when it is stored and the time when it is used again the register that was stored can be copied or used directly if register pressure is too high or an unseen function is called before the store and the second load it may not be possible to eliminate the second load assembly compiler coding rule h impact m generality pass parameters in registers instead of on the stack where possible passing arguments on the stack requires a store followed by a reload while this sequence is optimized in hardware by providing the value to the load directly from the memory order buffer without the need to access the data cache if permitted by store forwarding restrictions floating point values incur a significant latency in forwarding passing floating point arguments in preferably xmm registers should save this long latency operation parameter passing conventions may limit the choice of which parameters are passed in registers which are passed on the stack however these limitations may be over come if the compiler has control of the compilation of the whole binary using whole program optimization store to load forwarding restriction on size and alignment data size and alignment restrictions for store forwarding apply to processors based on intel netburst microarchitecture intel core microarchitecture intel core duo intel core solo and pentium m processors the performance penalty for violating store forwarding restrictions is less for shorter pipelined machines than for intel netburst microarchitecture store forwarding restrictions vary with each microarchitecture intel netburst microarchitecture places more constraints than intel core microarchitecture on code generation to enable store forwarding to make progress instead of experiencing stalls fixing store forwarding problems for intel netburst microarchitecture gener ally also avoids problems on pentium m intel core duo and intel core duo proces sors the size and alignment restrictions for store forwarding in processors based on intel netburst microarchitecture are illustrated in figure figure size and alignment restrictions in store forwarding the following rules help satisfy size and alignment restrictions for store forwarding assembly compiler coding rule h impact m generality a load that forwards from a store must have the same address start point and therefore the same alignment as the store data assembly compiler coding rule h impact m generality the data of a load which is forwarded from a store must be completely contained within the store data a load that forwards from a store must wait for the store data to be written to the store buffer before proceeding but other unrelated loads need not wait assembly compiler coding rule h impact ml generality if it is necessary to extract a non aligned portion of stored data read out the smallest aligned portion that completely contains the data and shift mask the data as necessary this is better than incurring the penalties of a failed store forward assembly compiler coding rule mh impact ml generality avoid several small loads after large stores to the same area of memory by using a single large read and register copies as needed example depicts several store forwarding situations in which small loads follow large stores the first three load operations illustrate the situations described in rule however the last load operation gets data from store forwarding without problem example situations showing small loads after large store example illustrates a store forwarding situation in which a large load follows several small stores the data needed by the load operation cannot be forwarded because all of the data that needs to be forwarded is not contained in the store buffer avoid large loads after small stores to the same area of memory example non forwarding example of large load after small store mov ebp a mov ebp b mov ebp c mov ebp d mov eax ebp blocked the first small store can be consolidated into a single dword store to prevent this non forwarding situation example illustrates a stalled store forwarding situation that may appear in compiler generated code sometimes a compiler generates code similar to that shown in example to handle a spilled byte to the stack and convert the byte to an integer value example a non forwarding situation in compiler generated code mov dword ptr esp mov byte ptr esp bl mov eax dword ptr esp stall and eax converting back to byte value example offers two alternatives to avoid the non forwarding situation shown in example example two ways to avoid non forwarding situation in example a use movz instruction to avoid large load after small store when spills are ignored movz eax bl replaces the last three instructions b use movz instruction and handle spills to the stack mov dword ptr esp mov byte ptr esp bl movz eax byte ptr esp not blocked when moving data that is smaller than bits between memory locations bit or bit simd register moves are more efficient if aligned and can be used to avoid unaligned loads although floating point registers allow the movement of bits at a time floating point instructions should not be used for this purpose as data may be inadvertently modified as an additional example consider the cases in example example large and small load stalls a large load stall mov mem eax store dword to address mem mov mem ebx store dword to address mem fld mem load qword at address mem stalls b small load stall fstp mem store qword to address mem mov bx mem load word at address mem stalls mov cx mem load word at address mem stalls in the first case a there is a large load after a series of small stores to the same area of memory beginning at memory address mem the large load will stall the fld must wait for the stores to write to memory before it can access all the data it requires this stall can also occur with other data types for example when bytes or words are stored and then words or doublewords are read from the same area of memory in the second case b there is a series of small loads after a large store to the same area of memory beginning at memory address mem the small loads will stall the word loads must wait for the quadword store to write to memory before they can access the data they require this stall can also occur with other data types for example when doublewords or words are stored and then words or bytes are read from the same area of memory this can be avoided by moving the store as far from the loads as possible store forwarding restrictions for processors based on intel core microarchitecture is listed in table table store forwarding restrictions of processors based on intel core microarchitecture store alignment width of store bits load alignment byte width of load bits store forwarding restriction to natural size word aligned not stalled to natural size not word aligned stalled to natural size dword aligned not stalled to natural size not dword aligned stalled to natural size word aligned not stalled to natural size not word aligned stalled to natural size qword aligned not stalled table store forwarding restrictions of processors based on intel core microarchitecture contd store alignment width of store bits load alignment byte width of load bits store forwarding restriction to natural size not qword aligned stalled to natural size dword aligned not stalled to natural size not dword aligned stalled to natural size dqword aligned not stalled to natural size not dqword aligned stalled to natural size dword aligned not stalled to natural size not dword aligned stalled to natural size qword aligned not stalled to natural size not qword aligned stalled unaligned start byte byte of store not stalled unaligned start byte not byte of store stalled unaligned start byte byte of store not stalled unaligned start byte not byte of store stalled unaligned start byte byte of store stalled unaligned start byte byte of store not stalled unaligned start byte not byte of store not stalled unaligned start byte don t care stalled unaligned start byte don t care stalled store forwarding restriction on data availability the value to be stored must be available before the load operation can be completed if this restriction is violated the execution of the load will be delayed until the data is available this delay causes some execution resources to be used unnecessarily and that can lead to sizable but non deterministic delays however the overall impact of this problem is much smaller than that from violating size and alignment require ments in processors based on intel netburst microarchitecture hardware predicts when loads are dependent on and get their data forwarded from preceding stores these predictions can significantly improve performance however if a load is scheduled too soon after the store it depends on or if the generation of the data to be stored is delayed there can be a significant penalty there are several cases in which data is passed through memory and the store may need to be separated from the load spills save and restore registers in a stack frame parameter passing global and volatile variables type conversion between integer and floating point when compilers do not analyze code that is inlined forcing variables that are involved in the interface with inlined code to be in memory creating more memory variables and preventing the elimination of redundant loads assembly compiler coding rule h impact mh generality where it is possible to do so without incurring other penalties prioritize the allocation of variables to registers as in register allocation and for parameter passing to minimize the likelihood and impact of store forwarding problems try not to store forward data generated from a long latency instruction for example mul or div avoid store forwarding data for variables with the shortest store load distance avoid store forwarding data for variables with many and or long dependence chains and especially avoid including a store forward on a loop carried dependence chain shows an example of a loop carried dependence chain example loop carried dependence chain assembly compiler coding rule m impact mh generality calculate store addresses as early as possible to avoid having stores block loads data layout optimizations user source coding rule h impact m generality pad data structures defined in the source code so that every data element is aligned to a natural operand size address boundary if the operands are packed in a simd instruction align to the packed element size bit or bit align data by providing padding inside structures and arrays programmers can reor ganize structures and arrays to minimize the amount of memory wasted by padding however compilers might not have this freedom the c programming language for example specifies the order in which structure elements are allocated in memory for more information see section stack and data alignment and appendix d stack alignment example shows how a data structure could be rearranged to reduce its size example rearranging a data structure struct unpacked fits in bytes due to padding int a char b int c char d int e struct packed fits in bytes int a int c int e char b char d cache line size of bytes can impact streaming applications for example multi media these reference and use data only once before discarding it data accesses which sparsely utilize the data within a cache line can result in less efficient utilization of system memory bandwidth for example arrays of structures can be decomposed into several arrays to achieve better packing as shown in example example decomposing an array struct bytes int a c e char b d struct bytes int a c e char b d struct bytes int a c e example decomposing an array contd struct bytes char b d the efficiency of such optimizations depends on usage patterns if the elements of the structure are all accessed together but the access pattern of the array is random then avoids unnecessary prefetch even though it wastes memory however if the access pattern of the array exhibits locality for example if the array index is being swept through then processors with hardware prefetchers will prefetch data from even if the elements of the structure are accessed together when the elements of the structure are not accessed with equal frequency such as when element a is accessed ten times more often than the other entries then not only saves memory but it also prevents fetching unneces sary data items b c d and e using also enables the use of the simd data types by the programmer and the compiler note that can have the disadvantage of requiring more indepen dent memory stream references this can require the use of more prefetches and additional address generation calculations it can also have an impact on dram page access efficiency an alternative blends the two approaches in this case only separate address streams are generated and refer enced for and for the second alterative also prevents fetching unnecessary data assuming that the variables a c and e are always used together and the variables b and d are always used together but not at the same time as a c and e the hybrid approach ensures simpler fewer address generations than fewer streams which reduces dram page misses fewer prefetches due to fewer streams efficient cache line packing of data elements that are used concurrently assembly compiler coding rule h impact m generality try to arrange data structures such that they permit sequential access if the data is arranged into a set of streams the automatic hardware prefetcher can prefetch data that will be needed by the application reducing the effective memory latency if the data is accessed in a non sequential manner the automatic hardware prefetcher cannot prefetch the data the prefetcher can recognize up to eight concurrent streams see chapter optimizing cache usage for more information on the hardware prefetcher on intel core duo intel core duo intel core solo pentium intel xeon and pentium m processors memory coherence is maintained on byte cache lines rather than byte cache lines as in earlier processors this can increase the opportunity for false sharing user source coding rule m impact l generality beware of false sharing within a cache line bytes and within a sector of bytes on processors based on intel netburst microarchitecture stack alignment the easiest way to avoid stack alignment problems is to keep the stack aligned at all times for example a language that supports bit bit bit and bit data quantities but never uses bit data quantities can require the stack to always be aligned on a bit boundary assembly compiler coding rule h impact m generality if bit data is ever passed as a parameter or allocated on the stack make sure that the stack is aligned to an byte boundary doing this will require using a general purpose register such as ebp as a frame pointer the trade off is between causing unaligned bit references if the stack is not aligned and causing extra general purpose register spills if the stack is aligned note that a performance penalty is caused only when an unaligned access splits a cache line this means that one out of eight spatially consecutive unaligned accesses is always penalized a routine that makes frequent use of bit data can avoid stack misalignment by placing the code described in example in the function prologue and epilogue example dynamic stack alignment prologue subl esp save frame ptr movl esp ebp movl ebp esp new frame pointer andl ebp aligned to bits movl ebp esp save old stack ptr subl esp framesize allocate space callee saves etc example dynamic stack alignment contd epilogue callee restores etc movl esp ebp restore stack ptr movl ebp esp restore frame ptr addl esp ret if for some reason it is not possible to align the stack for bits the routine should access the parameter and save it into a register or known aligned storage thus incur ring the penalty only once capacity limits and aliasing in caches there are cases in which addresses with a given stride will compete for some resource in the memory hierarchy typically caches are implemented to have multiple ways of set associativity with each way consisting of multiple sets of cache lines or sectors in some cases multiple memory references that compete for the same set of each way in a cache can cause a capacity issue there are aliasing conditions that apply to specific microarchitectures note that first level cache lines are bytes thus the least significant bits are not considered in alias comparisons for processors based on intel netburst microarchitecture data is loaded into the second level cache in a sector of bytes so the least significant bits are not considered in alias compar isons capacity limits in set associative caches capacity limits may be reached if the number of outstanding memory references that are mapped to the same set in each way of a given cache exceeds the number of ways of that cache the conditions that apply to the first level data cache and second level cache are listed below set conflicts multiple references map to the same first level cache set the conflicting condition is a stride determined by the size of the cache in bytes divided by the number of ways these competing memory references can cause excessive cache misses only if the number of outstanding memory references exceeds the number of ways in the working set on pentium and intel xeon processors with a cpuid signature of family encoding model encoding of or there will be an excess of first level cache misses for more than simultaneous competing memory references to addresses with kbyte modulus on pentium and intel xeon processors with a cpuid signature of family encoding model encoding there will be an excess of first level cache misses for more than simultaneous competing references to addresses that are apart by kbyte modulus on intel core duo intel core duo intel core solo and pentium m processors there will be an excess of first level cache misses for more than simultaneous references to addresses that are apart by kbyte modulus set conflicts multiple references map to the same second level cache set the conflicting condition is also determined by the size of the cache or the number of ways on pentium and intel xeon processors there will be an excess of second level cache misses for more than simultaneous competing references the stride sizes that can cause capacity issues are kbytes kbytes or kbytes depending of the size of the second level cache on pentium m processors the stride sizes that can cause capacity issues are kbytes or kbytes depending of the size of the second level cache on intel core duo intel core duo intel core solo processors stride size of kbytes can cause capacity issue if the number of simultaneous accesses exceeded the way associativity of the cache aliasing cases in processors based on intel netburst microarchitecture aliasing conditions that are specific to processors based on intel netburst microar chitecture are kbytes for code there can only be one of these in the trace cache at a time if two traces whose starting addresses are kbytes apart are in the same working set the symptom will be a high trace cache miss rate solve this by offsetting one of the addresses by one or more bytes data conflict there can only be one instance of the data in the first level cache at a time if a reference load or store occurs and its linear address matches a data conflict condition with another reference load or store that is under way then the second reference cannot begin until the first one is kicked out of the cache on pentium and intel xeon processors with a cpuid signature of family encoding model encoding of or the data conflict condition applies to addresses having identical values in bits this is also referred to as a kbyte aliasing conflict if you avoid this kind of aliasing you can speed up programs by a factor of three if they load frequently from preceding stores with aliased addresses and little other instruction level parallelism is available the gain is smaller when loads alias with other loads which causes thrashing in the first level cache on pentium and intel xeon processors with a cpuid signature of family encoding model encoding the data conflict condition applies to addresses having identical values in bits aliasing cases in the pentium m intel core solo intel core duo and intel core duo processors pentium m intel core solo intel core duo and intel core duo processors have the following aliasing case store forwarding if a store to an address is followed by a load from the same address the load will not proceed until the store data is available if a store is followed by a load and their addresses differ by a multiple of kbytes the load stalls until the store operation completes assembly compiler coding rule h impact m generality avoid having a store followed by a non dependent load with addresses that differ by a multiple of kbytes also lay out data or order computation to avoid having cache lines that have linear addresses that are a multiple of kbytes apart in the same working set avoid having more than cache lines that are some multiple of kbytes apart in the same first level cache working set and avoid having more than cache lines that are some multiple of kbytes apart in the same first level cache working set when declaring multiple arrays that are referenced with the same index and are each a multiple of kbytes as can happen with data layouts pad them to avoid declaring them contiguously padding can be accomplished by either intervening declarations of other variables or by artificially increasing the dimension user source coding rule h impact ml generality consider using a special memory allocation library with address offset capability to avoid aliasing one way to implement a memory allocator to avoid aliasing is to allocate more than enough space and pad for example allocate structures that are kb instead of kbytes to avoid the kbyte aliasing or have the allocator pad and return random offsets that are a multiple of bytes the size of a cache line user source coding rule m impact m generality when padding variable declarations to avoid aliasing the greatest benefit comes from avoiding aliasing on second level cache lines suggesting an offset of bytes or more kbyte memory aliasing occurs when the code accesses two different memory loca tions with a kbyte offset between them the kbyte aliasing situation can mani fest in a memory copy routine where the addresses of the source buffer and destination buffer maintain a constant offset and the constant offset happens to be a multiple of the byte increment from one iteration to the next example shows a routine that copies bytes of memory in each iteration of a loop if the offsets modular between source buffer eax and destination buffer edx differ by loads have to wait until stores have been retired before they can continue for example at offset the load of the next itera tion is kbyte aliased current iteration store therefore the loop must wait until the store operation completes making the entire loop serialized the amount of time needed to wait decreases with larger offset until offset of resolves the issue as there is no pending stores by the time of the load with same address the intel core microarchitecture provides a performance monitoring event see in intel and ia architectures software developer manual volume that allows software tuning effort to detect the occurrence of aliasing conditions example aliasing between loads and stores across loop iterations lp movaps eax ecx movaps edx ecx add ecx jnz lp mixing code and data the aggressive prefetching and pre decoding of instructions by intel processors have two related effects self modifying code works correctly according to the intel architecture processor requirements but incurs a significant performance penalty avoid self modifying code if possible placing writable data in the code segment might be impossible to distinguish from self modifying code writable data in the code segment might suffer the same performance penalty as self modifying code assembly compiler coding rule m impact l generality if hopefully read only data must occur on the same page as code avoid placing it immediately after an indirect jump for example follow an indirect jump with its mostly likely target and place the data after an unconditional branch tuning suggestion in rare cases a performance problem may be caused by executing data on a code page as instructions this is very likely to happen when execution is following an indirect branch that is not resident in the trace cache if this is clearly causing a performance problem try moving the data elsewhere or inserting an illegal opcode or a pause instruction immediately after the indirect branch note that the latter two alternatives may degrade performance in some circumstances assembly compiler coding rule h impact l generality always put code and data on separate pages avoid self modifying code wherever possible if code is to be modified try to do it all at once and make sure the code that performs the modifications and the code being modified are on separate kbyte pages or on separate aligned kbyte subpages self modifying code self modifying code smc that ran correctly on pentium iii processors and prior implementations will run correctly on subsequent implementations smc and cross modifying code when multiple processors in a multiprocessor system are writing to a code page should be avoided when high performance is desired software should avoid writing to a code page in the same kbyte subpage that is being executed or fetching code in the same kbyte subpage of that is being written in addition sharing a page containing directly or speculatively executed code with another processor as a data page can trigger an smc condition that causes the entire pipeline of the machine and the trace cache to be cleared this is due to the self modifying code condition dynamic code need not cause the smc condition if the code written fills up a data page before that page is accessed as code dynamically modified code for example from target fix ups is likely to suffer from the smc condition and should be avoided where possible avoid the condition by introducing indirect branches and using data tables on data pages not code pages using register indirect calls write combining write combining wc improves performance in two ways on a write miss to the first level cache it allows multiple stores to the same cache line to occur before that cache line is read for ownership rfo from further out in the cache memory hierarchy then the rest of line is read and the bytes that have not been written are combined with the unmodified bytes in the returned line write combining allows multiple writes to be assembled and written further out in the cache hierarchy as a unit this saves port and bus traffic saving traffic is particularly important for avoiding partial writes to uncached memory there are six write combining buffers on pentium and intel xeon processors with a cpuid signature of family encoding model encoding there are write combining buffers two of these buffers may be written out to higher cache levels and freed up for use on other write misses only four write combining buffers are guaranteed to be available for simultaneous use write combining applies to memory type wc it does not apply to memory type uc there are six write combining buffers in each processor core in intel core duo and intel core solo processors processors based on intel core microarchitecture have eight write combining buffers in each core assembly compiler coding rule h impact l generality if an inner loop writes to more than four arrays four distinct cache lines apply loop fission to break up the body of the loop such that only four arrays are being written to in each iteration of each of the resulting loops write combining buffers are used for stores of all memory types they are particu larly important for writes to uncached memory writes to different parts of the same cache line can be grouped into a single full cache line bus transaction instead of going across the bus since they are not cached as several partial writes avoiding partial writes can have a significant impact on bus bandwidth bound graphics appli cations where graphics buffers are in uncached memory separating writes to uncached memory and writes to writeback memory into separate phases can assure that the write combining buffers can fill before getting evicted by other write traffic eliminating partial write transactions has been found to have performance impact on the order of for some applications because the cache lines are bytes a write to the bus for bytes will result in partial bus transactions when coding functions that execute simultaneously on two threads reducing the number of writes that are allowed in an inner loop will help take full advantage of write combining store buffers for write combining buffer recommendations for hyper threading technology see chapter multicore and hyper threading tech nology store ordering and visibility are also important issues for write combining when a write to a write combining buffer for a previously unwritten cache line occurs there will be a read for ownership rfo if a subsequent write happens to another write combining buffer a separate rfo may be caused for that cache line subsequent writes to the first cache line and write combining buffer will be delayed until the second rfo has been serviced to guarantee properly ordered visibility of the writes if the memory type for the writes is write combining there will be no rfo since the line is not cached and there is no such delay for details on write combining see chapter power optimization for mobile usages of intel and ia archi tectures software developer manual volume locality enhancement locality enhancement can reduce data traffic originating from an outer level sub system in the cache memory hierarchy this is to address the fact that the access cost in terms of cycle count from an outer level will be more expensive than from an inner level typically the cycle cost of accessing a given cache level or memory system varies across different microarchitectures processor implementations and platform components it may be sufficient to recognize the relative data access cost trend by locality rather than to follow a large table of numeric values of cycle costs listed per locality per processor platform implementations etc the general trend is typically that access cost from an outer sub system may be approximately more expensive than accessing data from the immediate inner level in the cache memory hierarchy assuming similar degrees of data access parallelism thus locality enhancement should start with characterizing the dominant data traffic locality section a application performance tools describes some techniques that can be used to determine the dominant data traffic locality for any workload even if cache miss rates of the last level cache may be low relative to the number of cache references processors typically spend a sizable portion of their execution time waiting for cache misses to be serviced reducing cache misses by enhancing a program locality is a key optimization this can take several forms blocking to iterate over a portion of an array that will fit in the cache with the purpose that subsequent references to the data block or tile will be cache hit references loop interchange to avoid crossing cache lines or page boundaries loop skewing to make accesses contiguous locality enhancement to the last level cache can be accomplished with sequencing the data access pattern to take advantage of hardware prefetching this can also take several forms transformation of a sparsely populated multi dimensional array into a one dimension array such that memory references occur in a sequential small stride pattern that is friendly to the hardware prefetch see section data prefetch optimal tile size and shape selection can further improve temporal data locality by increasing hit rates into the last level cache and reduce memory traffic resulting from the actions of hardware prefetching see section hardware prefetching and cache blocking techniques it is important to avoid operations that work against locality enhancing techniques using the lock prefix heavily can incur large delays when accessing memory regard less of whether the data is in the cache or in system memory user source coding rule h impact h generality optimization techniques such as blocking loop interchange loop skewing and packing are best done by the compiler optimize data structures either to fit in one half of the first level cache or in the second level cache turn on loop optimizations in the compiler to enhance locality for nested loops optimizing for one half of the first level cache will bring the greatest performance benefit in terms of cycle cost per data access if one half of the first level cache is too small to be practical optimize for the second level cache optimizing for a point in between for example for the entire first level cache will likely not bring a substantial improvement over optimizing for the second level cache minimizing bus latency each bus transaction includes the overhead of making requests and arbitrations the average latency of bus read and bus write transactions will be longer if reads and writes alternate segmenting reads and writes into phases can reduce the average latency of bus transactions this is because the number of incidences of successive transactions involving a read following a write or a write following a read are reduced user source coding rule m impact ml generality if there is a blend of reads and writes on the bus changing the code to separate these bus transactions into read phases and write phases can help performance note however that the order of read and write operations on the bus is not the same as it appears in the program bus latency for fetching a cache line of data can vary as a function of the access stride of data references in general bus latency will increase in response to increasing values of the stride of successive cache misses independently bus latency will also increase as a function of increasing bus queue depths the number of outstanding bus requests of a given transaction type the combination of these two trends can be highly non linear in that bus latency of large stride bandwidth sensitive situations are such that effective throughput of the bus system for data parallel accesses can be significantly less than the effective throughput of small stride bandwidth sensitive situations to minimize the per access cost of memory traffic or amortize raw memory latency effectively software should control its cache miss pattern to favor higher concentra tion of smaller stride cache misses user source coding rule h impact h generality to achieve effective amortization of bus latency software should favor data access patterns that result in higher concentrations of cache miss patterns with cache miss strides that are significantly smaller than half the hardware prefetch trigger threshold non temporal store bus traffic peak system bus bandwidth is shared by several types of bus activities including reads from memory reads for ownership of a cache line and writes the data transfer rate for bus write transactions is higher if bytes are written out to the bus at a time typically bus writes to writeback wb memory must share the system bus band width with read for ownership rfo traffic non temporal stores do not require rfo traffic they do require care in managing the access patterns in order to ensure bytes are evicted at once rather than evicting several byte chunks although the data bandwidth of full byte bus writes due to non temporal stores is twice that of bus writes to wb memory transferring byte chunks wastes bus request bandwidth and delivers significantly lower data bandwidth this difference is depicted in examples and example using non temporal stores and byte bus write transactions define stridesize lea ecx mov edx xor eax eax slloop movntps xmmword ptr ecx eax movntps xmmword ptr ecx eax movntps xmmword ptr ecx eax movntps xmmword ptr ecx eax bytes is written in one bus transaction add eax stridesize cmp eax edx jl slloop example on temporal stores and partial bus write transactions define stridesize lea ecx mov edx xor eax eax slloop movntps xmmword ptr ecx eax movntps xmmword ptr ecx eax movntps xmmword ptr ecx eax storing bytes results in bus partial transactions add eax stridesize cmp eax edx prefetching recent intel processor families employ several prefetching mechanisms to accelerate the movement of data or code and improve performance hardware instruction prefetcher software prefetch for data hardware prefetch for cache lines of data or instructions hardware instruction fetching and software prefetching in processor based on intel netburst microarchitecture the hardware instruction fetcher reads instructions bytes at a time into the byte instruction streaming buffers instruction fetching for intel core microarchitecture is discussed in section software prefetching requires a programmer to use prefetch hint instructions and anticipate some suitable timing and location of cache misses in intel core microarchitecture software prefetch instructions can prefetch beyond page boundaries and can perform one to four page walks software prefetch instructions issued on fill buffer allocations retire after the page walk completes and the dcu miss is detected software prefetch instructions can trigger all hardware prefetchers in the same manner as do regular loads software prefetch operations work the same way as do load from memory opera tions with the following exceptions software prefetch instructions retire after virtual to physical address translation is completed if an exception such as page fault is required to prefetch the data then the software prefetch instruction retires without prefetching data software and hardware prefetching in prior microarchitectures pentium and intel xeon processors based on intel netburst microarchitecture intro duced hardware prefetching in addition to software prefetching the hardware prefetcher operates transparently to fetch data and instruction streams from memory without requiring programmer intervention subsequent microarchitectures continue to improve and add features to the hardware prefetching mechanisms earlier implementations of hardware prefetching mechanisms focus on prefetching data and instruction from memory to more recent implementations provide addi tional features to prefetch data from to in intel netburst microarchitecture the hardware prefetcher can track indepen dent streams the pentium m processor also provides a hardware prefetcher for data it can track separate streams in the forward direction and streams in the backward direc tion the processor prefetchnta instruction also fetches bytes into the first level data cache without polluting the second level cache intel core solo and intel core duo processors provide more advanced hardware prefetchers for data than pentium m processors key differences are summarized in table although the hardware prefetcher operates transparently requiring no intervention by the programmer it operates most efficiently if the programmer specifically tailors data access patterns to suit its characteristics it favors small stride cache miss patterns optimizing data access patterns to suit the hardware prefetcher is highly recommended and should be a higher priority consideration than using soft ware prefetch instructions the hardware prefetcher is best for small stride data access patterns in either direc tion with a cache miss stride not far from bytes this is true for data accesses to addresses that are either known or unknown at the time of issuing the load opera tions software prefetch can complement the hardware prefetcher if used carefully there is a trade off to make between hardware and software prefetching this pertains to application characteristics such as regularity and stride of accesses bus bandwidth issue bandwidth the latency of loads on the critical path and whether access patterns are suitable for non temporal prefetch will also have an impact for a detailed description of how to use prefetching see chapter optimizing cache usage chapter optimizing for simd integer applications contains an example that uses software prefetch to implement a memory copy algorithm tuning suggestion if a load is found to miss frequently either insert a prefetch before it or if issue bandwidth is a concern move the load up to execute earlier hardware prefetching for first level data cache the hardware prefetching mechanism for in intel core microarchitecture is discussed in section a similar prefetch mechanism is also available to processors based on intel netburst microarchitecture with cpuid signature of family and model example depicts a technique to trigger hardware prefetch the code demon strates traversing a linked list and performing some computational work on members of each element that reside in different cache lines each element is of size bytes the total size of all elements is larger than can be fitted in the cache example using dcu hardware prefetch original code modified sequence benefit from prefetch mov ebx dword ptr first mov ebx dword ptr first xor eax eax xor eax eax mov eax ebx mov eax ebx mov ecx mov eax ebx mov eax ebx add eax eax mov ecx and eax sub ecx add eax eax jnz and eax sub ecx jnz mov eax ebx mov ecx add eax eax and eax sub ecx jnz mov eax ebx mov ecx add eax eax and eax sub ecx jnz do_some_work_2 mov ebx ebx test ebx ebx jnz mov ebx ebx test ebx ebx jnz the additional instructions to load data from one member in the modified sequence can trigger the dcu hardware prefetch mechanisms to prefetch data in the next cache line enabling the work on the second member to complete sooner software can gain from the first level data cache prefetchers in two cases if data is not in the second level cache the first level data cache prefetcher enables early trigger of the second level cache prefetcher if data is in the second level cache and not in the first level data cache then the first level data cache prefetcher triggers earlier data bring up of sequential cache line to the first level data cache there are situations that software should pay attention to a potential side effect of triggering unnecessary dcu hardware prefetches if a large data structure with many members spanning many cache lines is accessed in ways that only a few of its members are actually referenced but there are multiple pair accesses to the same cache line the dcu hardware prefetcher can trigger fetching of cache lines that are not needed in example references to the pts array and altpts will trigger dcu prefetch to fetch additional cache lines that won t be needed if significant negative performance impact is detected due to dcu hardware prefetch on a portion of the code software can try to reduce the size of that contemporaneous working set to be less than half of the cache example avoid causing dcu hardware prefetch to fetch un needed lines while currbond null myatom currbond myatom currbond if currstep laststep currstep laststep currstep currstep double ux pts x pts x double uy pts y pts y double uz pts z pts z auxpts x ux auxpts y uy auxpts z uz auxpts x ux auxpts y uy auxpts z uz currbond currbond next to fully benefit from these prefetchers organize and access the data using one of the following methods method organize the data so consecutive accesses can usually be found in the same kbyte page access the data in constant strides forward or backward ip prefetcher method organize the data in consecutive lines access the data in increasing addresses in sequential cache lines example demonstrates accesses to sequential cache lines that can benefit from the first level cache prefetcher example technique for using hardware prefetch unsigned int j a b for j j num j a j b j use these two values by elevating the load operations from memory to the beginning of each iteration it is likely that a significant part of the latency of the pair cache line transfer from memory to the second level cache will be in parallel with the transfer of the first cache line the ip prefetcher uses only the lower bits of the address to distinguish a specific address if the code size of a loop is bigger than bytes two loads may appear similar in the lowest bits and the ip prefetcher will be restricted therefore if you have a loop bigger than bytes make sure that no two loads have the same lowest bits in order to use the ip prefetcher hardware prefetching for second level cache the intel core microarchitecture contains two second level cache prefetchers streamer loads data or instructions from memory to the second level cache to use the streamer organize the data or instructions in blocks of bytes aligned on bytes the first access to one of the two cache lines in this block while it is in memory triggers the streamer to prefetch the pair line to software the streamer functionality is similar to the adjacent cache line prefetch mechanism found in processors based on intel netburst microarchitecture data prefetch logic dpl dpl and streamer are triggered only by writeback memory type they prefetch only inside page boundary kbytes both prefetchers can be triggered by software prefetch instructions and by prefetch request from dcu prefetchers dpl can also be triggered by read for ownership rfo operations the streamer can also be triggered by dpl requests for cache misses software can gain from organizing data both according to the instruction pointer and according to line strides for example for matrix calculations columns can be prefetched by ip based prefetches and rows can be prefetched by dpl and the streamer cacheability instructions provides additional cacheability instructions that extend those provided in sse the new cacheability instructions include new streaming store instructions new cache line flush instruction new memory fencing instructions for more information see chapter optimizing cache usage rep prefix and data movement the rep prefix is commonly used with string move instructions for memory related library functions such as memcpy using rep movsd or memset using rep stos these string mov instructions with the rep prefixes are implemented in ms rom and have several implementation variants with different performance levels the specific variant of the implementation is chosen at execution time based on data layout alignment and the counter ecx value for example movsb stosb with the rep prefix should be used with counter value less than or equal to three for best performance string move store instructions have multiple data granularities for efficient data movement larger data granularities are preferable this means better efficiency can be achieved by decomposing an arbitrary counter value into a number of double words plus single byte moves with a count value less than or equal to because software can use simd data movement instructions to move bytes at a time the following paragraphs discuss general guidelines for designing and imple menting high performance library functions such as memcpy memset and memmove four factors are to be considered throughput per iteration if two pieces of code have approximately identical path lengths efficiency favors choosing the instruction that moves larger pieces of data per iteration also smaller code size per iteration will in general reduce overhead and improve throughput sometimes this may involve a comparison of the relative overhead of an iterative loop structure versus using rep prefix for iteration address alignment data movement instructions with highest throughput usually have alignment restrictions or they operate more efficiently if the destination address is aligned to its natural data size specifically byte moves need to ensure the destination address is aligned to byte boundaries and bytes moves perform better if the destination address is aligned to byte boundaries frequently moving at doubleword granularity performs better with addresses that are byte aligned rep string move vs simd move implementing general purpose memory functions using simd extensions usually requires adding some prolog code to ensure the availability of simd instructions preamble code to facilitate aligned data movement requirements at runtime throughput comparison must also take into consideration the overhead of the prolog when considering a rep string implementation versus a simd approach cache eviction if the amount of data to be processed by a memory routine approaches half the size of the last level on die cache temporal locality of the cache may suffer using streaming store instructions for example movntq movntdq can minimize the effect of flushing the cache the threshold to start using a streaming store depends on the size of the last level cache determine the size using the deterministic cache parameter leaf of cpuid techniques for using streaming stores for implementing a memset type library must also consider that the application can benefit from this technique only if it has no immediate need to reference the target addresses this assumption is easily upheld when testing a streaming store implementation on a micro benchmark configuration but violated in a full scale application situation when applying general heuristics to the design of general purpose high perfor mance library routines the following guidelines can are useful when optimizing an arbitrary counter value n and address alignment different techniques may be neces sary for optimal performance depending on the magnitude of n when n is less than some small count where the small count threshold will vary between microarchitectures empirically may be a good value when optimizing for intel netburst microarchitecture each case can be coded directly without the overhead of a looping structure for example bytes can be processed using two movsd instructions explicitly and a movsb with rep counter equaling when n is not small but still less than some threshold value which may vary for different micro architectures but can be determined empirically an simd implementation using run time cpuid and alignment prolog will likely deliver less throughput due to the overhead of the prolog a rep string implementation should favor using a rep string of doublewords to improve address alignment a small piece of prolog code using movsb stosb with a count less than can be used to peel off the non aligned data moves before starting to use movsd stosd when n is less than half the size of last level cache throughput consideration may favor either an approach using a rep string with the largest data granularity because a rep string has little overhead for loop iteration and the branch misprediction overhead in the prolog epilogue code to handle address alignment is amortized over many iterations an iterative approach using the instruction with largest data granularity where the overhead for simd feature detection iteration overhead and prolog epilogue for alignment control can be minimized the trade off between these approaches may depend on the microarchitecture an example of memset implemented using stosd for arbitrary counter value with the destination address aligned to doubleword boundary in bit mode is shown in example when n is larger than half the size of the last level cache using byte granularity streaming stores with prolog epilog for address alignment will likely be more efficient if the destination addresses will not be referenced immediately afterwards example rep stosd with arbitrary count size and byte aligned destination a c example of memset equivalent implementation using rep stosd void memset void dst int c size char d char dst i for i i size i d char c push edi movzx eax byte ptr esp mov ecx eax shl ecx or ecx eax mov ecx eax shl ecx or eax ecx mov edi esp byte aligned mov ecx esp byte count shr ecx do dword cmp ecx jle test edi jz stosd peel off one dword dec ecx byte aligned rep stosd mov ecx esp and ecx do count rep stosb optimal with pop edi ret memory routines in the runtime library generated by intel compilers are optimized across a wide range of address alignments counter values and microarchitectures in most cases applications should take advantage of the default memory routines provided by intel compilers in some situations the byte count of the data is known by the context as opposed to being known by a parameter passed from a call and one can take a simpler approach than those required for a general purpose library routine for example if the byte count is also small using rep movsb stosb with a count less than four can ensure good address alignment and loop unrolling to finish the remaining data using movsd stosd can reduce the overhead associated with iteration using a rep prefix with string move instructions can provide high performance in the situations described above however using a rep prefix with string scan instructions scasb scasw scasd scasq or compare instructions cmpsb cmpsw smpsd smpsq is not recommended for high performance consider using simd instructions instead floating point considerations when programming floating point applications it is best to start with a high level programming language such as c c or fortran many compilers perform floating point scheduling and optimization when it is possible however in order to produce optimal code the compiler may need some assistance guidelines for optimizing floating point code user source coding rule m impact m generality enable the compiler use of sse or instructions with appropriate switches follow this procedure to investigate the performance of your floating point applica tion understand how the compiler handles floating point code look at the assembly dump and see what transforms are already performed on the program study the loop nests in the application that dominate the execution time determine why the compiler is not creating the fastest code see if there is a dependence that can be resolved determine the problem area bus bandwidth cache locality trace cache bandwidth or instruction latency focus on optimizing the problem area for example adding prefetch instructions will not help if the bus is already saturated if trace cache bandwidth is the problem added prefetch µops may degrade performance also in general follow the general coding recommendations discussed in this chapter including blocking the cache using prefetch enabling vectorization unrolling loops user source coding rule h impact ml generality make sure your application stays in range to avoid denormal values underflows out of range numbers cause very high overhead user source coding rule m impact ml generality do not use double precision unless necessary set the precision control pc field in the fpu control word to single precision this allows single precision bit computation to complete faster on some operations for example divides due to early out however be careful of introducing more than a total of two values for the floating point control word or there will be a large performance penalty see section user source coding rule h impact ml generality use fast float to int routines fisttp or instructions if coding these routines use the fisttp instruction if is available or the and instructions if coding with streaming simd extensions many libraries generate code that does more work than is necessary the fisttp instruction in can convert floating point values to bit bit or bit inte gers using truncation without accessing the floating point control word fcw the instructions and save many µops and some store forwarding delays over some compiler implementations this avoids changing the rounding mode user source coding rule m impact ml generality removing data dependence enables the out of order engine to extract more ilp from the code when summing up the elements of an array use partial sums instead of a single accumulator for example to calculate z a b c d instead of x a b y x c z y d use x a b y c d z x y user source coding rule m impact ml generality usually math libraries take advantage of the transcendental instructions for example fsin when evaluating elementary functions if there is no critical need to evaluate the transcendental functions using the extended precision of bits applications should consider an alternate software based approach such as a look up table based algorithm using interpolation techniques it is possible to improve transcendental performance with these techniques by choosing the desired numeric precision and the size of the look up table and by taking advantage of the parallelism of the sse and the instructions floating point modes and exceptions when working with floating point numbers high speed microprocessors frequently must deal with situations that need special handling in hardware or code floating point exceptions the most frequent cause of performance degradation is the use of masked floating point exception conditions such as arithmetic overflow arithmetic underflow denormalized operand refer to chapter of intel and ia architectures software developer manual volume for definitions of overflow underflow and denormal exceptions denormalized floating point numbers impact performance in two ways directly when are used as operands indirectly when are produced as a result of an underflow situation if a floating point application never underflows the denormals can only come from floating point constants user source coding rule h impact ml generality denormalized floating point constants should be avoided as much as possible denormal and arithmetic underflow exceptions can occur during the execution of instructions or sse instructions processors based on intel netburst microarchitecture handle these exceptions more efficiently when executing sse instructions and when speed is more important than complying with the ieee standard the following paragraphs give recommendations on how to opti mize your code to reduce performance degradations related to floating point excep tions dealing with floating point exceptions in fpu code every special situation listed in section floating point exceptions is costly in terms of performance for that reason fpu code should be written to avoid these situations there are basically three ways to reduce the impact of overflow underflow situations with fpu code choose floating point data types that are large enough to accommodate results without generating arithmetic overflow and underflow exceptions scale the range of operands results to reduce as much as possible the number of arithmetic overflow underflow situations keep intermediate results on the fpu register stack until the final results have been computed and stored in memory overflow or underflow is less likely to happen when intermediate results are kept in the fpu stack this is because data on the stack is stored in double extended precision format and overflow underflow conditions are detected accordingly denormalized floating point constants which are read only and hence never change should be avoided and replaced if possible with zeros of the same sign floating point exceptions in sse code most special situations that involve masked floating point exceptions are handled efficiently in hardware when a masked overflow exception occurs while executing sse code processor hardware can handles it without performance penalty underflow exceptions and denormalized source operands are usually treated according to the ieee specification but this can incur significant performance delay if a programmer is willing to trade pure ieee compliance for speed two non ieee compliant modes are provided to speed situations where underflows and input are frequent ftz mode and daz mode when the ftz mode is enabled an underflow result is automatically converted to a zero with the correct sign although this behavior is not compliant with ieee it is provided for use in applications where performance is more important than ieee compliance since denormal results are not produced when the ftz mode is enabled the only denormal floating point numbers that can be encountered in ftz mode are the ones specified as constants read only the daz mode is provided to handle denormal source operands efficiently when running a simd floating point application when the daz mode is enabled input denormals are treated as zeros with the same sign enabling the daz mode is the way to deal with denormal floating point constants when performance is the objec tive if departing from the ieee specification is acceptable and performance is critical run sse applications with ftz and daz modes enabled note the daz mode is available with both the sse and extensions although the speed improvement expected from this mode is fully realized only in sse code floating point modes on the pentium iii processor the fldcw instruction is an expensive operation on early generations of pentium processors fldcw is improved only for situations where an application alternates between two constant values of the fpu control word fcw such as when performing conversions to integers on pentium m intel core solo intel core duo and intel core duo processors fldcw is improved over previous generations specifically the optimization for fldcw in the first two generations of pentium processors allow programmers to alternate between two constant values efficiently for the fldcw optimization to be effective the two constant fcw values are only allowed to differ on the following bits in the fcw fcw precision control fcw rounding control fcw infinity control if programmers need to modify other bits for example mask bits in the fcw the fldcw instruction is still an expensive operation in situations where an application cycles between three or more constant values fldcw optimization does not apply and the performance degradation occurs for each fldcw instruction one solution to this problem is to choose two constant fcw values take advantage of the optimization of the fldcw instruction to alternate between only these two constant fcw values and devise some means to accomplish the task that requires the fcw value without actually changing the fcw to a third constant value an alternative solution is to structure the code so that for periods of time the applica tion alternates between only two constant fcw values when the application later alternates between a pair of different fcw values the performance degradation occurs only during the transition it is expected that simd applications are unlikely to alternate between ftz and daz mode values consequently the simd control word does not have the short latencies that the floating point control register does a read of the mxcsr register has a fairly long latency and a write to the register is a serializing instruction there is no separate control word for single and double precision both use the same modes notably this applies to both ftz and daz modes assembly compiler coding rule h impact m generality minimize changes to bits of the floating point control word changes for more than two values each value being a combination of the following bits precision rounding and infinity control and the rest of bits in fcw leads to delays that are on the order of the pipeline depth rounding mode many libraries provide float to integer library routines that convert floating point values to integer many of these libraries conform to ansi c coding standards which state that the rounding mode should be truncation with the pentium processor one can use the and instructions to convert operands with truncation without ever needing to change rounding modes the cost savings of using these instructions over the methods below is enough to justify using sse and wherever possible when truncation is involved for floating point the fist instruction uses the rounding mode represented in the floating point control word fcw the rounding mode is generally round to nearest so many compiler writers implement a change in the rounding mode in the processor in order to conform to the c and fortran standards this implementation requires changing the control word on the processor using the fldcw instruction for a change in the rounding precision and infinity bits use the fstcw instruction to store the floating point control word then use the fldcw instruction to change the rounding mode to truncation in a typical code sequence that changes the rounding mode in the fcw a fstcw instruction is usually followed by a load operation the load operation from memory should be a bit operand to prevent store forwarding problem if the load opera tion on the previously stored fcw word involves either an bit or a bit operand this will cause a store forwarding problem due to mismatch of the size of the data between the store operation and the load operation to avoid store forwarding problems make sure that the write and read to the fcw are both bit operations if there is more than one change to the rounding precision and infinity bits and the rounding mode is not important to the result use the algorithm in example to avoid synchronization issues the overhead of the fldcw instruction and having to change the rounding mode note that the example suffers from a store forwarding problem which will lead to a performance penalty however its performance is still better than changing the rounding precision and infinity bits among more than two values example algorithm to avoid changing rounding mode lea ecx esp sub esp allocate frame and ecx align pointer on boundary of fld st duplicate fpu stack top fistp qword ptr ecx fild qword ptr ecx mov edx ecx high dword of integer mov eax ecx low dwird of integer test eax eax je example algorithm to avoid changing rounding mode contd fsubp st st tos d round d st st st pop st test edx edx what sign of integer jns positive number is negative fstp dword ptr ecx result of subtraction mov ecx ecx dword of diff single precision add esp xor ecx add ecx if diff then decrement integer adc eax inc eax add carry flag ret positive positive fstp dword ptr ecx result of subtraction mov ecx ecx dword of diff single precision add esp add ecx if diff then decrement integer sbb eax dec eax subtract carry flag ret test edx jnz add esp ret assembly compiler coding rule h impact l generality minimize the number of changes to the rounding mode do not use changes in the rounding mode to implement the floor and ceiling functions if this involves a total of more than two values of the set of rounding precision and infinity bits precision if single precision is adequate use it instead of double precision this is true because single precision operations allow the use of longer simd vectors since more single precision data elements can fit in a register if the precision control pc field in the fpu control word is set to single precision the floating point divider can complete a single precision computation much faster than either a double precision computation or an extended double precision computation if the pc field is set to double precision this will enable those fpu operations on double precision data to complete faster than extended double precision computation these characteristics affect computa tions including floating point divide and square root assembly compiler coding rule h impact l generality minimize the number of changes to the precision mode improving parallelism and the use of fxch the instruction set relies on the floating point stack for one of its operands if the dependence graph is a tree which means each intermediate result is used only once and code is scheduled carefully it is often possible to use only operands that are on the top of the stack or in memory and to avoid using operands that are buried under the top of the stack when operands need to be pulled from the middle of the stack an fxch instruction can be used to swap the operand on the top of the stack with another entry in the stack the fxch instruction can also be used to enhance parallelism dependent chains can be overlapped to expose more independent instructions to the hardware scheduler an fxch instruction may be required to effectively increase the register name space so that more operands can be simultaneously live in processors based on intel netburst microarchitecture however that fxch inhibits issue bandwidth in the trace cache it does this not only because it consumes a slot but also because of issue slot restrictions imposed on fxch if the application is not bound by issue or retirement bandwidth fxch will have no impact the effective instruction window size in processors based on intel netburst microar chitecture is large enough to permit instructions that are as far away as the next iter ation to be overlapped this often obviates the need to use fxch to enhance parallelism the fxch instruction should be used only when it needed to express an algorithm or to enhance parallelism if the size of register name space is a problem the use of xmm registers is recommended assembly compiler coding rule m impact m generality use fxch only where necessary to increase the effective name space this in turn allows instructions to be reordered and made available for execution in parallel out of order execution precludes the need for using fxch to move instruc tions for very short distances vs scalar simd floating point trade offs there are a number of differences between floating point code and scalar floating point code using sse and the following differences should drive decisions about which registers and instructions to use when an input operand for a simd floating point instruction contains values that are less than the representable range of the data type a denormal exception occurs this causes a significant performance penalty an simd floating point operation has a flush to zero mode in which the results will not underflow therefore subsequent computation will not face the performance penalty of handling denormal input operands for example in the case of applications with low lighting levels using flush to zero mode can improve performance by as much as for applications with large numbers of underflows scalar floating point simd instructions have lower latencies than equivalent instructions scalar simd floating point multiply instruction may be pipelined while multiply instruction is not only supports transcendental instructions supports bit precision double extended floating point sse support a maximum of bit precision supports a maximum of bit precision scalar floating point registers may be accessed directly avoiding fxch and top of stack restrictions the cost of converting from floating point to integer with truncation is signifi cantly lower with streaming simd extensions and streaming simd extensions in the processors based on intel netburst microarchitecture than with either changes to the rounding mode or the sequence prescribed in the example assembly compiler coding rule m impact m generality use streaming simd extensions or streaming simd extensions unless you need an feature most arithmetic operations have shorter latency then their counterpart and they eliminate the overhead associated with the management of the register stack scalar sse performance on intel core solo and intel core duo processors on intel core solo and intel core duo processors the combination of improved decoding and op fusion allows instructions which were formerly two three and four µops to go through all decoders as a result scalar sse code can match the performance of code executing through two floating point units on pentium m processors scalar sse code can experience approximately performance degradation relative to code executing through two floating point units in code sequences that have conversions from floating point to integer divide single precision instructions or any precision change code generation from a compiler typically writes data to memory in single precision and reads it again in order to reduce precision using sse scalar code instead of code can generate a large performance benefit using intel netburst microarchitecture and a modest benefit on intel core solo and intel core duo processors recommendation use the compiler switch to generate scalar floating point code rather than code when working with scalar sse code pay attention to the need for clearing the content of unused slots in an xmm register and the associated performance impact for example loading data from memory with movss or movsd causes an extra micro op for zeroing the upper part of the xmm register on pentium m intel core solo and intel core duo processors this penalty can be avoided by using movlpd however using movlpd causes a performance penalty on pentium processors another situation occurs when mixing single precision and double precision code on processors based on intel netburst microarchitecture using has perfor mance penalty relative to the alternative sequence xorps movss on intel core solo and intel core duo processors using is more desirable than the alternative sequence floating point operations with integer operands for processors based on intel netburst microarchitecture splitting floating point operations fiadd fisub fimul and fidiv that take bit integer operands into two instructions fild and a floating point operation is more efficient however for floating point operations with bit integer operands using fiadd fisub fimul and fidiv is equally efficient compared with using separate instructions assembly compiler coding rule m impact l generality try to use bit operands rather than bit operands for fild however do not do so at the expense of introducing a store forwarding problem by writing the two halves of the bit memory operand separately floating point comparison instructions the fcomi and fcmov instructions should be used when performing floating point comparisons using the fcom fcomp and fcompp instructions typically requires additional instruction like fstsw the latter alternative causes more ops to be decoded and should be avoided transcendental functions if an application needs to emulate math functions in software for performance or other reasons see section guidelines for optimizing floating point code it may be worthwhile to inline math library calls because the call and the prologue epilogue involved with such calls can significantly affect the latency of operations note that transcendental functions are supported only in floating point not in streaming simd extensions or streaming simd extensions maximizing pcie performance pcie performance can be dramatically impacted by the size and alignment of upstream reads and writes read and write transactions issued from a pcie agent to the host memory as a general rule the best performance in terms of both band width and latency is obtained by aligning the start addresses of upstream reads and writes on byte boundaries and ensuring that the request size is a multiple of bytes with modest further increases in bandwidth when larger multiples bytes are employed in particular a partial write will cause a delay for the following request read or write a second rule is to avoid multiple concurrently outstanding accesses to a single cache line this can result in a conflict which in turn can cause serialization of accesses that would otherwise be pipelined resulting in higher latency and or lower bandwidth patterns that violate this rule include sequential accesses reads or writes that are not a multiple of bytes as well as explicit accesses to the same cache line address overlapping requests those with different start addresses but with request lengths that result in overlap of the requests can have the same effect for example a byte read of address followed by a byte read of address will cause a conflict and a likely delay for the second read upstream writes that are a multiple of byte but are non aligned will have the performance of a series of partial and full sequential writes for example a write of length byte to address will perform similarly to sequential writes of lengths and to addresses and respectively for pcie cards implementing multi function devices such as dual or quad port network interface cards nics or dual gpu graphics cards it is important to note that non optimal behavior by one of those devices can impact the bandwidth and or latency observed by the other devices on that card with respect to the behavior described in this section all traffic on a given pcie port is treated as if it originated from a single device and function for the best pcie bandwidth align start addresses of upstream reads and writes on byte boundaries use read and write requests that are a multiple of bytes eliminate or avoid sequential and random partial line upstream writes eliminate or avoid conflicting upstream reads including sequential partial line reads techniques for avoiding performance pitfalls include cache line aligning all descrip tors and data buffers padding descriptors that are written upstream to byte alignment buffering incoming data to achieve larger upstream write payloads allo cating data structures intended for sequential reading by the pcie device in such a way as to enable use of multiple of byte reads the negative impact of unopti mized reads and writes depends on the specific workload and the microarchitecture on which the product is based chapter coding for simd architectures processors based on intel core microarchitecture supports mmx sse and processors based on enhanced intel core microarchitecture supports mmx sse and processors based on intel microarchi tecture nehalem supports mmx sse and intel pentium intel xeon and pentium m processors include support for sse and mmx technology were introduced with the pentium processor supporting hyper threading technology at nm technology intel core solo and intel core duo processors support sse and mmx single instruction multiple data simd technologies enable the development of advanced multimedia signal processing and modeling applications single instruction multiple data techniques can be applied to text string processing lexing and parser applications this is covered in chapter and simd programming for text processing lexing parsing to take advantage of the performance opportunities presented by these capabilities do the following ensure that the processor supports mmx technology sse and ensure that the operating system supports mmx technology and sse os support for and is the same as os support for sse employ the optimization and scheduling strategies described in this book use stack and data alignment techniques to keep data properly aligned for efficient memory use utilize the cacheability instructions offered by sse and where appropriate checking for processor support of simd technologies this section shows how to check whether a processor supports mmx technology sse and simd technology can be included in your application in three ways check for the simd technology during installation if the desired simd technology is available the appropriate dlls can be installed check for the simd technology during program execution and install the proper dlls at runtime this is effective for programs that may be executed on different machines create a fat binary that includes multiple versions of routines versions that use simd technology and versions that do not check for simd technology during program execution and run the appropriate versions of the routines this is especially effective for programs that may be executed on different machines checking for mmx technology support if mmx technology is available then cpuid edx bit use the code segment in example to test for mmx technology example identification of mmx technology with cpuid identify existence of cpuid instruction identify signature is genuine intel mov eax request for feature flags cpuid cpuid instruction test edx is mmx technology bit bit in feature flags equal to jnz found for more information on cpuid see intel processor identification with cpuid instruction order number checking for streaming simd extensions support checking for processor support of streaming simd extensions sse on your processor is similar to checking for mmx technology however operating system os must provide support for sse states save and restore on context switches to ensure consistent application behavior when using sse instructions to check whether your system supports sse follow these steps check that your processor supports the cpuid instruction check the feature bits of cpuid for sse existence example shows how to find the sse feature bit bit in cpuid feature flags example identification of sse with cpuid checking for streaming simd extensions support checking for support of is like checking for sse support the os requirements for support are the same as the os requirements for sse to check whether your system supports follow these steps check that your processor has the cpuid instruction check the feature bits of cpuid for technology existence example shows how to find the feature bit bit in the cpuid feature flags example identification of with cpuid identify existence of cpuid instruction identify signature is genuine intel mov eax request for feature flags cpuid cpuid instruction test edx bit in feature flags equal to jnz found checking for streaming simd extensions support includes instructions of those are suited for simd or style program ming checking for support of instructions is similar to checking for sse support the os requirements for support are the same as the requirements for sse to check whether your system supports the and simd instructions of follow these steps check that your processor has the cpuid instruction check the ecx feature bit of cpuid for technology existence example shows how to find the feature bit bit of ecx in the cpuid feature flags example identification of with cpuid identify existence of cpuid instruction identify signature is genuine intel mov eax request for feature flags cpuid cpuid instruction test ecx bit in feature flags equal to jnz found software must check for support of monitor and mwait before attempting to use monitor and mwait detecting the availability of monitor and mwait can be done using a code sequence similar to example the availability of monitor and mwait is indicated by bit of the returned value in ecx checking for supplemental streaming simd extensions support checking for support of is similar to checking for sse support the os require ments for support are the same as the requirements for sse to check whether your system supports follow these steps check that your processor has the cpuid instruction check the feature bits of cpuid for technology existence example shows how to find the feature bit in the cpuid feature flags example identification of with cpuid identify existence of cpuid instruction identify signature is genuine intel mov eax request for feature flags cpuid cpuid instruction test ecx ecx bit jnz found checking for support checking for support of is similar to checking for sse support the os requirements for support are the same as the requirements for sse to check whether your system supports follow these steps check that your processor has the cpuid instruction check the feature bits of cpuid for example shows how to find the feature bit in the cpuid feature flags example identification of with cpuid considerations for code conversion to simd programming the vtune performance enhancement environment cd provides tools to aid in the evaluation and tuning before implementing them you need answers to the following questions will the current code benefit by using mmx technology streaming simd extensions streaming simd extensions streaming simd extensions or supplemental streaming simd extensions is this code integer or floating point what integer word size or floating point precision is needed what coding techniques should i use what guidelines do i need to follow how should i arrange and align the datatypes figure provides a flowchart for the process of converting code to mmx tech nology sse or no code benefits from simd yes floating point integer or floating point integer why fp performance range or precision can convert to integer yes no can convert to single precision yes no stop figure converting to streaming simd extensions chart to use any of the simd technologies optimally you must evaluate the following situ ations in your code fragments that are computationally intensive fragments that are executed often enough to have an impact on performance fragments that with little data dependent control flow fragments that require floating point computations fragments that can benefit from moving data bytes at a time fragments of computation that can coded using fewer instructions fragments that require help in using the cache hierarchy efficiently identifying hot spots to optimize performance use the vtune performance analyzer to find sections of code that occupy most of the computation time such sections are called the hotspots see appendix a application performance tools the vtune analyzer provides a hotspots view of a specific module to help you identify sections in your code that take the most cpu time and that have potential perfor mance problems the hotspots view helps you identify sections in your code that take the most cpu time and that have potential performance problems the vtune analyzer enables you to change the view to show hotspots by memory location functions classes or source files you can double click on a hotspot and open the source or assembly view for the hotspot and see more detailed information about the performance of each instruction in the hotspot the vtune analyzer offers focused analysis and performance data at all levels of your source code and can also provide advice at the assembly language level the code coach analyzes and identifies opportunities for better performance of c c fortran and java programs and suggests specific optimizations where appropriate the coach displays pseudo code to suggest the use of highly optimized intrinsics and functions in the intel performance library suite because vtune analyzer is designed specifically for intel architecture ia based processors including the pentium processor it can offer detailed approaches to working with ia see appendix a recommended optimization settings for intel and ia proces sors for details determine if code benefits by conversion to simd execution identifying code that benefits by using simd technologies can be time consuming and difficult likely candidates for conversion are applications that are highly compu tation intensive such as the following speech compression algorithms and filters speech recognition algorithms video display and capture routines rendering routines graphics geometry image and video processing algorithms spatial audio physical modeling graphics cad workstation applications encryption algorithms complex arithmetics generally good candidate code is code that contains small sized repetitive loops that operate on sequential arrays of integers of or bits single precision bit floating point data double precision bit floating point data integer and floating point data items should be sequential in memory the repetitiveness of these loops incurs costly application processing time however these routines have potential for increased performance when you convert them to use one of the simd technologies once you identify your opportunities for using a simd technology you must evaluate what should be done to determine whether the current algorithm or a modified one will ensure the best performance coding techniques the simd features of sse and mmx technology require new methods of coding algorithms one of them is vectorization vectorization is the process of trans forming sequentially executing or scalar code into code that can execute in parallel taking advantage of the simd architecture parallelism this section discusses the coding techniques available for an application to make use of the simd architecture to vectorize your code and thus take advantage of the simd architecture do the following determine if the memory accesses have dependencies that would prevent parallel execution strip mine the inner loop to reduce the iteration count by the length of the simd operations for example four for single precision floating point simd eight for bit integer simd on the xmm registers re code the loop with the simd instructions each of these actions is discussed in detail in the subsequent sections of this chapter these sections also discuss enabling automatic vectorization using the intel c compiler coding methodologies software developers need to compare the performance improvement that can be obtained from assembly code versus the cost of those improvements programming directly in assembly language for a target platform may produce the required perfor mance gain however assembly code is not portable between processor architec tures and is expensive to write and maintain performance objectives can be met by taking advantage of the different simd tech nologies using high level languages as well as assembly the new c c language extensions designed specifically for sse and mmx technology help make this possible figure illustrates the trade offs involved in the performance of hand coded assembly versus the ease of programming and portability figure hand coded assembly and high level compiler performance trade offs the examples that follow illustrate the use of coding adjustments to enable the algo rithm to benefit from the sse the same techniques may be used for single precision floating point double precision floating point and integer data under sse and mmx technology as a basis for the usage model discussed in this section consider a simple loop shown in example example simple four iteration loop void add float a float b float c int i for i i i c i a i b i note that the loop runs for only four iterations this allows a simple replacement of the code with streaming simd extensions for the optimal use of the streaming simd extensions that need data alignment on the byte boundary all examples in this chapter assume that the arrays passed to the routine a b c are aligned to byte boundaries by a calling routine for the methods to ensure this alignment please refer to the application notes for the pentium processor the sections that follow provide details on the coding methodologies inlined assembly intrinsics c vector classes and automatic vectorization assembly key loops can be coded directly in assembly language using an assembler or by using inlined assembly c asm in c c code the intel compiler or assembler recognize the new instructions and registers then directly generate the corresponding code this model offers the opportunity for attaining greatest performance but this perfor mance is not portable across the different processor architectures example shows the streaming simd extensions inlined assembly encoding example streaming simd extensions using inlined assembly encoding void add float a float b float c asm mov eax a mov edx b mov ecx c movaps xmmword ptr eax addps xmmword ptr edx movaps xmmword ptr ecx intrinsics intrinsics provide the access to the isa functionality using c c style coding instead of assembly language intel has defined three sets of intrinsic functions that are implemented in the intel c compiler to support the mmx technology streaming simd extensions and streaming simd extensions four new c data types representing bit and bit objects are used as the operands of these intrinsic functions is used for mmx integer simd is used for single precision floating point simd is used for streaming simd extensions integer simd and is used for double precision floating point simd these types enable the programmer to choose the implementation of an algorithm directly while allowing the compiler to perform register allocation and instruction scheduling where possible the intrinsics are portable among all intel architecture based processors supported by a compiler the use of intrinsics allows you to obtain performance close to the levels achievable with assembly the cost of writing and maintaining programs with intrinsics is consid erably less for a detailed description of the intrinsics and their use refer to the intel c compiler documentation example shows the loop from example using intrinsics example simple four iteration loop coded with intrinsics include xmmintrin h void add float a float b float c a b t1 c the intrinsics map one to one with actual streaming simd extensions assembly code the xmmintrin h header file in which the prototypes for the intrinsics are defined is part of the intel c compiler included with the vtune performance enhancement environment cd intrinsics are also defined for the mmx technology isa these are based on the data type to represent the contents of an mm register you can specify values in bytes short integers bit values or as a bit object the intrinsic data types however are not a basic ansi c data type and therefore you must observe the following usage restrictions use intrinsic data types only on the left hand side of an assignment as a return value or as a parameter you cannot use it with other arithmetic expressions for example use intrinsic data type objects in aggregates such as unions to access the byte elements and structures the address of an object may be also used use intrinsic data type data only with the mmx technology intrinsics described in this guide for complete details of the hardware instructions see the intel architecture mmx technology programmer reference manual for a description of data types see the intel and ia architectures software developer manual classes a set of c classes has been defined and available in intel c compiler to provide both a higher level abstraction and more flexibility for programming with mmx tech nology streaming simd extensions and streaming simd extensions these classes provide an easy to use and flexible interface to the intrinsic functions allowing developers to write more natural c code without worrying about which intrinsic or assembly language instruction to use for a given operation since the intrinsic functions underlie the implementation of these c classes the perfor mance of applications using this methodology can approach that of one using the intrinsics further details on the use of these classes can be found in the intel c class libraries for simd operations user guide order number example shows the c code using a vector class library the example assumes the arrays passed to the routine are already aligned to byte boundaries example c code using the vector classes here fvec h is the class definition file and is the class representing an array of four floats the and operators are overloaded so that the actual streaming simd extensions implementation in the previous example is abstracted out or hidden from the developer note how much more this resembles the original code allowing for simpler and faster programming again the example is assuming the arrays passed to the routine are already aligned to byte boundary automatic vectorization the intel c compiler provides an optimization mechanism by which loops such as in example can be automatically vectorized or converted into streaming simd extensions code the compiler uses similar techniques to those used by a programmer to identify whether a loop is suitable for conversion to simd this involves determining whether the following might prevent vectorization the layout of the loop and the data structures used dependencies amongst the data accesses in each iteration and across iterations once the compiler has made such a determination it can generate vectorized code for the loop allowing the application to use the simd instructions the caveat to this is that only certain types of loops can be automatically vectorized and in most cases user interaction with the compiler is needed to fully enable this example shows the code for automatic vectorization for the simple four itera tion loop from example example automatic vectorization for a simple loop void add float restrict a float restrict b float restrict c int i for i i i c i a i b i compile this code using the qax and qrestrict switches of the intel c compiler version or later the restrict qualifier in the argument list is necessary to let the compiler know that there are no other aliases to the memory to which the pointers point in other words the pointer for which it is used provides the only means of accessing the memory in question in the scope in which the pointers live without the restrict qualifier the compiler will still vectorize this loop using runtime data dependence testing where the generated code dynamically selects between sequential or vector execution of the loop based on overlap of the parameters see documentation for the intel c compiler the restrict keyword avoids the associated overhead altogether see intel c compiler documentation for details stack and data alignment to get the most performance out of code written for simd technologies data should be formatted in memory according to the guidelines described in this section assembly code with an unaligned accesses is a lot slower than an aligned access alignment and contiguity of data access patterns the bit packed data types defined by mmx technology and the bit packed data types for streaming simd extensions and streaming simd extensions create more potential for misaligned data accesses the data access patterns of many algo rithms are inherently misaligned when using mmx technology and streaming simd extensions several techniques for improving data access such as padding orga nizing data elements into arrays etc are described below provides a special purpose instruction lddqu that can avoid cache line splits is discussed in section supplemental techniques for avoiding cache line splits using padding to align data however when accessing simd data using simd operations access to data can be improved simply by a change in the declaration for example consider a declaration of a structure which represents a point in space plus an attribute typedef struct short x y z char a point point pt n assume we will be performing a number of computations on x y z in three of the four elements of a simd word see section data structure layout for an example even if the first element in array pt is aligned the second element will start bytes later and not be aligned shorts at two bytes each plus a single byte bytes by adding the padding variable pad the structure is now bytes and if the first element is aligned to bytes bits all following elements will also be aligned the sample declaration follows typedef struct short x y z char a char pad point point pt n using arrays to make data contiguous in the following code for i i n i pt i y scale the second dimension y needs to be multiplied by a scaling value here the for loop accesses each y dimension in the array pt thus disallowing the access to contiguous data this can degrade the performance of the application by increasing cache misses by poor utilization of each cache line that is fetched and by increasing the chance for accesses which span multiple cache lines the following declaration allows you to vectorize the scaling operation and further improve the alignment of the data access patterns short ptx n pty n ptz n for i i n i pty i scale with the simd technology choice of data organization becomes more important and should be made carefully based on the operations that will be performed on the data in some applications traditional data arrangements may not lead to the maximum performance a simple example of this is an fir filter an fir filter is effectively a vector dot product in the length of the number of coefficient taps consider the following code data j coeff data j coeff data j num of taps coeff num of taps if in the code above the filter operation of data element i is the vector dot product that begins at data element j then the filter operation of data element i begins at data element j assuming you have a bit aligned data vector and a bit aligned coefficients vector the filter operation on the first data element will be fully aligned for the second data element however access to the data vector will be misaligned for an example of how to avoid the misalignment problem in the fir filter refer to intel application notes on streaming simd extensions and filters duplication and padding of data structures can be used to avoid the problem of data accesses in algorithms which are inherently misaligned section data struc ture layout discusses trade offs for organizing data structures note the duplication and padding technique overcomes the misalignment problem thus avoiding the expensive penalty for misaligned data access at the cost of increasing the data size when developing your code you should consider this tradeoff and use the option which gives the best performance stack alignment for bit simd technologies for best performance the streaming simd extensions and streaming simd exten sions require their memory operands to be aligned to byte boundaries unaligned data can cause significant performance penalties compared to aligned data however the existing software conventions for ia stdcall cdecl fast call as implemented in most compilers do not provide any mechanism for ensuring that certain local data and certain parameters are byte aligned there fore intel has defined a new set of ia software conventions for alignment to support the new datatypes and these meet the following conditions functions that use streaming simd extensions or streaming simd extensions data need to provide a byte aligned stack frame parameters need to be aligned to byte boundaries possibly creating holes due to padding in the argument block the new conventions presented in this section as implemented by the intel c compiler can be used as a guideline for an assembly language code as well in many cases this section assumes the use of the data types as defined by the intel c compiler which represents an array of four bit floats for more details on the stack alignment for streaming simd extensions and see appendix d stack alignment data alignment for mmx technology many compilers enable alignment of variables using controls this aligns variable bit lengths to the appropriate boundaries if some of the variables are not appropriately aligned as specified you can align them using the c algorithm in example example c algorithm for bit data alignment make newp a pointer to a bit aligned array of bit elements double p newp p double malloc sizeof double newp p the algorithm in example aligns an array of bit elements on a bit boundary the constant of is derived from one less than the number of bytes in a bit element or aligning data in this manner avoids the significant perfor mance penalties that can occur when an access crosses a cache line boundary another way to improve data alignment is to copy the data into locations that are aligned on bit boundaries when the data is accessed frequently this can provide a significant performance improvement data alignment for bit data data must be byte aligned when loading to and storing from the bit xmm registers used by sse this must be done to avoid severe perfor mance penalties and at worst execution faults there are move instructions and intrinsics that allow unaligned data to be copied to and out of xmm registers when not using aligned data but such operations are much slower than aligned accesses if data is not byte aligned and the programmer or the compiler does not detect this and uses the aligned instructions a fault occurs so keep data byte aligned such alignment also works for mmx technology code even though mmx technology only requires byte alignment the following describes alignment techniques for pentium processor as imple mented with the intel c compiler compiler supported alignment the intel c compiler provides the following methods to ensure that the data is aligned alignment by or data types when the compiler detects or data declarations or parameters it forces alignment of the object to a byte boundary for both global and local data as well as parameters if the declaration is within a function the compiler also aligns the function stack frame to ensure that local data and parameters are byte aligned for details on the stack frame layout that the compiler generates for both debug and optimized release mode compilations refer to intel compiler docu mentation declspec align specifications these can be placed before data declarations to force byte alignment this is useful for local or global data declarations that are assigned to bit data types the syntax for it is declspec align integer constant where the integer constant is an integral power of two but no greater than for example the following increases the alignment to bytes declspec align float buffer the variable buffer could then be used as if it contained objects of type or in the code below the construction of the object x will occur with aligned data void foo x buffer without the declaration of declspec align a fault may occur alignment by using a union structure when feasible a union can be used with bit data types to allow the compiler to align the data structure by default this is preferred to forcing alignment with declspec align because it exposes the true program intent to the compiler in that data is being used for example union float f m buffer now byte alignment is used by default due to the type in the union it is not necessary to use declspec align to force the result in c but not in c it is also possible to force the alignment of a class struct union type as in the code that follows struct declspec align float f if the data in such a class is going to be used with the streaming simd extensions or streaming simd extensions it is preferable to use a union to make this explicit in c an anonymous union can be used to make this more convenient class union m float f because the union is anonymous the names m and f can be used as immediate member names of my note that declspec align has no effect when applied to a class struct or union member in either c or c alignment by using or double data in some cases the compiler aligns routines with or double data to bytes by default the command line switch limits the compiler so that it only performs this alignment on routines that contain bit data the default behavior is to use this switch instructs the complier to align routines with or byte data types to bytes for more see the intel c compiler documentation improving memory utilization memory performance can be improved by rearranging data and algorithms for sse and mmx technology intrinsics methods for improving memory performance involve working with the following data structure layout strip mining for vectorization and memory utilization loop blocking using the cacheability instructions prefetch and streaming store also greatly enhance memory utilization see also chapter optimizing cache usage data structure layout for certain algorithms like transformations and lighting there are two basic ways to arrange vertex data the traditional method is the array of structures aos arrangement with a structure for each vertex example however this method does not take full advantage of simd technology capabilities example aos data structure typedef struct float x y z int a b c vertex vertex vertices numofvertices the best processing method for code using simd technology is to arrange the data in an array for each coordinate example this data arrangement is called struc ture of arrays soa example soa data structure there are two options for computing data in aos format perform operation on the data as it stands in aos format or re arrange it swizzle it into soa format dynami cally see example for code samples of each option based on a dot product computation example aos and soa code samples the dot product of an array of vectors array and a fixed vector fixed is a common operation in lighting operations where array and fixed xf yf zf a dot product is defined as the scalar quantity xf yf zf aos code all values marked dc are don t care example aos and soa code samples contd in the aos model the vertices are stored in the xyz format movaps array dc movaps fixed dc xf yf zf mulps dc xf yf zf movhlps xmm xmm dc dc dc xf addps dc dc dc xf zfmovaps shufps dc dc dc yf addps dc dc dc xf yf zf soa code x y z a xf xf xf xf b yf yf yf yf c zf zf zf zf movaps x movaps y movaps z mulps a xf xf xf xf mulps b yf yf yf xf mulps c zf zf zf zf addps addps xf yf zf performing simd operations on the original aos format can require more calculations and some operations do not take advantage of all simd elements available there fore this option is generally less efficient the recommended way for computing data in aos format is to swizzle each set of elements to soa format before processing it using simd technologies swizzling can either be done dynamically during program execution or statically when the data structures are generated see chapter and chapter for examples performing the swizzle dynamically is usually better than using aos but can be somewhat inefficient because there are extra instructions during computation performing the swizzle statically when data structures are being laid out is best as there is no runtime over head as mentioned earlier the soa arrangement allows more efficient use of the paral lelism of simd technologies because the data is ready for computation in a more optimal vertical manner multiplying components by xf xf xf xf using simd execution slots to produce unique results in contrast computing directly on aos data can lead to horizontal operations that consume simd execution slots but produce only a single scalar result as shown by the many don t care dc slots in example use of the soa format for data structures can lead to more efficient use of caches and bandwidth when the elements of the structure are not accessed with equal frequency such as when element x y z are accessed ten times more often than the other entries then soa saves memory and prevents fetching unnecessary data items a b and c example hybrid soa data structure numofgroups numofvertices simdwidth typedef struct float x simdwidth float y simdwidth float z simdwidth verticescoordlist typedef struct int a simdwidth int b simdwidth int c simdwidth verticescolorlist verticescoordlist verticescoord numofgroups verticescolorlist verticescolor numofgroups note that soa can have the disadvantage of requiring more independent memory stream references a computation that uses arrays x y and z see example would require three separate data streams this can require the use of more prefetches additional address generation calculations as well as having a greater impact on dram page access efficiency there is an alternative a hybrid soa approach blends the two alternatives see example in this case only separate address streams are generated and referenced one contains xxxx yyyy zzzz zzzz and the other aaaa bbbb cccc aaaa dddd the approach prevents fetching unnecessary data assuming the variables x y z are always used together whereas the variables a b c would also be used together but not at the same time as x y z the hybrid soa approach ensures data is organized to enable more efficient vertical simd computation simpler less address generation than aos fewer streams which reduces dram page misses use of fewer prefetches due to fewer streams efficient cache line packing of data elements that are used concurrently with the advent of the simd technologies the choice of data organization becomes more important and should be carefully based on the operations to be performed on the data this will become increasingly important in the pentium processor and future processors in some applications traditional data arrangements may not lead to the maximum performance application developers are encouraged to explore different data arrangements and data segmentation policies for efficient computa tion this may mean using a combination of aos soa and hybrid soa in a given application strip mining strip mining also known as loop sectioning is a loop transformation technique for enabling simd encodings of loops as well as providing a means of improving memory performance first introduced for vectorizers this technique consists of the generation of code when each vector operation is done for a size less than or equal to the maximum vector length on a given vector machine by fragmenting a large loop into smaller segments or strips this technique transforms the loop structure by increasing the temporal and spatial locality in the data cache if the data are reusable in different passes of an algorithm reducing the number of iterations of the loop by a factor of the length of each vector or number of operations being performed per simd operation in the case of streaming simd extensions this vector or strip length is reduced by times four floating point data items per single streaming simd extensions single precision floating point simd operation are processed consider example example pseudo code before strip mining typedef struct float x y z nx ny nz u v main v num for i i num i transform v i example pseudo code before strip mining contd for i i num i lighting v i the main loop consists of two functions transformation and lighting for each object the main loop calls a transformation routine to update some data then calls the lighting routine to further work on the data if the size of array v num is larger than the cache then the coordinates for v i that were cached during transform v i will be evicted from the cache by the time we do lighting v i this means that v i will have to be fetched from main memory a second time reducing performance in example the computation has been strip mined to a size the value is chosen such that elements of array v num fit into the cache hierarchy by doing this a given element v i brought into the cache by transform v i will still be in the cache when we perform lighting v i and thus improve performance over the non strip mined code example strip mined code main v num for i i num i for j i j min num i j transform v j for j i j min num i j lighting v j loop blocking loop blocking is another useful technique for memory performance optimization the main purpose of loop blocking is also to eliminate as many cache misses as possible this technique transforms the memory domain of a given problem into smaller chunks rather than sequentially traversing through the entire memory domain each chunk should be small enough to fit all the data for a given computation into the cache thereby maximizing data reuse in fact one can treat loop blocking as strip mining in two or more dimensions consider the code in example and access pattern in figure the two dimensional array a is referenced in the j column direction and then referenced in the i row direction column major order whereas array b is referenced in the opposite manner row major order assume the memory layout is in column major order therefore the access strides of array a and b for the code in example would be and max respectively example loop blocking a original loop float a max max b max max for i i max i for j j max j a i j a i j b j i b transformed loop after blocking float a max max b max max for i i max i for j j max j for ii i ii i ii for jj j jj j jj a ii jj a ii jj b jj ii for the first iteration of the inner loop each access to array b will generate a cache miss if the size of one row of array a that is a max is large enough by the time the second iteration starts each access to array b will always generate a cache miss for instance on the first iteration the cache line containing b will be brought in when b is referenced because the float type variable is four bytes and each cache line is bytes due to the limitation of cache capacity this line will be evicted due to conflict misses before the inner loop reaches the end for the next iteration of the outer loop another cache miss will be generated while referencing b in this manner a cache miss occurs when each element of array b is refer enced that is there is no data reuse in the cache at all for array b this situation can be avoided if the loop is blocked with respect to the cache size in figure a is selected as the loop blocking factor suppose that is then the blocked chunk of each array will be eight cache lines bytes each in the first iteration of the inner loop a and b will be brought into the cache b will be completely consumed by the first iteration of the outer loop consequently b will only experience one cache miss after applying loop blocking optimization in lieu of eight misses for the original algorithm as illustrated in figure arrays a and b are blocked into smaller rectangular chunks so that the total size of two blocked a and b chunks is smaller than the cache size this allows maximum data reuse figure loop blocking access pattern as one can see all the redundant cache misses can be eliminated by applying this loop blocking technique if max is huge loop blocking can also help reduce the penalty from dtlb data translation look aside buffer misses in addition to improving the cache memory performance this optimization technique also saves external bus bandwidth instruction selection the following section gives some guidelines for choosing instructions to complete a task one barrier to simd computation can be the existence of data dependent branches conditional moves can be used to eliminate data dependent branches conditional moves can be emulated in simd computation by using masked compares and logi cals as shown in example provides packed blend instruction that can vectorize data dependent branches in a loop example emulation of conditional moves high level code declspec align short a b c d e for i i i if a i b i c i d i else c i e i mmx assembly code processes short values per iteration xor eax eax movq a eax b eax create compare mask movq d eax pand drop elements where a b pandn e eax drop elements where a b por crete single word movq c eax add eax cmp eax jle assembly processes short values per iteration xor eax eax movdqq a eax b eax create compare mask movdqa e eax pblendv d eax movdqa c eax add eax cmp eax jle if there are multiple consumers of an instance of a register group the consumers together as closely as possible however the consumers should not be scheduled near the producer simd optimizations and microarchitectures pentium m intel core solo and intel core duo processors have a different microar chitecture than intel netburst microarchitecture the following sub section discusses optimizing simd code targeting intel core solo and intel core duo processors the register register variant of the following instructions has improved performance on intel core solo and intel core duo processor relative to pentium m processors this is because the instructions consist of two micro ops instead of three relevant instructions are unpcklps unpckhps packsswb packuswb packssdw pshufd shuffps and shuffpd recommendation when targeting code generation for intel core solo and intel core duo processors favor instructions consisting of two ops over those with more than two ops intel core microarchitecture generally executes simd instructions more efficiently than previous microarchitectures in terms of latency and throughput most bit simd operations have cycle throughput except shuffle pack unpack operations many of the restrictions specific to intel core duo intel core solo processors such as bit simd operations having cycle throughput at a minimum do not apply to intel core microarchitecture the same is true of intel core microarchitecture rela tive to intel netburst microarchitectures enhanced intel core microarchitecture provides dedicated bit shuffler and radix divider hardware these capabilities and instructions will make vectoriza tion using bit simd instructions even more efficient and effective recommendation with the proliferation of bit simd hardware in intel core microarchitecture and enhanced intel core microarchitecture integer simd code written using mmx instructions should consider more efficient implementations using bit simd instructions tuning the final application the best way to tune your application once it is functioning correctly is to use a profiler that measures the application while it is running on a system vtune analyzer can help you determine where to make changes in your application to improve performance using the vtune analyzer can help you with various phases required for optimized performance see appendix a intel vtune performance analyzer for details after every effort to optimize you should check the performance gains to see where you are making your major optimization gains chapter optimizing for simd integer applications simd integer instructions provide performance improvements in applications that are integer intensive and can take advantage of simd architecture guidelines in this chapter for using simd integer instructions in addition to those described in chapter may be used to develop fast and efficient code that scales across processor generations the collection of bit and bit simd integer instructions supported by mmx technology sse and pcmpeqq in are referred to as simd integer instructions code sequences in this chapter demonstrates the use of basic bit simd integer instructions and more efficient bit simd integer instructions processors based on intel core microarchitecture support mmx sse and processors based on enhanced intel core microarchitecture support and all previous generations of simd integer instructions processors based on intel microarchitecture nehalem supports mmx sse and single instruction multiple data techniques can be applied to text string processing lexing and parser applications simd programming in string text processing and lexing applications often require sophisticated techniques beyond those commonly used in simd integer programming this is covered in chapter and simd programming for text processing lexing parsing execution of bit simd integer instructions in intel core microarchitecture and enhanced intel core microarchitecture are substantially more efficient than on previous microarchitectures thus newer simd capabilities introduced in operate on bit operands and do not introduce equivalent bit simd capabili ties conversion from bit simd integer code to bit simd integer code is highly recommended this chapter contains examples that will help you to get started with coding your application the goal is to provide simple low level operations that are frequently used the examples use a minimum number of instructions necessary to achieve best performance on the current generation of intel and ia processors each example includes a short description sample code and notes if necessary these examples do not address scheduling as it is assumed the examples will be incorporated in longer code sequences for planning considerations of using the simd integer instructions refer to section general rules on simd integer code general rules and suggestions are do not intermix bit simd integer instructions with floating point instruc tions see section using simd integer with floating point note that all simd integer instructions can be intermixed without penalty favor bit simd integer code over bit simd integer code on microarchi tectures prior to intel core microarchitecture most bit simd instructions have two cycle throughput restrictions due to the underlying bit data path in the execution engine intel core microarchitecture executes most simd instruc tions except shuffle pack unpack operations with one cycle throughput and provides three ports to execute multiple simd instructions in parallel enhanced intel core microarchitecture speeds up bit shuffle pack unpack operations with cycle throughput when writing simd code that works for both integer and floating point data use the subset of simd convert instructions or load store instructions to ensure that the input operands in xmm registers contain data types that are properly defined to match the instruction code sequences containing cross typed usage produce the same result across different implementations but incur a significant performance penalty using sse instructions to operate on type mismatched simd data in the xmm register is strongly discouraged use the optimization rules and guidelines described in chapter and chapter take advantage of hardware prefetcher where possible use the prefetch instruction only when data access patterns are irregular and prefetch distance can be pre determined see chapter optimizing cache usage emulate conditional moves by using blend masked compares and logicals instead of using conditional branches using simd integer with floating point all bit simd integer instructions use mmx registers which share register state with the floating point stack because of this sharing certain rules and consider ations apply instructions using mmx registers cannot be freely intermixed with floating point registers take care when switching between bit simd integer instructions and floating point instructions to ensure functional correctness see section both section and section apply only to software that employs mmx instructions as noted before bit simd integer instructions should be favored to replace mmx code and achieve higher performance that also obviates the need to use emms and the performance penalty of using emms when intermixing mmx and instructions for performance considerations there is no penalty of intermixing simd floating point operations and bit simd integer operations and floating point opera tions using the emms instruction when generating bit simd integer code keep in mind that the eight mmx regis ters are aliased to floating point registers switching from mmx instructions to floating point instructions incurs a finite delay so it is the best to minimize switching between these instruction types but when switching the emms instruction provides an efficient means to clear the stack so that subsequent code can operate properly as soon as an instruction makes reference to an mmx register all valid bits in the floating point tag word are set which implies that all registers contain valid values in order for software to operate correctly the floating point stack should be emptied when starting a series of floating point calculations after operating on the mmx registers using emms clears all valid bits effectively emptying the floating point stack and making it ready for new floating point operations the emms instruction ensures a clean transition between using operations on the mmx registers and using opera tions on the floating point stack on the pentium processor there is a finite overhead for using the emms instruction failure to use the emms instruction or the intrinsic between opera tions on the mmx registers and floating point registers may lead to unexpected results note failure to reset the tag word for fp instructions after using an mmx instruction can result in faulty execution or poor performance guidelines for using emms instruction when developing code with both floating point and bit simd integer instruc tions follow these steps always call the emms instruction at the end of bit simd integer code when the code transitions to floating point code insert the emms instruction at the end of all bit simd integer code segments to avoid an floating point stack overflow exception when an floating point instruction is executed when writing an application that uses both floating point and bit simd integer instructions use the following guidelines to help you determine when to use emms if next instruction is fp use after a bit simd integer instruction if the next instruction is an fp instruction for example before doing calculations on floats doubles or long doubles don t empty when already empty if the next instruction uses an mmx register incurs a cost with no benefit group instructions try to partition regions that use fp instructions from those that use bit simd integer instructions this eliminates the need for an emms instruction within the body of a critical loop runtime initialization use during runtime initialization of and fp data types this ensures resetting the register between data type transitions see example for coding usage example resetting register between and fp data types code incorrect usage correct usage x y z x y z float f init float f init you must be aware that your code generates an mmx instruction which uses mmx registers with the intel c compiler in the following situations when using a bit simd integer intrinsic from mmx technology sse when using a bit simd integer instruction from mmx technology sse through inline assembly when referencing the data type variable additional information on the floating point programming model can be found in the intel and ia architectures software developer manual volume for more on emms visit http developer intel com data alignment make sure that bit simd integer data is byte aligned and that bit simd integer data is byte aligned referencing unaligned bit simd integer data can incur a performance penalty due to accesses that span cache lines referencing unaligned bit simd integer data results in an exception unless the movdqu move double quadword unaligned instruction is used using the movdqu instruc tion on unaligned data can result in lower performance than using byte aligned references refer to section stack and data alignment for more information loading bytes of simd data efficiently requires data alignment on byte bound aries provides the palignr instruction it reduces overhead in situations that requires software to processing data elements from non aligned address the palignr instruction is most valuable when loading or storing unaligned data with the address shifts by a few bytes you can replace a set of unaligned loads with aligned loads followed by using palignr instructions and simple register to register copies using palignrs to replace unaligned loads improves performance by eliminating cache line splits and other penalties in routines like memcpy palignr can boost the performance of misaligned cases example shows a situation that benefits by using palignr example fir processing example in c language code void fir float in float out float coeff int count int i j for i i count tap i float sum for j j tap j sum in j coeff j out sum in example compares an optimal sequence of the fir loop and an equivalent implementation both implementations unroll iteration of the fir inner loop to enable simd coding techniques the code can not avoid experiencing cache line split once every four iterations palgnr allows the code to avoid the delays associated with cache line splits example and implementation of fir processing code optimized for optimized for pxor xor ecx ecx mov eax dword ptr input mov ebx dword ptr pxor xor ecx ecx mov eax dword ptr input mov ebx dword ptr movaps xmmword ptr eax ecx mulps xmmword ptr ebx ecx addps movaps xmmword ptr eax ecx movaps mulps xmmword ptr ebx ecx addps movups xmmword ptr eax ecx mulps xmmword ptr ebx ecx addps movaps xmmword ptr eax ecx movaps palignr mulps xmmword ptr ebx ecx addps example and implementation of fir processing code contd optimized for optimized for movups xmmword ptr eax ecx movaps mulps xmmword palignr ptr ebx ecx addps mulps xmmword ptr ebx ecx addps movups xmmword ptr eax ecx mulps xmmword ptr ebx ecx addps movaps palignr mulps xmmword ptr ebx ecx addps add ecx cmp ecx tap jl add ecx cmp ecx tap jl mov eax dword ptr output movaps xmmword ptr eax mov eax dword ptr output movaps xmmword ptr eax data movement coding techniques in general better performance can be achieved if data is pre arranged for simd computation see section improving memory utilization this may not always be possible this section covers techniques for gathering and arranging data for more efficient simd computation unsigned unpack mmx technology provides several instructions that are used to pack and unpack data in the mmx registers extends these instructions so that they operate on bit source and destinations the unpack instructions can be used to zero extend an unsigned number example assumes the source is a packed word bit data type example zero extend bit values into bits using unsigned unpack instruc tions code input bit values in source a local variable can be used instead of the register if desired output four zero extended bit doublewords from four low end words four zero extended bit doublewords from four high end words movdqa copy source punpcklwd unpack the low end words into bit doubleword punpckhwd unpack the high end words into bit doublewords signed unpack signed numbers should be sign extended when unpacking values this is similar to the zero extend shown above except that the psrad instruction packed shift right arithmetic is used to sign extend the values example assumes the source is a packed word bit data type example signed unpack code input source value output four sign extended bit doublewords from four low end words four sign extended bit doublewords from four high end words example signed unpack code contd movdqa copy source punpcklwd unpack four low end words of the source into the upper bits of each doubleword in the destination punpckhwd unpack high end words of the source into the upper bits of each doubleword in the destination psrad sign extend the low end words of the source into four bit signed doublewords psrad sign extend the high end words of the source into four bit signed doublewords interleaved pack with saturation pack instructions pack two values into a destination register in a predetermined order packssdw saturates two signed doublewords from a source operand and two signed doublewords from a destination operand into four signed words and it packs the four signed words into a destination register see figure extends packssdw so that it saturates four signed doublewords from a source operand and four signed doublewords from a destination operand into eight signed words the eight signed words are packed into the destination figure packssdw mm mm instruction figure illustrates where two pairs of values are interleaved in a destination register example shows mmx code that accomplishes the operation two signed doublewords are used as source operands and the result is interleaved signed words the sequence in example can be extended in to interleave eight signed words using xmm registers d figure interleaved pack with saturation example interleaved pack with saturation code input signed value signed value output the first and third words contain the signed saturated doublewords from the second and fourth words contain signed saturated doublewords from packssdw pack and sign saturate packssdw pack and sign saturate punpcklwd interleave the low end bit values of the operands pack instructions always assume that source operands are signed numbers the result in the destination register is always defined by the pack instruction that performs the operation for example packssdw packs each of two signed bit values of two sources into four saturated bit signed values in a destination register packuswb on the other hand packs the four signed bit values of two sources into eight saturated eight bit unsigned values in the destination interleaved pack without saturation example is similar to example except that the resulting words are not satu rated in addition in order to protect against overflow only the low order bits of each doubleword are used again example can be extended in to accom plish interleaving eight words without saturation example interleaved pack without saturation code input signed source value signed source value output the first and third words contain the low bits of the doublewords in the second and fourth words contain the low bits of the doublewords in pslld shift the lsb from each of the doubleword values to the msb position pand ffff ffff mask to zero the msb of each doubleword value por merge the two operands non interleaved unpack unpack instructions perform an interleave merge of the data elements of the desti nation and source operands into the destination register the following example merges the two operands into destination registers without interleaving for example take two adjacent elements of a packed word data type in and place this value in the low bits of the results then take two adja cent elements of a packed word data type in and place this value in the high bits of the results one of the destination registers will have the combination illustrated in figure figure result of non interleaved unpack low in the other destination register will contain the opposite combination illustrated in figure figure result of non interleaved unpack high in code in the example unpacks two packed word sources in a non interleaved way the goal is to use the instruction which unpacks doublewords to a quadword instead of using the instruction which unpacks words to doublewords example unpacking two packed word sources in non interleaved way code input packed word source value packed word source value output contains the two low end words of the original sources non interleaved contains the two high end words of the original sources non interleaved movq copy punpckldq replace the two high end words of mmo with two low end words of leave the two low end words of in place punpckhdq move two high end words of to the two low end words of place the two high end words of in two high end words of extract data element the pextrw instruction in sse takes the word in the designated mmx register selected by the two least significant bits of the immediate value and moves it to the lower half of a bit integer register see figure and example with pextrw can extract a word from an xmm register to the lower bits of an integer register provides extraction of a byte word dword and qword from an xmm register into either a memory location or integer register figure pextrw instruction example pextrw instruction code input eax source value immediate value output edx bit integer register containing the extracted word in the low order bits the high order bits zero extended movq eax pextrw edx insert data element the pinsrw instruction in sse loads a word from the lower half of a bit integer register or from memory and inserts it in an mmx technology destination register at a position defined by the two least significant bits of the immediate constant inser tion is done in such a way that three other words from the destination register are left untouched see figure and example with pinsrw can insert a word from the lower bits of an integer register or memory into an xmm register provides insertion of a byte dword and qword from either a memory location or integer register into an xmm register figure pinsrw instruction example pinsrw instruction code input edx pointer to source value output register with new bit value inserted mov eax edx pinsrw eax if all of the operands in a register are being replaced by a series of pinsrw instruc tions it can be useful to clear the content and break the dependence chain by either using the pxor instruction or loading the register see example and section clearing registers and dependency breaking idioms example repeated pinsrw instruction code input edx pointer to structure containing source values at offsets of and immediate value output mmx register with new bit value inserted pxor breaks dependency on previous value of mov eax edx pinsrw eax mov eax edx pinsrw eax mov eax edx pinsrw eax mov eax edx pinsrw eax non unit stride data movement provides instructions to insert a data element from memory into an xmm register and to extract a data element from an xmm register into memory directly separate instructions are provided to handle floating point data and integer byte word or dword these instructions are suited for vectorizing code that loads stores non unit stride data from memory see example example non unit stride load store using instructions example provides two examples using insertps and pextrd to perform gather operations on floating point data using extractps and pextrd to perform scatter operations on floating point data example scatter and gather operations using instructions move byte mask to integer the pmovmskb instruction returns a bit mask formed from the most significant bits of each byte of its source operand when used with bit mmx registers this produces an bit mask zeroing out the upper bits in the destination register when used with bit xmm registers it produces a bit mask zeroing out the upper bits in the destination register the bit version of this instruction is shown in figure and example figure pmovsmkb instruction example pmovmskb instruction code input source value output bit register containing the byte mask in the lower eight bits movq edi pmovmskb eax packed shuffle word for bit registers the pshufw instruction uses the immediate operand to select between the four words in either two mmx registers or one mmx register and a bit memory location provides pshuflw to shuffle the lower four words into an xmm register in addition to the equivalent to the pshufw also provides pshufhw to shuffle the higher four words furthermore offers pshufd to shuffle four dwords into an xmm register all of these four pshuf instructions use an immediate byte to encode the data path of individual words within the corresponding bytes from source to destination shown in table table pshuf encoding bits words packed shuffle word for bit registers the pshuflw pshufhw instruction performs a full shuffle of any source word field within the low high bits to any result word field in the low high bits using an bit immediate operand other high low bits are passed through from the source operand pshufd performs a full shuffle of any double word field within the bit source to any double word field in the bit result using an bit immediate operand no more than instructions using pshuflw pshufhw pshufd are required to implement many common data shuffling operations broadcast swap and reverse are illustrated in example and example example broadcast a word across xmm using instructions example swap reverse words in an xmm using instructions shuffle bytes provides pshufb this instruction carries out byte manipulation within a byte range pshufb can replace up to other instructions including shift or and and mov use pshufb if the alternative uses or more instructions conditional data movement provides two packed blend instructions on byte and word data elements in bit operands packed blend instructions conditionally copies data elements from selected positions in the source to the corresponding data element using a mask specified by an immediate control byte or an implied xmm register the mask can be generated by a packed compare instruction for example thus packed blend instructions are most useful for vectorizing conditional flows within a loop and can be more efficient than inserting single element one at a time for some situations unpacking interleaving bit data in bit registers the punpcklqdq punpchqdq instructions interleave the low high order bits of the source operand and the low high order bits of the destination operand it then writes the results to the destination register the high low order bits of the source operands are ignored data movement there are two additional instructions to enable data movement from bit simd integer registers to bit simd registers the instruction moves the bit integer data from an mmx register source to a bit destination register the high order bits of the destination register are zeroed out the instruction moves the low order bits of integer data from a bit source register to an mmx register destination conversion instructions sse provides instructions to support wide conversion of single precision data to from double word integer data conversions between double precision data to double word integer data have been added in provides rounding instructions to convert floating point values to integer values with rounding control specified in a more flexible manner and independent of the rounding control in mxcsr the integer values produced by roundxx instruc tions are maintained as floating point data also provides instructions to convert integer data from packed bytes to packed word dword qword format using either sign extension or zero extension packed words to packed dword qword format using either sign extension or zero extension packed dword to packed qword format using either sign extension or zero extension generating constants simd integer instruction sets do not have instructions that will load immediate constants to the simd registers the following code segments generate frequently used constants in the simd register these examples can also be extended in by substituting mmx with xmm registers see example example generating constants example generating constants contd note because simd integer instruction sets do not support shift instruc tions for bytes and are relevant only for packed words and packed doublewords building blocks this section describes instructions and algorithms which implement common code building blocks absolute difference of unsigned numbers example computes the absolute difference of two unsigned numbers it assumes an unsigned packed byte data type here we make use of the subtract instruction with unsigned saturation this instruc tion receives unsigned operands and subtracts them with unsigned saturation this support exists only for packed bytes and packed words not for packed double words example absolute difference of two unsigned numbers input source operand source operand output absolute difference of the unsigned operands movq make a copy of compute difference one way compute difference the other way por or them together this example will not work if the operands are signed note that psadbw may also be used in some situations see section for details absolute difference of signed numbers example computes the absolute difference of two signed numbers using instruction pabsw this sequence is more efficient than using previous generation of simd instruction extensions example absolute difference of signed numbers input signed source operand signed source operand output difference of the unsigned operands psubw subtract words pabsw results in absolute value example show an mmx code sequence to compute x where x is signed this example assumes signed words to be the operands with this sequence of three instructions can be replaced by the pabsw instruction additionally provides a bit version using xmm registers and supports byte word and doubleword granularity example computing absolute value input signed source operand output abs mmo pxor set to all zeros psubw make each word contain the negative of each word will contain only the positive larger values the absolute value note the absolute value of the most negative number that is for bit cannot be represented using positive numbers this algorithm will return the original value for the absolute value pixel format conversion provides the pshufb instruction to carry out byte manipulation within a byte range pshufb can replace a set of up to other instruction including shift or and and mov use pshufb if the alternative code uses or more instructions example 21 shows the basic form of conversion of color pixel formats example 21 basic c implementation of rgba to bgra conversion standard c code struct rgba byte r g b a struct bgra byte b g r a void bgra source rgba dest int for int i i i dest i r source i r dest i g source i g dest i b source i b dest i a source i a example and example show code and code for pixel format conversion in the example pshufb replaces six instructions example 22 color pixel format conversion using optimized for mov esi src mov edi dest mov ecx iterations movdqa ff ff ff ff ff ff ff ff movdqa ff ff ff ff ff ff ff ff mov eax remainder pixels byte per iteration movdqa esi r1g1b1a1 movdqa movdqa abgr psrld pslld por grab pand pand por argb movdqa edi repeats for another bytes add esi add edi sub ecx jnz example 23 color pixel format conversion using optimized for mov esi src mov edi dest mov ecx iterations movdqa mov eax remainder pixels byte per iteration movdqa esi r1g1b1a1 movdqa esi pshufb b1g1r1a1 movdqa edi repeats for another bytes add esi add edi sub ecx jnz endian conversion the pshufb instruction can also be used to reverse byte ordering within a double word it is more efficient than traditional techniques such as bswap example a shows the traditional technique using four bswap instructions to reverse the bytes within a dword each bswap requires executing two ops in addition the code requires loads and stores for processing dwords of data example b shows an implementation of endian conversion using pshufb the reversing of four dwords requires one load one store and pshufb on intel core microarchitecture reversing dwords using pshufb can be approx imately twice as fast as using bswap example big endian to little endian conversion a using bswap lea eax src lea ecx dst mov edx elcount start mov edi eax mov esi eax bswap edi mov ebx eax bswap esi mov ebp eax mov ecx edi mov ecx esi bswap ebx mov ecx ebx bswap ebp mov ecx ebp add eax add ecx sub edx jnz start b using pshufb declspec align byte bswapmask 11 lea eax src lea ecx dst mov edx elcount movaps bswapmask start movdqa eax pshufb movdqa ecx add eax add ecx sub edx jnz start clipping to an arbitrary range high low this section explains how to clip a values to a range high low specifically if the value is less than low or greater than high then clip to low or high respectively this technique uses the packed add and packed subtract instructions with saturation signed or unsigned which means that this technique can only be used on packed byte and packed word data types the examples in this section use the constants and and show operations on word values for simplicity we use the following constants corresponding constants are used in case the operation is done on byte values equals equals contains the value low in all four words of the packed words data type contains the value high in all four words of the packed words data type all values equal high_us adds the high value to all data elements words of adds the low value to all data elements words of highly efficient clipping for clipping signed words to an arbitrary range the pmaxsw and pminsw instruc tions may be used for clipping unsigned bytes to an arbitrary range the pmaxub and pminub instructions may be used example shows how to clip signed words to an arbitrary range the code for clipping unsigned bytes is similar example clipping to a signed range of words high low input signed source operands output signed words clipped to the signed range high low pminsw with example can be easily extended to clip signed bytes unsigned words signed and unsigned dwords example clipping to an arbitrary signed range high low input signed source operands output signed operands clipped to the unsigned range high low paddw add with no saturation to convert to unsigned in effect this clips to high low_us in effect this clips to low paddw undo the previous two offsets the code above converts values to unsigned numbers first and then clips them to an unsigned range the last instruction converts the data back to signed data and places the data within the signed range conversion to unsigned data is required for correct results when high low if high low simplify the algorithm as in example example simplified clipping to an arbitrary signed range input signed source operands output signed operands clipped to the unsigned range high low paddssw in effect this clips to high psubssw packed_high clips to low paddw low undo the previous two offsets this algorithm saves a cycle when it is known that high low the three instruction algorithm does not work when high low because minus any number will yield a number greater in magnitude than which is a negative number when the second instruction psubssw high low in the three step algorithm example is executed a negative number is subtracted the result of this subtraction causes the values in to be increased instead of decreased as should be the case and an incorrect answer is generated clipping to an arbitrary unsigned range high low example clips an unsigned value to the unsigned range high low if the value is less than low or greater than high then clip to low or high respectively this tech nique uses the packed add and packed subtract instructions with unsigned satura tion thus the technique can only be used on packed bytes and packed words data types figure illustrates operation on word values example clipping to an arbitrary unsigned range high low input unsigned source operands output unsigned operands clipped to the unsigned range high low paddusw high in effect this clips to high psubusw 0xffff high low in effect this clips to low paddw low undo the previous two offsets packed max min of byte word and dword the pmaxsw instruction returns the maximum between four signed words in either of two simd registers or one simd register and a memory location the pminsw instruction returns the minimum between the four signed words in either of two simd registers or one simd register and a memory location the pmaxub instruction returns the maximum between the eight unsigned bytes in either of two simd registers or one simd register and a memory location the pminub instruction returns the minimum between the eight unsigned bytes in either of two simd registers or one simd register and a memory location extended pmaxsw pmaxub pminsw pminub to bit operations adds bit operations for signed bytes unsigned word signed and unsigned dword packed multiply integers the pmulhuw pmulhw instruction multiplies the unsigned signed words in the destination operand with the unsigned signed words in the source operand the high order bits of the bit intermediate results are written to the destination operand the pmullw instruction multiplies the signed words in the destination operand with the signed words in the source operand the low order bits of the bit intermediate results are written to the destination operand extended pmulhuw pmulhw pmullw to bit operations and adds pmuludq the pmuludq instruction performs an unsigned multiply on the lower pair of double word operands within bit chunks from the two sources the full bit result from each multiplication is returned to the destination register this instruction is added in both a bit and bit version the latter performs independent operations on the low and high halves of a bit register adds bit operations of pmuldq and pmulld the pmulld instruction multiplies the signed dwords in the destination operand with the signed dwords in the source operand the low order bits of the bit intermediate results are written to the destination operand the pmuldq instruction multiplies the two low order signed dwords in the destination operand with the two low order signed dwords in the source operand and stores two bit results in the destination operand packed sum of absolute differences the psadbw instruction computes the absolute value of the difference of unsigned bytes for either two simd registers or one simd register and a memory location the differences of pairs of unsigned bytes are then summed to produce a word result in the lower bit field and the upper three words are set to zero with psadbw is extended to compute two word results the subtraction operation presented above is an absolute difference that is t abs x y byte values are stored in temporary space all values are summed together and the result is written to the lower word of the destination register motion estimation involves searching reference frames for best matches sum abso lute difference sad on two blocks of pixels is a common ingredient in video processing algorithms to locate matching blocks of pixels psadbw can be used as building blocks for finding best matches by way of calculating sad results on blocks of pixels 10 mpsadbw and phminposuw the mpsadbw instruction in performs eight sad operations each sad oper ation produces a word result from pairs of unsigned bytes with sad result in an xmm register phminposum can help search for the best match between eight pixel blocks for motion estimation algorithms mpsadbw is likely to improve over psadbw in several ways simplified data movement to construct packed data format for sad computation on pixel blocks higher throughput in terms of sad results per iteration less iteration required per frame mpsadbw results are amenable to efficient search using phminposuw examples of mpsadbw vs psadbw for and block search can be found in the white paper listed in the reference section of chapter 11 packed average byte word the pavgb and pavgw instructions add the unsigned data elements of the source operand to the unsigned data elements of the destination register along with a carry in the results of the addition are then independently shifted to the right by one bit position the high order bits of each element are filled with the carry bits of the corre sponding sum the destination operand is an simd register the source operand can either be an simd register or a memory operand the pavgb instruction operates on packed unsigned bytes and the pavgw instruc tion operates on packed unsigned words complex multiply by a constant complex multiplication is an operation which requires four multiplications and two additions this is exactly how the pmaddwd instruction operates in order to use this instruction you need to format the data into multiple bit values the real and imaginary components should be bits each consider example which assumes that the bit mmx registers are being used let the input data be dr and di where dr is real component of the data and di is imaginary component of the data format the constant complex coefficients in memory as four bit values cr ci ci cr remember to load the values into the mmx register using movq the real component of the complex product is pr dr cr di ci and the imaginary component of the complex product is pi dr ci di cr the output is a packed doubleword if needed a pack instruction can be used to convert the result to bit thereby matching the format of the input example complex multiply by a constant input complex value dr di constant complex coefficient in the form cr ci ci cr output two bit dwords containing pr pi punpckldq makes dr di dr di pmaddwd done the result is dr cr di ci dr ci di cr packed bit add subtract the paddq psubq instructions add subtract quad word operands within each bit chunk from the two sources the bit result from each computation is written to the destination register like the integer add sub instruction paddq psubq can operate on either unsigned or signed two complement notation integer operands when an individual result is too large to be represented in bits the lower bits of the result are written to the destination operand and therefore the result wraps around these instructions are added in both a bit and bit version the latter performs independent operations on the low and high halves of a bit register 14 bit shifts the pslldq psrldq instructions shift the first operand to the left right by the number of bytes specified by the immediate operand the empty low high order bytes are cleared set to zero if the value specified by the immediate operand is greater than then the destina tion is set to all zeros ptest and conditional branch offers ptest instruction that can be used in vectorizing loops with conditional branches ptest is an bit version of the general purpose instruction test the zf or cf field of the eflags register are modified as a result of ptest example a depicts a loop that requires a conditional branch to handle the special case of divide by zero in order to vectorize such loop any iteration that may encounter divide by zero must be treated outside the vectorizable iterations example using ptest to separate vectorizable and non vectorizable loop iterations a loops requiring infrequent exception handling float a cnt unsigned int i b ptest enables early out to handle infrequent non vectorizable portion xor eax eax movaps xorps for i i cnt i if a i lp movaps a eax cmpeqps convert each non zero to a i a i else call divexception ones ptest jnc carry will be set if all were non zero movaps divps movaps a eax add eax cmp eax cnt jnz lp jmp end execute one by one call exception when value is zero example b shows an assembly sequence that uses ptest to cause an early out branch whenever any one of the four floating point values in is zero the fall through path enables the rest of the floating point calculations to be vectorized because none of the four values are zero vectorization of heterogeneous computations across loop iterations vectorization techniques on un rolled loops generally rely on repetitive homoge neous operations between each loop iteration using ptest and variable blend instructions vectorization of heterogeneous operations across loop iterations may be possible example a depicts a simple heterogeneous loop the heterogeneous operation and conditional branch makes simple loop unrolling technique infeasible for vector ization example using ptest and variable blend to vectorize heterogeneous loops a loops with heterogeneous operation across iterations float a cnt unsigned int i for i i cnt i if a i b i a i b i else a i b i b vectorize condition flow with ptest blendvps xor eax eax lp movaps a eax movaps b eax movaps compare a and b values cmpgtps will hold b movaps xorps select values for the add operation true condition produce a b false will become a b blend mask is blendvps addps movaps a eax add eax cmp eax cnt jnz lp example b depicts an assembly sequence that uses blendvps and ptest to vectorize the handling of heterogeneous computations occurring across four consec utive loop iterations vectorization of control flows in nested loops the ptest and blendvpx instructions can be used as building blocks to vectorize more complex control flow statements where each control flow statement is creating a working mask used as a predicate of which the conditional code under the mask will operate the mandelbrot set map evaluation is useful to illustrate a situation with more complex control flows in nested loops the mandelbrot set is a set of height values mapped to a d grid the height value is the number of mandelbrot iterations defined over the complex number space as in in i0 needed to get in it is common to limit the map generation by setting some maximum threshold value of the height all other points are assigned with a height equal to the threshold example shows an example of mandelbrot map evaluation implemented in c example baseline c code for mandelbrot set map evaluation define dimx define dimy define dimx define dimy int map dimx dimy void int i j float x y for i x i dimx i x for j y j dimy j y float sx sy int iter sx x sy y while iter if sx sx sy sy 0f break float sx sx x sx sx sy sy sy y sy iter map i j iter example 33 shows a vectorized implementation of mandelbrot map evaluation vectorization is not done on the inner most loop because the presence of the break statement implies the iteration count will vary from one pixel to the next the vector ized version take into account the parallel nature of d vectorize over four itera tions of y values of consecutive pixels and conditionally handles three scenarios in the inner most iteration when all pixels do not reach break condition vectorize pixels when one or more pixels reached break condition use blend intrinsics to accumulate the complex height vector for the remaining pixels not reaching the break condition and continue the inner iteration of the complex height vector when all four pixels reached break condition exit the inner loop example 33 vectorized mandelbrot set map evaluation using intrinsics declspec align float y_step 0f 0f void int i j x y for i x 8f i dimx i x for j dimy y j dimy j y f32vec4 sx sy iter int sx x sy y while int mask f32vec4 sx vmask sx sx sy sy if all data points in our vector are hitting the exit condition the vectorized loop can exit if vmask break continue example 33 vectorized mandelbrot set map evaluation using intrinsics if non of the data points are out we don t need the extra code which blends the results if vmask vmask sx x sx sx sy sy sy y old_sx sy iter else blended flavour of the code this code blends values from previous iteration with the values from current iteration only values which did not hit the exit condition are being stored values which are already out are maintaining their value sx x sx sx sy sy sx vmask sy y old_sx sy sy vmask iter iter iter vmask map i j iter memory optimizations you can improve memory access using the following techniques avoiding partial memory accesses increasing the bandwidth of memory fills and video fills prefetching data with streaming simd extensions see chapter optimizing cache usage mmx registers and xmm registers allow you to move large quantities of data without stalling the processor instead of loading single array values that are or bits long consider loading the values in a single quadword or double quadword and then incrementing the structure or array pointer accordingly any data that will be manipulated by simd integer instructions should be loaded using either an simd integer instruction that loads a bit or bit operand for example movq the register memory form of any simd integer instruction that operates on a quadword or double quadword memory operand for example pmaddw all simd data should be stored using an simd integer instruction that stores a bit or bit operand for example movq mm0 the goal of the above recommendations is twofold first the loading and storing of simd data is more efficient using the larger block sizes second following the above recommendations helps to avoid mixing of or bit load and store opera tions with simd integer technology load and store operations to the same simd data this prevents situations in which small loads follow large stores to the same area of memory or large loads follow small stores to the same area of memory the pentium ii pentium iii and pentium processors may stall in such situations see chapter for details partial memory accesses consider a case with a large load after a series of small stores to the same area of memory beginning at memory address mem the large load stalls in the case shown in example 34 example 34 a large load after a series of small stores penalty movq must wait for the stores to write memory before it can access all data it requires this stall can also occur with other data types for example when bytes or words are stored and then words or doublewords are read from the same area of memory when you change the code sequence as shown in example 35 the processor can access the data without delay example 35 accessing data without delay movd ebx build data into a qword first before storing it to memory movd eax psllq por mm2 movq mem store simd variable to mem as a qword movq mem load qword simd mem no stall consider a case with a series of small loads after a large store to the same area of memory beginning at memory address mem as shown in example 36 most of the small loads stall because they are not aligned with the store see section store forwarding for details example 36 a series of small loads after a large store movq mem store qword to address mem mov bx mem load word at mem stalls mov cx mem load word at mem stalls the word loads must wait for the quadword store to write to memory before they can access the data they require this stall can also occur with other data types for example when doublewords or words are stored and then words or bytes are read from the same area of memory when you change the code sequence as shown in example 37 the processor can access the data without delay example 37 eliminating delay for a series of small loads after a large store movq mem mm0 store qword to address mem movq mem load qword at address mem movd eax transfer mem to eax from mmx register not memory example 37 eliminating delay for a series of small loads after a large store psrlq shr eax movd ebx mm1 transfer mem to bx from mmx register not memory and ebx these transformations in general increase the number of instructions required to perform the desired operation for pentium ii pentium iii and pentium processors the benefit of avoiding forwarding problems outweighs the performance penalty due to the increased number of instructions supplemental techniques for avoiding cache line splits video processing applications sometimes cannot avoid loading data from memory addresses that are not aligned to byte boundaries an example of this situation is when each line in a video frame is averaged by shifting horizontally half a pixel example shows a common operation in video processing that loads data from memory address not aligned to a byte boundary as video processing traverses each line in the video frame it experiences a cache line split for each byte chunk loaded from memory example 38 an example of video processing with cache line splits average half pels horizontally on the x axis from one reference frame only nextlinesloop movdqu xmmword ptr edx may not be aligned movdqu xmmword ptr edx movdqu xmmword ptr edx eax movdqu xmmword ptr edx eax pavgbxmm0 movdqaxmmword ptr ecx movdqaxmmword ptr ecx eax repeat provides an instruction lddqu for loading from memory address that are not byte aligned lddqu is a special bit unaligned load designed to avoid cache line splits if the address of the load is aligned on a byte boundary ldqqu loads the bytes requested if the address of the load is not aligned on a byte boundary lddqu loads a byte block starting at the byte aligned address immediately below the address of the load request it then provides the requested bytes if the address is aligned on a byte boundary the effective number of memory requests is implementation dependent one or more lddqu is designed for programming usage of loading data from memory without storing modified data back to the same address thus the usage of lddqu should be restricted to situations where no store to load forwarding is expected for situations where store to load forwarding is expected use regular store load pairs either aligned or unaligned based on the alignment of the data accessed example 39 video processing using lddqu to avoid cache line splits average half pels horizontally on the x axis from one reference frame only nextlinesloop lddqu xmmword ptr edx may not be aligned lddqu xmmword ptr edx lddqu xmmword ptr edx eax lddqu xmmword ptr edx eax pavgbxmm0 movdqaxmmword ptr ecx results stored elsewhere movdqaxmmword ptr ecx eax repeat increasing bandwidth of memory fills and video fills it is beneficial to understand how memory is accessed and filled a memory to memory fill for example a memory to video fill is defined as a byte cache line load from memory which is immediately stored back to memory such as a video frame buffer the following are guidelines for obtaining higher bandwidth and shorter latencies for sequential memory fills video fills these recommendations are relevant for all intel architecture processors with mmx technology and refer to cases in which the loads and stores do not hit in the first or second level cache increasing memory bandwidth using the movdq instruction loading any size data operand will cause an entire cache line to be loaded into the cache hierarchy thus any size load looks more or less the same from a memory bandwidth perspective however using many smaller loads consumes more microar chitectural resources than fewer larger stores consuming too many resources can cause the processor to stall and reduce the bandwidth that the processor can request of the memory subsystem using movdq to store the data back to uc memory or wc memory in some cases instead of using bit stores for example movd will reduce by three quarters the number of stores per memory fill cycle as a result using the movdq in memory fill cycles can achieve significantly higher effective bandwidth than using movd increasing memory bandwidth by loading and storing to and from the same dram page dram is divided into pages which are not the same as operating system os pages the size of a dram page is a function of the total size of the dram and the organiza tion of the dram page sizes of several kilobytes are common like os pages dram pages are constructed of sequential addresses sequential memory accesses to the same dram page have shorter latencies than sequential accesses to different dram pages in many systems the latency for a page miss that is an access to a different page instead of the page previously accessed can be twice as large as the latency of a memory page hit access to the same page as the previous access therefore if the loads and stores of the memory fill cycle are to the same dram page a significant increase in the bandwidth of the memory fill cycles can be achieved increasing uc and wc store bandwidth by using aligned stores using aligned stores to fill uc or wc memory will yield higher bandwidth than using unaligned stores if a uc store or some wc stores cross a cache line boundary a single store will result in two transaction on the bus reducing the efficiency of the bus transactions by aligning the stores to the size of the stores you eliminate the possibility of crossing a cache line boundary and the stores will not be split into sepa rate transactions reverse memory copy copying blocks of memory from a source location to a destination location in reverse order presents a challenge for software to make the most out of the machines capa bilities while avoiding microarchitectural hazards the basic un optimized c code is shown in example 40 the simple c code in example 40 is sub optimal because it loads and stores one byte at a time even in situations that hardware prefetcher might have brought data in from system memory to cache example 40 un optimized reverse memory copy in c unsigned char src unsigned char dst while len dst src len using movdqa or movdqu software can load and store up to bytes at a time but must either ensure byte alignment requirement if using movdqa or minimize the delays movdqu may encounter if data span across cache line boundary figure data alignment of loads and stores in reverse memory copy given the general problem of arbitrary byte count to copy arbitrary offsets of leading source byte and destination bytes address alignment relative to byte and cache line boundaries these alignment situations can be a bit complicated figure a and b depict the alignment situations of reverse memory copy of n bytes the general guidelines for dealing with unaligned loads and stores are in order of importance avoid stores that span cache line boundaries minimize the number of loads that span cacheline boundaries favor byte aligned loads and stores over unaligned versions in figure a the guidelines above can be applied to the reverse memory copy problem as follows peel off several leading destination bytes until it aligns on byte boundary then the ensuing destination bytes can be written to using movaps until the remaining byte count falls below bytes after the leading source bytes have been peeled corresponding to step above the source alignment in figure a allows loading bytes at a time using movaps until the remaining byte count falls below bytes switching the byte ordering of each bytes of data can be accomplished by a byte mask with pshufb the pertinent code sequence is shown in example 41 example 41 using pshufb to reverse byte ordering bytes at a time declspec align static const unsigned char bswapmask 14 13 11 10 mov esi src mov edi dst mov ecx len movaps bswapmask start movdqa esi pshufb movdqa edi sub edi add esi sub ecx cmp ecx jae start handle left overs in figure b we also start with peeling the destination bytes peel off several leading destination bytes until it aligns on byte boundary then the ensuing destination bytes can be written to using movaps until the remaining byte count falls below bytes however the remaining source bytes are not aligned on byte boundaries replacing movdqa with movdqu for loads will inevitably run into cache line splits to achieve higher data throughput than loading unaligned bytes with movdqu the bytes of data targeted to each of bytes of aligned destination addresses can be assembled using two aligned loads this technique is illustrated in figure 9 figure 9 a technique to avoid cacheline split loads in reverse memory copy using two aligned loads converting from bit to bit simd integers defines a superset of bit integer instructions currently available in mmx technology the operation of the extended instructions remains the superset simply operates on data that is twice as wide this simplifies porting of bit integer appli cations however there are few considerations computation instructions which use a memory operand that may not be aligned to a byte boundary must be replaced with an unaligned bit load movdqu followed by the same computation operation that uses instead register operands use of bit integer computation instructions with memory operands that are not byte aligned will result in a gp unaligned bit loads and stores are not as efficient as corresponding aligned versions this fact can reduce the performance gains when using the bit simd integer extensions general guidelines on the alignment of memory operands are the greatest performance gains can be achieved when all memory streams are byte aligned reasonable performance gains are possible if roughly half of all memory streams are byte aligned and the other half are not little or no performance gain may result if all memory streams are not aligned to bytes in this case use of the bit simd integer instructions may be preferable loop counters need to be updated because each bit integer instruction operates on twice the amount of data as its bit integer counterpart extension of the pshufw instruction shuffle word across bit integer operand across a full bit operand is emulated by a combination of the following instructions pshufhw pshuflw and pshufd use of the bit shift by bit instructions psrlq psllq are extended to bits by use of psrlq and psllq along with masking logic operations a code sequence rewritten to use the psrldq and pslldq instructions shift double quad word operand by bytes simd optimizations and microarchitectures pentium m intel core solo and intel core duo processors have a different microar chitecture than intel netburst microarchitecture the following sections discuss opti mizing simd code that targets intel core solo and intel core duo processors on intel core solo and intel core duo processors lddqu behaves identically to movdqu by loading bytes of data irrespective of address alignment packed integer versus mmx instructions in general bit simd integer instructions should be favored over bit mmx instructions on intel core solo and intel core duo processors this is because improved decoder bandwidth and more efficient op flows relative to the pentium m processor wider width of the xmm registers can benefit code that is limited by either decoder bandwidth or execution latency xmm registers can provide twice the space to store data for in flight execution wider xmm registers can facilitate loop unrolling or in reducing loop overhead by halving the number of loop iterations in microarchitectures prior to intel core microarchitecture execution throughput of bit simd integration operations is basically the same as bit mmx operations some shuffle unpack shift operations do not benefit from the front end improve ments the net impact of using bit simd integer instruction on intel core solo and intel core duo processors is likely to be slightly positive overall but there may be a few situations where their use will generate an unfavorable performance impact intel core microarchitecture generally executes bit simd instructions more effi ciently than previous microarchitectures in terms of latency and throughput many of the limitations specific to intel core duo intel core solo processors do not apply the same is true of intel core microarchitecture relative to intel netburst microarchitec tures enhanced intel core microarchitecture provides even more powerful bit simd execution capabilities and more comprehensive sets of simd instruction extensions than intel core microarchitecture the integer simd instructions offered by operates on bit xmm register only all of these highly encourages software to favor bit vectorizable code to take advantage of processors based on enhanced intel core microarchitecture and intel core microarchitecture work around for false dependency issue in processor based on intel microarchitecture nehalem using pmovsx and pmovzx instructions to combine data type conversion and data movement in the same instruction will create a false dependency due to hardware causes a simple work around to avoid the false dependency issue is to use pmovsx pmovzx instruc tion solely for data type conversion and issue separate instruction to move data to destination or from origin example 42 pmovsx pmovzx work around to avoid false dependency 9 tuning partially vectorizable code some loop structured code are more difficult to vectorize than others example 43 depicts a loop carrying out table look up operation and some arithmetic computation example 43 table look up operations in c code integer input arrays pout integer output array count size of array lookuptable integer values size of the look up table for unsigned i i count i pout i lookuptable i i although some of the arithmetic computations and input output to data array in each iteration can be easily vectorizable but the table look up via an index array is not this creates different approaches to tuning a compiler can take a scalar approach to execute each iteration sequentially hand tuning of such loops may use a couple of different techniques to handle the non vectorizable table look up operation one vectorization technique is to load the input data for four iteration at once then use instruction to shift out individual index out of an xmm register to carry out table look up sequentially the shift technique is depicted by example 44 another tech nique is to use pextrd in to extract the index from an xmm directly and then carry out table look up sequentially the pextrd technique is depicted by example 45 example 44 shift techniques on non vectorizable table look up int modulo int c mov esi mov ebx pout mov ecx count mov edx plookuptableptr movaps modulo movaps c lloop vectorizable multiple consecutive data accesses movaps esi read indices from movaps pand tablesize table look up is not vectorizable shift out one data element to look up table one by one movd eax get first index movd word ptr edx eax psrldq movd eax get index movd word ptr edx eax psrldq movd eax get movd word ptr edx eax psrldq movd eax get fourth index movd word ptr edx eax end of scalar part packing movlhps psllq movlhps orps end of packing continue example 44 shift techniques on non vectorizable table look up contd vectorizable computation operations paddd paddd por andps mod movaps ebx end of vectorizable operation add ebx add esi add edi sub ecx test ecx ecx jne lloop example 45 pextrd techniques on non vectorizable table look up int modulo 256 256 256 int c 17 17 17 17 mov esi mov ebx pout mov ecx count mov edx plookuptableptr movaps modulo movaps c lloop vectorizable multiple consecutive data accesses movaps esi read indices from movaps pand tablesize table look up is not vectorizable extract one data element to look up table one by one movd eax get first index mov eax edx eax movd eax continue example 45 pextrd techniques on non vectorizable table look up contd pextrd eax extract index mov eax edx eax pinsrd eax pextrd eax extract index mov eax edx eax pinsrd eax pextrd eax extract index mov eax edx eax pinsrd eax end of scalar part packing not needed vectorizable operations paddd paddd 17 por andps mod movaps ebx add ebx add esi add edi sub ecx test ecx ecx jne lloop the effectiveness of these two hand tuning techniques on partially vectorizable code depends on the relative cost of transforming data layout format using various forms of pack and unpack instructions the shift technique requires additional instructions to pack scalar table values into an xmm to transition into vectorized arithmetic computations the net performance gain or loss of this technique will vary with the characteristics of different microarchitec tures the alternate pextrd technique uses less instruction to extract each index does not require extraneous packing of scalar data into packed simd data format to begin vectorized arithmetic computation chapter optimizing for simd floating point applications this chapter discusses rules for optimizing for the single instruction multiple data simd floating point instructions available in sse and the chapter also provides examples that illustrate the optimization techniques for single precision and double precision simd floating point applications general rules for simd floating point code the rules and suggestions in this section help optimize floating point code containing simd floating point instructions generally it is important to understand and balance port utilization to create efficient simd floating point code basic rules and sugges tions include the following follow all guidelines in chapter and chapter mask exceptions to achieve higher performance when exceptions are unmasked software performance is slower utilize the flush to zero and denormals are zero modes for higher performance to avoid the penalty of dealing with denormals and underflows use the reciprocal instructions followed by iteration for increased accuracy these instructions yield reduced accuracy but execute much faster note the following if reduced accuracy is acceptable use them with no iteration if near full accuracy is needed use a newton raphson iteration if full accuracy is needed then use divide and square root which provide more accuracy but slow down performance planning considerations whether adapting an existing application or creating a new one using simd floating point instructions to achieve optimum performance gain requires programmers to consider several issues in general when choosing candidates for optimization look for code segments that are computationally intensive and floating point intensive also consider efficient use of the cache architecture the sections that follow answer the questions that should be raised before imple mentation can data layout be arranged to increase parallelism or cache utilization which part of the code benefits from simd floating point instructions is the current algorithm the most appropriate for simd floating point instruc tions is the code floating point intensive do either single precision floating point or double precision floating point computations provide enough range and precision does the result of computation affected by enabling flush to zero or denormals to zero modes is the data arranged for efficient utilization of the simd floating point registers is this application targeted for processors without simd floating point instruc tions see also section considerations for code conversion to simd programming using simd floating point with floating point because the xmm registers used for simd floating point computations are separate registers and are not mapped to the existing floating point stack simd floating point code can be mixed with floating point or bit simd integer code with intel core microarchitecture bit simd integer instructions provides substantially higher efficiency than bit simd integer instructions software should favor using simd floating point and integer simd instructions with xmm registers where possible scalar floating point code there are simd floating point instructions that operate only on the lowest order element in the simd register these instructions are known as scalar instructions they allow the xmm registers to be used for general purpose floating point computa tions in terms of performance scalar floating point code can be equivalent to or exceed floating point code and has the following advantages simd floating point code uses a flat register model whereas floating point code uses a stack model using scalar floating point code eliminates the need to use fxch instructions these have performance limits on the intel pentium processor mixing with mmx technology code without penalty flush to zero mode shorter latencies than floating point when using scalar floating point instructions it is not necessary to ensure that the data appears in vector form however the optimizations regarding alignment sched uling instruction selection and other optimizations covered in chapter and chapter should be observed data alignment simd floating point data is byte aligned referencing unaligned bit simd floating point data will result in an exception unless movups or movupd move unaligned packed single or unaligned packed double is used the unaligned instruc tions used on aligned or unaligned data will also suffer a performance penalty relative to aligned accesses see also section stack and data alignment data arrangement because sse and incorporate simd architecture arranging data to fully use the simd registers produces optimum performance this implies contiguous data for processing which leads to fewer cache misses correct data arrangement can poten tially quadruple data throughput when using sse or double throughput when using performance gains can occur because four data elements can be loaded with bit load instructions into xmm registers using sse movaps similarly two data elements can loaded with bit load instructions into xmm registers using movapd refer to the section stack and data alignment for data arrangement recom mendations duplicating and padding techniques overcome misalignment problems that occur in some data structures and arrangements this increases the data space but avoids penalties for misaligned data access for some applications for example geometry traditional data arrangement requires some changes to fully utilize the simd registers and parallel techniques traditionally the data layout has been an array of structures aos to fully utilize the simd registers in such applications a new data layout has been proposed a struc ture of arrays soa resulting in more optimized performance vertical versus horizontal computation the majority of the floating point arithmetic instructions in sse provide greater performance gain on vertical data processing for parallel data elements this means each element of the destination is the result of an arithmetic operation performed from the source elements in the same vertical position figure to supplement these homogeneous arithmetic operations on parallel data elements sse and provides data movement instructions e g shufps unpcklps unpckhps movlhps movhlps etc that facilitate moving data elements horizontally op op x op figure homogeneous operation on parallel data elements the organization of structured data have a significant impact on simd programming efficiency and performance this can be illustrated using two common type of data structure organizations array of structure this refers to the arrangement of an array of data structures within the data structure each member is a scalar this is shown in figure typically a repetitive sequence of computation is applied to each element of an array i e a data structure computational sequence for the scalar members of the structure is likely to be non homogeneous within each iteration aos is generally associated with a horizontal computation model x y z w figure horizontal computation model structure of array here each member of the data structure is an array each element of the array is a scalar this is shown table repetitive computa tional sequence is applied to scalar elements and homogeneous operation can be easily achieved across consecutive iterations within the same structural member consequently soa is generally amenable to the vertical computation model table soa form of representing vertices data vx array xn vy array yn vz array y4 zn vw array w4 wn using simd instructions with vertical computation on soa arrangement can achieve higher efficiency and performance than aos and horizontal computation this can be seen with dot product operation on vectors the dot product operation on soa arrangement is shown in figure x1 x2 x3 x4 fx fx fx fx y2 y4 fy fy fy fy z4 fz fz fz fz w4 fw fw fw fw r4 figure dot product operation example shows how one result would be computed for seven instructions if the data were organized as aos and using sse alone four results would require instructions example pseudocode for horizontal xyz aos computation mulps x x y y z z movaps reg reg move since next steps overwrite shufps get b a d c from a b c d addps get a b a b c d c d movaps reg reg move shufps get c d c d a b a b from prior addps addps get a b c d a b c d a b c d a b c d now consider the case when the data is organized as soa example demon strates how four results are computed for five instructions example pseudocode for vertical xxxx yyyy zzzz soa computation mulps x x for all x components of vertices mulps y y for all y components of vertices mulps z z for all z components of vertices addps x x y y addps x x y y z z for the most efficient use of the four component wide registers reorganizing the data into the soa format yields increased throughput and hence much better perfor mance for the instructions used as seen from this simple example vertical computation can yield use of the available simd registers to produce four results the results may vary for other situ ations if the data structures are represented in a format that is not friendly to vertical computation it can be rearranged on the fly to facilitate better utilization of the simd registers this operation is referred to as swizzling operation and the reverse operation is referred to as deswizzling data swizzling swizzling data from soa to aos format can apply to a number of application domains including geometry video and imaging two different swizzling techniques can be adapted to handle floating point and integer data example illustrates a swizzle function that uses shufps movlhps movhlps instructions example swizzling data using shufps movlhps movhlps example shows a similar data swizzling algorithm using simd instructions in the integer domain example swizzling data using unpckxxx instructions the technique in example loading bytes using shufps and copying halves of xmm registers is preferable over an alternate approach of loading halves of each vector using movlps movhps on newer microarchitectures this is because loading bytes using movlps movhps can create code dependency and reduce the throughput of the execution engine the performance considerations of example and example often depends on the characteristics of each microarchitecture for example in intel core microarchi tecture executing a shufps tend to be slower than a punpckxxx instruction in enhanced intel core microarchitecture shufps and punpckxxx instruction all executes with cycle throughput due to the bit shuffle execution unit then the next important consideration is that there is only one port that can execute punpckxxx vs movlhps movhlps can execute on multiple ports the performance of both techniques improves on intel core microarchitecture over previous microar chitectures due to ports for executing simd instructions both techniques improves further on enhanced intel core microarchitecture due to the bit shuffle unit data deswizzling in the deswizzle operation we want to arrange the soa format back into aos format so the xxxx yyyy zzzz are rearranged and stored in memory as xyz example illustrates one deswizzle function for floating point data example deswizzling single precision simd data void in out asm mov ecx in load structure addresses mov edx out movaps ecx movaps ecx movaps ecx movaps ecx movaps movaps unpcklps unpcklps movdqa movlhps movhlps unpckhps unpckhps z3 movdqa movlhps movhlps z3 x3 movaps edx z0 y0 movaps edx y1 movaps edx movaps edx z3 x3 example shows a similar deswizzle function using simd integer instructions both of these techniques demonstrate loading bytes and performing horizontal data movement in registers this approach is likely to be more efficient than alterna tive techniques of storing 8 byte halves of xmm registers using movlps and movhps example deswizzling data using simd integer instructions mov mov movdqa ecx in edx out ecx load structure addresses load r1 movdqa ecx load movdqa ecx load movdqa ecx load start deswizzli movdqa ng here movdqa punpckldq r1 punpckldq movdqa punpcklqdq r1 punpckhqdq punpckhdq punpckhdq movdqa punpcklqdq g3 r3 punpckhqdq r4 movdqa edx movdqa edx movdqa edx movdqa edx deswizzling ends here horizontal add using sse although vertical computations generally make use of simd performance better than horizontal computations in some cases code must use a horizontal operation movlhps movhlps and shuffle can be used to sum data horizontally for example starting with four bit registers to sum up each register horizontally while having the final results in one register use the movlhps movhlps to align the upper and lower parts of each register this allows you to use a vertical add with the resulting partial horizontal summation full summation follows easily figure presents a horizontal add using movhlps movlhps example and example 8 provide the code for this operation a figure horizontal add using movhlps movlhps example horizontal add using movhlps movlhps void in float out asm mov ecx in load structure addresses mov edx out movaps ecx load movaps ecx load movaps ecx load movaps ecx load start horizontal add movaps movlhps movhlps addps movaps movlhps movhlps addps movaps shufps b1 shufps addps d c b a end horizontal add movaps edx example 8 horizontal add using intrinsics with movhlps movlhps example 8 horizontal add using intrinsics with movhlps movlhps contd in y in z in w _mm_movehl_ps tmm1 b1 tmm0 b1 tmm1 a1 a2 b1 b3 tmm2 tmm2 tmm2 _mm_movehl_ps _mm_add_ps tmm2 c3 d1 d3 c1 c3 c2 d1 d3 tmm3 0xdd a1 a3 b1 b3 c1 c3 d1 d3 tmm5 a2 c2 c4 d2 d4 _mm_add_ps tmm5 a1 a2 a3 a4 b1 b2 b3 b4 c1 c2 c3 c4 d1 d2 d3 d4 out use of instructions the and instructions encode the truncate chop rounding mode implicitly in the instruction they take precedence over the rounding mode specified in the mxcsr register this behavior can eliminate the need to change the rounding mode from round nearest to truncate chop and then back to round nearest to resume computation avoid frequent changes to the mxcsr register since there is a penalty associated with writing this register typically when using rounding control in mxcsr can always be set to round nearest flush to zero and denormals are zero modes the flush to zero ftz and denormals are zero daz modes are not compatible with the ieee standard they are provided to improve performance for applica tions where underflow is common and where the generation of a denormalized result is not necessary see also section 8 floating point modes and exceptions simd optimizations and microarchitectures pentium m intel core solo and intel core duo processors have a different microar chitecture than intel netburst microarchitecture intel core microarchitecture offers significantly more efficient simd floating point capability than previous microarchi tectures in addition instruction latency and throughput of instructions are significantly improved in intel core microarchitecture over previous microarchitec tures simd floating point programming using enhances sse and with nine instructions targeted for simd floating point programming in contrast to many sse instructions offering homogeneous arithmetic operations on parallel data elements and favoring the vertical computation model offers instructions that performs asymmetric arithmetic operation and arithmetic operation on horizontal data elements addsubps and addsubpd are two instructions with asymmetric arithmetic processing capability see figure haddps haddpd hsubps and hsubpd offers horizontal arithmetic processing capability see figure in addition movsldup movshdup and movddup load data from memory or xmm register and replicate data elements at once figure asymmetric arithmetic operation of the instruction figure horizontal arithmetic operation of the instruction haddpd and complex arithmetics the flexibility of in dealing with aos type of data structure can be demon strated by the example of multiplication and division of complex numbers for example a complex number can be stored in a structure consisting of its real and imaginary part this naturally leads to the use of an array of structure example 9 demonstrates using instructions to perform multiplications of single precision complex numbers example 10 demonstrates using instructions to perform division of complex numbers example 9 multiplication of two pair of single precision complex number multiplication of ak i bk ck i dk a i b can be stored as a data structure movsldup load real parts into the destination movaps load the pair of complex values i e mulps temporary results shufps reorder the real and imaginary parts movshdup load the imaginary parts into the destination example 9 multiplication of two pair of single precision complex number mulps temporary results addsubps example 10 division of two pair of single precision complex numbers division of ak i bk ck i dk movshdup load imaginary parts into the destination b0 movaps load the pair of complex values i e mulps temporary results shufps reorder the real and imaginary parts c1 d1 c0 d0 movsldup load the real parts into the destination a0 mulps temp results addsubps b0d0 a0d0 mulps movps shufps addps divps shufps b1c1 a1d1 c1c1 d1d1 a1c1 b1d1 c1c1 d1d1 b0c0 a0d0 c0c0 a0c0 b0d0 c0c0 d0d0 in both examples the complex numbers are store in arrays of structures movsldup movshdup and the asymmetric addsubps allow performing complex arithmetics on two pair of single precision complex number simultaneously and without any unnecessary swizzling between data elements due to microarchitectural differences software should implement multiplication of complex double precision numbers using instructions on processors based on intel core microarchitecture in intel core duo and intel core solo processors soft ware should use scalar instructions to implement double precision complex multiplication this is because the data path between simd execution units is bits in intel core microarchitecture and only bits in previous microarchitectures processors based on the enhanced intel core microarchitecture generally executes instruction more efficiently than previous microarchitectures they also have a bit shuffle unit that will benefit complex arithmetic operations further than intel core microarchitecture did example 11 shows two equivalent implementations of double precision complex multiply of two pair of complex numbers using vector versus instructions example 12 shows the equivalent scalar implementation example 11 double precision complex multiplication of two pairs vector implementation vector implementation movapd eax y x movapd eax y x movapd eax w z movapd eax z z unpcklpd z z movapd movapd eax w z unpcklpd unpckhpd w w unpckhpd mulpd z y z x mulpd z y z x mulpd w y w x mulpd w y w x xorpd w y w x shufpd w x w y shufpd w x w y addsubpd w x z y z x w y addpd z y w x z x w y movapd ecx movapd ecx example 12 double precision complex multiplication using scalar example 12 double precision complex multiplication using scalar contd mulsd w y subsd z x w y addsd z y w x movsd ecx movsd ecx 8 packed floating point performance in intel core duo processor most packed simd floating point code will speed up on intel core solo processors relative to pentium m processors this is due to improvement in decoding packed simd instructions the improvement of packed floating point performance on the intel core solo processor over pentium m processor depends on several factors generally code that is decoder bound and or has a mixture of integer and packed floating point instruc tions can expect significant gain code that is limited by execution latency and has a cycles per instructions ratio greater than one will not benefit from decoder improvement when targeting complex arithmetics on intel core solo and intel core duo proces sors using single precision instructions can deliver higher performance than alternatives on the other hand tasks requiring double precision complex arith metics may perform better using scalar instructions on intel core solo and intel core duo processors this is because scalar instructions can be dispatched through two ports and executed using two separate floating point units packed horizontal instructions haddps and hsubps can simplify the code sequence for some tasks however these instruction consist of more than five micro ops on intel core solo and intel core duo processors care must be taken to ensure the latency and decoding penalty of the horizontal instruction does not offset any algorithmic benefits dot product and horizontal simd instructions sometimes the aos type of data organization are more natural in many algebraic formula one common example is the dot product operation dot product operation can be implemented using sse instruction sets added a few horizontal add subtract instructions for applications that rely on the horizontal computation model provides additional enhancement with instructions that are capable of directly evaluating dot product operations of vectors of or components example 13 dot product of vector length using sse using sse to compute one dot product movaps eax mulps eax movhlps x x upper half not needed addps x x pshufd x x x addss movss ecx example 14 dot product of vector length using using to compute one dot product movaps eax mulps eax haddps b4 b1 b4 b2 b1 movaps b4 b3 b2 b1 b4 b3 b2 b1 psrlq b4 b3 b4 a3 b3 addss a1 b1 a3 b3 b2 b4 movss eax example dot product of vector length using using to compute one dot product movaps eax dpps eax a1 b1 a3 b3 a2 b2 a4 b4 movss eax example 13 example 14 and example 15 compare the basic code sequence to compute one dot product result for a pair of vectors the selection of an optimal sequence in conjunction with an application memory access patterns may favor different approaches for example if each dot product result is immediately consumed by additional computational sequences it may be more optimal to compare the relative speed of these different approaches if dot products can be computed for an array of vectors and kept in the cache for subse quent computations then more optimal choice may depend on the relative throughput of the sequence of instructions in intel core microarchitecture example 14 has higher throughput than example 13 due to the relatively longer latency of haddps the speed of example 14 is slightly slower than example 13 in enhanced intel core microarchitecture example 15 is faster in both speed and throughput than example 13 and example 14 although the latency of dpps is also relatively long it is compensated by the reduction of number of instructions in example 15 to do the same amount of work unrolling can further improve the throughput of each of three dot product implemen tations example shows two unrolled versions using the basic and sequences the version can also be unrolled and using insertps to pack dot product results example unrolled implementation of four dot products implementation implementation movaps eax movaps eax mulps eax mulps eax w1 z0 z1 y0 y1 x0 movaps eax movaps eax mulps eax mulps eax movaps eax w3 z2 z3 y3 x3 mulps eax movaps eax movaps eax mulps eax mulps eax y5 haddps movaps eax haddps mulps eax 96 haddps z7 movaps ecx example unrolled implementation of four dot products contd implementation implementation movaps unpcklps y3 y0 y1 x2 x3 x0 x1 unpckhps w3 w0 w1 z2 z3 z0 z1 movaps unpcklps y5 x7 x5 unpckhps w7 w5 z7 addps addps movaps movhlps movlhps addps movaps ecx vector normalization normalizing vectors is a common operation in many floating point applications example 17 shows an example in c of normalizing an array of x y z vectors example 17 normalization of an array of vectors for i i cnt i float size nodes i vec dot if size size 0f sqrtf size else size nodes i vec x size nodes i vec y size nodes i vec z size example 18 shows an assembly sequence that normalizes the x y z components of a vector example 18 normalize x y z components of an array of vectors using p nodes i vec asm mov eax p xorps movups eax loads the x y z of input vector plus x of next vector movaps save a copy of data from memory to restore the unnormalized value movaps _mask mask to select x y z values from an xmm register to normalize andps mask elements movaps save a copy of x y z to compute normalized vector later mulps z z y y x x pshufd x x y y z z addps x x z z y y z z y y x x pshufd z z y y x x x x z z y y addps x x y y z z x x y y z z x x y y z z x x y y z z comisd xmm1 xmm2 compare size to jz zero movaps preloaded unitary vector sqrtps xmm1 xmm1 divps xmm1 jmp store zero movaps xmm2 store mulps normalize the vector in the lower elements andnps mask off the lower elements to keep the un normalized value orps order the un normalized component after the normalized vector movaps eax writes normalized x y z followed by unmodified value example 19 shows an assembly sequence using to normalizes the x y z components of a vector example 19 normalize x y z components of an array of vectors using p nodes i vec asm mov eax p xorps xmm2 xmm2 movups xmm1 eax loads the x y z of input vector plus x of next vector movaps xmm1 save a copy of data from memory dpps xmm1 xmm1 x x y y z z x x y y z z x x y y z z x x y y z z comisd xmm1 xmm2 compare size to jz zero movaps preloaded unitary vector 0 0 1 0 1 0 sqrtps xmm1 xmm1 divps xmm1 jmp store zero movaps xmm2 store mulps normalize the vector in the lower elements blendps copy the un normalized component next to the normalized vector movaps eax in example 18 and example 19 the throughput of these instruction sequences are basically limited by the long latency instructions of divps and sqrtps in example 19 the use of dpps replaces eight instructions to evaluate and broadcast the dot product result to four elements of an xmm register this could result in improvement of the relative speed of example 19 over example 18 using horizontal simd instruction sets and data layout sse and provide packed add subtract multiply divide instructions that are ideal for situations that can take advantage of vertical computation model such as soa data layout and 1 added horizontal simd instructions including horizontal add subtract dot product operations these more recent simd extensions provide tools to solve problems involving data layouts or operations that do not conform to the vertical simd computation model in this section we consider a vector matrix multiplication problem and discuss the relevant factors for choosing various horizontal simd instructions example 20 shows the vector matrix data layout in aos where the input and out vectors are stored as an array of structure example 20 data organization in memory for aos vector matrix multiplication matrix pmat input vertices pvert ouput vertices poutvert o1w o3w example 21 shows an example using haddps and mulps to perform vector matrix multiplication with data layout in aos after three haddps completing the summations of each output vector component the output components are arranged in aos example 21 aos vector matrix multiplication with haddps mov eax pmat mov ebx pvert mov ecx poutvert xor edx edx movaps eax load row movaps eax load row movaps xmm7 eax load row lloop movaps ebx edx load input vector movaps mulps eax vw vz vy vx movaps xmm1 mulps xmm1 vw vz vy vx example 21 aos vector matrix multiplication with haddps movaps xmm2 mulps xmm2 xmm6 vw vz vy vx movaps mulps xmm7 vw vz vy vx haddps xmm1 haddps xmm2 haddps xmm2 movaps ecx edx store a vector of length add edx cmp edx top jb lloop example 22 shows an example using dpps to perform vector matrix multiplication in aos example 22 aos vector matrix multiplication with dpps mov eax pmat mov ebx pvert mov ecx poutvert xor edx edx movaps eax load row movaps xmm6 eax load row movaps xmm7 eax load row lloop movaps ebx edx load input vector movaps xmm0 dpps xmm0 eax calculate dot product of length store to lowest dword movaps xmm1 dpps xmm1 movaps xmm2 dpps xmm2 xmm6 movaps xmm4 dpps xmm7 movss ecx edx 0 xmm0 store one element of vector length movss ecx edx 1 xmm1 movss ecx edx xmm2 movss ecx edx xmm3 add edx cmp edx top jb lloop example 21 and example 22 both work with aos data layout using different horizontal processing techniques provided by and 1 the effectiveness of either techniques will vary depending on the degree of exposures of long latency instruction in the inner loop the overhead efficiency of data movement and the latency of haddps vs dpps on processors that support both haddps and dpps the choice between either tech nique may depend on application specific considerations if the output vectors are written back to memory directly in a batch situation example 21 may be prefer able over example 22 because the latency of dpps is long and storing each output vector component individually is less than ideal for storing an array of vectors there may be partially vectorizable situations that the individual output vector component is consumed immediately by other non vectorizable computations then using dpps producing individual component may be more suitable than dispersing the packed output vector produced by three haddps as in example 21 1 soa and vector matrix multiplication if the native data layout of a problem conforms to soa then vector matrix multiply can be coded using mulps addps without using the longer latency horizontal arith metic instructions or packing scalar components into packed format example 22 to achieve higher throughput with soa data layout there are either pre requisite data preparation or swizzling deswizzling on the fly that must be comprehended for example an soa data layout for vector matrix multiplication is shown in example 23 each matrix element is replicated four times to minimize data movement overhead for producing packed results example 23 data organization in memory for soa vector matrix multiplication matrix pmat m00 m01 m02 m03 m03 m10 m11 m12 m13 m13 m20 m21 m22 m23 m23 m30 m31 m32 m33 m33 input vertices pvert ouput vertices poutvert o3x o1y o3y o3z o1w o3w the corresponding vector matrix multiply example in soa unrolled for four iteration of vectors is shown in example example 6 24 vector matrix multiplication with native soa data layout mov ebx pvert mov ecx poutvert xor edx edx movaps xmm5 eax load row movaps xmm6 eax load row movaps xmm7 eax load row mov eax pmat xor edi edi movaps xmm0 ebx load v2x v0x movaps xmm1 ebx load v2y v1y v0y movaps xmm2 ebx load v2z v0z movaps xmm3 ebx load v2w v1w v0w movaps xmm4 eax mulps xmm4 xmm0 v2x v1x v0x movaps xmm4 eax 16 mulps xmm5 xmm1 v3y v2y v1y v0y addps xmm4 xmm5 movaps xmm5 eax mulps xmm5 xmm2 v2z v1z v0z addps xmm4 xmm5 movaps xmm5 eax mulps xmm5 xmm3 v3w v2w v1w v0w addps xmm4 xmm5 movaps ecx edx xmm4 add eax add edx 16 add edi 1 cmp edi jb add ebx cmp edx top jb chapter optimizing cache usage over the past decade processor speed has increased memory access speed has increased at a slower pace the resulting disparity has made it important to tune applications in one of two ways either a a majority of data accesses are fulfilled from processor caches or b effectively masking memory latency to utilize peak memory bandwidth as much as possible hardware prefetching mechanisms are enhancements in microarchitecture to facili tate the latter aspect and will be most effective when combined with software tuning the performance of most applications can be considerably improved if the data required can be fetched from the processor caches or if memory traffic can take advantage of hardware prefetching effectively standard techniques to bring data into the processor before it is needed involve addi tional programming which can be difficult to implement and may require special steps to prevent performance degradation streaming simd extensions addressed this issue by providing various prefetch instructions streaming simd extensions introduced the various non temporal store instructions extends this support to new data types and also introduce non temporal store support for the 32 bit integer registers this chapter focuses on hardware prefetch mechanism software prefetch and cacheability instructions discusses microarchitectural feature and instructions that allow you to affect data caching in an application memory optimization using hardware prefetching software prefetch and cache ability instructions discusses techniques for implementing memory optimiza tions using the above instructions note in a number of cases presented the prefetching and cache utilization described are specific to current implementations of intel netburst microarchitecture but are largely applicable for the future processors using deterministic cache parameters to manage cache hierarchy 1 general prefetch coding guidelines the following guidelines will help you to reduce memory traffic and utilize peak memory system bandwidth more effectively when large amounts of data movement must originate from the memory system take advantage of the hardware prefetcher ability to prefetch data that are accessed in linear patterns in either a forward or backward direction take advantage of the hardware prefetcher ability to prefetch data that are accessed in a regular pattern with access strides that are substantially smaller than half of the trigger distance of the hardware prefetch see table 10 use a current generation compiler such as the intel c compiler that supports c language level features for streaming simd extensions streaming simd extensions and mmx technology instructions provide intrinsics that allow you to optimize cache utilization examples of intel compiler intrinsics include and for details refer to intel c compiler user guide documentation facilitate compiler optimization by minimize use of global variables and pointers minimize use of complex control flow use the const modifier avoid register modifier choose data types carefully see below and avoid type casting use cache blocking techniques for example strip mining as follows improve cache hit rate by using cache blocking techniques such as strip mining one dimensional arrays or loop blocking two dimensional arrays explore using hardware prefetching mechanism if your data access pattern has sufficient regularity to allow alternate sequencing of data accesses for example tiling for improved spatial locality otherwise use prefetchnta balance single pass versus multi pass execution single pass or unlayered execution passes a single data element through an entire computation pipeline multi pass or layered execution performs a single stage of the pipeline on a batch of data elements before passing the entire batch on to the next stage if your algorithm is single pass use prefetchnta if your algorithm is multi pass use resolve memory bank conflict issues minimize memory bank conflicts by applying array grouping to group contiguously used data together or by allocating data within kbyte memory pages resolve cache management issues minimize the disturbance of temporal data held within processor caches by using streaming store instructions optimize software prefetch scheduling distance far ahead enough to allow interim computations to overlap memory access time near enough that prefetched data is not replaced from the data cache use software prefetch concatenation arrange prefetches to avoid unnecessary prefetches at the end of an inner loop and to prefetch the first few iterations of the inner loop inside the next outer loop minimize the number of software prefetches prefetch instructions are not completely free in terms of bus cycles machine cycles and resources excessive usage of prefetches can adversely impact application performance interleave prefetches with computation instructions for best performance software prefetch instructions must be interspersed with computational instruc tions in the instruction sequence rather than clustered together hardware prefetching of data pentium m intel core solo and intel core duo processors and processors based on intel core microarchitecture and intel netburst microarchitecture provide hardware data prefetch mechanisms which monitor application data access patterns and prefetches data automatically this behavior is automatic and does not require programmer intervention for processors based on intel netburst microarchitecture characteristics of the hardware data prefetcher are 1 it requires two successive cache misses in the last level cache to trigger the mechanism these two cache misses must satisfy the condition that strides of the cache misses are less than the trigger distance of the hardware prefetch mechanism see table 10 attempts to stay 256 bytes ahead of current data access locations follows only one stream per kbyte page load or store can prefetch up to 8 simultaneous independent streams from eight different kbyte regions does not prefetch across kbyte boundary this is independent of paging modes 6 fetches data into second third level cache does not prefetch uc or wc memory types 8 follows load and store streams issues read for ownership rfo transactions for store streams and data reads for load streams other than items and discussed above most other characteristics also apply to pentium m intel core solo and intel core duo processors the hardware prefetcher implemented in the pentium m processor fetches data to a second level cache it can track 12 independent streams in the forward direction and independent streams in the backward direction the hardware prefetcher of intel core solo processor can track 16 forward streams and backward streams on the intel core duo processor the hardware prefetcher in each core fetches data independently hardware prefetch mechanisms of processors based on intel core microarchitecture are discussed in section and section 3 despite differences in hardware implementation technique the overall benefit of hardware prefetching to software are similar between intel core microarchitecture and prior microarchitectures 3 prefetch and cacheability instructions the prefetch instruction inserted by the programmers or compilers accesses a minimum of two cache lines of data on the pentium processor prior to the data actually being needed one cache line of data on the pentium m processor this hides the latency for data access in the time required to process data already resi dent in the cache many algorithms can provide information in advance about the data that is to be required in cases where memory accesses are in long regular data patterns the automatic hardware prefetcher should be favored over software prefetches the cacheability control instructions allow you to control data caching strategy in order to increase cache efficiency and minimize cache pollution data reference patterns can be classified as follows temporal data will be used again soon spatial data will be used in adjacent locations for example on the same cache line non temporal data which is referenced once and not reused in the immediate future for example for some multimedia data types as the vertex buffer in a graphics application these data characteristics are used in the discussions that follow prefetch this section discusses the mechanics of the software prefetch instructions in general software prefetch instructions should be used to supplement the practice of tuning an access pattern to suit the automatic hardware prefetch mechanism 1 software data prefetch the prefetch instruction can hide the latency of data access in performance critical sections of application code by allowing data to be fetched in advance of actual usage prefetch instructions do not change the user visible semantics of a program although they may impact program performance prefetch merely provides a hint to the hardware and generally does not generate exceptions or faults prefetch loads either non temporal data or temporal data in the specified cache level this data access type and the cache level are specified as a hint depending on the implementation the instruction fetches 32 or more aligned bytes including the specified address byte into the instruction specified cache levels prefetch is implementation specific applications need to be tuned to each imple mentation to maximize performance note using the prefetch instruction is recommended only if data does not fit in cache prefetch provides a hint to the hardware it does not generate exceptions or faults except for a few special cases see section 3 prefetch and load instructions however excessive use of prefetch instructions may waste memory bandwidth and result in a performance penalty due to resource constraints nevertheless prefetch can lessen the overhead of memory transactions by preventing cache pollution and by using caches and memory efficiently this is partic ularly important for applications that share critical system resources such as the memory bus see an example in section 1 video encoder prefetch is mainly designed to improve application performance by hiding memory latency in the background if segments of an application access data in a predictable manner for example using arrays with known strides they are good candidates for using prefetch to improve performance use the prefetch instructions in predictable memory access patterns time consuming innermost loops locations where the execution pipeline may stall if data is not available 2 prefetch instructions pentium processor implementation streaming simd extensions include four prefetch instructions variants one non temporal and three temporal they correspond to two types of operations temporal and non temporal note at the time of prefetch if data is already found in a cache level that is closer to the processor than the cache level specified by the instruction no data movement occurs the non temporal instruction is prefetchnta fetch the data into the second level cache minimizing cache pollution temporal instructions are fetch the data into all cache levels that is to the second level cache for the pentium processor this instruction is identical to this instruction is identical to 3 prefetch and load instructions the pentium processor has a decoupled execution and memory architecture that allows instructions to be executed independently with memory accesses if there are no data and resource dependencies programs or compilers can use dummy load instructions to imitate prefetch functionality but preloading is not completely equivalent to using prefetch instructions prefetch provides greater performance than preloading currently prefetch provides greater performance than preloading because has no destination register it only updates cache lines does not stall the normal instruction retirement does not affect the functional behavior of the program has no cache line split accesses does not cause exceptions except when the lock prefix is used the lock prefix is not a valid prefix for use with prefetch does not complete its own execution if that would cause a fault currently the advantage of prefetch over preloading instructions are processor specific this may change in the future there are cases where a prefetch will not perform the data prefetch these include prefetch causes a dtlb data translation lookaside buffer miss this applies to pentium processors with cpuid signature corresponding to family 15 model 0 1 or 2 prefetch resolves dtlb misses and fetches data on pentium processors with cpuid signature corresponding to family 15 model 3 an access to the specified address that causes a fault exception if the memory subsystem runs out of request buffers between the first level cache and the second level cache prefetch targets an uncacheable memory region for example uswc and uc the lock prefix is used this causes an invalid opcode exception cacheability control this section covers the mechanics of cacheability control instructions 1 the non temporal store instructions this section describes the behavior of streaming stores and reiterates some of the information presented in the previous section in streaming simd extensions the movntps movntpd movntq movntdq movnti maskmovq and maskmovdqu instructions are streaming non temporal stores with regard to memory characteristics and ordering they are similar to the write combining wc memory type write combining successive writes to the same cache line are combined write collapsing successive writes to the same byte result in only the last write being visible weakly ordered no ordering is preserved between wc stores or between wc stores and other loads or stores uncacheable and not write allocating stored data is written around the cache and will not generate a read for ownership bus request for the corre sponding cache line 1 1 fencing because streaming stores are weakly ordered a fencing operation is required to ensure that the stored data is flushed from the processor to memory failure to use an appropriate fence may result in data being trapped within the processor and will prevent visibility of this data by other processors or system agents wc stores require software to ensure coherence of data by performing the fencing operation see section fence instructions 1 2 streaming non temporal stores streaming stores can improve performance by increasing store bandwidth if the bytes that fit within a cache line are written consecutively since they do not require read for ownership bus requests and bytes are combined into a single bus write transaction reducing disturbance of frequently used cached temporal data since they write around the processor caches streaming stores allow cross aliasing of memory types for a given memory region for instance a region may be mapped as write back wb using page attribute tables pat or memory type range registers mtrrs and yet is written using a streaming store 1 3 memory type and non temporal stores memory type can take precedence over a non temporal hint leading to the following considerations if the programmer specifies a non temporal store to strongly ordered uncacheable memory for example uncacheable uc or write protect wp memory types then the store behaves like an uncacheable store the non temporal hint is ignored and the memory type for the region is retained if the programmer specifies the weakly ordered uncacheable memory type of write combining wc then the non temporal store and the region have the same semantics and there is no conflict if the programmer specifies a non temporal store to cacheable memory for example write back wb or write through wt memory types two cases may result case 1 if the data is present in the cache hierarchy the instruction will ensure consistency a particular processor may choose different ways to implement this the following approaches are probable a updating data in place in the cache hierarchy while preserving the memory type semantics assigned to that region or b evicting the data from the caches and writing the new non temporal data to memory with wc semantics the approaches separate or combined can be different for future processors pentium intel core solo and intel core duo processors implement the latter policy of evicting data from all processor caches the pentium m processor implements a combination of both approaches if the streaming store hits a line that is present in the first level cache the store data is combined in place within the first level cache if the streaming store hits a line present in the second level the line and stored data is flushed from the second level to system memory case 2 if the data is not present in the cache hierarchy and the destination region is mapped as wb or wt the transaction will be weakly ordered and is subject to all wc memory semantics this non temporal store will not write allocate different implementations may choose to collapse and combine such stores 1 write combining generally wc semantics require software to ensure coherence with respect to other processors and other system agents such as graphics cards appropriate use of synchronization and a fencing operation must be performed for producer consumer usage models see section fence instructions fencing ensures that all system agents have global visibility of the stored data for instance failure to fence may result in a written cache line staying within a processor and the line would not be visible to other agents for processors which implement non temporal stores by updating data in place that already resides in the cache hierarchy the destination region should also be mapped as wc otherwise if mapped as wb or wt there is a potential for speculative processor reads to bring the data into the caches in such a case non temporal stores would then update in place and data would not be flushed from the processor by a subsequent fencing operation the memory type visible on the bus in the presence of memory type aliasing is imple mentation specific as one example the memory type written to the bus may reflect the memory type for the first store to the line as seen in program order other alter natives are possible this behavior should be considered reserved and dependence on the behavior of any particular implementation risks future incompatibility 2 streaming store usage models the two primary usage domains for streaming store are coherent requests and non coherent requests 2 1 coherent requests coherent requests are normal loads and stores to system memory which may also hit cache lines present in another processor in a multiprocessor environment with coherent requests a streaming store can be used in the same way as a regular store that has been mapped with a wc memory type pat or mtrr an sfence instruc tion must be used within a producer consumer usage model in order to ensure coher ency and visibility of data between processors within a single processor system the cpu can also re read the same memory loca tion and be assured of coherence that is a single consistent view of this memory location the same is true for a multiprocessor mp system assuming an accepted mp software producer consumer synchronization policy is employed 2 2 non coherent requests non coherent requests arise from an i o device such as an agp graphics card that reads or writes system memory using non coherent requests which are not reflected on the processor bus and thus will not query the processor caches an sfence instruction must be used within a producer consumer usage model in order to ensure coherency and visibility of data between processors in this case if the processor is writing data to the i o device a streaming store can be used with a processor with any behavior of case 1 section 1 3 only if the region has also been mapped with a wc memory type pat mtrr note failure to map the region as wc may allow the line to be speculatively read into the processor caches via the wrong path of a mispredicted branch in case the region is not mapped as wc the streaming might update in place in the cache and a subsequent sfence would not result in the data being written to system memory explicitly mapping the region as wc in this case ensures that any data read from this region will not be placed in the processor caches a read of this memory location by a non coherent i o device would return incorrect out of date results for a processor which solely implements case 2 section 1 3 a streaming store can be used in this non coherent domain without requiring the memory region to also be mapped as wb since any cached data will be flushed to memory by the streaming store 3 streaming store instruction descriptions movntq movntdq non temporal store of packed integer in an mmx technology or streaming simd extensions register store data from a register to memory they are implicitly weakly ordered do no write allocate and so minimize cache pollution movntps non temporal store of packed single precision floating point is similar to movntq it stores data from a streaming simd extensions register to memory in 16 byte granularity unlike movntq the memory address must be aligned to a 16 byte boundary or a general protection exception will occur the instruction is implicitly weakly ordered does not write allocate and thus minimizes cache pollu tion maskmovq maskmovdqu non temporal byte mask store of packed integer in an mmx technology or streaming simd extensions register store data from a register to the location specified by the edi register the most significant bit in each byte of the second mask register is used to selectively write the data of the first register on a per byte basis the instructions are implicitly weakly ordered that is successive stores may not write memory in original program order do not write allocate and thus minimize cache pollution 5 the streaming load instruction 1 introduces the movntdqa instruction movntdqa loads 16 bytes from memory using a non temporal hint if the memory source is wc type for wc memory type the non temporal hint may be implemented by loading into a temporary internal buffer with the equivalent of an aligned cache line without filling this data to the cache subsequent movntdqa reads to unread portions of the buffered wc data will cause 16 bytes of data transferred from the temporary internal buffer to an xmm register if data is available if used appropriately movntdqa can help software achieve significantly higher throughput when loading data in wc memory region into the processor than other means chapter 1 provides a reference to an application note on using movntdqa addi tional information and requirements to use movntdqa appropriately can be found in chapter 12 programming with ssse3 and of intel and ia 32 architectures software developer manual volume 1 and the instruction reference pages of movntdqa in intel and ia 32 architectures software developer manual volume 5 5 fence instructions the following fence instructions are available sfence lfence and mfence 5 5 1 sfence instruction the sfence store fence instruction makes it possible for every store instruc tion that precedes an sfence in program order to be globally visible before any store that follows the sfence sfence provides an efficient way of ensuring ordering between routines that produce weakly ordered results the use of weakly ordered memory types can be important under certain data sharing relationships such as a producer consumer relationship using weakly ordered memory can make assembling the data more efficient but care must be taken to ensure that the consumer obtains the data that the producer intended to see some common usage models may be affected by weakly ordered stores examples are library functions which use weakly ordered memory to write results compiler generated code which also benefits from writing weakly ordered results hand crafted code the degree to which a consumer of data knows that the data is weakly ordered can vary for different cases as a result sfence should be used to ensure ordering between routines that produce weakly ordered data and routines that consume this data 5 5 2 lfence instruction the lfence load fence instruction makes it possible for every load instruction that precedes the lfence instruction in program order to be globally visible before any load instruction that follows the lfence the lfence instruction provides a means of segregating load instructions from other loads 5 5 3 mfence instruction the mfence memory fence instruction makes it possible for every load store instruction preceding mfence in program order to be globally visible before any load store following mfence mfence provides a means of segregating certain memory instructions from other memory references the use of a lfence and sfence is not equivalent to the use of a mfence since the load and store fences are not ordered with respect to each other in other words the load fence can be executed before prior stores and the store fence can be executed before prior loads mfence should be used whenever the cache line flush instruction clflush is used to ensure that speculative memory references generated by the processor do not interfere with the flush see section 5 6 clflush instruction 5 6 clflush instruction the clflush instruction invalidates the cache line associated with the linear address that contain the byte address of the memory location in all levels of the processor cache hierarchy data and instruction this invalidation is broadcast throughout the coherence domain if at any level of the cache hierarchy a line is inconsistent with memory dirty it is written to memory before invalidation other characteristics include the data size affected is the cache coherency size which is bytes on pentium processor the memory attribute of the page containing the affected line has no effect on the behavior of this instruction the clflush instruction can be used at all privilege levels and is subject to all permission checking and faults associated with a byte load clflush is an unordered operation with respect to other memory traffic including other clflush instructions software should use a memory fence for cases where ordering is a concern as an example consider a video usage model where a video capture device is using non coherent agp accesses to write a capture stream directly to system memory since these non coherent writes are not broadcast on the processor bus they will not flush copies of the same locations that reside in the processor caches as a result before the processor re reads the capture buffer it should use clflush to ensure that stale copies of the capture buffer are flushed from the processor caches due to speculative reads that may be generated by the processor it is important to observe appropriate fencing using mfence example 1 provides pseudo code for clflush usage example 1 pseudo code using clflush 6 memory optimization using prefetch the pentium 4 processor has two mechanisms for data prefetch software controlled prefetch and an automatic hardware prefetch 6 1 software controlled prefetch the software controlled prefetch is enabled using the four prefetch instructions introduced with streaming simd extensions instructions these instructions are hints to bring a cache line of data in to various levels and modes in the cache hier archy the software controlled prefetch is not intended for prefetching code using it can incur significant penalties on a multiprocessor system when code is shared software prefetching has the following characteristics can handle irregular access patterns which do not trigger the hardware prefetcher can use less bus bandwidth than hardware prefetching see below software prefetches must be added to new code and do not benefit existing applications 6 2 hardware prefetch automatic hardware prefetch can bring cache lines into the unified last level cache based on prior data misses it will attempt to prefetch two cache lines ahead of the prefetch stream characteristics of the hardware prefetcher are it requires some regularity in the data access patterns if a data access pattern has constant stride hardware prefetching is effective if the access stride is less than half of the trigger distance of hardware prefetcher see table 2 10 if the access stride is not constant the automatic hardware prefetcher can mask memory latency if the strides of two successive cache misses are less than the trigger threshold distance small stride memory traffic the automatic hardware prefetcher is most effective if the strides of two successive cache misses remain less than the trigger threshold distance and close to bytes there is a start up penalty before the prefetcher triggers and there may be fetches an array finishes for short arrays overhead can reduce effectiveness the hardware prefetcher requires a couple misses before it starts operating hardware prefetching generates a request for data beyond the end of an array which is not be utilized this behavior wastes bus bandwidth in addition this behavior results in a start up penalty when fetching the beginning of the next array software prefetching may recognize and handle these cases it will not prefetch across a 4 kbyte page boundary a program has to initiate demand loads for the new page before the hardware prefetcher starts prefetching from the new page the hardware prefetcher may consume extra system bandwidth if the appli cation memory traffic has significant portions with strides of cache misses greater than the trigger distance threshold of hardware prefetch large stride memory traffic the effectiveness with existing applications depends on the proportions of small stride versus large stride accesses in the application memory traffic an application with a preponderance of small stride memory traffic with good temporal locality will benefit greatly from the automatic hardware prefetcher in some situations memory traffic consisting of a preponderance of large stride cache misses can be transformed by re arrangement of data access sequences to alter the concentration of small stride cache misses at the expense of large stride cache misses to take advantage of the automatic hardware prefetcher 6 3 example of effective latency reduction with hardware prefetch consider the situation that an array is populated with data corresponding to a constant access stride circular pointer chasing sequence see example 2 the potential of employing the automatic hardware prefetching mechanism to reduce the effective latency of fetching a cache line from memory can be illustrated by varying the access stride between bytes and the trigger threshold distance of hardware prefetch when populating the array for circular pointer chasing example 2 populating an array for circular pointer chasing with constant stride register char p char next populating parray for circular pointer chasing with constant access stride p char p loads a value pointing to next load p char parray for i 0 i aperture i stride p char parray i if i stride next parray 0 else next parray i stride p next populate the address of the next node the effective latency reduction for several microarchitecture implementations is shown in figure 1 for a constant stride access pattern the benefit of the auto matic hardware prefetcher begins at half the trigger threshold distance and reaches maximum benefit when the cache miss stride is bytes figure 1 effective latency reduction as a function of access stride 6 4 example of latency hiding with s w prefetch instruction achieving the highest level of memory optimization using prefetch instructions requires an understanding of the architecture of a given machine this section trans lates the key architectural implications into several simple guidelines for program mers to use figure 2 and figure 3 show two scenarios of a simplified geometry pipeline as an example a geometry pipeline typically fetches one vertex record at a time and then performs transformation and lighting functions on it both figures show two separate pipelines an execution pipeline and a memory pipeline front side bus since the pentium 4 processor similar to the pentium ii and pentium iii processors completely decouples the functionality of execution and memory access the two pipelines can function concurrently figure 2 shows bubbles in both the execution and memory pipelines when loads are issued for accessing vertex data the execu tion units sit idle and wait until data is returned on the other hand the memory bus sits idle while the execution units are processing vertices this scenario severely decreases the advantage of having a decoupled architecture figure 2 memory access latency and execution without prefetch figure 3 memory access latency and execution with prefetch the performance loss caused by poor utilization of resources can be completely elim inated by correctly scheduling the prefetch instructions as shown in figure 3 prefetch instructions are issued two vertex iterations ahead this assumes that only one vertex gets processed in one iteration and a new data cache line is needed for each iteration as a result when iteration n vertex vn is being processed the requested data is already brought into cache in the meantime the front side bus is transferring the data needed for iteration n 1 vertex vn 1 because there is no dependence between vn 1 data and the execution of vn the latency for data access of vn 1 can be entirely hidden behind the execution of vn under such circumstances no bubbles are present in the pipelines and thus the best possible performance can be achieved prefetching is useful for inner loops that have heavy computations or are close to the boundary between being compute bound and memory bandwidth bound it is prob ably not very useful for loops which are predominately memory bandwidth bound when data is already located in the first level cache prefetching can be useless and could even slow down the performance because the extra µops either back up waiting for outstanding memory accesses or may be dropped altogether this behavior is platform specific and may change in the future 6 5 software prefetching usage checklist the following checklist covers issues that need to be addressed and or resolved to use the software prefetch instruction properly determine software prefetch scheduling distance use software prefetch concatenation minimize the number of software prefetches mix software prefetch with computation instructions use cache blocking techniques for example strip mining balance single pass versus multi pass execution resolve memory bank conflict issues resolve cache management issues subsequent sections discuss the above items 6 6 software prefetch scheduling distance determining the ideal prefetch placement in the code depends on many architectural parameters including the amount of memory to be prefetched cache lookup latency system memory latency and estimate of computation cycle the ideal distance for prefetching data is processor and platform dependent if the distance is too short the prefetch will not hide the latency of the fetch behind computation if the prefetch is too far ahead prefetched data may be flushed out of the cache by the time it is required since prefetch distance is not a well defined metric for this discussion we define a new term prefetch scheduling distance psd which is represented by the number of iterations for large loops prefetch scheduling distance can be set to 1 that is schedule prefetch instructions one iteration ahead for small loop bodies that is loop iterations with little computation the prefetch scheduling distance must be more than one iteration a simplified equation to compute psd is deduced from the mathematical model for a simplified equation complete mathematical model and methodology of prefetch distance determination see appendix e summary of rules and suggestions example 3 illustrates the use of a prefetch within the loop body the prefetch scheduling distance is set to 3 esi is effectively the pointer to a line edx is the address of the data being referenced and xmm4 are the data used in compu tation example 4 uses two independent cache lines of data per iteration the psd would need to be increased decreased if more less than two cache lines are used per iteration example 3 prefetch scheduling distance prefetchnta edx esi 3 prefetchnta edx 4 esi 3 example 3 prefetch scheduling distance contd movaps xmm1 edx esi movaps xmm2 edx 4 esi movaps xmm3 edx esi 16 movaps xmm4 edx 4 esi 16 add esi cmp esi ecx jl 6 software prefetch concatenation maximum performance can be achieved when the execution pipeline is at maximum throughput without incurring any memory latency penalties this can be achieved by prefetching data to be used in successive iterations in a loop de pipelining memory generates bubbles in the execution pipeline to explain this performance issue a geometry pipeline that processes vertices in strip format is used as an example a strip contains a list of vertices whose predefined vertex order forms contiguous triangles it can be easily observed that the memory pipe is de pipelined on the strip boundary due to ineffective prefetch arrangement the execution pipeline is stalled for the first two iterations for each strip as a result the average latency for completing an iteration will be fix clocks see appendix e summary of rules and suggestions for a detailed description this memory de pipelining creates inefficiency in both the memory pipeline and execution pipeline this de pipelining effect can be removed by applying a technique called prefetch concatenation with this technique the memory access and execu tion can be fully pipelined and fully utilized for nested loops memory de pipelining could occur during the interval between the last iteration of an inner loop and the next iteration of its associated outer loop without paying special attention to prefetch insertion loads from the first iteration of an inner loop can miss the cache and stall the execution pipeline waiting for data returned thus degrading the performance in example 4 the cache line containing a ii 0 is not prefetched at all and always misses the cache this assumes that no array a footprint resides in the cache the penalty of memory de pipelining stalls can be amortized across the inner loop iterations however it may become very harmful when the inner loop is short in addition the last prefetch in the last psd iterations are wasted and consume machine resources prefetch concatenation is introduced here in order to eliminate the performance issue of memory de pipelining example 4 using prefetch concatenation for ii 0 ii ii for jj 0 jj 32 jj 8 prefetch a ii jj 8 computation a ii jj prefetch concatenation can bridge the execution pipeline bubbles between the boundary of an inner loop and its associated outer loop simply by unrolling the last iteration out of the inner loop and specifying the effective prefetch address for data used in the following iteration the performance loss of memory de pipelining can be completely removed example 5 gives the rewritten code example 5 concatenation and unrolling the last iteration of inner loop for ii 0 ii ii for jj 0 jj 24 jj 8 n 1 iterations prefetch a ii jj 8 computation a ii jj prefetch a ii 1 0 computation a ii jj last iteration this code segment for data prefetching is improved and only the first iteration of the outer loop suffers any memory access latency penalty assuming the computation time is larger than the memory latency inserting a prefetch of the first data element needed prior to entering the nested loop computation would eliminate or reduce the start up penalty for the very first iteration of the outer loop this uncomplicated high level code optimization can improve memory performance significantly 6 8 minimize number of software prefetches prefetch instructions are not completely free in terms of bus cycles machine cycles and resources even though they require minimal clock and memory bandwidth excessive prefetching may lead to performance penalties because of issue penalties in the front end of the machine and or resource contention in the memory sub system this effect may be severe in cases where the target loops are small and or cases where the target loop is issue bound one approach to solve the excessive prefetching issue is to unroll and or software pipeline loops to reduce the number of prefetches required figure 4 presents a code example which implements prefetch and unrolls the loop to remove the redun dant prefetch instructions whose prefetch addresses hit the previously issued prefetch instructions in this particular example unrolling the original loop once saves six prefetch instructions and nine instructions for conditional jumps in every other iteration figure 4 prefetch and loop unrolling figure 5 demonstrates the effectiveness of software prefetches in latency hiding figure 5 memory access latency and execution with prefetch the x axis in figure 5 indicates the number of computation clocks per loop each iteration is independent the y axis indicates the execution time measured in clocks per loop the secondary y axis indicates the percentage of bus bandwidth utilization the tests vary by the following parameters number of load store streams each load and store stream accesses one byte cache line each per iteration amount of computation per loop this is varied by increasing the number of dependent arithmetic operations executed number of the software prefetches per loop for example one every 16 bytes 32 bytes bytes bytes as expected the leftmost portion of each of the graphs in figure 5 shows that when there is not enough computation to overlap the latency of memory access prefetch does not help and that the execution is essentially memory bound the graphs also illustrate that redundant prefetches do not increase performance the virtual runtime the vruntime variable stores the virtual runtime of a process which is the actual runtime the amount of time spent running normalized or weighted by the number of runnable processes the virtual runtime units are nanoseconds and therefore vruntime is decou pled from the timer tick the virtual runtime is used to help us approximate the ideal multitasking processor that cfs is modeling with such an ideal processor we wouldn t need vruntime because all runnable processes would perfectly multitask that is on an ideal processor the virtual runtime of all processes of the same priority would be identi cal all tasks would have received an equal fair share of the processor because processors are not capable of perfect multitasking and we must run each process in succession cfs uses vruntime to account for how long a process has run and thus how much longer it ought to run the function defined in kernel c manages this accounting static void struct struct curr curr now clock unsigned long if unlikely curr return get the amount of time the current task was running since the last time we changed load this cannot overflow on bits unsigned long now curr if return curr curr now if curr struct curtask curr curtask curr vruntime curtask curtask calculates the execution time of the current process and stores that value in it then passes that runtime to which weights the time by the number of runnable processes the current process vruntime is then incre mented by the weighted value update the current task runtime statistics skip current tasks that are not in our scheduling class static inline void struct struct curr unsigned long unsigned long curr max curr curr delta_exec delta_exec curr curr vruntime is invoked periodically by the system timer and also whenever a process becomes runnable or blocks becoming unrunnable in this manner vruntime is an accurate measure of the runtime of a given process and an indicator of what process should run next process selection in the last section we discussed how vruntime on an ideal perfectly multitasking proces sor would be identical among all runnable processes in reality we cannot perfectly multi task so cfs attempts to balance a process virtual runtime with a simple rule when cfs is deciding what process to run next it picks the process with the smallest vruntime this is in fact the core of cfs scheduling algorithm pick the task with the smallest vruntime that it the rest of this subsection describes how the selection of the process with the smallest vruntime is implemented cfs uses a red black tree to manage the list of runnable processes and efficiently find the process with the smallest vruntime a red black tree called an rbtree in linux is a type of self balancing binary search tree we discuss self balancing binary search trees in general and red black trees in particular in chapter for now if you are unfamiliar you need to know only that red black trees are a data structure that store nodes of arbitrary data iden tified by a specific key and that they enable efficient search for a given key specifically obtaining a node identified by a given key is logarithmic in time as a function of total nodes in the tree picking the next task let start with the assumption that we have a red black tree populated with every runnable process in the system where the key for each node is the runnable process vir tual runtime we ll look at how we build that tree in a moment but for now let assume we have it given this tree the process that cfs wants to run next which is the process with the smallest vruntime is the leftmost node in the tree that is if you follow the tree from the root down through the left child and continue moving to the left until you reach a leaf node you find the process with the smallest vruntime again if you are unfamiliar with binary search trees don t worry just know that this process is efficient cfs process selection algorithm is thus summed up as run the process represented by the leftmost node in the rbtree the function that performs this selection is defined in kernel c static struct struct struct left if left return null return left struct note that does not actually traverse the tree to find the left most node because the value is cached by although it is efficient to walk the tree to find the leftmost node o height of tree which is o log n for n nodes if the tree is balanced it is even easier to cache the leftmost node the return value from this function is the process that cfs next runs if the function returns null there is no leftmost node and thus no nodes in the tree in that case there are no runnable processes and cfs schedules the idle task adding processes to the tree now let look at how cfs adds processes to the rbtree and caches the leftmost node this would occur when a process becomes runnable wakes up or is first created via fork as discussed in chapter adding processes to the tree is performed by static void struct struct se int flags update the normalized vruntime before updating through callig if flags flags se vruntime update run time statistics of the current se if flags se se se se if se curr se this function updates the runtime and other statistics and then invokes to perform the actual heavy lifting of inserting the entry into the red black tree enqueue an entity into the rb tree static void struct struct se struct link struct parent null struct entry key se int leftmost find the right place in the rbtree while link parent link entry parent struct we dont care about collisions nodes with the same key stay together if key entry link parent else link parent leftmost maintain a cache of leftmost tree entries it is frequently used if leftmost se se parent link se let look at this function the body of the while loop traverses the tree in search of a matching key which is the inserted process vruntime per the rules of the balanced tree it moves to the left child if the key is smaller than the current node key and to the right child if the key is larger if it ever moves to the right even once it knows the inserted process cannot be the new leftmost node and it sets leftmost to zero if it moves only to the left leftmost remains one and we have a new leftmost node and can update the cache by setting to the inserted process the loop terminates when we compare ourselves to a node that has no child in the direction we move link is then null and the loop terminates when out of the loop the function calls on the parent node making the inserted process the new child the function updates the self balancing properties of the tree we discuss the col oring in chapter removing processes from the tree finally let look at how cfs removes processes from the red black tree this happens when a process blocks becomes unrunnable or terminates ceases to exist static void struct struct se int sleep update run time statistics of the current se se if se curr se se cfs_rq normalize the entity after updating the because the update can refer to the curr item and we need to reflect this movement in our normalized position if sleep se vruntime cfs_rq min_vruntime as with adding a process to the red black tree the real work is performed by a helper function static void struct cfs_rq cfs_rq struct se if cfs_rq se struct se cfs_rq next_node se cfs_rq removing a process from the tree is much simpler because the rbtree implementation provides the function that performs all the work the rest of this function updates the rb_leftmost cache if the process to remove is the leftmost node the func tion invokes to find what would be the next node in an in order traversal this is what will be the leftmost node when the current leftmost node is removed the scheduler entry point the main entry point into the process schedule is the function schedule defined in kernel sched c this is the function that the rest of the kernel uses to invoke the process scheduler deciding which process to run and then running it schedule is generic with respect to scheduler classes that is it finds the highest priority scheduler class with a runnable process and asks it what to run next given that it should be no surprise that schedule is simple the only important part of the function which is otherwise too uninteresting to reproduce here is its invocation of also defined in kernel sched c the function goes through each scheduler class starting with the highest priority and selects the highest priority process in the highest priority class pick up the highest prio task static inline struct struct rq rq const struct class struct p optimization we know that if all tasks are in the fair class we can call that function directly if likely rq rq cfs p rq if likely p return p class for p class rq if p return p will never be null as the idle class always returns a non null p class class next note the optimization at the beginning of the function because cfs is the scheduler class for normal processes and most systems run mostly normal processes there is a small hack to quickly select the next cfs provided process if the number of runnable processes is equal to the number of cfs runnable processes which suggests that all runnable processes are provided by cfs the core of the function is the for loop which iterates over each class in priority order starting with the highest priority class each class implements the function which returns a pointer to its next runnable process or if there is not one null the first class to return a non null value has selected the next runnable process cfs implementation of calls which in turn calls the function that we discussed in the previous section sleeping and waking up tasks that are sleeping blocked are in a special nonrunnable state this is important because without this special state the scheduler would select tasks that did not want to run or worse sleeping would have to be implemented as busy looping a task sleeps for a number of reasons but always while it is waiting for some event the event can be a spec ified amount of time more data from a file i o or another hardware event a task can also involuntarily go to sleep when it tries to obtain a contended semaphore in the kernel this is covered in chapter an introduction to kernel synchronization a common reason to sleep is file i o for example the task issued a read request on a file which needs to be read in from disk as another example the task could be waiting for keyboard input whatever the case the kernel behavior is the same the task marks itself as sleeping puts itself on a wait queue removes itself from the red black tree of runnable and calls schedule to select a new process to execute waking back up is the inverse the task is set as runnable removed from the wait queue and added back to the red black tree as discussed in the previous chapter two states are associated with sleeping and they differ only in that tasks in the state ignore signals whereas tasks in the state wake up prematurely and respond to a signal if one is issued both types of sleeping tasks sit on a wait queue waiting for an event to occur and are not runnable wait queues sleeping is handled via wait queues a wait queue is a simple list of processes waiting for an event to occur wait queues are represented in the kernel by wait queues are created statically via or dynamically via processes put themselves on a wait queue and mark themselves not runnable when the event associated with the wait queue occurs the processes on the queue are awakened it is important to implement sleeping and waking correctly to avoid race conditions some simple interfaces for sleeping used to be in wide use these interfaces however have races it is possible to go to sleep after the condition becomes true in that case the task might sleep indefinitely therefore the recommended method for sleeping in the ker nel is a bit more complicated q is the wait queue we wish to sleep on wait q wait while condition condition is the event that we are waiting for q wait if current handle signal schedule q wait the task performs the following steps to add itself to a wait queue creates a wait queue entry via the macro adds itself to a wait queue via this wait queue awakens the process when the condition for which it is waiting occurs of course there needs to be code elsewhere that calls on the queue when the event actually does occur calls to change the process state to either or this function also adds the task back to the wait queue if necessary which is needed on subsequent iterations of the loop if the state is set to a signal wakes the process up this is called a spurious wake up a wake up not caused by the occurrence of the event so check and handle signals when the task awakens it again checks whether the condition is true if it is it exits the loop otherwise it again calls schedule and repeats now that the condition is true the task sets itself to and removes itself from the wait queue via if the condition occurs before the task goes to sleep the loop terminates and the task does not erroneously go to sleep note that kernel code often has to perform various other tasks in the body of the loop for example it might need to release locks before calling schedule and reacquire them after or react to other events the function in fs notify inotify c which handles reading from the inotify file descriptor is a straightforward example of using wait queues static struct file file char user buf count pos struct group struct kevent char user start int ret wait start buf group file while group wait group kevent group count group if kevent ret kevent if kevent break ret group kevent buf kevent if ret break buf ret count ret continue ret eagain if file break ret eintr if current break if start buf break schedule group wait if start buf ret efault ret buf start return ret this function follows the pattern laid out in our example the main difference is that it checks for the condition in the body of the while loop instead of in the while statement itself this is because checking the condition is complicated and requires grab bing locks the loop is terminated via break waking up waking is handled via which wakes up all the tasks waiting on the given wait queue it calls which sets the task state to calls to add the task to the red black tree and sets if the awakened task priority is higher than the priority of the current task the code that causes the event to occur typically calls itself for example when data arrives from the hard disk the vfs calls on the wait queue that holds the processes waiting for the data an important note about sleeping is that there are spurious wake ups just because a task is awakened does not mean that the event for which the task is waiting has occurred sleeping should always be handled in a loop that ensures that the condition for which the task is waiting has indeed occurred figure depicts the relationship between each scheduler state adds task to a wait queue sets the task state to and calls schedule schedule calls which removes the task from the runqueue task is runnable task is not runnable receives a signal task state is set to and task executes signal handler event the task is waiting for occurs and sets the task to calls to add the task to a runqueue and calls schedule removes the task from the wait queue figure sleeping and waking up preemption and context switching context switching the switching from one runnable task to another is handled by the function defined in kernel sched c it is called by schedule when a new process has been selected to run it does two basic jobs n calls which is declared in asm h to switch the vir tual memory mapping from the previous process to that of the new process n calls declared in asm system h to switch the processor state from the previous process to the current this involves saving and restoring stack infor mation and the processor registers and any other architecture specific state that must be managed and restored on a per process basis the kernel however must know when to call schedule if it called schedule only when code explicitly did so user space programs could run indefinitely instead the kernel provides the flag to signify whether a reschedule should be performed see table this flag is set by when a process should be preempted and by when a process that has a higher priority than the currently run ning process is awakened the kernel checks the flag sees that it is set and calls schedule to switch to a new process the flag is a message to the kernel that the scheduler should be invoked as soon as possible because another process deserves to run table functions for accessing and manipulating function purpose set the flag in the given process clear the flag in the given process test the value of the flag return true if set and false otherwise upon returning to user space or returning from an interrupt the flag is checked if it is set the kernel invokes the scheduler before continuing the flag is per process and not simply global because it is faster to access a value in the process descriptor because of the speed of current and high probability of it being cache hot than a global variable historically the flag was global before the kernel in and the flag was an int inside the in it was moved into a sin gle bit of a special flag variable inside the structure user preemption user preemption occurs when the kernel is about to return to user space is set and therefore the scheduler is invoked if the kernel is returning to user space it knows it is in a safe quiescent state in other words if it is safe to continue executing the current task it is also safe to pick a new task to execute consequently whenever the ker nel is preparing to return to user space either on return from an interrupt or after a sys tem call the value of is checked if it is set the scheduler is invoked to select a new more fit process to execute both the return paths for return from interrupt and return from system call are architecture dependent and typically implemented in assembly in entry s which aside from kernel entry code also contains kernel exit code in short user preemption can occur n when returning to user space from a system call n when returning to user space from an interrupt handler kernel preemption the linux kernel unlike most other unix variants and many other operating systems is a fully preemptive kernel in nonpreemptive kernels kernel code runs until completion that is the scheduler cannot reschedule a task while it is in the kernel kernel code is scheduled cooperatively not preemptively kernel code runs until it finishes returns to user space or explicitly blocks in the kernel however the linux kernel became pre emptive it is now possible to preempt a task at any point so long as the kernel is in a state in which it is safe to reschedule so when is it safe to reschedule the kernel can preempt a task running in the kernel so long as it does not hold a lock that is locks are used as markers of regions of nonpre emptibility because the kernel is smp safe if a lock is not held the current code is reen trant and capable of being preempted the first change in supporting kernel preemption was the addition of a preemption counter to each process this counter begins at zero and increments once for each lock that is acquired and decrements once for each lock that is released when the counter is zero the kernel is preemptible upon return from interrupt if returning to kernel space the kernel checks the values of and if is set and is zero then a more impor tant task is runnable and it is safe to preempt thus the scheduler is invoked if is nonzero a lock is held and it is unsafe to reschedule in that case the interrupt returns as usual to the currently executing task when all the locks that the cur rent task is holding are released returns to zero at that time the unlock code checks whether is set if so the scheduler is invoked enabling and disabling kernel preemption is sometimes required in kernel code and is discussed in chapter kernel preemption can also occur explicitly when a task in the kernel blocks or explicitly calls schedule this form of kernel preemption has always been supported because no additional logic is required to ensure that the kernel is in a state that is safe to preempt it is assumed that the code that explicitly calls schedule knows it is safe to reschedule kernel preemption can occur n when an interrupt handler exits before returning to kernel space n when kernel code becomes preemptible again n if a task in the kernel explicitly calls schedule n if a task in the kernel blocks which results in a call to schedule real time scheduling policies linux provides two real time scheduling policies and the nor mal not real time scheduling policy is via the scheduling classes framework these real time policies are managed not by the completely fair scheduler but by a spe cial real time scheduler defined in kernel c the rest of this section discusses the real time scheduling policies and algorithm implements a simple first in first out scheduling algorithm without timeslices a runnable task is always scheduled over any tasks when a task becomes runnable it continues to run until it blocks or explic itly yields the processor it has no timeslice and can run indefinitely only a higher prior ity or task can preempt a task two or more tasks at the same priority run round robin but again only yielding the processor when they explicitly choose to do so if a task is runnable all tasks at a lower priority cannot run until it becomes unrunnable is identical to except that each process can run only until it exhausts a predetermined timeslice that is is with timeslices it is a real time round robin scheduling algorithm when a task exhausts its times lice any other real time processes at its priority are scheduled round robin the timeslice is used to allow only rescheduling of same priority processes as with a higher priority process always immediately preempts a lower priority one and a lower priority process can never preempt a task even if its timeslice is exhausted both real time scheduling policies implement static priorities the kernel does not cal culate dynamic priority values for real time tasks this ensures that a real time process at a given priority always preempts a process at a lower priority the real time scheduling policies in linux provide soft real time behavior soft real time refers to the notion that the kernel tries to schedule applications within timing deadlines but the kernel does not promise to always achieve these goals conversely hard real time systems are guaranteed to meet any scheduling requirements within certain lim its linux makes no guarantees on the capability to schedule real time tasks despite not having a design that guarantees hard real time behavior the real time scheduling per formance in linux is quite good the linux kernel is capable of meeting stringent timing requirements real time priorities range inclusively from zero to minus by default is therefore the default real time priority range is zero to this priority space is shared with the nice values of tasks they use the space from to by default this means the to nice range maps directly onto the priority space from to scheduler related system calls linux provides a family of system calls for the management of scheduler parameters these system calls allow manipulation of process priority scheduling policy and processor affinity as well as provide an explicit mechanism to yield the processor to other tasks various books and your friendly system man pages provide reference to these sys tem calls which are all implemented in the c library without much wrapper they just invoke the system call table lists the system calls and provides a brief description how system calls are implemented in the kernel is discussed in chapter system calls table scheduler related system calls system call description nice sets a process nice value sets a process scheduling policy gets a process scheduling policy sets a process real time priority gets a process real time priority gets the maximum real time priority gets the minimum real time priority gets a process timeslice value sets a process processor affinity gets a process processor affinity temporarily yields the processor scheduling policy and priority related system calls the and system calls set and get a given process scheduling policy and real time priority respectively their implementation like most system calls involves a lot of argument checking setup and cleanup the important work however is merely to read or write the policy and values in the process the and system calls set and get a process real time priority these calls merely encode in a special structure the calls and return the maximum and minimum priorities respectively for a given scheduling policy the maximum priority for the real time policies is minus one the mini mum is one for normal tasks the nice function increments the given process static priority by the given amount only root can provide a negative value thereby lowering the nice value and increasing the priority the nice function calls the kernel function which sets the and prio values in the task as appropriate processor affinity system calls the linux scheduler enforces hard processor affinity that is although it tries to provide soft or natural affinity by attempting to keep processes on the same processor the sched uler also enables a user to say this task must remain on this subset of the available processors no matter what this hard affinity is stored as a bitmask in the task as the bitmask contains one bit per possible processor on the system by default all bits are set and therefore a process is potentially runnable on any processor the user however via can provide a different bit mask of any combination of one or more bits likewise the call returns the current bitmask the kernel enforces hard affinity in a simple manner first when a process is initially created it inherits its parent affinity mask because the parent is running on an allowed processor the child thus runs on an allowed processor second when a processor affinity is changed the kernel uses the migration threads to push the task onto a legal processor finally the load balancer pulls tasks to only an allowed processor therefore a process only ever runs on a processor whose bit is set in the field of its process descriptor yielding processor time linux provides the system call as a mechanism for a process to explicitly yield the processor to other waiting processes it works by removing the process from the active array where it currently is because it is running and inserting it into the expired array this has the effect of not only preempting the process and putting it at the end of its priority list but also putting it on the expired list guaranteeing it will not run for a while because real time tasks never expire they are a special case therefore they are merely moved to the end of their priority list and not inserted into the expired array in earlier versions of linux the semantics of the call were quite different at best the task was moved only to the end of its priority list the yielding was often not for a long time nowadays applications and even kernel code should be certain they truly want to give up the processor before calling kernel code as a convenience can call yield which ensures that the task state is and then call user space applications use the system call conclusion the process scheduler is an important part of any kernel because running processes is for most of us at least the point of using the computer in the first place juggling the demands of process scheduling is nontrivial however a large number of runnable processes scalability concerns trade offs between latency and throughput and the demands of various workloads make a one size fits all algorithm hard to achieve the linux kernel new cfs process scheduler however comes close to appeasing all parties and providing an optimal solution for most use cases with good scalability through a novel interesting approach the previous chapter covered process management this chapter ruminated on the theory behind process scheduling and the specific implementation algorithms and inter faces used by the current linux kernel the next chapter covers the primary interface that the kernel provides to running processes system calls system calls in any modern operating system the kernel provides a set of interfaces by which processes running in user space can interact with the system these interfaces give appli cations controlled access to hardware a mechanism with which to create new processes and communicate with existing ones and the capability to request other operating system resources the interfaces act as the messengers between applications and the kernel with the applications issuing various requests and the kernel fulfilling them or returning an error the existence of these interfaces and the fact that applications are not free to directly do whatever they want is key to providing a stable system communicating with the kernel system calls provide a layer between the hardware and user space processes this layer serves three primary purposes first it provides an abstracted hardware interface for user space when reading or writing from a file for example applications are not concerned with the type of disk media or even the type of filesystem on which the file resides sec ond system calls ensure system security and stability with the kernel acting as a middle man between system resources and user space the kernel can arbitrate access based on permissions users and other criteria for example this arbitration prevents applications from incorrectly using hardware stealing other processes resources or otherwise doing harm to the system finally a single common layer between user space and the rest of the system allows for the virtualized system provided to processes discussed in chapter process management if applications were free to access system resources without the kernel knowledge it would be nearly impossible to implement multitasking and virtual memory and certainly impossible to do so with stability and security in linux system calls are the only means user space has of interfacing with the kernel they are the only legal entry point into the kernel other than exceptions and traps indeed other interfaces such as device files or proc are ultimately accessed via system calls interestingly linux implements far fewer system calls than most systems chapter addresses the role and implementation of system calls in linux apis posix and the c library typically applications are programmed against an application programming interface api implemented in user space not directly to system calls this is important because no direct correlation is needed between the interfaces that applications make use of and the actual interface provided by the kernel an api defines a set of programming inter faces used by applications those interfaces can be implemented as a system call imple mented through multiple system calls or implemented without the use of system calls at all the same api can exist on multiple systems and provide the same interface to applica tions while the implementation of the api itself can differ greatly from system to system see figure for an example of the relationship between a posix api the c library and system calls application c library kernel figure the relationship between applications the c library and the kernel with a call to printf one of the more common application programming interfaces in the unix world is based on the posix standard technically posix is composed of a series of standards from the that aim to provide a portable operating system standard roughly based on unix linux strives to be posix and compliant where applicable posix is an excellent example of the relationship between apis and system calls on most unix systems the posix defined api calls have a strong correlation to the system calls indeed the posix standard was created to resemble the interfaces provided by ear lier unix systems on the other hand some systems that are rather un unix such as microsoft windows offer posix compatible libraries there are about system calls are on each architecture is allowed to define unique system calls although not all operating systems publish their exact system calls some operating systems are estimated to have more than one thousand in the previous edition of this book had only sys tem calls ieee eye triple e is the institute of electrical and electronics engineers it is a nonprofit professional association involved in numerous technical areas and responsible for many important standards such as posix for more information visit http www ieee org syscalls the system call interface in linux as with most unix systems is provided in part by the c library the c library implements the main api on unix systems including the standard c library and the system call interface the c library is used by all c programs and because of c nature is easily wrapped by other programming languages for use in their programs the c library additionally provides the majority of the posix api from the application programmer point of view system calls are irrelevant all the programmer is concerned with is the api conversely the kernel is concerned only with the system calls what library calls and applications make use of the system calls is not of the kernel concern nonetheless it is important for the kernel to keep track of the potential uses of a system call and keep the system call as general and flexible as possible a meme related to interfaces in unix is provide mechanism not policy in other words unix system calls exist to provide a specific function in an abstract sense the manner in which the function is used is not any of the kernel business syscalls system calls often called syscalls in linux are typically accessed via function calls defined in the c library they can define zero one or more arguments inputs and might result in one or more side effects for example writing to a file or copying some data into a provided pointer system calls also provide a return value of type that signifies suc cess or error usually although not always a negative return value denotes an error a return value of zero is usually but again not always a sign of success the c library when a system call returns an error writes a special error code into the global errno variable this variable can be translated into human readable errors via library functions such as perror finally system calls have a defined behavior for example the system call getpid is defined to return an integer that is the current process pid the implementation of this syscall in the kernel is simple getpid return current returns current tgid note that the definition says nothing of the implementation the kernel must provide the intended behavior of the system call but is free to do so with whatever implementation note the might here although nearly all system calls have a side effect that is they result in some change of the system state a few syscalls such as getpid merely return some data from the kernel the use of type long is for compatibility with bit architectures it wants as long as the result is correct of course this system call is as simple as they come and there are not too many other ways to implement it is simply a macro that defines a system call with no parameters hence the the expanded code looks like this asmlinkage long void let look at how system calls are defined first note the asmlinkage modifier on the function definition this is a directive to tell the compiler to look only on the stack for this function arguments this is a required modifier for all system calls second the func tion returns a long for compatibility between and bit systems system calls defined to return an int in user space return a long in the kernel third note that the getpid system call is defined as in the kernel this is the naming con vention taken with all system calls in linux system call bar is implemented in the ker nel as function system call numbers in linux each system call is assigned a syscall number this is a unique number that is used to reference a specific system call when a user space process executes a system call the syscall number identifies which syscall was executed the process does not refer to the syscall by name the syscall number is important when assigned it cannot change or compiled appli cations will break likewise if a system call is removed its system call number cannot be recycled or previously compiled code would aim to invoke one system call but would in reality invoke another linux provides a not implemented system call which does nothing except return enosys the error corresponding to an invalid system call this function is used to plug the hole in the rare event that a syscall is removed or otherwise made unavailable the kernel keeps a list of all registered system calls in the system call table stored in this table is architecture on it is defined in arch kernel c this table assigns each valid syscall to a unique syscall number system call performance system calls in linux are faster than in many other operating systems this is partly because of linux fast context switch times entering and exiting the kernel is a stream lined and simple affair the other factor is the simplicity of the system call handler and the individual system calls themselves you might be wondering why does getpid return tgid the thread group id in normal process es the tgid is equal to the pid with threads the tgid is the same for all threads in a thread group this enables the threads to call getpid and get the same pid system call handler system call handler it is not possible for user space applications to execute kernel code directly they cannot simply make a function call to a method existing in kernel space because the kernel exists in a protected memory space if applications could directly read and write to the kernel address space system security and stability would be nonexistent instead user space applications must somehow signal to the kernel that they want to execute a system call and have the system switch to kernel mode where the system call can be executed in kernel space by the kernel on behalf of the application the mechanism to signal the kernel is a software interrupt incur an exception and the system will switch to kernel mode and execute the exception handler the exception handler in this case is actually the system call handler the defined software interrupt on is interrupt number which is incurred via the int instruction it triggers a switch to kernel mode and the execution of exception vector which is the system call handler the system call handler is the aptly named function it is architecture dependent on it is implemented in assembly in s recently processors added a feature known as sysenter this feature provides a faster more specialized way of trapping into a kernel to execute a system call than using the int interrupt instruction support for this feature was quickly added to the kernel regardless of how the system call handler is invoked however the important notion is that somehow user space causes an exception or trap to enter the kernel denoting the correct system call simply entering kernel space alone is not sufficient because multiple system calls exist all of which enter the kernel in the same manner thus the system call number must be passed into the kernel on the syscall number is fed to the kernel via the eax regis ter before causing the trap into the kernel user space sticks in eax the number corre sponding to the desired system call the system call handler then reads the value from eax other architectures do something similar the function checks the validity of the given system call number by comparing it to if it is larger than or equal to the function returns enosys otherwise the specified system call is invoked call rax because each element in the system call table is bits bytes the kernel multiplies the given system call number by four to arrive at its location in the system call table on the code is similar with the replaced by see figure much of the following description of the system call handler is based on the version they are all similar figure invoking the system call handler and executing a system call parameter passing in addition to the system call number most syscalls require that one or more parameters be passed to them somehow user space must relay the parameters to the kernel during the trap the easiest way to do this is via the same means that the syscall number is passed the parameters are stored in registers on the registers ebx ecx edx esi and edi contain in order the first five arguments in the unlikely case of six or more argu ments a single register is used to hold a pointer to user space where all the parameters are stored the return value is sent to user space also via register on it is written into the eax register system call implementation the actual implementation of a system call in linux does not need to be concerned with the behavior of the system call handler thus adding a new system call to linux is rela tively easy the hard work lies in designing and implementing the system call registering it with the kernel is simple let look at the steps involved in writing a new system call for linux implementing system calls the first step in implementing a system call is defining its purpose what will it do the syscall should have exactly one purpose multiplexing syscalls a single system call that does wildly different things depending on a flag argument is discouraged in linux look at ioctl as an example of what not to do what are the new system call arguments return value and error codes the system call should have a clean and simple interface with the smallest number of arguments possi ble the semantics and behavior of a system call are important they must not change because existing applications will come to rely on them be forward thinking consider how the function might change over time can new functionality be added to your system call or will any change require an entirely new function can you easily fix bugs without breaking backward compatibility many system calls provide a flag argument to address forward compatibility the flag is not used to multiplex different behavior across a single system call as mentioned that is not acceptable but to enable new functionality and options without breaking backward compatibility or needing to add a new system call designing the interface with an eye toward the future is important are you needlessly limiting the function design the system call to be as general as possible do not assume its use today will be the same as its use tomorrow the purpose of the system call will remain constant but its uses may change is the system call portable do not make assumptions about an architecture word size or endianness chapter portability dis cusses these issues make sure you are not making poor assumptions that will break the system call in the future remember the unix motto provide mechanism not policy when you write a system call you need to realize the need for portability and robust ness not just today but in the future the basic unix system calls have survived this test of time most of them are just as useful and applicable today as they were years ago verifying the parameters system calls must carefully verify all their parameters to ensure that they are valid and legal the system call runs in kernel space and if the user can pass invalid input into the kernel without restraint the system security and stability can suffer for example file i o syscalls must check whether the file descriptor is valid process related functions must check whether the provided pid is valid every parameter must be checked to ensure it is not just valid and legal but correct processes must not ask the ker nel to access resources to which the process does not have access one of the most important checks is the validity of any pointers that the user pro vides imagine if a process could pass any pointer into the kernel unchecked with warts and all even passing a pointer to which it did not have read access processes could then trick the kernel into copying data for which they did not have access permission such as data belonging to another process or data mapped unreadable before following a pointer into user space the system must ensure that n the pointer points to a region of memory in user space processes must not be able to trick the kernel into reading data in kernel space on their behalf n the pointer points to a region of memory in the process address space the process must not be able to trick the kernel into reading someone else data n if reading the memory is marked readable if writing the memory is marked writable if executing the memory is marked executable the process must not be able to bypass memory access restrictions the kernel provides two methods for performing the requisite checks and the desired copy to and from user space note kernel code must never blindly follow a pointer into user space one of these two methods must always be used for writing into user space the method is provided it takes three parameters the first is the destination memory address in the process address space the second is the source pointer in kernel space finally the third argument is the size in bytes of the data to copy for reading from user space the method is analogous to the function reads from the second parameter into the first parameter the number of bytes specified in the third parameter both of these functions return the number of bytes they failed to copy on error on success they return zero it is standard for the syscall to return efault in the case of such an error let consider an example system call that uses both and this syscall is utterly worthless it copies data from its first parameter into its second this is suboptimal in that it involves an intermediate and extraneous copy into kernel space for no gain but it helps illustrate the point pointless syscall that copies the len bytes from src to dst using the kernel as an intermediary in the copy intended as an example of copying to and from the kernel unsigned long src unsigned long dst unsigned long len unsigned long buf copy src which is in the user address space into buf if buf src len return efault copy buf into dst which is in the user address space if dst buf len return efault return amount of data copied return len both and may block this occurs for example if the page containing the user data is not in physical memory but is swapped to disk in that case the process sleeps until the page fault handler can bring the page from the swap file on disk into physical memory a final possible check is for valid permission in older versions of linux it was stan dard for syscalls that require root privilege to use suser this function merely checked whether a user was root this is now removed and a finer grained capabilities system is in place the new system enables specific access checks on specific resources a call to capable with a valid capabilities flag returns nonzero if the caller holds the specified capability and zero otherwise for example capable checks whether the caller has the ability to modify nice values of other processes by default the superuser possesses all capabilities and nonroot possesses none for example here is the reboot system call note how its first step is ensuring that the calling process has the if that one conditional statement were removed any process could reboot the system reboot int int unsigned int cmd void user arg char buffer we only trust the superuser with rebooting the system if capable return eperm for safety we require magic arguments if linux_reboot_magic2 linux_reboot_magic2b return einval instead of trying to make the code look like halt when is not set do it the easy way if cmd cmd switch cmd case null break case break case break case do_exit break case do_exit break case if buffer arg sizeof buffer return efault buffer sizeof buffer buffer break default return einval return see linux capability h for a list of all capabilities and what rights they entail system call context as discussed in chapter the kernel is in process context during the execution of a sys tem call the current pointer points to the current task which is the process that issued the syscall in process context the kernel is capable of sleeping for example if the system call blocks on a call or explicitly calls schedule and is fully preemptible these two points are important first the capability to sleep means that system calls can make use of the majority of the kernel functionality as we will see in chapter interrupts and interrupt handlers the capability to sleep greatly simplifies kernel programming the fact that process context is preemptible implies that like user space the current task may be preempted by another task because the new task may then execute the same system call care must be exercised to ensure that system calls are reentrant of course this is the same concern that symmetrical multiprocessing introduces synchronizing reentrancy is covered in chapter an introduction to kernel synchronization and chapter kernel synchronization methods when the system call returns control continues in which ultimately switches to user space and continues the execution of the user process final steps in binding a system call after the system call is written it is trivial to register it as an official system call add an entry to the end of the system call table this needs to be done for each architecture that supports the system call which for most calls is all the architec tures the position of the syscall in the table starting at zero is its system call num ber for example the tenth entry in the list is assigned syscall number nine for each supported architecture define the syscall number in asm unistd h compile the syscall into the kernel image as opposed to compiling as a module this can be as simple as putting the system call in a relevant file in kernel such as sys c which is home to miscellaneous system calls look at these steps in more detail with a fictional system call foo first we want to add to the system call table for most architectures the table is located in entry s and looks like this entry long long long long long long long long long interrupt handlers cannot sleep and thus are much more limited in what they can do than system calls running in process context long long long long long long long the new system call is then appended to the tail of this list long although it is not explicitly specified the system call is then given the next subsequent syscall number in this case for each architecture you want to support the system call must be added to the architecture system call table the system call does not need to receive the same syscall number under each architecture as the system call number is part of the architecture unique abi usually you would want to make the system call avail able to each architecture note the convention of placing the number in a comment every five entries this makes it easy to find out which syscall is assigned which number next the system call number is added to asm unistd h which currently looks somewhat like this this file contains the system call numbers define define define define define define define define define define define define define define define define define the following is then added to the end of the list define finally the actual foo system call is implemented because the system call must be compiled into the core kernel image in all configurations in this example we define it in kernel sys c you should put it wherever the function is most relevant for example if the function is related to scheduling you could define it in kernel sched c include asm page h everyone favorite system call returns the size of the per process kernel stack asmlinkage long void return that is it boot this kernel and user space can invoke the foo system call accessing the system call from user space generally the c library provides support for system calls user applications can pull in function prototypes from the standard headers and link with the c library to use your system call or the library routine that in turn uses your syscall call if you just wrote the system call however it is doubtful that glibc already supports it thankfully linux provides a set of macros for wrapping access to system calls it sets up the register contents and issues the trap instructions these macros are named where n is between and the number corresponds to the number of parameters passed into the syscall because the macro needs to know how many parame ters to expect and consequently push into registers for example consider the system call open defined as long open const char filename int flags int mode the syscall macro to use this system call without explicit library support would be define long open const char filename int flags int mode then the application can simply call open for each macro there are n parameters the first parameter corresponds to the return type of the syscall the second is the name of the system call next follows the type and name for each parameter in order of the system call the define is in asm unistd h it is the system call number the macro expands into a c function with inline assembly the assembly performs the steps discussed in the previous section to push the system call number and parameters into the correct registers and issue the software interrupt to trap into the kernel placing this macro in an application is all that is required to use the open system call let write the macro to use our splendid new foo system call and then write some test code to show off our efforts define long foo int main long foo printf the kernel stack size is ld n return why not to implement a system call the previous sections have shown that it is easy to implement a new system call but that in no way should encourage you to do so indeed you should exercise caution and restraint in adding new syscalls often much more viable alternatives to providing a new system call are available let look at the pros cons and alternatives the pros of implementing a new interface as a syscall are as follows n system calls are simple to implement and easy to use n system call performance on linux is fast the cons n you need a syscall number which needs to be officially assigned to you n after the system call is in a stable series kernel it is written in stone the interface cannot change without breaking user space applications n each architecture needs to separately register the system call and support it n system calls are not easily used from scripts and cannot be accessed directly from the filesystem n because you need an assigned syscall number it is hard to maintain and use a sys tem call outside of the master kernel tree n for simple exchanges of information a system call is overkill the alternatives n implement a device node and read and write to it use ioctl to manipu late specific settings or retrieve specific information n certain interfaces such as semaphores can be represented as file descriptors and manipulated as such n add the information as a file to the appropriate location in sysfs for many interfaces system calls are the correct answer linux however has tried to avoid simply adding a system call to support each new abstraction that comes along the result has been an incredibly clean system call layer with few regrets or deprecations interfaces no longer used or supported the slow rate of addition of new system calls is a sign that linux is a relatively stable and feature complete operating system conclusion in this chapter we discussed what system calls are and how they relate to library calls and the application programming interface api we then looked at how the linux kernel implements system calls and the chain of events required to execute a system call trapping into the kernel transmitting the syscall number and any arguments executing the correct system call function and returning to user space with the syscall return value we then went over how to add system calls and provided a simple example of using a new system call from user space the whole process was quite easy as the simplicity of adding a new system call demonstrates the work is all in the syscall implementation the rest of this book discusses concepts and kernel interfaces needed to write well behaved optimal and safe system calls finally we wrapped up the chapter with a discussion on the pros and cons of imple menting system calls and a brief list of the alternatives to adding new ones kernel data structures this chapter introduces several built in data structures for use in linux kernel code as with any large software project the linux kernel provides these generic data structures and primitives to encourage code reuse kernel developers should use these data struc tures whenever possible and not roll your own solutions in the following sections we cover the most useful of these generic data structures which are the following n linked lists n queues n maps n binary trees we conclude the chapter with a discussion on algorithmic complexity the ease with which algorithms and data structures scale to support ever larger inputs linked lists the linked list is the simplest and most common data structure in the linux kernel a linked list is a data structure that allows the storage and manipulation of a variable number of elements called the nodes of the list unlike in a static array the elements in a linked list are dynamically created and inserted into the list this enables the management of a vary ing number of elements unknown at compile time because the elements are created at different times they do not necessarily occupy contiguous regions in memory therefore the elements need to be linked together thus each element in the list contains a pointer to the next element as elements are added to or removed from the list the pointer to the next node is simply adjusted singly and doubly linked lists the simplest data structure representing such a linked list might look similar to the following an element in a linked list struct void data the payload struct next pointer to the next element figure is a linked list figure a singly linked list in some linked lists each element also contains a pointer to the previous element these lists are called doubly linked lists because they are linked both forward and backward linked lists such as the list in figure that do not have a pointer to the previous ele ment are called singly linked lists a data structure representing a doubly linked list would look similar to this an element in a linked list struct void data the payload struct next pointer to the next element struct prev pointer to the previous element figure is a doubly linked list null prev next prev next prev next figure a doubly linked list circular linked lists normally because the last element in a linked list has no next element it is set to point to a special value such as null to indicate it is the last element in the list in some linked lists the last element does not point to a special value instead it points back to the first value this linked list is called a circular linked list because the list is cyclic circular linked lists can come in both doubly and singly linked versions in a circular doubly linked list the first node previous pointer points at the last node figures and are singly and doubly circular linked lists respectively figure a circular singly linked list prev next prev next prev next figure a circular doubly linked list although the linux kernel linked list implementation is unique it is fundamentally a circular doubly linked list using this type of linked list provides the greatest flexibility moving through a linked list movement through a linked list occurs linearly you visit one element follow the next pointer and visit the next element rinse and repeat this is the easiest method of moving through a linked list and the one for which linked lists are best suited linked lists are ill suited for use cases where random access is an important operation instead you use linked lists when iterating over the whole list is important and the dynamic addition and removal of elements is required in linked list implementations the first element is often represented by a special pointer called the head that enables easy access to the start of the list in a noncircu lar linked list the last element is delineated by its next pointer being null in a circular linked list the last element is delineated because it points to the head element traversing the list therefore occurs linearly through each element from the first to the last in a dou bly linked list movement can also occur backward linearly from the last element to the first of course given a specific element in the list you can iterate backward and forward any number of elements too you need not traverse the whole list the linux kernel implementation in comparison to most linked list implementations including the generic approach described in the previous sections the linux kernel implementation is unique recall from the earlier discussion that data or a grouping of data such as a struct is maintained in a linked list by adding a next and perhaps a previous node pointer to the data for example assume we had a fox structure to describe that member of the canidae family struct fox unsigned long length in centimeters of tail unsigned long weight weight in kilograms bool is this fox fantastic the common pattern for storing this structure in a linked list is to embed the list pointer in the structure for example struct fox unsigned long length in centimeters of tail unsigned long weight weight in kilograms bool is this fox fantastic struct fox next next fox in linked list struct fox prev previous fox in linked list the linux kernel approach is different instead of turning the structure into a linked list the linux approach is to embed a linked list node in the structure the linked list structure in the old days there were multiple implementations of linked lists in the kernel a single powerful linked list implementation was needed to remove duplicate code during the kernel development series the official kernel linked list implementation was intro duced all existing uses of linked lists now use the official implementation do not rein vent the wheel the linked list code is declared in the header file linux list h and the data struc ture is simple struct struct next struct prev the next pointer points to the next list node and the prev pointer points to the pre vious list node yet seemingly this is not particularly useful what value is a giant linked list of linked list nodes the utility is in how the structure is used struct fox unsigned long length in centimeters of tail unsigned long weight weight in kilograms bool is this fox fantastic struct list list of all fox structures with this list next in fox points to the next element and list prev in fox points to the previous now this is becoming useful but it gets better the kernel provides a family of routines to manipulate linked lists for example the method adds a new node to an existing linked list these methods however are generic they accept only structures using the macro we can easily find the par ent structure containing any given member variable this is because in c the offset of a given variable into a structure is fixed by the abi at compile time define ptr type member const typeof type member mptr ptr type char mptr offsetof type member using we can define a simple function to return the parent structure containing any define ptr type member ptr type member armed with the kernel provides routines to create manipulate and otherwise manage linked lists all without knowing anything about the structures that the resides within defining a linked list as shown a by itself is worthless it is normally embedded inside your own structure struct fox unsigned long length in centimeters of tail unsigned long weight weight in kilograms bool is this fox fantastic struct list list of all fox structures the list needs to be initialized before it can be used because most of the elements are created dynamically probably why you need a linked list the most common way of ini tializing the linked list is at runtime struct fox kmalloc sizeof weight false list if the structure is statically created at compile time and you have a direct reference to it you can simply do this struct fox tail_length weight list red_fox list list heads the previous section shows how easy it is to take an existing structure such as our struct fox example and turn it into a linked list with simple code changes our struc ture is now manageable by the kernel linked list routines but before we can use those routines we need a canonical pointer to refer to the list as a whole a head pointer one nice aspect of the kernel linked list implementation is that our fox nodes are indistinguishable each contains a and we can iterate from any one node to the next until we have seen every node this approach is elegant but you will generally want a special pointer that refers to your linked list without being a list node itself inter estingly this special node is in fact a normal static this defines and initializes a named the majority of the linked list routines accept one or two parameters the head node or the head node plus an actual list node let look at those routines manipulating linked lists the kernel provides a family of functions to manipulate linked lists they all take point ers to one or more structures the functions are implemented as inline func tions in generic c and can be found in linux list h interestingly all these functions are o this means they execute in constant time regardless of the size of the list or any other inputs for example it takes the same amount of time to add or remove an entry to or from a list whether that list has or entries this is perhaps not surprising but still good to know adding a node to a linked list to add a node to a linked list struct new struct head see the section algorithmic complexity later in this chapter for a discussion on o this function adds the new node to the given list immediately after the head node because the list is circular and generally has no concept of first or last nodes you can pass any element for head if you do pass the last element however this function can be used to implement a stack returning to our fox example assume we had a new struct fox that we wanted to add to the list we d do this f list to add a node to the end of a linked list struct new struct head this function adds the new node to the given list immediately before the head node as with because the lists are circular you can generally pass any element for head this function can be used to implement a queue however if you pass the first element deleting a node from a linked list after adding a node to a linked list deleting a node from a list is the next most important operation to delete a node from a linked list use struct entry this function removes the element entry from the list note that it does not free any memory belonging to entry or the data structure in which it is embedded this function merely removes the element from the list after calling this you would typically destroy your data structure and the inside it for example to delete the fox node we previous added to f list note the function does not receive as input it simply receives a specific node and modifies the pointers of the previous and subsequent nodes such that the given node is no longer part of the list the implementation is instructive static inline void struct prev struct next next prev prev prev next next static inline void struct entry entry prev entry next to delete a node from a linked list and reinitialize it the kernel provides struct entry this function behaves the same as except it also reinitializes the given with the rationale that you no longer want the entry in the list but you can reuse the data structure itself moving and splicing linked list nodes to move a node from one list to another struct list struct head this function removes the list entry from its linked list and adds it to the given list after the head element to move a node from one list to the end of another struct list struct head this function does the same as but inserts the list element before the head entry to check whether a list is empty struct head this returns nonzero if the given list is empty otherwise it returns zero to splice two unconnected lists together struct list struct head this function splices together two lists by inserting the list pointed to by list to the given list after the element head to splice two unconnected lists together and reinitialize the old list struct list struct head this function works the same as except that the emptied list pointed to by list is reinitialized saving a couple dereferences if you happen to already have the next and prev pointers available you can save a couple cycles specifically the dereferences to get the pointers by calling the internal list functions directly every previously discussed function actually does nothing except find the next and prev pointers and then call the internal functions the internal functions generally have the same name as their wrappers except they are prefixed by double underscores for exam ple rather than call list you can call prev next this is useful only if the next and previous pointers are already dereferenced otherwise you are just writing ugly code see the header linux list h for the exact interfaces traversing linked lists now you know how to declare initialize and manipulate a linked list in the kernel this is all very well and good but it is meaningless if you have no way to access your data the linked lists are just containers that hold your important data you need a way to use lists to move around and access the actual structures that contain the data the kernel thank goodness provides a nice set of interfaces for traversing linked lists and referencing the data structures that include them note that unlike the list manipulation routines iterating over a linked list in its entirety is clearly an o n operation for n entries in the list the basic approach the most basic way to iterate over a list is with the macro the macro takes two parameters both structures the first is a pointer used to point to the current entry it is a temporary variable that you must provide the second is the acting as the head node of the list you want to traverse see the earlier section list heads on each iteration of the loop the first parameter points to the next entry in the list until each entry has been visited usage is as follows struct p p p points to an entry in the list well that is still worthless a pointer to the list structure is usually no good what we need is a pointer to the structure that contains the for example with the pre vious fox structure example we want a pointer to each fox not a pointer to the list member in the structure we can use the macro which we discussed ear lier to retrieve the structure that contains a given for example struct p struct fox f p f points to the structure in which the list is embedded f p struct fox list the usable approach the previous approach does not make for particularly intuitive or elegant code although it does illustrate how nodes function consequently most kernel code uses the macro to iterate over a linked list this macro handles the work performed by making list iteration simple pos head member here pos is a pointer to the object containing the nodes think of it as the return value from head is a pointer to the head node from which you want to start iterating in our previous example member is the vari able name of the structure in pos list in our example this sounds confus ing but it is easy to use here is how we would rewrite the previous to iterate over every fox node struct fox f f list on each iteration f points to the next fox structure now let look at a real example from inotify the kernel filesystem notification system static struct struct inode inode struct ih struct watch watch inode if watch ih ih return watch return null this function iterates over all the entries in the inode list each entry is of type struct and the in that structure is named with each iteration of the loop watch points at a new node in the list the purpose of this simple function is to search the list in the provided inode struc ture to find an entry whose matches the provided handle iterating through a list backward the macro works just like except that it moves through the list in reverse that is instead of following the next pointers forward through the list it follows the prev pointers backward usage is the same as with pos head member there are only a handful of reasons to favor moving through a list in reverse one is performance if you know the item you are searching for is likely behind the node you are starting your search from you can move backward in hopes of finding it sooner a second reason is if ordering is important for example if you use a linked list as a stack you can walk the list from the tail backward to achieve last in first out lifo ordering if you do not have an explicit reason to move through the list in reverse don t just use iterating while removing the standard list iteration methods are not appropriate if you are removing entries from the list as you iterate the standard methods rely on the fact that the list entries are not changing out from under them and thus if the current entry is removed in the body of the loop the subsequent iteration cannot advance to the next or previous pointer this is a common pattern in loops and programmers solve it by storing the next or previous pointer in a temporary variable prior to a potential removal operation the linux kernel provides a routine to handle this situation for you pos next head member you use this version in the same manner as except that you provide the next pointer which is of the same type as pos the next pointer is used by the macro to store the next entry in the list making it safe to remove the current entry let consider an example again in inotify void struct inode inode struct watch next inode watch next inode struct ih watch ih ih mutex ih watch deletes watch ih mutex inode this function iterates over and removes all the entries in the list if the standard were used this code would introduce a use after free bug as moving to the next item in the list would require accessing watch which was destroyed if you need to iterate over a linked list in reverse and potentially remove elements the kernel provides pos n head member usage is the same as with you may still need locking the safe variants of protect you only from removals from the list within the body of the loop if there is a chance of concurrent removals from other code or any other form of concurrent list manipulation you need to properly lock access to the list see chapters an introduction to kernel synchronization and chapter kernel syn chronization methods for a discussion on synchronization and locking other linked list methods linux provides myriad other list methods enabling seemingly every conceivable way to access and manipulate a linked list all these methods are defined in the header file linux list h queues a common programming pattern in any operating system kernel is producer and consumer in this pattern a producer creates data say error messages to be read or networking packets to be processed while a consumer in turn reads processes or otherwise consumes the data often the easiest way to implement this pattern is with a queue the producer pushes data onto the queue and the consumer pulls data off the queue the consumer retrieves the data in the order it was enqueued that is the first data on the queue is the first data off the queue for this reason queues are also called fifos short for first in first out see figure for an example of a standard queue figure a queue fifo the linux kernel generic queue implementation is called kfifo and is implemented in kernel kfifo c and declared in linux kfifo h this section discusses the api after an update in usage is slightly different in kernel versions prior to double check linux kfifo h before writing code kfifo linux kfifo works like most other queue abstractions providing two primary operations enqueue unfortunately named in and dequeue out the kfifo object maintains two off sets into the queue an in offset and an out offset the in offset is the location in the queue to which the next enqueue will occur the out offset is the location in the queue from which the next dequeue will occur the out offset is always less than or equal to the in offset it wouldn t make sense for it to be greater otherwise you could dequeue data that had not yet been enqueued the enqueue in operation copies data into the queue starting at the in offset when complete the in offset is incremented by the amount of data enqueued the dequeue out operation copies data out of the queue starting from the out offset when complete the out offset is incremented by the amount of data enqueued when the out offset is equal to the in offset the queue is empty no more data can be dequeued until more data is enqueued when the in offset is equal to the length of the queue no more data can be enqueued until the queue is reset creating a queue to use a kfifo you must first define and initialize it as with most kernel objects you can do this dynamically or statically the most common method is dynamic int struct kfifo fifo unsigned int size this function creates and initializes a kfifo with a queue of size bytes the kernel uses the gfp mask to allocate the queue we discuss memory allocations in chapter memory management upon success returns zero on error it returns a negative error code following is a simple example struct kfifo fifo int ret ret kifo if ret return ret fifo now represents a sized queue if you want to allocate the buffer yourself you can void struct kfifo fifo void buffer unsigned int size this function creates and initializes a kfifo that will use the size bytes of memory pointed at by buffer for its queue with both and size must be a power of two statically declaring a kfifo is simpler but less common name size name this creates a static kfifo named name with a queue of size bytes as before size must be a power of enqueuing data when your kfifo is created and initialized enqueuing data into the queue is performed via the function unsigned int struct kfifo fifo const void from unsigned int len this function copies the len bytes starting at from into the queue represented by fifo on success it returns the number of bytes enqueued if less than len bytes are free in the queue the function copies only up to the amount of available bytes thus the return value can be less than len or even zero if nothing was copied dequeuing data when you add data to a queue with you can remove it with unsigned int struct kfifo fifo void to unsigned int len this function copies at most len bytes from the queue pointed at by fifo to the buffer pointed at by to on success the function returns the number of bytes copied if less than len bytes are in the queue the function copies less than requested when dequeued data is no longer accessible from the queue this is the normal usage of a queue but if you want to peek at data within the queue without removing it you can use unsigned int struct kfifo fifo void to unsigned int len unsigned offset this works the same as except that the out offset is not incremented and thus the dequeued data is available to read on a subsequent call to the parameter offset specifies an index into the queue specify zero to read from the head of the queue as does obtaining the size of a queue to obtain the total size in bytes of the buffer used to store a kfifo queue call static inline unsigned int struct kfifo fifo in another example of horrible kernel naming use to obtain the number of bytes enqueued in a kfifo static inline unsigned int struct kfifo fifo to find out the number of bytes available to write into a kfifo call static inline unsigned int struct kfifo fifo finally and return nonzero if the given kfifo is empty or full respectively and zero if not static inline int struct kfifo fifo static inline int struct kfifo fifo resetting and destroying the queue to reset a kfifo jettisoning all the contents of the queue call static inline void struct kfifo fifo to destroy a kfifo allocated with call void struct kfifo fifo if you created your kfifo with it is your responsibility to free the asso ciated buffer how you do so depends on how you created it see chapter for a dis cussion on allocating and freeing dynamic memory example queue usage with these interfaces under our belt let take a look at a simple example of using a kfifo assume we created a kfifo pointed at by fifo with a queue size of we can now enqueue data onto the queue in this example we enqueue simple integers in your own code you will likely enqueue more complicated task specific structures using integers in this example let see exactly how the kfifo works unsigned int i enqueue to the kfifo named fifo for i i i fifo i sizeof i the kfifo named fifo now contains through inclusive we can take a peek at the first item in the queue and verify it is unsigned int val int ret ret fifo val sizeof val if ret sizeof val return einval printk u n val should print to dequeue and print all the items in the kfifo we can use while there is data in the queue while fifo unsigned int val int ret read it one integer at a time ret fifo val sizeof val if ret sizeof val return einval printk u n val this prints through inclusive and in that order if this code snippet printed the numbers backward from to we would have a stack not a queue maps a map also known as an associative array is a collection of unique keys where each key is associated with a specific value the relationship between a key and its value is called a mapping maps support at least three operations n add key value n remove key n value lookup key although a hash table is a type of map not all maps are implemented via hashes instead of a hash table maps can also use a self balancing binary search tree to store their data although a hash offers better average case asymptotic complexity see the section algorithmic complexity later in this chapter a binary search tree has better worst case behavior logarithmic versus linear a binary search tree also enables order preservation enabling users to efficiently iterate over the entire collection in a sorted order finally a binary search tree does not require a hash function instead any key type is suitable so long as it can define the operator although the general term for all collections mapping a key to a value the name maps often refers specifically to an associated array implemented using a binary search tree as opposed to a hash table for example the c stl container std map is implemented using a self balancing binary search tree or similar data structure because it provides the ability to in order traverse the collection the linux kernel provides a simple and efficient map data structure but it is not a general purpose map instead it is designed for one specific use case mapping a unique identification number uid to a pointer in addition to providing the three main map operations linux implementation also piggybacks an allocate operation on top of the add operation this allocate operation not only adds a uid value pair to the map but also generates the uid the idr data structure is used for mapping user space uids such as inotify watch descriptors or posix timer ids to their associated kernel data structure such as the or structures respectively following the linux kernel scheme of obfuscated confusing names this map is called idr initializing an idr setting up an idr is easy first you statically define or dynamically allocate an idr struc ture then you call void struct idr idp for example struct idr statically define idr structure initialize provided idr structure allocating a new uid once you have an idr set up you can allocate a new uid which is a two step process first you tell the idr that you want to allocate a new uid allowing it to resize the back ing tree as necessary then with a second call you actually request the new uid this complication exists to allow you to perform the initial resizing which may require a memory allocation without a lock we discuss memory allocations in chapter and locking in chapters and for now let concentrate on using idr without concern to how we handle locking the first function to resize the backing tree is int struct idr idp this function will if needed to fulfill a new uid allocation resize the idr pointed at by idp if a resize is needed the memory allocation will use the gfp flags gfp flags are discussed in chapter you do not need to synchronize concurrent access to this call inverted from nearly every other function in the kernel returns one on success and zero on error be careful the second function to actually obtain a new uid and add it to the idr is int struct idr idp void ptr int id this function uses the idr pointed at by idp to allocate a new uid and associate it with the pointer ptr on success the function returns zero and stores the new uid in id on error it returns a nonzero error code eagain if you need to again call and enospc if the idr is full let look at a full example int id do if gfp_kernel return enospc ret ptr id while ret eagain if successful this snippet obtains a new uid which is stored in the integer id and maps that uid to ptr which we don t define in the snippet the function enables the caller to specify a minimum uid value to return int struct idr idp void ptr int int id this works the same as except that the new uid is guaranteed to be equal to or greater than using this variant of the function allows idr users to ensure that a uid is never reused allowing the value to be unique not only among currently allocated ids but across the entirety of a system uptime this code snippet is the same as our previous example except that we request strictly increasing uid values int id do if gfp_kernel return enospc ret ptr id while ret eagain if ret id looking up a uid when we have allocated some number of uids in an idr we can look them up the caller provides the uid and the idr returns the associated pointer this is accomplished in a much simpler manner than allocating a new uid with the function void struct idr idp int id a successful call to this function returns the pointer associated with the uid id in the idr pointed at by idp on error the function returns null note if you mapped null to a uid with or this function successfully returns null giving you no way to distinguish success from failure consequently you should not map uids to null usage is simple struct ptr id if ptr return einval error removing a uid to remove a uid from an idr use void struct idr idp int id a successful call to removes the uid id from the idr pointed at by idp unfortunately has no way to signify error for example if id is not in idp destroying an idr destroying an idr is a simple affair accomplished with the function void struct idr idp a successful call to deallocates only unused memory associated with the idr pointed at by idp it does not free any memory currently in use by allocated uids generally kernel code wouldn t destroy its idr facility until it was shutting down or unloading and it wouldn t unload until it had no more users and thus no more uids but to force the removal of all uids you can call void struct idr idp you would call on the idr pointed at by idp before calling ensuring that all idr memory was freed binary trees a tree is a data structure that provides a hierarchical tree like structure of data mathemati cally it is an acyclic connected directed graph in which each vertex called a node has zero or more outgoing edges and zero or one incoming edges a binary tree is a tree in which nodes have at most two outgoing edges that is a tree in which nodes have zero one or two children see figure for a sample binary tree figure a binary tree binary search trees a binary search tree often abbreviated bst is a binary tree with a specific ordering imposed on its nodes the ordering is often defined via the following induction n the left subtree of the root contains only nodes with values less than the root n the right subtree of the root contains only nodes with values greater than the root n all subtrees are also binary search trees a binary search tree is thus a binary tree in which all nodes are ordered such that left children are less than their parent in value and right children are greater than their parent consequently both searching for a given node and in order traversal are efficient loga rithmic and linear respectively see figure for a sample binary search tree figure a binary search tree bst self balancing binary search trees the depth of a node is measured by how many parent nodes it is from the root nodes at the bottom of the tree those with no children are called leaves the height of a tree is the depth of the deepest node in the tree a balanced binary search tree is a binary search tree in which the depth of all leaves differs by at most one see figure a self balancing binary search tree is a binary search tree that attempts as part of its normal operations to remain semi balanced figure a balanced binary search tree red black trees a red black tree is a type of self balancing binary search tree linux primary binary tree data structure is the red black tree red black trees have a special color attribute which is either red or black red black trees remain semi balanced by enforcing that the following six properties remain true all nodes are either red or black leaf nodes are black leaf nodes do not contain data all non leaf nodes have two children if a node is red both of its children are black the path from a node to one of its leaves contains the same number of black nodes as the shortest path to any of its other leaves taken together these properties ensure that the deepest leaf has a depth of no more than double that of the shallowest leaf consequently the tree is always semi balanced why this is true is surprisingly simple first by property five a red node cannot be the child or parent of another red node by property six all paths through the tree to its leaves have the same number of black nodes the longest path through the tree alternates red and black nodes thus the shortest path which must have the same number of black nodes contains only black nodes therefore the longest path from the root to a leaf is no more than double the shortest path from the root to any other leaf if the insertion and removal operations enforce these six properties the tree remains semi balanced now it might seem odd to require insert and remove to maintain these particular properties why not implement the operations such that they enforce other simpler rules that result in a balanced tree it turns out that these properties are relatively easy to enforce although complex to implement allowing insert and remove to guaran tee a semi balanced tree without burdensome extra overhead describing how insert and remove enforce these rules is beyond the scope of this book although simple rules the implementation is complex any good undergraduate level data structures textbook ought to give a full treatment rbtrees the linux implementation of red black trees is called rbtrees they are defined in lib rbtree c and declared in linux rbtree h aside from optimizations linux rbtrees resemble the classic red black tree as described in the previous section they remain balanced such that inserts are always logarithmic with respect to the number of nodes in the tree the root of an rbtree is represented by the structure to create a new tree we allocate a new and initialize it to the special value struct root individual nodes in an rbtree are represented by the structure given an we can move to its left or right child by following pointers off the node of the same name the rbtree implementation does not provide search and insert routines users of rbtrees are expected to define their own this is because c does not make generic pro gramming easy and the linux kernel developers believed the most efficient way to imple ment search and insert was to require each user to do so manually using provided rbtree helper functions but their own comparison operators the best way to demonstrate search and insert is to show a real world example first let look at search the following function implements a search of linux page cache for a chunk of a file represented by an inode and offset pair each inode has its own rbtree keyed off of page offsets into file this function thus searches the given inode rbtree for a matching offset value struct page struct inode inode unsigned long offset struct n inode while n struct page page n struct page if offset page offset n n else if offset page offset n n else return page return null in this example the while loop iterates over the rbtree traversing as needed to the left or right child in the direction of the given offset the if and else statements implement the rbtree comparison function thus enforcing the tree ordering if the loop finds a node with a matching offset the search is complete and the function returns the associ ated page structure if the loop reaches the end of the rbtree without finding a match one does not exist in the tree and the function returns null insert is even more complicated because it implements both search and insertion logic the following isn t a trivial function but if you need to implement your own insert rou tine this is a good guide struct page struct inode inode unsigned long offset struct node struct p inode struct parent null struct page page while p parent p page parent struct page if offset page offset p p else if offset page offset p p else return page node parent p node inode return null as with our search function the while loop is iterating over the tree moving in the direction of the provided offset unlike with search however the function is hoping not to find a matching offset but instead reach the leaf node that is the correct insertion point for the new offset when the insertion point is found is called to insert the new node at the given spot is then called to perform the complicated rebalancing dance the function returns null if the page was added to the page cache and the address of an existing page structure if the page is already in the cache what data structure to use when thus far we ve discussed four of linux most important data structures linked lists queues maps and red black trees in this section we cover some tips to help you decide which data structure to use in your own code if your primary access method is iterating over all your data use a linked list intuitively no data structure can provide better than linear complexity when visiting every element so you should favor the simplest data structure for that simple job also consider linked lists when performance is not important when you need to store a relatively small number of items or when you need to interface with other kernel code that uses linked lists if your code follows the producer consumer pattern use a queue particularly if you want or can cope with a fixed size buffer queues make adding and removing items simple and efficient and they provide first in first out fifo semantics which is what most producer consumer use cases demand on the other hand if you need to store an unknown potentially large number of items a linked list may make more sense because you can dynamically add any number of items to the list if you need to map a uid to an object use a map maps make such mappings easy and efficient and they also maintain and allocate the uid for you linux map interface being specific to uid to pointer mappings isn t good for much else however if you are dealing with descriptors handed out to user space consider this option if you need to store a large amount of data and look it up efficiently consider a red black tree red black trees enable the searching in logarithmic time while still providing an efficient linear time in order traversal although more complicated to implement than the other data structures their in memory footprint isn t significantly worse if you are not performing many time critical look up operations a red black tree probably isn t your best bet in that case favor a linked list none of these data structures fit your needs the kernel implements other seldom used data structures that might meet your needs such as radix trees a type of trie and bitmaps only after exhausting all kernel provided solutions should you consider rolling your own data structure one common data structure often implemented in individual source files is the hash table because a hash table is little more than some buckets and a hash function and the hash function is so specific to each use case there is little value in providing a kernelwide solution in a nongeneric programming language such as c algorithmic complexity algorithmic complexity often in computer science and related disciplines it is useful to express the algorithmic complexity or scalability of algorithms quantitatively various methods exist for repre senting scalability one common technique is to study the asymptotic behavior of the algo rithm this is the behavior of the algorithm because its inputs grow exceedingly large and approach infinity asymptotic behavior shows how well an algorithm scales as its input grows larger and larger studying an algorithm scalability how it performs as the size of its input increases enables us to model the algorithm against a benchmark and better understand its behavior algorithms an algorithm is a series of instructions possibly one or more inputs and ultimately a result or output for example the steps carried out to count the number of people in a room are an algorithm with the people being the input and the count being the output in the linux kernel both page eviction and the process scheduler are examples of algo rithms mathematically an algorithm is like a function or at least you can model it as one for example if you call the people counting algorithm f and the number of people to count x you can write y f x people counting function where y is the time required to count the x people big o notation one useful asymptotic notation is the upper bound which is a function whose value after an initial point is always greater than the value of the function that you are studying it is said that the upper bound grows as fast or faster than the function in question a special notation big o pronounced big oh notation is used to describe this growth it is written f x is o g x and is read as f is big oh of g the formal mathematical definition is if f x is o g x then c x such that f x c g x x x in english the time to complete f x is always less than or equal to the time to com plete g x multiplied by some arbitrary constant so long as the input x is larger than some initial value x essentially you are looking for a function whose behavior is as bad as or worse than the algorithm you can then look at the result of large inputs to this function and obtain an understanding of the bound of your algorithm big theta notation when most people talk about big o notation they are more accurately referring to what donald knuth describes as big theta notation technically big o notation refers to an upper bound for example is an upper bound of so are and subsequently when most people discuss function growth they talk about the least upper bound or a function that models both the upper and lower bounds professor knuth the father of the field of algorithmic analysis describes this as big theta notation and gives the following definition if f x is big theta of g x then g x is both an upper bound and a lower bound for f x then you can say that f x is of order g x the order or big theta of an algorithm is one of the most important mathematical tools for understanding algorithms in the kernel consequently when people refer to big o notation they are more often talking about the least such big o the big theta you really do not have to worry about this unless you want to make professor knuth really happy time complexity consider the original example of having to count the number of people in a room pre tend you can count one person per second then if there are people in the room it will take seconds to count them more generally given n people it will take n seconds to count everyone thus you can say this algorithm is o n what if the task was to dance in front of everyone in the room because it would take the same amount of time to dance whether there were or people in the room this task is o see table for other common complexities table table of common time complexity values o g x name constant perfect scalability log n logarithmic n linear quadratic cubic exponential n factorial if you re curious the lower bound is modeled by big omega notation the definition is the same as big o except g x is always less than or equal to f x not greater than or equal to big omega notation is less useful than big o because finding functions smaller than your function is rarely indicative of behavior conclusion what is the complexity of introducing everyone in the room to everyone else what is a possible function that models this algorithm if it took seconds to introduce each person how long would it take to introduce people to each other what about people to each other understanding how an algorithm performs as it has ever more work to do is a crucial component in determining the best algorithm for a given job of course it is wise to avoid complexities such as o n or o likewise it is usu ally an improvement to replace an o n algorithm with a functionally equivalent o log n algorithm this is not always the case however and a blind assumption should not be made based solely on big o notation recall that given o g x there is a constant c multiplied by g x therefore it is possible that an o algorithm takes hours to com plete sure it is always hours regardless of how large the input but that can still be a long time compared to an o n algorithm with few inputs the typical input size should always be taken into account when comparing algorithms favor less complex algorithms but keep in mind the overhead of the algorithm in relation to the typical input size do not blindly optimize to a level of scalability you will never need to support conclusion in this chapter we discussed many of the generic data structures that linux kernel developers use to implement everything from the process scheduler to device drivers you will find these data structures useful as we continue our study of the linux kernel when writing your own kernel code always reuse existing kernel infrastructure and don t reinvent the wheel we also covered algorithmic complexity and tools for measuring and expressing it the most notable being big o notation throughout this book and the linux kernel big o notation is an important notion of how well algorithms and kernel components scale in light of many users processes processors network connections and other ever expanding inputs interrupts and interrupt handlers a core responsibility of any operating system kernel is managing the hardware con nected to the machine hard drives and blu ray discs keyboards and mice processors and wireless radios to meet this responsibility the kernel needs to communicate with the machine individual devices given that processors can be orders of magnitudes faster than the hardware they talk to it is not ideal for the kernel to issue a request and wait for a response from the significantly slower hardware instead because the hardware is com paratively slow to respond the kernel must be free to go and handle other work dealing with the hardware only after that hardware has actually completed its work how can the processor work with hardware without impacting the machine overall performance one answer to this question is polling periodically the kernel can check the status of the hardware in the system and respond accordingly polling incurs overhead however because it must occur repeatedly regardless of whether the hardware is active or ready a better solution is to provide a mechanism for the hardware to signal to the kernel when attention is needed this mechanism is called an interrupt in this chapter we discuss interrupts and how the kernel responds to them with special functions called interrupt handlers interrupts interrupts enable hardware to signal to the processor for example as you type the key board controller the hardware device that manages the keyboard issues an electrical sig nal to the processor to alert the operating system to newly available key presses these electrical signals are interrupts the processor receives the interrupt and signals the oper ating system to enable the operating system to respond to the new data hardware devices generate interrupts asynchronously with respect to the processor clock they can occur at any time consequently the kernel can be interrupted at any time to process interrupts an interrupt is physically produced by electronic signals originating from hardware devices and directed into input pins on an interrupt controller a simple chip that multi plexes multiple interrupt lines into a single line to the processor upon receiving an inter rupt the interrupt controller sends a signal to the processor the processor detects this sig nal and interrupts its current execution to handle the interrupt the processor can then notify the operating system that an interrupt has occurred and the operating system can handle the interrupt appropriately different devices can be associated with different interrupts by means of a unique value associated with each interrupt this way interrupts from the keyboard are distinct from interrupts from the hard drive this enables the operating system to differentiate between interrupts and to know which hardware device caused which interrupt in turn the operating system can service each interrupt with its corresponding handler these interrupt values are often called interrupt request irq lines each irq line is assigned a numeric value for example on the classic pc irq zero is the timer inter rupt and irq one is the keyboard interrupt not all interrupt numbers however are so rigidly defined interrupts associated with devices on the pci bus for example generally are dynamically assigned other non pc architectures have similar dynamic assignments for interrupt values the important notion is that a specific interrupt is associated with a specific device and the kernel knows this the hardware then issues interrupts to get the kernel attention hey i have new key presses waiting read and process these bad boys exceptions in os texts exceptions are often discussed at the same time as interrupts unlike inter rupts exceptions occur synchronously with respect to the processor clock indeed they are often called synchronous interrupts exceptions are produced by the processor while execut ing instructions either in response to a programming error for example divide by zero or abnormal conditions that must be handled by the kernel for example a page fault because many processor architectures handle exceptions in a similar manner to interrupts the kernel infrastructure for handling the two is similar much of the discussion of interrupts asynchronous interrupts generated by hardware in this chapter also pertains to exceptions synchronous interrupts generated by the processor you are already familiar with one exception in the previous chapter you saw how system calls on the architecture are implemented by the issuance of a software interrupt which traps into the kernel and causes execution of a special system call handler inter rupts work in a similar way you will see except hardware not software issues interrupts interrupt handlers the function the kernel runs in response to a specific interrupt is called an interrupt handler or interrupt service routine isr each device that generates interrupts has an associated interrupt handler for example one function handles interrupts from the system timer whereas another function handles interrupts generated by the keyboard the interrupt handler for a device is part of the device driver the kernel code that manages the device in linux interrupt handlers are normal c functions they match a specific prototype which enables the kernel to pass the handler information in a standard way but otherwise top halves versus bottom halves they are ordinary functions what differentiates interrupt handlers from other kernel func tions is that the kernel invokes them in response to interrupts and that they run in a spe cial context discussed later in this chapter called interrupt context this special context is occasionally called atomic context because as we shall see code executing in this context is unable to block in this book we will use the term interrupt context because an interrupt can occur at any time an interrupt handler can in turn be exe cuted at any time it is imperative that the handler runs quickly to resume execution of the interrupted code as soon as possible therefore while it is important to the hardware that the operating system services the interrupt without delay it is also important to the rest of the system that the interrupt handler executes in as short a period as possible at the very least an interrupt handler job is to acknowledge the interrupt receipt to the hardware hey hardware i hear ya now get back to work often however interrupt han dlers have a large amount of work to perform for example consider the interrupt handler for a network device on top of responding to the hardware the interrupt handler needs to copy networking packets from the hardware into memory process them and push the packets down to the appropriate protocol stack or application obviously this can be a lot of work especially with today gigabit and gigabit ethernet cards top halves versus bottom halves these two goals that an interrupt handler execute quickly and perform a large amount of work clearly conflict with one another because of these competing goals the pro cessing of interrupts is split into two parts or halves the interrupt handler is the top half the top half is run immediately upon receipt of the interrupt and performs only the work that is time critical such as acknowledging receipt of the interrupt or resetting the hardware work that can be performed later is deferred until the bottom half the bottom half runs in the future at a more convenient time with all interrupts enabled linux pro vides various mechanisms for implementing bottom halves and they are all discussed in chapter bottom halves and deferring work let look at an example of the top half bottom half dichotomy using our old friend the network card when network cards receive packets from the network they need to alert the kernel of their availability they want and need to do this immediately to opti mize network throughput and latency and avoid timeouts thus they immediately issue an interrupt hey kernel i have some fresh packets here the kernel responds by executing the network card registered interrupt the interrupt runs acknowledges the hardware copies the new networking packets into main memory and readies the network card for more packets these jobs are the important time critical and hardware specific work the kernel generally needs to quickly copy the networking packet into main memory because the network data buffer on the networking card is fixed and miniscule in size particularly compared to main memory delays in copying the packets can result in a buffer overrun with incoming packets overwhelming the networking card buffer and thus packets being dropped after the networking data is safely in the main memory the interrupt job is done and it can return control of the system to whatever code was interrupted when the interrupt was generated the rest of the processing and handling of the packets occurs later in the bot tom half in this chapter we look at the top half in the next chapter we study the bottom registering an interrupt handler interrupt handlers are the responsibility of the driver managing the hardware each device has one associated driver and if that device uses interrupts and most do then that driver must register one interrupt handler drivers can register an interrupt handler and enable a given interrupt line for handling with the function which is declared in linux interrupt h allocate a given interrupt line int unsigned int irq handler unsigned long flags const char name void dev the first parameter irq specifies the interrupt number to allocate for some devices for example legacy pc devices such as the system timer or keyboard this value is typically hard coded for most other devices it is probed or otherwise determined programmati cally and dynamically the second parameter handler is a function pointer to the actual interrupt handler that services this interrupt this function is invoked whenever the operating system receives the interrupt typedef int void note the specific prototype of the handler function it takes two parameters and has a return value of this function is discussed later in this chapter interrupt handler flags the third parameter flags can be either zero or a bit mask of one or more of the flags defined in linux interrupt h among these flags the most important are n when set this flag instructs the kernel to disable all interrupts when executing this interrupt handler when unset interrupt handlers run with all interrupts except their own enabled most interrupt handlers do not set this flag as disabling all interrupts is bad form its use is reserved for performance sensitive inter rupts that execute quickly this flag is the current manifestation of the flag which in the past distinguished between fast and slow interrupts n this flag specifies that interrupts generated by this device should contribute to the kernel entropy pool the kernel entropy pool provides truly random numbers derived from various random events if this flag is specified the timing of interrupts from this device are fed to the pool as entropy do not set registering an interrupt handler this if your device issues interrupts at a predictable rate for example the system timer or can be influenced by external attackers for example a networking device on the other hand most other hardware generates interrupts at nondeter ministic times and is therefore a good source of entropy n this flag specifies that this handler processes interrupts for the sys tem timer n this flag specifies that the interrupt line can be shared among mul tiple interrupt handlers each handler registered on a given line must specify this flag otherwise only one handler can exist per line more information on shared handlers is provided in a following section the fourth parameter name is an ascii text representation of the device associated with the interrupt for example this value for the keyboard interrupt on a pc is key board these text names are used by proc irq and proc interrupts for communica tion with the user which is discussed shortly the fifth parameter dev is used for shared interrupt lines when an interrupt handler is freed discussed later dev provides a unique cookie to enable the removal of only the desired interrupt handler from the interrupt line without this parameter it would be impossible for the kernel to know which handler to remove on a given interrupt line you can pass null here if the line is not shared but you must pass a unique cookie if your interrupt line is shared and unless your device is old and crusty and lives on the isa bus there is a good chance it must support sharing this pointer is also passed into the inter rupt handler on each invocation a common practice is to pass the driver device struc ture this pointer is unique and might be useful to have within the handlers on success returns zero a nonzero value indicates an error in which case the specified interrupt handler was not registered a common error is ebusy which denotes that the given interrupt line is already in use and either the current user or you did not specify note that can sleep and therefore cannot be called from interrupt context or other situations where code cannot block it is a common mistake to call when it is unsafe to sleep this is partly because of why can block it is indeed unclear on registration an entry corresponding to the interrupt is created in proc irq the function creates new procfs entries this func tion calls to set up the new procfs entries which in turn calls kmalloc to allocate memory as you will see in chapter memory management kmalloc can sleep so there you go an interrupt example in a driver requesting an interrupt line and installing a handler is done via if irqn printk cannot register irq d n irqn return eio in this example irqn is the requested interrupt line is the handler we specify via flags that the line can be shared the device is named and we passed for dev on failure the code prints an error and returns if the call returns zero the handler has been successfully installed from that point forward the handler is invoked in response to an interrupt it is important to initialize hardware and register an interrupt handler in the proper order to prevent the interrupt handler from running before the device is fully initialized freeing an interrupt handler when your driver unloads you need to unregister your interrupt handler and potentially disable the interrupt line to do this call void unsigned int irq void dev if the specified interrupt line is not shared this function removes the handler and dis ables the line if the interrupt line is shared the handler identified via dev is removed but the interrupt line is disabled only when the last handler is removed now you can see why a unique dev is important with shared interrupt lines a unique cookie is required to dif ferentiate between the multiple handlers that can exist on a single line and enable to remove only the correct handler in either case shared or unshared if dev is non null it must match the desired handler a call to must be made from process context table reviews the functions for registering and deregistering an interrupt handler table interrupt registration methods function description register a given interrupt handler on a given interrupt line unregister a given interrupt handler if no handlers remain on the line the given interrupt line is disabled writing an interrupt handler the following is a declaration of an interrupt handler static int irq void dev note that this declaration matches the prototype of the handler argument given to the first parameter irq is the numeric value of the interrupt line the handler is servicing this value is passed into the handler but it is not used very often except in printing log messages before version of the linux kernel there was not a dev parameter and thus irq was used to differentiate between multiple devices using the same driver and therefore the same interrupt handler as an example of this consider a computer with multiple hard drive controllers of the same type the second parameter dev is a generic pointer to the same dev that was given to when the interrupt handler was registered if this value is unique which is required to support sharing it can act as a cookie to differentiate between multiple devices potentially using the same interrupt handler dev might also point to a structure of use to the interrupt handler because the device structure is both unique to each device and potentially useful to have within the handler it is typically passed for dev the return value of an interrupt handler is the special type an interrupt handler can return two special values or the former is returned when the interrupt handler detects an interrupt for which its device was not the origina tor the latter is returned if the interrupt handler was correctly invoked and its device did indeed cause the interrupt alternatively val may be used if val is nonzero this macro returns otherwise the macro returns these special values are used to let the kernel know whether devices are issuing spurious that is unrequested interrupts if all the interrupt handlers on a given interrupt line return then the kernel can detect the problem note the curious return type which is simply an int this value provides backward compatibility with earlier kernels which did not have this feature before interrupt handlers returned void drivers may simply typedef to void and define the different return values to no ops and then work in without further modification the interrupt han dler is normally marked static because it is never called directly from another file the role of the interrupt handler depends entirely on the device and its reasons for issuing the interrupt at a minimum most interrupt handlers need to provide acknowl edgment to the device that they received the interrupt devices that are more complex need to additionally send and receive data and perform extended work in the interrupt handler as mentioned the extended work is pushed as much as possible into the bottom half handler which is discussed in the next chapter reentrancy and interrupt handlers interrupt handlers in linux need not be reentrant when a given interrupt handler is execut ing the corresponding interrupt line is masked out on all processors preventing another interrupt on the same line from being received normally all other interrupts are enabled so other interrupts are serviced but the current line is always disabled consequently the same interrupt handler is never invoked concurrently to service a nested interrupt this greatly simplifies writing your interrupt handler shared handlers a shared handler is registered and executed much like a nonshared handler following are three main differences n the flag must be set in the flags argument to n the dev argument must be unique to each registered handler a pointer to any per device structure is sufficient a common choice is the device structure as it is both unique and potentially useful to the handler you cannot pass null for a shared handler n the interrupt handler must be capable of distinguishing whether its device actually generated an interrupt this requires both hardware support and associated logic in the interrupt handler if the hardware did not offer this capability there would be no way for the interrupt handler to know whether its associated device or some other device sharing the line caused the interrupt all drivers sharing the interrupt line must meet the previous requirements if any one device does not share fairly none can share the line when is called with specified the call succeeds only if the interrupt line is currently not regis tered or if all registered handlers on the line also specified shared handlers however can mix usage of when the kernel receives an interrupt it invokes sequentially each registered handler on the line therefore it is important that the handler be capable of distinguishing whether it generated a given interrupt the handler must quickly exit if its associated device did not generate the interrupt this requires the hardware device to have a status register or similar mechanism that the handler can check most hardware does indeed have such a feature a real life interrupt handler let look at a real interrupt handler from the real time clock rtc driver found in drivers char rtc c an rtc is found in many machines including pcs it is a device separate from the system timer which sets the system clock provides an alarm or supplies a periodic timer on most architectures the system clock is set by writing the desired time into a specific register or i o range any alarm or periodic timer functionality is normally implemented via interrupt the interrupt is equivalent to a real world clock alarm the receipt of the interrupt is analogous to a buzzing alarm when the rtc driver loads the function is invoked to initialize the driver one of its duties is to register the interrupt handler register on if rtc void printk rtc cannot register irq d n return eio in this example the interrupt line is stored in this variable is set to the rtc interrupt for a given architecture on a pc the rtc is located at irq the second parameter is the interrupt handler which is willing to share the interrupt line with other handlers thanks to the flag from the fourth parameter you can see that the driver name is rtc because this device shares the interrupt line it passes a unique per device value for dev finally the handler itself static int irq void dev can be an alarm interrupt update complete interrupt or a periodic interrupt we store the status in the low byte and the number of interrupts received since the last read in the remainder of rtc_intr_flags if rtc_irq_timer jiffies hz hz now do the rest of the actions if func rtc_task_lock rtc_async_queue sigio return this function is invoked whenever the machine receives the rtc interrupt first note the spin lock calls the first set ensures that is not accessed concurrently by another processor on an smp machine and the second set protects from the same locks are discussed in chapter kernel synchronization methods the variable is an unsigned long that stores information about the rtc and is updated on each interrupt to reflect the status of the interrupt next if an rtc periodic timer is set it is updated via timers are dis cussed in chapter timers and time management the final bunch of code under the comment now do the rest of the actions executes a possible preset callback function the rtc driver enables a callback function to be reg istered and executed on each rtc interrupt finally this function returns to signify that it properly handled this device because the interrupt handler does not support sharing and there is no mecha nism for the rtc to detect a spurious interrupt this handler always returns interrupt context when executing an interrupt handler the kernel is in interrupt context recall that process context is the mode of operation the kernel is in while it is executing on behalf of a process for example executing a system call or running a kernel thread in process con text the current macro points to the associated task furthermore because a process is coupled to the kernel in process context process context can sleep or otherwise invoke the scheduler interrupt context on the other hand is not associated with a process the current macro is not relevant although it points to the interrupted process without a backing process interrupt context cannot sleep how would it ever reschedule therefore you cannot call certain functions from interrupt context if a function sleeps you cannot use it from your interrupt handler this limits the functions that one can call from an interrupt handler interrupt context is time critical because the interrupt handler interrupts other code code should be quick and simple busy looping is possible but discouraged this is an important point always keep in mind that your interrupt handler has interrupted other code possibly even another interrupt handler on a different line because of this asyn chronous nature it is imperative that all interrupt handlers be as quick and as simple as possible as much as possible work should be pushed out from the interrupt handler and performed in a bottom half which runs at a more convenient time the setup of an interrupt handler stacks is a configuration option historically inter rupt handlers did not receive their own stacks instead they would share the stack of the process that they interrupted the kernel stack is two pages in size typically that is on bit architectures and on bit architectures because in this setup interrupt handlers share the stack they must be exceptionally frugal with what data they allocate there of course the kernel stack is limited to begin with so all kernel code should be cautious early in the kernel process an option was added to reduce the stack size from two pages down to one providing only a stack on bit systems this reduced memory pressure because every process on the system previously needed two pages of contiguous nonswappable kernel memory to cope with the reduced stack size interrupt handlers were given their own stack one stack per processor one page in size this stack is referred to as the interrupt stack although the total size of the interrupt stack is half that of the original shared stack the average stack space available is greater because interrupt handlers get the full page of memory to themselves a process is always running when nothing else is schedulable the idle task runs your interrupt handler should not care what stack setup is in use or what the size of the kernel stack is always use an absolute minimum amount of stack space implementing interrupt handlers perhaps not surprising the implementation of the interrupt handling system in linux is architecture dependent the implementation depends on the processor the type of inter rupt controller used and the design of the architecture and machine figure is a diagram of the path an interrupt takes through hardware and the kernel hardware generates an interrupt yes interrupt controller processor interrupts the kernel is there an interrupt handler on this line no run all interrupt handlers on this line return to the kernel code that was interrupted processor figure the path that an interrupt takes from hardware and on through the kernel a device issues an interrupt by sending an electric signal over its bus to the interrupt controller if the interrupt line is enabled they can be masked out the interrupt con troller sends the interrupt to the processor in most architectures this is accomplished by an electrical signal sent over a special pin to the processor unless interrupts are disabled in the processor which can also happen the processor immediately stops what it is doing disables the interrupt system and jumps to a predefined location in memory and executes the code located there this predefined point is set up by the kernel and is the entry point for interrupt handlers the interrupt journey in the kernel begins at this predefined entry point just as system calls enter the kernel through a predefined exception handler for each interrupt line the processor jumps to a unique location in memory and executes the code located there in this manner the kernel knows the irq number of the incoming interrupt the initial entry point simply saves this value and stores the current register values which belong to the interrupted task on the stack then the kernel calls from here onward most of the interrupt handling code is written in c however it is still architecture dependent the function is declared as unsigned int struct regs because the c calling convention places function arguments at the top of the stack the structure contains the initial register values that were previously saved in the assembly entry routine because the interrupt value was also saved can extract it after the interrupt line is calculated acknowledges the receipt of the inter rupt and disables interrupt delivery on the line on normal pc machines these operations are handled by next ensures that a valid handler is registered on the line and that it is enabled and not currently executing if so it calls defined in kernel irq handler c to run the installed interrupt handlers for the line irq action chain handler irq the interrupt number action the interrupt action chain for this irq handles the action chain of an irq event unsigned int irq struct irqaction action ret retval unsigned int status if action flags do irq action ret action handler irq action irq action ret switch ret case set result to handled so the spurious check does not trigger ret catch drivers which return but did not set up a thread function if unlikely action irq action break wake up the handler thread for this action in case the thread crashed and was killed we just pretend that we handled the interrupt the hardirq handler above has disabled the device interrupt so no irq storm is lurking if likely action action action thread fall through to add to randomness case status action flags break default break retval ret action action next while action if status irq return retval first because the processor disabled interrupts they are turned back on unless was specified during the handler registration recall that specifies that the handler must be run with interrupts disabled next each potential handler is executed in a loop if this line is not shared the loop terminates after the first iteration otherwise all handlers are executed after that is called if was specified during registration this function uses the timing of the interrupt to generate entropy for the ran dom number generator finally interrupts are again disabled expects them still to be off and the function returns back in the function cleans up and returns to the initial entry point which then jumps to the routine is as with the initial entry code written in assembly this routine checks whether a reschedule is pending recall from chapter process scheduling that this implies that is set if a reschedule is pending and the kernel is returning to user space that is the interrupt interrupted a user process schedule is called if the kernel is returning to kernel space that is the interrupt inter rupted the kernel itself schedule is called only if the is zero other wise it is not safe to preempt the kernel after schedule returns or if there is no work pending the initial registers are restored and the kernel resumes whatever was interrupted on the initial assembly routines are located in arch kernel s s for bit and the c methods are located in arch kernel irq c other supported architectures are similar proc interrupts procfs is a virtual filesystem that exists only in kernel memory and is typically mounted at proc reading or writing files in procfs invokes kernel functions that simulate reading or writing from a real file a relevant example is the proc interrupts file which is popu lated with statistics related to interrupts on the system here is sample output from a uniprocessor pc xt pic timer xt pic xt pic cascade xt pic uhci hcd xt pic xt pic uhci hcd xt pic nmi loc err the first column is the interrupt line on this system interrupts numbered and are present handlers are not installed on lines not displayed the second column is a counter of the number of interrupts received a column is present for each processor on the system but this machine has only one processor as you can see the timer interrupt has received interrupts whereas the sound card has received none which is an indication that it has not been used since the machine booted the third col umn is the interrupt controller handling this interrupt xt pic corresponds to the standard as an exercise after reading chapter can you tell how long the system has been up in terms of hz knowing the number of timer interrupts that have occurred pc programmable interrupt controller on systems with an i o apic most interrupts would list io apic level or io apic edge as their interrupt controller finally the last column is the device associated with this interrupt this name is supplied by the devname parameter to as discussed previously if the interrupt is shared as is the case with interrupt number in this example all the devices registered on the interrupt line are listed for the curious procfs code is located primarily in fs proc the function that provides proc interrupts is not surprisingly architecture dependent and named interrupt control the linux kernel implements a family of interfaces for manipulating the state of inter rupts on a machine these interfaces enable you to disable the interrupt system for the current processor or mask out an interrupt line for the entire machine these routines are all architecture dependent and can be found in asm system h and asm irq h see table later in this chapter for a complete listing of the interfaces reasons to control the interrupt system generally boil down to needing to provide synchronization by disabling interrupts you can guarantee that an interrupt handler will not preempt your current code moreover disabling interrupts also disables kernel pre emption neither disabling interrupt delivery nor disabling kernel preemption provides any protection from concurrent access from another processor however because linux supports multiple processors kernel code more generally needs to obtain some sort of lock to prevent another processor from accessing shared data simultaneously these locks are often obtained in conjunction with disabling local interrupts the lock provides pro tection against concurrent access from another processor whereas disabling interrupts provides protection against concurrent access from a possible interrupt handler chapters and discuss the various problems of synchronization and their solutions neverthe less understanding the kernel interrupt control interfaces is important disabling and enabling interrupts to disable interrupts locally for the current processor and only the current processor and then later reenable them do the following interrupts are disabled these functions are usually implemented as a single assembly operation of course this depends on the architecture indeed on is a simple cli and is a simple sti instruction cli and sti are the assembly calls to clear and set the allow interrupts flag respectively in other words they disable and enable interrupt delivery on the issuing processor the routine is dangerous if interrupts were already disabled prior to its invocation the corresponding call to unconditionally enables interrupts despite the fact that they were off to begin with instead a mechanism is needed to restore interrupts to a previous state this is a common concern because a given code path in the kernel can be reached both with and without interrupts enabled depending on the call chain for example imagine the previous code snippet is part of a larger function imagine that this function is called by two other functions one that dis ables interrupts and one that does not because it is becoming harder as the kernel grows in size and complexity to know all the code paths leading up to a function it is much safer to save the state of the interrupt system before disabling it then when you are ready to reenable interrupts you simply restore them to their original state unsigned long flags flags interrupts are now disabled flags interrupts are restored to their previous state note that these methods are implemented at least in part as macros so the flags parameter which must be defined as an unsigned long is seemingly passed by value this parameter contains architecture specific data containing the state of the interrupt sys tems because at least one supported architecture incorporates stack information into the value ahem sparc flags cannot be passed to another function specifically it must remain on the same stack frame for this reason the call to save and the call to restore interrupts must occur in the same function all the previous functions can be called from both interrupt and process context no more global cli the kernel formerly provided a method to disable interrupts on all processors in the system furthermore if another processor called this method it would have to wait until interrupts were enabled before continuing this function was named cli and the corresponding enable call was named sti very centric despite existing for all architectures these interfaces were deprecated during and consequently all interrupt synchronization must now use a combination of local interrupt control and spin locks discussed in chapter an introduction to kernel synchronization this means that code that previously only had to disable interrupts globally to ensure mutual exclusive access to shared data now needs to do a bit more work previously driver writers could assume a cli used in their interrupt handlers and any where else the shared data was accessed would provide mutual exclusion the cli call would ensure that no other interrupt handlers and thus their specific handler would run furthermore if another processor entered a cli protected region it would not continue until the original processor exited its cli protected region with a call to sti removing the global cli has a handful of advantages first it forces driver writers to imple ment real locking a fine grained lock with a specific purpose is faster than a global lock which is effectively what cli is second the removal streamlined a lot of code in the interrupt sys tem and removed a bunch more the result is simpler and easier to comprehend disabling a specific interrupt line in the previous section we looked at functions that disable all interrupt delivery for an entire processor in some cases it is useful to disable only a specific interrupt line for the entire system this is called masking out an interrupt line as an example you might want to disable delivery of a device interrupts before manipulating its state linux provides four interfaces for this task void unsigned int irq void unsigned int irq void unsigned int irq void unsigned int irq the first two functions disable a given interrupt line in the interrupt controller this disables delivery of the given interrupt to all processors in the system additionally the function does not return until any currently executing handler completes thus callers are assured not only that new interrupts will not be delivered on the given line but also that any already executing handlers have exited the function does not wait for current handlers to complete the function waits for a specific interrupt handler to exit if it is executing before returning calls to these functions nest for each call to or on a given interrupt line a corresponding call to is required only on the last call to is the interrupt line actually enabled for example if is called twice the interrupt line is not actually reenabled until the second call to all three of these functions can be called from interrupt or process context and do not sleep if calling from interrupt context be careful you do not want for example to enable an interrupt line while you are handling it recall that the interrupt line of a handler is masked out while it is serviced it would be rather rude to disable an interrupt line shared among multiple interrupt handlers disabling the line disables interrupt delivery for all devices on the line there fore drivers for newer devices tend not to use these interfaces because pci devices have to support interrupt line sharing by specification they should not use these interfaces at all thus and friends are found more often in drivers for older legacy devices such as the pc parallel port many older devices particularly isa devices do not provide a method of obtaining whether they gener ated an interrupt therefore often interrupt lines for isa devices cannot be shared because the pci specification mandates the sharing of interrupts modern pci based devices support interrupt sharing in contemporary computers nearly all interrupt lines can be shared status of the interrupt system it is often useful to know the state of the interrupt system for example whether inter rupts are enabled or disabled or whether you are currently executing in interrupt context the macro defined in asm system h returns nonzero if the interrupt system on the local processor is disabled otherwise it returns zero two macros defined in linux hardirq h provide an interface to check the ker nel current context they are the most useful is the first it returns nonzero if the kernel is performing any type of interrupt handling this includes either executing an interrupt handler or a bottom half handler the macro returns nonzero only if the kernel is specifically executing an interrupt handler more often you want to check whether you are in process context that is you want to ensure you are not in interrupt context this is often the case because code wants to do something that can only be done from process context such as sleep if returns zero the kernel is in process context yes the names are confusing and do little to impart their meaning table is a sum mary of the interrupt control methods and their description table interrupt control methods function description disables local interrupt delivery enables local interrupt delivery saves the current state of local interrupt delivery and then disables it restores local interrupt delivery to the given state disables the given interrupt line and ensures no handler on the line is executing before returning disables the given interrupt line enables the given interrupt line returns nonzero if local interrupt delivery is disabled other wise returns zero returns nonzero if in interrupt context and zero if in process context returns nonzero if currently executing an interrupt handler and zero otherwise conclusion conclusion this chapter looked at interrupts a hardware resource used by devices to asynchro nously signal the processor interrupts in effect are used by hardware to interrupt the operating system most modern hardware uses interrupts to communicate with operating systems the device driver that manages a given piece of hardware registers an interrupt handler to respond to and process interrupts issued from their associated hardware work performed in interrupts includes acknowledging and resetting hardware copying data from the device to main memory and vice versa processing hardware requests and sending out new hardware requests the kernel provides interfaces for registering and unregistering interrupt handlers dis abling interrupts masking out interrupt lines and checking the status of the interrupt sys tem table provided an overview of many of these functions because interrupts interrupt other executing code processes the kernel itself and even other interrupt handlers they must execute quickly often however there is a lot of work to do to balance the large amount of work with the need for quick execution the kernel divides the work of processing interrupts into two halves the interrupt handler the top half was discussed in this chapter the next chapter looks at the bottom half bottom halves and deferring work the previous chapter discussed interrupt handlers the kernel mechanism for dealing with hardware interrupts interrupt handlers are an important indeed required part of any operating system due to various limitations however interrupt handlers can form only the first half of any interrupt processing solution these limitations include n interrupt handlers run asynchronously and thus interrupt other potentially impor tant code including other interrupt handlers therefore to avoid stalling the inter rupted code for too long interrupt handlers need to run as quickly as possible n interrupt handlers run with the current interrupt level disabled at best if is unset and at worst if is set with all interrupts on the current processor disabled as disabling interrupts prevents hardware from communicating with the operating systems interrupt handlers need to run as quickly as possible n interrupt handlers are often timing critical because they deal with hardware n interrupt handlers do not run in process context therefore they cannot block this limits what they can do it should now be evident that interrupt handlers are only a piece of the solution to managing hardware interrupts operating systems certainly need a quick asynchronous simple mechanism for immediately responding to hardware and performing any time critical actions interrupt handlers serve this function well but other less critical work can and should be deferred to a later point when interrupts are enabled consequently managing interrupts is divided into two parts or halves the first part interrupt handlers top halves are executed by the kernel asynchronously in immediate response to a hardware interrupt as discussed in the previous chapter this chapter looks at the second part of the interrupt solution bottom halves bottom halves the job of bottom halves is to perform any interrupt related work not performed by the interrupt handler in an ideal world this is nearly all the work because you want the interrupt handler to perform as little work and in turn be as fast as possible by offload ing as much work as possible to the bottom half the interrupt handler can return control of the system to whatever it interrupted as quickly as possible nonetheless the interrupt handler must perform some of the work for example the interrupt handler almost assuredly needs to acknowledge to the hardware the receipt of the interrupt it may need to copy data to or from the hardware this work is timing sensitive so it makes sense to perform it in the interrupt handler almost anything else is fair game for performing in the bottom half for example if you copy data from the hardware into memory in the top half it certainly makes sense to process it in the bottom half unfortunately no hard and fast rules exist about what work to perform where the decision is left entirely up to the device driver author although no arrangement is illegal an arrangement can certainly be suboptimal remember inter rupt handlers run asynchronously with at least the current interrupt line disabled mini mizing their duration is important although it is not always clear how to divide the work between the top and bottom half a couple of useful tips help n if the work is time sensitive perform it in the interrupt handler n if the work is related to the hardware perform it in the interrupt handler n if the work needs to ensure that another interrupt particularly the same interrupt does not interrupt it perform it in the interrupt handler n for everything else consider performing the work in the bottom half when attempting to write your own device driver looking at other interrupt handlers and their corresponding bottom halves can help when deciding how to divide your interrupt processing work between the top and bottom half ask yourself what must be in the top half and what can be in the bottom half generally the quicker the interrupt han dler executes the better why bottom halves it is crucial to understand why to defer work and when exactly to defer it you want to limit the amount of work you perform in an interrupt handler because interrupt handlers run with the current interrupt line disabled on all processors worse handlers that register with run with all interrupt lines disabled on the local processor plus the current interrupt line disabled on all processors minimizing the time spent with inter rupts disabled is important for system response and performance add to this the fact that interrupt handlers run asynchronously with respect to other code even other interrupt handlers and it is clear that you should work to minimize how long interrupt handlers run processing incoming network traffic should not prevent the kernel receipt of key strokes the solution is to defer some of the work until later but when is later the important thing to realize is that later is often simply not now the point of a bottom half is not to do work at some specific point in the future but sim ply to defer work until any point in the future when the system is less busy and interrupts are again enabled often bottom halves run immediately after the interrupt returns the key is that they run with all interrupts enabled linux is not alone in separating the processing of hardware interrupts into two parts most operating systems do so the top half is quick and simple and runs with some or all interrupts disabled the bottom half however it is implemented runs later with all inter rupts enabled this design keeps system latency low by running with interrupts disabled for as little time as necessary a world of bottom halves unlike the top half which is implemented entirely via the interrupt handler multiple mechanisms are available for implementing a bottom half these mechanisms are different interfaces and subsystems that enable you to implement bottom halves whereas the pre vious chapter looked at just a single way of implementing interrupt handlers this chapter looks at multiple methods of implementing bottom halves over the course of linux history there have been many bottom half mechanisms confusingly some of these mechanisms have similar or even dumb names it requires a special type of programmer to name bottom halves this chapter discusses both the design and implementation of the bottom half mecha nisms that exist in we also discuss how to use them in the kernel code you write the old but long since removed bottom half mechanisms are historically significant and so they are mentioned when relevant the original bottom half in the beginning linux provided only the bottom half for implementing bottom halves this name was logical because at the time that was the only means available for deferring work the infrastructure was also known as bh which is what we will call it to avoid confusion with the generic term bottom half the bh interface was simple like most things in those good old days it provided a statically created list of bottom halves for the entire system the top half could mark whether the bottom half would run by setting a bit in a bit integer each bh was globally synchronized no two could run at the same time even on different processors this was easy to use yet inflexible a simple approach yet a bottleneck task queues later on the kernel developers introduced task queues both as a method of deferring work and as a replacement for the bh mechanism the kernel defined a family of queues each queue contained a linked list of functions to call the queued functions were run at certain times depending on which queue they were in drivers could register their bot tom halves in the appropriate queue this worked fairly well but it was still too inflexible to replace the bh interface entirely it also was not lightweight enough for performance critical subsystems such as networking softirqs and tasklets during the development series the kernel developers introduced softirqs and tasklets with the exception of compatibility with existing drivers softirqs and tasklets could com pletely replace the bh interface softirqs are a set of statically defined bottom halves that can run simultaneously on any processor even two of the same type can run concur rently tasklets which have an awful and confusing name are flexible dynamically cre ated bottom halves built on top of softirqs two different tasklets can run concurrently on different processors but two of the same type of tasklet cannot run simultaneously thus tasklets are a good trade off between performance and ease of use for most bottom half processing the tasklet is sufficient softirqs are useful when performance is critical such as with networking using softirqs requires more care however because two of the same softirq can run at the same time in addition softirqs must be registered statically at com pile time conversely code can dynamically register tasklets to further confound the issue some people refer to all bottom halves as software inter rupts or softirqs in other words they call both the softirq mechanism and bottom halves in general softirqs ignore those people they run with the same crowd that named the bh and tasklet mechanisms while developing the kernel the bh interface was finally tossed to the curb because all bh users were converted to the other bottom half interfaces additionally the task queue interface was replaced by the work queue interface work queues are a simple yet useful method of queuing work to later be performed in process context we get to them later consequently today has three bottom half mechanisms in the kernel softirqs tasklets and work queues the old bh and task queue interfaces are but mere memories kernel timers another mechanism for deferring work is kernel timers unlike the mechanisms discussed in the chapter thus far timers defer work for a specified amount of time that is although the tools discussed in this chapter are useful to defer work to any time but now you use timers to defer work until at least a specific time has elapsed therefore timers have different uses than the general mechanisms discussed in this chap ter a full discussion of timers is given in chapter timers and time management it is nontrivial to convert bhs to softirqs or tasklets because bhs are globally synchronized and there fore assume that no other bh is running during their execution the conversion did eventually happen however in they have nothing to do with tasks think of a tasklet as a simple and easy to use softirq dispelling the confusion this is some seriously confusing stuff but actually it involves just naming issues let go over it again bottom half is a generic operating system term referring to the deferred portion of interrupt processing so named because it represents the second or bottom half of the interrupt processing solution in linux the term currently has this meaning too all the kernel mechanisms for deferring work are bottom halves some people also confus ingly call all bottom halves softirqs bottom half also refers to the original deferred work mechanism in linux this mech anism is also known as a bh so we call it by that name now and leave the former as a generic description the bh mechanism was deprecated a while back and fully removed in the development kernel series currently three methods exist for deferring work softirqs tasklets and work queues tasklets are built on softirqs and work queues are their own subsystem table presents a history of bottom halves table bottom half status bottom half status bh removed in task queues removed in softirq available since tasklet available since work queues available since with this naming confusion settled let look at the individual mechanisms softirqs the place to start this discussion of the actual bottom half methods is with softirqs softirqs are rarely used directly tasklets are a much more common form of bottom half nonetheless because tasklets are built on softirqs we cover them first the softirq code lives in the file kernel softirq c in the kernel source tree implementing softirqs softirqs are statically allocated at compile time unlike tasklets you cannot dynamically register and destroy softirqs softirqs are represented by the structure which is defined in linux interrupt h struct void action struct a entry array of this structure is declared in kernel softirq c static struct each registered softirq consumes one entry in the array consequently there are registered softirqs the number of registered softirqs is statically determined at compile time and cannot be changed dynamically the kernel enforces a limit of registered softirqs in the current kernel however only nine exist the softirq handler the prototype of a softirq handler action looks like void struct when the kernel runs a softirq handler it executes this action function with a pointer to the corresponding structure as its lone argument for example if pointed to an entry in the array the kernel would invoke the softirq handler function as action it seems a bit odd that the kernel passes the entire structure to the softirq handler this trick enables future additions to the structure without requiring a change in every softirq handler a softirq never preempts another softirq the only event that can preempt a softirq is an interrupt handler another softirq even the same one can run on another processor however executing softirqs a registered softirq must be marked before it will execute this is called raising the softirq usually an interrupt handler marks its softirq for execution before returning then at a suitable time the softirq runs pending softirqs are checked for and executed in the fol lowing places n in the return from hardware interrupt code path n in the ksoftirqd kernel thread n in any code that explicitly checks for and executes pending softirqs such as the net working subsystem regardless of the method of invocation softirq execution occurs in which is invoked by the function is quite simple if there are pending most drivers use tasklets or work queues for their bottom half tasklets are built off softirqs as the next section explains softirqs loops over each one invoking its handler let look at a simpli fied variant of the important part of pending pending if pending struct h reset the pending bitmask h do if pending h action h h pending while pending this snippet is the heart of softirq processing it checks for and executes any pending softirqs specifically it sets the pending local variable to the value returned by the macro this is a bit mask of pending softirqs if bit n is set the nth softirq is pending now that the pending bitmask of softirqs is saved it clears the actual bitmask the pointer h is set to the first entry in the if the first bit in pending is set h action h is called the pointer h is incremented by one so that it now points to the second entry in the array the bitmask pending is right shifted by one this tosses the first bit away and moves all other bits one place to the right consequently the second bit is now the first and so on the pointer h now points to the second entry in the array and the pending bit mask now has the second bit as the first repeat the previous steps this actually occurs with local interrupts disabled but that is omitted in this simplified example if interrupts were not disabled a softirq could have been raised and thus be pending in the intervening time between saving the mask and clearing it this would result in incorrectly clearing a pending bit continue repeating until pending is zero at which point there are no more pend ing softirqs and the work is done note this check is sufficient to ensure h always points to a valid entry in because pending has at most set bits and thus this loop executes at most times using softirqs softirqs are reserved for the most timing critical and important bottom half processing on the system currently only two subsystems networking and block devices directly use softirqs additionally kernel timers and tasklets are built on top of softirqs if you add a new softirq you normally want to ask yourself why using a tasklet is insufficient tasklets are dynamically created and are simpler to use because of their weaker locking require ments and they still perform quite well nonetheless for timing critical applications that can do their own locking in an efficient way softirqs might be the correct solution assigning an index you declare softirqs statically at compile time via an enum in linux interrupt h the kernel uses this index which starts at zero as a relative priority softirqs with the lowest numerical priority execute before those with a higher numerical priority creating a new softirq includes adding a new entry to this enum when adding a new softirq you might not want to simply add your entry to the end of the list as you would elsewhere instead you need to insert the new entry depending on the priority you want to give it by convention is always the first and is always the last entry a new entry likely belongs in between and table contains a list of the existing tasklet types table softirq types tasklet priority softirq description high priority tasklets timers send network packets receive network packets block devices normal priority tasklets scheduler high resolution timers rcu locking registering your handler next the softirq handler is registered at run time via which takes two parameters the softirq index and its handler function the networking subsystem for example registers its softirqs like this in net core dev c the softirq handlers run with interrupts enabled and cannot sleep while a handler runs softirqs on the current processor are disabled another processor however can exe cute other softirqs if the same softirq is raised again while it is executing another proces sor can run it simultaneously this means that any shared data even global data used only within the softirq handler needs proper locking as discussed in the next two chapters this is an important point and it is the reason tasklets are usually preferred simply pre venting your softirqs from running concurrently is not ideal if a softirq obtained a lock to prevent another instance of itself from running simultaneously there would be no reason to use a softirq consequently most softirq handlers resort to per processor data data unique to each processor and thus not requiring locking and other tricks to avoid explicit locking and provide excellent scalability the raison d être to softirqs is scalability if you do not need to scale to infinitely many processors then use a tasklet tasklets are essentially softirqs in which multiple instances of the same handler cannot run concurrently on multiple processors raising your softirq after a handler is added to the enum list and registered via it is ready to run to mark it pending so it is run at the next invocation of call for example the networking subsystem would call this raises the softirq its handler runs the next time the kernel executes softirqs this function disables interrupts prior to actually raising the softirq and then restores them to their previous state if interrupts are already off the function can be used as a small optimization for example interrupts must already be off softirqs are most often raised from within interrupt handlers in the case of interrupt handlers the interrupt handler performs the basic hardware related work raises the softirq and then exits when processing interrupts the kernel invokes the softirq then runs and picks up where the interrupt handler left off in this example the top half and bottom half naming should make sense tasklets tasklets are a bottom half mechanism built on top of softirqs as mentioned they have nothing to do with tasks tasklets are similar in nature and behavior to softirqs however they have a simpler interface and relaxed locking rules as a device driver author the decision whether to use softirqs versus tasklets is simple you almost always want to use tasklets as we saw in the previous section you can almost count on one hand the users of softirqs softirqs are required only for high frequency and highly threaded uses tasklets on the other hand see much greater use tasklets work just fine for the vast majority of cases and are very easy to use implementing tasklets because tasklets are implemented on top of softirqs they are softirqs as discussed tasklets are represented by two softirqs and the only difference in these types is that the based tasklets run prior to the based tasklets the tasklet structure tasklets are represented by the structure each structure represents a unique tasklet the structure is declared in linux interrupt h struct struct next next tasklet in the list unsigned long state state of the tasklet count reference counter void func unsigned long tasklet handler function unsigned long data argument to the tasklet function the func member is the tasklet handler the equivalent of action to a softirq and receives data as its sole argument the state member is exactly zero or denotes a tasklet that is scheduled to run and denotes a tasklet that is running as an optimization is used only on multiprocessor machines because a uniprocessor machine always knows whether the tasklet is running it is either the currently executing code or not the count field is used as a reference count for the tasklet if it is nonzero the tasklet is disabled and cannot run if it is zero the tasklet is enabled and can run if marked pending scheduling tasklets scheduled tasklets the equivalent of raised softirqs are stored in two per processor struc tures for regular tasklets and for high priority tasklets both of these structures are linked lists of structures each structure in the list represents a different tasklet tasklets are scheduled via the and functions which receive a pointer to the tasklet as their lone argu ment each function ensures that the provided tasklet is not yet scheduled and then calls and as appropriate the two func tions are similar the difference is that one uses and one uses writing and using tasklets is covered in the next section now let look at the steps undertakes check whether the tasklet state is if it is the tasklet is already scheduled to run and the function can immediately return call save the state of the interrupt system and then disable local interrupts this ensures that nothing on this processor will mess with the tasklet code while is manipulating the tasklets add the tasklet to be scheduled to the head of the or linked list which is unique to each processor in the system raise the or softirq so executes this tasklet in the near future restore interrupts to their previous state and return at the next earliest convenience is run as discussed in the previous sec tion because most tasklets and softirqs are marked pending in interrupt handlers most likely runs when the last interrupt returns because or is now raised executes the associated handlers these handlers and are the heart of tasklet processing let look at the steps these handlers perform disable local interrupt delivery there is no need to first save their state because the code here is always called as a softirq handler and interrupts are always enabled and retrieve the or list for this processor clear the list for this processor by setting it equal to null yet another example of the confusing naming schemes at work here why are softirqs raised but tasklets scheduled who knows both terms mean to mark that bottom half pending so that it is exe cuted soon enable local interrupt delivery again there is no need to restore them to their pre vious state because this function knows that they were always originally enabled loop over each pending tasklet in the retrieved list if this is a multiprocessing machine check whether the tasklet is running on another processor by checking the flag if it is currently run ning do not execute it now and skip to the next pending tasklet recall that only one tasklet of a given type may run concurrently if the tasklet is not currently running set the flag so another processor will not run it check for a zero count value to ensure that the tasklet is not disabled if the tasklet is disabled skip it and go to the next pending tasklet we now know that the tasklet is not running elsewhere is marked as running so it will not start running elsewhere and has a zero count value run the tasklet handler after the tasklet runs clear the flag in the tasklet state field repeat for the next pending tasklet until there are no more scheduled tasklets waiting to run the implementation of tasklets is simple but rather clever as you saw all tasklets are multiplexed on top of two softirqs and when a tasklet is scheduled the kernel raises one of these softirqs these softirqs in turn are handled by special functions that then run any scheduled tasklets the special functions ensure that only one tasklet of a given type runs at the same time but other tasklets can run simulta neously all this complexity is then hidden behind a clean and simple interface using tasklets in most cases tasklets are the preferred mechanism with which to implement your bot tom half for a normal hardware device tasklets are dynamically created easy to use and quick moreover although their name is mind numbingly confusing it grows on you it is cute declaring your tasklet you can create tasklets statically or dynamically what option you choose depends on whether you have or want a direct or indirect reference to the tasklet if you are going to statically create the tasklet and thus have a direct reference to it use one of two macros in linux interrupt h name func data name func data both these macros statically create a struct with the given name when the tasklet is scheduled the given function func is executed and passed the argu ment data the difference between the two macros is the initial reference count the first macro creates the tasklet with a count of zero and the tasklet is enabled the second macro sets count to one and the tasklet is disabled here is an example dev this line is equivalent to struct null dev this creates a tasklet named enabled with as its han dler the value of dev is passed to the handler when it is executed to initialize a tasklet given an indirect reference a pointer to a dynamically created struct t call t dev dynamically as opposed to statically writing your tasklet handler the tasklet handler must match the correct prototype void unsigned long data as with softirqs tasklets cannot sleep this means you cannot use semaphores or other blocking functions in a tasklet tasklets also run with all interrupts enabled so you must take precautions for example disable interrupts and obtain a lock if your tasklet shares data with an interrupt handler unlike softirqs however two of the same tasklets never run concurrently although two different tasklets can run at the same time on two dif ferent processors if your tasklet shares data with another tasklet or softirq you need to use proper locking see chapter an introduction to kernel synchronization and chapter kernel synchronization methods scheduling your tasklet to schedule a tasklet for execution is called and passed a pointer to the relevant tasklet_schedule mark as pending after a tasklet is scheduled it runs once at some time in the near future if the same tasklet is scheduled again before it has had a chance to run it still runs only once if it is already running for example on another processor the tasklet is rescheduled and runs again as an optimization a tasklet always runs on the processor that scheduled it mak ing better use of the processor cache you hope you can disable a tasklet via a call to which disables the given tasklet if the tasklet is currently running the function will not return until it finishes exe cuting alternatively you can use which disables the given tasklet but does not wait for the tasklet to complete prior to returning this is usually not safe because you cannot assume the tasklet is not still running a call to enables the tasklet this function also must be called before a tasklet created with is usable for example tasklet is now disabled we can now do stuff knowing that the tasklet cannot run tasklet is now enabled you can remove a tasklet from the pending queue via this function receives a pointer as a lone argument to the tasklet removing a scheduled tasklet from the queue is useful when dealing with a tasklet that often resched ules itself this function first waits for the tasklet to finish executing and then it removes the tasklet from the queue nothing stops some other code from rescheduling the tasklet of course this function must not be used from interrupt context because it sleeps ksoftirqd softirq and thus tasklet processing is aided by a set of per processor kernel threads these kernel threads help in the processing of softirqs when the system is overwhelmed with softirqs because tasklets are implemented using softirqs the following discussion applies equally to softirqs and tasklets for brevity we will refer mainly to softirqs as already described the kernel processes softirqs in a number of places most com monly on return from handling an interrupt softirqs might be raised at high rates such as during heavy network traffic further softirq functions can reactivate themselves that is while running a softirq can raise itself so that it runs again for example the network ing subsystem softirq raises itself the possibility of a high frequency of softirqs in con junction with their capability to remark themselves active can result in user space programs being starved of processor time not processing the reactivated softirqs in a timely manner however is unacceptable when softirqs were first designed this caused a dilemma that needed fixing and neither obvious solution was a good one first let look at each of the two obvious solutions the first solution is simply to keep processing softirqs as they come in and to recheck and reprocess any pending softirqs before returning this ensures that the kernel processes softirqs in a timely manner and most important that any reactivated softirqs are also immediately processed the problem lies in high load environments in which many softirqs occur that continually reactivate themselves the kernel might continually service softirqs without accomplishing much else user space is neglected indeed nothing but softirqs and interrupt handlers run and in turn the system users get mad this approach might work fine if the system is never under intense load if the system experiences mod erate interrupt levels this solution is not acceptable user space cannot be starved for sig nificant periods the second solution is not to handle reactivated softirqs on return from interrupt the kernel merely looks at all pending softirqs and executes them as normal if any softirqs reactivate themselves however they will not run until the next time the kernel handles pending softirqs this is most likely not until the next interrupt occurs which can equate to a lengthy amount of time before any new or reactivated softirqs are executed worse on an otherwise idle system it is beneficial to process the softirqs right away unfortu nately this approach is oblivious to which processes are runnable therefore although this method prevents starving user space it does starve the softirqs and does not take good advantage of an idle system in designing softirqs the kernel developers realized that some sort of compromise was needed the solution ultimately implemented in the kernel is to not immediately process reactivated softirqs instead if the number of softirqs grows excessive the kernel wakes up a family of kernel threads to handle the load the kernel threads run with the lowest pos sible priority nice value of which ensures they do not run in lieu of anything impor tant this concession prevents heavy softirq activity from completely starving user space of processor time conversely it also ensures that excess softirqs do run eventually finally this solution has the added property that on an idle system the softirqs are handled rather quickly because the kernel threads will schedule immediately there is one thread per processor the threads are each named ksoftirqd n where n is the processor number on a two processor system you would have ksoftirqd and ksoftirqd having a thread on each processor ensures an idle processor if available can always service softirqs after the threads are initialized they run a tight loop similar to this for if cpu schedule while cpu if schedule if any softirqs are pending as reported by ksoftirqd calls to handle them note that it does this repeatedly to handle any reactivated softirqs too after each iteration schedule is called if needed to enable more impor tant processes to run after all processing is complete the kernel thread sets itself and invokes the scheduler to select a new runnable process the softirq kernel threads are awakened whenever detects an executed kernel thread reactivating itself the old bh mechanism although the old bh interface thankfully is no longer present in it was around for a long time since the earliest versions of the kernel because it had immense staying power it certainly carries some historical significance that requires more than a passing look nothing in this brief section actually pertains to but the history is important the bh interface is ancient and it showed each bh must be statically defined and there are a maximum of because the handlers must all be defined at compile time modules could not directly use the bh interface they could piggyback off an existing bh however over time this static requirement and the maximum of bottom halves became a major hindrance to their use all bh handlers are strictly serialized no two bh handlers even of different types can run concurrently this made synchronization easy but it wasn t beneficial to multi processor scalability performance on large smp machines was sub par a driver using the bh interface did not scale well to multiple processors the networking layer in particular suffered other than these attributes the bh mechanism is similar to tasklets in fact the bh interface was implemented on top of tasklets in the possible bottom halves were represented by constants defined in linux interrupt h to mark a bh as pending the function was called and passed the number of the bh in this in turn scheduled the bh tasklet to run before the kernel the bh mechanism was independently implemented and did not rely on any lower level bottom half mecha nism much as softirqs are implemented today because of the shortcomings of this form of bottom half kernel developers introduced task queues to replace bottom halves task queues never accomplished this goal although they did win many new users in the softirq and tasklet mechanisms were introduced to put an end to the bh the bh mechanism was reimplemented on top of tasklets unfortunately it was complicated to port bottom halves from the bh interface to tasklets or softirqs because of the weaker inherent serialization of the new interfaces during however the conversion did occur when timers and scsi the remaining bh users finally moved over to softirqs the kernel developers summarily removed the bh inter face good riddance bh that is the weaker serialization was beneficial to performance but also harder to program converting a bh to a tasklet for example required careful thinking is this code safe running at the same time as any other tasklet when finally converted however the performance was worth it work queues work queues are a different form of deferring work from what we have looked at so far work queues defer work into a kernel thread this bottom half always runs in process context thus code deferred to a work queue has all the usual benefits of process context most important work queues are schedulable and can therefore sleep normally it is easy to decide between using work queues and softirqs tasklets if the deferred work needs to sleep work queues are used if the deferred work need not sleep softirqs or tasklets are used indeed the usual alternative to work queues is kernel threads because the kernel developers frown upon creating a new kernel thread and in some locales it is a punishable offense work queues are strongly preferred they are really easy to use too if you need a schedulable entity to perform your bottom half processing you need work queues they are the only bottom half mechanisms that run in process context and thus the only ones that can sleep this means they are useful for situations in which you need to allocate a lot of memory obtain a semaphore or perform block i o if you do not need a kernel thread to handle your deferred work consider a tasklet instead implementing work queues in its most basic form the work queue subsystem is an interface for creating kernel threads to handle work queued from elsewhere these kernel threads are called worker threads work queues let your driver create a special worker thread to handle deferred work the work queue subsystem however implements and provides a default worker thread for handling work therefore in its most common form a work queue is a simple interface for deferring work to a generic kernel thread the default worker threads are called events n where n is the processor number there is one per processor for example on a uniprocessor system there is one thread events a dual processor system would additionally have an events thread the default worker thread handles deferred work from multiple locations many drivers in the kernel defer their bottom half work to the default thread unless a driver or subsystem has a strong requirement for creating its own thread the default thread is preferred nothing stops code from creating its own worker thread however this might be advantageous if you perform large amounts of processing in the worker thread processor intense and performance critical work might benefit from its own thread this also light ens the load on the default threads which prevents starving the rest of the queued work data structures representing the threads the worker threads are represented by the structure the externally visible workqueue abstraction is an array of per cpu workqueues struct struct struct list const char name int singlethread int freezeable int rt this structure defined in kernel workqueue c contains an array of struct one per possible processor on the system because the worker threads exist on each processor in the system there is one of these structures per worker thread per processor on a given machine the is the core data structure and is also defined in kernel workqueue c struct lock lock protecting this structure struct worklist list of work struct struct wq associated thread associated thread note that each type of worker thread has one associated to it inside there is one for every thread and thus every processor because there is one worker thread on each processor data structures representing the work all worker threads are implemented as normal kernel threads running the function after initial setup this function enters an infinite loop and goes to sleep when work is queued the thread is awakened and processes the work when there is no work left to process it goes back to sleep the work is represented by the structure defined in linux workqueue h struct data struct entry func these structures are strung into a linked list one for each type of queue on each processor for example there is one list of deferred work for the generic thread per processor when a worker thread wakes up it runs any work in its list as it completes work it removes the corresponding entries from the linked list when the list is empty it goes back to sleep let look at the heart of simplified for cwq wait if cwq worklist schedule cwq wait cwq this function performs the following functions in an infinite loop the thread marks itself sleeping the task state is set to and adds itself to a wait queue if the linked list of work is empty the thread calls schedule and goes to sleep if the list is not empty the thread does not go to sleep instead it marks itself and removes itself from the wait queue if the list is nonempty the thread calls to perform the deferred work the function in turn actually performs the deferred work while cwq worklist struct work f void data work cwq worklist next struct entry f work func cwq worklist next work f work this function loops over each entry in the linked list of pending work and executes the func member of the for each entry in the linked list while the list is not empty it grabs the next entry in the list it retrieves the function that should be called func and its argument data it removes this entry from the list and clears the pending bit in the structure itself it invokes the function repeat work queue implementation summary the relationship between the different data structures is admittedly a bit convoluted figure provides a graphical example which should bring it all together wor struc ture uct figure the relationship between work work queues and the worker threads at the highest level there are worker threads there can be multiple types of worker threads there is one worker thread per processor of a given type parts of the kernel can create worker threads as needed by default there is the events worker thread each worker thread is represented by the structure the structure represents all the worker threads of a given type for example assume that in addition to the generic events worker type you also create a falcon worker type also assume you have a four processor computer then there are four events threads and thus four structures and four falcon threads and thus another four structures there is one for the events type and one for the falcon type now let approach from the lowest level which starts with work your driver creates work which it wants to defer to later the structure represents this work among other things this structure contains a pointer to the function that handles the deferred work the work is submitted to a specific worker thread in this case a specific falcon thread the worker thread then wakes up and performs the queued work most drivers use the existing default worker threads named events they are easy and simple some more serious situations however demand their own worker threads the xfs filesystem for example creates two new types of worker threads using work queues using work queues is easy we cover the default events queue first and then look at creat ing new worker threads creating work the first step is actually creating some work to defer to create the structure statically at runtime use name void func void void data this statically creates a structure named name with handler function func and argument data alternatively you can create work at runtime via a pointer struct work void func void void data this dynamically initializes the work queue pointed to by work with handler function func and argument data your work queue handler the prototype for the work queue handler is void void data a worker thread executes this function and thus the function runs in process context by default interrupts are enabled and no locks are held if needed the function can sleep note that despite running in process context the work handlers cannot access user space memory because there is no associated user space memory map for kernel threads the kernel can access user memory only when running on behalf of a user space process such as when executing a system call only then is user memory mapped in locking between work queues or other parts of the kernel is handled just as with any other process context code this makes writing work handlers much easier the next two chapters cover locking scheduling work now that the work is created we can schedule it to queue a given work handler func tion with the default events worker threads simply call work the work is scheduled immediately and is run as soon as the events worker thread on the current processor wakes up sometimes you do not want the work to execute immediately but instead after some delay in those cases you can schedule work to execute at a given time in the future work delay in this case the represented by work will not execute for at least delay timer ticks into the future using ticks as a unit of time is covered in chapter flushing work queued work is executed when the worker thread next wakes up sometimes you need to ensure that a given batch of work has completed before continuing this is especially important for modules which almost certainly want to call this function before unload ing other places in the kernel also might need to make certain no work is pending to prevent race conditions for these needs there is a function to flush a given work queue void void this function waits until all entries in the queue are executed before returning while waiting for any pending work to execute the function sleeps therefore you can call it only from process context note that this function does not cancel any delayed work that is any work that was scheduled via and whose delay is not yet up is not flushed via to cancel delayed work call int struct work this function cancels the pending work if any associated with the given creating new work queues if the default queue is insufficient for your needs you can create a new work queue and corresponding worker threads because this creates one worker thread per processor you should create unique work queues only if your code needs the performance of a unique set of threads you create a new work queue and the associated worker threads via a simple function struct const char name the parameter name is used to name the kernel threads for example the default events queue is created via struct create_workqueue events this function creates all the worker threads one for each processor in the system and prepares them to handle work creating work is handled in the same manner regardless of the queue type after the work is created the following functions are analogous to and except that they work on the given work queue and not the default events queue int struct wq struct work int struct wq struct work unsigned long delay finally you can flush a wait queue via a call to the function struct wq as previously discussed this function works identically to except that it waits for the given queue to empty before returning the old task queue mechanism like the bh interface which gave way to softirqs and tasklets the work queue interface grew out of shortcomings in the task queue interface the task queue interface often called simply tq in the kernel like tasklets also has nothing to do with tasks in the process sense the users of the task queue interface were ripped in half during the development kernel half of the users were converted to tasklets whereas the other half continued using the task queue interface what was left of the task queue interface then became the work queue interface briefly looking at task queues which were around for some time is a useful historical exercise task queues work by defining a bunch of queues the queues have names such as the scheduler queue the immediate queue or the timer queue each queue is run at a specific point in the kernel a kernel thread keventd ran the work associated with the scheduler queue this was the precursor to the full work queue interface the timer queue was run at each tick of the system timer and the immediate queue was run in a handful of differ ent places to ensure it was run immediately hack there were other queues too addi tionally you could dynamically create new queues all this might sound useful but the reality is that the task queue interface was a mess all the queues were essentially arbitrary abstractions scattered about the kernel as if thrown in the air and kept where they landed the only meaningful queue was the sched uler queue which provided the only way to defer work to process context the other good thing about task queues was the brain dead simple interface despite the myriad of queues and the arbitrary rules about when they ran the interface was as simple as possible but that about it the rest of task queues needed to go bottom half names are apparently a conspiracy to confuse new kernel developers seriously these names are awful the various task queue users were converted to other bottom half mechanisms most of them switched to tasklets the scheduler queue users stuck around finally the keventd code was generalized into the excellent work queue mechanism we have today and task queues were finally ripped out of the kernel which bottom half should i use the decision over which bottom half to use is important in the current kernel you have three choices softirqs tasklets and work queues tasklets are built on softirqs and therefore both are similar the work queue mechanism is an entirely different creature and is built on kernel threads softirqs by design provide the least serialization this requires softirq handlers to go through extra steps to ensure that shared data is safe because two or more softirqs of the same type may run concurrently on different processors if the code in question is already highly threaded such as in a networking subsystem that is chest deep in per processor variables softirqs make a good choice they are certainly the fastest alternative for timing critical and high frequency uses tasklets make more sense if the code is not finely threaded they have a simpler inter face and because two tasklets of the same type might not run concurrently they are easier to implement tasklets are effectively softirqs that do not run concurrently a driver devel oper should always choose tasklets over softirqs unless prepared to utilize per processor variables or similar magic to ensure that the softirq can safely run concurrently on multi ple processors if your deferred work needs to run in process context your only choice of the three is work queues if process context is not a requirement specifically if you have no need to sleep softirqs or tasklets are perhaps better suited work queues involve the highest over head because they involve kernel threads and therefore context switching this is not to say that they are inefficient but in light of thousands of interrupts hitting per second as the networking subsystem might experience other methods make more sense for most situations however work queues are sufficient in terms of ease of use work queues take the crown using the default events queue is child play next come tasklets which also have a simple interface coming in last are softirqs which need to be statically created and require careful thinking with their implementation table is a comparison between the three bottom half interfaces table bottom half comparison bottom half context inherent serialization softirq interrupt none tasklet interrupt against the same tasklet work queues process none scheduled as process context in short normal driver writers have two choices first do you need a schedulable entity to perform your deferred work fundamentally do you need to sleep for any rea son then work queues are your only option otherwise tasklets are preferred only if scalability becomes a concern do you investigate softirqs locking between the bottom halves we have not discussed locking yet which is such a fun and expansive topic that we devote the next two chapters to it nonetheless you need to understand that it is crucial to protect shared data from concurrent access while using bottom halves even on a single processor machine remember a bottom half can run at virtually any moment you might want to come back to this section after reading the next two chapters if the concept of locking is foreign to you one of the benefits of tasklets is that they are serialized with respect to themselves the same tasklet will not run concurrently even on two different processors this means you do not have to worry about intra tasklet concurrency issues inter tasklet concur rency that is when two different tasklets share the same data requires proper locking because softirqs provide no serialization even two instances of the same softirq might run simultaneously all shared data needs an appropriate lock if process context code and a bottom half share data you need to disable bottom half processing and obtain a lock before accessing the data doing both ensures local and smp protection and prevents a deadlock if interrupt context code and a bottom half share data you need to disable interrupts and obtain a lock before accessing the data this also ensures both local and smp protec tion and prevents a deadlock any shared data in a work queue requires locking too the locking issues are no dif ferent from normal kernel code because work queues run in process context chapter an introduction to kernel synchronization provides a background on the issues surrounding concurrency and chapter covers the kernel locking primitives these chapters cover how to protect data that bottom halves use disabling bottom halves normally it is not sufficient to only disable bottom halves more often to safely protect shared data you need to obtain a lock and disable bottom halves such methods which you might use in a driver are covered in chapter if you are writing core kernel code however you might need to disable just the bottom halves to disable all bottom half processing specifically all softirqs and thus all tasklets call to enable bottom half processing call yes the function is misnamed no one bothered to change the name when the bh interface gave way to softirqs table is a summary of these functions table bottom half control methods method description void disables softirq and tasklet processing on the local processor void enables softirq and tasklet processing on the local processor the calls can be nested only the final call to actually enables bottom halves for example the first time is called local softirq processing is disabled if is called three more times local processing remains disabled processing is not reenabled until the fourth call to the functions accomplish this by maintaining a per task counter via the interestingly the same counter used by kernel preemption when the counter reaches zero bottom half processing is possible because bottom halves were dis abled also checks for any pending bottom halves and executes them the functions are unique to each supported architecture and are usually written as complicated macros in asm softirq h the following are close c representations for the curious disable local bottom halves by incrementing the void void struct t t decrement the this will automatically enable bottom halves if the count returns to zero optionally run any bottom halves that are pending void void this counter is used both by the interrupt and bottom half subsystems thus in linux a single per task counter represents the atomicity of a task this has proven useful for work such as debugging sleeping while atomic bugs struct t t is zero and are any bottom halves pending if so run them if unlikely t do_softirq these calls do not disable the execution of work queues because work queues run in process context there are no issues with asynchronous execution and thus there is no need to disable them because softirqs and tasklets can occur asynchronously say on return from handling an interrupt however kernel code may need to disable them with work queues on the other hand protecting shared data is the same as in any process con text chapters and give the details conclusion in this chapter we covered the three mechanisms used to defer work in the linux kernel softirqs tasklets and work queues we went over their design and implementation we dis cussed how to use them in your own code and we insulted their poorly conceived names for historical completeness we also looked at the bottom half mechanisms that existed in previous versions of the linux kernel bh and task queues we talked a lot in this chapter about synchronization and concurrency because such topics apply quite a bit to bottom halves we even wrapped up the chapter with a discus sion on disabling bottom halves for reasons of concurrency protection it is now time to dive head first into these topics chapter discusses kernel synchronization and concur rency in the abstract providing a foundation for understanding the issues at the heart of the problem chapter discusses the specific interfaces provided by our beloved kernel to solve these problems armed with the next two chapters the world is your oyster an introduction to kernel synchronization in a shared memory application developers must ensure that shared resources are pro tected from concurrent access the kernel is no exception shared resources require pro tection from concurrent access because if multiple threads of access and manipulate the data at the same time the threads may overwrite each other changes or access data while it is in an inconsistent state concurrent access of shared data is a recipe for instability that often proves hard to track down and debug getting it right at the start is important properly protecting shared resources can be tough years ago before linux supported symmetrical multiprocessing preventing concurrent access of data was simple because only a single processor was supported the only way data could be concurrently accessed was if an interrupt occurred or if kernel code explicitly rescheduled and enabled another task to run with earlier kernels development was simple those halcyon days are over symmetrical multiprocessing support was introduced in the kernel and has been continually enhanced ever since multiprocessing support implies that kernel code can simultaneously run on two or more processors conse quently without protection code in the kernel running on two different processors can simultaneously access shared data at exactly the same time with the introduction of the kernel the linux kernel is preemptive this implies that again in the absence of pro tection the scheduler can preempt kernel code at virtually any point and reschedule another task today a number of scenarios enable for concurrency inside the kernel and they all require protection the term threads of execution implies any instance of executing code this includes for example a task in the kernel an interrupt handler a bottom half or a kernel thread this chapter may shorten threads of execution to simply threads keep in mind that this term describes any executing code this chapter discusses the issues of concurrency and synchronization in the abstract as they exist in any operating system kernel the next chapter details the specific mecha nisms and interfaces that the linux kernel provides to solve synchronization issues and prevent race conditions critical regions and race conditions code paths that access and manipulate shared data are called critical regions also called critical sections it is usually unsafe for multiple threads of execution to access the same resource simultaneously to prevent concurrent access during critical regions the pro grammer must ensure that code executes atomically that is operations complete without interruption as if the entire critical region were one indivisible instruction it is a bug if it is possible for two threads of execution to be simultaneously executing within the same critical region when this does occur we call it a race condition so named because the threads raced to get there first note how rare a race condition in your code might mani fest itself debugging race conditions is often difficult because they are not easily repro ducible ensuring that unsafe concurrency is prevented and that race conditions do not occur is called synchronization why do we need protection to best understand the need for synchronization let look at the ubiquity of race condi tions for a first example let consider a real world case an atm automated teller machine called a cash machine cashpoint or abm outside of the united states one of the most common functions performed by cash machines is withdrawing money from an individual personal bank account a person walks up to the machine inserts an atm card types in a pin selects withdrawal inputs a pecuniary amount hits ok takes the money and mails it to me after the user has asked for a specific amount of money the cash machine needs to ensure that the money actually exists in that user account if the money exists it then needs to deduct the withdrawal from the total funds available the code to implement this would look something like int total total funds in account int withdrawal amount user asked to withdrawal check whether the user has enough funds in her account if total withdrawal error you do not have that much money return ok the user has enough money deduct the withdrawal amount from her total total withdrawal total give the user their money withdrawal now let presume that another deduction in the user funds is happening at the same time it does not matter how the simultaneous deduction is happening assume that the user spouse is initiating another withdrawal at another atm a payee is electronically trans ferring funds out of the account or the bank is deducting a fee from the account as banks these days are so wont to do any of these scenarios fits our example both systems performing the withdrawal would have code similar to what we just looked at first check whether the deduction is possible then compute the new total funds and finally execute the physical deduction now let make up some numbers pre sume that the first deduction is a withdrawal from an atm for and that the second deduction is the bank applying a fee of because the customer walked into the bank assume the customer has a total of in the bank obviously one of these transactions cannot correctly complete without sending the account into the red what you would expect is something like this the fee transaction happens first ten dollars is less than so is subtracted from to get a new total of and is pocketed by the bank then the atm withdrawal comes along and fails because is less than with race conditions life can be much more interesting assume that the two transac tions are initiated at roughly the same time both transactions verify that sufficient funds exist is more than both and so all is good then the withdrawal process subtracts from yielding the fee transaction then does the same subtracting from and getting the withdrawal process then updates the user new total available funds to now the fee transaction also updates the new total resulting in free money clearly financial institutions must ensure that this can never happen they must lock the account during certain operations making each transaction atomic with respect to any other transaction such transactions must occur in their entirety without interrup tion or not occur at all the single variable now let look at a specific computing example consider a simple shared resource a sin gle global integer and a simple critical region the operation of merely incrementing it i this might translate into machine instructions to the computer processor that resem ble the following get the current value of i and copy it into a register add one to the value stored in the register write back to memory the new value of i now assume that there are two threads of execution both enter this critical region and the initial value of i is the desired outcome is then similar to the following with each row representing a unit of time thread thread get i increment i write back i get i increment i write back i as expected incremented twice is a possible outcome however is the following thread thread get i get i increment i increment i write back i write back i if both threads of execution read the initial value of i before it is incremented both threads increment and save the same value as a result the variable i contains the value when in fact it should now contain this is one of the simplest examples of a critical region thankfully the solution is equally as simple we merely need a way to perform these operations in one indivisible step most processors provide an instruction to atomi cally read increment and write back a single variable using this atomic instruction the only possible outcome is thread thread increment store i increment store i 9 or conversely thread thread increment store increment store 9 it would never be possible for the two atomic operations to interleave the processor would physically ensure that it was impossible using such an instruction would alleviate the problem the kernel provides a set of interfaces that implement these atomic instruc tions they are discussed in the next chapter locking now let consider a more complicated race condition that requires a more complicated solution assume you have a queue of requests that needs to be serviced for this exercise let assume the implementation is a linked list in which each node represents a request two functions manipulate the queue one function adds a new request to the tail of the queue another function removes a request from the head of the queue and does some thing useful with the request various parts of the kernel invoke these two functions thus requests are continually being added removed and serviced manipulating the request queues certainly requires multiple instructions if one thread attempts to read from the queue while another is in the middle of manipulating it the reading thread will find the queue in an inconsistent state it should be apparent the sort of damage that could occur if access to the queue could occur concurrently often when the shared resource is a complex data structure the result of a race condition is corruption of the data structure the previous scenario at first might not have a clear solution how can you prevent one processor from reading from the queue while another processor is updating it although it is feasible for a particular architecture to implement simple instructions such as arithmetic and comparison atomically it is ludicrous for architectures to provide instructions to support the indefinitely sized critical regions that would exist in the previ ous example what is needed is a way of making sure that only one thread manipulates the data structure at a time a mechanism for preventing access to a resource while another thread of execution is in the marked region a lock provides such a mechanism it works much like a lock on a door imagine the room beyond the door as the critical region inside the room only one thread of execu tion can be present at a given time when a thread enters the room it locks the door behind it when the thread is finished manipulating the shared data it leaves the room and unlocks the door if another thread reaches the door while it is locked it must wait for the thread inside to exit the room and unlock the door before it can enter threads hold locks locks protect data in the previous request queue example a single lock could have been used to protect the queue whenever there was a new request to add to the queue the thread would first obtain the lock then it could safely add the request to the queue and ultimately release the lock when a thread wanted to remove a request from the queue it too would obtain the lock then it could read the request and remove it from the queue finally it would release the lock any other access to the queue would similarly need to obtain the lock because the lock can be held by only one thread at a time only a single thread can manipulate the queue at a time if a thread comes along while another thread is already updating it the second thread has to wait for the first to release the lock before it can continue the lock prevents concurrency and protects the queue from race conditions any code that accesses the queue first needs to obtain the relevant lock if another thread of execution comes along the lock prevents concurrency thread thread try to lock the queue try to lock the queue succeeded acquired lock failed waiting access queue waiting unlock the queue waiting succeeded acquired lock access queue unlock the queue notice that locks are advisory and voluntary locks are entirely a programming con struct that the programmer must take advantage of nothing prevents you from writing code that manipulates the fictional queue without the appropriate lock such a practice of course would eventually result in a race condition and corruption locks come in various shapes and sizes linux alone implements a handful of differ ent locking mechanisms the most significant difference between the various mechanisms is the behavior when the lock is unavailable because another thread already holds it some lock variants busy wait whereas other locks put the current task to sleep until the lock becomes available the next chapter discusses the behavior of the different locks in linux and their interfaces astute readers are now screaming the lock does not solve the problem it simply shrinks the critical region down to just the lock and unlock code probably much smaller sure but still a potential race fortunately locks are implemented using atomic operations that ensure no race exists a single instruction can verify whether the key is taken and if not seize it how this is done is architecture specific but almost all processors implement an atomic test and set instruction that tests the value of an integer and sets it to a new value only if it is zero a value of zero means unlocked on the popular architecture locks are implemented using such a similar instruction called compare and exchange that is spin in a tight loop checking the status of the lock over and over waiting for the lock to become available causes of concurrency in user space the need for synchronization stems from the fact that programs are sched uled preemptively at the will of the scheduler because a process can be preempted at any time and another process can be scheduled onto the processor a process can be involun tarily preempted in the middle of accessing a critical region if the newly scheduled process then enters the same critical region say if the two processes manipulate the same shared memory or write to the same file descriptor a race can occur the same problem can occur with multiple single threaded processes sharing files or within a single program with signals because signals can occur asynchronously this type of concurrency in which two things do not actually happen at the same time but interleave with each other such that they might as well is called pseudo concurrency if you have a symmetrical multiprocessing machine two processes can actually be exe cuted in a critical region at the exact same time that is called true concurrency although the causes and semantics of true versus pseudo concurrency are different they both result in the same race conditions and require the same sort of protection the kernel has similar causes of concurrency n interrupts an interrupt can occur asynchronously at almost any time inter rupting the currently executing code n softirqs and tasklets the kernel can raise or schedule a softirq or tasklet at almost any time interrupting the currently executing code n kernel preemption because the kernel is preemptive one task in the kernel can preempt another n sleeping and synchronization with user space a task in the kernel can sleep and thus invoke the scheduler resulting in the running of a new process n symmetrical multiprocessing two or more processors can execute kernel code at exactly the same time kernel developers need to understand and prepare for these causes of concurrency it is a major bug if an interrupt occurs in the middle of code that is manipulating a resource and the interrupt handler can access the same resource similarly it is a bug if kernel code is preemptive while it is accessing a shared resource likewise it is a bug if code in the kernel sleeps while in the middle of a critical section finally two processors should never simultaneously access the same piece of data with a clear picture of what data needs pro tection it is not hard to provide the locking to keep the system stable rather the hard part is identifying these conditions and realizing that to prevent concurrency you need some form of protection let us reiterate this point because it is important implementing the actual locking in your code to protect shared data is not difficult especially when done early on during the design phase of development the tricky part is identifying the actual shared data and the corresponding critical sections this is why designing locking into your code from the get go and not as an afterthought is of paramount importance it can be difficult to go in ex post and identify critical regions and retrofit locking into the existing code the resulting code is often not pretty either the takeaway from this is to always design proper locking into your code from the beginning code that is safe from concurrent access from an interrupt handler is said to be interrupt safe code that is safe from concurrency on symmetrical multiprocessing machines is smp safe code that is safe from concurrency with kernel preemption is preempt safe the actual mechanisms used to provide synchronization and protect against race conditions in all these cases is covered in the next chapter knowing what to protect identifying what data specifically needs protection is vital because any data that can be accessed concurrently almost assuredly needs protection it is often easier to identify what data does not need protection and work from there obviously any data that is local to one particular thread of execution does not need protection because only that thread can access the data for example local automatic variables and dynamically allocated data structures whose address is stored only on the stack do not need any sort of locking because they exist solely on the stack of the executing thread likewise data that is accessed by only a specific task does not require locking because a process can execute on only one processor at a time what does need locking most global kernel data structures do a good rule of thumb is that if another thread of execution can access the data the data needs some sort of locking if anyone else can see it lock it remember to lock data not code config options smp versus up because the linux kernel is configurable at compile time it makes sense that you can tailor the kernel specifically for a given machine most important the configure option controls whether the kernel supports smp many locking issues disappear on uniprocessor machines consequently when is unset unnecessary code is not compiled into the kernel image for example such configuration enables uniprocessor machines to forego the overhead of spin locks the same trick applies to the configure option enabling kernel preemption this was an excellent design decision the kernel maintains one clean source base and the various locking mechanisms are used as needed different combinations of and on different archi tectures compile in varying lock support in your code provide appropriate protection for the most pessimistic case smp with kernel preemption and all scenarios will be covered you will also see that barring a few exceptions being smp safe implies being preempt safe whenever you write kernel code you should ask yourself these questions n is the data global can a thread of execution other than the current one access it n is the data shared between process context and interrupt context is it shared between two different interrupt handlers n if a process is preempted while accessing this data can the newly scheduled process access the same data n can the current process sleep block on anything if it does in what state does that leave any shared data n what prevents the data from being freed out from under me n what happens if this function is called again on another processor n given the proceeding points how am i going to ensure that my code is safe from concurrency in short nearly all global and shared data in the kernel requires some form of the synchronization methods discussed in the next chapter deadlocks a deadlock is a condition involving one or more threads of execution and one or more resources such that each thread waits for one of the resources but all the resources are already held the threads all wait for each other but they never make any progress toward releasing the resources that they already hold therefore none of the threads can con tinue which results in a deadlock a good analogy is a four way traffic stop if each car at the stop decides to wait for the other cars before going no car will ever proceed and we have a traffic deadlock the simplest example of a deadlock is the self deadlock if a thread of execution attempts to acquire a lock it already holds it has to wait for the lock to be released but it will never release the lock because it is busy waiting for the lock and the result is deadlock acquire lock acquire lock again wait for lock to become available some kernels prevent this type of deadlock by providing recursive locks these are locks that a single thread of execution may acquire multiple times linux thankfully does not provide recursive locks this is widely considered a good thing although recursive locks might alleviate the self deadlock problem they very readily lead to sloppy locking semantics similarly consider n threads and n locks if each thread holds a lock that the other thread wants all threads block while waiting for their respective locks to become avail able the most common example is with two threads and two locks which is often called the deadly embrace or the abba deadlock thread thread acquire lock a acquire lock b try to acquire lock b try to acquire lock a wait for lock b wait for lock a each thread is waiting for the other and neither thread will ever release its original lock therefore neither lock will become available prevention of deadlock scenarios is important although it is difficult to prove that code is free of deadlocks you can write deadlock free code a few simple rules go a long way n implement lock ordering nested locks must always be obtained in the same order this prevents the deadly embrace deadlock document the lock ordering so others will follow it n prevent starvation ask yourself does this code always finish if foo does not occur will bar wait forever n do not double acquire the same lock n design for simplicity complexity in your locking scheme invites deadlocks the first point is most important and worth stressing if two or more locks are acquired at the same time they must always be acquired in the same order let assume you have the cat dog and fox locks that protect data structures of the same name now assume you have a function that needs to work on all three of these data structures simul taneously perhaps to copy data between them whatever the case the data structures require locking to ensure safe access if one function acquires the locks in the order cat dog and then fox then every other function must obtain these locks or a subset of them in this same order for example it is a potential deadlock and hence a bug to first obtain the fox lock and then obtain the dog lock because the dog lock must always be acquired prior to the fox lock here is an example in which this would cause a deadlock thread thread acquire lock cat acquire lock fox acquire lock dog try to acquire lock dog try to acquire lock fox wait for lock dog wait for lock fox thread one is waiting for the fox lock which thread two holds while thread two is waiting for the dog lock which thread one holds neither ever releases its lock and hence both wait forever bam deadlock if the locks were always obtained in the same order a deadlock in this manner would not be possible whenever locks are nested within other locks a specific ordering must be obeyed it is good practice to place the ordering in a comment above the lock something like the fol lowing is a good idea locks access to the cat structure always obtain before the dog lock the order of unlock does not matter with respect to deadlock although it is common practice to release the locks in an order inverse to that in which they were acquired preventing deadlocks is important the linux kernel has some basic debugging facili ties for detecting deadlock scenarios in a running kernel these features are discussed in the next chapter contention and scalability the term lock contention or simply contention describes a lock currently in use but that another thread is trying to acquire a lock that is highly contended often has threads waiting to acquire it high contention can occur because a lock is frequently obtained held for a long time after it is obtained or both because a lock job is to serialize access to a resource it comes as no surprise that locks can slow down a system performance a highly contended lock can become a bottleneck in the system quickly limiting its per formance of course the locks are also required to prevent the system from tearing itself to shreds so a solution to high contention must continue to provide the necessary concurrency protection scalability is a measurement of how well a system can be expanded in operating sys tems we talk of the scalability with a large number of processes a large number of processors or large amounts of memory we can discuss scalability in relation to virtually any component of a computer to which we can attach a quantity ideally doubling the number of processors should result in a doubling of the system processor performance this of course is never the case the scalability of linux on a large number of processors has increased dramatically in the time since multiprocessing support was introduced in the kernel in the early days of linux multiprocessing support only one task could execute in the kernel at a time during this limitation was removed as the locking mechanisms grew more fine grained through and onward kernel locking became even finer grained today in the 6 linux kernel kernel locking is very fine grained and scalability is good the granularity of locking is a description of the size or amount of data that a lock protects a very coarse lock protects a large amount of data for example an entire sub system set of data structures on the other hand a very fine grained lock protects a small amount of data say only a single element in a larger structure in reality most locks fall somewhere in between these two extremes protecting neither an entire subsystem nor an individual element but perhaps a single structure or list of structures most locks start off fairly coarse and are made more fine grained as lock contention proves to be a problem one example of evolving to finer grained locking is the scheduler runqueues dis cussed in chapter process scheduling in and prior kernels the scheduler had a single runqueue recall that a runqueue is the list of runnable processes early in the 6 series the o scheduler introduced per processor runqueues each with a unique lock the locking evolved from a single global lock to separate locks for each processor this was an important optimization because the runqueue lock was highly contended on large machines essentially serializing the entire scheduling process down to a single processor executing in the scheduler at a time later in the 6 series the cfs scheduler improved scalability further generally this scalability improvement is a good thing because it improves linux per formance on larger and more powerful systems rampant scalability improvements can lead to a decrease in performance on smaller smp and up machines however because smaller machines may not need such fine grained locking but will nonetheless need to put up with the increased complexity and overhead consider a linked list an initial locking scheme would provide a single lock for the entire list in time this single lock might prove to be a scalability bottleneck on large multiprocessor machines that fre quently access this linked list in response the single lock could be broken up into one lock per node in the linked list for each node that you wanted to read or write you obtained the node unique lock now there is only lock contention when multiple processors are accessing the same exact node what if there is still lock contention how ever do you provide a lock for each element in each node each bit of each element the answer is no even though this fine grained locking might ensure excellent scalability on large smp machines how does it perform on dual processor machines the overhead of all those extra locks is wasted if a dual processor machine does not see significant lock contention to begin with nonetheless scalability is an important consideration designing your locking from the beginning to scale well is important coarse locking of major resources can easily become a bottleneck on even small machines there is a thin line between too coarse locking and too fine locking locking that is too coarse results in poor scalability if there is high lock contention whereas locking that is too fine results in wasteful overhead if there is little lock contention both scenarios equate to poor performance start simple and grow in com plexity only as needed simplicity is key conclusion making your code smp safe is not something that can be added as an afterthought proper synchronization locking that is free of deadlocks scalable and clean requires design decisions from start through finish whenever you write kernel code whether it is a new system call or a rewritten driver protecting data from concurrent access needs to be a primary concern provide sufficient protection for every scenario smp kernel preemption and so on and rest assured the data will be safe on any given machine and configuration the next chapter discusses just how to do this with the fundamentals and the theories of synchronization concurrency and locking behind us let now dive into the actual tools that the linux kernel provides to ensure that your code is race and deadlock free kernel synchronization methods the previous chapter discussed the sources of and solutions to race conditions thank fully the linux kernel provides a family of synchronization methods the linux kernel synchronization methods enable developers to write efficient and race free code this chapter discusses these methods and their interfaces behavior and use atomic operations we start our discussion of synchronization methods with atomic operations because they are the foundation on which other synchronization methods are built atomic operations provide instructions that execute atomically without interruption just as the atom was originally thought to be an indivisible particle atomic operators are indivisible instruc tions for example as discussed in the previous chapter an atomic increment can read and increment a variable by one in a single indivisible and uninterruptible step recall the simple race in incrementing an integer that we discussed in the previous chapter thread thread get i get i increment i increment i write back i write back i with atomic operators this race does not indeed cannot occur instead the out come is always one of the following thread thread get increment and store i get increment and store i 9 or thread thread get increment and store i get increment and store i 9 the ultimate value always nine is correct it is never possible for the two atomic oper ations to occur on the same variable concurrently therefore it is not possible for the in crements to race the kernel provides two sets of interfaces for atomic operations one that operates on integers and another that operates on individual bits these interfaces are implemented on every architecture that linux supports most architectures contain instructions that pro vide atomic versions of simple arithmetic operations other architectures lacking direct atomic operations provide an operation to lock the memory bus for a single operation thus guaranteeing that another memory affecting operation cannot occur simultaneously atomic integer operations the atomic integer methods operate on a special data type this special type is used as opposed to having the functions work directly on the c int type for several rea sons first having the atomic functions accept only the type ensures that the atomic operations are used only with these special types likewise it also ensures that the data types are not passed to any nonatomic functions indeed what good would atomic operations be if they were not consistently used on the data next the use of ensures the compiler does not erroneously but cleverly optimize access to the value it is important the atomic operations receive the correct memory address and not an alias finally use of can hide any architecture specific differences in its implementa tion the type is defined in linux types h typedef struct volatile int counter despite being an integer and thus bits on all the machines that linux supports de velopers and their code once had to assume that an was no larger than bits in size the sparc port in linux has an odd implementation of atomic operations a lock was embedded in the lower 8 bits of the bit int it looked like figure the lock was used to protect concurrent access to the atomic type because the sparc archi tecture lacks appropriate support at the instruction level consequently only usable bits were available on sparc machines although code that assumed that the full bit range existed would work on other machines it would have failed in strange and subtle ways on sparc machines and that is just rude recently clever hacks have allowed sparc to provide a fully usable bit and this limitation is no more bit bit figure old layout of the bit on sparc the declarations needed to use the atomic integer operations are in asm atomic h some architectures provide additional methods that are unique to that architecture but all architectures provide at least a minimum set of operations that are used throughout the kernel when you write kernel code you can ensure that these operations are correctly implemented on all architectures defining an is done in the usual manner optionally you can set it to an ini tial value v define v u define u and initialize it to zero operations are all simple v v atomically v v v 6 atomically v v v atomically if you ever need to convert an to an int use printk d n v will print a common use of the atomic integer operations is to implement counters protecting a sole counter with a complex locking scheme is overkill so instead developers use and which are much lighter in weight another use of the atomic integer operators is atomically performing an operation and testing the result a common example is the atomic decrement and test int v this function decrements by one the given atomic value if the result is zero it returns true otherwise it returns false a full listing of the standard atomic integer operations those found on all architectures is in table all the operations implemented on a specific architecture can be found in asm atomic h table atomic integer methods atomic integer operation description int i at declaration initialize to i int v atomically read the integer value of v void v int i atomically set v equal to i void int i v atomically add i to v void int i v atomically subtract i from v void v atomically add one to v void v atomically subtract one from v int int i v atomically subtract i from v and return true if the result is zero otherwise false int int i v atomically add i to v and return true if the result is negative otherwise false int int i v atomically add i to v and return the result int int i v atomically subtract i from v and return the result int int i v atomically increment v by one and return the result int int i v atomically decrement v by one and return the result int v atomically decrement v by one and return true if zero false otherwise int v atomically increment v by one and return true if the result is zero false otherwise the atomic operations are typically implemented as inline functions with inline as sembly in the case where a specific function is inherently atomic the given function is usually just a macro for example on most architectures a word sized read is always atomic that is a read of a single word cannot complete in the middle of a write to that word the read always returns the word in a consistent state either before or after the write completes but never in the middle consequently is usually just a macro returning the integer value of the read atomic variable v pointer of type atomically reads the value of v static inline int const v return v counter atomicity versus ordering the preceding discussion on atomic reading begs a discussion on the differences between atomicity and ordering as discussed a word sized read always occurs atomically it never in terleaves with a write to the same word the read always returns the word in a consistent state perhaps before the write completes perhaps after but never during for example if an integer is initially and then set to a read on the integer always returns or and never some commingling of the two values we call this atomicity your code however might have more stringent requirements than this perhaps you require that the read always occurs before the pending write this type of requirement is not atomic ity but ordering atomicity ensures that instructions occur without interruption and that they complete either in their entirety or not at all ordering on the other hand ensures that the desired relative ordering of two or more instructions even if they are to occur in separate threads of execution or even separate processors is preserved the atomic operations discussed in this section guarantee only atomicity ordering is en forced via barrier operations which we discuss later in this chapter in your code it is usually preferred to choose atomic operations over more compli cated locking mechanisms on most architectures one or two atomic operations incur less overhead and less cache line thrashing than a more complicated synchronization method as with any performance sensitive code however testing multiple approaches is always smart bit atomic operations with the rising prevalence of bit architectures it is no surprise that the linux kernel developers augmented the bit type with a bit variant for portability the size of cannot change between architectures so is bit even on bit architectures instead the type provides a bit atomic integer that functions otherwise identical to its bit brother usage is exactly the same except that the usable range of the integer is rather than bits nearly all the classic bit atomic operations are implemented in bit variants they are prefixed with in lieu of atomic table is a full listing of the standard operations some archi tectures implement more but they are not portable as with the type is just a simple wrapper around an integer this type a long typedef struct volatile long counter table atomic integer methods atomic integer operation description long i at declaration initialize to i long v atomically read the integer value of v void v int i atomically set v equal to i void int i v atomically add i to v void int i v atomically subtract i from v void v atomically add one to v void v atomically subtract one from v int int i v atomically subtract i from v and return true if the result is zero otherwise false int int i v atomically add i to v and return true if the result is negative otherwise false long int i v atomically add i to v and return the result long int i v atomically subtract i from v and return the result long int i v atomically increment v by one and return the result long int i v atomically decrement v by one and return the result int v atomically decrement v by one and return true if zero false otherwise int v atomically increment v by one and return true if the result is zero false otherwise all bit architectures provide and a family of arithmetic functions to operate on it most bit architectures do not however support x86 is a notable exception for portability between all linux supported architectures develop ers should use the bit type the bit is reserved for code that is both architecture specific and that requires bits atomic bitwise operations in addition to atomic integer operations the kernel also provides a family of functions that operate at the bit level not surprisingly they are architecture specific and defined in asm bitops h what might be surprising is that the bitwise functions operate on generic memory ad dresses the arguments are a pointer and a bit number bit zero is the least significant bit of the given address on bit machines bit is the most significant bit and bit is the least significant bit of the following word there are no limitations on the bit number supplied although most uses of the functions provide a word and consequently a bit number between and on bit machines and and on bit machines because the functions operate on a generic pointer there is no equivalent of the atomic integer type instead you can work with a pointer to whatever data you want consider an example unsigned long word 0 word bit zero is now set atomically word bit one is now set atomically printk ul n word will print 1 word bit one is now unset atomically 0 word bit zero is flipped now it is unset atomically atomically sets bit zero and returns the previous value zero if 0 word never true the following is legal you can mix atomic bit instructions with normal c word 7 a listing of the standard atomic bit operations is in table table atomic bitwise methods atomic bitwise operation description void int nr void addr atomically set the nr th bit starting from addr void int nr void addr atomically clear the nr th bit starting from addr void int nr void addr atomically flip the value of the nr th bit starting from addr int int nr void addr atomically set the nr th bit starting from addr and return the previous value int int nr void addr atomically clear the nr th bit starting from addr and return the previous value int int nr void addr atomically flip the nr th bit starting from addr and return the previous value int int nr void addr atomically return the value of the nr th bit starting from addr conveniently nonatomic versions of all the bitwise functions are also provided they behave identically to their atomic siblings except they do not guarantee atomicity and their names are prefixed with double underscores for example the nonatomic form of is if you do not require atomicity say for example because a lock already protects your data these variants of the bitwise functions might be faster what the heck is a nonatomic bit operation on first glance the concept of a nonatomic bit operation might not make any sense only a single bit is involved thus there is no possibility of inconsistency if one of the operations succeeds what else could matter sure ordering might be important but we are talking about atomicity here at the end of the day if the bit has a value provided by any of the in structions we should be good to go right let jump back to just what atomicity means atomicity requires that either instructions succeed in their entirety uninterrupted or instructions fail to execute at all therefore if you issue two atomic bit operations you expect two operations to succeed after both opera tions complete the bit needs to have the value as specified by the second operation more over however at some point in time prior to the final operation the bit needs to hold the value as specified by the first operation put more generally real atomicity requires that all intermediate states be correctly realized for example assume you issue two atomic bit operations initially set the bit and then clear the bit without atomic operations the bit might end up cleared but it might never have been set the set operation could occur simultaneously with the clear operation and fail the clear operation would succeed and the bit would emerge cleared as intended with atomic operations however the set would actually occur there would be a moment in time when a read would show the bit as set and then the clear would execute and the bit would be zero this behavior can be important especially when ordering comes into play or when dealing with hardware registers the kernel also provides routines to find the first set or unset bit starting at a given address int unsigned long addr unsigned int size int unsigned long addr unsigned int size both functions take a pointer as their first argument and the number of bits in total to search as their second they return the bit number of the first set or first unset bit respec tively if your code is searching only a word the routines ffs and ffz which take a single parameter of the word in which to search are optimal unlike the atomic integer operations code typically has no choice whether to use the bitwise operations they are the only portable way to set a specific bit the only question is whether to use the atomic or nonatomic variants if your code is inherently safe from race conditions you can use the nonatomic versions which might be faster depending on the architecture spin locks although it would be nice if every critical region consisted of code that did nothing more complicated than incrementing a variable reality is much crueler in real life critical re gions can span multiple functions for example it is often the case that data must be re moved from one structure formatted and parsed and added to another structure this entire operation must occur atomically it must not be possible for other code to read from or write to either structure before the update is completed because simple atomic operations are clearly incapable of providing the needed protection in such a complex sce nario a more general method of synchronization is needed locks the most common lock in the linux kernel is the spin lock a spin lock is a lock that can be held by at most one thread of execution if a thread of execution attempts to ac quire a spin lock while it is already held which is called contended the thread busy loops spins waiting for the lock to become available if the lock is not contended the thread can immediately acquire the lock and continue the spinning prevents more than one thread of execution from entering the critical region at any one time the same lock can be used in multiple locations so all access to a given data structure for example can be protected and synchronized going back to the door and key analogy from the last chapter spin locks are akin to sitting outside the door waiting for the fellow inside to come out and hand you the key if you reach the door and no one is inside you can grab the key and enter the room if you reach the door and someone is currently inside you must wait outside for the key effec tively checking for its presence repeatedly when the room is vacated you can grab the key and go inside thanks to the key read spin lock only one person read thread of ex ecution is allowed inside the room read critical region at the same time the fact that a contended spin lock causes threads to spin essentially wasting processor time while waiting for the lock to become available is salient this behavior is the point of the spin lock it is not wise to hold a spin lock for a long time this is the nature of the spin lock a lightweight single holder lock that should be held for short durations an al ternative behavior when the lock is contended is to put the current thread to sleep and wake it up when it becomes available then the processor can go off and execute other code this incurs a bit of overhead most notably the two context switches required to switch out of and back into the blocking thread which is certainly a lot more code than the handful of lines used to implement a spin lock therefore it is wise to hold spin locks for less than the duration of two context switches because most of us have better things to do than measure context switches just try to hold the lock for as little time as possible 1 later in this chapter we discuss semaphores which provide a lock that makes the waiting thread sleep rather than spin when contended spin lock methods spin locks are architecture dependent and implemented in assembly the architecture dependent code is defined in asm spinlock h the actual usable interfaces are defined in linux spinlock h the basic use of a spin lock is 1 this is especially important now that the kernel is preemptive the duration that locks are held is equivalent to the scheduling latency of the system critical region the lock can be held simultaneously by at most only one thread of execution conse quently only one thread is allowed in the critical region at a time this provides the needed protection from concurrency on multiprocessing machines on uniprocessor ma chines the locks compile away and do not exist they simply act as markers to disable and enable kernel preemption if kernel preempt is turned off the locks compile away entirely warning spin locks are not recursive unlike spin lock implementations in other operating systems and threading libraries the linux kernel spin locks are not recursive this means that if you attempt to acquire a lock you already hold you will spin waiting for yourself to release the lock but because you are busy spinning you will never release the lock and you will deadlock be careful spin locks can be used in interrupt handlers whereas semaphores cannot be used be cause they sleep if a lock is used in an interrupt handler you must also disable local inter rupts interrupt requests on the current processor before obtaining the lock otherwise it is possible for an interrupt handler to interrupt kernel code while the lock is held and at tempt to reacquire the lock the interrupt handler spins waiting for the lock to become available the lock holder however does not run until the interrupt handler completes this is an example of the double acquire deadlock discussed in the previous chapter note that you need to disable interrupts only on the current processor if an interrupt occurs on a different processor and it spins on the same lock it does not prevent the lock holder which is on a different processor from eventually releasing the lock the kernel provides an interface that conveniently disables interrupts and acquires the lock usage is unsigned long flags flags critical region flags the routine saves the current state of interrupts disables them locally and then obtains the given lock conversely unlocks the given lock and returns interrupts to their previous state this way if interrupts were initially disabled your code would not erroneously enable them but instead keep them disabled note that the flags variable is seemingly passed by value this is because the lock routines are implemented partially as macros on uniprocessor systems the previous example must still disable interrupts to prevent an interrupt handler from accessing the shared data but the lock mechanism is compiled away the lock and unlock also disable and enable kernel preemption respectively what do i lock it is important that each lock is clearly associated with what it is locking more important you should protect data and not code despite the examples in this chapter explaining the importance of protecting the critical sections it is the actual data inside that needs protec tion and not the code big fat rule locks that simply wrap code regions are hard to understand and prone to race conditions lock data not code rather than lock code always associate your shared data with a specific lock for example the struct foo is locked by whenever you access shared data make sure it is safe most likely this means obtaining the appropriate lock before manipulating the data and releasing the lock when finished if you always know before the fact that interrupts are initially enabled there is no need to restore their previous state you can unconditionally enable them on unlock in those cases and are optimal critical section as the kernel grows in size and complexity it is increasingly hard to ensure that interrupts are always enabled in any given code path in the kernel use of therefore is not recommended if you do use it you had better be posi tive that interrupts were originally on or people will be upset when they expect interrupts to be off but find them on debugging spin locks the configure option enables a handful of debugging checks in the spin lock code for example with this option the spin lock code checks for the use of uninitialized spin locks and unlocking a lock that is not yet locked when testing your code you should always run with spin lock debugging enabled for additional debugging of lock lifecycles enable other spin lock methods you can use the method to initialize a dynamically created spin lock a that you do not have a direct reference to just a pointer the method attempts to obtain the given spin lock if the lock is contended rather than spin and wait for the lock to be released the function immediately returns zero if it succeeds in obtaining the lock it returns nonzero similarly returns nonzero if the given lock is currently acquired otherwise it returns zero in neither case does actually obtain the lock table 4 shows a complete list of the standard spin lock methods table 4 spin lock methods method description acquires given lock disables local interrupts and acquires given lock saves current state of local interrupts disables local inter rupts and acquires given lock releases given lock releases given lock and enables local interrupts releases given lock and restores local interrupts to given pre vious state dynamically initializes given spin_trylock tries to acquire given lock if unavailable returns nonzero returns nonzero if the given lock is currently acquired other wise it returns zero spin locks and bottom halves as discussed in chapter 8 bottom halves and deferring work certain locking precau tions must be taken when working with bottom halves the function obtains the given lock and disables all bottom halves the function performs the inverse because a bottom half might preempt process context code if data is shared between a bottom half process context you must protect the data in process context with both a lock and the disabling of bottom halves likewise because an interrupt handler might preempt a bottom half if data is shared between an interrupt handler and a bottom half you must both obtain the appropriate lock and disable interrupts 2 use of these two functions can lead to convoluted code you should not frequently have to check the values of spin locks your code should either always acquire the lock itself or always be called while the lock is already held some legitimate uses do exist however so these interfaces are provided recall that two tasklets of the same type do not ever run simultaneously thus there is no need to protect data used only within a single type of tasklet if the data is shared be tween two different tasklets however you must obtain a normal spin lock before access ing the data in the bottom half you do not need to disable bottom halves because a tasklet never preempts another running tasklet on the same processor with softirqs regardless of whether it is the same softirq type if data is shared by softirqs it must be protected with a lock recall that softirqs even two of the same type might run simultaneously on multiple processors in the system a softirq never preempts another softirq running on the same processor however so disabling bottom halves is not needed reader writer spin locks sometimes lock usage can be clearly divided into reader and writer paths for example consider a list that is both updated and searched when the list is updated written to it is important that no other threads of execution concurrently write to or read from the list writing demands mutual exclusion on the other hand when the list is searched read from it is only important that nothing else writes to the list multiple concurrent readers are safe so long as there are no writers the task list access patterns discussed in chapter process management fit this description not surprisingly a reader writer spin lock protects the task list when a data structure is neatly split into reader writer or consumer producer usage patterns it makes sense to use a locking mechanism that provides similar semantics to satisfy this use the linux kernel provides reader writer spin locks reader writer spin locks provide separate reader and writer variants of the lock one or more readers can concurrently hold the reader lock the writer lock conversely can be held by at most one writer with no concurrent readers reader writer locks are sometimes called shared exclusive or concurrent exclusive locks because the lock is available in a shared for readers and an exclusive for writers form usage is similar to spin locks the reader writer spin lock is initialized via then in the reader code path critical section read only finally in the writer code path critical section read and write mr_lock normally the readers and writers are in entirely separate code paths such as in this example note that you cannot upgrade a read lock to a write lock for example consider this code snippet mr_rwlock executing these two functions as shown will deadlock as the write lock spins waiting for all readers to release the shared lock including yourself if you ever need to write obtain the write lock from the start if the line between your readers and writers is mud dled it might be an indication that you do not need to use reader writer locks in that case a normal spin lock is optimal it is safe for multiple readers to obtain the same lock in fact it is safe for the same thread to recursively obtain the same read lock this lends itself to a useful and common optimization if you have only readers in interrupt handlers but no writers you can mix the use of the interrupt disabling locks you can use instead of for reader protection you still need to disable interrupts for write access à la otherwise a reader in an interrupt could deadlock on the held write lock see table for a full listing of the reader writer spin lock methods table reader writer spin lock methods method description acquires given lock for reading disables local interrupts and acquires given lock for reading saves the current state of local interrupts disables local in terrupts and acquires the given lock for reading releases given lock for reading releases given lock and enables local interrupts irqrestore releases given lock and restores local interrupts to the given previous state acquires given lock for writing disables local interrupts and acquires the given lock for writing saves current state of local interrupts disables local inter rupts and acquires the given lock for writing releases given lock releases given lock and enables local interrupts table reader writer spin lock methods method description continued releases given lock and restores local interrupts to given previous state tries to acquire given lock for writing if unavailable returns nonzero initializes given a final important consideration in using the linux reader writer spin locks is that they favor readers over writers if the read lock is held and a writer is waiting for exclusive ac cess readers that attempt to acquire the lock continue to succeed the spinning writer does not acquire the lock until all readers release the lock therefore a sufficient number of readers can starve pending writers this is important to keep in mind when designing your locking sometimes this behavior is beneficial sometimes it is catastrophic spin locks provide a quick and simple lock the spinning behavior is optimal for short hold times and code that cannot sleep interrupt handlers for example in cases where the sleep time might be long or you potentially need to sleep while holding the lock the semaphore is a solution semaphores semaphores in linux are sleeping locks when a task attempts to acquire a semaphore that is unavailable the semaphore places the task onto a wait queue and puts the task to sleep the processor is then free to execute other code when the semaphore becomes available one of the tasks on the wait queue is awakened so that it can then acquire the semaphore let jump back to the door and key analogy when a person reaches the door he can grab the key and enter the room the big difference lies in what happens when another dude reaches the door and the key is not available in this case instead of spinning the fel low puts his name on a list and takes a number when the person inside the room leaves he checks the list at the door if anyone name is on the list he goes over to the first name and gives him a playful jab in the chest waking him up and allowing him to enter the room in this manner the key read semaphore continues to ensure that there is only one person read thread of execution inside the room read critical region at one time this provides better processor utilization than spin locks because there is no time spent busy looping but semaphores have much greater overhead than spin locks life is always a trade off you can draw some interesting conclusions from the sleeping behavior of semaphores n because the contending tasks sleep while waiting for the lock to become available semaphores are well suited to locks that are held for a long time n conversely semaphores are not optimal for locks that are held for short periods be cause the overhead of sleeping maintaining the wait queue and waking back up can easily outweigh the total lock hold time n because a thread of execution sleeps on lock contention semaphores must be ob tained only in process context because interrupt context is not schedulable n you can although you might not want to sleep while holding a semaphore be cause you will not deadlock when another process acquires the same semaphore it will just go to sleep and eventually let you continue n you cannot hold a spin lock while you acquire a semaphore because you might have to sleep while waiting for the semaphore and you cannot sleep while holding a spin lock these facts highlight the uses of semaphores versus spin locks in most uses of sema phores there is little choice as to what lock to use if your code needs to sleep which is often the case when synchronizing with user space semaphores are the sole solution it is often easier if not necessary to use semaphores because they allow you the flexibility of sleeping when you do have a choice the decision between semaphore and spin lock should be based on lock hold time ideally all your locks should be held as briefly as pos sible with semaphores however longer lock hold times are more acceptable additionally unlike spin locks semaphores do not disable kernel preemption and consequently code holding a semaphore can be preempted this means semaphores do not adversely affect scheduling latency counting and binary semaphores a final useful feature of semaphores is that they can allow for an arbitrary number of si multaneous lock holders whereas spin locks permit at most one task to hold the lock at a time the number of permissible simultaneous holders of semaphores can be set at declara tion time this value is called the usage count or simply the count the most common value is to allow like spin locks only one lock holder at a time in this case the count is equal to one and the semaphore is called either a binary semaphore because it is either held by one task or not held at all or a mutex because it enforces mutual exclusion alterna tively the count can be initialized to a nonzero value greater than one in this case the semaphore is called a counting semaphore and it enables at most count holders of the lock at a time counting semaphores are not used to enforce mutual exclusion because they en able multiple threads of execution in the critical region at once instead they are used to enforce limits in certain code they are not used much in the kernel if you use a sema phore you almost assuredly want to use a mutex a semaphore with a count of one semaphores were formalized by edsger wybe in as a generalized lock ing mechanism a semaphore supports two atomic operations p and v named after the dutch word proberen to test literally to probe and the dutch word verhogen to in crement later systems called these methods down and up respectively and so does linux the down method is used to acquire a semaphore by decrementing the count by one if the new count is zero or greater the lock is acquired and the task can enter the critical region if the count is negative the task is placed on a wait queue and the proces sor moves on to something else these names are used as verbs you down a semaphore to acquire it the up method is used to release a semaphore upon completion of a critical region this is called upping the semaphore the method increments the count value if the semaphore wait queue is not empty one of the waiting tasks is awakened and allowed to acquire the semaphore memory management main memory ram is an important resource that must be carefully man aged while the average home computer nowadays has times more memo ry as the ibm the largest computer in the world in the early pro grams are getting bigger faster than memories to paraphrase parkinson law programs expand to fill the memory available to hold them in this chapter we will study how operating systems create abstractions from memory and how they manage them what every programmer would like is a private infinitely large infinitely fast memory that is also nonvolatile that is does not lose its contents when the elec tric power is switched off while we are at it why not make it inexpensive too unfortunately technology does not provide such memories at present maybe you will discover how to do it what is the second choice over the years people discovered the concept of a memory hierarchy in which computers have a few megabytes of very fast ex pensive volatile cache memory a few gigabytes of medium speed medium priced volatile main memory and a few terabytes of slow cheap nonvolatile disk storage not to mention removable storage such as dvds and usb sticks it is the job of the operating system to abstract this hierarchy into a useful model and then manage the abstraction the part of the operating system that manages part of the memory hierarchy is called the memory manager its job is to efficiently manage memory keep track of which parts of memory are in use allocate memory to processes when they need it and deallocate it when they are done memory management chap in this chapter we will investigate several different memory management schemes ranging from very simple to highly sophisticated since managing the lowest level of cache memory is normally done by the hardware the focus of this chapter will be on the programmer model of main memory and how it can be managed well the abstractions for and the management of permanent stor age the disk are the subject of the next chapter we will start at the beginning and look first at the simplest possible schemes and then gradually progress to more and more elaborate ones sec no memory abstraction oxfff no memory abstraction the simplest memory abstraction is no abstraction at all early mainframe computers before early minicomputers before and early personal computers before had no memory abstraction every program simply saw the physical memory when a program executed an instruction like mov registers the computer just moved the contents of physical memory location to register thus the model of memory presented to the programmer was sim ply physical memory a set of addresses from to some maximum each address corresponding to a cell containing some number of bits commonly eight under these conditions it was not possible to have two running programs in memory at the same time if the first program wrote a new value to say location this would erase whatever value the second program was storing there nouiing would work and both programs would crash almost immediately even with the model of memory being just physical memory several options are possible three variations are shown in fig the operating system may be at the bottom of memory in ram random access memory as shown in fig a or it may be in rom read only memory at the top of memory as shown in fig b or the device drivers may be at the top of memory in a rom and the rest of the system in ram down below as shown in fig c the first model was formerly used on mainframes and minicomputers but is rarely used any more the second model is used on some handheld computers and embedded systems the third model was used by early personal computers e g running ms dos where the portion of the system in the rom is called the bios basic input output system models a and c have the disadvantage that a bug in the user program can wipe out the operating system possibly with disastrous results such as garbling the disk when the system is organized in this way generally only one process at a time can be running as soon as the user types a command the operating system copies the requested program from disk to memory and executes it when the process finishes the operating system displays a prompt character and waits for a a b c figure three simple ways of organizing memory with an operating system and one user process other possibilities also exist new command when it receives the command it loads a new program into mem ory overwriting the first one one way to get some parallelism in a system with no memory abstraction is to program with multiple threads since all threads in a process are supposed to see the same memory image the fact that they are forced to is not a problem while this idea works it is of limited use since what people often want is unrelated pro grams to be running at the same time something the threads abstraction does not provide furthermore any system that is so primitive as to provide no memory abstraction is unlikely to provide a threads abstraction running multiple programs without a memory abstraction however even with no memory abstraction it is possible to run multiple pro grams at the same time what the operating system has to do is save the entire contents of memory to a disk file then bring in and run the next program as long as there is only one program at a time in memory there are no conflicts this concept swapping will be discussed below with the addition of some special hardware it is possible to run multiple pro grams concurrently even without swapping the early models of the ibm solved the problem as follows memory was divided into kb blocks and each one was assigned a bit protection key held in special registers inside the cpu a machine with a i mb memory needed only of these bit registers for a total of bytes of key storage the psw program status word also contained a bit key the hardware trapped any attempt by a running process to access memory with a protection code different from the psw key since only the oper ating system could change the protection keys user processes were prevented from interfering with one another and with the operating system itself memory management chap nevertheless this solution had a major drawback depicted in fig here we have two programs each kb in size as shown in fig a and b the former is shaded to indicate that it has a different memory key than the latter the first program starts out by jumping to address which contains a mov instruc tion the second program starts out by jumping to address which contains a cmp instruction the instructions that are not relevant to this discussion are not shown when the two programs are loaded consecutively in memory starting at address we have the situation of fig c for this example we assume the operating system is in high memory and thus not shown 16396 k o j o r j a o b c figure illustration of the relocation problem a a kb program b another kb program c the two programs loaded consecutively into mem ory after the programs are loaded they can be run since they have different memory keys neither one can damage the other but the problem is of a different nature when the first program starts it executes the jmp instruction which jumps to the instruction as expected this program functions normally however after the first program has run long enough the operating system may decide to run the second program which has been loaded above the first one at address the first instruction executed is jmp which jumps to the add instruction in the first program instead of the cmp instruction it is supposed to jump to the program will most likely crash in well under sec sec no memory abstraction the core problem here is that the two programs both reference absolute physi cal memory that is not what we want at all we want each program to reference a private set of addresses local to it we will show how this is achieved shortly what the ibm did as a stop gap solution was modify the second program on the fly as it loaded it into memory using a technique known as static relocation it worked like this when a program was loaded at address the constant was added to every program address during the load process while this mechanism works if done right it is not a very general solution and slows down loading furthermore it requires extra information in all executable programs to indicate which words contain relocatable addresses and which do not after all the in fig b has to be relocated but an instruction like which moves the number to registerl must not be relocated the loader needs some way to tell what is an address and what is a constant finally as we pointed out in chap history tends to repeat itself in the com puter world while direct addressing of physical memory is but a distant memory sorry on mainframes minicomputers desktop computers and notebooks the lack of a memory abstraction is still common in embedded and smart card sys tems devices such as radios washing machines and microwave ovens are all full of software in rom these days and in most cases the software addresses abso lute memory this works because all the programs are known in advance and users are not free to run their own software on their toaster while high end embedded systems such as cell phones have elaborate oper ating systems simpler ones do not in some cases there is an operating system but it is just a library that is linked with the application program and provides sys tem calls for performing i o and other common tasks the popular e cos operat ing system is a common example of an operating system as library a memory abstraction address spaces ah in all exposing physical memory to processes has several major draw backs first if user programs can address every byte of memory they can easily trash the operating system intentionally or by accident bringing the system to a grinding halt unless there is special hardware like the ibm lock and key scheme this problem exists even if only one user program application is run ning second with this model it is difficult to have multiple programs running at once taking turns if there is only one cpu on personal computers it is com mon to have several programs open at once a word processor an e mail program and a web browser with one of them having the current focus but the others being reactivated at the click of a mouse since this situation is difficult to achieve when there is no abstraction from physical memory something had to be done memory management chap the notion of an address space two problems have to be solved to allow multiple applications to be in mem ory at the same time without their interfering with each other protection and relo cation we looked at a primitive solution to the former used on the ibm label chunks of memory with a protection key and compare the key of the execut ing process to that of every memory word fetched however this approach by it self does not solve the latter problem although it can be solved by relocating pro grams as they are loaded but this is a slow and complicated solution a better solution is to invent a new abstraction for memory the address space lust as the process concept creates a kind of abstract cpu to run programs the ad dress space creates a kind of abstract memory for programs to live in an ad dress space is the set of addresses that a process can use to address memory each process has its own address space independent of those belonging to other proc esses except in some special circumstances where processes want to share their address spaces the concept of an address space is very general and occurs in many contexts consider telephone numbers in the u s and many other countries a local tele phone number is usually a digit number the address space for telephone num bers thus runs from to although some numbers such as those beginning with not used with the growth of cell phones modems and fax machines this space is becoming too small in which case more digits have to be used the address space for i o ports on the pentium runs from to addresses are bit numbers so their address space runs from to again with some reserved numbers address spaces do not have to be numeric the set of com internet domains is also an address space this address space consists of all the strings of length to characters that can be made using letters numbers and hyphens followed by com by now you should get the idea it is fairly simple somewhat harder is how to give each program its own address space so ad dress in one program means a different physical location than address in an other program below we will discuss a simple way that used to be common but has fallen into disuse due to the ability to put much more complicated and better schemes on modern cpu chips base and limit registers this simple solution uses a particularly simple version of dynamic reloca tion what it does is map each process address space onto a different part of physical memory in a simple way the classical solution which was used on ma chines ranging from the cdc the world first supercomputer to the intel the heart of the original ibm pc is to equip each cpu with two special hardware registers usually called the base and limit registers when base and sec a memory abstraction address spaces limit registers are used programs are loaded into consecutive memory locations wherever there is room and without relocation during loading as shown in fig c when a process is run the base register is loaded with the physical address where its program begins in memory and the limit register is loaded with the length of the program in fig c the base and limit values that would be loaded into these hardware registers when the first program is run are and respectively the values used when the second program is run are and respectively if a third kb program were loaded directly above the second one and run the base and limit registers would be and every time a process references memory either to fetch an instruction or read or write a data word the cpu hardware automatically adds the base value to the address generated by the process before sending the address out on the memory bus simultaneously it checks if the address offered is equal to or greater than the value in the limit register in which case a fault is generated and the access is aborted thus in the case of the first instruction of the second program in fig c the process executes a instruction but the hardware treats it as though it were jmp so it lands on the cmp instruction as expected the settings of the base and limit registers during the execution of the second program of fig c are shown in fig using base and limit registers is an easy way to give each process its own pri vate address space because every memory address generated automatically has the base register contents added to it before being sent to memory in many imple mentations the base and limit registers are protected in such a way that only the operating system can modify them this was the case on the cdc but not on the intel which did not even have the limit register it did however have multiple base registers allowing program text and data for example to be independently relocated but offered no protection from out of range memory ref erences a disadvantage of relocation using base and limit registers is the need to per form an addition and a comparison on every memory reference comparisons can be done fast but additions are slow due to carry propagation time unless special addition circuits are used swapping if the physical memory of the computer is large enough to hold all the proc esses the schemes described so far will more or less do but in practice the total amount of ram needed by all the processes is often much more than can fit in memory on a typical windows or linux system something like processes memory management chap limit register sec a memory abstraction address spaces the operation of a swapping system is illustrated in fig initially only process a is in memory then processes b and c are created or swapped in from disk in fig d a is swapped out to disk then d comes in and b goes out finally a comes in again since a is now at a different location addresses con tained in it must be relocated either by software when it is swapped in or more likely by hardware during program execution for example base and limit regis ters would work fine here base register ad d jmr c figure base and limit registers can be used to give each process a separate address space or more may be started up when the computer is booted for example when a windows application is installed it often issues commands so that on subsequent system boots a process will be started that does nothing except check for updates to the application such a process can easily occupy mb of memory other background processes check for incoming mail incoming network connections and many other things and all this is before the first user program is started serious user application programs nowadays can easily run from to mb and more consequently keeping all processes in memory all the time requires a huge amount of memory and cannot be done if there is insufficient memory two general approaches to dealing with memory overload have been devel oped over the years the simplest strategy called swapping consists of bringing in each process in its entirety running it for a while then putting it back on the disk idle processes are mostly stored on disk so they do not take up any memory when they are not running although some of them wake up periodically to do their work then go to sleep again the other strategy called virtual memory allows programs to run even when they are only partially in main memory below we will study swapping in sec we will examine virtual memory when swapping creates multiple holes in memory it is possible to combine them all into one big one by moving all the processes downward as far as pos sible this technique is known as memory compaction it is usually not done be cause it requires a lot of cpu time for example on a gb machine that can copy bytes in nsec it would take about sec to compact all of memory a point that is worth making concerns how much memory should be allocated for a process when it is created or swapped in if processes are created with a fix ed size that never changes then the allocation is simple the operating system al locates exactly what is needed no more and no less if however processes data segments can grow for example by dynamically allocating memory from a heap as in many programming languages a problem occurs whenever a process tries to grow if a hole is adjacent to the process it can be allocated and the process allowed to grow into the hole on the other hand if the process is adjacent to another process the growing process will either have to be moved to a hole in memory large enough for it or one or more proc esses will have to be swapped out to create a large enough hole if a process can not grow in memory and the swap area on the disk is full the process will have to suspended until some space is freed up or it can be killed memory management chap if it is expected that most processes will grow as they run it is probably a good idea to allocate a little extra memory whenever a process is swapped in or moved to reduce the overhead associated with moving or swapping processes that no longer fit in their allocated memory however when swapping processes to disk only the memory actually in use should be swapped it is wasteful to swap the extra memory as well in fig a we see a memory configuration in which space for growth has been allocated to two processes sec a memory abstraction address spaces memory management with bitmaps with a bitmap memory is divided into allocation units as small as a few words and as large as several kilobytes corresponding to each allocation unit is a bit in the bitmap which is if the unit is free and if it is occupied or vice versa figure shows part of memory and the corresponding bitmap room for growth actually in use room for growth b stack i b data b program a stack room for growth f t actually in use a data room for growth b hole starts length at process c operating system a a program operating system b figure a a part of memory with five processes and three holes the tick marks show the memory allocation units the shaded regions in the bitmap are free b the corresponding bitmap c the same information as a list the size of the allocation unit is an important design issue the smaller the al location unit the larger the bitmap however even with an allocation unit as small as bytes bits of memory will require only bit of the map a memory figure a allocating space for a growing data segment b allocating space for a growing stack and a growing data segment if processes can have two growing segments for example the data segment being used as a heap for variables that are dynamically allocated and released and a stack segment for the normal local variables and return addresses an alterna tive arrangement suggests itself namely that of fig b in this figure we see that each process illustrated has a stack at the top of its allocated memory that is growing downward and a data segment just beyond the program text that is grow ing upward the memory between them can be used for either segment if it runs out the process will either have to be moved to a hole with sufficient space swapped out of memory until a large enough hole can be created or killed managing free memory when memory is assigned dynamically the operating system must manage it in general terms there are two ways to keep track of memory usage bitmaps and free lists in this section and the next one we will look at these two methods of bits will use n map bits so the bitmap will take up only of memory if the allocation unit is chosen large the bitmap will be smaller but appreciable memory may be wasted in the last unit of the process if the process size is not an exact multiple of the allocation unit a bitmap provides a simple way to keep track of memory words in a fixed amount of memory because the size of the bitmap depends only on the size of memory and the size of the allocation unit the main problem is that when it has been decided to bring a k unit process into memory the memory manager must search the bitmap to find a run of k consecutive bits in the map searching a bit map for a run of a given length is a slow operation because the run may straddle word boundaries in the map this is an argument against bitmaps memory management with linked lists another way of keeping track of memory is to maintain a linked list of allo cated and free memory segments where a segment either contains a process or is an empty hole between two processes the memory of fig a is represented memory management chap in fig c as a linked list of segments each entry in the list specifies a hole h or process p the address at which it starts the length and a pointer to the next entry in this example the segment list is kept sorted by address sorting this way has the advantage that when a process terminates or is swapped out updating the list is straightforward a terminating process normally has two neighbors except when it is at the very top or bottom of memory these may be either processes or holes leading to the four combinations of fig in fig a updating the list requires replacing a p by an h in fig b and fig c two entries are coa lesced into one and the list becomes one entry shorter in fig d three en tries are merged and two items are removed from the list since the process table slot for the terminating process will normally point to the list entry for the process itself it may be more convenient to have the list as a double linked list rather than the single linked list of fig c this structure makes it easier to find the previous entry and to see if a merge is possible befor e x terminate after x terminate sec a memory abstraction address spaces rather than breaking up a big hole that might be needed later best fit tries to find a hole that is close to the actual size needed to best match the request and the available holes as an example of first fit and best fit consider fig again if a block of size is needed first fit will allocate the hole at but best fit will allocate the hole at best fit is slower than first fit because it must search the entire list every time it is called somewhat surprisingly it also results in more wasted memory than first fit or next fit because it tends to fill up memory with tiny useless holes first fit generates larger holes on the average to get around the problem of breaking up nearly exact matches into a process and a tiny hole one could think about worst fit that is always take the largest available hole so that the new hole will be big enough to be useful simulation has shown that worst fit is not a very good idea either all four algorithms can be speeded up by maintaining separate lists for proc esses and holes in this way all of them devote their full energy to inspecting holes not processes the inevitable price that is paid for this speedup on alloca tion is the additional complexity and slowdown when deallocating memory since b x become become become become a freed segment has to be removed from the process list and inserted into the hole list if distinct lists are maintained for processes and holes the hole list may be kept sorted on size to make best fit faster when best fit searches a list of holes from smallest to largest as soon as it finds a hole that fits it knows that the hole is the smallest one that will do the job hence the best fit no further searching is needed as it is with the single list scheme with a hole list sorted by size first fit and best fit are equally fast and next fit is pointless figur e fou r neighbo r combination fo r th e terminatin g process x when the processes and holes are kept on a list sorted by address several al gorithms can be used to allocate memory for a created process or an existing process being swapped in from disk we assume that the memory manager knows how much memory to allocate the simplest algorithm is first fit the memory manager scans along the list of segments until it finds a hole that is big enough the hole is then broken up into two pieces one for the process and one for the unused memory except in the statistically unlikely case of an exact fit first fit is a fast algorithm because it searches as little as possible a minor variation of first fit is next fit it works the same way as first fit ex cept that it keeps track of where it is whenever it finds a suitable hole the next time it is called to find a hole it starts searching the list from the place where it left off last time instead of always at the beginning as first fit does simulations by bays show that next fit gives slightly worse performance than first fit another well known and widely used algorithm is best fit best fit searches the entire list from beginning to end and takes the smallest hole that is adequate when the holes are kept on separate lists from the processes a small optimi zation is possible instead of having a separate set of data structures for maintain ing the hole list as is done in fig c the information can be stored in the holes the first word of each hole could be the hole size and the second word a pointer to the following entry the nodes of the list of fig c which require three words and one bit p h are no longer needed yet another allocation algorithm is quick fit which maintains separate lists for some of the more common sizes requested for example it might have a table with n entries in which the first entry is a pointer to the head of a list of kb holes the second entry is a pointer to a list of kb holes the third entry a pointer to kb holes and so on holes of say kb could be put on either the kb list or on a special list of odd sized holes with quick fit finding a hole of the required size is extremely fast but it has the same disadvantage as all schemes that sort by hole size namely when a proc ess terminates or is swapped out finding its neighbors to see if a merge is possible is expensive if merging is not done memory will quickly fragment into a large number of smjfofo into which no processes fit memory management chap virtua l memor y while base and limit registers can be used to create the abstraction of address spaces there is another problem that has to be solved managing bloatware while memory sizes are increasing rapidly software sizes are increasing much faster in the many universities ran a timesharing system with dozens of more or less satisfied users running simultaneously on a mb vax now microsoft recommends having at least mb for a single user vista system to run simple applications and gb if you are doing anything serious the trend toward multi media puts even more demands on memory as a consequence of these developments there is a need to run programs that are too large to fit in memory and there is certainly a need to have systems that can support multiple programs running simultaneously each of which fits in memory but which collectively exceed memory swapping is not an attractive option since a typical sata disk has a peak transfer rate of at most mb sec which means it takes at least sec to swap out a gb program and another sec to swap in a gb program the problem of programs larger than memory has been around since the beginning of computing albeit in limited areas such as science and engineering simulating the creation of the universe or even simulating a new aircraft takes a lot of memory a solution adopted in the was to split programs into little pieces called overlays when a program started all that was loaded into memory sec virtual memory in a sense virtual memory is a generalization of the base and limit register idea the had separate base registers but no limit registers for text and data with virtual memory instead of having separate relocation for just the text and data segments the entire address space can be mapped onto physical memory in fairly small units we will show how virtual memory is implemented below virtual memory works just fine in a multiprogramming system with bits and pieces of many programs in memory at once while a program is waiting for piece of itself to be read in the cpu can be given to another process pagin g most virtual memory systems use a technique called paging which we will now describe on any computer programs reference a set of memory addresses when a program executes an instruction like mov reg it does so to copy the contents of memory address to reg or vice versa de pending on the computer addresses can be generated using indexing base reg isters segment registers and other ways was the overlay manager which immediately loaded and ran overlay when it was done it would tell the overlay manager to load overlay either above over lay in memory if there was space for it or on top of overlay if there was no space some overlay systems were highly complex allowing many overlays in memory at once the overlays were kept on the disk and swapped in and out of memory by the overlay manager although the actual work of swapping overlays in and out was done by the operating system the work of splitting the program into pieces had to be done manually by the programmer splitting large programs up into small modular cpu cpu package the cpu sends virtual addresses to the mmu bus pieces was time consuming boring and error prone few programmers were good at this it did not take long before someone thought of a way to turn the whole job over to the computer the method that was devised fotheringham has come to be known as virtual memory the basic idea behind virtual memory is that each program has its own address space which is broken up into chunks called pages each page is a contiguous range of addresses these pages are mapped onto physical memory but not all pages have to be in physical memory to run the program when the program references a part of its address space that is in physical memory the hardware performs the necessary mapping on the fly when the program refer ences a part of its address space that is not in physical memory the operating sys tem is alerted to go get the missing piece and re execute the instruction that failed the mmu sends physical addresses to the memory figure the position and function of the mmu here the mmu is shown as being a part of the cpu chip because it commonly is nowadays however logically it could be a separate chip and was in years gone by these program generated addresses are called virtual addresses and form the virtual address space on computers without virtual memory the virtual address is put directly onto the memory bus and causes the physical memory word with the same address to be read or written when virtual memory is used the virtual addresses do not go directly to the memory bus instead they go to an mmu memory management chap memory management unit that maps the virtual addresses onto the physical memory addresses as illustrated in fig a very simple example of how this mapping works is shown in fig in this example we have a computer that generates bit addresses from up to these are the virtual addresses this computer however has only kb of physical memory so although kb programs can be written they cannot be loaded into memory in their entirety and run a complete copy of a program core image up to kb must be present on the disk however so that pieces can be brought in as needed the virtual address space is divided into fixed size units called pages the corresponding units in the physical memory are called page frames the pages and page frames are generally the same size in this example they are kb but page sizes from bytes to kb have been used in real systems with kb of virtual address space and kb of physical memory we get virtual pages and page frames transfers between ram and disk are always in whole pages virtual address space sec virtual memory refers to addresses to and so on each page contains exactly ad dresses starting at a multiple of and ending one shy of a multiple of when the program tries to access address for example using the instruction mov reg virtual address is sent to the mmu the mmu sees that this virtual address falls in page to which according to its mapping is page frame to it thus transforms the address to and outputs address onto the bus the memory knows nothing at all about the mmu and just sees a request for reading or writing address which it honors thus the mmu has effectively mapped all virtual addresses between and onto physical addresses to similarly the instruction mov reg is effectively transformed into mov reg because virtual address in virtual page is mapped onto in physical 52k 44k 32k virtual page physical memory address 32k 28k page frame page frame as a third example virtual address is bytes from the start of virtual page virtual addresses to and maps onto physical address by itself this ability to map the virtual pages onto any of the eight page frames by setting the mmu map appropriately does not solve the problem that the virtual address space is larger than the physical memory since we have only eight physical page frames only eight of the virtual pages in fig are mapped onto physical memory the others shown as a cross in the figure are not mapped in the actual hardware a present absent bit keeps track of which pages are phys ically present in memory what happens if the program references an unmapped addresses for example by using the instruction mov reg which is byte within virtual page starting at the mmu notices that the page is unmapped indicated by a cross in the figure and causes the cpu to trap to the operating system this trap is called a page fault the operating sys tem picks a little used page frame and writes its contents back to the disk if it is figure the relation between virtual addresses and physical memory ad dresses is given by the page table every page begins on a multiple of and ends addresses higher so really means and to means the notation in fig is as follows the range marked ok k means that the virtual or physical addresses in that page are to the range not already there it then fetches the page just referenced into the page frame just freed changes the map and restarts the trapped instruction for example if the operating system decided to evict page frame i it would load virtual page at physical address and make two changes to the mmu map first it would mark virtual page l entry as unmapped to trap any future accesses to virtual addresses between and then it would replace the memory management chap cross in virtual page entry with a so that when the trapped instruction is re executed it will map virtual address to physical address now let us look inside the mmu to see how it works and why we have cho sen to use a page size that is a power of in fig we see an example of a virtual address in binary being mapped using the mmu map of fig the incoming bit virtual address is split into a bit page number and a bit offset with bits for the page number we can have pages and with bits for the offset we can address all bytes within a page outgoing sec virtual memory page tables in a simple implementation the mapping of virtual addresses onto physical addresses can be summarized as follows the virtual address is split into a virtual page number high order bits and an offset low order bits for example with a bit address and a kb page size the upper bits could specify one of the virtual pages and the lower bits would then specify the byte offset to within the selected page however a split with or or some other number of bits for the page is also possible different splits imply different page sizes the virtual page number is used as an index into the page table to find the table i o o j o o o o j o vjojol i bit offset copied directly from input to output present absent bit virtual page is used as an index into the page table physical address incoming virtual entry for that virtual page from the page table entry the page frame number if any is found the page frame number is attached to the high order end of the offset replacing the virtual page number to form a physical address that can be sent to the memory thus the purpose of the page table is to map virtual pages onto page frames mathematically speaking the page table is a function with the virtual page num ber as argument and the physical frame number as result using the result of this function the virtual page field in a virtual address can be replaced by a page frame field thus forming a physical memory address structure of a page table entry let us now turn from the structure of the page tables in the large to the details of a single page table entry the exact layout of an entry is highly machine depen dent but the kind of information present is roughly the same from machine to ma chine in fig we give a sample page table entry the size varies from com puter to computer but bits is a common size the most important field is the page frame number after all the goal of the page mapping is to output this val ue next to it we have the present absent bit if this bit is the entry is valid and can be used if it is the virtual page to which the entry belongs is not currently foloh iolo oio ojo o oioloh address in memory accessing a page table entry with this bit set to causes a page fault caching disabled modified present absent figure the internal operation of the mmu with kb pages the page number is used as an index into the page table yielding the number of the page frame corresponding to that virtual page if the present absent bit is a trap to the operating system is caused if the bit is the page frame number found in the page table is copied to the high order bits of the output register along with the bit offset which is copied unmodified from the incoming virtual address together they form a bit physical address the output register is then put onto the memory bus as the physical memory address page frame number y referenced protection figure a typical page table entry memory management chap the protection bits tell what kinds of access are permitted in the simplest form this field contains bit with for read write and for read only a more sophisticated arrangement is having bits one bit each for enabling reading writ ing and executing the page the modified and referenced bits keep track of page usage when a page is written to the hardware automatically sets the modified bit this bit is of value when the operating system decides to reclaim a page frame if the page in it has been modified i e is dirty it must be written back to the disk if it has not been modified i e is clean it can just be abandoned since the disk copy is still valid the bit is sometimes called the dirty bit since it reflects the page state the referenced bit is set whenever a page is referenced either for reading or writing its value is to help the operating system choose a page to evict when a page fault occurs pages that are not being used are better candidates than pages that are and this bit plays an important role in several of the page replacement al gorithms that we will study later in this chapter finally the last bit allows caching to be disabled for the page this feature is important for pages that map onto device registers rather than memory if the op erating system is sitting in a tight loop waiting for some i o device to respond to a command it was just given it is essential that the hardware keep fetching the word from the device and not use an old cached copy with this bit caching can be turned off machines that have a separate i o space and do not use memory map ped i o do not need this bit note that the disk address used to hold the page when it is not in memory is not part of the page table the reason is simple the page table holds only that information the hardware needs to translate a virtual address to a physical address information the operating system needs to handle page faults is kept in software tables inside the operating system the hardware does not need it before getting into more implementation issues it is worth pointing out again that what virtual memory fundamentally does is create a new abstraction the ad dress space which is an abstraction of physical memory just as a process is an abstraction of the physical processor cpu virtual memory can be implemented by breaking the virtual address space up into pages and mapping each one onto some page frame of physical memory or having it temporarily unmapped thus this chapter is basically about an abstraction created by the operating system and how that abstraction is managed speeding up paging we have just seen the basics of virtual memory and paging it is now time to go into more detail about possible implementations in any paging system two major issues must be faced sec virtual memory the mapping from virtual address to physical address must be fast if the virtual address space is large the page table will be large the first point is a consequence of the fact that the virtual to physical map ping must be done on every memory reference all instructions must ultimately come from memory and many of them reference operands in memory as well consequently it is necessary to make one two or sometimes more page table ref erences per instruction if an instruction execution takes say nsec the page table lookup must be done in under nsec to avoid having the mapping become a major bottleneck the second point follows from the fact that all modern computers use virtual addresses of at least bits with bits becoming increasingly common with say a kb page size a bit address space has million pages and a bit ad dress space has more than you want to contemplate with i million pages in the virtual address space the page table must have million entries and remember that each process needs its own page table because it has its own virtual address space the need for large fast page mapping is a significant constraint on the way computers are built the simplest design at least conceptually is to have a single page table consisting of an array of fast hardware registers with one entry for each virtual page indexed by virtual page number as shown in fig when a process is started up the operating system loads the registers with the process page table taken from a copy kept in main memory during process execution no more memory references are needed for the page table the advantages of this method are that it is straightforward and requires no memory references during mapping a disadvantage is that it is unbearably expensive if the page table is large another is that having to load the full page table at every context switch hurts performance at the other extreme the page table can be entirely in main memory all the hardware needs then is a single register that points to the start of the page table this design allows the virtual to physical map to be changed at a context switch by reloading one register of course it has the disadvantage of requiring one or more memory references to read page table entries during the execution of each instruction making it very slow translation lookaside buffers let us now look at widely implemented schemes for speeding up paging and for handling large virtual address spaces starting with the former the starting point of most optimization techniques is that the page table is in memory poten tially this design has an enormous impact on performance consider for example a byte instruction that copies one register to another in the absence of paging this instruction makes only one memory reference to fetch the instruction with memory management chap paging at least one additional memory reference will be needed to access the page table since execution speed is generally limited by the rate at which the cpu can get instructions and data out of the memory having to make two memo ry references per memory reference reduces performance by half under these conditions no one would use paging computer designers have known about this problem for years and have come up with a solution their solution is based on the observation that most programs tend to make a large number of references to a small number of pages and not the other way around thus only a small fraction of the page table entries are heavily read the rest are barely used at all the solution that has been devised is to equip computers with a small hard ware device for mapping virtual addresses to physical addresses without going through the page table the device called a tlb translation lookaside buff er or sometimes an associative memory is illustrated in fig it is usually inside the mmu and consists of a small number of entries eight in this example but rarely more than each entry contains information about one page includ ing the virtual page number a bit that is set when the page is modified the protec tion code read write execute permissions and the physical page frame in which the page is located these fields have a one to one correspondence with the fields in the page table except for the virtual page number which is not needed in the page table another bit indicates whether the entry is valid i e in use or not valid virtual page modified protection pag e frame rw r x rw rw r x r x rw rw figure a tlb to speed up paging an example that might generate the tlb of fig is a process in a loop that spans virtual pages and so that these tlb entries have protection codes for reading and executing the main data currently being used say an array being processed are on pages and page contains the indices used in the array calculations finally the stack is on pages and let us now see how the tlb functions when a virtual address is presented to the mmu for translation the hardware first checks to see if its virtual page num ber is present in the tlb by comparing it to all the entries simultaneously i e in sec virtual memory parallel if a valid match is found and the access does not violate the protection bits the page frame is taken directly from the tlb without going to the page table if the virtual page number is present in the tlb but the instruction is trying to write on a read only page a protection fault is generated the interesting case is what happens when the virtual page number is not in the tlb the mmu detects the miss and does an ordinary page table lookup it then evicts one of the entries from the tlb and replaces it with the page table entry just looked up thus if that page is used again soon the second time it will result in a tlb hit rather than a miss when an entry is purged from the tlb the modified bit is copied back into the page table entry in memory the other values are already there except the reference bit when the tlb is loaded from the page table all the fields are taken from memory software tlb management up until now we have assumed that every machine with paged virtual memo ry has page tables recognized by the hardware plus a tlb in this design tlb management and handling tlb faults are done entirely by the mmu hardware traps to the operating system occur only when a page is not in memory in the past this assumption was true however many modern risc ma chines including the sparc mips and hp pa do nearly all of this page man agement in software on these machines the tlb entries are explicitly loaded by the operating system when a tlb miss occurs instead of the mmu just going to the page tables to find and fetch the needed page reference it just generates a tlb fault and tosses the problem into the lap of the operating system the system must find the page remove an entry from the tlb enter the new one and restart the instruction that faulted and of course all of this must be done in a handful of instructions because tlb misses occur much more frequently than page faults surprisingly enough if the tlb is reasonably large say entries to reduce the miss rate software management of the tlb turns out to be acceptably effi cient the main gain here is a much simpler mmu which frees up a considerable amount of area on the cpu chip for caches and other features that can improve performance software tlb management is discussed by uhlig et al various strategies have been developed to improve performance on machines that do tlb management in software one approach attacks both reducing tlb misses and reducing the cost of a tlb miss when it does occur bala et al to reduce tlb misses sometimes the operating system can use its intuition to figure out which pages are likely to be used next and to preload entries for them in the tlb for example when a client process sends a message to a server process on the same machine it is very likely that the server will have to run soon know ing this while processing the trap to do the send the system can also check to see where the server code data and stack pages are and map them in before they get a chance to cause tlb faults memory management chap the normal way to process a tlb miss whether in hardware or in software is to go to the page table and perform the indexing operations to locate the page referenced the problem with doing this search in software is that the pages hold ing the page table may not be in the tlb which will cause additional tlb faults during the processing these faults can be reduced by maintaining a large e g kb software cache of tlb entries in a fixed location whose page is always kept in the tlb by first checking the software cache the operating system can substantially reduce tlb misses when software tlb management is used it is essential to understand the dif ference between two kinds of misses a soft miss occurs when the page refer enced is not in the tlb but is in memory all that is needed here is for the tlb to be updated no disk i o is needed typically a soft miss takes machine instructions to handle and can be completed in a few nanoseconds in contrast a hard miss occurs when the page itself is not in memory and of course also not in the tlb a disk access is required to bring in the page which takes several milliseconds a hard miss is easily a million times slower than a soft miss page tables for large memories tlbs can be used to speed up virtual address to physical address translation over the original page table in memory scheme but that is not the only problem we have to tackle another problem is how to deal with very large virtual address spaces below we will discuss two ways of dealing with them multilevel page tables as a first approach consider the use of a multilevel page table a simple ex ample is shown in fig in fig a we have a bit virtual address that is partitioned into a bit field a bit field and a bit offset field since offsets are bits pages are kb and there are a total of of them the secret to the multilevel page table method is to avoid keeping all the page tables in memory all the time in particular those that are not needed should not be kept around suppose for example that a process needs megabytes the bottom megabytes of memory for program text the next megabytes for data and the top megabytes for the stack in between the top of the data and the bot tom of the stack is a gigantic hole that is not used in fig b we see how the two level page table works in this example on the left we have the top level page table with entries corresponding to the bit field when a virtual address is presented to the mmu it first extracts the field and uses this value as an index into the top level page table each of these entries represents because the entire gigabyte i e bit virtual address space has been chopped into chunks of bytes sec virtual memory bits pt offset a b figure a a bit address with two page table fields b two level page tables the entry located by indexing into the top level page table yields the address or the page frame number of a second level page table entry of the top level page table points to the page table for the program text entry points to the page table for the data and entry points to the page table for the stack the other shaded entries are not used the field is now used as an index into the selected second level page table to find the page frame number for the page itself as an example consider the bit virtual address decimal which is bytes into the data this virtual address corresponds to memory management chap pt and offset the mmu first uses to index into the top level page table and obtain entry which corresponds to addresses to it then uses to index into the second level page table just found and extract entry which corresponds to addresses to within its chunk i e absolute addresses to this entry contains the page frame number of the page containing virtual address if that page is not in memory the present absent bit in the page table entry will be zero causing a page fault if the page is in memory the page frame number taken from the sec ond level page table is combined with the offset to construct the physical ad dress this address is put on the bus and sent to memory the interesting thing to note about fig is that although the address space contains over a million pages only four page tables are actually needed the top level table and the second level tables for to for the program text to for the data and the top for the stack the present absent bits in entries of the top level page table are set to forcing a page fault if they are ever accessed should this occur the operating system will notice that the process is trying to reference memory that it is not supposed to and will take appropriate ac tion such as sending it a signal or killing it in this example we have chosen round numbers for the various sizes and have picked equal to but in ac tual practice other values are also possible of course sec virtual memory page table for an entry n p furthermore this search must be done on every memory reference not just on page faults searching a table on every mem ory reference is not the way to make your machine blindingly fast the way out of this dilemma is to use the tlb if the tlb can hold all of the heavily used pages translation can happen just as fast as with regular page tables on a tlb miss however the inverted page table has to be searched in software one feasible way to accomplish this search is to have a hash table hashed on the virtual address all the virtual pages currently in memory that have the same hash value are chained together as shown in fig if the hash table has as many slots as the machine has physical pages the average chain will be only one entry long greatly speeding up the mapping once the page frame number has been found the new virtual physical pair is entered into the tlb traditional page table with an entry for each of the pages gb physical memory has kb page frames hash table the two level page table system of fig can be expanded to three four or more levels additional levels give more flexibility but it is doubtful that the additional complexity is worth it beyond three levels s inverted page tables for bit virtual address spaces the multilevel page table works reasonably well however as bit computers become more common the situation changes drastically if the address space is now bytes with kb pages we need a page table with entries if each entry is bytes the table is over million gigabytes pb tying up million gigabytes just for the page table is not a good idea not now and probably not next year either consequently a different solution is needed for bit paged virtual address spaces one such solution is the inverted page table in this design there is one entry per page frame in real memory rather than one entry per page of virtual ad dress space for example with bit virtual addresses a kb page and gb of ram an inverted page table only requires entries the entry keeps track of which process virtual page is located in the page frame although inverted page tables save vast amounts of space at least when the virtual address space is much larger than the physical memory they have a seri ous downside virtual to physical translation becomes much harder when process n references virtual page p the hardware can no longer find the physical page by using p as an index into the page table instead it must search the entire inverted indexed indexed by virtual by hash on virtual page page virtual page page frame figure comparison of a traditional page table with an inverted page table inverted page tables are common on bit machines because even with a very large page size the number of page table entries is enormous for example with mb pages and bit virtual addresses page table entries are needed other approaches to handling large virtual memories can be found in talluri et al page replacement algorithms when a page fault occurs the operating system has to choose a page to evict remove from memory to make room for the incoming page if the page to be re moved has been modified while in memory it must be rewritten to the disk to bring the disk copy up to date if however the page has not been changed e g it memory managemen t chap contains program text the disk copy is already up to date so no rewrite is need ed the page to be read in just overwrites the page being evicted while it would be possible to pick a random page to evict at each page fault system performance is much better if a page that is not heavily used is chosen if a heavily used page is removed it will probably have to be brought back in quick ly resulting in extra overhead much work has been done on the subject of page replacement algorithms both theoretical and experimental below we will describe some of the most important algorithms it is worth noting that the problem of page replacement occurs in other areas of computer design as well for example most computers have one or more memory caches consisting of recently used byte or byte memory blocks when the cache is full some block has to be chosen for removal this problem is precisely the same as page replacement except on a shorter time scale it has to be done in a few nanoseconds not milliseconds as with page replacement the rea son for the shorter time scale is that cache block misses are satisfied from main memory which has no seek time and no rotational latency a second example is in a web server the server can keep a certain number of heavily used web pages in its memory cache however when the memory cache is full and a new page is referenced a decision has to be made which web page to evict the considerations are similar to pages of virtual memory except for the fact that the web pages are never modified in the cache so there is always a fresh copy on disk in a virtual memory system pages in main memory may be either clean or dirty in all the page replacement algorithms to be studied below a certain issue arises when a page is to be evicted from memory does it have to be one of the faulting process own pages or can it be a page belonging to another process in the former case we are effectively limiting each process to a fixed number of pages in the latter case we are not both are possibilities we will come back to this point in sec the optimal page replacement algorithm the best possible page replacement algorithm is easy to describe but impossi ble to implement it goes like this at the moment that a page fault occurs some set of pages is in memory one of these pages will be referenced on the very next instruction the page containing that instruction other pages may not be refer enced until or perhaps instructions later each page can be labeled with the number of instructions that will be executed before that page is first ref erenced the optimal page replacement algorithm says that the page with the highest label should be removed if one page will not be used for million instructions and another page will not be used for million instructions removing the former sec page replacement algorithms pushes the page fault that will fetch it back as far into the future as possible com puters like people try to put off unpleasant events for as long as they can the only problem with this algorithm is that it is unrealizable at the time of the page fault the operating system has no way of knowing when each of the pages will be referenced next we saw a similar situation earlier with the shor test job first scheduling algorithm how can the system tell which job is shor test still by running a program on a simulator and keeping track of all page ref erences it is possible to implement optimal page replacement on the second run by using the page reference information collected during the first run in this way it is possible to compare the performance of realizable algorithms with the best possible one if an operating system achieves a performance of say only worse than the optimal algorithm effort spent in looking for a better al gorithm will yield at most a improvement to avoid any possible confusion it should be made clear that this log of page references refers only to the one program just measured and then with only one specific input the page replacement algorithm derived from it is thus specific to that one program and input data although this method is useful for evaluating page replacement algorithms it is of no use in practical systems below we will study algorithms that are useful on real systems the not recently used page replacement algorithm in order to allow the operating system to collect useful page usage statistics most computers with virtual memory have two status bits associated with each page r is set whenever the page is referenced read or written m is set when the page is written to i e modified the bits are contained in each page table entry as shown in fig it is important to realize that these bits must be updated on every memory reference so it is essential that they be set by the hard ware once a bit has been set to it stays until the operating system resets it if the hardware does not have these bits they can be simulated as follows when a process is started up all of its page table entries are marked as not in memory as soon as any page is referenced a page fault will occur the operat ing system then sets the r bit in its internal tables changes the page table entry to point to the correct page with mode read only and restarts the instruction if the page is subsequently modified another page fault will occur allowing the operating system to set the m bit and change the page mode to readavrite the r and m bits can be used to build a simple paging algorithm as follows when a process is started up both page bits for all its pages are set to by the op erating system periodically e g on each clock interrupt the r bit is cleared to distinguish pages that have not been referenced recently from those that have been when a page fault occurs the operating system inspects all the pages and divides them into categories based on the current values of their r and m bits memory management chap class not referenced not modified class not referenced modified class referenced not modified class referenced modified sec page replacement algorithms the operation of this algorithm called second chance is shown in fig in fig a we see pages a through h kept on a linked list and sorted by the time they arrived in memory page loaded first although class pages seem at first glance impossible they occur when a class page has its r bit cleared by a clock interrupt clock interrupts do not clear the m bit because this information is needed to know whether the page has to be rewritten to disk or not clearing r but not m leads to a class page the nru not recently used algorithm removes a page at random from the lowest numbered nonempty class implicit in this algorithm is the idea that it is better to remove a modified page that has not been referenced in at least one clock tick typically about msec than a clean page that is in heavy use the main attraction of nru is that it is easy to understand moderately efficient to imple ment and gives a performance that while certainly not optimal may be adequate h c h d a b most recently loaded page a is treated like a newly loaded page the first in first out fifo page replacement algorithm another low overhead paging algorithm is the fifo first in first out al gorithm to illustrate how this works consider a supermarket that has enough shelves to display exactly k different products one day some company intro duces a new convenience food instant freeze dried organic yogurt that can be reconstituted in a microwave oven it is an immediate success so our finite su permarket has to get rid of one old product in order to stock it one possibility is to find the product that the supermarket has been stocking the longest i e something it began selling years ago and get rid of it on the grounds that no one is interested any more in effect the supermarket maintains a linked list of all the products it currently sells in the order they were introduced the new one goes on the back of the list the one at the front of the list is dropped as a page replacement algorithm the same idea is applicable the operating system maintains a list of all pages currently in memory with the most recent arrival at the tail and the least recent arrival at the head on a page fault the page at the head is removed and the new page added to the tail of the list when applied to stores fifo might remove mustache wax but it might also remove flour salt or butter when applied to computers the same problem arises for this reason fifo in its pure form is rarely used the second chance page replacement algorithm a simple modification to fifo that avoids the problem of throwing out a heavily used page is to inspect the r bit of the oldest page if it is the page is both old and unused so it is replaced immediately if the r bit is the bit is cleared the page is put onto the end of the list of pages and its load time is updated as though it had just arrived in memory then the search continues figure operation of second chance a pages sorted in fifo order b page list if a page fault occurs at time and a has its r bit set the numbers above the pages are their load times suppose that a page fault occurs at time the oldest page is a which arrived at time when the process started if a has the r bit cleared it is evicted from memory either by being written to the disk if it is dirty or just abandoned if it is clean on the other hand if the r bit is set a is put onto the end of the list and its load time is reset to the current time the r bit is also cleared the search for a suitable page continues with b what second chance is looking for is an old page that has not been referenced in the most recent clock interval if all the pages have been referenced second chance degenerates into pure fifo specifically imagine that all the pages in fig a have their r bits set one by one the operating system moves the pages to the end of the list clearing the r bit each time it appends a page to the end of the list eventually it comes back to page a which now has its r bit cleared at this point a is evicted thus the algorithm always terminates the clock page replacement algorithm although second chance is a reasonable algorithm it is unnecessarily ineffi cient because it is constantly moving pages around on its list a better approach is to keep all the page frames on a circular list in the form of a clock as shown in fig the hand points to the oldest page when a page fault occurs the page being pointed to by the hand is inspected if its r bit is the page is evicted the new page is inserted into the clock in its place and the hand is advanced one position if r is it is cleared and the hand is advanced to the next page this process is repeated until a page is found with r not surprisingly this algorithm is called clock memory managemen t chap b whe n a pag e fault occurs th e pag e the han d i pointing to is inspected th e actio n take n depend o n the r bit r m evict th e pag e r dea r r an d advanc e han d figure the clock page replacement algorithm the least recently used lru page replacement algorithm a good approximation to the optimal algorithm is based on the observation that pages that have been heavily used in the last few instructions will probably be heavily used again in the next few conversely pages that have not been used for ages will probably remain unused for a long time this idea suggests a realizable sec page replacement algorithms next lowest is next least recently used and so forth the workings of this algo rithm are given in fig for four page frames and page references in the order after page is referenced we have the situation of fig a after page is referenced we have the situation of fig b and so forth pag e a b c algorithm when a page fault occurs throw out the page that has been unused for f h i i the longest time this strategy is called lru least recently used paging although lru is theoretically realizable it is not cheap to fully implement lru it is necessary to maintain a linked list of all pages in memory with the most recently used page at the front and the least recently used page at the rear the difficulty is that the list must be updated on every memory reference find ing a page in the list deleting it and then moving it to the front is a very time con suming operation even in hardware assuming that such hardware could be built however there are other ways to implement lru with special hardware let us consider the simplest way first this method requires equipping the hardware with a bit counter c that is automatically incremented after each instruction furthermore each page table entry must also have a field large enough to contain the counter after each memory reference the current value of c is stored in the page table entry for the page just referenced when a page fault occurs the oper ating system examines all the counters in the page table to find the lowest one that page is the least recently used now let us look at a second hardware lru algorithm for a machine with n page frames the lru hardware can maintain a matrix of n x n bits initially all zero whenever page frame k is referenced the hardware first sets all the bits of row k to then sets all the bits of column k to at any instant of time the row whose binary value is lowest is the least recently used the row whose value is figure lru using a matrix when pages are referenced in the order simulating lru in software although both of the previous lru algorithms are in principle realizable few if any machines have the required hardware instead a solution that can be implemented in software is needed one possibility is called the nfu not fre quently used algorithm it requires a software counter associated with each page initially zero at each clock interrupt the operating system scans all the pages in memory for each page the r bit which is or is added to the count er the counters roughly keep track of how often each page has been referenced when a page fault occurs the page with the lowest counter is chosen for replace ment the main problem with nfu is that it never forgets anything for example in a multipass compiler pages that were heavily used during pass may still have a high count well into later passes in fact if pass happens to have the longest ex ecution time of all the passes the pages containing the code for subsequent passes memory management chap may always have lower counts than the pass pages consequently the operating system will remove useful pages instead of pages no longer in use fortunately a small modification to nfu makes it able to simulate lru quite well the modification has two parts first the counters are each shifted right bit before the r bit is added in second the r bit is added to the leftmost rather than the rightmost bit figure illustrates how the modified algorithm known as aging works suppose that after the first clock tick the r bits for pages to have the values and respectively page is page is page is etc in other words between tick and tick pages and were referenced setting their r bits to while the other ones remain after the six corresponding count ers have been shifted and the r bit inserted at the left they have the values shown in fig a the four remaining columns show the six counters after the next four clock ticks sec page replacement algorithms should choose one of these two the trouble is we do not know which of them was referenced last in the interval between tick and tick by recording only one bit per time interval we have lost the ability to distinguish references early in the clock interval from those occurring later all we can do is remove page be cause page was also referenced two ticks earlier and page was not the second difference between lru and aging is that in aging the counters have a finite number of bits bits in this example which limits its past horizon suppose that two pages each have a counter value of all we can do is pick one of them at random in reality it may well be that one of the pages was last refer enced nine ticks ago and the other was last referenced ticks ago we have no way of seeing that in practice however bits is generally enough if a clock tick is around msec if a page has not been referenced in msec it probably is not that important th e workin g set pag e replacemen t algorith m page r bits for pages clock tick r bits for pages clock tick j in the purest form of paging processes are started up with none of their pages in memory as soon as the cpu tries to fetch the first instruction it gets a page fault causing the operating system to bring in the page containing the first instruc tion other page faults for global variables and the stack usually follow quickly after a while the process has most of the pages it needs and setdes down to run 00000000 a 10000000 00000000 b c d e with relatively few page faults this strategy is called demand paging because pages are loaded only on demand not in advance of course it is easy enough to write a test program that systematically reads all the pages in a large address space causing so many page faults that there is not enough memory to hold them all fortunately most processes do not work this way they exhibit a locality of reference meaning that during any phase of ex ecution the process references only a relatively small fraction of its pages each pass of a multipass compiler for example references only a fraction of all the pages and a different fraction at that the set of pages that a process is currendy using is known as its working set denning denning if the entire working set is in memory the process will run without causing many faults until it moves into another execution figure the aging algorithm simulates lru in software shown are six pages for five clock ticks the five clock ticks are represented by a to e when a page fault occurs the page whose counter is the lowest is removed it is clear that a page that has not been referenced for say four clock ticks will have four leading zeros in its counter and thus will have a lower value than a counter that has not been referenced for three clock ticks this algorithm differs from lru in two ways consider pages and in fig e neither has been referenced for two clock ticks both were refer enced in the tick prior to that according to lru if a page must be replaced we phase e g the next pass of the compiler if the available memory is too small to hold the entire working set the process will cause many page faults and run slowly since executing an instruction takes a few nanoseconds and reading in a page from the disk typically takes milliseconds at a rate of one or two in structions per milliseconds it will take ages to finish a program causing page faults every few instructions is said to be thrashing denning in a multiprogramming system processes are frequently moved to disk i e all their pages are removed from memory to let other processes have a turn at the cpu the question arises of what to do when a process is brought back in again technically nothing need be done the process will just cause page faults until memory managemen t chap its working set has been loaded the problem is that having or even page faults every time a process is loaded is slow and it also wastes considerable cpu time since it takes the operating system a few milliseconds of cpu time to process a page fault therefore many paging systems try to keep track of each process working set and make sure that it is in memory before letting the process run this ap proach is called the working set model denning it is designed to greatly reduce the page fault rate loading the pages before letting processes run is also called prepaging note that the working set changes over time it has long been known that most programs do not reference their address space uniformly but that the references tend to cluster on a small number of pages a memory reference may fetch an instruction it may fetch data or it may store data at any instant of time there exists a set consisting of all the pages used by the k most recent memory references this set w k t is the working set because the k most recent references must have used all the pages used by the k most recent references and possibly others w k t is a monotonically non decreasing function of k the limit of w k t as k becomes large is finite because a program cannot reference more pages than its address space contains and few programs will use every single page figure depicts the size of the working set as a function of k w k t k figure the working set is the set of pages used by the k most recent memory references the function w k t is the size of the working set at time t the fact that most programs randomly access a small number of pages but that this set changes slowly in time explains the initial rapid rise of the curve and then the slow rise for large k for example a program that is executing a loop occupying two pages using data on four pages may reference all six pages every instructions but the most recent reference to some other page may be a mil lion instructions earlier during the initialization phase due to this asymptotic be havior the contents of the working set is not sensitive to the value of k chosen sec page replacement algorithms to put it differently there exists a wide range of k values for which the working set is unchanged because the working set varies slowly with time it is possible to make a reasonable guess as to which pages will be needed when the program is restarted on the basis of its working set when it was last stopped prepaging con sists of loading these pages before resuming the process to implement the working set model it is necessary for the operating system to keep track of which pages are in the working set having this information also immediately leads to a possible page replacement algorithm when a page fault occurs find a page not in the working set and evict it to implement such an al gorithm we need a precise way of determining which pages are in the working set by definition the working set is the set of pages used in the k most recent memory references some authors use the k most recent page references but the choice is arbitrary to implement any working set algorithm some value of k must be chosen in advance once some value has been selected after every mem ory reference the set of pages used by the most recent k memory references is uniquely determined of course having an operational definition of the working set does not mean that there is an efficient way to compute it during program execution one could imagine a shift register of length k with every memory reference shifting the reg ister left one position and inserting the most recently referenced page number on the right the set of all k page numbers in the shift register would be the working set in theory at a page fault the contents of the shift register could be read out and sorted duplicate pages could then be removed the result would be the working set however maintaining the shift register and processing it at a page fault would both be prohibitively expensive so this technique is never used instead various approximations are used one commonly used approximation is to drop the idea of counting back k memory references and use execution time instead for example instead of defining the working set as those pages used dur ing the previous million memory references we can define it as the set of pages used during the past msec of execution time in practice such a defini tion is just as good and much easier to work with note that for each process only its own execution time counts thus if a process starts running at time t and has had msec of cpu time at real time t msec for working set purposes its time is msec the amount of cpu time a process has actually used since it started is often called its current virtual time with this approximation the working set of a process is the set of pages it has referenced during the past x sec onds of virtual time now let us look at a page replacement algorithm based on the working set the basic idea is to find a page that is not in the working set and evict it in fig we see a portion of a page table for some machine because only pages that are in memory are considered as candidates for eviction pages that are absent from memory are ignored by this algorithm each entry contains at least two key items of information the approximate time the page was last used and the r memory management chap sec page replacemen t algorithm s rent clock tick and thus all have r so one is chosen at random for removal referenced bit the empty white rectangle symbolizes the other fields not need ed for th l algorithm such as the page frame number the protection bits and the u modified bit j i current virtual time information about r referenced bit preferably a clean page if one exists the wsciock page replacement algorithm the basic working set algorithm is cumbersome since the entire page table has to be scanned at each page fault until a suitable candidate is located an improved algorithm that is based on the clock algorithm but also uses the work ing set information is called wsciock carr and hennessey due to its one page time of last use page referenced during this tick page not referenced during this tick j i j page table scan ail pages examining r bit if r set time of last use to current virtual time if r c remove this page if r and age t remember the smallest time simplicity of implementation and good performance it is widely used in practice the data structure needed is a circular list of page frames as in the clock al gorithm and as shown in fig a initially this list is empty when the first page is loaded it is added to the list as more pages are added they go into the list to form a ring each entry contains the time of last use field from the basic working set algorithm as well as the r bit shown and the m bit not shown as with the clock algorithm at each page fault the page pointed to by the hand is examined first if the r bit is set to the page has been used during the current tick so it is not an ideal candidate to remove the r bit is then set to the hand advanced to the next page and the algorithm repeated for that page the state after this sequence of events is shown in fig b now consider what happens if the page pointed to has r as shown in figure the working set algorithm the algorithm works as follows the hardware is assumed to set the r and m bits as discussed earlier similarly a periodic clock interrupt is assumed to cause software to run that clears the referenced bit on every clock tick on every page fault the page table is scanned to look for a suitable page to evict as each entry is processed the r bit is examined if it is the current virtual time is written into the time of last use field in the page table indicating that the page was in use at the time the fault occurred since the page has been referenced during the current clock tick it is clearly in the working set and is not a candidate for removal is assumed to span multiple clock ticks if r is the page has not been referenced during the current clock tick and may be a candidate for removal to see whether or not it should be removed its age the current virtual time minus its time of last use is computed and compared to x if the age is greater than x the page is no longer in the working set and the new page replaces it the scan continues updating the remaining entries however if r is but the age is less than or equal to x the page is still in the working set the page is temporarily spared but the page with the greatest age smallest value of time of last use is noted if the entire table is scanned without finding a candidate to evict that means that all pages are in the working set in that case if one or more pages with r were found the one with the greatest age is evicted in the worst case all pages have been referenced during the cur fig c if the age is greater than x and the page is clean it is not in the working set and a valid copy exists on the disk the page frame is simply claimed and the new page put there as shown in fig d on the other hand if the page is dirty it cannot be claimed immediately since no valid copy is present on disk to avoid a process switch the write to disk is scheduled but the hand is ad vanced and the algorithm continues with the next page after all there might be an old clean page further down the line that can be used immediately in principle all pages might be scheduled for disk i o on one cycle around the clock to reduce disk traffic a limit might be set allowing a maximum of n pages to be written back once this limit has been reached no new writes are scheduled what happens if the hand comes all the way around to its starting point there are two cases to consider at least one write has been scheduled no writes have been scheduled in the first case the hand just keeps moving looking for a clean page since one or more writes have been scheduled eventually some write will complete and its page will be marked as clean the first clean page encountered is evicted this page is not necessarily the first write scheduled because the disk driver may reorder writes in order to optimize disk performance memory management chap sec page replacement algorithms summar y of page replacement algorithms j current virtual time we have now looked at a variety of page replacement algorithms in this sec tion we will briefly summarize them the list of algorithms discussed is given in fig a t v time of last use rbit b figure page replacement algorithms discussed in the text the optimal algorithm evicts the page that will be referenced furthest in the future unfortunately there is no way to determine which page this is so in prac 208 203 c tice this algorithm cannot be used it is useful as a benchmark against which other algorithms can be measured however the nru algorithm divides pages into four classes depending on the state of the r and m bits a random page from the lowest numbered class is chosen this algorithm is easy to implement but it is very crude better ones exist fifo keeps track of the order in which pages were loaded into memory by keeping them in a linked list removing the oldest page then becomes trivial but that page might still be in use so fifo is a bad choice second chance is a modification to fifo that checks if a page is in use before removing it if it is the page is spared this modification greatly improves the performance clock is simply a different implementation of second chance it has the same performance properties but takes a little less time to execute the algo figure operation of the wsclock algorithm a and b give an example of what happens when r c and d give an example of r in the second case all pages are in the working set otherwise at least one write would have been scheduled lacking additional information the simplest thing to do is claim any clean page and use it the location of a clean page could be kept track of during the sweep if no clean pages exist then the current page is chosen as the victim and written back to disk rithm lru is an excellent algorithm but it cannot be implemented without special hardware if this hardware is not available it cannot be used nfu is a crude at tempt to approximate lru it is not very good however aging is a much better approximation to lru and can be implemented efficiently it is a good choice the last two algorithms use the working set the working set algorithm gives reasonable performance but it is somewhat expensive to implement wsclock is a variant that not only gives good performance but is also efficient to implement memory management chap all in all the two best algorithms are aging and wsclock they are based on lru and the working set respectively both give good paging performance and can be implemented efficiently a few other algorithms exist but these two are probably the most important in practice d e s i g n i s s u e s f o r p a g i n g s y s t e m s in the previous sections we have explained how paging works and have given a few of the basic page replacement algorithms and shown how to model them but knowing the bare mechanics is not enough to design a system you have to know a lot more to make it work well it is like the difference between knowing how to move the rook knight bishop and other pieces in chess and being a good player in the following sections we will look at other issues that operating sys tem designers must consider carefully in order to get good performance from a sec design issues for paging systems ao ao ao a a a a a a a a a a a a a a bo b b b b b b b b b b b b c c c c c c c c paging system a lb c loca l versu globa l allocatio n policie in the preceding sections we have discussed several algorithms for choosing a page to replace when a fault occurs a major issue associated with this choice which we have carefully swept under the rug until now is how memory should be allocated among the competing runnable processes take a look at fig a in this figure three processes a b and c make up the set of runnable processes suppose a gets a page fault should the page replacement algorithm try to find the least recently used page considering only the six pages currently allocated to a or should it consider all the pages in memory if it looks only at a pages the page with the lowest age value is so we get the situation of fig b on the other hand if the page with the lowest age value is removed without regard to whose page it is page will be chosen and we will get the situation of fig c the algorithm of fig b is said to be a local page replacement algorithm whereas that of fig c is said to be a global algorithm local al gorithms effectively correspond to allocating every process a fixed fraction of the memory global algorithms dynamically allocate page frames among the runnable processes thus the number of page frames assigned to each process varies in time in general global algorithms work better especially when the working set size can vary over the lifetime of a process if a local algorithm is used and the working set grows thrashing will result even if there are plenty of free page frames if the working set shrinks local algorithms waste memory if a global al gorithm is used the system must continually decide how many page frames to assign to each process one way is to monitor the working set size as indicated by figure local versus global page replacement a original configuration b local page replacement c global page replacement the aging bits but this approach does not necessarily prevent thrashing the work ing set may change size in microseconds whereas the aging bits are a crude meas ure spread over a number of clock ticks another approach is to have an algorithm for allocating page frames to proc esses one way is to periodically determine the number of running processes and allocate each process an equal share thus with available i e non operat ing system page frames and processes each process gets frames the remaining six go into a pool to be used when page faults occur although this method seems fair it makes little sense to give equal shares of the memory to a kb process and a kb process instead pages can be allo cated in proportion to each process total size with a kb process getting times the allotment of a kb process it is probably wise to give each process some minimum number so that it can run no matter how small it is on some ma chines for example a single two operand instruction may need as many as six pages because the instruction itself the source operand and the destination oper and may all straddle page boundaries with an allocation of only five pages pro grams containing such instructions cannot execute at all if a global algorithm is used it may be possible to start each process up with some number of pages proportional to the process size but the allocation has to be updated dynamically as the processes run one way to manage the allocation is to use the pff page fault frequency algorithm it tells when to increase or decrease a process page allocation but says nothing about which page to replace on a fault it just controls the size of the allocation set memory managemen t chap for a large class of page replacement algorithms including lru it is known that the fault rate decreases as more pages are assigned as we discussed above this is the assumption behind pff this property is illustrated in fig number of page frames assigned figure page fault rate as a function of the number of page frames assigned measuring the page fault rate is straightforward just count the number of faults per second possibly taking a running mean over past seconds as well one easy way to do this is to add the number of page faults during the immediately preceding second to the current running mean and divide by two the dashed line marked a corresponds to a page fault rate that is unacceptably high so the fault ing process is given more page frames to reduce the fault rate the dashed line marked b corresponds to a page fault rate so low that we can assume the process has too much memory in this case page frames may be taken away from it thus pff tries to keep the paging rate for each process within acceptable bounds it is important to note that some page replacement algorithms can work with either a local replacement policy or a global one for example fifo can replace the oldest page in all of memory global algorithm or the oldest page owned by the current process local algorithm similarly lru or some approximation to it can replace the least recently used page in all of memory global algorithm or the least recently used page owned by the current process local algorithm the choice of local versus global is independent of the algorithm in some cases on the other hand for other page replacement algorithms only a local strate gy makes sense in particular the working set and wsclock algorithms refer to some specific process and must be applied in that context there really is no working set for the machine as a whole and trying to use the union of all the working sets would lose the locality property and not work well load control even with the best page replacement algorithm and optimal global allocation of page frames to processes it can happen that the system thrashes in fact when ever the combined working sets of all processes exceed the capacity of memory sec design issues for paging systems thrashing can be expected one symptom of this situation is that the pff algo rithm indicates that some processes need more memory but no processes need less memory in this case there is no way to give more memory to those processes needing it without hurting some other processes the only real solution is to tem porarily get rid of some processes a good way to reduce the number of processes competing for memory is to swap some of them to the disk and free up all the pages they are holding for ex ample one process can be swapped to disk and its page frames divided up among other processes that are thrashing if the thrashing stops the system can run for a while this way if it does not stop another process has to be swapped out and so on until the thrashing stops thus even with paging swapping is still needed only now swapping is used to reduce potential demand for memory rather than to reclaim pages swapping processes out to relieve the load on memory is reminiscent of two level scheduling in which some processes are put on disk and a short term sched uler is used to schedule the remaining processes clearly the two ideas can be combined with just enough processes swapped out to make the page fault rate ac ceptable periodically some processes are brought in from disk and other ones are swapped out however another factor to consider is the degree of multiprogramming when the number of processes in main memory is too low the cpu may be idle for substantial periods of time this consideration argues for considering not only process size and paging rate when deciding which process to swap out but also its characteristics such as whether it is cpu bound or i o bound and what charac teristics the remaining processes have pag e size the page size is often a parameter that can be chosen by the operating system even if the hardware has been designed with for example byte pages the operating system can easily regard page pairs and and and and so on as kb pages by always allocating two consecutive byte page frames for them determining the best page size requires balancing several competing factors as a result there is no overall optimum to start with there are two factors that argue for a small page size a randomly chosen text data or stack segment will not fill an integral number of pages on the average half of the final page will be empty the extra space in that page is wasted this wastage is called internal fragmentation with n segments in memory and a page size of p bytes np bytes will be wasted on internal fragmentation this reasoning argues for a small page size another argument for a small page size becomes apparent if we think about a program consisting of eight sequential phases of kb each with a kb page memory management chap size the program must be allocated kb ail the time with a kb page size it needs only kb with a page size of kb or smaller it requires only kb at any instant in general a large page size will cause more unused program to be in memory than a small page size on the other hand small pages mean that programs will need many pages hence a large page table a kb program needs only four kb pages but byte pages transfers to and from the disk are generally a page at a time with most of the time being for the seek and rotational delay so that transferring a small page takes almost as much time as transferring a large page it might take x msec to load byte pages but only x msec to load four kb pages on some machines the page table must be loaded into hardware registers ev ery time the cpu switches from one process to another on these machines hav ing a small page size means that the time required to load the page registers gets longer as the page size gets smaller furthermore the space occupied by the page table increases as the page size decreases this last point can be analyzed mathematically let the average process size be bytes and the page size be p bytes furthermore assume that each page entry requires e bytes the approximate number of pages needed per process is then p occupying se p bytes of page table space the wasted memory in the last page of the process due to internal fragmentation is p thus the total overhead due to the page table and the internal fragmentadon loss is given by the sum of these two terms overhead se p p the first term page table size is large when the page size is small the sec ond term internal fragmentation is large when the page size is large the optimum must lie somewhere in between by taking the first derivative with respect to p and equating it to zero we get the equation se from this equation we can derive a formula that gives the optimum page size considering only memory wasted in fragmentation and page table size the re sult is p for and e bytes per page table entry the optimum page size is kb commercially available computers have used page sizes ranging from bytes to kb a typical value used to be kb but nowadays kb or kb is more common as memories get larger the page size tends to get larger as well but not linearly quadrupling the ram size rarely even doubles the page size sec design issues for paging systems separat e instruction an d dat a spaces most computers have a single address space that holds both programs and data as shown in fig a if this address space is large enough everything works fine however it is often too small forcing programmers to stand on their heads to fit everything into the address space single address space i space d space unused page data a b figure a one address space b separate i and d spaces one solution pioneered on the bit pdp is to have separate address spaces for instructions program text and data called i space and d space re spectively as illustrated in fig b each address space runs from to some maximum typically or the linker must know when separate i and d spaces are being used because when they are the data are relocated to vir tual address instead of starting after the program in a computer with this design both address spaces can be paged indepen dently from one another each one has its own page table with its own mapping of virtual pages to physical page frames when the hardware wants to fetch an in struction it knows that it must use i space and the i space page table similarly references to data must go through the d space page table other than this dis tinction having separate i and d spaces does not introduce any special complica tions and it does double the available address space share d pages another design issue is sharing in a large multiprogramming system it is common for several users to be running the same program at the same time it is clearly more efficient to share the pages to avoid having two copies of the same page in memory at the same time one problem is that not all pages are sharable in particular pages that are read only such as program text can be shared but data pages cannot if separate i and d spaces are supported it is relatively straightforward to share programs by having two or more processes use the same page table for their memory management chap sec design issues for paging systems i space but different page tables for their d spaces typically in an implementa tion that supports sharing in this way page tables are data structures independent of the process table each process then has two pointers in its process table one to the i space page table and one to the d space page table as shown in fig when the scheduler chooses a process to run it uses these pointers to locate the appropriate page tables and sets up the mmu using them even without separate i and d spaces processes can share programs or sometimes libraries but the mechanism is more complicated program data data page tables figure tw o processes sharin g the sam e progra m sharin g its pag e table when two or more processes share some code a problem occurs with the shared pages suppose that processes a and b are both running the editor and sharing its pages if the scheduler decides to remove a from memory evicting all its pages and filling the empty page frames with some other program will cause b to generate a large number of page faults to bring them back in again similarly when a terminates it is essential to be able to discover that the pages are still in use so that their disk space will not be freed by accident search ing all the page tables to see if a page is shared is usually too expensive so special data structures are needed to keep track of shared pages especially if the unit of sharing is the individual page or run of pages rather than an entire page table sharing data is trickier than sharing code but it is not impossible in particu lar in unix after a fork system call the parent and child are required to share both program text and data in a paged system what is often done is to give each of these processes its own page table and have both of them point to the same set of pages thus no copying of pages is done at fork time however all the data pages are mapped into both processes as read only as long as both processes just read their data without modifying it this situa tion can continue as soon as either process updates a memory word the viola tion of the read only protection causes a trap to the operating system a copy is then made of the offending page so that each process now has its own private copy both copies are now set to read write so subsequent writes to either copy proceed without trapping this strategy means that those pages that are never modified including all the program pages need not be copied only the data pages that are actually modified need to be copied this approach called copy on write improves performance by reducing copying shared libraries sharing can be done at other granularities than individual pages if a program is started up twice most operating systems will automatically share all the text pages so that only one copy is in memory text pages are always read only so there is no problem here depending on the operating system each process may get its own private copy of the data pages or they may be shared and marked read only if any process modifies a data page a private copy will be made for it that is copy on write will be applied in modern systems there are many large libraries used by many processes for example the library that handles the dialog for browsing for files to open and multiple graphics libraries statically binding all these libraries to every ex ecutable program on the disk would make them even more bloated than they al ready are instead a common technique is to use shared libraries which are called dlls or dynamic link libraries on windows to make the idea of a shared library clear first consider traditional linking when a program is linked one or more object files and possibly some libraries are named in the command to the linker such as the unix command id o ic im which links all the o object files in the current directory and then scans two li braries usr lib libc a and usr lib libm a any functions called in the object files but not present there e g print are called undefined externals and are sought in the libraries if they are found they are included in the executable binary any functions they call but are not yet present also become undefined externals for example printf needs write so if write is not already included the linker will look for it and include it when found when the linker is done an executable binary memory management chap file is written to the disk containing all the functions needed functions present in the libraries but not called are not included when the program is loaded into memory and executed all the functions it needs are there now suppose common programs use mb worth of graphics and user interface functions statically linking hundreds of programs with all these libraries would waste a tremendous amount of space on the disk as well as wasting space in ram when they were loaded since the system would have no way of knowing it could share them this is where shared libraries come in when a program is linked with shared libraries which are slightly different than static ones instead of including the actual function called the linker includes a small stub routine that binds to the called function at run time depending on the system and the confi guration details shared libraries are loaded either when the program is loaded or when functions in them are called for the first time of course if another program has already loaded the shared library there is no need to load it again that is the whole point of it note that when a shared library is loaded or used the entire li brary is not read into memory in a single blow it is paged in page by page as needed so functions that are not called will not be brought into ram in addition to making executable files smaller and saving space in memory shared libraries have another advantage if a function in a shared library is up dated to remove a bug it is not necessary to recompile the programs that call it the old binaries continue to work this feature is especially important for com mercial software where the source code is not distributed to the customer for ex ample if microsoft finds and fixes a security error in some standard dll win dows update will download the new dll and replace the old one and all pro grams that use the dll will automatically use the new version the next time they are launched shared libraries cojne with one little problem that has to be solved however the problem is illustrated in fig here we see two processes sharing a li brary of size kb assuming each box is kb however the library is located at a different address in each process presumably because the programs them selves are not the same size in process the library starts at address in process it starts at suppose that the first thing the first function in the li brary has to do is jump to address in the library if the library were not shared it could be relocated on the fly as it was loaded so that the jump in process could be to virtual address note that the physical address in the ram where the library is located does not matter since all the pages are mapped from virtual to physical addresses by the mmu hardware however since the library is shared relocation on the fly will not work after all when the first function is called by process at address the jump in struction has to go to not this is the little problem one way to solve it is to use copy on write and create new pages for each process sharing the library relocating them on the fly as they are created but this scheme defeats the purpose of sharing the library of course sec design issues for paging systems process ram process figure a shared library being used by two processes a better solution is to compile shared libraries with a special compiler flag telling the compiler not to produce any instructions that use absolute addresses instead only instructions using relative addresses are used for example there is almost always an instruction that says jump forward or backward by n bytes as opposed to an instruction that gives a specific address to jump to this instruc tion works correctly no matter where the shared library is placed in the virtual ad dress space by avoiding absolute addresses the problem can be solved code that uses only relative offsets is called position independent code mapped files shared libraries are really a special case of a more general facility called memory mapped files the idea here is that a process can issue a system call to map a file onto a portion of its virtual address space in most implementations no pages are brought in at the time of the mapping but as pages are touched they are demand paged in one at a time using the disk file as the backing store when the process exits or explicitly unmaps the file all the modified pages are written back to the file mapped files provide an alternative model for i o instead of doing reads and writes the file can be accessed as a big character array in memory in some situa tions programmers find this model more convenient if two or more processes map onto the same file at the same time they can communicate over shared memory writes done by one process to the shared memory are immediately visible when the other one reads from the part of its vir tual address spaced mapped onto the file this mechanism thus provides a high bandwidth channel between processes and is often used as such even to the extent of mapping a scratch file now it should be clear that if memory mapped files are available shared libraries can use this mechanism memory management chap cleaning policy paging works best when there are plenty of free page frames that can be claimed as page faults occur if every page frame is full and furthermore modi fied before a new page can be brought in an old page must first be written to disk to ensure a plentiful supply of free page frames many paging systems have a background process called the paging daemon that sleeps most of the time but is awakened periodically to inspect the state of memory if too few page frames are free the paging daemon begins selecting pages to evict using some page replacement algorithm if these pages have been modified since being loaded they are written to disk in any event the previous contents of the page are remembered in the event one of the evicted pages is needed again before its frame has been overwritten it can be reclaimed by removing it from the pool of free page frames keeping a supply of page frames around yields better performance than using all of memory and then trying to find a frame at the moment it is needed at the very least the paging daemon ensures that all the free frames are clean so they need not be writ ten to disk in a big hurry when they are required one way to implement this cleaning policy is with a two handed clock the front hand is controlled by the paging daemon when it points to a dirty page that page is written back to disk and the front hand is advanced when it points to a clean page it is just advanced the back hand is used for page replacement as in the standard clock algorithm only now the probability of the back hand hitting a clean page is increased due to the work of the paging daemon virtual memory interface up until now our whole discussion has assumed that virtual memory is trans parent to processes and programmers that is all they see is a large virtual address space on a computer with a small er physical memory with many systems that is true but in some advanced systems programmers have some control over the memory map and can use it in nontraditional ways to enhance program behavior in this section we will briefly look at a few of these one reason for giving programmers control over their memory map is to allow two or more processes to share the same memory if programmers can name regions of their memory it may be possible for one process to give another process the name of a memory region so that process can also map it in with two or more processes sharing the same pages high bandwidth sharing becomes pos sible one process writes into the shared memory and another one reads from it sharing of pages can also be used to implement a high performance mes sage passing system normally when messages are passed the data are copied from one address space to another at considerable cost if processes can control their page map a message can be passed by having the sending process unmap the sec design issues for paging systems page containing the message and the receiving process mapping them in here only the page names have to be copied instead of all the data yet another advanced memory management technique is distributed shared memory feeley et al li li and hudak and zekauskas et al the idea here is to allow multiple processes over a network to share a set of pages possibly but not necessarily as a single shared linear address space when a process references a page that is not currently mapped in it gets a page fault the page fault handler which may be in the kernel or in user space then locates the machine holding the page and sends it a message asking it to unmap the page and send it over the network when the page arrives it is mapped in and the faulting instruction is restarted we will examine distributed shared memory in more detail in chap implementatio n issue s implemented of virtual memory systems have to make choices among the major theoretical algorithms such as second chance versus aging local versus global page allocation and demand paging versus prepaging but they also have to be aware of a number of practical implementation issues as well in this sec tion we will take a look at a few of the common problems and some solutions operating system involvement with paging there are four times when the operating system has paging related work to do process creation time process execution time page fault time and process termination time we will now briefly examine each of these to see what has to be done when a new process is created in a paging system the operating system has to determine how large the program and data will be initially and create a page table for them space has to be allocated in memory for the page table and it has to be initialized the page table need not be resident when the process is swapped out but has to be in memory when the process is running in addition space has to be allocated in the swap area on disk so that when a page is swapped out it has somewhere to go the swap area also has to be initialized with program text and data so that when the new process starts getting page faults the pages can be brought in some systems page the program text directiy from the executable file thus saving disk space and initialization time finally information about the page table and swap area on disk must be recorded in the process table when a process is scheduled for execution the mmu has to be reset for the new process and the tlb flushed to get rid of traces of the previously executing process the new process page table has to be made current usually by copying it or a pointer to it to some hardware register optionally some or all of the memory management chap process pages can be brought into memory to reduce the number of page faults initially e g it is certain that the page pointed to by the pc will be needed when a page fault occurs the operating system has to read out hardware reg isters to determine which virtual address caused the fault from this information it must compute which page is needed and locate that page on disk it must then find an available page frame to put the new page evicting some old page if need be then it must read the needed page into the page frame finally it must back up the program counter to have it point to the faulting instruction and let that in struction execute again when a process exits the operating system must release its page table its pages and the disk space that the pages occupy when they are on disk if some of the pages are shared with other processes the pages in memory and on disk can only be released when the last process using them has terminated page fault handling we are finally in a position to describe in detail what happens on a page fault the sequence of events is as follows the hardware traps to the kernel saving the program counter on the stack on most machines some information about the state of the current instruction is saved in special cpu registers an assembly code routine is started to save the general registers and other volatile information to keep the operating system from des troying it this routine calls the operating system as a procedure the operating system discovers that a page fault has occurred and tries to discover which virtual page is needed often one of the hard ware registers contains this information if not the operating system must retrieve the program counter fetch the instruction and parse it in software to figure out what it was doing when the fault hit once the virtual address that caused the fault is known the system checks to see if this address is valid and the protection consistent with the access if not the process is sent a signal or killed if the address is valid and no protection fault has occurred the system sec implementation issues as soon as the page frame is clean either immediately or after it is written to disk the operating system looks up the disk address where the needed page is and schedules a disk operation to bring it in while the page is being loaded the faulting process is still suspended and another user process is run if one is available when the disk interrupt indicates that the page has arrived the page tables are updated to reflect its position and the frame is marked as being in normal state the faulting instruction is backed up to the state it had when it began and the program counter is reset to point to that instruction the faulting process is scheduled and the operating system returns to the assembly language routine that called it this routine reloads the registers and other state information and re turns to user space to continue execution as if no fault had occurred instruction backup when a program references a page that is not in memory the instruction caus ing the fault is stopped partway through and a trap to the operating system occurs after the operating system has fetched the page needed it must restart the instruc tion causing the trap this is easier said than done to see the nature of this problem at its worst consider a cpu that has instruc tions with two addresses such as the motorola widely used in embedded systems the instruction mov l a0 is bytes for example see fig in order to restart the instruction the oper ating system must determine where the first byte of the instruction is the value of the program counter at the time of the trap depends on which operand faulted and how the cpu microcode has been implemented move l a0 bits checks to see if a page frame is free if no frames are free the page replacement algorithm is run to select a victim if the page frame selected is dirty the page is scheduled for transfer to the disk and a context switch takes place suspending the faulting move opcode first operand second operand process and letting another one run until the disk transfer has com pleted in any event the frame is marked as busy to prevent it from being used for another purpose figure an instruction causing a page fault in fig we have an instruction starting at address that makes three memory references the instruction word itself and two offsets for the operands memory management chap depending on which of these three memory references caused the page fault the program counter might be or at the time of the fault it is fre quently impossible for the operating system to determine unambiguously where the instruction began if the program counter is at the time of the fault the operating system has no way of telling whether the word in is a memory ad dress associated with an instruction at e g the location of an operand or an instruction opcode bad as this problem may be it could have been worse some ad dressing modes use autoincrementing which means that a side effect of executing the instruction is to increment one or more registers instructions that use autoin crement mode can also fault depending on the details of the microcode the increment may be done before the memory reference in which case the operating system must decrement the register in software before restarting the instruction or the autoincrement may be done after the memory reference in which case it will not have been done at the time of the trap and must not be undone by the op erating system autodecrement mode also exists and causes a similar problem the precise details of whether autoincrements and autodecrements have or have not been done before the corresponding memory references may differ from in struction to instruction and from cpu model to cpu model fortunately on some machines the cpu designers provide a solution usually in the form of a hidden internal register into which the program counter is copied just before each instruction is executed these machines may also have a second register telling which registers have already been autoincremented or autodecre mented and by how much given this information the operating system can unambiguously undo all the effects of the faulting instruction so that it can be re started if this information is not available the operating system has to jump through hoops to figure out what happened and how to repair it it is as though the hardware designers were unable to solve the problem so they threw up their hands and told the operating system writers to deal with it nice guys locking pages in memory although we have not discussed i o much in this chapter the fact that a com puter has virtual memory does not mean that i o is absent virtual memory and i o interact in subtle ways consider a process that has just issued a system call to read from some file or device into a buffer within its address space while wait ing for the i o to complete the process is suspended and another process is allow ed to run this other process gets a page fault if the paging algorithm is global there is a small but nonzero chance that the page containing the i o buffer will be chosen to be removed from memory if an i o device is currently in the process of doing a dma transfer to that page removing it will cause part of the data to be written in the buffer where they be long and part of the data to be written over the just loaded page one solution to sec implementation issues this problem is to lock pages engaged in i o in memory so that they will not be re moved locking a page is often called pinning it in memory another solution is to do all i o to kernel buffers and then copy the data to user pages later backing store in our discussion of page replacement algorithms we saw how a page is selected for removal we have not said much about where on the disk it is put when it is paged out let us now describe some of the issues related to disk man agement the simplest algorithm for allocating page space on the disk is to have a spe cial swap partition on the disk or even better on a separate disk from the fde sys tem to balance the i o load most unix systems work like this this partition does not have a normal fde system on it which eliminates all the overhead of con verting offsets in files to block addresses instead block numbers relative to the start of the partition are used throughout when the system is booted this swap partition is empty and is represented in memory as a single entry giving its origin and size in the simplest scheme when the first process is started a chunk of the partition area the size of the first process is reserved and the remaining area reduced by that amount as new processes are started they are assigned chunks of the swap partition equal in size to their core images as they finish their disk space is freed the swap partition is managed as a list of free chunks better algorithms will be discussed in chap associated with each process is the disk address of its swap area that is where on the swap partition its image is kept this information is kept in the proc ess table calculating the address to write a page to becomes simple just add the offset of the page within the virtual address space to the start of the swap area however before a process can start the swap area must be initialized one way is to copy the entire process image to the swap area so that it can be brought in as needed the other is to load the entire process in memory and let it be paged out as needed however this simple model has a problem processes can increase in size after starting although the program text is usually fixed the data area can some times grow and the stack can always grow consequently it may be better to reserve separate swap areas for the text data and stack and allow each of these areas to consist of more than one chunk on the disk the other extreme is to allocate nothing in advance and allocate disk space for each page when it is swapped out and deallocate it when it is swapped back in in this way processes in memory do not tie up any swap space the disadvantage is that a disk address is needed in memory to keep track of each page on disk in other words there must a table per process telling for each page on disk where it is the two alternatives are shown in fig memory management chap main memory l main memory a figure a paging to a static swap area b backing up pages dynamically in fig a a page table with eight pages is illustrated pages and are in main memory pages and are on disk the swap area on disk is as large as the process virtual address space eight pages with each page having a fixed location to which it is written when it is evicted from main memory calcu lating this address requires knowing only where the process paging area begins since pages are stored in it contiguously in order of their virtual page number a page that is in memory always has a shadow copy on disk but this copy may be out of date if the page has been modified since being loaded the shaded pages in memory indicate pages not present in memory the shaded pages on the disk are in principle superseded by the copies in memory although if a memory page has to be swapped back to disk and it has not been modified since it was loaded the shaded disk copy will be used in fig b pages do not have fixed addresses on disk when a page is swapped out an empty disk page is chosen on the fly and the disk map which has room for one disk address per virtual page is updated accordingly a page in memory has no copy on disk their entries in the disk map contain an invalid disk address or a bit marking them as not in use having a fixed swap partition is not always possible for example no disk partitions may be available in this case one or more large preallocated files within the normal file system can be used windows uses this approach howev er an optimization can be used here to reduce the amount of disk space needed since the program text of every process came from some executable file in the file system the executable file can be used as the swap area better yet since the program text is generally read only when memory is tight and program pages have to be evicted from memory they are just discarded and read in again from the executable file when needed shared libraries can also work this way sec implementation issues separation of policy and mechanism an important tool for managing the complexity of any system is to separate policy from mechanism this principle can be applied to memory management by having most of the memory manager run as a user level process such a separa tion was first done in mach young et al the discussion below is loosely based on mach a simple example of how policy and mechanism can be separated is shown in fig here the memory management system is divided into three parts a low level mmu handler a page fault handler that is part of the kernel an external pager running in user space all the details of how the mmu works are encapsulated in the mmu handler which is machine dependent code and has to be rewritten for each new platform the operating system is ported to the page fault handler is machine independent code and contains most of the mechanism for paging the policy is largely deter mined by the external pager which runs as a user process figure page fault handling with an external pager when a process starts up the external pager is notified in order to set up the process page map and allocate backing store on the disk if need be as the proc ess runs it may map new objects into its address space so the external pager is again notified once the process starts running it may get a page fault the fault handler fig ures out which virtual page is needed and sends a message to the external pager telling it the problem the external pager then reads the needed page in from the memory management chap disk and copies it to a portion of its own address space then it tells the fault handler where the page is the fault handler then unmaps the page from the ex ternal pager address space and asks the mmu handler to put it into the user address space at the right place then the user process can be restarted this implementation leaves open where the page replacement algorithm is put it would be cleanest to have it in the external pager but there are some prob lems with this approach principal among these is that the external pager does not have access to the r and m bits of all the pages these bits play a role in many of sec segmentation virtual addres spac e fre e addres spac e the paging algorithms thus either some mechanism is needed to pass this infor mation up to the external pager or the page replacement algorithm must go in the kernel in the latter case the fault handler tells the external pager which page it has selected for eviction and provides the data either by mapping it into the exter nal pager address space or including it in a message either way the external pager writes the data to disk the main advantage of this implementation is more modular code and greater flexibility the main disadvantage is the extra overhead of crossing the user kernel boundary several times and the overhead of the various messages being sent between the pieces of the system at the moment the subject is highly con troversial but as computers get faster and faster and the software gets more and more complex in the long run sacrificing some performance for more reliable software will probably be acceptable to most implementers allocate d to the pars e tre e constan t tablse i spac e currently bein g use d b y th e pars e tre e symbo l tabl e ha bumpe d into th e sourc e text table segmentatio n the virtual memory discussed so far is one dimensional because the virtual addresses go from to some maximum address one address after another for many problems having two or more separate virtual address spaces may be much better than having only one for example a compiler has many tables that are built up as compilation proceeds possibly including the source text being saved for the printed listing on batch systems the symbol table containing the names and attributes of variables the table containing all the integer and floating point constants used the parse tree containing the syntactic analysis of the program the stack used for procedure calls within the compiler each of the first four tables grows continuously as compilation proceeds the last one grows and shrinks in unpredictable ways during compilation in a one dimensional memory these five tables would have to be allocated contiguous chunks of virtual address space as in fig consider what happens if a program has a much larger than usual number of variables but a normal amount of everything else the chunk of address space allocated for the symbol table may fill up but there may be lots of room in the other tables the compiler could of course simply issue a message saying that the compilation cannot continue due to too many variables but doing so does not seem very sporting when unused space is left in the other tables another possibility is to play robin hood taking space from the tables with an excess of room and giving it to the tables with little room this shuffling can be done but it is analogous to managing one own overlays a nuisance at best and a great deal of tedious unrewarding work at worst what is really needed is a way of freeing the programmer from having to manage the expanding and contracting tables in the same way that virtual memo ry eliminates the worry of organizing the program into overlays a straightforward and extremely general solution is to provide the machine with many completely independent address spaces called segments each seg ment consists of a linear sequence of addresses from to some maximum the length of each segment may be anything from to the maximum allowed dif ferent segments may and usually do have different lengths moreover segment lengths may change during execution the length of a stack segment may be in creased whenever something is pushed onto the stack and decreased whenever something is popped off the stack because each segment constitutes a separate address space different seg ments can grow or shrink independently without affecting each other if a stack in memory management chap sec segmentation all procedures that call any of the moved procedures in order to incorporate their a certain segment needs more address space to grow it can have it because there is nothing else in its address space to bump into of course a segment can fill up but segments are usually very large so this occurrence is rare to specify an ad dress in this segmented or two dimensional memory the program must supply a two part address a segment number and an address within the segment figure illustrates a segmented memory being used for the compiler tables discussed earlier five independent segments are shown here 20k symbol table new starting addresses if a program contains hundreds of procedures this proc ess can be costly segmentation also facilitates sharing procedures or data between several proc esses a common example is the shared library modern workstations that run advanced window systems often have extremely large graphical libraries com piled into nearly every program in a segmented system the graphical library can be put in a segment and shared by multiple processes eliminating the need for having it in every process address space while it is also possible to have shared libraries in pure paging systems it is more complicated in effect these systems do it by simulating segmentation since each segment forms a logical entity of which the programmer is aware such as a procedure or an array or a stack different segments can have different kinds of protection a procedure segment can be specified as execute only prohi biting attempts to read from it or store into it a floating point array can be speci fied as read write but not execute and attempts to jump to it will be caught such protection is helpful in catching programming errors ok segmen t constants ok segmen t segmen t segmen t you should try to understand why protection is sensible in a segmented mem ory but not in a one dimensional paged memory in a segmented memory the user is aware of what is in each segment normally a segment would not contain a procedure and a stack for example but only one or the other not both since each segment contains only a single type of object the segment can have the protection appropriate for that particular type paging and segmentation are compared in fig the contents of a page are in a sense accidental the programmer is unaware figur e a segmente d memor y allows eac h table t o gro w o r shrin k inde pendentl y o f the other tables we emphasize that a segment is a logical entity which the programmer is aware of and uses as a logical entity a segment might contain a procedure or an array or a stack or a collection of scalar variables but usually it does not contain a mixture of different types a segmented memory has other advantages besides simplifying the handling of data structures that are growing or shrinking if each procedure occupies a sep arate segment with address as its starting address the linking of procedures compiled separately is greatly simplified after all the procedures that constitute a program have been compiled and linked up a procedure call to the procedure in segment n will use the two part address n to address word the entry point if the procedure in segment n is subsequently modified and recompiled no other procedures need be changed because no starting addresses have been modi fied even if the new version is larger than the old one with a one dimensional memory the procedures are packed tightly next to each other with no address space between them consequently changing one procedure size can affect the starting address of other unrelated procedures this in turn requires modifying of the fact that paging is even occurring although putting a few bits in each entry of the page table to specify the access allowed would be possible to utilize this feature the programmer would have to keep track of where in his address space the page boundaries were that is precisely the sort of administration that paging was invented to eliminate because the user of a segmented memory has the illusion that all segments are in main memory all the time that is he can ad dress them as though they were he can protect each segment separately without having to be concerned with the administration of overlaying them implementation of pure segmentation the implementation of segmentation differs from paging in an essential way pages are fixed size and segments are not figure a shows an example of physical memory initially containing five segments now consider what happens if segment is evicted and segment which is smaller is put in its place we arrive at the memory configuration of fig b between segment and seg ment is an unused area that is a hole then segment is replaced by segment as in fig c and segment is replaced by segment as in fig d memory managemen t chap consideratio n pagin g segmentatio n s e c segmentation warn mm a a i d t j figure a d development of checkerboarding e removal of the checkerboarding by compaction figure comparison of paging and segmentation after the system has been running for a while memory will be divided up into a number of chunks some containing segments and some containing holes this phenomenon called checkerboarding or external fragmentation wastes memo ry in the holes it can be dealt with by compaction as shown in fig e segmentation with paging multics if the segments are large it may be inconvenient or even impossible to keep them in main memory in their entirety this leads to the idea of paging them so that only those pages that are actually needed have to be around several signifi cant systems have supported paged segments in this section we will describe the first one multics in the next one we will discuss a more recent one the intel pentium multics ran on the honeywell machines and their descendants and provided each program with a virtual memory of up to segments more than each of which could be up to bit words long to implement this the multics designers chose to treat each segment as a virtual memory and to page it combining the advantages of paging uniform page size and not having to keep the whole segment in memory if only part of it is being used with the ad vantages of segmentation ease of programming modularity protection sharing each multics program has a segment table with one descriptor per seg ment since there are potentially more than a quarter of a million entries in the table the segment table is itself a segment and is paged a segment descriptor contains an indication of whether the segment is in main memory or not if any part of the segment is in memory the segment is considered to be in memory and its page table will be in memory if the segment is in memory its descriptor con tains an bit pointer to its page table as in fig a because physical ad dresses are bits and pages are aligned on byte boundaries implying that the low order bits of page addresses are only bits are needed in the de scriptor to store a page table address the descriptor also contains the segment size the protection bits and a few other items figure b illustrates a mul tics segment descriptor the address of the segment in secondary memory is not in the segment descriptor but in another table used by the segment fault handler each segment is an ordinary virtual address space and is paged in the same way as the nonsegmented paged memory described earlier in this chapter the normal page size is words although a few small segments used by mul tics itself are not paged or are paged in units of words to save physical mem ory an address in multics consists of two parts the segment and the address within the segment the address within the segment is further divided into a page number and a word within the page as shown in fig when a memory ref erence occurs the following algorithm is carried out memory management chap bits page entry page entry sec segmentation memory the main memory address of the start of the page is ex tracted from the page table entry the offset is added to the page origin to give the main memory ad dress where the word is located the read or store finally takes place segment descriptor segment descriptor segment descriptor segment descriptor segment descriptor page entry page table for segment segment number address within the segment segment descriptor segment descriptor descriptor segment fa page entry page entry page entry page table for segment figure a bit multics virtual address this process is illustrated in fig for simplicity the fact that the de scriptor segment is itself paged has been omitted what really happens is that a register the descriptor base register is used to locate the descriptor segment page table which in turn points to the pages of the descriptor segment once the descriptor for the needed segment has been found the addressing proceeds as shown in fig main memory address of the page table segment length in pages page size words words segment is paged segment is not paged miscellaneous bits protection bits b multics virtual address segment number page offset number figure the multics virtual memory a the descriptor segment points to the page tables b a segment descriptor the numbers are the field lengths the segment number is used to find the segment descriptor a check is made to see if the segment page table is in memory if segment number descriptor segment page table page offset the page table is in memory it is located if it is not a segment fault occurs if there is a protection violation a fault trap occurs the page table entry for the requested virtual page is examined if the page itself is not in memory a page fault is triggered if it is in figure conversion of a two part multics address into a main memory address as you have no doubt guessed by now if the preceding algorithm were ac tually carried out by the operating system on every instruction programs would memory management chap not run very fast in reality the multics hardware contains a word high speed tlb that can search all its entries in parallel for a given key it is illustrat ed in fig when an address is presented to the computer the addressing hardware first checks to see if the virtual address is in the tlb if so it gets the page frame number directly from the tlb and forms the actual address of the ref erenced word without having to look in the descriptor segment or page table comparison n entry segment virtual page number page frame protection age read write read only read write execute only execute only figure a simplified version of the multics tlb the existence of two page sizes makes the actual tlb more complicated the addresses of the most recently referenced pages are kept in the tlb programs whose working set is smaller than the tlb size will come to equili brium with the addresses of the entire working set in the tlb and therefore will run efficiently if the page is not in the tlb the descriptor and page tables are actually referenced to find the page frame address and the tlb is updated to in clude this page the least recently used page being thrown out the age field keeps track of which entry is the least recently used the reason that a tlb is used is for comparing the segment and page numbers of all the entries in parallel segmentation with paging the intel pentium in many ways the virtual memory on the pentium resembles that of mul tics including the presence of both segmentation and paging whereas mul tics has independent segments each up to bit words the pentium has 16k independent segments each holding up to billion bit words al though there are fewer segments the larger segment size is far more important as few programs need more than segments but many programs need large seg ments the heart of the pentium virtual memory consists of two tables called the ldt local descriptor table and the gdt global descriptor table each sec segmentation program has its own ldt but there is a single gdt shared by all the programs on the computer the ldt describes segments local to each program including its code data stack and so on whereas the gdt describes system segments in cluding the operating system itself to access a segment a pentium program first loads a selector for that segment into one of the machine six segment registers during execution the cs register holds the selector for the code segment and the ds register holds the selector for the data segment the other segment registers are less important each selector is a bit number as shown in fig bits t index gdt ldt privilege level figure a pentium selector one of the selector bits tells whether the segment is local or global i e whether it is in the ldt or gdt thirteen other bits specify the ldt or gdt entry number so these tables are each restricted to holding segment descrip tors the other bits relate to protection and will be described later descriptor is forbidden it may be safely loaded into a segment register to indicate that the segment register is not currently available it causes a trap if used at the time a selector is loaded into a segment register the corresponding de scriptor is fetched from the ldt or gdt and stored in microprogram registers so it can be accessed quickly as depicted in fig a descriptor consists of bytes including the segment base address size and other information the format of the selector has been cleverly chosen to make locating the de scriptor easy first either the ldt or gdt is selected based on selector bit then the selector is copied to an internal scratch register and the low order bits set to finally the address of either the ldt or gdt table is added to it to give a direct pointer to the descriptor for example selector refers to entry in the gdt which is located at address gdt let us trace the steps by which a selector offset pair is converted to a physi cal address as soon as the microprogram knows which segment register is being used it can find the complete descriptor corresponding to that selector in its inter nal registers if the segment does not exist selector or is currently paged out a trap occurs the hardware then uses the limit field to check if the offset is beyond the end of the segment in which case a trap also occurs logically there should be a bit field in the descriptor giving the size of the segment but there are only bits memory management chap sec segmentation overlapping probably because it would be too much trouble and take too much bit segment bit segment li is in bytes li is in page bits segmen t i absen t fro m memor y segmen t is presen t in memor y privilege level f syste m application segmen t typ e an d protection relativ e addres time to verify that they were all disjoint on the other hand if paging is enabled the linear address is interpreted as a virtual address and mapped onto the physical address using page tables pretty much as in our earlier examples the only real complication is that with a bit virtual address and a kb page a segment might contain million pages so a two level mapping is used to reduce the page table size for small segments each running program has a page directory consisting of bit entries it is located at an address pointed to by a global register each entry in this direc tory points to a page table also containing bit entries the page table en tries point to page frames the scheme is shown in fig linea r addres figure pentium code segment descriptor data segments differ slightly bits available so a different scheme is used if the gbit granularity field is the limit field is the exact segment size up to mb if it is the limit field gives the segment size in pages instead of bytes the pentium page size is fixed at kb so bits are enough for segments up to bytes assuming that the segment is in memory and the offset is in range the pen tium then adds the bit base field in the descriptor to the offset to form what is called a linear address as shown in fig the base field is broken up into three pieces and spread all over the descriptor for compatibility with the in which the base is only bits in effect the base field allows each segment to start at an arbitrary place within the bit linear address space offse t descriptor bi t linear addres figure conversion of a selector offset pair to a linear address if paging is disabled by a bit in a global control register the linear address is interpreted as the physical address and sent to the memory for the read or write thus with paging disabled we have a pure segmentation scheme with each seg ment base address given in its descriptor segments are not prevented from dir pag e offset a figure mapping of a linear address onto a physical address in fig a we see a linear address divided into three fields dir page and offset the dir field is used to index into the page directory to locate a point er to the proper page table then the page field is used as an index into the page table to find the physical address of the page frame finally offset is added to the address of the page frame to get the physical address of the byte or word needed the page table entries are bits each of which contain a page frame number the remaining bits contain access and dirty bits set by the hardware for the benefit of the operating system protection bits and other utility bits each page table has entries for kb page frames so a single page table handles megabytes of memory a segment shorter than will have a page memory managemen t chap directory with a single entry a pointer to its one and only page table in this way the overhead for short segments is only two pages instead of the million pages that would be needed in a one level page table to avoid making repeated references to memory the pentium like mul tics has a small tlb that directly maps the most recently used dir page com binations onto the physical address of the page frame only when the current com bination is not present in the tlb is the mechanism of fig actually carried out and the tlb updated as long as tlb misses are rare performance is good it is also worth noting that if some application does not need segmentation but is content with a single paged bit address space that model is possible all the segment registers can be set up with the same selector whose descriptor has base and limit set to the maximum the instruction offset will then be the lin ear address with only a single address space used in effect normal paging in fact all current operating systems for the pentium work this way os was the only one that used the full power of the intel mmu architecture all in all one has to give credit to the pentium designers given the conflict ing goals of implementing pure paging pure segmentation and paged segments while at the same time being compatible with the and doing all of this effi ciently the resulting design is surprisingly simple and clean although we have covered the complete architecture of the pentium virtual memory albeit briefly it is worth saying a few words about protection since this subject is intimately related to the virtual memory just as the virtual memory scheme is closely modeled on multics so is the protection system the pen tium supports four protection levels with level being the most privileged and level the least these are shown in fig at each instant a running pro gram is at a certain level indicated by a bit field in its psw each segment in the system also has a level as long as a program restricts itself to using segments at its own level every thing works fine attempts to access data at a higher level are permitted at tempts to access data at a lower level are illegal and cause traps attempts to call procedures at a different level higher or lower are allowed but in a carefully controlled way to make an interlevel call the call instruction must contain a selector instead of an address this selector designates a descriptor called a call gate which gives the address of the procedure to be called thus it is not possible to jump into the middle of an arbitrary code segment at a different level only official entry points may be used the concepts of protection levels and call gates were pioneered in multics where they were viewed as protection rings a typical use for this mechanism is suggested in fig at level we find the kernel of the operating system which handles i o memory management and other critical matters at level the system call handler is present user pro grams may call procedures here to have system calls carried out but only a spe cific and protected list of procedures may be called level contains library pro cedures possibly shared among many running programs user programs may call sec segmentation figure protection on the pentium these procedures and read their data but they may not modify them finally user programs run at level which has the least protection traps and interrupts use a mechanism similar to the call gates they too ref erence descriptors rather than absolute addresses and these descriptors point to specific procedures to be executed the type field in fig distinguishes be tween code segments data segments and the various kinds of gates research on memory management memory management especially paging algorithms was once a fruitful area for research but most of that seems to have largely died off at least for general purpose systems most real systems tend to use some variation on clock because it is easy to implement and relatively effective one recent exception however is a redesign of the bsd virtual memory system cranor and parulkar there is still research going on concerning paging in newer kinds of systems though for example cell phones and pdas have become small pcs and many of them page ram to disk only disk on a cell phone is flash memory which has different properties than a rotating magnetic disk some recent work is reported by in et al joo et al and park et al park et al have also looked at energy aware demand paging in mobile devices research is also taking place on modeling paging performance albers et al burton and kelly cascaval et al panagiotou and souza and peserico also of interest is memory management for multimedia sys tems dasigenis et al hand and real time systems pizlo and vitek memory management chap summar y in this chapter we have examined memory management we saw that the simplest systems do not swap or page at all once a program is loaded into mem ory it remains there in place until it finishes some operating systems allow only one process at a time in memory while others support multiprogramming the next step up is swapping when swapping is used the system can handle more processes than it has room for in memory processes for which there is no room are swapped out to the disk free space in memory and on disk can be kept track of with a bitmap or a hole list modern computers often have some form of virtual memory in the simplest form each process address space is divided up into uniform sized blocks called pages which can be placed into any available page frame in memory there are many page replacement algorithms two of the better algorithms are aging and wsclock paging systems can be modeled by abstracting the page reference string from the program and using the same reference string with different algorithms these models can be used to make some predictions about paging behavior to make paging systems work well choosing an algorithm is not enough attention to such issues as determining the working set memory allocation policy and page size is required segmentation helps in handling data structures that change size during execu tion and simplifies linking and sharing it also facilitates providing different pro tection for different segments sometimes segmentation and paging are combined to provide a two dimensional virtual memory the multics system and the intel pentium support segmentation and paging problems the ibm had a scheme of locking kb blocks by assigning each one a bit key and having the cpu compare the key on every memory reference to the bit key in the psw name two drawbacks of this scheme not mentioned in the text in fig the base and limit registers contain the same value is this just an accident or are they always the same if this is just an accident why are they the same in this example a swapping system eliminates holes by compaction assuming a random distribution chap problems consider a swapping system in which memory consists of the following hole sizes in memory order kb kb kb kb kb kb kb and kb which hole is taken for successive segment requests of a kb b kb c kb for first fit now repeat the question for best fit worst fit and next fit what is the difference between a physical address and a virtual address using the page table of fig give the physical address corresponding to each of the following virtual addresses a b c the amount of disk space that must be available for page storage is related to the max imum number of processes n the number of bytes in the virtual address space v and the number of bytes of ram r give an expression for the worst case disk space re quirements how realistic is this amount if an instruction takes nsec and a page fault takes an additional n nsec give a for mula for the effective instruction time if page faults occur every k instructions a machine has a bit address space and an kb page the page table is entirely in hardware with one bit word per entry when a process starts the page table is copied to the hardware from memory at one word every nsec if each process runs for msec including the time to load the page table what fraction of the cpu time is devoted to loading the page tables suppose that a machine has bit virtual addresses and bit physical addresses a if pages are kb how many entries are in the page table if it has only a single level explain b suppose this same system has a tlb translation lookaside buffer with en tries furthermore suppose that a program contains instructions that fit into one page and it sequentially reads long integer elements from an array that spans thousands of pages how effective will the tlb be for this case a computer with a bit address uses a two level page table virtual addresses are split into a bit top level page table field an bit second level page table field and an offset how large are the pages and how many are there in the address space suppose that a bit virtual address is broken up into four fields a b c and d the first three are used for a three level page table system the fourth field d is the offset does the number of pages depend on the sizes of all four fields if not which ones matter and which ones do not n a comrjuter has bit virtual addresses and kb pages the program and data toget es t page the stack fits in the highest page how en memory management chap tries are needed in the page table if traditional one level paging is used how many page table entries are needed for two level paging with bits in each part below is an execution trace of a program fragment for a computer with byte pages the program is located at address and its stack pointer is at the stack grows toward give the page reference string generated by this program each instruction occupies bytes word including immediate constants both in struction and data references count in the reference string load word into register push register onto the stack call a procedure at stacking the return address subtract the immediate constant from the stack pointer compare the actual parameter to the immediate constant jump if equal to a computer whose processes have pages in their address spaces keeps its page tables in memory the overhead required for reading a word from the page table is nsec to reduce this overhead the computer has a tlb which holds virtual page physical page frame pairs and can do a look up in nsec what hit rate is needed to reduce the mean overhead to nsec the tlb on the vax does not contain an r bit why how can the associative memory device needed for a tlb be implemented in hard ware and what are the implications of such a design for expandability a computer with an kb page a kb main memory and a gb virtual address space uses an inverted page table to implement its virtual memory how big should the hash table be to ensure a mean hash chain length of less than assume that the hash table size is a power of two a student in a compiler design course proposes to the professor a project of writing a compiler that will produce a list of page references that can be used to implement the optimal page replacement algorithm is this possible why or why not is there any thing that could be done to improve paging efficiency at run time suppose that the virtual page reference stream contains repetitions of long sequences of page references followed occasionally by a random page reference for example the sequence consists of repetitions of the sequence followed by a random reference to pages and a why won t the standard replacement algorithms lru fifo clock be effective in handling this workload for a page allocation that is less than the sequence length b if this program were allocated page frames describe a page replacement ap proach that would perform much better than the lru fifo or clock algorithms if fifo page replacement is used with four page frames and eight pages how many page faults will occur with the reference string if the four frames are ini tially empty now repeat this problem for lru chap problems consider the page sequence of fig b suppose that the r bits for the pages b through a are respectively which page will second chance remove a small computer has four page frames at the first clock tick the r bits are page is the rest are at subsequent clock ticks the values are 1010 and if the aging algorithm is used with an bit count er give the values of the four counters after the last tick suppose that in fig which page will be removed give a simple example of a page reference sequence where the first page selected for replacement will be different for the clock and lru page replacement algorithms as sume that a process is allocated frames and the reference string contains page num bers from the set suppose that the wsclock page replacement algorithm uses a x of two ticks and the system state is the following pag e tim e stam p v r m where the three flag bits stand for valid referenced and modified respectively a if a clock interrupt occurs at tick show the contents of the new table entries explain you can omit entries that are unchanged b suppose that instead of a clock interrupt a page fault occurs at tick due to a read request to page show the contents of the new table entries explain you can omit entries that are unchanged how long does it take to load a kb program from a disk whose average seek time is msec whose rotation time is msec and whose tracks hold kb a for a kb page size b for a kb page size the pages are spread randomly around the disk and the number of cylinders is so large that the chance of two pages being on the same cylinder is negligible a computer has four page frames the time of loading time of last access and the r and m bits for each page are as shown below the times are in clock ticks pag e loade d las t ref r m memory management chap a which page will nru replace b which page will fifo replace c which page will lru replace d which page will second chance replace one of the first timesharing machines the pdp had a memory of bit words it held one process at a time in memory when the scheduler decided to run another process the process in memory was written to a paging drum with 4k bit words around the circumference of the drum the drum could start writing or reading at any word rather than only at word why do you suppose this drum was chosen a computer provides each process with bytes of address space divided into pages of bytes a particular program has a text size of bytes a data size of bytes and a stack size of bytes will this program fit in the address space if the page size were bytes would it fit remember that a page may not contain parts of two different segments if a page is shared between two processes is it possible that the page is read only for one process and read write for the other why or why not it has been observed that the number of instructions executed between page faults is directly proportional to the number of page frames allocated to a program if the available memory is doubled the mean interval between page faults is also doubled suppose that a normal instruction takes microsec but if a page fault occurs it takes usee i e msec to handle the fault if a program takes sec to run during which time it gets page faults how long would it take to run if twice as much memory were available a group of operating system designers for the frugal computer company are thinking about ways to reduce the amount of backing store needed in their new operating sys tem the head guru has just suggested not bothering to save the program text in the swap area at all but just page it in directly from the binary file whenever it is needed under what conditions if any does this idea work for the program text under what conditions if any does it work for the data a machine language instruction to load a bit word into a register contains the bit address of the word to be loaded what is the maximum number of page faults this instruction can cause explain the difference between internal fragmentation and external fragmentation which one occurs in paging systems which one occurs in systems using pure seg mentation when segmentation and paging are both being used as in multics first the seg ment descriptor must be looked up then the page descriptor does the tlb also work this way with two levels of lookup we consider a program which has the two segments shown below consisting of in structions in segment and read write data in segment segment has read execute protection and segment has read write protection the memory system is a de mand paged virtual memory system with virtual addresses that have a bit page num chap problems ber and an bit offset the page tables and protection are as follows all numbers in the table are in decimal segmen t segmen t read e ecut e read writ e virtua l f ag e pag e fram e virtua l pag e pag e fram e o n disk o n dis k o n dis k o n disk o n dis k for each of the following cases either give the real actual memory address which re sults from dynamic address translation or identify the type of fault which occurs ei ther page or protection fault a fetch from segment page offset b store into segment page offset c fetch from segment page offset d jump to location in segment page offset can you think of any situations where supporting virtual memory would be a bad idea and what would be gained by not having to support virtual memory explain virtual memory provides a mechanism for isolating one process from another what memory management difficulties would be involved in allowing two operating sys tems to run concurrently how might these difficulties be addressed plot a histogram and calculate the mean and median of the sizes of executable binary files on a computer to which you have access on a windows system look at all exe and dll files on a unix system look at all executable files in bin hisr bin and aocal bin that are not scripts or use the file utility to find all executables determine the optimal page size for this computer just considering the code not data consider internal fragmentation and page table size making some reasonable assumption about the size of a page table entry assume that all programs are equally likely to be run and thus should be weighted equally small programs for ms dos can be compiled as com files these files are always loaded at address in a single memory segment that is used for code data and stack instructions that transfer control of execution such as jmp and call or that ac cess static data from fixed addresses have the addresses compiled into the object code write a program that can relocate such a program file to run starting at an arbitrary ad dress your program must scan through code looking for object codes for instructions that refer to fixed memory addresses then modify those addresses that point to memo ry locations within the range to be relocated you can find the object codes in an as memory management chap sembly language programming text note that doing this perfectly without additional information is in general an impossible task because some data words may have values that mimic instruction object codes write a program that simulates a paging system using the aging algorithm the num ber of page frames is a parameter the sequence of page references should be read from a file for a given input file plot the number of page faults per memory ref erences as a function of the number of page frames available write a program that demonstrates the effect of tlb misses on the effective memory access time by measuring the per access time it takes to stride through a large array a explain the main concepts behind the program and describe what you expect the output to show for some practical virtual memory architecture b run the program on some computer and explain how well the data fit your expec tations c repeat part b but for an older computer with a different architecture and explain any major differences in the output write a program that will demonstrate the difference between using a local page replacement policy and a global one for the simple case of two processes you will need a routine that can generate a page reference string based on a statistical model this model has n states numbered from to n representing each of the possible page references and a probability p associated with each state i representing the chance that the next reference is to the same page otherwise the next page reference will be one of the other pages with equal probability a demonstrate that the page reference string generation routine behaves properly for some small n b compute the page fault rate for a small example in which there is one process and a fixed number of page frames explain why the behavior is correct c repeat part b with two processes with independent page reference sequences and twice as many page frames as in part b d repeat part c but using a global policy instead of a local one also contrast the per process page fault rate with that of the local policy approach file systems all computer applications need to store and retrieve information twhile a process is running it can store a limited amount of information within its own ad dress space however the storage capacity is restricted to the size of the virtual address space for some applications this size is adequate but for others such as airline reservations banking or corporate record keeping it is far too small a second problem with keeping information within a process address space is that when the process terminates the information is lost for many applications e g for databases the information must be retained for weeks months or even forever having it vanish when the process using it terminates is unacceptable furthermore it must not go away when a computer crash kills the process a third problem is that it is frequently necessary for multiple processes to ac cess parts of the information at the same time if we have an online telephone directory stored inside the address space of a single process only that process can access it the way to solve this problem is to make the information itself indepen dent of any one process thus we have three essential requirements for long term information storage it must be possible to store a very large amount of information the information must survive the termination of the process using it multiple processes must be able to access the information concurrently magnetic disks have been used for years for this long term storage tapes and optical disks are also used but they have much lower performance we will study file systems chap disks more in chap but for the moment it is sufficient to think of a disk as a linear sequence of fixed size blocks and supporting two operations read block k write block k in reality there are more but with these two operations one could in principle solve the long term storage problem however these are very inconvenient operations especially on large systems used by many applications and possibly multiple users e g on a server just a few of the questions that quickly arise are how do you find information how do you keep one user from reading another user data how do you know which blocks are free and there are many more just as we saw how the operating system abstracted away the concept of the processor to create the abstraction of a process and how it abstracted away the concept of physical memory to offer processes virtual address spaces we can solve this problem with a new abstraction the file together the abstractions of processes and threads address spaces and files are the most important concepts relating to operating systems if you really understand these three concepts from beginning to end you are well on your way to becoming an operating systems expert files are logical units of information created by processes a disk will usual ly contains thousands or even millions of them each one independent of the oth ers in fact if you think of each file as a kind of address space you are not that far off except that they are used to model the disk instead of modeling the ram processes can read existing files and create new ones if need be information stored in files must be persistent that is not be affected by process creation and termination a file should only disappear when its owner explicitly removes it although operations for reading and writing files are the most common ones there exist many others some of which we will examine below files are managed by the operating system how they are structured named accessed used protected implemented and managed are major topics in operat ing system design as a whole that part of the operating system dealing with files is known as the file system and is the subject of this chapter from the user standpoint the most important aspect of a file system is how it appears that is what constitutes a file how files are named and protected what operations are allowed on files and so on the details of whether linked lists or bitmaps are used to keep track of free storage and how many sectors there are in a logical disk block are of no interest although they are of great importance to the designers of the file system for this reason we have structured the chapter as several sections the first two are concerned with the user interface to files and directories respectively then comes a detailed discussion of how the file system is implemented and managed finally we give some examples of real file sys tems files in the following pages we will look at files from the user point of view that is how they are used and what properties they have file naming files are an abstraction mechanism they provide a way to store information on the disk and read it back later this must be done in such a way as to shield the user from the details of how and where the information is stored and how the disks actually work probably the most important characteristic of any abstraction mechanism is the way the objects being managed are named so we will start our examination of file systems with the subject of file naming when a process creates a file it gives the file a name when the process terminates the file continues to exist and can be accessed by other processes using its name the exact rules for file naming vary somewhat from system to system but all current operating systems allow strings of one to eight letters as legal file names thus andrea bruce and cathy are possible file names frequently digits and spe cial characters are also permitted so names like urgent and fig are often valid as well many file systems support names as long as characters some file systems distinguish between upper and lower case letters whereas others do not unix falls in the first category ms dos falls in the second thus a unix system can have all of the following as three distinct files maria maria and maria in ms dos all these names refer to the same file an aside on file systems is probably in order here windows and windows both use the ms dos file system called fat and thus inherit many of its properties such as how file names are constructed windows introduced some extensions to fat leading to fat but these two are quite similar in ad dition windows nt windows windows xp and wv support both fat file systems which are really obsolete now these four nt based operating sys tems have a native file system ntfs that has different properties such as file names in unicode in this chapter when we refer to the ms dos or fat file systems we mean fat and fat as used on windows unless specified otherwise we will discuss the fat file systems later in this chapter and ntfs in chap where we will examine windows vista in detail file systems chap many operating systems support two part file names with the two parts sepa rated by a period as in prog c the part following the period is called the file extension and usually indicates something about the file in ms dos for ex ample file names are to characters plus an optional extension of to char acters in unix the size of the extension if any is up to the user and a file may even have two or more extensions as in homepage html zip where html indicates a web page in html and zip indicates that the file homepage html has been compressed using the zip program some of the more common file extensions and their meanings are shown in fig extension meaning file bak backup file file c c m irr fi program file gif compuserve graphical interchange format image file hip help file file htmi world wide web hypertext markup language document file jpg still picture encoded with the jpeg standard file music encoded in mpeg layer audio format file mpg movie encoded with the mpeg standard file o object file compiler output not yet linked file pdf portable document format file fiie ps postscript file file tex input for the tex formatting program file txt general text file fiie zip compressed archive figure some typical file extensions in some systems e g unix file extensions are just conventions and are not enforced by the operating system a file named file txt might be some kind of text file but that name is more to remind the owner than to convey any actual infor mation to the computer on the other hand a c compiler may actually insist that files it is to compile end in c and it may refuse to compile them if they do not conventions like this are especially useful when the same program can handle several different kinds of files the c compiler for example can be given a list of several files to compile and link together some of them c files and some of them assembly language files the extension then becomes essential for the compiler to tell which are c files which are assembly files and which are other files in contrast windows is aware of the extensions and assigns meaning to them users or processes can register extensions with the operating system and specify for each one which program owns that extension when a user double clicks on sec files a file name the program assigned to its file extension is launched with the file as parameter for example double clicking onfile doc starts microsoft word with file doc as the initial file to edit file structure files can be structured in any of several ways three common possibilities are depicted in fig the file in fig a is an unstructured sequence of bytes in effect the operating system does not know or care what is in the file all it sees are bytes any meaning must be imposed by user level programs both unix and windows use this approach byt e record figure three kinds of files a byte sequence b record sequence c tree having the operating system regard files as nothing more than byte sequences provides the maximum flexibility user programs can put anything they want in their files and name them any way that is convenient the operating system does not help but it also does not get in the way for users who want to do unusual things the latter can be very important all versions of unix ms dos and win dows use this file model the first step up in structure is shown in fig b in this model a file is a sequence of fixed length records each with some internal structure central to the idea of a file being a sequence of records is the idea that the read operation returns one record and the write operation overwrites or appends one record as a histori cal note in decades gone by when the column punched card was king many mainframe operating systems based their file systems on files consisting of character records in effect card images these systems also supported files of file systems chap sec files character records which were intended for the line printer which in those days were big chain printers having columns programs read input in units of characters and wrote it in units of characters although the final could be spaces of course no current general purpose system uses this model as its primary file system any more but back in the days of column punched cards and character line printer paper this was a common model on mainframe computers the third kind of file structure is shown in fig c in this organization a file consists of a tree of records not necessarily all the same length each con taining a key field in a fixed position in the record the tree is sorted on the key field to allow rapid searching for a particular key the basic operation here is not to get the next record although that is also possible but to get the record with a specific key for the zoo file of fig c one could ask the system to get the record whose key is pony for example with for example in fig a we see a simple executable binary file taken from an early version of unix although technically the file is just a sequence of bytes the operating system will only execute a file if it has the proper format it has five sections header text data relocation bits and symbol table the header starts with a so called magic number identifying the file as an executable file to prevent the accidental execution of a file not in this format then come the sizes of the various pieces of the file the address at which execution starts and some flag bits following the header are the text arid data of the program itself these are loaded into memory and relocated using the relocation bits the symbol table is used for debugging modul e out worrying about its exact position in the file furthermore new records can be added to the file with the operating system and not the user deciding where to place them this type of file is clearly quite different from the unstructured byte streams used in unix and windows but is widely used on the large mainframe computers still used in some commercial data processing file types many operating systems support several types of files unix and windows for example have regular files and directories unix also has character and block special files regular files are the ones that contain user information all the files of fig are regular files directories are system files for maintaining the structure of the file system we will study directories below character special files are related to input output and used to model serial i o devices such as ter minals printers and networks block special files are used to model disks in this chapter we will be primarily interested in regular files regular files are generally either ascii files or binary files ascii files con sist of lines of text in some systems each line is terminated by a carriage return character in others the line feed character is used some systems e g ms dos use both lines need not all be of the same length the great advantage of ascii files is that they can be displayed and printed as is and they can be edited with any text editor furthermore if large numbers of programs use ascii files for input and output it is easy to connect the output of one program to the input of another as in shell pipelines the interprocess plumbing is not any easier but interpreting the information certainly is if a stan magic number text siz e dat a siz e bs s siz e symbo l table siz e entry point flag text dat a relocatio n bits symbo l table a heade r objec t modul e heade r objec t modul e heade r objec t modul e b nam e dat e owner protection size dard convention such as ascii is used for expressing it other files are binary which just means that they are not ascii files listing them on the printer gives an incomprehensible listing full of random junk usual ly they have some internal structure known to programs that use them figure a an executable file b an archive our second example of a binary file is an archive also from unix it consists of a collection of library procedures modules compiled but not linked each one is prefaced by a header telling its name creation date owner protection code and file systems chap size just as with the executable file the module headers are full of binary num bers copying them to the printer would produce complete gibberish every operating system must recognize at least one file type its own ex ecutable file but some recognize more the old tops system for the decsystem went so far as to examine the creation time of any file to be exe cuted then it located the source file and saw if the source had been modified since the binary was made if it had been it automatically recompiled the source in unix terms the make program had been built into the shell the file extensions were mandatory so the operating system could tell which binary program was derived from which source having strongly typed files like this causes problems whenever the user does anything that the system designers did not expect consider as an example a sys tem in which program output files have extension dat data files if a user writes a program formatter that reads a c file c program transforms it e g by con verting it to a standard indentation layout and then writes the transformed file as output the output file will be of type dat if the user tries to offer this to the c compiler to compile it the system will refuse because it has the wrong extension attempts to copy fde dat lofde c will be rejected by the system as invalid to pro tect the user against mistakes while this kind of user friendliness may help novices it drives experienced users up the wall since they have to devote considerable effort to circumventing the operating system idea of what is reasonable and what is not file access early operating systems provided only one kind of file access sequential access in these systems a process could read all the bytes or records in a file in order starting at the beginning but could not skip around and read them ouf of order sequential files could be rewound however so they could be read as often as needed sequential files were convenient when the storage medium was mag netic tape rather than disk when disks came into use for storing files it became possible to read the bytes or records of a file out of order or to access records by key rather than by position files whose bytes or records can be read in any order are called random access files they are required by many applications random access files are essential for many applications for example data base systems if an airline customer calls up and wants to reserve a seat on a par ticular flight the reservation program must be able to access the record for that flight without having to read the records for thousands of other flights first two methods can be used for specifying where to start reading in the first one every read operation gives the position in the file to start reading at in the second one a special operation seek is provided to set the current position after a seek the file can be read sequentially from the now current position the latter method is used in unix and windows sec piles file attributes every file has a name and its data in addition all operating systems associate other information with each file for example the date and time the file was last modified and the file size we will call these extra items the file attributes some people call them metadata the list of attributes varies considerably from system to system the table of fig shows some of the possibilities but other ones also exist no existing system has all of these but each one is present in some system attribute meaning protection who can access the file and in what wav password password needed to access the file creator id of the person who created the file owner current owner read only flag for read write for read only hidden flag for normal for do not display in listings system flag for normal files for system file archive flag for has been backed up for needs to be backed up ascii binary flag for ascii file for binary file random access flag for sequential access only for random access temporary flag for normal for delete file on process exit lock flags for unlocked nonzero for locked record length number of bytes in a record key position offset of the key within each record key length number of bytes in the key field creation time date and time the file was created time of last access date and time the file was last accessed time of last chanqe date and time the file was last chanqed current size number of bytes in the file maximum size number of bytes the file may grow to figure some possible file attributes the first four attributes relate to the file protection and tell who may access it and who may not all kinds of schemes are possible some of which we will study later in some systems the user must present a password to access a file in wmcn case the password must be one of the attributes the flags are bits or short fields that control or enable some specific property hidden files for example do not appear in listings of all the files the archive file systems chap flag is a bit that keeps track of whether the file has been backed up recently the backup program clears it and the operating system sets it whenever a file is changed in this way the backup program can tell which files need backing up the temporary flag allows a file to be marked for automatic deletion when the process that created it terminates the record length key position and key length fields are only present in files whose records can be looked up using a key they provide the information re quired to find the keys the various times keep track of when the file was created most recently ac cessed and most recently modified these are useful for a variety of purposes for example a source file that has been modified after the creation of the corres ponding object file needs to be recompiled these fields provide the necessary information the current size tells how big the file is at present some old mainframe oper ating systems require the maximum size to be specified when the file is created in order to let the operating system reserve the maximum amount of storage in ad vance workstation and personal computer operating systems are clever enough to do without this feature file operations files exist to store information and allow it to be retrieved later different sys tems provide different operations to allow storage and retrieval below is a dis cussion of the most common system calls relating to files create the file is created with no data the purpose of the call is to announce that the file is coming and to set some of the attributes delete when the file is no longer needed it has to be deleted to free up disk space there is always a system call for this purpose open before using a file a process must open it the purpose of the open call is to allow the system to fetch the attributes and list of disk addresses into main memory for rapid access on later calls close when all the accesses are finished the attributes and disk ad dresses are no longer needed so the file should be closed to free up internal table space many systems encourage this by imposing a maximum number of open files on processes a disk is written in blocks and closing a file forces writing of the file last block even though that block may not be entirely full yet read data are read from file usually the bytes come from the cur rent position the caller must specify how many data are needed and must also provide a buffer to put them in sec files write data are written to the file again usually at the current posi tion if the current position is the end of the file the file size increases if the current position is in the middle of the file existing data are overwritten and lost forever append this call is a restricted form of write it can only add data to the end of the file systems that provide a minimal set of system calls do not generally have append but many systems provide multi ple ways of doing the same thing and these systems sometimes have append seek for random access files a method is needed to specify from where to take the data one common approach is a system call seek that repositions the file pointer to a specific place in the file after this call has completed data can be read from or written to that position get attributes processes often need to read file attributes to do their work for example the unix make program is commonly used to manage software development projects consisting of many source files when make is called it examines the modification times of all the source and object files and arranges for the minimum number of compilations required to bring everything up to date to do its job it must look at the attributes namely the modification times set attributes some of the attributes are user settable and can be changed after the file has been created this system call makes that possible the protection mode information is an obvious example most of the flags also fall in this category rename it frequently happens that a user needs to change the name of an existing file this system call makes that possible it is not al ways strictly necessary because the file can usually be copied to a new file with the new name and the old file then deleted an example program using file system calls in this section we will examine a simple unix program that copies one file from its source file to a destination file it is listed in fig the program has minimal functionality and even worse error reporting but it gives a reasonable idea of how some of the system calls related to files work the program copyfde can be called for example by the command line copyfile abc xyz to copy the file abc to xyz if xyz already exists it will be overwritten otherwise file systems chap file copy program error checking and reporting is minimal include ty es b n c l u d e n e c e a r y h e a d e r include fcntl h ffinclude stdlib h include unistd h sec files but will not concern us further the next line is a function prototype for main something required by ansi c but also not important for our purposes the first tfdefine statement is a macro definition that defines the character string bufsize as a macro that expands into the number the program will read and write in chunks of bytes it is considered good programming practice to give names to constants like this and to use the names instead of the constants not only does this convention make programs easier to read but it also int main int argc char argvq define define int main int argc char argv int char buffer if argc exit ansi prototype us e a buffer size of bytes protection bits for output file syntax error if argc is not makes them easier to maintain the second define statement determines who can access the output file the main program is called main and it has two arguments argc and argv these are supplied by the operating system when the program is called the first one tells how many strings were present on the command line that invoked the program including the program name it should be the second one is an array of pointers to the arguments in the example call given above the elements of this array would contain pointers to the following values argv copyfile open the input file and create the output file open argv open the source file if exit if it cannot be opened exit out jd creat argv create the destination file l if exit if it cannot be created exit copy loop while true rd count read buffer read a block of data if break if end of file or error exit loop write buffer write data if exit is an error close the files close close out fd if no error on last read exit else exit error on last read figure a simple program to copy a file it will be created the program must be called with exactly two arguments both legal file names the first is the source the second is the output file the four include statements near the top of the program cause a large num ber of definitions and function prototypes to be included in the program these are needed to make the program conformant to the relevant international standards argv l abc argv xyz it is via this array that the program accesses its arguments five variables are declared the first two and out fd will hold the file descriptors small integers returned when a file is opened the next two rd count and wt count are the byte counts returned by the read and write system calls respectively the last one buffer is the buffer used to hold the data read and supply the data to be written the first actual statement checks argc to see if it is if not it exits with stat us code any status code other than means that an error has occurred the status code is the only error reporting present in this program a production ver sion would normally print error messages as well then we try to open the source file and create the destination file if the source file is successfully opened the system assigns a small integer to in fd to identify the file subsequent calls must include this integer so that the system knows which file it wants similarly if the destination is successfully created out fd is given a value to identify it the second argument to creat sets the pro tection mode if either the open or the create fails the corresponding file descrip tor is set to and the program exits with an error code now comes the copy loop it starts by trying to read in kb of data to buffer it does this by calling the library procedure read which actually invokes the read system call the first parameter identifies the file the second gives the buffer and the third tells how many bytes to read the value assigned to rd count gives the number of bytes actually read normally this will be except if fewer bytes are remaining in the file when the end of file ihas been reached it will be if file systems chap rd count is ever zero or negative the copying cannot continue so the break state ment is executed to terminate the otherwise endless loop the call to write outputs the buffer to the destination file the first parameter identifies the file the second gives the buffer and the third tells how many bytes to write analogous to read note that the byte count is the number of bytes ac tually read not buf size this point is important because the last read will not return unless the file just happens to be a multiple of kb when the entire file has been processed the first call beyond the end of file will return to which will make it exit the loop at this point the two files are closed and the program exits with a status indicating normal termination although the windows system calls are different from those of unix the general structure of a command line windows program to copy a file is moderate ly similar to that of fig we will examine the windows vista calls in chap directorie s to keep track of files file systems normally have directories or folders which in many systems are themselves files in this section we will discuss direc tories their organization their properties and the operations that can be perform ed on them single level directory systems the simplest form of directory system is having one directory containing all the files sometimes it is called the root directory but since it is the only one the name does not matter much on early personal computers this system was com mon in part because there was only one user interestingly enough the world first supercomputer the cdc also had only a single directory for all files even though it was used by many users at once this decision was no doubt made to keep the software design simple an example of a system with one directory is given in fig here the di rectory contains four files the advantages of this scheme are its simplicity and the ability to locate files quickly there is only one place to look after all it is often used on simple embedded devices such as telephones digital cameras and some portable music players hierarchical directory systems the single level is adequate for simple dedicated applications and was even used on the first personal computers but for modern users with thousands of files it would be impossible to find anything if all files were in a single directory sec directories figure a single level directory system containing four files consequently a way is needed to group related files together a professor for example might have a collection of files that together form a book that he is writ ing for one course a second collection of files containing student programs sub mitted for another course a third group of files containing the code of an ad vanced compiler writing system he is building a fourth group of files containing grant proposals as well as other files for electronic mail minutes of meetings papers he is writing games and so on what is needed is a hierarchy i e a tree of directories with this approach there can be as many directories as are needed to group the files in natural ways furthermore if multiple users share a common file server as is the case on many company networks each user can have a private root directory for his or her own hierarchy this approach is shown in fig here the directories a b and c contained in the root directory each belong to a different user two of whom have created subdirectories for projects they are working on di figure a hierarchical directory system the ability for users to create an arbitrary number of subdirectories provides a powerful structuring tool for users to organize their work for this reason nearly all modern file systems are organized in this manner path names when the file system is organized as a directory tree some way is needed for specifying file names two different methods are commonly used in the first method each file is given an absolute path name consisting of the path from the file systems chap root directory to the file as an example the path usr ast mailbox means that the root directory contains a subdirectory usr which in turn contains a subdirectory ast which contains the file mailbox absolute path names always start at the root directory and are unique in unix the components of the path are separated by in windows the separator is in multics it was thus the same path name would be written as follows in these three systems windows usr ast maiibox unix usr ast maiibox multics usr ast mailbox no matter which character is used if the first character of the path name is the separator then the path is absolute the other kind of name is the relative path name this is used in conjunc tion with the concept of the working directory also called the current direc tory a user can designate one directory as the current working directory in which case all path names not beginning at the root directory are taken relative to the working directory for example if the current working directory is usr ast then the file whose absolute path is usr ast mailbox can be referenced simply as mailbox in other words the unix command cp usr ast mailbox usr ast mailbox bak and the command cp mailbox mailbox bak do exactly the same thing if the working directory is usr ast the relative form is often more convenient but it does the same thing as the absolute form some programs need to access a specific file without regard to what the work ing directory is in that case they should always use absolute path names for example a spelling checker might need to read usr lib dictionary to do its work it should use the full absolute path name in this case because it does not know what the working directory will be when it is called the absolute path name will always work no matter what the working directory is of course if the spelling checker needs a large number of files from usr lib an alternative approach is for it to issue a system call to change its working direc tory to usr lib and then use just dictionary as the first parameter to open by explicitiy changing the working directory it knows for sure where it is in the di rectory tree so it can then use relative paths each process has its own working directory so when it changes its working directory and later exits no other processes are affected and no traces of the change are left behind in the file system in this way it is always perfectly safe for a process to change its working directory whenever that is convenient on the other hand if a library procedure changes the working directory and does not change back to where it was when it is finished the rest of the program may not sec directories work since its assumption about where it is may now suddenly be invalid for this reason library procedures rarely change the working directory and when they must they always change it back again before returning most operating systems that support a hierarchical directory system have two special entries in every directory and generally pronounced dot and dotdot dot refers to the current directory dotdot refers to its parent except in the root directory where it refers to itself to see how these are used consider the unix file tree of fig a certain process has usr ast as its working direc tory it can use to go higher up the tree for example it can copy the file usr lib dictionary to its own directory using the command cp lib dictionary the first path instructs the system to go upward to the usr directory then to go down to the directory lib to find the file dictionary bin etc lib usr tmp figure a unix directory tree the second argument dot names the current directory when the cp com mand gets a directory name including dot as its last argument it copies all the file systems chap files to that directory of course a more normal way to do the copy would be to use the full absolute path name of the source file cp usr lib dictionary here the use of dot saves the user the trouble of typing dictionary a second time nevertheless typing cp usr lib dictionary dictionary also works fine as does cp usr lib dictionary usr ast dictionary all of these do exactly the same thing directory operations the allowed system calls for managing directories exhibit more variation from system to system than system calls for files to give an impression of what they are and how they work we will give a sample taken from unix create a directory is created it is empty except for dot and dotdot which are put there automatically by the system or in a few cases by the mkdir program delete a directory is deleted only an empty directory can be de leted a directory containing only dot and dotdot is considered em pty as these cannot usually be deleted opendir directories can be read for example to list all the files in a directory a listing program opens the directory to read out the names of all the files it contains before a directory can be read it must be opened analogous to opening and reading a file closedir when a directory has been read it should be closed to free up internal table space readdir this call returns the next entry in an open directory form erly it was possible to read directories using the usual read system call but that approach has the disadvantage of forcing the pro grammer to know and deal with the internal structure of directories in contrast readdir always returns one entry in a standard format no matter which of the possible directory structures is being used rename in many respects directories are just like files and can be renamed the same way files can be link linking is a technique that allows a file to appear in more than one directory this system call specifies an existing file and a path sec directories name and creates a link from the existing file to the name specified by the path in this way the same file may appear in multiple direc tories a link of this kind which increments the counter in the file i node to keep track of the number of directory entries containing the file is sometimes called a hard link unlink a directory entry is removed if the file being unlinked is only present in one directory the normal case it is removed from the file system if it is present in multiple directories only the path name specified is removed the others remain in unix the system call for deleting files discussed earlier is in fact unlink the above list gives the most important calls but there are a few others as well for example for managing the protection information associated with a directory a variant on the idea of linking files is the symbolic link instead of having two names point to the same internal data structure representing a file a name can be created that points to a tiny file naming another file when the first file is used for example opened the file system follows the path and finds the name at the end then it starts the lookup process all over using the new name symbolic links have the advantage that they can cross disk boundaries and even name files on remote computers their implementation is somewhat less efficient than hard links though fil e syste m implementatio n now it is time to turn from the user view of the file system to the imple mentor view users are concerned with how files are named what operations are allowed on them what the directory tree looks like and similar interface is sues implementors are interested in how files and directories are stored how disk space is managed and how to make everything work efficiently and reliably in the following sections we will examine a number of these areas to see what the is sues and trade offs are file system layout file systems are stored on disks most disks can be divided up into one or more partitions with independent file systems on each partition sector of the disk is called the mbr master boot record and is used to boot the computer the end of the mbr contains the partition table this table gives the starting and ending addresses of each partition one of the partitions in the table is marked as active when the computer is booted the bios reads in and executes the mbr the first thing the mbr program does is locate the active partition read in its first block called the boot block and execute it the program in the boot block loads file systems chap the operating system contained in that partition for uniformity every partition starts with a boot block even if it does not contain a bootable operating system besides it might contain one in the future other than starting with a boot block the layout of a disk partition varies a lot from file system to file system often the file system will contain some of the items shown in fig the first one is the superblock it contains all the key parameters about the file system and is read into memory when the computer is booted or the file system is first touched typical information in the superblock includes a magic number to identify the file system type the number of blocks in the file system and other key administrative information partition table entire disk disk partition mb r iii boot block superbloc k fre e spac e mgm t l n ode roo t dir files an d directories figure a possible file system layout next might come information about free blocks in the file system for ex ample in the form of a bitmap or a list of pointers this might be followed by die i nodes an array of data structures one per file telling all about the file after that might come the root directory which contains the top of the file system tree finally the remainder of the disk contains all the other directories and files implementing files probably the most important issue in implementing file storage is keeping track of which disk blocks go with which file various methods are used in dif ferent operating systems in this section we will examine a few of them contiguous allocation the simplest allocation scheme is to store each file as a contiguous run of disk blocks thus on a disk with kb blocks a kb file would be allocated con secutive blocks with kb blocks it would be allocated consecutive blocks sec file system implementation we see an example of contiguous storage allocation in fig a here the first disk blocks are shown starting with block on the left initially the disk was empty then a file a of length four blocks was written to disk starting at the beginning block after that a six block file b was written starting right after the end of file a note that each file begins at the start of a new block so that if file a was real ly blocks some space is wasted at the end of the last block in the figure a total of seven files are shown each one starting at the block following the end of the previous one shading is used just to make it easier to tell the files apart it has no actual significance in terms of storage file a file c file e file g blocks blocks blocks blocks ii iuxlll i m i j ifht rtttttt i i i mmm kurr r file b file d file f blocks blocks blocks a file a file c file e fil e g i tvi l g i g i ii it i mi l i i i i frkhms l i i i i in file b s fre e blocks fre e blocks b figure a contiguous allocation of disk space for seven files b the state of the disk after files d and f have been removed contiguous disk space allocation has two significant advantages first it is simple to implement because keeping track of where a file blocks are is reduced to remembering two numbers the disk address of the first block and the number of blocks in the file given the number of the first block the number of any other block can be found by a simple addition second the read performance is excellent because the entire file can be read from the disk in a single operation only one seek is needed to the first block after that no more seeks or rotational delays are needed so data come in at the full bandwidth of the disk thus contiguous allocation is simple to implement and has high performance unfortunately contiguous allocation also has a fairly significant drawback over the course of time the disk becomes fragmented to see how this comes file systems chap about examine fig b here two files d and f have been removed when a file is removed its blocks are naturally freed leaving a run of free blocks on the disk the disk is not compacted on the spot to squeeze out the hole since that would involve copying all the blocks following the hole potentially millions of blocks as a result the disk ultimately consists of files and holes as illustrated in the figure initially this fragmentation is not a problem since each new file can be writ ten at the end of disk following the previous one however eventually the disk will fill up and it will become necessary to either compact the disk which is prohibitively expensive or to reuse the free space in the holes reusing the space requires maintaining a list of holes which is doable however when a new file is to be created it is necessary to know its final size in order to choose a hole of the correct size to place it in imagine the consequences of such a design the user starts a text editor or word processor in order to type a document the first thing the program asks is how many bytes the final document will be the question must be answered or the program will not continue if the number given ultimately proves too small the program has to terminate prematurely because the disk hole is full and there is no place to put the rest of the file if the user tries to avoid this problem by giving an unrealistically large number as the final size say mb the editor may be un able to find such a large hole and announce that the file cannot be created of course the user would be free to start the program again and say mb this time and so on until a suitable hole was located still this scheme is not likely to lead to happy users however there is one situation in which contiguous allocation is feasible and in fact widely used on cd roms here all the file sizes are known in advance and will never change during subsequent use of the cd rom file system we will study the most common cd rom file system later in this chapter the situation with dvds is a bit more complicated in principle a min movie could be encoded as a single file of length about gb but the file system used udf universal disk format uses a bit number to represent file length which limits files to gb as a consequence dvd movies are generally stored as three or four gb files each of which is contiguous these physical pieces of the single logical file the movie are called extents as we mentioned in chap history often repeats itself in computer science as new generations of technology occur contiguous allocation was actually used on magnetic disk file systems years ago due to its simplicity and high per formance user friendliness did not count for much then then the idea was dropped due to the nuisance of having to specify final file size at file creation time but with the advent of cd roms dvds and other write once optical me dia suddenly contiguous files are a good idea again it is thus important to study old systems and ideas that were conceptually clean and simple because they may be applicable to future systems in surprising ways sec file system implementation linked list allocation the second method for storing files is to keep each one as a linked list of disk blocks as shown in fig the first word of each block is used as a pointer to the next one the rest of the block is for data file a physical block file b physica l block figure storing a file as a linked list of disk blocks unlike contiguous allocation every disk block can be used in this method no space is lost to disk fragmentation except for internal fragmentation in the last block also it is sufficient for the directory entry to merely store the disk ad dress of the first block the rest can be found starting there on the other hand although reading a file sequentially is straightforward ran dom access is extremely slow to get to block n the operating system has to start at the beginning and read the n blocks prior to it one at a time clearly doing so many reads will be painfully slow also the amount of data storage in a block is no longer a power of two be cause the pointer takes up a few bytes while not fatal having a peculiar size is less efficient because many programs read and write in blocks whose size is a power of two with the first few bytes of each block occupied to a pointer to the next block reads of the full block size require acquiring and concatenating infor mation from two disk blocks which generates extra overhead due to the copying linked list allocation using a table in memory both disadvantages of the linked list allocation can be eliminated by taking the pointer word from each disk block and putting it in a table in memory figure shows what the table looks like for the example of fig in both figures file systems chap we have two files file a uses disk blocks and in that order and fde b uses disk blocks and in that order using the table of fig we can start with block and follow the chain all the way to the end the same can be done starting with block both chains are terminated with a special marker e g that is not a valid block number such a table in main memory is called a fat file allocation table physical block sec file system implementation i nodes our last method for keeping track of which blocks belong to which file is to associate with each file a data structure called an i node index node which lists the attributes and disk addresses of the file blocks a simple example is de picted in fig given the i node it is then possible to find all the blocks of the file the big advantage of this scheme over linked files using an in memory table is that the i node need only be in memory when the corresponding file is open if each i node occupies n bytes and a maximum of k files may be open at once the total memory occupied by the array holding the i nodes for the open files is only kn bytes only this much space need be reserved in advance j file a starts her e file b start her e unuse d block figure linked list allocation using a file allocation table in main memory using this organization the entire block is available for data furthermore random access is much easier although the chain must still be followed to find a given offset within the file the chain is entirely in memory so it can be followed without making any disk references like the previous method it is sufficient for figure an example i node disk block containing additional disk addresse the directory entry to keep a single integer the starting block number and still be able to locate all the blocks no matter how large the file is the primary disadvantage of this method is that the entire table must be in memory all the time to make it work with a gb disk and a kb block size the table needs million entries one for each of the million disk blocks each entry has to be a minimum of bytes for speed in lookup they should be bytes thus the table will take up mb or mb of main memory all the time depending on whether the system is optimized for space or time not wildly practical clearly the fat idea does not scale well to large disks this array is usually far smaller than the space occupied by the file table de scribed in the previous section the reason is simple the table for holding the linked list of all disk blocks is proportional in size to the disk itself if the disk has n blocks the table needs n entries as disks grow larger this table grows line arly with them in contrast the i node scheme requires an array in memory whose size is proportional to the maximum number of files that may be open at once it does not matter if the disk is gb or gb or gb one problem with i nodes is that if each one has room for a fixed number of disk addresses what happens when a file grows beyond this limit one solution file systems chap is to reserve the last disk address not for a data block but instead for the address of a block containing more disk block addresses as shown in fig even more advanced would be two or more such blocks containing disk addresses or even disk blocks pointing to other disk blocks full of addresses we will come back to i nodes when studying unix later implementing directories before a file can be read it must be opened when a file is opened the oper ating system uses the path name supplied by the user to locate the directory entry the directory entry provides the information needed to find the disk blocks de pending on the system this information may be the disk address of the entire file with contiguous allocation the number of the first block both linked list schemes or the number of the i node in all cases the main function of the di rectory system is to map the ascii name of the file onto the information needed to locate the data a closely related issue is where the attributes should be stored every file sys tem maintains file attributes such as each file owner and creation time and they must be stored somewhere one obvious possibility is to store them directly in the directory entry many systems do precisely that this option is shown in fig a in this simple design a directory consists of a list of fixed size entries one per file containing a fixed length file name a structure of the file attributes and one or more disk addresses up to some maximum telling where the disk blocks are dat a structure sec file system implementation in fig b as we shall see later this method has some advantages over put ting them in the directory entry the two approaches shown in fig corres pond to windows and unix respectively as we will see later so far we have made the assumption that files have short fixed length names in ms dos files have a character base name and an optional extension of characters in unix version file names were characters including any extensions however nearly all modern operating systems support longer vari able length file names how can these be implemented the simplest approach is to set a limit on file name length typically char acters and then use one of the designs of fig with characters reserved for each file name this approach is simple but wastes a great deal of directory space since few files have such long names for efficiency reasons a different structure is desirable one alternative is to give up the idea that all directory entries are the same size with this method each directory entry contains a fixed portion typically starting with the length of the entry and then followed by data with a fixed for mat usually including the owner creation time protection information and other attributes this fixed length header is followed by the actual file name however long it may be as shown in fig a in big endian format e g sparc in this example we have three files project budget personnel and foo each file name is terminated by a special character usually which is represented in the figure by a box with a cross in it to allow each directory entry to begin on a word boundary each file name is filled out to an integral number of words shown by shaded boxes in the figure a disadvantage of this method is that when a file is removed a variable sized gap is introduced into the directory into which the next file to be entered may not fit this problem is the same one we saw with contiguous disk files only now compacting the directory is feasible because it is entirely in memory another problem is that a single directory entry may span multiple pages so a page fault may occur while reading a file name another way to handle variable length names is to make the directory entries themselves all fixed length and keep the file names together in a heap at the end of the directory as shown in fig b this method has the advantage that when an entry is removed the next file entered will always fit there of course a containing the attributes the heap must be managed and page faults can still occur while processing file names one minor win here is that there is no longer any real need for file names to begin at word boundaries so no filler characters are needed after file names in figur e a a simpl e directory containin g fixed siz e entries wit h the dis k addresses and attributes in the directory entry b a directory in whic h eac h entry jus t refers to an i node for systems that use i nodes another possibility for storing the attributes is in the i nodes rather than in the directory entries in that case the directory entry can be shorter just a file name and an i node number this approach is illustrated fig b as they are in fig a in all of the designs so far directories are searched linearly from beginning to end when a file name has to be looked up for extremely long directories linear searching can be slow one way to speed up the search is to use a hash table in each directory call the size of the table n to enter a file name the name is hashed onto a value between and n l for example by dividing it by n and file systems chap sec file system implementation shared files file entry length file attributes file entry length file attributes file entry length file attributes j i o i a pointer to file name file attributes pointer to file name file attributes pointer to file name file attributes p r i e c t b u d g e t is p e r p n e i f is b when several users are working together on a project they often need to share files as a result it is often convenient for a shared file to appear simultaneously in different directories belonging to different users figure shows the file system of fig again only with one of cs files now present in one of b di rectories as well the connection between ts directory and the shared file is call ed a link the file system itself is now a directed acyclic graph or dag rath er than a tree figure two ways of handling long file names in a directory a in line b in a heap taking the remainder alternatively the words comprising the file name cantje added up and this quantity divided by n or something similar either way the table entry corresponding to the hash code is inspected if it is unused a pointer is placed there to the file entry file entries follow the hash table if that slot is already in use a linked list is constructed headed at the table entry and threading through all entries with the same hash value looking up a file follows the same procedure the file name is hashed to select a hash table entry all the entries on the chain headed at that slot are checked to see if the file name is present if the name is not on the chain the file is not present in the directory using a hash table has the advantage of much faster lookup but the disadvan tage of more complex administration it is only really a serious candidate in sys tems where it is expected that directories will routinely contain hundreds or thousands of files a different way to speed up searching large directories is to cache the results of searches before starting a search a check is first made to see if the file name is in the cache if so it can be located immediately of course caching only works if a relatively small number of files comprise the majority of the lookups shared file figure file system containing a shared file sharing files is convenient but it also introduces some problems to start with if directories really do contain disk addresses then a copy of the disk ad dresses will have to be made in s directory when the file is linked if either b or c subsequently appends to the file the new blocks will be listed only in the direc tory of the user doing the append the changes will not be visible to the other user thus defeating the purpose of sharing this problem can be solved in two ways in the first solution disk blocks are not listed in directories but in a little data structure associated with the file itself the directories would then point just to the little data structure this is the ap proach used in unix where the little data structure is the i node in the second solution b links to one of c files by having the system create a new file of type link and entering that file in b directory the new file con tains just the path name of the file to which it is linked when b reads from the linked file the operating system sees that the file being read from is of type link looks up the name of the file and reads that file this approach is called symbolic linking to contrast it with traditional hard linking file systems chap each of these methods has its drawbacks in the first method at the moment that b links to the shared file the i node records the file owner as c creating a link does not change the ownership see fig but it does increase the link count in the i node so the system knows how many directory entries currently point to the file c directory b directory c directory b directory sec file system implementation optimization symbolic links have the advantage that they can be used to link to files on machines anywhere in the world by simply providing the network address of the machine where the file resides in addition to its path on that machine there is also another problem introduced by links symbolic or otherwise when links are allowed files can have two or more paths programs that start at a given directory and find all the files in that directory and its subdirectories will locate a linked file multiple times for example a program that dumps all the files in a directory and its subdirectories onto a tape may make multiple copies of a linked file furthermore if the tape is then read into another machine unless the dump program is clever the linked file will be copied twice onto the disk instead of being linked j coount log structured file systems changes in technology are putting pressure on current file systems in partic a c figur e a situation prior t o linking b afte r the lin k i created c afte r the original owner removes the file if c subsequently tries to remove the file the system is faced with a problem if it removes the file and clears the i node b will have a directory entry pointing to an invalid i node if the i node is later reassigned to another file b link will point to the wrong file the system can see from the count in the i node that the file is still in use but there is no easy way for it to find all the directory entries for the file in order to erase them pointers to the directories cannot be stored in the i node because there can be an unlimited number of directories the only thing to do is remove c directory entry but leave the i node intact with count set to as shown in fig c we now have a situation in which b is the only user having a directory entry for a file owned by c if the system does accounting or has quotas c will continue to be billed for the file until b decides to remove it if ever at which time the count goes to and the file is deleted with symbolic links this problem does not arise because only the true owner has a pointer to the i node users who have linked to the file just have path names not i node pointers when the owner removes the file it is destroyed subsequent attempts to use the file via a symbolic link will fail when the system is unable to locate the file removing a symbolic link does not affect the file at all the problem with symbolic links is the extra overhead required the file con taining the path must be read then the path must be parsed and followed com ponent by component until the i node is reached all of this activity may require a considerable number of extra disk accesses furthermore an extra i node is needed for each symbolic link as is an extra disk block to store the path although if the path name is short the system could store it in the i node itself as a kind of ular cpus keep getting faster disks are becoming much bigger and cheaper but not much faster and memories are growing exponentially in size the one pa rameter that is not improving by leaps and bounds is disk seek time the combina tion of these factors means that a performance bottleneck is arising in many file systems research done at berkeley attempted to alleviate this problem by de signing a completely new kind of file system lfs the log structured file sys tem in this section we will briefly describe how lfs works for a more com plete treatment see rosenblum and ousterhout the idea that drove the lfs design is that as cpus get faster and ram memories get larger disk caches are also increasing rapidly consequently it is now possible to satisfy a very substantial fraction of all read requests directly from the file system cache with no disk access needed it follows from this observation that in the future most disk accesses will be writes so the read ahead mechanism used in some file systems to fetch blocks before they are needed no longer gains much performance to make matters worse in most file systems writes are done in very small chunks small writes are highly inefficient since a psec disk write is often pre ceded by a msec seek and a msec rotational delay with these parameters disk efficiency drops to a fraction of to see where all the small writes come from consider creating a new file on a unix system to write this file the i node for the directory the directory block the i node for the file and the file itself must all be written while these writes can be delayed doing so exposes the file system to serious consistency problems if a crash occurs before the writes are done for this reason the i node writes are generally done immediately from this reasoning the lfs designers decided to re implement the unix file system in such a way as to achieve the full bandwidth of the disk even in the face of a workload consisting in large part of small random writes the basic idea is to file systems chap structure the entire disk as a log periodically and when there is a special need for it all the pending writes being buffered in memory are collected into a single segment and written to the disk as a single contiguous segment at the end of the log a single segment may thus contain i nodes directory blocks and data blocks all mixed together at the start of each segment is a segment summary telling what can be found in the segment if the average segment can be made to be about mb almost the full bandwidth of the disk can be utilized in this design i nodes still exist and have the same structure as in unix but they are now scattered all over the log instead of being at a fixed position on the disk nevertheless when an i node is located locating the blocks is done in the usual way of course finding an i node is now much harder since its address cannot simply be calculated from its i number as in unix to make it possible to find i nodes an i node map indexed by i number is maintained entry i in this map points to i node i on the disk the map is kept on disk but it is also cached so the most heavily used parts will be in memory most of the time to summarize what we have said so far all writes are initially buffered in memory and periodically all the buffered writes are written to the disk in a single segment at the end of the log opening a file now consists of using the map to locate the i node for the file once the i node has been located the addresses of the blocks can be found from it all of the blocks will themselves be in segments somewhere in the log if disks were infinitely large the above description would be the entire story however real disks are finite so eventually the log will occupy the entire disk at which time no new segments can be written to the log fortunately many existing segments may have blocks that are no longer needed for example if a file isnover written its i node will now point to the new blocks but the old ones will still be occupying space in previously written segments to deal with this problem lfs has a cleaner thread that spends its time scan ning the log circularly to compact it it starts out by reading the summary of the first segment in the log to see which i nodes and files are there it then checks the current i node map to see if the i nodes are still current and file blocks are still in use if not that information is discarded the i nodes and blocks that are still in use go into memory to be written out in the next segment the original segment is then marked as free so that the log can use it for new data in this manner the cleaner moves along the log removing old segments from the back and putting any live data into memory for rewriting in the next segment consequently the disk is a big circular buffer with the writer thread adding new segments to the front and the cleaner thread removing old ones from the back the bookkeeping here is nontrivial since when a file block is written back to a new segment the i node of the file somewhere in the log must be located updated and put into memory to be written out in the next segment the i node map must then be updated to point to the new copy nevertheless it is possible to do the administration and the performance results show that all this complexity is sec file system implementation worthwhile measurements given in the papers cited above show that lfs outper forms unix by an order of magnitude on small writes while having a per formance that is as good as or better than unix for reads and large writes journaling file systems while log structured file systems are an interesting idea they are not widely used in part due to their being highly incompatible with existing file systems nevertheless one of the ideas inherent in them robustness in the face of failure can be easily applied to more conventional file systems the basic idea here is to keep a log of what the file system is going to do before it does it so that if the sys tem crashes before it can do its planned work upon rebooting the system can look in the log to see what was going on at the time of the crash and finish the job such file systems called journaling file systems are actually in use microsoft ntfs file system and the linux and reiserfs file systems use journaling below we will give a brief introduction to this topic to see the nature of the problem consider a simple garden variety operation that happens all the time removing a file this operation in unix requires three steps remove the file from its directory release the i node to the pool of free i nodes return all the disk blocks to the pool of free disk blocks in windows analogous steps are required in the absence of system crashes the order in which these steps are taken does not matter in the presence of crashes it does suppose that the first step is completed and then the system crashes the i node and file blocks will not be accessible from any file but will also not be available for reassignment they are just off in limbo somewhere decreasing the available resources if the crash occurs after the second step only the blocks are lost if the order of operations is changed and the i node is released first then after rebooting the i node may be reassigned but the old directory entry will continue to point to it hence to the wrong file if the blocks are released first then a crash before the i node is cleared will mean that a valid directory entry points to an i node listing blocks now in the free storage pool and which are likely to be reused shortly leading to two or more files randomly sharing the same blocks none of these outcomes are good what the journaling file system does is first write a log entry listing the three actions to be completed the log entry is then written to disk and for good meas ure possibly read back from the disk to verify its integrity only after the log entry has been written do the various operations begin after the operations file systems chap complete successfully the log entry is erased if the system now crashes upon re covery the fde system can check the log to see if any operations were pending if so all of them can be rerun multiple times in the event of repeated crashes until the file is correctly removed to make journaling work the logged operations must be idempotent which means they can be repeated as often as necessary without harm operations such as update the bitmap to mark i node k or block n as free can be repeated until the cows come home with no danger similarly searching a directory and remov ing any entry called foobar is also idempotent on the other hand adding the newly freed blocks from i node k to the end of the free list is not idempotent since they may already be there the more expensive operation search the list of free blocks and add block n to it if it is not already present is idempotent journaling fde systems have to arrange their data structures and loggable operations so they all of them are idempotent under these conditions crash recovery can be made fast and secure for added reliability a file system can introduce the database concept of an atomic transaction when this concept is used a group of actions can be brack eted by the begin transaction and end transaction operations the fde system then knows it must complete either all the bracketed operations or none of them but not any other combinations sec file system implementation temporarily mounted on mm from the user point of view there is a single file system hierarchy that it happens to encompass multiple incompatible file sys tems is not visible to users or processes however the presence of multiple file systems is very definitely visible to the implementation and since the pioneering work of sun microsystems kleiman most unix systems have used the concept of a vfs virtual file system to try to integrate multiple file systems into an orderly structure the key idea is to abstract out that part of the file system that is common to all file systems and put that code in a separate layer that calls the underlying concrete file systems to ac tual manage the data the overall structure is illustrated in fig the dis cussion below is not specific to linux or freebsd or any other version of unix but gives the general flavor of how virtual file systems work in unix systems i i i vf s interface ntfs has an extensive journaling system and its structure is rarely corrupted by system crashes it has been in development since its first release with win dows nt in the first linux file system to do journaling was reiserfs but its popularity was impeded by the fact that it was incompatible with the then stan dard file system in contrast which is a less ambitious project than reiserfs also does journaling while maintaining compatibility with the previous system file syste m s t t buffer cach e figure position of the virtual file system virtual file systems many different file systems are in use often on the same computer even for the same operating system a windows system may have a main ntfs file system but also a legacy fat or fat drive or partition that contains old but still needed data and from time to time a cd rom or dvd each with its own unique file system may be required as well windows handles these disparate file systems by identifying each one with a different drive letter as in c d etc when a process opens a file the drive letter is explicitly or implicitly present so windows knows which file system to pass the request to there is no attempt to integrate heterogeneous file systems into a unified whole in contrast all modern unix systems make a very serious attempt to integrate multiple file systems into a single structure a linux system could have as the root file system with an partition mounted on usr and a second hard disk with a reiserfs file system mounted on home as well as an iso cd rom all system calls relating to files are directed to the virtual file system for ini tial processing these calls coming from user processes are the standard posix calls such as open read write iseek and so on thus the vfs has an upper interface to user processes and it is the well known posix interface the vfs also has a lower interface to the concrete file systems which is labeled vfs interface in fig this interface consists of several dozen func tion calls that the vfs can make to each file system to get work done thus to create a new file system that works with the vfs the designers of the new file system must make sure that it supplies the function calls the vfs requires an obvious example of such a function is one that reads a specific block from disk puts it in the file system buffer cache and returns a pointer to it thus the vfs has two distinct interfaces the upper one to the user processes and the lower one to the concrete file systems while most of die file systems under the vfs represent partitions on a local disk this is not always the case in fact the original motivation for sun to build file systems chap the vfs was to support remote file systems using the nfs network file sys tem protocol the vfs design is such that as long as the concrete file system supplies the functions the vfs requires the vfs does not know or care where the data are stored or what the underlying file system is like internally most vfs implementations are essentially object oriented even if they are written in c rather than c there are several key object types that are normally supported these include the superblock which describes a file system the v node which describes a file and the directory which describes a file sys tem directory each of these has associated operations methods that the con crete file systems must support in addition the vfs has some internal data struc tures for its own use including the mount table and an array of file descriptors to keep track of all the open files in the user processes to understand how the vfs works let us run through an example chronologi cally when the system is booted the root file system is registered with the vfs in addition when other file systems are mounted either at boot time or during op eration they too must register with the vfs when a file system registers what it basically does is provide a list of the addresses of the functions the vfs re quires either as one long call vector table or as several of them one per vfs object as the vfs demands thus once a file system has registered with the vfs the vfs knows how to say read a block from it it simply dalls the fourth or whatever function in the vector supplied by the file system similarly the vfs then also knows how to carry out every other function the concrete file system must supply it just calls the function whose address was supplied when the file system registered after a file system has been mounted it can be used for example if a file system has been mounted on usr and a process makes the call open inciude unistd h clrdonly while parsing the path the vfs sees that a new file system has been mounted on usr and locates its superblock by searching the list of superblocks of mounted file systems having done this it can find the root directory of the mounted file sys tem and look up the path include unistdh there the vfs then creates a v node and makes a call to the concrete file system to return all the information in the file i node this information is copied into the v node in ram along with other information most importantly the pointer to the table of functions to call for operations on v nodes such as read write close and so on after the v node has been created the vfs makes an entry in the file descrip tor table for the calling process and sets it to point to the new v node for the purists the file descriptor actually points to another data structure that contains the current file position and a pointer to the v node but this detail is not important for our purposes here finally the vfs returns the file descriptor to the caller so it can use it to read write and close the file sec file system implementation later when the process does a read using the file descriptor the vfs locates the v node from the process and file descriptor tables and follows the pointer to the table of functions all of which are addresses within the concrete file system on which the requested file resides the function that handles read is now called and code within the concrete file system goes and gets the requested block the vfs has no idea whether the data are coming from the local disk a remote file system over the network a cd rom a usb stick or something different the data structures involved are shown in fig starting with the caller process number and the file descriptor successively the v node read function pointer and access function within the concrete file system are located j f c s i m p l i f i e d v i e w o f t h e structures and code used b y the vfs and concrete file system to do a read tn v tt j b e c o m e s r e l c i v e i y st ghtfbrward to add new file systems to make one the designers first get a list of function calls the vfs expects as then wnte their file system to provide all of them alternatively i f f t e s needs usually by making one or more native calls to the concrete file system file systems chap file system management and optimization making the file system work is one thing making it work efficiently and robustly in real life is something quite different in the following sections we will look at some of the issues involved in managing disks disk space management files are normally stored on disk so management of disk space is a major concern to file system designers two general strategies are possible for storing an n byte file n consecutive bytes of disk space are allocated or the file is split up into a number of not necessarily contiguous blocks the same trade off is pres ent in memory management systems between pure segmentation and paging as we have seen storing a file as a contiguous sequence of bytes has the ob vious problem that if a file grows it will probably have to be moved on the disk the same problem holds for segments in memory except that moving a segment in memory is a relatively fast operation compared to moving a file from one disk position to another for this reason nearly all file systems chop files up into fixed size blocks that need not be adjacent block size once it has been decided to store files in fixed size blocks the question arises of how big the block should be given the way disks are organized the sector the track and the cylinder are obvious candidates for the unit of allocation although these are all device dependent which is a minus in a paging system the page size is also a major contender having a large block size means that every file even a byte file ties up an entire cylinder it also means that small files waste a large amount of disk space on the other hand a small block size means that most files will span multiple blocks and thus need multiple seeks and rotational delays to read them reducing performance thus if the allocation unit is too large we waste space if it is too small we waste time making a good choice requires having some information about the file size distribution tanenbaum et al studied the file size distribution in the computer science department of a large research university the vu in and then again in as well as on a commercial web server hosting a political website www electoral vote com the results are shown in fig where for each power of two file size the percentage of all files smaller or equal to it is list ed for each of the three data sets for example in of all files at the vu were kb or smaller and of all files were kb or smaller the median file size was bytes some people may find this small size surprising sec file system managemen t an d optimization figure percentage of files smaller than a given size in bytes what conclusions can we draw from these data for one thing with a block size of kb only about of all files fit in a single block whereas with a kb block the percentage of files that fit in a block goes up to the range other data in the paper show that with a kb block of the disk blocks are used by the largest files this means that wasting some space at the end of each small file hardly matters because the disk is filled up by a small number of large files videos and the total amount of space taken up by the small files hardly matters at all even doubling the space the smallest of the files take up would be barely noticeable on the other hand using a small block means that each file will consist of many blocks reading each block normally requires a seek and a rotational delay so reading a file consisting of many small blocks will be slow as an example consider a disk with mb per track a rotation time of msec and an average seek time of msec the time in milliseconds to read a block of k bytes is then the sum of the seek rotational delay and transfer times x the solid curve of fig shows the data rate for such a disk as a function of block size to compute the space efficiency we need to make an assumption about the mean file size for simplicity let us assume that all files are kb al though this number is slightly larger than the data measured at the vu students probably have more small files than would be present in a corporate data center file systems chap so it might be a better guess on the whole the dashed curve of fig shows the space efficiency as a function of block size figure the solid curve left hand scale gives ihe data rate of a disk the dashed curve right hand scale gives the disk space efficiency all files are kb sec file system management and optimization nevertheless he observed a median size weighted by usage of files just read at kb files just written as kb and files read and written as kb given the different data sets measurement techniques and the year these results are cer tainly compatible with the vu results keeping track of free blocks once a block size has been chosen the next issue is how to keep track of free blocks two methods are widely used as shown in fig the first one con sists of using a linked list of disk blocks with each block holding as many free disk block numbers as will fit with a kb block and a bit disk block number each block on the free list holds the numbers of free blocks one slot is re quired for the pointer to the next block consider a gb disk which has about million disk blocks to store all these address at per block requires about million blocks generally free blocks are used to hold the free list so the storage is essentially free free disk blocks the two curves can be understood as follows the access time for a block is completely dominated by the seek time and rotational delay so given that it is going to cost msec to access a block the more data that are fetched the better hence the data rate goes up almost linearly with block size until the transfers take so long that the transfer time begins to matter now consider space efficiency with kb files and kb kb or kb blocks files use and block respectively with no wastage with an kb block and kb files the space efficiency drops to and with a kb block it is down to in reality few files are an exact multiple of the disk block size so some space is always wasted in the last block of a file what the curves show however is that performance and space utilization are inherently in conflict small blocks are bad for performance but good for disk space utilization for these data no reasonable compromise is available the size closest to where the two curves cross is kb but the data rate is only mb sec and the space efficiency is about neither of which is very good his torically file systems have chosen sizes in the kb to kb range but with disks now exceeding tb it might be better to increase the block size to kb and accept the wasted disk space disk space is hardly in short supply any more in an experiment to see if windows nt file usage was appreciably different r a kb disk block can hold bit disk block numbers a 0110110110111011 1011101101101111 a bitmap b from unix file usage vogels made measurements on files at cornell university vogels he observed that nt file usage is more complicated than on unix he wrote when we type a few characters in the notepad text editor saving this to a file will trigger system calls including failed open attempts file overwrite and additional open and close sequences figure a storing the free list on a linked list b a bitmap the other free space management technique is the bitmap a disk with n blocks requires a bitmap with n bits free blocks are represented by is in the map allocated blocks by or vice versa for our example gb disk we need million bits for the map which requires just under kb blocks to file systems chap store it is not surprising that the bitmap requires less space since it uses bit per block versus bits in the linked list model only if the disk is nearly full i e has few free blocks will the linked list scheme require fewer blocks than the bit map if free blocks tend to come in long runs of consecutive blocks the free list system can be modified to keep track of runs of blocks rather than single blocks an or bit count could be associated with each block giving the number of consecutive free blocks in the best case a basically empty disk could be represented by two numbers the address of the first free block followed by the count of free blocks on the other hand if the disk becomes severely fragmented keeping track of runs is less efficient than keeping track of individual blocks be cause not only must the address be stored but also the count this issue illustrates a problem operating system designers often have there are multiple data structures and algorithms that can be used to solve a problem but choosing the best one requires data that the designers do not have and will not have until the system is deployed and heavily used and even then the data may not be available for example our own measurements of file sizes at the vu in and the website data and the cornell data are only four samples while a lot better than nothing we have little idea if they are also representative of home computers corporate computers government computers and others with some effort we might have been able to get a couple of samples from other kinds of computers but even then it would be foolish to extrapolate to ail com puters of the kind measured getting back to the free list method for a moment only one block of pointers need be kept in main memory when a file is created the needed blocks are taken from the block of pointers when it runs out a new block of pointers is read in from the disk similarly when a file is deleted its blocks are freed and added to the block of pointers in main memory when this block fills up it is written to disk under certain circumstances this method leads to unnecessary disk i o con sider the situation of fig a in which the block of pointers in memory has room for only two more entries if a three block file is freed the pointer block overflows and has to be written to disk leading to the situation of fig b if a three block file is now written the full block of pointers has to be read in again taking us back to fig a if the three block file just written was a temporary file when it is freed another disk write is needed to write the full block of point ers back to the disk in short when the block of pointers is almost empty a series of short lived temporary files can cause a lot of disk i o an alternative approach that avoids most of this disk i o is to split the full block of pointers thus instead of going from fig a to fig b we go from fig a to fig c when three blocks are freed now the system can handle a series of temporary files without doing any disk i o if the block in memory fills up it is written to the disk and the half full block from the disk is sec file system management and optimization a b figure a an almost full block of pointers to free disk blocks in memory and three blocks of pointers on disk b result of freeing a three block file c an alternative strategy for handling the three free blocks the shaded entries represent pointers to free disk blocks read in the idea here is to keep most of the pointer blocks on disk full to minim ize disk usage but keep the one in memory about half full so it can handle both file creation and file removal without disk i o on the free list with a bitmap it is also possible to keep just one block in memory going to disk for another only when it becomes full or empty an additional benefit of this approach is that by doing all the allocation from a single block of the bitmap the disk blocks will be close together thus minimizing disk arm motion since the bit map is a fixed size data structure if the kernel is partially paged the bitmap can be put in virtual memory and have pages of it paged in as needed disk quotas to prevent people from hogging too much disk space multiuser operating systems often provide a mechanism for enforcing disk quotas the idea is that the system administrator assigns each user a maximum allotment of files and blocks and the operating system makes sure that the users do not exceed their quotas a typical mechanism is described below when a user opens a file the attributes and disk addresses are located and put into an open file table in main memory among the attributes is an entry telling who the owner is any increases in the file size will be charged to the owner quota a second table contains the quota record for every user with a currently open file even if the file was opened by someone else this table is shown in fig it is an extract from a quota file on disk for the users whose files are currently open when all the files are closed the record is written back to the quota file file systems chap open file table quota table soft block limit hard block limit sec file system management and optimization except at universities where issuing a purchase order takes three committees five signatures and days if a computer file system is irrevocably lost whether due to hardware or software restoring all the information will be difficult time consuming and in many cases impossible for the people whose programs documents tax records current of blocks block warnings left soft file limit hard file limit current of files file warnings left quota v record i for user l customer files databases marketing plans or other data are gone forever the consequences can be catastrophic while the file system cannot offer any protec tion against physical destruction of the equipment and media it can help protect the information it is pretty straightforward make backups but that is not quite as simple as it sounds let us take a look most people do not think making backups of their files is worth the time and effort until one fine day their disk abruptly dies at which time most of them undergo a deathbed conversion companies however usually well understand the value of their data and generally do a backup at least once a day usually to tape modern tapes hold hundreds of gigabytes and cost pennies per gigabyte nevertheless making backups is not quite as trivial as it sounds so we will exam figure quotas are kept track of on a per user basis in a quota table when a new entry is made in the open file table a pointer to the owner quota record is entered into it to make it easy to find the various limits every time a block is added to a file the total number of blocks charged to the owner is incremented and a check is made against both the hard and soft limits the soft limit may be exceeded but the hard limit may not an attempt to append to a file when the hard block limit has been reached will result in an error analogous checks also exist for the number of files when a user attempts to log in the system examines the quota file to see if the user has exceeded the soft limit for either number of files or number of disk blocks if either limit has been violated a warning is displayed and the count of warnings remaining is reduced by one if the count ever gets to zero the user has ignored the warning one time too many and is not permitted to log in getting permission to log in again will require some discussion with the system adminis trator this method has the property that users may go above their soft limits during a login session provided they remove the excess before logging out the hard limits may never be exceeded file system backups destruction of a file system is often a far greater disaster than destruction of a computer if a computer is destroyed by fire lightning surges or a cup of coffee poured onto the keyboard it is annoying and will cost money but generally a replacement can be purchased with a minimum of fuss inexpensive personal computers can even be replaced within an hour by just going to a computer store ine some of the related issues below backups to tape are generally made to handle one of two potential problems recover from disaster recover from stupidity the first one covers getting the computer running again after a disk crash fire flood or other natural catastrophe in practice these things do not happen very often which is why many people do not bother with backups these people also tend not to have fire insurance on their houses for the same reason the second reason is that users often accidentally remove files that they later need again this problem occurs so often that when a file is removed in win dows it is not deleted at all but just moved to a special directory the recycle bin so it can be fished out and restored easily later backups take this principle further and allow files that were removed days even weeks ago to be restored from old backup tapes making a backup takes a long time and occupies a large amount of space so doing it efficiently and conveniently is important these considerations raise the following issues first should the entire file system be backed up or only part of it at many installations the executable binary programs are kept in a limited part of the file system tree it is not necessary to back up these files if they can all be reinstalled from the manufacturer cd roms also most systems have a di rectory for temporary files there is usually no reason to back it up either in undc all the special files i o devices are kept in a directory dev not only is backing up this directory not necessary it is downright dangerous because the backup program would hang forever if it tried to read each of these to completion in short it is usually desirable to back up only specific directories and everything in them rather than the entire file system file systems chap second it is wasteful to back up files that have not changed since the previous backup which leads to the idea of incremental dumps the simplest form of in cremental dumping is to make a complete dump backup periodically say weekly or monthly and to make a daily dump of only those files that have been modified since the last full dump even better is to dump only those files that have changed since they were last dumped while this scheme minimizes dumping time it makes recovery more complicated because first the most recent full dump has to be restored followed by all the incremental dumps in reverse order to ease recovery more sophisticated incremental dumping schemes are often used third since immense amounts of data are typically dumped it may be desir able to compress the data before writing them to tape however with many com pression algorithms a single bad spot on the backup tape can foil the decompres sion algorithm and make an entire file or even an entire tape unreadable thus the decision to compress the backup stream must be carefully considered fourth it is difficult to perform a backup on an active file system if files and directories are being added deleted and modified during the dumping process the resulting dump may be inconsistent however since making a dump may take hours it may be necessary to take the system offline for much of the night to make the backup something that is not always acceptable for this reason algo rithms have been devised for making rapid snapshots of the file system state by copying critical data structures and then requiring future changes to files and di rectories to copy the blocks instead of updating them in place hutchinson et al in this way the file system is effectively frozen at the moment of the snapshot so it can be backed up at leisure afterward fifth and last making backups introduces many nontechnical problems into an organization the best online security system in the world may be useless if the system administrator keeps all the backup tapes in his office and leaves it open and unguarded whenever he walks down the hall to get output from the printer all a spy has to do is pop in for a second put one tiny tape in his pocket and saunter off jauntily goodbye security also making a daily backup has little use if the fire that bums down the computers also burns up all the backup tapes for this reason backup tapes should be kept off site but that introduces more security risks because now two sites must be secured for a thorough discussion of these and other practical administration issues see nemeth et al below we will discuss only the technical issues involved in making file system backups two strategies can be used for dumping a disk to tape a physical dump or a logical dump a physical dump starts at block of the disk writes all the disk blocks onto the output tape in order and stops when it has copied the last one such a program is so simple that it can probably be made bug free some thing that can probably not be said about any other useful program nevertheless it is worth making several comments about physical dumping for one thing there is no value in backing up unused disk blocks if the dumping program can obtain access to the free block data structure it can avoid dumping sec file system management and optimization unused blocks however skipping unused blocks requires writing the number of each block in front of the block or the equivalent since it is no longer true that block k on the tape was block k on the disk a second concern is dumping bad blocks it is nearly impossible to manufac ture large disks without any defects some bad blocks are always present some times when a low level format is done the bad blocks are detected marked as bad and replaced by spare blocks reserved at the end of each track for just such emergencies in many cases the disk controller handles bad block replacement transparently without the operating system even knowing about it however sometimes blocks go bad after formatting in which case the operat ing system will eventually detect them usually it solves the problem by creating a file consisting of all the bad blocks just to make sure they never appear in the free block pool and are never assigned needless to say this file is completely unreadable if all bad blocks are remapped by the disk controller and hidden from the op erating system as just described physical dumping works fine on the other hand if they are visible to the operating system and maintained in one or more bad block files or bitmaps it is absolutely essential that the physical dumping program get access to this information and avoid dumping them to prevent endless disk read errors while trying to back up the bad block file the main advantages of physical dumping are simplicity and great speed basically it can run at the speed of the disk the main disadvantages are the inability to skip selected directories make incremental dumps and restore indivi dual files upon request for these reasons most installations make logical dumps a logical dump starts at one or more specified directories and recursively dumps all files and directories found there that have changed since some given base date e g the last backup for an incremental dump or system installation for a full dump thus in a logical dump the dump tape gets a series of carefully identified directories and files which makes it easy to restore a specific file or di rectory upon request since logical dumping is the most common form let us examine a common algorithm in detail using the example of fig to guide us most unix systems use this algorithm in the figure we see a file tree with directories squares and files circles the shaded items have been modified since the base date and thus need to be dumped the unshaded ones do not need to be dumped this algorithm also dumps all directories even unmodified ones that lie on the path to a modified file or directory for two reasons first to make it possible to restore the dumped files and directories to a fresh file system on a different computer in this way the dump and restore programs can be used to transport entire file systems between computers the second reason for dumping unmodified directories above modified files is to make it possible to incrementally restore a single file possibly to handle re covery from stupidity suppose that a full file system dump is done sunday file systems chap sec file system management and optimization themselves have not been modified because they will be needed to restore today changes to a fresh machine for efficiency phases and can be combined in one tree walk b i a q lpjl lp lfjla 23j2 3o c d j j y j m figure bitmaps used by the logical dumping algorithm a a file system to be dumped the squares are directories and the x l the se a items have been modified since the last dump each directory and file is labeled by its i node number evening and an incremental dump is done on monday evening on tuesday the directory usr jhs proj is removed along with all the directories and files under it on wednesday morning bright and early the user wants to restore the file usr jhs proj plans summary however is not possible to just restore the file summary because there is no place to put it the directories and plans must be restored first to get their owners modes times and whatever correct these directories must be present on the dump tape even though they themselves were not modified since the previous full dump the dump algorithm maintains a bitmap indexed by i node number with sev eral bits per i node bits will be set and cleared in this map as the algorithm proceeds the algorithm operates in four phases phase begins at the starting di rectory the root in this example and examines all the entries in it for each modi fied file its i node is marked in the bitmap each directory is also marked whether or not it has been modified and then recursively inspected at the end of phase all modified files and all directories have been marked in the bitmap as shown by shading in fig a phase conceptually recur sively walks the tree again unmarking any directories that have no modified files or directories in them or under them this phase leaves the bitmap as shown in fig b note that directories and are now unmarked because they contain nothing under them that has been modified they will not be dumped by way of contrast directories and will be dumped even though they at this point it is known which directories and files must be dumped these are the ones marked in fig b phase consists of scanning the i nodes in numerical order and dumping all the directories that are marked for dumping these are shown in fig c each directory is prefixed by the directory at tributes owner times etc so that they can be restored finally in phase the files marked in fig d are also dumped again prefixed by their attributes this completes the dump restoring a file system from the dump tapes is straightforward to start with an empty file system is created on the disk then the most recent full dump is re stored since the directories appear first on the tape they are all restored first giv ing a skeleton of the file system then the files themselves are restored this process is then repeated with the first incremental dump made after the full dump then the next one and so on although logical dumping is straightforward there are a few tricky issues for one since the free block list is not a file it is not dumped and hence it must be reconstructed from scratch after all the dumps have been restored doing so is al ways possible since the set of free blocks is just the complement of the set of blocks contained in all the files combined another issue is links if a file is linked to two or more directories it is im portant that the file is restored only one time and that all the directories that are supposed to point to it do so still another issue is the fact that unix files may contain holes it is legal to open a file write a few bytes then seek to a distant file offset and write a few more bytes the blocks in between are not part of the file and should not be dumped and must not be restored core files often have a hole of hundreds of file systems chap megabytes between the data segment and the stack if not handled properly each restored core file will fill this area with zeros and thus be the same size as the vir tual address space e g bytes or worse yet bytes finally special files named pipes and the like should never be dumped no matter in which directory they may occur they need not be confined to dev for more information about file system backups see chervenak et al and zwicky tape densities are not improving as fast as disk densities this is gradually leading to a situation in which backing up a very large disk may require multiple sec file system management and optimization if the file system is consistent each block will have a either in the first table or in the second table as illustrated in fig a however as a result of a crash the tables might look like fig b in which block does not occur in either table it will be reported as being a missing block while missing blocks do no real harm they waste space and thus reduce the capacity of the disk the solution to missing blocks is straightforward the file system checker just adds them to the free list block number block number s tapes while tape robots are available to change tapes automatically if this trend continues tapes will eventually become too small to use as a backup medium in that case the only way to back up a disk will be on another disk while simply mirroring each disk with a spare is one possibility more sophisticated schemes called raids will be discussed in chap file system consistency a blocks in use free blocks b blocks in use free blocks another area where reliability is an issue is file system consistency many file systems read blocks modify them and write them out later if the system crashes before all the modified blocks have been written out the file system can be left in blocks in use free blocks i i d i i blocks in use free blocks an inconsistent state this problem is especially critical if some of the blocks that have not been written out are i node blocks directory blocks or blocks containing the free list to deal with the problem of inconsistent file systems most computers have a utility program that checks file system consistency for example unix has fsck and windows has scandisk this utility can be run whenever the system is boot ed especially after a crash the description below tells how fsck works scandisk is somewhat different because it works on a different file system but the general principle of using the file system inherent redundancy to repair it is still valid all file system checkers verify each file system disk partition independently of the other ones two kinds of consistency checks can be made blocks and files to check for block consistency the program builds two tables each one containing a counter for each block initially set to the counters in the first table keep track of how many times each block is present in a file the counters in the second table record how often each block is present in the free list or the bitmap of free blocks the program then reads all the i nodes using a raw device which ignores the file structure and just returns all the disk blocks starting at starting from an i node it is possible to build a list of all the block numbers used in the correspond ing file as each block number is read its counter in the first table is incre mented the program then examines the free list or bitmap to find all the blocks that are not in use each occurrence of a block in the free list results in its counter in the second table being incremented figure file system states a consistent b missing block c dupli cate block in free list d duplicate data block another situation that might occur is that of fig c here we see a block number that occurs twice in the free list duplicates can occur only if the free list is really a list with a bitmap it is impossible the solution here is also simple rebuild the free list the worst thing that can happen is that the same data block is present in two or more files as shown in fig d with block if either of these files is re moved block will be put on the free list leading to a situation in which the same block is both in use and free at the same time if both files are removed the block will be put onto the free list twice the appropriate action for the file system checker to take is to allocate a free block copy the contents of block into it and insert the copy into one of the files in this way the information content of the files is unchanged although almost assuredly one is garbled but the file system structure is at least made consistent the error should be reported to allow the user to inspect the damage in addition to checking to see that each block is properly accounted for the file system checker also checks the directory system it too uses a table of counters but these are per file rather than per block it starts at the root directory and recursively descends the tree inspecting each directory in the file system for every i node in every directory it increments a counter for that file usage count file systems chap remember that due to hard links a file may appear in two or more directories symbolic links do not count and do not cause the counter for the target file to be incremented when the checker is all done it has a list indexed by i node number telling how many directories contain each file it then compares these numbers with the link counts stored in the i nodes themselves these counts start at i when a file is created and are incremented each time a hard link is made to the file in a con sistent file system both counts will agree however two kinds of errors can oc cur the link count in the i node can be too high or it can be too low if the link count is higher than the number of directory entries then even if all the files are removed from the directories the count will still be nonzero and the i node will not be removed this error is not serious but it wastes space on the disk with files that are not in any directory it should be fixed by setting the link count in the i node to the correct value the other error is potentially catastrophic if two directory entries are linked to a file but the i node says that there is only one when either directory entry is removed the i node count will go to zero when an i node count goes to zero the file system marks it as unused and releases all of its blocks this action will result in one of the directories now pointing to an unused i node whose blocks may soon be assigned to other files again the solution is just to force the link count in the i node to the actual number of directory entries these two operations checking blocks and checking directories are often integrated for efficiency reasons i e only one pass over the i nodes is required other checks are also possible for example directories have a definite format with i node numbers and ascii names if an i node number is larger than the number of i nodes on the disk the directory has been damaged furthermore each i node has a mode some of which are legal but strange such as which allows the owner and his group no access at all but allows outsiders to read write and execute the file it might be useful to at least report files that give outsiders more rights than the owner directories with more than say entries are also suspicious files located in user directories but which are owned by the superuser and have the setuid bit on are potential security problems because such files acquire the powers of the superuser when executed by any user with a little effort one can put together a fairly long list of techni cally legal but still peculiar situations that might be worth reporting the previous paragraphs have discussed the problem of protecting the user against crashes some file systems also worry about protecting the user against himself if the user intends to type rm o to remove all the files ending with o compiler generated object files but ac cidentally types rm o sec file system managemen t an d optimization note the space after the asterisk rm will remove all the files in the current direc tory and then complain that it cannot find o in ms dos and some other systems when a file is removed all that happens is that a bit is set in the directory or i node marking the file as removed no disk blocks are returned to the free list until they are actually needed thus if the user discovers the error immediately it is possible to run a special utility program that unremoves i e restores the removed files in windows files that are removed are placed in the recycle bin a special directory from which they can later be retrieved if need be of course no storage is reclaimed until they are actually deleted from this directory file system performance access to disk is much slower than access to memory reading a bit mem ory word might take nsec reading from a hard disk might proceed at mb sec which is four times slower per bit word but to this must be added msec to seek to the track and then wait for the desired sector to arrive under the read head if only a single word is needed the memory access is on the order of a million times as fast as disk access as a result of this difference in access time many file systems have been designed with various optimizations to improve performance in this section we will cover three of them caching the most common technique used to reduce disk accesses is the block cache or buffer cache cache is pronounced cash and is derived from the french cacher meaning to hide in this context a cache is a collection of blocks that logically belong on the disk but are being kept in memory for performance rea sons various algorithms can be used to manage the cache but a common one is to check all read requests to see if the needed block is in the cache if it is the read request can be satisfied without a disk access if the block is not in the cache it is first read into the cache and then copied to wherever it is needed subsequent re quests for the same block can be satisfied from the cache operation of the cache is illustrated in fig since there are many often thousands of blocks in the cache some way is needed to determine quickly if a given block is present the usual way is to hash the device and disk address and look up the result in a hash table all the blocks with the same hash value are chained together on a linked list so that the collision chain can be followed when a block has to be loaded into a full cache some block has to be re moved and rewritten to the disk if it has been modified since being brought in this situation is very much like paging and all the usual page replacement algo rithms described in chap such as fifo second chance and lru are applica ble one pleasant difference between paging and caching is that cache references file systems chap figure the buffer cache data structures sec file system management and optimization been modified it should be written to disk immediately regardless of which end of the lru list it is put on by writing critical blocks quickly we greatly reduce the probability that a crash will wreck the file system while a user may be unhappy if one of his files is ruined in a crash he is likely to be far more unhappy if the whole file system is lost even with this measure to keep the file system integrity intact it is undesir able to keep data blocks in the cache too long before writing them out consider the plight of someone who is using a personal computer to write a book even if our writer periodically tells the editor to write the file being edited to the disk there is a good chance that everything will still be in the cache and nothing on the disk if the system crashes the file system structure will not be corrupted but a whole day work will be lost this situation need not happen very often before we have a fairly unhappy user systems take two approaches to dealing with it the unix way is to have a are relatively infrequent so that it is feasible to keep all the blocks in exact lru order with linked lists in fig we see that in addition to the collision chains starting at the hash table there is also a bidirectional list running through all the blocks in the order of usage with the least recently used block on the front of this list and the most recently used block at the end of this list when a block is referenced it can be re moved from its position on the bidirectional list and put at the end in this way exact lru order can be maintained unfortunately there is a catch now that we have a situation in which exact lru is possible it turns out that lru is undesirable the problem has to do with the crashes and file system consistency discussed in the previous section if a crit ical block such as an i node block is read into the cache and modified but not rewritten to the disk a crash will leave the file system in an inconsistent state if the i node block is put at the end of the lru chain it may be quite a while before it reaches the front and is rewritten to the disk furthermore some blocks such as i node blocks are rarely referenced two times within a short interval these considerations lead to a modified lru scheme taking two factors into account is the block likely to be needed again soon is the block essential to the consistency of the file system for both questions blocks can be divided into categories such as i node blocks indirect blocks directory blocks full data blocks and partially full data blocks blocks that will probably not be needed again soon go on the front rather than the rear of the lru list so their buffers will be reused quickly blocks that might be needed again soon such as a partly full block that is being written go on the end of the list so they will stay around for a long time the second question is independent of the first one if the block is essential to the file system consistency basically everything except data blocks and it has system call sync which forces all the modified blocks out onto the disk im mediately when the system is started up a program usually called update is started up in the background to sit in an endless loop issuing sync calls sleeping for sec between calls as a result no more than seconds of work is lost due to a crash although windows now has a system call equivalent to sync flushfilebuff ers in the past it did not instead it had a different strategy that was in seme ways better than the unix approach and in some ways worse what it did was to write every modified block to disk as soon as it has been written to the cache caches in which all modified blocks are written back to the disk immediately are called write through caches they require more disk i o than nonwrite through caches the difference between these two approaches can be seen when a program writes a kb block full one character at a time unix will collect all the charac ters in the cache and write the block out once every seconds or whenever the block is removed from the cache with a write through cache there is a disk ac cess for every character written of course most programs do internal buffering so they normally write not a character but a line or a larger unit on each write sys tem call a consequence of this difference in caching strategy is that just removing a floppy disk from a unix system without doing a sync will almost always result in lost data and frequently in a corrupted file system as well with write through caching no problem arises these differing strategies were chosen because unix was developed in an environment in which all disks were hard disks and not removable whereas the first windows file system was inherited from ms dos which started out in the floppy disk world as hard disks became the norm the unix approach with its better efficiency but worse reliability became the norm and is also used now on windows for hard disks however ntfs takes other measures journaling to improve reliability as discussed earlier file systems chap some operating systems integrate the buffer cache with the page cache this is especially attractive when memory mapped files are supported if a file is map ped onto memory then some of its pages may be in memory because they were demand paged in such pages are hardly different from file blocks in the buffer cache in this case they can be treated the same way with a single cache for both file blocks and pages block read ahead a second technique for improving perceived file system performance is to try to get blocks into the cache before they are needed to increase the hit rate in par ticular many files are read sequentially when the file system is asked to produce block k in a file it does that but when it is finished it makes a sneaky check in the cache to see if block k is already there if it is not it schedules a read for block k l m the hope that when it is needed it will have already arrived in the cache at the very least it will be on the way of course this read ahead strategy only works for files that are being read se quentially if a file is being randomly accessed read ahead does not help in fact it hurts by tying up disk bandwidth reading in useless blocks and removing poten tially useful blocks from the cache and possibly tying up more disk bandwidth writing them back to disk if they are dirty to see whether read ahead is worth doing the file system can keep track of the access patterns to each open file for example a bit associated with each file can keep track of whether the file is in sequential access mode or random access mode initially the file is given the benefit of the doubt and put in sequential access mode however whenever a seek is done the bit is cleared if sequential reads start happening again the bit is set once again in this way the file system can make a reasonable guess about whether it should read ahead or not if it gets it wrong once in a while it is not a disaster just a little bit of wasted disk bandwidth reducing disk arm motion caching and read ahead are not the only ways to increase file system per formance another important technique is to reduce the amount of disk arm motion by putting blocks that are likely to be accessed in sequence close to each other preferably in the same cylinder when an output file is written the file sys tem has to allocate the blocks one at a time on demand if the free blocks are recorded in a bitmap and the whole bitmap is in main memory it is easy enough to choose a free block as close as possible to the previous block with a free list part of which is on disk it is much harder to allocate blocks close together however even with a free list some block clustering can be done the trick is to keep track of disk storage not in blocks but in groups of consecutive blocks if all sectors consist of bytes the system could use kb blocks sectors sec file system managemen t an d optimization but allocate disk storage in units of blocks sectors this is not the same as having a kb disk blocks since the cache would still use kb blocks and disk transfers would still be kb but reading a file sequentially on an otherwise idle system would reduce the number of seeks by a factor of two considerably im proving performance a variation on the same theme is to take account of rota tional positioning when allocating blocks the system attempts to place consecu tive blocks in a file in the same cylinder another performance bottleneck in systems that use i nodes or anything like them is that reading even a short file requires two disk accesses one for the i node and one for the block the usual i node placement is shown in fig a here all the i nodes are near the beginning of the disk so the average distance between an i node and its blocks will be about half the number of cylinders requiring long seeks figure a i nodes placed at the start of the disk b disk divided into cylinder groups each with its own blocks and i nodes one easy performance improvement is to put the i nodes in the middle of the disk rather than at the start thus reducing the average seek between the i node and the first block by a factor of two another idea shown in fig b is to divide the disk into cylinder groups each with its own i nodes blocks and free list mckusick et al when creating a new file any i node can be chosen but an attempt is made to find a block in the same cylinder group as the i node if none is available then a block in a nearby cylinder group is used defragmenting disks when the operating system is initially installed the programs and files it needs are installed consecutively starting at the beginning of the disk each one di rectly following the previous one all free disk space is in a single contiguous unit file systems chap following the installed files however as time goes on files are created and re moved and typically the disk becomes badly fragmented with files and holes all over the place as a consequence when a new file is created the blocks used for it may be spread all over the disk giving poor performance the performance can be restored by moving files around to make them con tiguous and to put all or at least most of the free space in one or more large con tiguous regions on the disk windows has a program dejrag that does precisely this windows users should run it regularly defragmentation works better on file systems that have a fair amount of free space in a contiguous region at the end of the partition this space allows the defragmentation program to select fragmented files near the start of the partition and copy all their blocks to the free space this action frees up a contiguous block of space near the start of the partition into which the original or other files can be placed contiguously the process can then be repeated with the next chunk of disk space and so on some files cannot be moved including the paging file the hibernation file and the journaling log because the administration that would be required to do this is more trouble than it is worth in some systems these are fixed size contig uous areas anyway so they do not have to be defragmented the one time when their lack of mobility is a problem is when they happen to be near the end of the partition and the user wants to reduce the partition size the only way to solve this problem is to remove them altogether resize the partition and then recreate them afterward linux file systems especially and generally suffer less from defragmentation than windows systems due to the way disk blocks are selected so manual defragmentation is rarely required exampl e fil e system s in the following sections we will discuss several example file systems rang ing from quite simple to more sophisticated since modem unix file systems and windows vista native file system are covered in the chapter on unix chap and the chapter on windows vista chap we will not cover those systems here we will however examine their predecessors below cd rom file systems as our first example of a file system let us consider the file systems used on cd roms these systems are particularly simple because they were designed for write once media among other things for example they have no provision for keeping track of free blocks because on a cd rom files cannot be freed or added after the disk has been manufactured below we will take a look at the main cd rom file system type and two extensions to it sec example file systems some years after the cd rom made its debut the cd r cd recordable was introduced unlike the cd rom it is possible to add files after the initial burning but these are simply appended to the end of the cd r files are never removed although the directory can be updated to hide existing files as a consequence of this append only file system the fundamental properties are not altered in particular all the free space is in one contiguous chunk at the end of the cd the iso file system the most common standard for cd rom file systems was adopted as an international standard in under the name iso virtually every cd rom currently on the market is compatible with this standard sometimes with the extensions to be discussed below one of the goals of this standard was to make every cd rom readable on every computer independent of the byte order ing used and independent of the operating system used as a consequence some limitations were placed on the file system to make it possible for the weakest op erating systems then in use such as ms dos to read it cd roms do not have concentric cylinders the way magnetic disks do in stead there is a single continuous spiral containing the bits in a linear sequence although seeks across die spiral are possible the bits along the spiral are divid ed into logical blocks also called logical sectors of bytes some of these are for preambles error correction and other overhead the payload portion of each logical block is bytes when used for music cds have leadins ieadouts and intertrack gaps but these are not used for data cd roms often the position of a block along the spiral is quoted in minutes and seconds it can be converted to a linear block number using the conversion factor of sec blocks iso supports cd rom sets with as many as cds in the set the individual cd roms may also be partitioned into logical volumes partitions however below we will concentrate on iso for a single unpartitioned cd rom every cd rom begins with blocks whose function is not defined by the iso standard a cd rom manufacturer could use this area for providing a bootstrap program to allow the computer to be booted from the cd rom or for some other purpose next comes one block containing the primary volume descriptor which contains some general information about the cd rom this information includes the system identifier bytes volume identifier bytes publisher identifier bytes and data preparer identifier bytes the manufacturer can fill in these fields in any desired way except that only upper case letters digits and a very small number of punctuation marks may be used to ensure cross platform compatibility file systems chap the primary volume descriptor also contains the names of three files which may contain the abstract copyright notice and bibliographic information respec tively in addition certain key numbers are also present including the logical block size normally but and larger powers of two are allowed in certain cases the number of blocks on the cd rom and the creation and expiration dates of the cd rom finally the primary volume descriptor also contains a directory entry for the root directory telling where to find it on the cd rom i e which block it starts at from this directory the rest of the file system can be located in addition to the primary volume descriptor a cd rom may contain a sup plementary volume descriptor it contains similar information to the primary but that will not concern us here the root directory and all other directories for that matter consists of a vari able number of entries the last of which contains a bit marking it as the final one the directory entries themselves are also variable length each directory entry consists of to fields some of which are in ascii and others of which are numerical fields in binary the binary fields are encoded twice once in little endian format used on pentiums for example and once in big endian format used on sparcs for example thus a bit number uses bytes and a bit number uses bytes the use of this redundant coding was necessary to avoid hurting anyone feelings when the standard was developed if the standard had dictated little end ian then people from companies whose products were big endian would have felt like second class citizens and would not have accepted the standard the emo tional content of a cd rom can thus be quantified and measured exactly in kilobytes hour of wasted space the format of an iso directory entry is illustrated in fig since directory entries have variable lengths the first field is a byte telling how long the entry is this byte is defined to have the high order bit on the left to avoid any ambiguity padding se c example file systems next comes the starting block of the file itself files are stored as contiguous runs of blocks so a file location is completely specified by the starting block and the size which is contained in the next field the date and time that the cd rom was recorded is stored in the next field with separate bytes for the year month day hour minute second and time zone years begin to count at which means that cd roms will suffer from a problem because the year following will be this problem could have been delayed by defining the origin of time to be the year the standard was adopted had that been done the problem would have been post poned until every extra years helps the flags field contains a few miscellaneous bits including one to hide the entry in listings a feature copied from ms dos one to distinguish an entry that is a file from an entry that is a directory one to enable the use of the extended at tributes and one to mark the last entry in a directory a few other bits are also present in this field but they will not concern us here the next field deals with interleaving pieces of files in a way that is not used in the simplest version of iso so we will not consider it further the next field tells which cd rom the file is located on it is permitted that a directory entry on one cd rom refers to a file located on another cd rom in the set in this way it is possible to build a master directory on the first cd rom that lists all the files on all the cd roms in the complete set the field marked l in fig gives the size of the file name in bytes it is followed by the file name itself a file name consists of a base name a dot an extension a semicolon and a binary version number or bytes the base name and extension may use upper case letters the digits and the underscore character all other characters are forbidden to make sure that every computer can handle every file name the base name can be up to eight characters the exten sion can be up to three characters these choices were dictated by the need to be ms dos compatible a file name may be present in a directory multiple times as long as each one has a different version number the last two fields are not always present the padding field is used to force every directory entry to be an even number of bytes to align the numeric fields of subsequent entries on byte boundaries if padding is needed a byte is used bytes a finally we have the system use field its function and size are undefined except location of file file size date and time gd file name j sys that it must be an even number of bytes different systems use it in different ways a ffags t extended attribute record length interleave i base name directory entry length figure the iso directory eiity ext ver the macintosh keeps finder flags here for example entries within a directory are listed in alphabetical order except for the first two entries the first entry is for the directory itself the second one is for its par ent in this respect these entries are similar to the unix and directory entries the files themselves need not be in directory order there is no explicit limit to the number of entries in a directory however directory entries may optionally have extended attributes if this feature is used the second byte tells how long the extended attributes are there is a limit to the depth of nesting the maximum depth of directory nesting is eight this limit was arbitrarily set to make some implementations simpler file systems chap iso defines what are called three levels level is the most restrictive and specifies that file names are limited to characters as we have described and also requires all files to be contiguous as we have described furthermore it specifies that directory names be limited to eight characters with no extensions use of this level maximizes the chances that a cd rom can be read on every computer level relaxes the length restriction it allows files and directories to have names of up to characters but still from the same set of characters level uses the same name limits as level but partially relaxes the assump tion that files have to be contiguous with this level a file may consist of several sections extents each of which is a contiguous run of blocks the same run may occur multiple times in a file and may also occur in two or more files if large chunks of data are repeated in several files level provides some space optimiza tion by not requiring the data to be present multiple times rock ridge extensions as we have seen iso is highly restrictive in several ways shortly after it came out people in the unix community began working on an extension to make it possible to represent unix file systems on a cd rom these extensions were named rock ridge after a town in the gene wilder movie blazing saddles probably because one of the committee members liked the film the extensions use the system use field in order to make rock ridge cd roms readable on any computer all the other fields retain their normal iso meaning any system not aware of the rock ridge extensions just ignores them and sees a normal cd rom the extensions are divided up into the following fields px posix attributes pn major and minor device numbers sl symbolic link nm alternative name cl child location pl parent location re relocation tf time stamps the px field contains the standard unix rwxrwxrwx permission bits for the owner group and others it also contains the other bits contained in the mode word such as the setuid and setgid bits and so on sec example file systems to allow raw devices to be represented on a cd rom the pn field is pres ent it contains the major and minor device numbers associated with the file in this way the contents of the dev directory can be written to a cd rom and later reconstructed correctly on the target system the sl field is for symbolic links it allows a file on one file system to refer to a file on a different file system probably the most important field is mm it allows a second name to be asso ciated with the file this name is not subject to the character set or length restric tions of iso making it possible to express arbitrary unix file names on a cd rom the next three fields are used together to get around the iso limit of di rectories that may only be nested eight deep using them it is possible to specify that a directory is to be relocated and to tell where it goes in the hierarchy it is effectively a way to work around the artificial depth limit finally the tf field contains the three timestamps included in each unix i node namely the time the file was created the time it was last modified and the time it was last accessed together these extensions make it possible to copy a unix file system to a cd rom and then restore it correctly to a different system joliet extensions the unix community was not the only group that wanted a way to extend iso microsoft also found it too restrictive although it was microsoft own ms dos that caused most of the restrictions in the first place therefore microsoft invented some extensions that were called joliet they were designed to allow windows file systems to be copied to cd rom and then restored in pre cisely the same way that rock ridge was designed for unix virtually all pro grams that run under windows and use cd roms support joliet including pro grams that bum cd recordables usually these programs offer a choice between the various iso levels and joliet the major extensions provided by joliet are long file names unicode character set directory nesting deeper than eight levels directory names with extensions the first extension allows file names up to characters the second extension enables the use of the unicode character set for file names this extension is im portant for software intended for use in countries that do not use the latin alpha bet such as japan israel and greece since unicode characters are bytes the maximum file name in joliet occupies bytes file systems chap like rock ridge the limitation on directory nesting is removed by joliet di rectories can be nested as deeply as needed finally directory names can have extensions it is not clear why this extension was included since windows direc tories virtually never use extensions but maybe some day they will the ms dos file system sec example file systems bytes file name extension attributes reserved time date first block number the ms dos file system is the one the first ibm pcs came with it was the main file system up through windows and windows me it is still supported on windows windows xp and windows vista although it is no longer standard on new pcs now except for floppy disks however it and an extension of it fat have become widely used for many embedded systems most digi tal cameras use it many players use it exclusively the popular apple ipod uses it as the default file system although knowledgeable hackers can reformat the ipod and instatta different file system thus the number of electronic devices using the ms dos file system is vastly larger now than at any time in the past and certainly much larger than the number using the more modern ntfs file system for that reason alone it is worth looking at in some detail to read a file an ms dos program must first make an open system call to get a handle for it the open system call specifies a path which may be either abso lute or relative to the current working directory the path is looked up component by component until the final directory is located and read into memory it is then searched for the file to be opened although ms dos directories are variable sized they use a fixed size byte directory entry the format of an ms dos directory entry is shown in fig it contains the file name attributes creation date and time starting block and exact file size file names shorter than characters are left justified and padded with spaces on the right in each field separately the attributes field is new and contains bits to indicate that a file is read only needs to be archived is hidden or is a system file read only files cannot be written this is to protect them from accidental damage the archived bit has no actual operating system function i e ms dos does not examine or set it the intention is to allow user level archive programs to clear it upon archiving a file and to have other programs set it when modifying a file in this way a backup program can just examine this attribute bit on every file to see which files to back up the hidden bit can be set to prevent a file from appearing in directory listings its main use is to avoid confusing novice users with files they might not understand finally the system bit also hides files in addition system files cannot accidentally be deleted using the del command the main components of ms dos have this bit set the directory entry also contains the date and time the file was created or last modified the time is accurate only to sec because it is stored in a byte field which can store only unique values a day contains seconds the figure the ms dos directory entry time field is subdivided into seconds bits minutes bits and hours bits the date counts in days using three subfields day bits month bits and year bits with a bit number for the year and time beginning in the highest expressible year is thus ms dos has a built in problem to avoid catastrophe ms dos users should begin with compliance as early as possible if ms dos had used the combined date and time fields as a bit seconds counter it could have represented every second exactly and delayed the catastrophe until ms dos stores the file size as a bit number so in theory files can be as large as gb however other limits described below restrict the maximum file size to gb or less a surprisingly large part of the entry bytes is unused ms dos keeps track of file blocks via a file allocation table in main memory the directory entry contains the number of the first file block this number is used as an index into a entry fat in main memory by following the chain all the blocks can be found the operation of the fat is illustrated in fig the fat file system comes in three versions fat fat and fat depending on how many bits a disk address contains actually fat is some thing of a misnomer since only the low order bits of the disk addresses are used it should have been called fat but powers of two sound so much neater for all fats the disk block can be set to some multiple of bytes possib ly different for each partition with the set of allowed block sizes called cluster sizes by microsoft being different for each variant the first version of ms dos used fat with byte blocks giving a maximum partition size of x bytes actually only x bytes because of the disk addresses were used as special markers such as end of file bad block etc with these parameters the maximum disk partition size was about mb and the size of the fat table in memory was entries of bytes each using a bit table entry would have been too slow this system worked well for floppy disks but when hard disks came out it became a problem microsoft solved the problem by allowing additional block sizes of kb kb and kb this change preserved the structure and size of the fat table but allowed disk partitions of up to mb file systems chap since ms dos supported four disk partitions per disk drive the new fat fde system worked up to mb disks beyond that something had to give what happened was the introduction of fat with bit disk pointers additionally block sizes of kb kb and kb were permitted is the largest power of two that can be represented in bits the fat table now occupied kb of main memory all the time but with the larger memories by then avail able it was v idely used and rapidly replaced the fat file system the largest disk partition that can be supported by fat is gb entries of kb each and the largest disk gb namely four partitions of gb each for business letters this limit is not a problem but for storing digital video using the dv standard a gb file holds just over minutes of video as a con sequence of the fact that a pc disk can support only four partitions the largest video that can be stored on a disk is about minutes no matter how large the disk is this limit also means that the largest video that can be edited on line is less than minutes since both input and output files are needed starting with the second release of windows the fat file system with its bit disk addresses was introduced and the version of ms dos underlying windows was adapted to support fat in this system partitions could theoretically be x bytes but they are actually limited to tb gb because internally the system keeps track of partition sizes in byte sectors using a bit number and x is tb the maximum partition size for var ious block sizes and ail three fat types is shown in fig block size fat fat fat kb kb b mb mb tb tb mb tb kb mb tb figure maximum partition size for different block sizes the empty boxes represent forbidden combinations in addition to supporting larger disks the fat file system has two other advantages over fat first an gb disk using fat can be a single parti tion using fat it has to be four partitions which appears to the windows user as the c d e and f logical disk drives it is up to the user to decide which file to place on which drive and keep track of what is where the other advantage of fat over fat is that for a given size disk par tition a smaller block size can be used for example for a gb disk partition sec example file systems fat must use kb blocks otherwise with only available disk ad dresses it cannot cover the whole partition in contrast fat can use for ex ample kb blocks for a gb disk partition the advantage of the smaller block size is that most files are much shorter than kb if the block size is kb a file of bytes ties up kb of disk space if the average file is say kb then with a kb block of the disk will be wasted not a terribly efficient way to use the disk with an kb file and a kb block there is no disk wastage but the price paid is more ram eaten up by the fat with a kb block and a gb disk partition there are blocks so the fat must have entries in mem ory occupying mb of ram ms dos uses the fat to keep track of free disk blocks any block that is not currently allocated is marked with a special code when ms dos needs a new disk block it searches the fat for an entry containing this code thus no bitmap or free list is required th e unix file system even early versions of unix had a fairly sophisticated multiuser file system since it was derived from multics below we will discuss the file system the one for the pdp that made unix famous we will examine a modem unix file system in the context of linux in chap the file system is in the form of a tree starting at the root directory with the addition of links forming a directed acyclic graph file names are up to char acters and can contain any ascii characters except because that is the separator between components in a path and nul because that is used to pad out names shorter than characters nul has the numerical value of a unix directory entry contains one entry for each file in that directory each entry is extremely simple because unix uses the i node scheme illustrated in fig a directory entry contains only two fields the file name bytes and the number of the i node for that file bytes as shown in fig these pa rameters limit the number of files per file system to like the i node of fig the unix i nodes contains some attributes the attributes contain the file size three times creation last access and last modifica tion owner group protection information and a count of the number of direc tory entries that point to the i node the latter field is needed due to links when ever a new link is made to an i node the count in the i node is increased when a link is removed the count is decremented when it gets to the i node is re claimed and the disk blocks are put back in the free list keeping track of disk blocks is done using a generalization of fig in order to handle very large files the first disk addresses are stored in the i node file systems chap bytes file name i node number figure a unix directory entry sec example file systems up we will use unix as an example but the algorithm is basically the same for all hierarchical directory systems first the fde system locates the root directory in unix its i node is located at a fixed place on the disk from this i node it locates the root directory which can be anywhere on the disk but say block then it reads the root directory and looks up the first component of the path usr in the root directory to find the i node number of the file usr locating an i node from its number is straightforward since each one has a fixed location on the disk from this i node the system locates the directory for usr and looks up the next component ast in it when it has found the entry for ast it has the i node for the directory usr ast from this i node it can find the directory itself and look up mbox the i node for this file is then read into memory and kept there until the itself so for small files all the necessary information is right in the i node which is fetched from disk to main memory when the fde is opened for somewhat larg er fdes one of the addresses in the i node is the address of a disk block called a single indirect block this block contains additional disk addresses if this still is not enough another address in the i node called a double indirect block con file is closed the lookup process is illustrated in fig block is usr block is usr ast tains the address of a block that contains a list of single indirect blocks each of these single indirect blocks points to a few hundred data blocks if even this is not enough a triple indirect block can also be used the complete picture is given in fig i node root directory directory directory looking up usr yields i node i node says that usr is in block usr ast is i node i node says that usr ast is in block usr ast mbox is i node figure the steps in looking up usr ast mbox figure a unix i node sanarjacaaeasaea relative path names are looked up the same way as absolute ones only start ing from the working directory instead of starting from the root directory every directory has entries for and which are put there when the directory is created the entry has the i node number for the current directory and the entry for has the i node number for the parent directory thus a procedure looking up dick prog c simply looks up in the working directory finds the i node number for the parent directory and searches that directory for dick no special mechan ism is needed to handle these names as far as the directory system is concerned they are just ordinary ascii strings just the same as any other names the only bit of trickery here is that in the root directory points to itself file systems chap research on file systems file systems have always attracted more research than other parts of the oper ating system and that is still the case while standard file systems are fairly well understood there is still quite a bit of research going on about optimizing buffer cache management burnett et al ding et al gnaidy et al kroeger and long pai et al and zhou et al work is going on about new kinds of fde systems such as user level file systems mazietes flash fde systems gal et al journaling file systems prabhakaran et al and stein et al versioning file systems cornell et al peer to peer file systems muthitacharoen et al and others the google file system is also unusual due to its great fault tolerance ghemawat et al different ways of finding things in file systems are also of interest padioleau and ridoux another area that has been getting attention is provenance keeping track of the history of the data including where they came from who owns them and how they has been transformed muniswarmy reddy et al and shah et al this information can be used in a variety of ways making backups is still getting some attention too cox et ai and rycroft as is the related topic of recovery keeton et al related to backups is keeping data around and usable for decades baker et al maniatis et al reliability and security are also far from solved problems greenan and miller wires and feeley wright et al and yang et al and finally per formance has always been a research topic and still is caudill and gavrikovska chiang and huang srein wang et al and zhang and ghose summary when seen from the outside a file system is a collection of files and direc tories plus operations on them files can be read and written directories can be created and destroyed and files can be moved from directory to directory most modern file systems support a hierarchical directory system in which directories may have subdirectories and these may have subsubdirectories ad infinitum when seen from the inside a file system looks quite different the file system designers have to be concerned with how storage is allocated and how the system keeps track of which block goes with which file possibilities include contiguous files linked lists file allocation tables and i nodes different systems have dif ferent directory structures attributes can go in the directories or somewhere else e g an i node disk space can be managed using free lists of bitmaps file sys szzbses i s e n b a n l e i b y m a k i n s incremental dumps and by having a pro gram that can repair s l c k file systems file system performance is important and sec summary can be enhanced in several ways including caching read ahead and carefully placing the blocks of a file close to each other log structured file systems also improve performance by doing writes in large units examples of file systems include iso ms dos and unix these differ in many ways including how they keep track of which blocks go with which file directory structure and management of free disk space problems give five different path names for the file etc posswd hint think about the direc tory entries and in windows when a user double clicks on a file listed by windows explorer a pro gram is run and given that fde as a parameter list two different ways the operating system could know which program to run in early unix systems executable files a out files began with a very specific magic number not one chosen at random these files began with a header followed by the text and data segments why do you think a very specific number was chosen for ex ecutable files whereas other file types had a more or less random magic number as the first word in fig one of the attributes is the record length why does the operating system ever care about this systems that support sequential files always have an operation to rewind files do systems that support random access files need this too in some systems it is possible to map part of a file into memory what restrictions must such systems impose how is this partial mapping implemented a simple operating system only supports a single directory but allows that directory to have arbitrarily many files with arbitrarily long file names can something approxi mating a hierarchical file system be simulated how in unix and windows random access is done by having a special system call that moves the current position pointer associated with a file to a given byte in the file propose an alternative way to do random access without having this system call consider the directory tree of fig if usr jim is the working directory what is the absolute path name for the file whose relative path name is jast xl contiguous allocation of files leads to disk fragmentation as mentioned in the text because some space in the last disk block will be wasted in files whose length is not an integral number of blocks is this internal fragmentation or external fragmentation mate an analogy i t t something toed iii ft jefflllj ttg in light of the answer to the previous question dors any sense wrai flfria file systems chap some digital consumer devices need to store data for example as files name a mod ern device that requires file storage and for which contiguous allocation would be a fine idea how does ms dos implement random access to files consider the i node shown in fig if it contains direct addresses of bytes each and all disk blocks are kb what is the largest possible file it has been suggested that efficiency could be improved and disk space saved by stor ing the data of a short file within the i node for the i node of fig 13 how many bytes of data could be stored inside the i node name one advantage of hard links over symbolic links and one advantage of symbolic links over hard links free disk space can be kept track of using a free list or a bitmap disk addresses re quire d bits for a disk with b blocks f of which are free state the condition under which the free list uses less space than the bitmap for d having the value bits express your answer as a percentage of the disk space that must be free is what would happen if the bitmap or free list containing the information about free disk blocks was completely lost due to a crash is there any way to recover from this disaster or is it bye bye disk discuss your answers for unix and the fat file system separately chap problems a certain file system uses kb disk blocks the median file size is kb if all files were exactly kb what fraction of the disk space would be wasted do you think the wastage for a real file system will be higher than this number or lower than it explain your answer the ms dos fat table contains entries suppose that one of the bits had been needed for some other purpose and that the table contained exactly entries in stead with no other changes what would the largest ms dos file have been under this condition files in ms dos have to compete for space in the fat table in memory if one file uses k entries that is k entries that are not available to any other file what con straint does this place on the total length of all files combined a unix file system has i kb blocks and byte disk addresses what is the maximum file size if i nodes contain direet entries and one single double and triple indirect entry each how many disk operations are needed to fetch the i node for the file usr ast courses os handout tl assume that the i node for the root directory is in memory but nothing else along the path is in memory also assume that all directories fit in one disk block in many unix systems the i nodes are kept at the start of the disk an alternative de sign is to allocate an i node when a file is created and put the i node at the starttof the oliver owl night job at the university computing center is to change the tapes used for overnight data backups while waiting for each tape to complete he works on writing his thesis that proves shakespeare plays were written by extraterrestrial visi tors his text processor runs on the system being backed up since that is the only one they have is there a problem with this arrangement we discussed making incremental dumps in some detail in the text in windows it is easy to tell when to dump a file because every file has an archive bit this bit is miss ing in unix how do unix backup programs know which files to dump suppose that file in fig was not modified since the last dump in what way would the four bitmaps of fig be different it has been suggested that the first part of each unix file be kept in the same disk block as its i node what good would this do consider fig is it possible that for some particular block number the counters in both lists have the value how should this problem be corrected the performance of a file system depends upon the cache hit rate fraction of blocks found in the cache if it takes msec to satisfy a request from the cache but msec to satisfy a request if a disk read is needed give a formula for the mean time required to satisfy a request if the hit rate is h plot this function for values of h varying from to i jo consider the idea behind fig but now for a disk with a mean seek time of msec a rotational rafe of rpm and bytes per track what are the data rates for block sizes of kb kb and kb respectively first block of the file discuss the pros and cons of this alternative write a program that reverses the bytes of a file so that the last byte is now first and the first byte is now last it must work with an arbitrarily long file but try to make it reasonably efficient write a program that starts at a given directory and descends the file tree from that point recording the sizes of all the files it finds when it is all done it should print a histogram of the file sizes using a bin width specified as a parameter e g with file sizes of to go in one bin to go in the next bin etc write a program that scans all directories in a unix file system and finds and locates all i nodes with a hard link count of two or more for each such file it lists together all file names that point to the file write a new version of the unix is program this version takes as an argument one or more directory names and for each directory lists all the files in that directory one line per file each field should be formatted in a reasonable way given its type list only the first disk address if any input output in addition to providing abstractions such as processes and threads address spaces and files an operating system also controls all the computer input output devices it must issue commands to the devices catch interrupts and handle errors it should also provide an interface between the devices and the rest of the system that is simple and easy to use to the extent possible the inter face should be the same for all devices device independence the i o code rep resents a significant fraction of the total operating system how the operating sys tem manages i o is the subject of this chapter this chapter is organized as follows first we will look at some of the princi ples of i o hardware and then we will look at i o software in general i o soft ware can be structured in layers with each layer having a well defined task we will look at these layers to see what they do and how they fit together following that introduction we will look at several j devices in detail disks clocks keyboards and displays for each device we will look at its hard ware and software finally we will consider power management principle s o f i o hardwar e different people look at i o hardware in different ways electrical engineers look at it in terms of chips wires power supplies motors and all the other physi cal components that make up the hardware programmers look at the interface input output chap presented to the software the commands the hardware accepts the functions it sec principles of i o hardware carries out and the errors that can be reported back in this book we are con cerned with programming i o devices not designing building or maintaining them so our interest will be restricted to how the hardware is programmed not how it works inside nevertheless the programming of many i o devices is often intimately connected with their internal operation in the next three sections we will provide a little general background on i o hardware as it relates to pro gramming it may be regarded as a review and expansion of the introductory material in sec i o devices i o devices can be roughly divided into two categories block devices and character devices a block device is one that stores information in fixed size blocks each one with its own address common block sizes range from bytes to bytes all transfers are in units of one or more entire consecutive blocks the essential property of a block device is that it is possible to read or write each block independently of ail the other ones hard disks cd roms and usb sticks are common block devices if you look closely the boundary between devices that are block addressable and those that are not is not well defined everyone agrees that a disk is a block addressable device because no matter where the arm currently is it is always pos sible to seek to another cylinder and then wait for the required block to rotate under the head now consider a tape drive used for making disk backups tapes contain a sequence of blocks if the tape drive is given a command to read block n it can always rewind the tape and go forward until it comes to block n this operation is analogous to a disk doing a seek except that it takes much longer also it may or may not be possible to rewrite one block in the middle of a tape even if it were possible to use tapes as random access block devices that is stretching the point somewhat they are normally not used that way the other type of i o device is the character device a character device de livers or accepts a stream of characters without regard to any block structure it is not addressable and does not have any seek operation printers network inter faces mice for pointing rats for psychology lab experiments and most other devices that are not disk like can be seen as character devices this classification scheme is not perfect some devices just do not fit in clocks for example are not block addressable nor do they generate or accept character streams all they do is cause interrupts at well defined intervals mem ory mapped screens do not fit the model well either still the model of block and character devices is general enough that it can be used as a basis for making some of the operating system software dealing with i o device independent the file system for example deals just with abstract block devices and leaves the device dependent part to lower level software s h u g e r a n g e i n s p e e d w h i c h p u t considerable pressure o n he software to perform well over many orders of magnitude in data rates fs shows the data rates of some common devices most of these devices tend torn taster as time goes on to device data rate keyboard bytes sec mouse bytes sec modem kb sec scanner kb sec digital camcorder mb sec wireless mb sec cd rom mb sec fast ethernet mb sec compact flash card mb sec firewire ieee mb sec usb mb sec sonet oc network mb sec scsi ultra disk mb sec gigabit ethernet mb sec sata disk drive mb sec ultrium tape mb sec pci bus mb sec figure some typical device network and bus data rates device controllers i o units typically consist of a mechanical component and an electronic com ponent it is often possible to separate the two portions to provide a more modular and general design the electronic component is called the device controller or adapter on personal computers it often takes the form of a chip on the par entboard or a printed circuit card that can be inserted into a pci expansion slot the mechanical component is the device itself this arrangement is shown in fig the controller card usually has a connector on it into which a cable leading to the device itself can be plugged many controllers can handle two four or even eight identical devices if the interface between the controller and device is a standard interface either an official ansi ieee or iso standard or a de facto input output chap sec principles of i o hardwar e one then companies can make controllers or devices that fit that interface many companies for example make disk drives that match the ide sata scsi usb or firewire ieee interface the interface between the controller and the device is often a very low level interface a disk for example might be formatted with sectors of bytes per track what actually comes off the drive however is a serial bit stream starting with a preamble then the bits in a sector and finally a checksum also called an error correcting code ecc the preamble is written when the disk is formatted and contains the cylinder and sector number the sector size and similar data as well as synchronization information the controller job is to convert the serial bit stream into a block of bytes and perform any error correction necessary the block of bytes is typically first as sembled bit by bit in a buffer inside the controller after its checksum has been verified and the block has been declared to be error free it can then be copied to main memory the controller for a monitor also works as a bit serial device at an equally low level it reads bytes containing the characters to be displayed from memory and generates the signals used to modulate the crt beam to cause it to write on the screen the controller also generates the signals for making the crt beam do a horizontal retrace after it has finished a scan line as well as the signals for mak ing it do a vertical retrace after the entire screen has been scanned if it were not for the crt controller the operating system programmer would have to explicitly program the analog scanning of the tube with the controller the operating system initializes the controller with a few parameters such as the number of characters or pixels per line and number of lines per screen and lets the controller take care of actually driving the beam flat screen tft displays are different but just as complicated memory mapped i o each controller has a few registers that are used for communicating with the cpu by writing into these registers the operating system can command the de vice to deliver data accept data switch itself on or off or otherwise perform some action by reading from these registers the operating system can learn what the device state is whether it is prepared to accept a new command and so on in addition to the control registers many devices have a data buffer that the operating system can read and write for example a common way for computers to display pixels on the screen is to have a video ram which is basically just a data buffer available for programs or the operating system to write into the issue thus arises of how the cpu communicates with the control registers and the device data buffers two alternatives exist in the first approach each control register is assigned an i o port number an or bit integer the set of all the i o ports form the i o port space and is protected so that ordinary user programs cannot access it only the operating system can using a special i o in struction such as in reg port the cpu can read in control register port and store the result in cpu register reg similarly using out port reg the cpu can write the contents of reg to a control register most early computers including nearly all mainframes such as the ibm and all of its successors worked this way in this scheme the address spaces for memory and i o are different as shown in fig a the instructions in and mov m r t h e t h e e e a m p t e different and unrelated address ce two address one address space two address spaces oxffff memory i o ports figure a separate i o and memory space b memory mapped i o c hybrid the second approach introduced with the pdp is to map all the control registers into the memory space as shown in fig b each control register is assigned a unique memory address to which no memory is assigned this system is called memory mapped i o usually the assigned addresses are at the top of the address space a hybrid scheme with memory mapped i o data buffers and separate i o ports for the control registers is shown in fig c the pentium uses this architecture with addresses to being reserved for device data buffers in ibm pc compatibles in addition to i o ports through input output chap how do these schemes work in all cases when the cpu wants to read a word either from memory or from an i o port it puts the address it needs on the bus address lines and then asserts a rea d signal on a bus control line a second signal line is used to tell whether i o space or memory space is needed if it is memory space the memory responds to the request if it is i o space the i o de vice responds to the request if there is only memory space as in fig b ev ery memory module and every i o device compares the address lines to the range of addresses that it services if the address falls in its range it responds to the re quest since no address is ever assigned to both memory and an i o device there is no ambiguity and no conflict the two schemes for addressing the controllers have different strengths and weaknesses let us start with the advantages of memory mapped i o first if special i o instructions are needed to read and write the device control registers access to them requires the use of assembly code since there is no way to execute an in or ou t instruction in c or c calling such a procedure adds overhead to controlling i o in contrast with memory mapped i o device control registers are just variables in memory and can be addressed in c the same way as any other variables thus with memory mapped i o a i o device driver can be written en tirely in c without memory mapped i o some assembly code is needed second with memory mapped i o no special protection mechanism is need ed to keep user processes from performing i o all the operating system has to do sec principles of i o hardware in computer design practically everything involves trade offs and that is the case here too memory mapped i o also has its disadvantages first most com puters nowadays have some form of caching of memory words caching a device control register would be disastrous consider the assembly code loop given above in the presence of caching the first reference to would cause it to be cached subsequent references would just take the value from the cache and not even ask the device then when the device finally became ready the software would have no way of finding out instead the loop would go on forever to prevent this situation with memory mapped i o the hardware has to be equipped with the ability to selectively disable caching for example on a per page basis this feature adds extra complexity to both the hardware and the oper ating system which has to manage the selective caching second if there is only one address space then all memory modules and all i o devices must examine all memory references to see which ones to respond to if the computer has a single bus as in fig a having everyone look at every address is straightforward cpu reads and writes of memory go over this high bandwidth bus is refrain from putting that portion of the address space containing the control reg isters in any user virtual address space better yet if each device has its control registers on a different page of the address space the operating system can give a user control over specific devices but not others by simply including the desired pages in its page table such a scheme can allow different device drivers to be placed in different address spaces not only reducing kernel size but also keeping one driver from interfering with others third with memory mapped i o every instruction that can reference memory cpu memory i o all addresses memory and i o go here a bus cpu memory b i o this memory port is to allow i o devices access to memory can also reference control registers for example if there is an instruction test that tests a memory word for it can also be used to test a control register for which might be the signal that the device is idle and can accept a new command the assembly language code might look like this loop tes t check if port is be q read y if it is go to ready branc h loo p otherwise continue testing ready if memory mapped i o is not present the control register must first be read into the cpu then tested requiring two instructions instead of one in the case of the loop given above a fourth instruction has to be added slightly slowing down the responsiveness of detecting an idle device figure a a single bus architecture b a dual bus memory architecture however the trend in modern personal computers is to have a dedicated high speed memory bus as shown in fig b a property also found in main frames incidentally this bus is tailored to optimize memory performance with no compromises for the sake of slow i o devices pentium systems can have mul tiple buses memory pci scsi usb isa as shown in fig the trouble with having a separate memory bus on memory mapped machines is that the i o devices have no way of seeing memory addresses as they go by on the memory bus so they have no way of responding to them again special meas ures have to be taken to make memory mapped i o work on a system with multi ple buses one possibility is to first send all memory references to the memory if the memory fails to respond then the cpu tries the other buses this design can be made to work but requires additional hardware complexity input output chap a second possible design is to put a snooping device on the memory bus to pass all addresses presented to potentially interested i o devices the problem here is that i o devices may not be able to process requests at the speed the mem ory can a third possible design which is the one used on the pentium configuration of fig is to filter addresses in the pci bridge chip this chip contains range registers that are preloaded at boot time for example to could be marked as a nonmemory range addresses that fall within one of the ranges mark ed as nonmemory are forwarded onto the pci bus instead of to memory the disadvantage of this scheme is the need for figuring out at boot time which mem ory addresses are not really memory addresses thus each scheme has arguments for and against it so compromises and trade offs are inevitable direct memory access dma no matter whether a cpu does or does not have memory mapped i o it needs to address the device controllers to exchange data with them the cpu can request data from an i o controller one byte at a time but doing so wastes the cpu time so a different scheme called dma direct memory access is often used the operating system can only use dma if the hardware has a dma controller which most systems do sometimes this controller is integrated into disk controllers and other controllers but such a design requires a separate dma controller for each device more commonly a single dma controller is available e g on the parentboard for regulating transfers to multiple devices often concurrently no matter where it is physically located the dma controller has access to the system bus independent of the cpu as shown in fig it contains several reg isters that can be written and read by the cpu these include a memory address register a byte count register and one or more control registers the control reg isters specify the i o port to use the direction of the transfer reading from the i o device or writing to the i o device the transfer unit byte at a time or word at a time and the number of bytes to transfer in one burst to explain how dma works let us first look at how disk reads occur when dma is not used first the disk controller reads the block one or more sectors from the drive serially bit by bit until the entire block is in the controller inter nal buffer next it computes the checksum to verify that no read errors have oc curred then the controller causes an interrupt when the operating system starts running it can read the disk block from the controller buffer a byte or a word at a time by executing a loop with each iteration reading one byte or word from a controller device register and storing it in main memory when dma is used the procedure is different first the cpu programs the dma controller by setting its registers so it knows what to transfer where step sec principles of i o hardware drive figure operation of a dma transfer in fig it also issues a command to the disk controller telling it to read data from the disk into its internal buffer and verify the checksum when valid data are in the disk controller buffer dma can begin the dma controller initiates the transfer by issuing a read request over the bus to the disk controller step this read request looks like any other read re quest and the disk controller does not know or care whether it came from the cpu or from a dma controller typically the memory address to write to is on the bus address lines so when the disk controller fetches the next word from its internal buffer it knows where to write it the write to memory is another stan dard bus cycle step when the write is complete the disk controller sends an acknowledgement signal to the dma controller also over the bus step the dma controller then increments the memory address to use and decrements the byte count if the byte count is still greater than steps through are repeated until the count reaches at that time the dma controller interrupts the cpu to let it know that the transfer is now complete when the operating system starts up it does not have to copy the disk block to memory it is already there dma controllers vary considerably in their sophistication the simplest ones handle one transfer at a time as described above more complex ones can be pro grammed to handle multiple transfers at once such controllers have multiple sets of registers internally one for each channel the cpu starts by loading each set of registers with the relevant parameters for its transfer each transfer must use a dif ferent device controller after each word is transferred steps through in fig the dma controller decides which device to service next it may be set up to use a round robin algorithm or it may have a priority scheme design to favor some devices over others multiple requests to different device controllers may be pending at the same time provided that there is an unambiguous way to input output chap tell the acknowledgements apart often a different acknowledgement line on the bus is used for each dma channel for this reason many buses can operate in two modes word at a time mode and block mode some dma controllers can also operate in either mode in the former mode the operation is as described above the dma controller requests for the transfer of one word and gets it if the cpu also wants the bus it has to wait the mechan ism is called cycle stealing because the device controller sneaks in and steals an occasional bus cycle from the cpu once in a while delaying it slightly in block mode the dma controller tells the device to acquire the bus issue a series of transfers then release the bus this form of operation is called burst mode it is more efficient than cycle stealing because acquiring the bus takes time and multi ple words can be transferred for the price of one bus acquisition the down side to burst mode is that it can block the cpu and other devices for a substantial period of time if a long burst is being transferred in the model we have been discussing sometimes called fly by mode the dma controller tells the device controller to transfer the data directly to main memory an alternative mode that some dma controllers use is to have the de vice controller send the word to the dma controller which then issues a second bus request to write the word to wherever it is supposed to go this scheme re quires an extra bus cycle per word transferred but is more flexible in that it can also perform device to device copies and even memory to memory copies by first issuing a read to memory and then issuing a write to memory at a different address most dma controllers use physical memory addresses for their transfers using physical addresses requires the operating system to convert the virtual ad dress of the intended memory buffer into a physical address and write this physi sec principles of i o hardwar e the next disk word arrived before the previous one had been stored the controller would have to store it somewhere if the bus were very busy the controller might end up storing quite a few words and having a lot of administration to do as well when the block is buffered internally the bus is not needed until the dma begins so the design of the controller is much simpler because the dma transfer to memory is not time critical some older controllers did in fact go directly to memory with only a small amount of internal buffering but when the bus was very busy a transfer might have had to be terminated with an overrun error not all computers use dma the argument against it is that the main cpu is often far faster than the dma controller and can do the job much faster when the limiting factor is not the speed of the i o device if there is no other work for it to do having the fast cpu wait for the slow dma controller to finish is point less also getting rid of the dma controller and having the cpu do ail the work in software saves money important on low end embedded computers interrupts revisited we briefly introduced interrupts in sec but there is more to be said in a typical persona computer system the interrupt structure is as shown in fig at the hardware level interrupts work as follows when an i o device has fin ished the work given to it it causes an interrupt assuming that interrupts have been enabled by the operating system it does this by asserting a signal on a bus line that it has been assigned this signal is detected by the interrupt controller chip on the parentboard which then decides what to do cal address into the dma controller address register an alternative scheme used in a few dma controllers is to write virtual addresses into the dma con troller instead then the dma controller must use the mmu to have the virtual to physical translation done only in the case that the mmu is part of the memory possible but rare rather than part of the cpu can virtual addresses be put on the bus we mentioned earlier that the disk first reads data into its internal buffer be fore dma can start you may be wondering why the controller does not just store the bytes in main memory as soon as it gets them from the disk in other words why does it need an internal buffer there are two reasons first by doing inter cpu interrupt controller devic e is finished bagaesss keyboar d printer bus nal buffering the disk controller can verify the checksum before starting a trans fer if the checksum is incorrect an error is signaled and no transfer is done the second reason is that once a disk transfer has started the bits keep arriv ing from the disk at a constant rate whether the controller is ready for them or not if the controller tried to write data directly to memory it would have to go over the system bus for each word transferred if the bus were busy due to some other device using it e g in burst mode the controller would have to wait if figure how an interrupt happens the connections between the devices and the interrupt controller actually use interrupt lines on the bus rather than dedicated wires if no other interrupts are pending the interrupt controller processes the inter rupt immediately if another one is in progress or another device has made a si multaneous request on a higher priority interrupt request line on the bus the input output chap device is just ignored for the moment in this case it continues to assert an inter rupt signal on the bus until it is serviced by the cpu to handle the interrupt the controller puts a number on the address lines specifying which device wants attention and asserts a signal to interrupt the cpu the interrupt signal causes the cpu to stop what it is doing and start doing something else the number on the address lines is used as an index into a table called the interrupt vector to fetch a new program counter this program counter points to the start of the corresponding interrupt service procedure typically traps and interrupts use the same mechanism from this point on and frequently share the same interrupt vector the location of the interrupt vector can be hardwired into the machine or it can be anywhere in memory with a cpu register loaded by the operating system pointing to its origin shortly after it starts running the interrupt service procedure acknowledges the interrupt by writing a certain value to one of the interrupt controller i o ports this acknowledgement tells the controller that it is free to issue another in terrupt by having the cpu delay this acknowledgement until it is ready to hand le the next interrupt race conditions involving multiple almost simultaneous in terrupts can be avoided as an aside some older computers do not have a cen tralized interrupt controller so each device controller requests its own interrupts the hardware always saves certain information before starting the service procedure which information is saved and where it is saved varies greatly from cpu to cpu as a bare minimum the program counter must be saved so the in terrupted process can be restarted at the other extreme all the visible registers and a large number of internal registers may be saved as well one issue is where to save this information one option is to put it in internal registers that the operating system can read out as needed a problem with this approach is that then the interrupt controller cannot be acknowledged until all potentially relevant information has been read out lest a second interrupt over write the internal registers saving the state this strategy leads to long dead times when interrupts are disabled and possibly lost interrupts and lost data consequently most cpus save the information on the stack however this approach too has problems to start with whose stack if the current stack is used it may well be a user process stack the stack pointer may not even be legal which would cause a fatal error when the hardware tried to write some words at the address pointed to also it might point to the end of a page after several memory writes the page boundary might be exceeded and a page fault generated having a page fault occur during the hardware interrupt processing creates a bigger problem where to save the state to handle the page fault if the kernel stack is used there is a much better chance of the stack pointer being legal and pointing to a pinned page however switching into kernel mode may require changing mmu contexts and will probably invalidate most or all of the cache and tlb reloading all of these statically or dynamically will increase the time to process an interrupt and thus waste cpu time sec principles of i o hardware precise and imprecise interrupts another problem is caused by the fact that most modern cpus are heavily pipelined and often superscalar internally parallel in older systems after each instruction was finished executing the microprogram or hardware checked to see if there was an interrupt pending if so the program counter and psw were pushed onto the stack and the interrupt sequence begun after the interrupt hand ler ran the reverse process took place with the old psw and program counter popped from the stack and the previous process continued this model makes the implicit assumption that if an interrupt occurs just after some instruction all the instructions up to and including that instruction have been executed completely and no instructions after it have executed at all on older machines this assumption was always valid on modem ones it may not be for starters consider the pipeline model of fig l a what happens if an interrupt occurs while the pipeline is full the usual case many instructions are in various stages of execution when the interrupt occurs the value of the pro gram counter may not reflect the correct boundary between executed instructions and nonexecuted instructions in fact many instructions may have been partially executed with different instructions being more or less complete in this situa tion the program counter most likely reflects the address of the next instruction to be fetched and pushed into the pipeline rather than the address of the instruction that just was processed by the execution unit on a superscalar machine such as that of fig l b things are even worse instructions may be decomposed into micro operations and the micro operations may execute out of order depending on the availability of internal resources such as functional units and registers at the time of an interrupt some instructions started long ago may not have started and others started more recently may be al most done at the point when an interrupt is signaled there may be many instruc tions in various states of completeness with less relation between them and the program counter an interrupt that leaves the machine in a well defined state is called a precise interrupt walker and cragon such an interrupt has four properties the pc program counter is saved in a known place all instructions before the one pointed to by the pc have fully executed no instruction beyond the one pointed to by the pc has been executed the execution state of the instruction pointed to by the pc is known note that there is no prohibition on instructions beyond the one pointed to by the pc from starting it is just that any changes they make to registers or memory must be undone before the interrupt happens it is permitted that the instruction pointed to has been executed it is also permitted that it has not been executed input output chap however it must be clear which case applies often if the interrupt is an i o in terrupt the instruction will not yet have started however if the interrupt is really a trap or page fault then the pc generally points to the instruction that caused the fault so it can be restarted later the situation of fig a illustrates a precise interrupt all instructions up to the program counter have completed and none of those beyond it have started or have been rolled back to undo their ef fects 332 sec principles of i o hardware some point are allowed to finish and none beyond that point are allowed to have any noticeable effect on the machine state here the price is paid not in time but in chip area and in complexity of the design if precise interrupts were not re quired for backward compatibility purposes this chip area would be available for larger on chip caches making the cpu faster on the other hand imprecise inter rupts make the operating system far more complicated and slower so it is hard to tell which approach is really better not executed executed 324 principle s o f i o softwar e cecutetf pc a pc 35 executed executed b 304 let us now turn away from the i o hardware and look at the i o software first we will look at the goals of the i o software and then at the different ways i o can be done from the point of view of the operating system goals of the i o software figure a a precise interrupt b an imprecise interrupt an interrupt that does not meet these requirements is called an imprecise interrupt and makes life most unpleasant for the operating system writer who now has to figure out what has happened and what still has to happen fig b shows an imprecise interrupt where different instructions near the program count er are in different stages of completion with older ones not necessarily more com plete than younger ones machines with imprecise interrupts usually vomit a large amount of internal state onto the stack to give the operating system the pos sibility of figuring out what was going on the code necessary to restart the ma chine is typically extremely complicated also saving a large amount of infor mation to memory on every interrupt makes interrupts slow and recovery even worse this leads to the ironic situation of having very fast superscalar cpus sometimes being unsuitable for real time work due to slow interrupts some computers are designed so that some kinds of interrupts and traps are precise and others are not for example having i o interrupts be precise but traps due to fatal programming errors be imprecise is not so bad since no attempt need be made to restart a running process after it has divided by zero some machines have a bit that can be set to force all interrupts to be precise the downside of set ting this bit is that it forces the cpu to carefully log everything it is doing and maintain shadow copies of registers so it can generate a precise interrupt at any instant all this overhead has a major impact on performance some superscalar machines such as the pentium series have precise interrupts to allow old software to work correctly the price paid for precise interrupts is extremely complex interrupt logic within the cpu to make sure that when the in terrupt controller signals that it wants to cause an interrupt all instructions up to a key concept in the design of i o software is known as device indepen dence what it means is that it should be possible to write programs that can ac cess any i o device without having to specify the device in advance for example a program that reads a file as input should be able to read a file on a hard disk a cd rom a dvd or a usb stick without having to modify the program for each different device similarly one should be able to type a command such as sort input output and have it work with input coming from any kind of disk or the keyboard and the output going to any kind of disk or the screen it is up to the operating system to take care of the problems caused by the fact that these devices really are different and require very different command sequences to read or write closely related to device independence is the goal of uniform naming the name of a file or a device should simply be a string or an integer and not depend on the device in any way in unix all disks can be integrated in the file system hierarchy in arbitrary ways so the user need not be aware of which name corresponds to which device for example a usb stick can be mounted on top of the directory usr ast backup so that copying a file to usr astabackup monday cop ies the file to the usb stick in this way all files and devices are addressed the same way by a path name another important issue for i o software is error handling in general er rors should be handled as close to the hardware as possible if the controller dis covers a read error it should try to correct the error itself if it can if it cannot then the device driver should handle it perhaps by just trying to read the block again many errors are transient such as read errors caused by specks of dust on the read head and will frequently go away if the operation is repeated only if the input output chap lower layers are not able to deal with the problem should the upper layers be told about it in many cases error recovery can be done transparently at a low level without the upper levels even knowing about the error still another key issue is that of synchronous blocking versus asynchro nous interrupt driven transfers most physical i o is asynchronous the cpu sec principles of i o software use r starts the transfer and goes off to do something else until the interrupt arrives user programs are much easier to write if the i o operations are blocking after a read system call the program is automatically suspended until the data are avail able in the buffer it is up to the operating system to make operations that are ac tually interrupt driven look blocking to the user programs another issue for the i o software is buffering often data that come off a device cannot be stored directly in its final destination for example when a spac e kernel spac e nex t abc d efg h printe d pag e nex t a b abc d efg h packet comes in off the network the operating system does not know where to put it until it has stored the packet somewhere and examined it also some devices have severe real time constraints for example digital audio devices so the data must be put into an output buffer in advance to decouple the rate at which the buffer is filled from the rate at which it is emptied in order to avoid buffer under runs buffering involves considerable copying and often has a major impact on i o performance the final concept that we will mention here is sharable versus dedicated de vices some i o devices such as disks can be used by many users at the same time no problems are caused by multiple users having open files on the same disk at the same time other devices such as tape drives have to be dedicated to a single user until that user is finished then another user can have the tape drive having two or more users writing blocks intermixed at random to the same tape will definitely not work introducing dedicated unshared devices also introduces a variety of problems such as deadlocks again the operating system must be able to handle both shared and dedicated devices in a way that avoids problems programme d i o there are three fundamentally different ways that i o can be performed in this section we will look at the first one programmed i o in the next two sec tions we will examine the others interrupt driven i o and i o using dma the simplest form of i o is to have the cpu do all the work this method is called programmed i o it is simplest to illustrate programmed i o by means of an example consider a user process that wants to print the eight character string abcdefgh on the printer it first assembles the string in a buffer in user space as shown in fig a the user process then acquires the printer for writing by making a system call to open it if the printer is currently in use by another process this call will fail b c figure steps i n printin g a string and return an error code or will block until the printer is available depending on the operating system and the parameters of the call once it has the printer the user process makes a system call telling the operating system to print the string on the printer the operating system then usually copies the buffer with the strmg to an array say p in kernel space where it is more easily accessed because the kernel may have to change the memory map to get at user space it then checks to see if the printer is currently available if not it waits until it is available as soon as the printer is available the operating system copies the first character to the print er data register in this example using memory mapped i o this action activates the printer the character may not appear yet because some printers buf fer a line or a page before printing anything in fig b however we see that the first character has been printed and that the system has marked the b as the next character to be printed as soon as it has copied the first character to the printer the operating system checks to see if the printer is ready to accept another one generally the printer has a second register which gives its status the act of writing to the data register causes the status to become not ready when the printer controller has processed the current character it indicates its availability by setting some bit in its status register or putting some value in it at this point the operating system waits for the printer to become ready again when that happens it prints the next character as shown in fig c this loop continues until the entire string has been printed then control returns to the user process the actions followed by the operating system are summarized in fig first the data are copied to the kernel then the operating system enters a tight loop outputting the characters one at a time the essential aspect of programmed input output chap i o clearly illustrated in this figure is that after outputting a character the cpu continuously polls the device to see if it is ready to accept another one this be havior is often called polling or busy waiting buffer p count p is the kernel buffer for i i count i loop on every character while ready loop until ready p i output one character figure writing a string to the printer using programmed i o programmed i o is simple but has the disadvantage of tying up the cpu full time until all the i o is done if the time to print a character is very short because all the printer is doing is copying the new character to an internal buffer then busy waiting is fine also in an embedded system where the cpu has nothing else to do busy waiting is reasonable however in more complex systems where the cpu has other work to do busy waiting is inefficient a better i o method is needed interrupt driven i o now let us consider the case of printing on a printer that does not buffer char acters but prints each one as it arrives if the printer can print say charac ters sec each character takes msec to print this means that after every charac ter is written to the printer data register the cpu will sit in an idle loop for msec waiting to be allowed to output the next character this is more than enough time to do a context switch and run some other process for the msec that would otherwise be wasted the way to allow the cpu to do something else while waiting for the printer to become ready is to use interrupts when the system call to print the string is made the buffer is copied to kernel space as we showed earlier and the first character is copied to the printer as soon as it is willing to accept a character at that point the cpu calls the scheduler and some other process is run the process that asked for the string to be printed is blocked until the entire string has printed the work done on the system call is shown in fig a when the printer has printed the character and is prepared to accept the next one it generates an interrupt this interrupt stops the current process and saves its state then the printer interrupt service procedure is run a crude version of this code is shown in fig b if there are no more characters to print the interrupt handler takes some action to unblock the user otherwise it outputs the next char acter acknowledges the interrupt and returns to the process that was running just before the interrupt which continues from where it left off sec principles of i o software buffer p count if count while ready else p register p ij scheduler count count i i acknowledge_jnterrupt interrupt a b figure wriung a string to the printer using interrupt driven i o a code executed at the time the print system call is made b interrupt service proce dure for the printer i o using dma an obvious disadvantage of interrupt driven i o is that an interrupt occurs on every character interrupts take time so this scheme wastes a certain amount of cpu time a solution is to use dma here the idea is to let the dma controller feed the characters to the printer one at time without the cpu being bothered in essence dma is programmed i o only with the dma controller doing all the work instead of the main cpu this strategy requires special hardware the dma controller but frees up the cpu during the i o to do other work an out line of the code is given in fig buffer p count scheduler a b figure printing a string using dma a code executed when the print system call is made b interrupt service procedure the big win with dma is reducing the number of interrupts from one per cha racter to one per buffer printed if there are many characters and interrupts are slow this can be a major improvement on the other hand the dma controller is usually much slower than the main cpu if the dma controller is not capable of driving the device at full speed or the cpu usually has nothing to do anyway while waiting for the dma interrupt then interrupt driven i o or even pro grammed i o may be better most of the time dma is worth it though input output chap i o software layers i o software is typically organized in four layers as shown in fig each layer has a well defined function to perform and a well defined interface to the adjacent layers the functionality and interfaces differ from system to system so the discussion that follows which examines all the layers starting at the bottom is not specific to one machine user level i o software device independent operating system software device drivers interrupt handlers hardware figure layers of the i o software system interrupt handlers while programmed i o is occasionally useful for most i o interrupts are an unpleasant fact of life and cannot be avoided they should be hidden away deep in the bowels of the operating system so that as little of the operating system as possible knows about them the best way to hide them is to have the driver start ing an i o operation block until the i o has completed and the interrupt occurs the driver can block itself by doing a down on a semaphore a wait on a condition variable a receive on a message or something similar for example when the interrupt happens the interrupt procedure does whatever it has to in order to handle the interrupt then it can unblock the driver that started it in some cases it will just complete up on a semaphore in others it will do a signal on a condition variable in a monitor in still others it will send a message to the blocked driver in all cases the net effect of the interrupt will be that a driver that was previously blocked will now be able to run this model works best if drivers are structured as kernel processes with their own states stacks and program counters of course reality is not quite so simple processing an interrupt is not just a matter of taking the interrupt doing an up on some semaphore and then executing an iret instruction to return from the interrupt to the previous process there is a great deal more work involved for the operating system we will now give an outline of this work as a series of steps that must be performed in software after the hardware interrupt has completed it should be noted that the details are very sec i o software layers system dependent so some of the steps listed below may not be needed on a par ticular machine and steps not listed may be required also the steps that do occur may be in a different order on some machines save any registers including the psw that have not already been saved by the interrupt hardware set up a context for the interrupt service procedure doing this may involve setting up the tlb mmu and a page table set up a stack for the interrupt service procedure acknowledge the interrupt controller if there is no centralized inter rupt controller reenable interrupts copy the registers from where they were saved possibly some stack to the process table run the interrupt service procedure it will extract information from the interrupting device controller registers choose which process to run next if the interrupt has caused some high priority process that was blocked to become ready it may be chosen to run now set up the mmu context for the process to run next some tlb set up may also be needed load the new process registers including its psw start running the new process as can be seen interrupt processing is far from trivial it also takes a consid erable number of cpu instructions especially on machines in which virtual mem ory is present and page tables have to be set up or the state of the mmu stored e g the r and m bits on some machines the tlb and cpu cache may also have to be managed when switching between user and kernel modes which takes additional machine cycles device drivers earlier in this chapter we looked at what device controllers do we saw that each controller has some device registers used to give it commands or some de vice registers used to read out its status or both the number of device registers and the nature of the commands vary radically from device to device for ex ample a mouse driver has to accept information from the mouse telling how far it has moved and which buttons are currently depressed in contrast a disk driver may have to know all about sectors tracks cylinders heads arm motion motor input output chap drives head settling times and all the other mechanics of making the disk work properly obviously these drivers will be very different as a consequence each i o device attached to a computer needs some de vice specific code for controlling it this code called the device driver is gen erally written by the device manufacturer and delivered along with the device since each operating system needs its own drivers device manufacturers com monly supply drivers for several popular operating systems each device driver normally handles one device type or at most one class of closely related devices for example a scsi disk driver can usually handle multi ple scsi disks of different sizes and different speeds and perhaps a scsi cd rom as well on the other hand a mouse and joystick are so different that dif ferent drivers are usually required however there is no technical restriction on having one device driver control multiple unrelated devices it is just not a good idea in order to access the device hardware meaning the controller registers the device driver normally has to be part of the operating system kernel at least with current architectures actually it is possible to construct drivers that run in user space with system calls for reading and writing the device registers this de sign isolates the kernel from the drivers and the drivers from each other eliminat ing a major source of system crashes buggy drivers that interfere with the kernel in one way or another for building highly reliable systems this is definitely the way to go an example of a system in which the device drivers run as user proc esses is minix however since most other desktop operating systems expect drivers to run in the kernel that is the model we will consider here since the designers of every operating system know that pieces of code driv ers written by outsiders will be installed in it it needs to have an architecture that allows such installation this means having a well defined model of what a driver does and how it interacts with the rest of the operating system device drivers are normally positioned below the rest of the operating system as is illustrated in fig operating systems usually classify drivers into one of a small number of cat egories the most common categories are the block devices such as disks which contain multiple data blocks that can be addressed independently and the charac ter devices such as keyboards and printers which generate or accept a stream of characters most operating systems define a standard interface that all block drivers must support and a second standard interface that all character drivers must support these interfaces consist of a number of procedures that the rest of the operating system can call to get the driver to do work for it typical procedures are those to read a block block device or write a character string character device in some systems the operating system is a single binary program that contains all of the drivers that it will need compiled into it this scheme was the norm for years with unix systems because they were run by computer centers and i o de sec i o software layers user process user space kernel space hardware devices figure logical positioning of device drivers in reality all communica tion between drivers and device controllers goes over the bus vices rarely changed if a new device was added the system administrator simply recompiled the kernel with the new driver to build a new binary with the advent of personal computers with their myriad i o devices this model no longer worked few users are capable of recompiling or relinking the kernel even if they have the source code or object modules which is not always the case instead operating systems starting with ms dos went over to a model in which drivers were dynamically loaded into the system during execution dif ferent systems handle loading drivers in different ways a device driver has several functions the most obvious one is to accept abstract read and write requests from the device independent software above it and see that they are carried out but there are also a few other functions they must perform for example the driver must initialize the device if needed it may also need to manage its power requirements and log events many device drivers have a similar general structure a typical driver starts out by checking the input parameters to see if they are valid if not an error is input output chap returned if they are valid a translation from abstract to concrete terms may be needed for a disk driver this may mean converting a linear block number into the head track sector and cylinder numbers for the disk geometry next the driver may check if the device is currently in use if it is the request will be queued for later processing if the device is idle the hardware status will be examined to see if the request can be handled now it may be necessary to switch the device on or start a motor before transfers can be begun once the de vice is on and ready to go the actual control can begin controlling the device means issuing a sequence of commands to it the driv er is the place where the command sequence is determined depending on what has to be done after the driver knows which commands it is going to issue it starts writing them into the controller device registers after writing each com mand to the controller it may be necessary to check to see if the controller ac cepted the command and is prepared to accept the next one this sequence contin ues until all the commands have been issued some controllers can be given a linked list of commands in memory and told to read and process them all by it self without further help from the operating system after the commands have been issued one of two situations will apply in many cases the device driver must wait until the controller does some work for it so it blocks itself until the interrupt comes in to unblock it in other cases howev er the operation finishes without delay so the driver need not block as an ex ample of the latter situation scrolling the screen in character mode requires just writing a few bytes into the controller registers no mechanical motion is need ed so the entire operation can be completed in nanoseconds in the former case the blocked driver will be awakened by the interrupt in the latter case it will never go to sleep either way after the operation has been completed the driver must check for errors if everything is all right the driver may have data to pass to the device independent software e g a block just read finally it returns some status information for error reporting back to its caller if any other requests are queued one of them can now be selected and started if nothing is queued the driver blocks waiting for the next request this simple model is only a rough approximation to reality many factors make the code much more complicated for one thing an i o device may com plete while a driver is running interrupting the driver the interrupt may cause a device driver to run in fact it may cause the current driver to run for example while the network driver is processing an incoming packet another packet may arrive consequently drivers have to be reentrant meaning that a running driver has to expect that it will be called a second time before the first call has com pleted in a hot pluggable system devices can be added or removed while the com puter is running as a result while a driver is busy reading from some device the system may inform it that the user has suddenly removed that device from the sys tem not only must the current i o transfer be aborted without damaging any sec i o software layers kernel data structures but any pending requests for the now vanished device must also be gracefully removed from the system and their callers given the bad news furthermore the unexpected addition of new devices may cause the kernel to jug gle resources e g interrupt request lines taking old ones away from the driver and giving it new ones in their place drivers are not allowed to make system calls but they often need to interact with the rest of the kernel usually calls to certain kernel procedures are permit ted for example there are usually calls to allocate and deallocate hardwired pages of memory for use as buffers other useful calls are needed to manage the mmu timers the dma controller the interrupt controller and so on device independent i o software although some of the i o software is device specific other parts of it are de vice independent the exact boundary between the drivers and the device inde pendent software is system and device dependent because some functions that could be done in a device independent way may actually be done in the drivers for efficiency or other reasons the functions shown in fig 13 are typically done in the device independent software uniform interfacing for device drivers buffering error reporting allocating and releasing dedicated devices providing a device independent block size figure 13 functions of the device independent i o software the basic function of the device independent software is to perform the i o functions that are common to all devices and to provide a uniform interface to the user level software below we will look at the above issues in more detail uniform interfacing for device drivers a major issue in an operating system is how to make all i o devices and driv ers look more or less the same if disks printers keyboards and so on are all in terfaced in different ways every time a new device comes along the operating system must be modified for the new device having to hack on the operating sys tem for each new device is not a good idea one aspect of this issue is the interface between the device drivers and the rest of the operating system in fig a we illustrate a situation in which each input output chap device driver has a different interface to the operating system what this means is that the driver functions available for the system to call differ from driver to driv er it might also mean that the kernel functions that the driver needs also differ from driver to driver taken together it means that interfacing each new driver re quires a lot of new programming effort operating system operating system sata disk driver ide disk driver scsi disk driver sata disk driver ide disk driver scsi disk driver a b figur e s a without a standard driver interface b with a standard driver interface in contrast in fig b we show a different design in which all drivers have the same interface now it becomes much easier to plug in a new driver pro viding it conforms to the driver interface it also means that driver writers know what is expected of them in practice not all devices are absolutely identical but usually there are only a small number of device types and even these are generally almost the same the way this works is as follows for each class of devices such as disks or printers the operating system defines a set of functions that the driver must sup ply for a disk these would naturally include read and write but also turning the power on and off formatting and other disky things often the driver contains a table with pointers into itself for these functions when the driver is loaded the operating system records the address of this table of function pointers so when it needs to all one of the functions it can make an indirect call via this table this table of function pointers defines the interface between the driver and the rest of the operating system all devices of a given class disks printers etc must obey it another aspect of having a uniform interface is how i o devices are named the de j independent software takes care of mapping symbolic device names onto tl hfier driver for example in unix a device name such as dev disko uniquely specifies the i node for a special file and this i node tains the major device number which is used to locate the appropriate drh p he i node also contains the minor device number which is passed as a parameter to the driver sec i o software layers in order to specify the unit to be read or written all devices have major and minor numbers and all drivers are accessed by using the major device number to select the driver closely related to naming is protection how does the system prevent users from accessing devices that they are not entitied to access in both unix and windows devices appear in the file system as named objects which means that the usual protection rules for files also apply to i o devices the system adminis trator can then set the proper permissions for each device buffering buffering is also an issue both for block and character devices for a variety of reasons to see one of them consider a process that wants to read data from a modem one possible strategy for dealing with the incoming characters is to have the user process do a read system call and block waiting for one character each arriving character causes an interrupt the interrupt service procedure hands the character to the user process and unblocks it after putting the character some where the process reads another character and blocks again this model is indi cated in fig a user process m o d e m modem modem modem a b c d figure a unbuffered input b buffering in user space c buffering in the kernel followed by copying to user space d double buffering in the kernel the trouble with this way of doing business is that the user process has to be started up for every incoming character allowing a process to run many times for short runs is inefficient so this design is not a good one an improvement is shown in fig b here the user process provides an n character buffer in user space and does a read of n characters the interrupt ser vice procedure puts incoming characters in this buffer until it fills up then it wakes up the user process this scheme is far more efficient than the previous input output chap one but it has a drawback what happens if the buffer is paged out when a charac ter arrives the buffer could be locked in memory but if many processes start locking pages in memory the pool of available pages will shrink and performance will degrade yet another approach is to create a buffer inside the kernel and have the inter rupt handler put the characters there as shown in fig c when this buffer is full the page with the user buffer is brought in if needed and the buffer copied there in one operation this scheme is far more efficient however even this scheme suffers from a problem what happens to charac sec i o software layers use r proces use r j spac e kerne l j space networ k ters that arrive while the page with the user buffer is being brought in from the disk since the buffer is full there is no place to put them a way out is to have a second kernel buffer after the first buffer fills up but before it has been emp tied the second one is used as shown in fig d when the second buffer fills up it is available to be copied to the user assuming the user has asked for it controller networ k r while the second buffer is being copied to user space the first one can be used for new characters in this way the two buffers take turns while one is being copied to user space the other is accumulating new input a buffering scheme like this is called double buffering another form of buffering that is widely used is the circular buffer it con sists of a region of memory and two pointers one pointer points to the next free word where new data can be placed the other pointer points to the first word of data in the buffer that has not been removed yet in many situations the hardware advances the first pointer as it adds new data e g just arriving from the network and the operating system advances the second pointer as it removes and processes data both pointers wrap around going back to the bottom when they hit the top buffering is also important on output consider for example how output is done to the modem without buffering using the model of fig b the user process executes a write system call to output n characters the system has two choices at this point it can block the user until all the characters have been writ ten but this could take a very long time over a slow telephone line it could also release the user immediately and do the i o while the user computes some more but this leads to an even worse problem how does the user process know that the output has been completed and it can reuse the buffer the system could generate a signal or software interrupt but that style of programming is difficult and prone to race conditions a much better solution is for the kernel to copy the data to a kernel buffer analogous in fig c but the other way and unblock the caller immediately now it does not matter when the actual i o has been completed the user is free to reuse the buffer the instant it is unblocked buffering is a widely used technique but it has a downside as well if data get buffered too many times performance suffers consider for example the net work of fig here a user does a system call to write to the network the kernel copies the packet to a kernel buffer to allow the user to proceed immediate ly step at this point the user program can reuse the buffer figure networking may involve many copies of a packet when the driver is called it copies the packet to the controller for output step the reason it does not output to the wire directly from kernel memory is that once a packet transmission has been started it must continue at a uniform speed the driver cannot guarantee that it can get to memory at a uniform speed because dma channels and other i o devices may be stealing many cycles faili ng to get a word on time would ruin the packet by buffering the packet inside the con troller this problem is avoided after the packet has been copied to the controller internal buffer it is copied out onto the network step bits arrive at the receiver shortly after being sent so just after the last bit has been sent that bit arrives at the receiver where the packet has been buffered in the controller next the packet is copied to the re ceiver kernel buffer step finally it is copied to the receiving process buff er step usually the receiver then sends back an acknowledgement when the sender gets the acknowledgement it is free to send the next packet however it should be clear that all this copying is going to slow down the transmission rate considerably because all the steps must happen sequentially error reporting errors are far more common in the context of i o than in other contexts when they occur the operating system must handle them as best it can many er rors are device specific and must be handled by the appropriate driver but the framework for error handling is device independent one class of i o errors is programming errors these occur when a process asks for something impossible such as writing to an input device keyboard scan ner mouse etc or reading from an output device printer plotter etc other errors are providing an invalid buffer address or other parameter and specifying input output chap an invalid device e g disk when the system has only two disks and so on the action to take on these errors is straightforward just report back an error code to the caller another class of errors is the class of actual i o errors for example trying to write a disk block that has been damaged or trying to read from a camcorder that has been switched off in these circumstances it is up to the driver to determine what to do if the driver does not know what to do it may pass the problem back up to device independent software what this software does depends on the environment and the nature of the er ror if it is a simple read error and there is an interactive user available it may display a dialog box asking the user what to do the options may include retrying a certain number of times ignoring the error or killing the calling process if there is no user available probably the only real option is to have the system call fail with an error code however some errors cannot be handled this way for example a critical data structure such as the root directory or free block list may have been des troyed in this case the system may have to display an error message and termi nate allocating and releasing dedicated devices some devices such as cd rom recorders can be used only by a single proc ess at any given moment it is up to the operating system to examine requests for device usage and accept or reject them depending on whether the requested de vice is available or not a simple way to handle these requests is to require proc esses to perform opens on the special files for devices directly if the device is unavailable the open fails closing such a dedicated device then releases it an alternative approach is to have special mechanisms for requesting and releasing dedicated devices an attempt to acquire a device that is not available blocks the caller instead of failing blocked processes are put on a queue sooner or later the requested device becomes available and the first process on the queue is allowed to acquire it and continue execution device independent block size different disks may have different sector sizes it is up to the device indepen dent software to hide this fact and provide a uniform block size to higher layers for example by treating several sectors as a single logical block in this way the higher layers only deal with abstract devices that all use the same logical block size independent of the physical sector size similarly some character devices deliver their data one byte at a time e g modems while others deliver theirs in larger units e g network interfaces these differences may also be hidden sec i o soffware layers user space i o software although most of the i o software is within the operating system a small por tion of it consists of libraries linked together with user programs and even whole programs running outside the kernel system calls including the i o system calls are normally made by library procedures when a c program contains the call count write fd buffer nbytes the library procedure write will be linked with the program and contained in the binary program present in memory at run time the collection of all these library procedures is clearly part of the i o system while these procedures do little more than put their parameters in the appropriate place for the system call there are other i o procedures that actually do real work in particular formatting of input and output is done by library pro cedures one example from c is printf which takes a format string and possibly some variables as input builds an ascii string and then calls write to output the string as an example of printf consider the statement printf the square of is n i i i it formats a string consisting of the character string the square of followed by the value i as a character string then the character string is then i as six characters and finally a line feed an example of a similar procedure for input is scanf which reads input and stores it into variables described in a format string using the same syntax as printf the standard i o library contains a number of procedures that involve i o and all run as part of user programs not all user level i o software consists of library procedures another impor tant category is the spooling system spooling is a way of dealing with dedicated i o devices in a multiprogramming system consider a typical spooled device a printer although it would be technically easy to let any user process open the character special file for the printer suppose a process opened it and then did nothing for hours no other process could print anything instead what is done is to create a special process called a daemon and a special directory called a spooling directory to print a file a process fust gen erates the entire file to be printed and puts it in the spooling directory it is up to the daemon which is the only process having permission to use the printer spe cial file to print the files in the directory by protecting the special file against direct use by users the problem of having someone keeping it open unnecessarily long is eliminated spooling is not only used for printers it is also used in other i o situations for example file transfer over a network often uses a network daemon to send a file somewhere a user puts it in a network spooling directory later on the net work daemon takes it out and transmits it one particular use of spooled file input output chap transmission is the usenet news system this network consists of millions of machines around the world communicating using the internet thousands of news groups exist on many topics to post a news message the user invokes a news program which accepts the message to be posted and then deposits it in a spool ing directory for transmission to other machines later the entire news system runs outside the operating system figure summarizes the i o system showing all the layers and the princi pal functions of each layer starting at the bottom the layers are the hardware in terrupt handlers device drivers device independent software and finally the user processes sec disks disk hardware disks come in a variety of types the most common ones are the magnetic disks hard disks and floppy disks they are characterized by the fact that reads and writes are equally fast which makes them ideal as secondary memory pag ing file systems etc arrays of these disks are sometimes used to provide high ly reliable storage for distribution of programs data and movies various kinds of optical disks cd roms cd recordables and dvds are also important in the following sections we will first describe the hardware and then the software for these devices i o laye r y reply i o functions magnetic disks i o request user processes a device independen t software device drivers interrupt handlers hardwar e mak e i o call format i o spooling naming protection blocking buffering allocation se t up device registers check status wak e u p driver whe n i o complete d perform i o operation magnetic disks are organized into cylinders each one containing as many tracks as there are heads stacked vertically the tracks are divided into sectors with the number of sectors around the circumference typically being to on floppy disks and up to several hundred on hard disks the number of heads varies from to about older disks have littie electronics and just deliver a simple serial bit stream on these disks the controller does most of the work on other disks in particular ide integrated drive electronics and sat a serial ata disks the disk drive itself contains a microcontroller that does considerable work and allows the real controller to issue a set of higher level commands the controller often does track caching bad block remapping and much more figure layers of the i o system and the main functions of each layer the arrows in fig show the flow of control when a user program tries to read a block from a file for example the operating system is invoked to carry out the call the device independent software looks for it in the buffer cache for example if the needed block is not there it calls the device driver to issue the re quest to the hardware to go get it from the disk the process is then blocked until the disk operation has been completed when the disk is finished the hardware generates an interrupt the interrupt handler is run to discover what has happened that is which device wants atten tion right now it then extracts the status from the device and wakes up the sleep ing process to finish off the i o request and let the user process continue disks now we will begin studying some real i o devices we will begin with disks which are conceptually simple yet very important after that we will examine clocks keyboards and displays a device feature that has important implications for the disk driver is the pos sibility of a controller doing seeks on two or more drives at the same time these are known as overlapped seeks while the controller and software are waiting for a seek to complete on one drive the controller can initiate a seek on another drive many controllers can also read or write on one drive while seeking on one or more other drives but a floppy disk controller cannot read or write on two drives at the same time reading or writing requires the controller to move bits on a microsecond time scale so one transfer uses up most of its computing pow er the situation is different for hard disks with integrated controllers and in a system with more than one of these hard drives they can operate simultaneously at least to the extent of transferring between the disk and the controller buffer memory only one transfer between the controller and the main memory is pos sible at once however the ability to perform two or more operations at the same time can reduce the average access time considerably figure compares parameters of the standard storage medium for the orig inal ibm pc with parameters of a disk made years later to show how much disks changed in years it is interesting to note that not all parameters have improved as much average seek time is seven times better than it was transfer rate is times better while capacity is up by a factor of this pattern input output chap has to do with relatively gradual improvements in the moving parts but much higher bit densities on the recording surfaces parameter ibm kb floppy disk wd hard disk number of cylinders tracks per cylinder sectors per track 281 avg sectors per disk bytes per sector disk capacity kb gb seek time adjacent cylinders mse c mse c seek time average case mse c mse c rotation time mse c mse c motor stop start time mse c se c time to transfer sector mse c use e figure disk parameters for the original ibm pc kb floppy disk and a western digital wd hard disk one thing to be aware of in looking at the specifications of modern hard disks is that the geometry specified and used by the driver software is almost always different from the physical format on old disks the number of sectors per track was the same for all cylinders modem disks are divided into zones with more sectors on the outer zones than the inner ones fig a illustrates a tiny disk with two zones the outer zone has sectors per track the inner one has sec tors per track a real disk such as the wd typically has or more zones with the number of sectors increasing by about per zone as one goes out from the innermost zone to the outermost zone to hide the details of how many sectors each track has most modem disks have a virtual geometry that is presented to the operating system the software is instructed to act as though there are x cylinders y heads and z sectors per track the controller then remaps a request for x y z onto the real cylinder head and sector a possible virtual geometry for the physical disk of fig a is shown in fig b in both cases the disk has sectors only the published arrange ment is different than the real one for pcs the maximum values for these three parameters are often and due to the need to be backward compatible with the limitations of the original ibm pc on this machine and bit fields were used to specify these numbers widi cylinders and sectors numbered starting at and heads num bered starting at with these parameters and bytes per sector the largest possible disk is gb to get around this limit all modern disks now support a sec disks figure a physical geometry of a disk with two zones b a possible virtual geometry for this disk system called logical block addressing in which disk sectors are just numbered consecutively starting at without regard to the disk geometry raid cpu performance has been increasing exponentially over the past decade roughly doubling every months not so with disk performance in the average seek times on minicomputer disks were to msec now seek times are slightly under msec in most technical industries say automobiles or avia tion a factor of to performance improvement in two decades would be major news imagine mpg cars but in the computer industry it is an embar rassment thus the gap between cpu performance and disk performance has be come much larger over time as we have seen parallel processing is being used more and more to speed up cpu performance it has occurred to various people over the years that parallel i o might be a good idea too in their paper patterson et al suggested six specific disk organizations that could be used to improve disk performance reliability or both patterson et al these ideas were quickly adopted by industry and have led to a new class of i o device called a raid patterson et al defined raid as redundant array of inexpensive disks but industry redefined the i to be independent rather than inexpensive maybe so they could charge more since a villain was also needed as in risc versus cisc also due to patterson the bad guy here was the sled single large expensive disk input output chap sec disks the basic idea behind a raid is to install a box full of disks next to the com puter typically a large server replace the disk controller card with a raid con troller copy the data over to the raid and then continue normal operation in other words a raid should look like a sled to the operating system but have better performance and better reliability since scsi disks have good per formance low price and the ability to have up to seven drives on a single con troller for wide scsi it is natural that most raids consist of a raid scsi controller plus a box of scsi disks that appear to the operating system as a single large disk in this way no software changes are required to use the raid a big selling point for many system administrators in addition to appearing like a single disk to the software all raids have the property that the data are distributed over the drives to allow parallel operation several different schemes for doing this were defined by patterson et al and they are now known as raid level through raid level in addition there are a few other minor levels that we will not discuss the term level is something of a misnomer since there is no hierarchy involved there are simply six different organizations possible raid level is illustrated in fig a it consists of viewing the virtual single disk simulated by the raid as being divided up into strips of k sectors each with sectors to k being strip sectors k to as strip and so on for k each strip is a sector for k a strip is two sectors etc the raid level organization writes consecutive strips over the drives in round robin fashion as depicted in fig a for a raid with four disk drives distributing data over multiple drives like this is called striping for ex ample if the software issues a command to read a data block consisting of four consecutive strips starting at a strip boundary the raid controller will break this command up into four separate commands one for each of the four disks and have them operate in parallel thus we have parallel i o without the software knowing about it raid level works best with large requests the bigger the better if a re quest is larger than the number of drives times the strip size some drives will get multiple requests so that when they finish the first request they start the second one it is up to the controller to split the request up and feed the proper commands to the proper disks in the right sequence and then assemble the results in memory correctly performance is excellent and the implementation is straightforward raid level works worst with operating systems that habitually ask for data one sector at a time the results will be correct but there is no parallelism and hence no performance gain another disadvantage of this organization is that the reliability is potentially worse than having a sled if a raid consists of four disks each with a mean time to failure of hours about once every hours a drive will fail and all the data will be completely lost a sled with a mean time to failure of hours would be four times more reliable because no redundancy is present in this design it is not really a true raid the next option raid level shown in fig b is a true raid it duplicates all the disks so there are four primary disks and four backup disks on a write every strip is written twice on a read either copy can be used distribut ing the load over more drives consequently write performance is no better than for a single drive but read performance can be up to twice as good fault toler ance is excellent if a drive crashes the copy is simply used instead recovery consists of simply installing a new drive and copying the entire backup drive to it unlike levels and which work with strips of sectors raid level works on a word basis possibly even a byte basis imagine splitting each byte of the sin gle virtual disk into a pair of bit nibbles then adding a hamming code to each one to form a bit word of which bits and were parity bits further ima gine that the seven drives of fig c were synchronized in terms of arm posi tion and rotational position then it would be possible to write the bit hamming coded word over the seven drives one bit per drive the thinking machines cm computer used this scheme taking bit data words and adding parity bits to form a bit hainming word plus an extra bit for word parity and spread each word over disk drives the total throughput was immense because in one sector time it could write sectors worth of data also losing one drive did not cause problems because loss of a drive amounted to losing bit in each bit word read something the hamming code could hand le on the fly on the down side this scheme requires all the drives to be rotationally syn chronized and it only makes sense with a substantial number of drives even with data drives and parity drives the overhead is it also asks a lot of the controller since it must do a hamming checksum every bit time raid level is a simplified version of raid level it is illustrated in fig d here a single parity bit is computed for each data word and written to a parity drive as in raid level the drives must be exactly synchronized since individual data words are spread over multiple drives at first thought it might appear that a single parity bit gives only error detec tion not error correction for the case of random undetected errors this observa tion is true however for the case of a drive crashing it provides full bit error correction since the position of the bad bit is known if a drive crashes the con troller just pretends that all its bits are if a word has a parity error the bit from the dead drive must have been a so it is corrected although both raid levels and offer very high data rates the number of separate i o requests per second they can handle is no better than for a single drive raid levels and work with strips again not individual words with parity and do not require synchronized drives raid level see fig e is like raid level with a strip for strip parity written onto an extra drive for ex ample if each strip is k bytes long all the strips are exclusive ored together resulting in a parity strip k bytes long if a drive crashes the lost bytes can be recomputed from the parity drive by reading the entire set of drives input output strip chap sec disks this design protects against the loss of a drive but performs pooriy for small updates if one sector is changed it is necessary to read all the drives in order to recalculate the parity which must then be rewritten alternatively it can read the old user data and the old parity data and recompute the new parity from them a strip strip strip strip i strip i strip raid level i strip strip raid even with this optimization a small update requires two reads and two writes as a consequence of the heavy load on the parity drive it may become a bottleneck this bottleneck is eliminated in raid level by distributing the par ity bits uniformly over all the drives round robin fashion as shown in fig f however in the event of a drive crash reconstructing the contents of the failed drive is a complex process b strip i strip strip strip strip istrip strip strip strip strip strip p12 pi 19 strip strip strip strip p4 strip strip strip strip strip strip strip level cd roms in recent years optical as opposed to magnetic disks have become available they have much higher recording densities than conventional magnetic disks optical disks were originally developed for recording television programs but they can be put to more esthetic use as computer storage devices due to their potentially enormous capacity optical disks have been the subject of a great deal of research and have gone through an incredibly rapid evolution first generation optical disks were invented by the dutch electronics conglomerate philips for holding movies they were cm across and marketed under the name laser vision but they did not catch on except in japan in philips together with sony developed the cd compact disc which rapidly replaced the rpm vinyl record for music except among connoisseurs who still prefer vinyl the precise technical details for the cd were published in an official international standard is popularly called the red book due to the color of its cover international standards are issued by the international organization for standardization which is the international counterpart of national standards groups like ansi din etc each one has an is number the point of publishing the disk and drive specifications as an interna tional standard is to allow cds from different music publishers and players from different electronics manufacturers to work together all cds are mm across and mm thick with a mm hole in the middle the audio cd was the first successful mass market digital storage medium they are supposed to last years please check back in for an update on how well the first batch did a cd is prepared in several steps the step consists of using a high power infrared laser to burn micron diameter holes in a coated glass master disk from this master a mold is made with bumps where the laser holes were into this mold molten polycarbonate resin is injected to form a cd with the same pat tern of holes as the glass master then a very thin layer of reflective aluminum is deposited on the polycarbonate topped by a protective lacquer and finally a label the depressions in the polycarbonate substrate are called pits the unburned areas between the pits are called lands figure raid levels through backup and parity drives are shown shaded input output chap when played back a low power laser diode shines infrared light with a wave length of micron on the pits and lands as they stream by the laser is on the polycarbonate side so the pits stick out toward the laser as bumps in the otherwise flat surface because the pits have a height of one quarter the wavelength of the laser light light reflecting off a pit is half a wavelength out of phase with light reflecting off the surrounding surface as a result the two parts interfere destruct ively and return less light to the player photodetector than light bouncing off a land this is how the player tells a pit from a land although it might seem sim pler to use a pit to record a and a land to record a it is more reliable to use a pit land or land pit transition for a and its absence as a so this scheme is used the pits and lands are written in a single continuous spiral starting near the hole and working out a distance of mm toward the edge the spiral makes 188 revolutions around the disk about per mm if unwound it would be km long the spiral is illustrated in fig figure 21 recording structure of a compact disc or cd rom sec disks what are now called cd roms compact disc read only memory to pig gyback on the by then already substantial audio cd market cd roms were to be the same physical size as audio cds mechanically and optically compatible with them and produced using the same polycarbonate injection molding ma chines the consequences of this decision were not only that slow variable speed motors were required but also that the manufacturing cost of a cd rom would be well under one dollar in moderate volume what the yellow book defined was the formatting of the computer data it also improved the error correcting abilities of the system an essential step be cause although music lovers do not mind losing a bit here and there computer lovers tend to be very picky about that the basic format of a cd rom consists of encoding every byte in a bit symbol which is enough to hamming encode an bit byte with bits left over in fact a more powerful encoding system is used the to mapping for reading is done in hardware by table lookup at the next level up a group of consecutive symbols forms a bit frame each frame holds data bits bytes the remaining bits are used for error correction and control of these are the error correction bits in the bit symbols and are carried in the bit symbol payloads so far this scheme is identical for audio cds and cd roms what the yellow book adds is the grouping of frames into a cd rom sector as shown in fig every cd rom sector begins with a byte preamble the first of which are ooffffffffffffffffffffoo hexade cimal to allow the player to recognize the start of a cd rom sector the next bytes contain the sector number needed because seeking on a cd rom with its single data spiral is much more difficult than on a magnetic disk with its uniform concentric tracks to seek the software in the drive calculates approximately where to go moves the head there and then starts hunting around for a preamble to see how good its guess was the last byte of the preamble is the mode each symbol holds data bits and error correction bits symbols make frame of x bits to make the music play at a uniform rate it is necessary for the pits and lands to stream by at a constant linear velocity consequently the rotation rate of the c n a r z j m a e z j c z i a each frame contains o a t z j o a o data bits bytes and cd must be continuously reduced as the reading head moves from the inside of the cd to the outside at the inside the rotation rate is rpm to achieve the preamble frames make sector error correction bits mode desired streaming rate of cm sec at the outside it has to drop to rpm to give the same linear velocity at the head a constant linear velocity drive is quite different than a magnetic disk drive which operates at a constant angular velocity bytes data ecc sector bytes independent of where the head is currently positioned also rpm is a far cry from the to rpm that most magnetic disks whirl at in philips and sony realized the potential for using cds to store com puter data so they published the yellow book defining a precise standard for figure logical data layout on a cd rom the yellow book defines two modes mode uses the layout of fig 22 with a byte preamble data bytes and a byte error correcting code a input output chap crossinterleaved reed solomon code mode combines the data and ecc fields into a byte data field for those applications that do not need or cannot afford the time to perform error correction such as audio and video note that to provide excellent reliability three separate error correcting schemes are used within a symbol within a frame and within a cd rom sector single bit errors are corrected at the lowest level short burst errors are corrected at the frame level and any residual errors are caught at the sector level the price paid for this reliability is that it takes frames of bits bytes to carry a single byte payload an efficiency of only single speed cd rom drives operate at sectors sec which gives a data rate of bytes sec in mode and bytes sec in mode double speed drives are twice as fast and so on up to the highest speed thus a drive can deliver data at a rate of x bytes sec assuming that the drive inter face bus and operating system can all handle this data rate a standard audio cd has room for minutes of music which if used for mode data gives a capa city of bytes this figure is usually reported as mb because mb is bytes 048 bytes not 000 bytes note that even a cd rom drive 915 bytes sec is no match for a fast scsi magnetic disk drive at mb sec even though many cd rom drives use the scsi interface ide cd rom drives also exist when you realize that the seek time is usually several hundred milliseconds it should be clear that cd rom drives are not in the same performance category as magnetic disk drives despite their large capacity in philips struck again with the green book adding graphics and the ability to interleave audio video and data in the same sector a feature essential for multimedia cd roms the last piece of the cd rom puzzle is the fde system to make it possible to use the same cd rom on different computers agreement was needed on cd rom fde systems to get this agreement representatives of many computer companies met at lake tahoe in the high sierras on the california nevada boun dary and devised a file system that they called high sierra it later evolved into an international standard is it has three levels level uses fde names of up to characters optionally followed by an extension of up to characters the ms dos fde naming convention file names may contain only upper case letters digits and the underscore directories may be nested up to eight deep but directory names may not contain extensions level requires all files to be con tiguous which is not a problem on a medium written only once any cd rom conformant to is level can be read using ms dos an apple computer a unix computer or just about any other computer cd rom publishers regard this property as being a big plus is level allows names up to characters and level allows noncon tiguous files the rock ridge extensions whimsically named after the town in the gene wilder film blazing saddles allow very long names for unix uids sec disks gids and symbolic iinks but cd roms not conforming to level will not be readable on all computers cd roms have become extremely popular for publishing games movies encylopedias atlases and reference works of all kinds most commercial software now comes on cd roms their combination of large capacity and low manufac turing cost makes them well suited to innumerable applications cd recordables initially the equipment needed to produce a master cd rom or audio cd for that matter was extremely expensive but as usual in the computer industry nothing stays expensive for long by the mid cd recorders no bigger than a cd player were a common peripheral available in most computer stores these devices were still different from magnetic disks because once written cd roms could not be erased nevertheless they quickly found a niche as a backup medium for large hard disks and also allowed individuals or startup companies to manufac ture their own small run cd roms or make masters for delivery to high volume commercial cd duplication plants these drives are known as cd rs cd recordables physically cd rs start with mm polycarbonate blanks that are like cd roms except that they contain a mm wide groove to guide the lasef for writ ing the groove has a sinusoidal excursion of mm at a frequency of exactly 22 05 khz to provide continuous feedback so the rotation speed can be accurately monitored and adjusted if need be cd rs look like regular cd roms except that they are gold colored on top instead of silver colored the gold color comes from the use of real gold instead of aluminum for the reflective layer unlike silver cds which have physical depressions on them on cd rs the differing reflectivity of pits and lands has to be simulated this is done by adding a layer of dye between the polycarbonate and the reflective gold layer as shown in fig two kinds of dye are used cyanine which is green and pthalocyanine which is a yellowish orange chemists can argue endlessly about which one is better these dyes are similar to those used in photography which explains why eastman kodak and fuji are major manufacturers of blank cd rs in its initial state the dye layer is transparent and lets the laser light pass through and reflect off the gold layer to write the cd r laser is turned up to high power mw when the beam hits a spot of dye it heats up breaking a chemical bond this change to the molecular structure creates a dark spot when read back at mw the photodetector sees a difference between the dark spots where the dye has been hit and transparent areas where it is intact this difference is interpreted as the difference between pits and lands even when read back on a regular cd rom reader or even on an audio cd player no new kind of cd could hold up its head with pride without a colored book so cd r has the orange book published in this document defines cd r input output chap printed label sec disks cd r makes it possible for individuals and companies to easily copy cd roms and audio cds generally in violation of the publisher copyright sever al schemes have been devised to make such piracy harder and to make it difficult protective lacquer reflective gold layer dark spot in the dye layer burned to read a cd rom using anything other than the publisher software one of them involves recording all the file lengths on the cd rom as multigigabyte mm polycarbonate direction dye layer substrate i by laser when writing thwarting any attempts to copy the files to hard disk using standard copying soft ware the true lengths are embedded in the publisher software or hidden pos sibly encrypted on the cd rom in an unexpected place another scheme uses intentionally incorrect eccs in selected sectors in the expectation that cd copy ing software will fix the errors the application software checks the eccs it of motion photodetector lens prism f j infrared l laser j diode self refusing to work if they are correct using nonstandard gaps between the tracks and other physical defects are also possibilities cd rewritables although people are used to other write once media such as paper and photo graphic film there is a demand for a rewritable cd rom one technology now available is cd rw cd rewritable which uses the same size media as cd figure cross section of a cd r disk and laser not to scale a silver cd rom has a similar structure except without the dye layer and with a pitted aluminum layer instead of a gold layer and also a new format cd rom xa which allows cd rs to be written incre mentally a few sectors today a few tomorrow and a few next month a group of consecutive sectors written at once is called a cd rom track one of the first uses of cd r was for the kodak photocd in this system the customer brings a roll of exposed film and his old photocd to the photo processor and gets back the same photocd with the new pictures added after the old ones the new batch which is created by scanning in the negatives is written onto the photocd as a separate cd rom track incremental writing was needed because when this product was introduced the cd r blanks were too expensive to provide a new one for every film roll however incremental writing creates a new problem prior to the orange book all cd roms had a single vtoc volume table of contents at the start that scheme does not work with incremental i e multitrack writes the orange book solution is to give each cd rom track its own vtoc the files listed in the vtoc can include some or all of the files from previous tracks after the cd r is inserted into the drive the operating system searches through all the cd rom tracks to locate the most recent vtoc which gives the current status of the disk by including some but not all of the files from previous tracks in the current vtoc it is possible to give the illusion that files have been deleted tracks can be grouped into sessions leading to multisession cd roms stan dard audio cd players cannot handle multisession cds since they expect a single vtoc at the start some computer applications can handle them though r however instead of cyanine or pthalocyanine dye cr rw uses an alloy of silver indium antimony and tellurium for the recording layer this alloy has two stable states crystalline and amorphous with different reflectivities cd rw drives use lasers with three different powers at high power the laser melts the alloy converting it from the high reflectivity crystalline state to the low reflectivity amorphous state to represent a pit at medium power the alloy melts and reforms in its natural crystalline state to become a land again at low power the state of the material is sensed for reading but no phase transition occurs the reason cd rw has not replaced cd r is that the cd rw blanks are more expensive than the cr r blanks also for applications consisting of back ing up hard disks the fact that once written a cd r cannot be accidentally erased is a big plus dvd the basic cd cd rom format has been around since the technology has improved since then so higher capacity optical disks are now economically feasible and there is great demand for them hollywood would dearly love to eliminate analog video tapes in favor of digital disks since disks have a higher quality are cheaper to manufacture last longer take up less shelf space in video stores and do not have to be rewound the consumer electronics companies are always looking for a new blockbuster product and many computer companies want to add multimedia features to their software this combination of technology and demand by three immensely rich and powerful industries led to dvd originally an acronym for digital video disk input output chap but now officially digital versatile disk dvds use the same general design as cds with mm injection molded polycarbonate disks containing pits and lands that are illuminated by a laser diode and read by a photodetector what is new is the use of smaller pits microns versus microns for cds a tighter spiral 74 microns between tracks versus microns for cds a red laser at 65 microns versus 78 microns for cds together these improvements raise the capacity sevenfold to gb a lx dvd drive operates at mb sec versus kb sec for cds unfortunately the switch to the red lasers used in supermarkets means that dvd players require a second laser or fancy conversion optics to be able to read existing cds and cd roms but with the drop in price of lasers most of them now have both of them so they can read both kinds of media is gb enough maybe using mpeg compression standardized in is a gb dvd disk can hold minutes of full screen full motion video at high resolution x as well as soundtracks in up to eight lan guages and subtitles in more about of all the movies hollywood has ever made are under minutes nevertheless some applications such as multimedia games or reference works may need more and hollywood would like to put mul tiple movies on the same disk so four formats have been defined single sided single layer gb single sided dual layer gb double sided single layer 9 gb double sided dual layer 17 gb why so many formats in a word politics philips and sony wanted single sided dual layer disks for the high capacity version but toshiba and time warner wanted double sided single layer disks philips and sony did not think people would be willing to turn the disks over and time warner did not believe putting two layers on one side could be made to work the compromise all combinations but the market will determine which ones survive the dual layering technology has a reflective layer at the bottom topped with a semireflective layer depending on where the laser is focused it bounces off one layer or the other the lower layer needs slightly larger pits and lands to be read reliably so its capacity is slightly smaller than the upper layer double sided disks are made by taking two mm single sided disks and gluing them together back to back to make the thicknesses of all versions the same a single sided disk consists of a mm disk bonded to a blank substrate or perhaps in the future one consisting of minutes of advertising in the hope sec disks tiiat people will be curious as to what is down there the structure of the double sided dual layer disk is illustrated in fig 6 mm polycarbonate substrate single sided disk 6 mm single sided disk polycarbonate substrate figure 24 a double sided dual layer dvd disk dvd was devised by a consortium of consumer electronics companies seven of them japanese in close cooperation with the major hollywood studios some of which are owned by the japanese electronics companies in the consor tium the computer and telecommunications industries were not invited to the picnic and the resulting focus was on using dvd for movie rental and sales shows for example standard features include real time skipping of dirty scenes to allow parents to turn a film rated into one safe for toddlers six channel sound and support for pan and scan the latter feature allows the dvd player to dynamically decide how to crop the left and right edges off movies whose width height ratio is to fit on current television sets whose aspect ratio is another item the computer industry probably would not have thought of is an intentional incompatibility between disks intended for the united states and disks intended for europe and yet other standards for other continents hollywood de manded this feature because new films are always released first in the united states and then shipped to europe when the videos come out in the united states the idea was to make sure european video stores could not buy videos in the u s too early thereby reducing new movies european theater sales if hollywood had been running the computer industry we would have had inch floppy disks in the united states and 9 cm floppy disks in europe the folks who brought you single double sided dvds and single double layer dvds are at it again the next generation also lacks a single standard due to polit ical bickering by the industry players one of the new devices is blu ray which uses a micron blue laser to pack gb onto a single layer disk and gb onto a double layer disk the other one is hd dvd which uses the same blue laser but has a capacity of only gb single layer and gb double layer this format war has split the movie studios the computer manufacturers input output chap and the software companies as a result of the lack of standardization this gen eration is taking off rather slowly as consumers wait for the dust to settle to see which format will win this stupidity on the part of the industry brings to mind george santayana famous remark those who cannot learn from history are doomed to repeat it disk formatting a hard disk consists of a stack of aluminum alloy or glass platters inch or inch in diameter or even smaller on notebook computers on each platter is deposited a thin magnetizable metal oxide after manufacturing there is no information whatsoever on the disk before the disk can be used each platter must receive a low level format done by software the format consists of a series of concentric tracks each con taining some number of sectors with short gaps between the sectors the format of a sector is shown in fig preamble data ec c figure a disk sector the preamble starts with a certain bit pattern that allows the hardware to recognize the start of the sector it also contains the cylinder and sector numbers and some other information the size of the data portion is determined by the low level formatting program most disks use byte sectors the ecc field contains redundant information that can be used to recover from read errors the size and content of this field varies from manufacturer to manufacturer depending on how much disk space the designer is willing to give up for higher reliability and how complex an ecc code the controller can handle a byte ecc field is not unusual furthermore all hard disks have some number of spare sectors allo cated to be used to replace sectors with a manufacturing defect the position of sector on each track is offset from the previous track when the low level format is laid down this offset called cylinder skew is done to improve performance the idea is to allow the disk to read multiple tracks in one continuous operation without losing data the nature of the problem can be seen by looking at fig 19 a suppose that a request needs sectors starting at sec tor on the innermost track reading the first sectors takes one disk rotation but a seek is needed to move outward one track to get the sector by the time the head has moved one track sector has rotated past the head so an entire rota tion is needed until it comes by again that problem is eliminated by offsetting the sectors as shown in fig sec disks figure an illustration of cylinder skew the amount of cylinder skew depends on the drive geometry for example a 000 rpm drive rotates in 6 msec if a track contains sectors a new sector passes under the head every usee if the track to track seek time is psec sectors will pass by during the seek so the cylinder skew should be sectors rather than the three sectors shown in fig 26 it is worth mentioning that switching between heads also takes a finite time so there is head skew as well as cylinder skew but head skew is not very large as a result of the low level formatting disk capacity is reduced depending on the sizes of the preamble intersector gap and ecc as well as the number of spare sectors reserved often the formatted capacity is lower than the unfor matted capacity the spare sectors do not count toward the formatted capacity so all disks of a given type have exactly the same capacity when shipped indepen dent of how many bad sectors they actually have if the number of bad sectors exceeds the number of spares the drive will be rejected and not shipped there is considerable confusion about disk capacity because some manufact urers advertised the unformatted capacity to make their drives look larger than they really are for example consider a drive whose unformatted capacity is x bytes this might be sold as a gb disk however after formatting perhaps only x bytes are available for data to add to the confusion the input output chap operating system will probably report this capacity as gb not gb be cause software considers a memory of gb to be bytes not 000 000 000 bytes to make things worse in the world of data communications gbps means 000 000 000 bits sec because the prefix giga really does mean a kilometer is meters not meters after all only with memory and disk sizes do kilo mega giga and tera mean and respectively formatting also affects performance if a 000 rpm disk has sectors per track of bytes each it takes 6 msec to read the bytes on a track for a data rate of 600 000 bytes sec or 24 mb sec it is not possible to go faster than this no matter what kind of interface is present even if it a scsi inter face at mb sec or mb sec actually reading continuously at this rate requires a large buffer in the con troller consider for example a controller with a one sector buffer that has been given a command to read two consecutive sectors after reading the first sector from the disk and doing the ecc calculation the data must be transferred to main memory while this transfer is taking place the next sector will fly by the head when the copy to memory is complete the controller will have to wait almost an entire rotation time for the second sector to come around again this problem can be eliminated by numbering the sectors in an interleaved fashion when formatting the disk in fig a we see the usual numbering pattern ignoring cylinder skew here in fig 27 b we see single interleav ing which gives the controller some breathing space between consecutive sectors in order to copy the buffer to main memory o m figure 27 a no interleaving b single interleaving c double interleaving if the copying process is very slow the double interleaving of fig c may be needed if the controller has a buffer of only one sector it does not matter whether the copying from the buffer to main memory is done by the controller the main cpu or a dma chip it still takes some time to avoid the need for inter leaving the controller should be able to buffer an entire track many modern con trollers can do this after low level formatting is completed the disk is partitioned logically each partition is like a separate disk partitions are needed to allow multiple oper sec disks ating systems to coexist also in some cases a partition can be used for swap ping on the pentium and most other computers sector contains the master boot record which contains some boot code plus the partition table at the end the partition table gives the starting sector and size of each partition on the pen tium the partition table has room for four partitions if all of them are for win dows they will be called c d e and f and treated as separate drives if three of them are for windows and one is for unix then windows will call its parti tions c d and e the first cd rom will then be f to be able to boot from the hard disk one partition must be marked as active in the partition table the final step in preparing a disk for use is to perform a high level format of each partition separately this operation lays down a boot block the free stor age administration free list or bitmap root directory and an empty file system it also puts a code in the partition table entry telling which file system is used in the partition because many operating systems support multiple incompatible file systems for historical reasons at this point the system can be booted when the power is turned on the bios runs initially and then reads in the master boot record and jumps to it this boot program then checks to see which partition is active then it reads in the boot sector from that partition and runs it the boot sector contains a small program that general loads a larger bootstrap loader that searches the file system to find the operating system kernel that pro gram is loaded into memory and executed disk arm scheduling algorithms in this section we will look at some issues related to disk drivers in general first consider how long it takes to read or write a disk block the time required is determined by three factors seek time the time to move the arm to the proper cylinder rotational delay the time for the proper sector to rotate under the head actual data transfer time for most disks the seek time dominates the other two times so reducing the mean seek time can improve system performance substantially if the disk driver accepts requests one at a time and carries them out in that order that is first come first served fcfs little can be done to optimize seek time however another strategy is possible when the disk is heavily loaded it is likely that while the arm is seeking on behalf of one request other disk re quests may be generated by other processes many disk drivers maintain a table indexed by cylinder number with all the pending requests for each cylinder chained together in a linked list headed by the table entries given this kind of data structure we can improve upon the first come first served scheduling algorithm to see how consider an imaginary disk with 378 input output chap sec disks cylinders a request comes in to read a block on cylinder while the seek to cylinder is in progress new requests come in for cylinders 34 9 and in that order they are entered into the table of pending requests with a sepa rate linked list for each cylinder the requests are shown in fig same direction until there are no more outstanding requests in that direction then they switch directions this algorithm known both in the disk world and the elevator world as the elevator algorithm requires the software to maintain bit the current direction bit up or down when a request finishes the disk or initial position pendin g requests elevator driver checks the bit if it is up the arm or cabin is moved to the next highest pending request if no requests are pending at higher positions the direc tion bit is reversed when the bit is set to down the move is to the next lowest requested position if any cylinde r sequenc e of seek figure shortest seek first ssf disk scheduling algorithm when the current request for cylinder is finished the disk driver has a choice of which request to handle next using fcfs it would go next to cylinder then to and so on this algorithm would require arm motions of 35 and respectively for a total of cylinders alternatively it could always handle the closest request next to minimize seek time given the requests of fig the sequence is 9 34 and 36 shown as the jagged line at the bottom of fig with this sequence the arm motions are 7 and for a total of cylinders this algorithm shortest seek first ssf cuts the total arm motion almost in half compared to fcfs unfortunately ssf has a problem suppose more requests keep coming in while the requests of fig are being processed for example if after going to cylinder a new request for cylinder is present that request will have priority over cylinder if a request for cylinder 13 then comes in the arm will next go to 13 instead of with a heavily loaded disk the arm will tend to stay in the mid dle of the disk most of the time so requests at either extreme will have to wait until a statisdcal fluctuation in the load causes there to be no requests near the middle requests far from the middle may get poor service the goals of minimal response time and fairness are in conflict here tall buildings also have to deal with this trade off the problem of scheduling an elevator in a tall building is similar to that of scheduling a disk arm requests come in continuously calling the elevator to floors cylinders at random the computer running the elevator could easily keep track of the sequence in which customers pushed the call button and service them using fcfs or ssf however most elevators use a different algorithm in order to reconcile the mutually conflicting goals of efficiency and fairness they keep moving in the figure shows the elevator algorithm using the same seven requests as fig 28 assuming the direction bit was initially up the order in which the cyl inders are serviced is 34 36 9 and which yields arm motions of 27 and for a total of cylinders in this case the elevator algorithm is slightly better than ssf although it is usually worse one nice property that the elevator algorithm has is that given any collection of requests the upper bound on the total motion is fixed it is just twice the number of cylinders initial position figure the elevator algorithm for scheduling disk requests a slight modification of this algorithm that has a smaller variance in response times teory is to always scan in the same direction when the highest numbered cylinder with a pending request has been serviced the arm goes to the lowest numbered cylinder with a pending request and then continues moving in an upward direction in effect the lowest numbered cylinder is thought of as being just above the highest numbered cylinder some disk controllers provide a way for the software to inspect the current sector number under the head with such a controller another optimization is pos sible if two or more requests for the same cylinder are pending the driver can issue a request for the sector that will pass under the head next note that when multiple tracks are present in a cylinder consecutive requests can be for different tracks with no penalty the controller can select any of its heads almost instan taneously head selection involves neither arm motion nor rotational delay input output chap if the disk has the property that seek time is much faster than the rotational delay then a different optimization should be used pending requests should be sorted by sector number and as soon as the next sector is about to pass under the head the arm should be zipped over to the right track to read or write it with a modern hard disk the seek and rotational delays so dominate per formance that reading one or two sectors at a time is very inefficient for this rea son many disk controllers always read and cache multiple sectors even when only one is requested typically any request to read a sector will cause that sector and much or all the rest of the current track to be read depending upon how much space is available in the controller cache memory the disk described in fig has a mb cache for example the use of the cache is determined dynami cally by the controller in its simplest mode the cache is divided into two sec tions one for reads and one for writes if a subsequent read can be satisfied out of the controller cache it can return the requested data immediately it is worth noting that the disk controller cache is completely independent of the operating system cache the controller cache usually holds blocks that have not actually been requested but which were convenient the read because they just happened to pass under the head as a side effect of some other read in contrast any cache maintained by the operating system will consist of blocks that were explicitly read and which the operating system thinks might be needed again in the near future e g a disk block holding a directory block when several drives are present on the same controller the operating system should maintain a pending request table for each drive separately whenever any drive is idle a seek should be issued to move its arm to the cylinder where it will be needed next assuming the controller allows overlapped seeks when the cur rent transfer finishes a check can be made to see if any drives are positioned on the correct cylinder if one or more are the next transfer can be started on a drive that is already on the right cylinder if none of the arms is in the right place the driver should issue a new seek on the drive that just completed a transfer and wait until the next interrupt to see which arm gets to its destination first it is important to realize that all of the above disk scheduling algorithms tacitly assume that the real disk geometry is the same as the virtual geometry if it is not then scheduling disk requests makes no sense because the operating system cannot really tell whether cylinder or cylinder is closer to cylinder on the other hand if the disk controller can aceept multiple outstanding requests it can use these scheduling algorithms internally in that case the algorithms are still valid but one level down inside the controller error handling disk manufacturers are constantly pushing the limits of the technology by increasing linear bit densities a track midway out on a inch disk has a cir cumference of about mm if the track holds sectors of bytes the sec disks linear recording density may be about bits mm taking into account the fact that some space is lost to preambles eccs and intersector gaps recording bits mm requires an extremely uniform substrate and a very fine oxide coating unfortunately it is not possible to manufacture a disk to such specifications with out defects as soon as manufacturing technology has improved to the point where it is possible to operate flawlessly at such densities disk designers will go to higher densities to increase the capacity doing so will probably reintroduce defects manufacturing defects introduce bad sectors that is sectors that do not cor rectly read back the value just written to them if the defect is very small say only a few bits it is possible to use the bad sector and just let the ecc correct the errors every time if the defect is bigger the error cannot be masked there are two general approaches to bad blocks deal with them in the con troller or deal with them in the operating system in the former approach before the disk is shipped from the factory it is tested and a list of bad sectors is written onto the disk for each bad sector one of the spares is substituted for it there are two ways to do this substitution in fig a we see a single disk track with data sectors and two spares sector 7 is defective what the controller can do is remap one of the spares as sector 7 as shown in fig b the other way is to shift all the sectors up one as shown in fig c in both cases the controller has to know which sector is which it can keep trade of this information through internal tables one per track or by rewriting the preambles to give the remapped sector numbers if the preambles are rewritten the method of fig c is more work because preambles must be rewritten but ulti mately gives better performance because an entire track can still be read in one rotation a b c figure a a disk track with a bad sector b substituting a spare for the bad sector c shifting all the sectors to bypass the bad one errors can also develop during normal operation after the drive has been installed the first line of defense upon getting an error that the ecc cannot hand le is to just try the read again some read errors are transient that is are caused by input output chap specks of dust under the head and will go away on a second attempt if the con troller notices that it is getting repeated errors on a certain sector it can switch to a spare before the sector has died completely in this way no data are lost and the operating system and user do not even notice the problem usually the method of fig b has to be used since the other sectors might now contain data using the method of fig c would require not only rewriting the preambles but copying all the data as well earlier we said there were two general approaches to handling errors handle them in the controller or in the operating system if the controller does not have the capability to transparently remap sectors as we have discussed the operating system must do the same thing in software this means that it must first acquire a list of bad sectors either by reading them from the disk or simply testing the en tire disk itself once it knows which sectors are bad it can build remapping tables if the operating system wants to use the approach of fig c it must shift the data in sectors 7 through 29 up one sector if the operating system is handling the remapping it must make sure that bad sectors do not occur in any files and also do not occur in the free list or bitmap one way to do this is to create a secret fde consisting of all the bad sectors if this fde is not entered into the fde system users will not accidentally read it or worse yet free it however there is still another problem backups if the disk is backed up fde by fde it is important that the backup utility not try to copy the bad block fde to prevent this the operating system has to hide the bad block fde so well that even a backup utility cannot find it if the disk is backed up sector by sector rather than file by file it will be difficult if not impossible to prevent read errors during backup the only hope is that the backup program has enough smarts to give up after failed reads and continue with the next sector bad sectors are not the only source of errors seek errors caused by mechani cal problems in the arm also occur the controller keeps track of the arm position internally to perform a seek it issues a series of pulses to the arm motor one pulse per cylinder to move the arm to the new cylinder when the arm gets to its destination the controller reads the actual cylinder number from the preamble of the next sector if the arm is in the wrong place a seek error has occurred most hard disk controllers correct seek errors automatically but most floppy controllers including the pentium just set an error bit and leave the rest to the driver the driver handles this error by issuing a recalibrate command to move the arm as far out as it will go and reset the controller internal idea of the current cylinder to usually this solves the problem if it does not the drive must be repaired as we have seen the controller is really a specialized little computer com plete with software variables buffers and occasionally bugs sometimes an unusual sequence of events such as an interrupt on one drive occurring simultan eously with a recalibrate command for another drive will trigger a bug and cause sec 4 disks the controller to go into a loop or lose track of what it was doing controller de signers usually plan for the worst and provide a pin on the chip which when asserted forces the controller to forget whatever it was doing and reset itself if all else fails the disk driver can set a bit to invoke this signal and reset the controller if that does not help all the driver can do is print a message and give up recalibrating a disk makes a funny noise but otherwise normally is not dis turbing however there is one situation where recalibration is a serious problem systems with real time constraints when a video is being played off a hard disk or files from a hard disk are being burned onto a cd rom it is essential that the bits arrive from the hard disk at a uniform rate under these circumstances recali brations insert gaps into the bit stream and are therefore unacceptable special drives called av disks audio visual disks which never recalibrate are avail able for such applications 4 stable storage as we have seen disks sometimes make errors good sectors can suddenly become bad sectors whole drives can die unexpectedly rajds protect against a few sectors going bad or even a drive falling out however they do not protect against write errors laying down bad data in the first place they also do not pro tect against crashes during writes corrupting the original data without replacing them by newer data for some applications it is essential that data never be lost or corrupted even in the face of disk and cpu errors ideally a disk should simply work all the time with no errors unfortunately that is not achievable what is achievable is a disk subsystem that has the following property when a write is issued to it the disk ei ther correctly writes the data or it does nothing leaving the existing data intact such a system is called stable storage and is implemented in software lampson and sturgis the goal is to keep the disk consistent at all costs below we will describe a slight variant of the original idea before describing the algorithm it is important to have a clear model of the possible errors the model assumes that when a disk writes a block one or more sectors either the write is correct or it is incorrect and this error can be detected on a subsequent read by examining the values of the ecc fields in principle guaranteed error detection is never possible because with a say byte ecc field guarding a byte sector there are 4 9 6 data values and only 4 4 ecc values thus if a block is garbled during writing but the ecc is not there are bil lions upon billions of incorrect combinations that yield the same ecc if any of them occur the error will not be detected on the whole the probability of ran dom data having the proper 16 byte ecc is about 4 4 which is small enough that we will call it zero even though it is really not the model also assumes that a correctly written sector can spontaneously go bad and become unreadable however the assumption is that such events are so iwut output chap rare that having the same sector go bad on a second independent drive during a reasonable time interval e g day is small enough to ignore the model also assumes the cpu can fail in which case it just stops any sec 4 disks in the presence of cpu crashes during stable writes it depends on precisely when the crash occurs there are five possibilities as depicted in fig ecc disk write in progress at the moment of failure also stops leading to incorrect data in one sector and an incorrect ecc that can later be detected under all these con ditions stable storage can be made reliable in the sense of writes either working correctly or leaving the old data in place of course it does not protect disk e r r o disk disk disk disk against physical disasters such as an earthquake happening and the computer fal ling meters into a fissure and landing in a pool of boiling magma it is tough to recover from this condition in software old old t old old t new f stable storage uses a pair of identical disks with the corresponding blocks working together to form one error free block in the absence of errors the cor responding blocks on both drives are the same either one can be read to get the crash a b crash crash c f crash crash same result to achieve this goal the following three operations are defined stable writes a stable write consists of first writing the block on drive then reading it back to verify that it was written correctly if it was not written correctly the write and reread are done again up to n times until they work after n consecutive failures the block is remapped onto a spare and the operation repeated until it succeeds no matter how many spares have to be tried after the write to drive has succeeded the corresponding block on drive is written and reread repeatedly if need be until it too finally succeeds in the absence of cpu crashes when a stable write completes the block has correctly been written onto both drives and verified on both of them stable reads a stable read first reads the block from drive if this yields an incorrect ecc the read is tried again up to n times if all of these give bad eccs the corresponding block is read from drive given the fact that a successful stable write leaves two good cop ies of the block behind and our assumption that the probability of the same block spontaneously going bad on both drives in a reasonable time interval is negligible a stable read always succeeds crash recovery after a crash a recovery program scans both disks comparing corresponding blocks if a pair of blocks are both good and the same nothing is done if one of them has an ecc error the bad block is overwritten with the corresponding good block if a pair of blocks are both good but different the block from drive is writ ten onto drive in the absence of cpljg ashes this scheme always works because stable writes always write two valid copies of every block and spontaneous errors are as sumed never to occur on both corresponding blocks at the same time what about figure analysis of the influence of crashes on stable writes in fig a the cpu crash happens before either copy of the block is writ ten during recovery neither will be changed and the old value will continue to exist which is allowed in fig b the cpu crashes during the write to drive destroying the contents of the block however the recovery program detects this error and re stores the block on drive from drive thus the effect of the crash is wiped out and the old state is fully restored in fig c the cpu crash happens after drive is written but before drive is written the point of no return has been passed here the recovery pro gram copies the block from drive to drive the write succeeds fig d is like fig b during recovery the good block overwrites the bad block again the final value of both blocks is the new one finally in fig e the recovery program sees that both blocks are the same so neither is changed and the write succeeds here too various optimizations and improvements are possible to this scheme for star ters comparing all the blocks pairwise after a crash is doable but expensive a huge improvement is to keep track of which block was being written during a stable write so that only one block has to be checked during recovery some com puters have a small amount of nonvolatile ram which is a special cmos mem ory powered by a lithium battery such batteries last for years possibly even the whole life of the computer unlike main memory which is lost after a crash non volatile ram is not lost after a crash the time of day is normally kept here and incremented by a special circuit which is why computers still know what time it is even after having been unplugged suppose that a few bytes of nonvolatile ram are available for operating sys tem purposes the stable write can put the number of the block it is about to up date in nonvolatile ram before starting the write after successfully completing ihpput output chap the stable write the block number in nonvolatile ram is overwritten with an invalid block number for example under these conditions after a crash the recovery program can check the nonvolatile ram to see if a stable write hap pened to be in progress during the crash and if so which block was being written when the crashed happened the two copies of the block can then be checked for correctness and consistency if nonvolatile ram is not available it can be simulated as follows at the start of a stable write a fixed disk block on drive is overwritten with the number of the block to be stably written this block is then read back to verify it after getting it correct the corresponding block on drive is written and verified when the stable write completes correctly both blocks are overwritten with an invalid block number and verified again here after a crash it is easy to determine wheth er or not a stable write was in progress during the crash of course this technique requires eight extra disk operations to write a stable block so it should be used exceedingly sparingly one last point is worth making we assumed that only one spontaneous decay of a good block to a bad block happens per block pair per day if enough days go by the other one might go bad too therefore once a day a complete scan of both disks must be done repairing any damage that way every morning both disks are always identical even if both blocks in a pair go bad within a period of a few days all errors are repaired correctly clocks clocks also called timers are essential to the operation of any multipro grammed system for a variety of reasons they maintain the time of day and pre vent one process from monopolizing the cpu among other things the clock software can take the form of a device driver even though a clock is neither a block device like a disk nor a character device like a mouse our examination of clocks will follow the same pattern as in the previous section first a look at clock hardware and then a look at the clock software clock hardwar e two types of clocks are commonly used in computers and both are quite dif ferent from the clocks and watches used by people the simpler clocks are tied to the or volt power line and cause an interrupt on every voltage cycle at or hz these clocks used to dominate but are rare nowadays the other kind of clock is built out of three components a crystal oscillator a counter and a holding register as shown in fig when a piece of quartz crystal is properly cut and mounted under tension it can be made to generate a periodic signal of very great accuracy typically in the range of several hundred sec clocks megahertz depending on the crystal chosen using electronics this base signal can be multiplied by a small integer to get frequencies up to mhz or even more at least one such circuit is usually found in any computer providing a syn chromzmg signal to the computer various circuits this signal is fed into the counter to make it count down to zero when the counter gets to zero it causes a cpu interrupt crystal oscillator idi j counter is decremented at each pulse holding register is used to load the counter figure a programmable clock programmable clocks typically have several modes of operation in one shot mode when the clock is started it copies the value of the holding register into the counter and then decrements the counter at each pulse from the crystal when the counter gets to zero it causes an interrupt and stops until it is explicitly started again by the software in square wave mode after getting to zero and causing the interrupt the holding register is automatically copied into the counter and the whole process is repeated again indefinitely these periodic interrupts are called clock ticks the advantage of the programmable clock is that its interrupt frequency can be controlled by software if a mhz crystal is used then the counter is pulsed every nsec with unsigned bit registers interrupts can be programmed to occur at intervals from nsec to 6 sec programmable clock chips usually con tain two or three independently programmable clocks and have many other options as well e g counting up instead of down interrupts disabled and more to prevent the current time from being lost when the computer power is turned off most computers have a battery powered backup clock implemented with the kind of low power circuitry used in digital watches the battery clock can be read at startup if the backup clock is not present the software may ask the user for the current date and time there is also a standard way for a networked system to get the current time from a remote host in any case the time is then translated into the number of clock ticks since a m utc universal coordi nated time formerly known as greenwich mean time on jan 1970 as unix does or since some other benchmark moment the origin of time for win dows is jan 1980 at every clock tick the real time is incremented by one input output chap count usually utility programs are provided to manually set the system clock and sec clocks the backup clock and to synchronize the two clocks clock software 6 4 bits tim e of da y in ticks h bits tim e o f da y x numbe r of ticks bits counte r in ticks ail the clock hardware does is generate interrupts at known intervals every thing else involving time must be done by the software the clock driver the ex act duties of the clock driver vary among operating systems but usually include most of the following a in second in current secon d syste m boot tim e i n second c maintaining the time of day 2 preventing processes from running longer than they are allowed to accounting for cpu usage 4 handling the alarm system call made by user processes providing watchdog timers for parts of the system itself 6 doing profiling monitoring and statistics gathering the first clock function maintaining the time of day also called the real time is not difficult it just requires incrementing a counter at each clock tick as mentioned before the only thing to watch out for is the number of bits in the time of day counter with a clock rate of hz a bit counter will overflow in just over 2 years clearly the system cannot store the real time as the number of ticks since jan 1970 in bits three approaches can be taken to solve this problem the first way is to use a bit counter although doing so makes maintaining the counter more expensive since it has to be done many times a second the second way is to maintain the time of day in seconds rather than in ticks using a subsidiary counter to count ticks until a whole second has been accumulated because 2 3 2 seconds is more than years this method will work until the twenty second century the third approach is to count in ticks but to do that relative to the time the system was booted rather than relative to a fixed external moment when the backup clock is read qj the user types in the real time the intern boot time is cal culated from the current time of day value and stored in memory in any con venient form later when the time of day is requested the stored time of day is added to the counter to get the current time of day all three approaches are shown in fig 33 the second clock function is preventing processes from running too long whenever a process is started the scheduler initializes a counter to the value of that process quantum in clock ticks at every clock interrupt the clock driver decrements the quantum counter by when it gets to zero the clock driver calls the scheduler to set up another process figure 33 three ways to maintain the time of day the third clock function is doing cpu accounting the most accurate way to do it is to start a second timer distinct from the main system timer whenever a process is started when that process is stopped the timer can be read out to tell how long the process has run to do things right the second timer should be saved when an interrupt occurs and restored afterward a less accurate but simpler way to do accounting is to maintain a pointer to the process table entry for the currently running process in a global variable at every clock tick a field in the current process entry is incremented in this way every clock tick is charged to the process running at the time of the tick a minor problem with this strategy is that if many interrupts occur during a process run it is still charged for a full tick even though it did not get much work done properly accounting for the cpu during interrupts is too expensive and is rarely done in many systems a process can request that the operating system give it a warning after a certain interval the warning is usually a signal interrupt mes sage or something similar one application requiring such warnings is network ing in which a packet not acknowledged within a certain time interval must be retransmitted another application is computer aided instruction where a student not providing a response within a certain time is told the answer if the clock driver had enough clocks it could set a separate clock for each re quest this not being the case it must simulate multiple virtual clocks with a sin gle physical clock one way is to maintain a table in which the signal time for all pending timers is kept as well as a variable giving the time of the next one whenever the time of day is updated the driver checks to see if the closest signal has occurred if it has the table is searched for the next one to occur if many signals are expected it is more efficient to simulate multiple clocks by chaining all the pending clock requests together sorted on time in a linked list as shown in fig 34 each entry on the list tells how many clock ticks following the previous one to wait before causing a signal in this example signals are pend ing for 4215 and input output chap current time next signal figure 34 simulating multiple timers with a single clock in fig 34 the next interrupt occurs in 3 ticks on each tick next signal is decremented when it gets to the signal corresponding to the first item on the list is caused and that item is removed from the list then next signal is set to the value in the entry now at the head of the list in this example 4 note that during a clock interrupt the clock driver has several things to do increment the real time decrement the quantum and check for do cpu account ing and decrement the alarm counter however each of these operations has been carefully arranged to be very fast because they have to be repeated many times a second parts of the operating system also need to set timers these are called watch dog timers for example floppy disks do not rotate when not in use to avoid wear and tear on the medium and disk head when data are needed from a floppy disk the motor must first be started only when the floppy disk is rotating at full speed can i o begin when a process attempts to read from an idle floppy disk the floppy disk driver starts the motor and then sets a watchdog timer to cause an interrupt after a sufficiently long time interval because there is no up to speed in terrupt from the floppy disk itself the mechanism used by the clock driver to handle watchdog timers is the same as for user signals the only difference is that when a timer goes off instead of causing a signal the clock driver calls a procedure supplied by the caller the procedure is part of the caller code the called procedure can do whatever is necessary even causing an interrupt although within the kernel interrupts are often inconvenient and signals do not exist that is why the watchdog mechanism is provided it is worth nothing that the watchdog mechanism works only when the clock driver and the procedure to be called are in the same address space the last thing in our list is profiling some operating systems provide a me chanism by which a user program can have the system build up a histogram of its program counter so it can see where it is spending its time when profiling is a possibility at every tick the driver checks to see if the current process is being profiled and if so computes the bin number a range of addresses corresponding to the current program counter it then jmfements that bin by one this mechan ism can also be used to profile the system itself sec clocks 3 soft timers most computers have a second programmable clock that can be set to cause timer interrupts at whatever rate a program needs this timer is in addition to the main system timer whose functions were described above as long as the inter rupt frequency is low there is no problem using this second timer for application specific purposes the trouble arrives when the frequency of the application spe cific timer is very high below we will briefly describe a software based timer scheme that works well under many circumstances even at fairly high frequen cies the idea is due to aron and druschel for more details please see their paper generally there are two ways to manage i o interrupts and polling inter rupts have low latency that is they happen immediately after the event itself with little or no delay on the other hand with modern cpus interrupts have a sub stantial overhead due to the need for context switching and their influence on the pipeline tlb and cache the alternative to interrupts is to have the application poll for the event expected itself doing this avoids interrupts but there may be substantial latency because an event may happen directly after a poll in which case it waits almost a whole polling interval on the average the latency is half the polling interval for certainapplications neither the overhead of interrupts nor the latency of polling is acceptable consider for example a high performance network such as gigabit ethernet this network is capable of accepting or delivering a full size packet every p sec to run at optimal performance on output one packet should be sent every p sec one way to achieve this rate is to have the completion of a packet transmis sion cause an interrupt or to set the second timer to interrupt every 12 p sec the problem is that this interrupt has been measured to take 4 45 ixsec on a mhz pentium ii aron and druschel this overhead is barely better than that of computers in the on most minicomputers for example an interrupt took four bus cycles to stack the program counter and psw and to load a new program counter and psw nowadays dealing with the pipeline mmu tlb and cache adds a great deal to the overhead these effects are likely to get worse rather than better in time thus canceling out faster clock rates soft timers avoid interrupts instead whenever the kernel is running for some other reason just before it returns to user mode it checks the real time clock to see if a soft timer has expired if the timer has expired the scheduled event e g packet transmission or checking for an incoming packet is performed with no need to switch into kernel mode since the system is already there after the work has been performed the soft timer is reset to go off again all that has to be done is copy the current clock value to the timer and add the timeout interval to it soft timers stand or fall with the rate at which kernel entries are made for other reasons these reasons include input output chap system calls 2 tlb misses 3 page faults 4 i o interrupts the cpu going idle to see how often these events happen aron and druschel made measurements with several cpu loads including a fully loaded web server a web server with a compute bound background job playing real time audio from the internet and recompiling the unix kernel the average entry rate into the kernel varied from 2 psec to psec with about half of these entries being system calls thus to a first order approximation having a soft timer go off every 12 psec is doable albeit with an occasional missed deadline for applications like sending packets or polling for incoming packets being psec late from time to time is better than having interrupts eat up 35 of the cpu of course there will be periods when there are no system calls tlb misses or page faults in which case no soft timers will go off to put an upper bound on these intervals the second hardware timer can be set to go off say every msec if the application can live with only packets sec for occasional intervals then the combination of soft timers and a low frequency hardware timer may be better than either pure interrupt driven i o or pure polling 6 user interfaces keyboard mouse monitor every general purpose computer has a keyboard and monitor and usually a mouse to allow people to interact with it although the keyboard and monitor are technically separate devices they work closely together on mainframes there are frequently many remote users each with a device containing a keyboard and an attached display as a unit these devices have historically been called termi nals people frequently still use that term even when discussing personal com puter keyboards and monitors mostly for lack of a better term 6 1 input software user input comes primarily from the keyboard and mouse so let us look at those on a personal computer the keyboard contains an embedded microproc essor which usually communicates through a specialized serial port with a con troller chip on the parentboard although increasingly keyboards are connected to a usb port an interrupt is generated whenever a key is struck and a second one is generated whenever a key is released at each of these keyboard interrupts the sec 6 user interfaces keyboard mouse monitor keyboard driver extracts the information about what happens from the i o port as sociated with the keyboard everything else happens in software and is pretty much independent of the hardware most of the rest of this section can be best understood when thinking of typing commands to a shell window command line interface this is how programmers commonly work we will discuss graphical interfaces below keyboard software the number in the i o port is the key number called the scan code not the ascii code keyboards have fewer than keys so only 7 bits are needed to represent the key number the eighth bit is set to on a key press and to 1 on a key release it is up to the driver to keep track of the status of each key up or down when the a key is struck for example the scan code is put in an i o reg ister it is up to the driver to determine whether it is lower case upper case ctrl a alt a ctrl alt a or some other combination since the driver can tell which keys have been struck but not yet released e g shift it has enough information to do the job for example the key sequence res s shift depres s a releas e a releas e shif t indicates an upper case a however the key sequence res s shift depres s a releas e shift releas e a also indicates an upper case a although this keyboard interface puts the full bur den on the software it is extremely flexible for example user programs may be interested in whether a digit just typed came from the top row of keys or the numeric key pad on the side in principle the driver can provide this information two possible philosophies can be adopted for the driver in the first one the driver job is just to accept input and pass it upward unmodified a program reading from the keyboard gets a raw sequence of ascii codes giving user pro grams the scan codes is too primitive as well as being highly keyboard depen dent this philosophy is well suited to the needs of sophisticated screen editors such as emacs which allow the user to bind an arbitrary action to any character or se quence of characters it does however mean that if the user types dste instead of date and then corrects the error by typing three backspaces and ate followed by a carriage return the user program will be given all ascii codes typed as fol lows dste atec r not all programs wa fnuch detail often they just want the corrected input not the exact sequence othow it was produced this observation leads to input output chaf the second philosophy the driver handles all the intraline edidng and just delivers corrected lines to the user programs the first philosophy is character oriented the second one is line oriented originally they were referred to as raw mode and cooked mode respectively the posix standard uses the less picturesque term canonical mode to describe line oriented mode noncanonical mode is equiva lent to raw mode although many details of the behavior can be changed posix compatible systems provide several library functions that support select ing either mode and changing many parameters if the keyboard is in canonical cooked mode characters must be stored until an enure line has been accumulated because the user may subsequently decide to erase part of it even if the keyboard is in raw mode the program may not yet have requested input so the characters must be buffered to allow type ahead ei ther a dedicated buffer can be used or buffers can be allocated from a pool the former puts a fixed limit on type ahead the latter does not this issue arises most acutely when the user is typing to a shell window command line window in win dows and has just issued a command such as a compilation that has not yet completed subsequent characters typed have to be buffered because the shell is not ready to read them system designers who do not permit users to type far ahead ought to be tarred and feathered or worse yet be forced to use their own system although the keyboard and monitor are logically separate devices many users have grown accustomed to seeing the characters they have just typed appear on the screen this process is called echoing echoing is complicated by the fact that a program may be writing to the screen while the user is typing again think about typing to a shell window at the very least the keyboard driver has to figure out where to put the new input without it being overwritten by program output echoing also gets complicated when more than characters have to be dis played in a window with character lines or some other number depending on the application wrapping around to the next line may be appropriate some drivers just truncate lines to characters by throwing away all characters beyond column another problem is tab handling it is usually up to the driver to compute where the cursor is currently located taking into account both output from pro grams and output from echoing and compute the proper number of spaces to be echoed now we come to the problem of device equivalence logically at the end of a line of text one wants a carriage return to move the cursor back to column 1 and a linefeed to advance to the next line requiring users to type both at the end of each line would not sell well it is up to the device driver to convert whatever comes in to the format used by the operating system in unix the enter key is converted to a line feed for internal storage in windows it is converted to a car riage return followed by a line feed sec 6 user interfaces keyboard mouse monitor if the standard form is just to store a linefeed the unix convention then car riage returns created by the enter key should be turned into linefeeds if the in ternal format is to store both the windows convention then the driver should generate a linefeed when it gets a carriage return and a carriage return when it gets a linefeed no matter what the internal convention the monitor may require both a linefeed and a carriage return to be echoed in order to get the screen updated properly on multiuser systems such as mainframes different users may have different types of terminals connected to it and it is up to the keyboard driver to get all the different carriage return linefeed combinations converted to the in ternal system standard and arrange for all echoing to be done right when operating in canonical mode some input characters have special mean ings figure 35 shows all of the special characters required by poslx the de faults are all control characters that should not conflict with text input or codes used by programs all except the last two can be changed under program control character posix nam e comment ctrl h erase backspace on e character ctrl u kill erase entire line being typed ctrl v lnext interpret next character literally ctrl s stop stop output ctrl q start start output del intr interrupt process sigint ctrl x quit force core dump sigquit ctrl d eof end of file ctrl m cr carriage return unchangeable ctrl j nl linefeed unchangeable figure s characters that are handled specially in canonical mode the erase character allows the user to rub out the character just typed it is usually the backspace ctrl h it is not added to the character queue but in stead removes the previous character from the queue it should be echoed as a se quence of three characters backspace space and backspace in order to remove the previous character from the screen if the previous character was a tab eras ing it depends on how it was processed when it was typed if it is immediately ex panded into spaces some extra information is needed to determine how far to back up if the tab itself is stored in the input queue it can be removed and the entire line just output again in most systems backspacing will only erase charac ters on the current line it will not erase a carriage return and back up into the previous line when the user notices an error at the start of the line being typed in it is often convenient to erase the entire line and start again the kill character erases the input output chap entire line most systems make the erased line vanish from the screen but a few older ones echo it plus a carriage return and linefeed because some users like to see the old line consequently how to echo kill is a matter of taste as with erase it is usually not possible to go further back than the current line when a block of characters is killed it may or may not be worth the trouble for the driver to return buffers to the pool if one is used sometimes the erase or kill characters must be entered as ordinary data the lnext character serves as an escape character in unix ctrl v is the default as an example older unix systems often used the sign for kill but the internet mail system uses addresses of the form linda cs washington edu someone who feels more comfortable with older conventions might redefine kill as but then need to enter an sign literally to address e mail this can be done by typing ctrl v the ctrl v itself can be entered literally by typ ing ctrl v ctrl v after seeing a ctrl v the driver sets a flag saying that the next character is exempt from special processing the lnext character itself is not entered in the character queue to allow users to stop a screen image from scrolling out of view control codes are provided to freeze the screen and restart it later in unix these are stop ctrl s and start ctrl q respectively they are not stored but are used to set and clear a flag in the keyboard data structure whenever output is attempted the flag is inspected if it is set no output occurs usually echoing is also suppressed along with program output it is often necessary to kill a runaway program being debugged the intr del and quit ctrl characters can be used for this purpose in unix del sends the sigint signal to all the processes started up from that keyboard implementing del can be quite tricky because unix was designed from the beginning to handle multiple users at the same time thus in the general case there may be many processes running on behalf of many users and the del key must only signal the user own processes the hard part is getting the information from the driver to the part of the system that handles signals which after all has not asked for this information ctrla is similar to del except that it sends the sigquit signal which forces a core dump if not caught or ignored when either of these keys is struck the drwer should echo a carriage return and linefeed and discard all accumulated input to allow for a fresh start the default value for intr is often ctrl c in stead of del since many programs use del interchangeably with the backspace for editing another special character is eof ctrl d which in unix causes any pending read requests for the terminal to be satisfied with whatever is available in the buffer even if the buffer is empty typing ctrl d at the start of a line causes the program to get a read of bytes which is conventionally interpreted as end of file and causes most programs to act the same way as they would upon seeing end of file on an input file sec 6 user interfaces keyboard mouse monitor mouse software most pcs have a mouse or sometimes a trackball which is just a mouse lying on its back one common type of mouse has a rubber ball inside that protrudes through a hole in the bottom and rotates as the mouse is moved over a rough sur face as the ball rotates it rubs against rubber rollers placed on orthogonal shafts motion in the east west direction causes the shaft parallel to the y axis to rotate motion in the north south direction causes the shaft parallel to the x axis to rotate another popular mouse type is the optical mouse which is equipped with one or more light emitting diodes and photodetectors on the bottom early ones had to operate on a special mousepad with a rectangular grid etched onto it so the mouse could count lines crossed modern optical mice have an image processing chip in them and make continuous low resolution photos of the surface under them look ing for changes from image to image whenever a mouse has moved a certain minimum distance in either direction or a button is depressed or released a message is sent to the computer the minimum distance is about 1 mm although it can be set in software some people call this unit a mickey mice or occasionally mouses can have one two or three buttons depending on the designers estimate of the users intellectual ability to keep track of more than one button some mice have wheels that can send additional data back to the computer wireless mice are the same as wired mice except instead of sending their data back to the computer over a wire they use low power radios for example using the bluetooth standard the message to the computer contains three items ax ay buttons the first item is the change in x position since the last message then comes the change in y position since the last message finally the status of the buttons is included the format of the message depends on the system and the number of buttons the mouse has usually it takes 3 bytes most mice report back a maximum of times sec so the mouse may have moved multiple mickeys since the last report note that the mouse only indicates changes in position not absolute position itself if the mouse is picked up and put down gently without causing the ball to rotate no messages will be sent some guis distinguish between single clicks and double clicks of a mouse button if two clicks are close enough in space mickeys and also close enough in time milliseconds a double click is signaled the maximum for close enough is up to the software with both parameters usually being user settable 6 2 output software now let us consider output software first we will look at simple output to a text window which is what programmers normally prefer to use then we will consider graphical user interfaces which other users often prefer input output chap text windows output is simpler than input when the output is sequentially in a single font size and color for the most part the program sends characters to the current win dow and they are displayed there usually a block of characters for example a line is written in one system call screen editors and many other sophisticated programs need to be able to update the screen in complex ways such as replacing one line in the middle of the screen to accommodate this need most output drivers support a series of com mands to move the cursor insert and delete characters or lines at the cursor and so on these commands are often called escape sequences in the heyday of the dumb 25 imes ascii terminal there were hundreds of terminal types each with its own escape sequences as a consequence it was difficult to write soft ware that worked on more than one terminal type one solution which was introduced in berkeley unix was a terminal data base called termcap this software package defined a number of basic actions such as moving the cursor to row column to move the cursor to a particular location the software say an editor used a generic escape sequence which was then converted to the actual escape sequence for the terminal being written to in this way the editor worked on any terminal that had an entry in the termcap data base much unix software still works this way even on personal computers eventually the industry saw the need for standardization of the escape se quence so an ansi standard was developed a few of the values are shown in fig 36 consider how these escape sequences might be used by a text editor suppose that the user types a command telling the editor to delete all of line 3 and then close up the gap between lines 2 and 4 the editor might send the following escape sequence over the serial line to the terminal esc 3 1 h esc k esc 1 m where the spaces are used above only to separate the symbols they are not trans mitted this sequence moves the cursor to the start of line 3 erases the entire line and then deletes the now empty line causing all the lines starting at to move up one line then what was line 4 becomes line 3 what was line becomes line 4 and so on analogous escape sequences can be used to add text to the mid dle of the display words and be added or removed in a similar way the x window system nearly all unix systems base their user interface on the x window system often just called x developed at m i t as part of project athena in the it is very portable and runs entirely in user space it was originally intended for connecting a large number of remote user terminals with a central compute server sec 6 user interfaces keyboard mouse monitor escap e sequenc e meaning esc n a move up n lines escfn b move down n lines sc n c move right n space esc n d move left n space esc m o h move cursor to m n escfs j clear screen from cursor to end 1 1from start 2 all esc k clear line from cursor to end 1 from start 2 all esc n l insert n lines at cursor escfn m delete n lines at cursor esc n p delete n chars at cursor esc n insert n chars at cursor esc n m enable rendition n normal 4 bold blinking 7 reverse esc m scroll the screen backward if the cursor is on the top line figure 36 the ansi escape sequences accepted by the terminal driver on output esc denotes the ascii escape character oxib and n m and are op tional numeric parameters so it is logically split into client software and host software which can potentially run on different computers on modern personal computers both parts can run on the same machine on linux systems the popular gnome and kde desktop envi ronments run on top of x when x is running on a machine the software that collects input from the keyboard and mouse and writes output to the screen is called the x server it has to keep track of which window is currently selected where the mouse pointer is so it knows which client to send any new keyboard input to it communicates with running programs possible over a network called x clients it sends them keyboard and mouse input and accepts display commands from them it may seem odd that the x server is always inside the user computer while the x client may be off on a remote compute server but just think of the x ser ver main job displaying bits on the screen so it makes sense to be near the user from the program point of view it is a client telling the server to do things like display text and geometric figures the server in the local pc just does what it is told as do all servers the arrangement of client and server is shown in fig 37 for the case where the x client and x server are on different machines but when running gnome or kde on a single machine the client is just some application program using the x library talking to the x server on the same machine but using a tcp connection over sockets the same as it would do in the remote case input output chap remot e host use r spac e kerne l spac e x protocol networ k figure 37 clients and servers in the m i t x window system the reason it is possible to run the x window system on top of unix or an other operating system on a single machine or over a network is that what x real ly defines is the x protocol between the x client and the x server as shown in fig 37 it does not matter whether the client and server are on the same ma chine separated by meters over a local area network or are thousands of kilometers apart and connected by the internet the protocol and operation of the system is identical in all cases x is just a windowing system it is not a complete gui to get a complete gui others layer of software are run on top of it one layer is xlib which is a set of library procedures for accessing the x functionality these procedures form the basis of the x window system and are what we will examine below but they are too primitive for most user programs to access directly for example each mouse click is reported separately so that determining that two clicks really form a dou ble click has to be handled above xlib to make programming with x easier a toolkit consisting of the intrinsics is supplied as part of x this layer manages buttons scroll bars and other gui ele ments called widgets to make a true gui interface with a uniform look and feel yet another layer is needed or several of them one example is motif shown in fig 37 which is the basis of the common desktop environment used on solaris and other commercial unix systems most applications make use of calls to motif rather than xlib gnome and kde have a similar structure to fig 37 only with different libraries gnome uses the gtk library and kde uses the qt library whether having two guis is better than one is debatable sec 6 user interfaces keyboard mouse monitor also worth noting is that window management is not part of x itself the decision to leave it out was fully intentional instead a separate x client process called a window manager controls the creation deletion and movement of win dows on the screen to manage windows it sends commands to the x server tel ling what to do it often runs on the same machine as the x client but in theory can run anywhere this modular design consisting of several layers and multiple programs makes x highly portable and flexible it has been ported to most versions of unix including solaris all variants of bsd aix linux and so on making it possible for application developers to have a standard user interface for multiple platforms it has also been ported to other operating systems in contrast in win dows the windowing and gui systems are mixed together in the gdi and located in the kernel which makes them harder to maintain and of course not portable now let us take a brief look at x as viewed from the xlib level when an x program starts it opens a connection to one or more x servers let us call them workstations even though they might be collocated on the same machine as the x program itself x considers this connection to be reliable in the sense that lost and duplicate messages are handled by the networking software and it does not have to worry about communication errors usually tcp ip is used between the client and server four kinds of messages go over the connection 1 drawing commands from the program to the workstation 2 replies by the workstation to program queries 3 keyboard mouse and other event announcements 4 error messages most drawing commands are sent from the program to the workstadon as one way messages no reply is expected the reason for this design is that when the client and server processes are on different machines it may take a substantial period of time for the command to reach the server and be carried out blocking the application program during this time would slow it down unnecessarily on the other hand when the program needs information from the workstation it sim ply has to wait until the reply comes back like windows x is highly event driven events flow from the workstation to the program usually in response to some human action such as keyboard strokes mouse movements or a window being uncovered each event message is bytes with the first byte giving the event type and the next bytes providing ad ditional information several dozen kinds of events exist but a program is sent only those events that it has said it is willing to handle for example if a program does not want to hear about key releases it is not sent any key release events as in windows events are queued and programs read events from the input queue input output chap however unlike windows the operating system never calls procedures within the application program on its own it does not even know which procedure handles which event a key concept in x is the resource a resource is a data structure that holds certain information application programs create resources on workstadons re sources can be shared among multiple processes on the workstation resources tend to be short lived and do not survive workstation reboots typical resources include windows fonts colormaps color palettes pixmaps bitmaps cursors and graphic contexts the latter are used to associate properties with windows and are similar in concept to device contexts in windows a rough incomplete skeleton of an x program is shown in fig 38 it begins by including some required headers and then declaring some variables it then connects to the x server specified as the parameter to xopendisplay then it allocates a window resource and stores a handle to it in win in practice some initialization would happen here after that it tells the window manager that the new window exists so the v indow manager can manage it the call to xcreategc creates a graphic context in which properties of the window are stored in a more complete program they might be initialized here the next statement the call to xselectlnput tells the x server which events the program is prepared to handle in this case it is interested in mouse clicks key strokes and windows being uncovered in practice a real program would be interested in other events as well finally the call to xmapraised maps the new sec 6 user interfaces keyboard mouse monitor include xlib h include xutil h main int argc char argvfj display disp server identifier window win window identifier gc gc graphic context identifier xevent event storage for one event int running 1 disp xopendisplay connect to the x server win xcreatesimplewindow disp allocate memory for new window xsetstandardpropertiesfdisp announces window to window mgr gc xcreategc disp win create graphic context xselectlnputfdisp win buttonpressmask keypressmask exposuremask xmapraised disp win display window send expose event whiie running xnextevent disp event get next event switch event type cas e expose break repaint window cas e buttonpress break process mouse click cas e keypress break process keyboard input window onto the screen as the uppermost window at this point the window be comes visible on the screen the main loop consists of two statements and is logically much simpler than the corresponding loop in windows the first statement here gets an event and the second one dispatches on the event type for processing when some event indicates that the program has finished running is set to and the loop terminates xfreegcfdisp gc xdestroywindow disp win xciosedisplay disp release graphic context deallocate window memory spac e fear down network connection before exiting the program releases the graphic context window and connection it is worth mentioning that not everyone likes a gui many programmers prefer a traditional command line oriented interface of the type discussed in sec 6 2 above x handles this via a client program called xterm this program emu lates a venerable intelligent terminal complete with all the escape se quences thus editors such as vi and emacs and other software that uses termcap work in these windows without modification graphical user interfaces most personal computers offer a gui graphical user interface the acro nym gui is pronounced gooey the gui was invented by douglas engelbart and his research group at the stanford research institute it was then corj jml esearchers at xerox parc one fine day steve jobs cofounder of apple wastouring parc and saw a gui figure 38 a skeleton of an x window application program on a xerox computer and said something to the effect of holy mackerel this is the future of computing the gui gave him the idea for a new computer which became the apple lisa the lisa was too expensive and was a commercial failure but its successor the macintosh was a huge success when microsoft got a macintosh prototype so it could develop microsoft office on it it begged apple to license the interface to all comers so it would be come the new industry standard microsoft made much more money from office than from ms dos so it was willing to abandon ms dos to have a better platform for office the apple executive in charge of the macintosh jean louis gassee refused and steve jobs was no longer around to overrule him eventually micro soft got a license for elements of the interface this formed the basis of win dows when windows began to catch on apple sued microsoft claiming micro soft had exceeded the license but the judge disagreed and windows went on to input output chap overtake the macintosh if gassee had agreed with the many people within apple who also wanted to license the macintosh software to everyone and his uncle sec 6 user interfaces keyboard mouse monitor apple would probably have become immensely rich on licensing fees and win dows would not exist now a gui has four essential elements denoted by the characters wimp these letters stand for windows icons menus and pointing device respectively win dows are rectangular blocks of screen area used to run programs icons are little symbols that can be clicked on to cause some action to happen menus are lists of actions from which one can be chosen finally a pointing device is a mouse trackball or other hardware device used to move a cursor around the screen to se lect items the gui software can be implemented in either user level code as is done in unix systems or in the operating system itself as in the case in windows input for gui systems still uses the keyboard and mouse but output almost always goes to a special hardware board called a graphics adapter a graphics adapter contains a special memory called a video ram that holds the images that appear on the screen high end graphics adapters often have powerful or bit cpus and up to 1 gb of their own ram separate from the computer main memory each graphics adapter supports some number of screen sizes common sizes are x 1280 x x and all of these except v menu bar tool bar window fsiiq client area i thumb scrofl bar j mb or more the graphics adapter can hold many images at once if the full screen is refreshed times sec the video ram must be capable of delivering data continuously at mb sec output software for guis is a massive topic many page books have been written about the windows gui alone e g petzold simon and rector and newcomer clearly in this section we can only scratch the surface and present a few of the underlying concepts to make the discussion concrete we will describe the api which is supported by all bit ver sions of windows the output software for other guis is roughly comparable in a general sense but the details are very different the basic item on the screen is a rectangular area called a window a win dow position and size are uniquely determined by giving the coordinates in pix els of two diagonally opposite corners a window may contain a title bar a menu bar a tool bar a vertical scroll bar and a horizontal scroll bar a typical window is shown in fig 39 note that the windows coordinate system puts the origin in the upper lefthand corner and has y increase downward which is dif ferent frornthe cartesian coordinates used in mathematics figure 39 a sample window located at on an xg a display when a window is created the parameters specify whether the window can be moved by the user resized by the user or scrolled by dragging the thumb on the scroll bar by the user the main window produced by most programs can be moved resized and scrolled which has enormous consequences for the way win dows programs are written in particular programs must be informed about changes to the size of their windows and must be prepared to redraw the contents of their windows at any time even when they least expect it as a consequence windows programs are message oriented user actions involving the keyboard or mouse are captured by windows and converted into messages to the program owning the window being addressed each program has a message queue to which messages relating to all its windows are sent the main loop of the program consists of fishing out the next message and processing it by calling an internal procedure for that message type in some cases windows it self may call these procedures directly bypassing the message queue this model is quite different than the unix model of procedural code that makes system calls to interact with the operating system x however is event oriented input output chap to make this orogramming model clearer consider the example of fig 40 h e j w e t z seon of a main program for window it i not comp and does no error checking but it shows enough detarl for our pulses it starts by including a header file windows h which contains many macros data types con z v r o and other information needed b y wmdows program include windows h int winapl win vlain h nstance h hinstance hprev char szcmd h icmdshow sec 6 user interfaces keyboard mouse monitor is an instance handle and is used to identify the program to the rest of the system to some extent is object oriented which means that the system contains objects e g programs files and windows that have some state and associated code called methods that operate on that state objects are referred to using handles and in this case h identifies the program the second parameter is pres ent only for reasons of backward compatibility it is no longer used the third pa rameter szcmd is a zero terminated string containing the command line that started the program even if it was not started from a command line the fourth parameter icmdshow tells whether the program initial window should occupy wndglass wndclass msg msg hwnd hwnd class object for this window incoming messages are stored here handle pointer to the window object the entire screen part of the screen or none of the screen task bar only this declaration illustrates a widely used microsoft convention called hun garian notation the name is a pun on polish notation the postfix system inven ted by the polish logician j lukasiewicz for representing algebraic formulas without using precedence or parentheses hungarian notation was invented by a hungarian programmer at microsoft charles simonyi and uses the first few v nddass lpszc e 2 t w c w s s progra m icon characters of an identifier to specify the type the allowed letters and types in clude c character w word now meaning an unsigned 16 bit integer i bit signed integer 1 long also a bit signed integer string sz string termi registerciass wndclass hwnd createwindow showwindow hwnd icmdshow updatewindow hwnd tell windows about wndctass allocate storage for the window display the window on the screen tell the window to paint itself nated by a zero byte p pointer fn function and h handle thus szcmd is a zero terminated string and icmdshow is an integer for example many pro grammers believe that encoding the type in variable names this way has little value and makes windows code exceptionally hard to read nothing analogous to white getmessage msg null 0 0 t z f j tran iatpmessaqe msq translate the message m s u heapproprlat e p r o c e d u r e retum msg wparam long callback wndproc hwnd hwnd message uint wparam long iparam declarations go here this convention is present in unix every window must have an associated class object that defines its properties in fig 40 that class object is wndclass an object of type wndclass has fields four of which are initialized in fig 40 in an actual program the other six would be initialized as well the most important field is ipfnwndproc which is a long i e bit pointer to the function that handles the messages directed to this window the other fields initialized here tell which name and icon to use in the title bar and which symbol to use for the mouse cursor after wndclass has been initialized registerclass is called to pass it to win dows in particular after this call windows knows which procedure to call when switch message case return case return destroy return create window repaint contents of window destroy window default various events occur that do not go through the message queue the next call createwindow allocates memory for the window data structure and returns a handle for referencing it later the program then makes two more calls in a row to put the window outline on the screen and finally fill it in completely retum defwindowproc hwnd message wparam iparam figure 40 a skeleton of a windows main program the main program starts with a declaration giving its name and p t h e w macro i s a n instruction t o the compiler t o passing convention and will ngfof further concern to us the first parameter h at this point we come to the program s main loop which consists of getting a message having certain translations done to it and then passing it back to win dows to have windows invoke wndproc to process it to answer the question of whether this whole mechanism could have been made simpler the answer is yes but it was done this way for historical reasons and we are now stuck with it following the main program is the procedure wndproc which handles the various messages that can be sent to the window the use of callback here input output chap like winapl above specifies the calling sequence to use for parameters the first parameter is the handle of the window to use the second parameter is the mes sage type the third and fourth parameters can be used to provide additional infor mation when needed message types wm create and wm destroy are sent at the start and end of the program respectively they give the program the opportunity for ex ample to allocate memory for data structures and then return it the third message type wm paint is an instruction to the program to fill in the window it is not only called when the window is first drawn but often during program execution as well in contrast to text based systems in windows a pro gram cannot assume that whatever it draws on the screen will stay there until it re moves it other windows can be dragged on top of this one menus can be pulled down over it dialog boxes and tool tips can cover part of it and so on when these items are removed the window has to be redrawn the way windows tells a program to redraw a window is to send it a wm paint message as a friendly gesture it also provides information about what part of the window has been over written in case it is easier to regenerate that part of the window instead of redraw ing the whole thing there are two ways windows can get a program to do something one way is to post a message to its message queue this method is used for keyboard input mouse input and timers that have expired the other way sending a message to the window involves having windows directly call wndproc itself this method is used for all other events since windows is notified when a message is fully processed it can refrain from making a new call until the previous one is finished in this way race conditions are avoided there are many more message types to avoid erratic behavior should an unexpected message arrive the program should call defwindowproc at the end of wndproc to let the default handler take care of the other cases in summary a windows program normally creates one or more windows with a class object for each one associated with each program is a message queue and a set of handler procedures ultimately the program s behavior is driven by the incoming events which are processed by the handler procedures this is a very different model of the world than the more procedural view that unix takes the actual drawing to the screen is handled by a package consisting of hun dreds of procedures that are bundled together to form the gdi graphics device interface it can handle text and all kinds of graphics and is designed to be plat form and device independent before a program can draw i e paint in a win dow it needs to acquire a device context which is an internal data structure con taining properties of the window such as the current font text color background color and so on most gdi calls use the device context either for drawing or for getting or setting the properties various ways exist to acquire the device context a simple example of its acquisition and use is sec 6 user interfaces keyboard mouse monitor hdc getdc hwnd textout hdc x y pstext ilength releasedc hwnd hdc the first statement gets a handle to a device content hdc the second one uses the device context to write a line of text on the screen specifying the x y coordi nates of where the string starts a pointer to the string itself and its length the third call releases the device context to indicate that the program is through draw ing for the moment note that hdc is used in a way analogous to a unix file de scriptor also note that releasedc contains redundant information the use of hdc uniquely specifies a window the use of redundant information that has no ac tual value is common in windows another interesting note is that when hdc is acquired in this way the program can only write in the client area of the window not in the title bar and other parts of it internally in the device context s data structure a clipping region is main tained any drawing outside the clipping region is ignored however there is an other way to acquire a device context getwindowdc which sets the clipping re gion to the entire window other calls restrict the clipping region in other ways having multiple calls that do almost the same thing is characteristic of windows a complete treatment of the gdi is out of the question here for the interested reader the references cited above provide additional information nevertheless a few words about the gdi are probably worthwhile given how important it is gdi has various procedure calls to get and release device contexts obtain information about device contexts get and set device context attributes e g the background color manipulate gdi objects such as pens brushes and fonts each of which has its own attributes finally of course there are a large number of gdi calls to actually draw on the screen the drawing procedures fall into four categories drawing lines and curves drawing filled areas managing bitmaps and displaying text we saw an example of drawing text above so let us take a quick look at one of the others the call rectangle hdc xleft ytop xright ybottom draws a filled rectangle whose corners are xleft ytop and xright ybottom for example rectangle hdc 2 1 6 4 will draw the rectangle shown in fig 41 the line width and color and fill color are taken from the device context other gdi calls are similar in flavor bitmaps the gdi procedures are examples of vector graphics they are used to place geometric figures and text on the screen they can be scaled easily to larger or smaller screens provided the number of pixels on the screen is the same they input output chap 0 1 2 3 4 6 7 0 _ 1 2 m 3 _ mm a k i 6 7 figure 41 an example rectangle drawn using rectangle each box represents one pixel are also relatively device independent a collection of calls to gdi procedures can be assembled in a file that can describe a complex drawing such a file is call ed a windows metafile and is widely used to transmit drawings from one win dows program to another such files have extension wmf many windows programs allow the user to copy part of a drawing and put in on the windows clipboard the user can then go to another program and paste the contents of the clipboard into another document one way of doing this is for the first program to represent the drawing as a windows metafile and put it on the clipboard in wmf format other ways also exist not all the images that computers manipulate can be generated using vector graphics photographs and videos for example do not use vector graphics in stead these items are scanned in by overlaying a grid on the image the average red green and blue values of each grid square are then sampled and saved as the value of one pixel such a file is called a bitmap there are extensive facilities in windows for manipulating bitmaps another use for bitmaps is for text one way to represent a particular charac ter in some font is as a small bitmap adding text to the screen then becomes a matter of moving bitmaps one general way to use bitmaps is through a procedure called bitblt it is call ed as follows bitblt dsthdc dx dy wid ht srchdc sx sy rasterop in its simplest form it copies a bitmap from a rectangle in one window to a rec tangle in another window or the same one the first three parameters specify the destination window and position then come the width and height next come the source window and position note that each window has its own coordinate sec 6 user interfaces keyboard mouse monitor system with 0 0 in the upper left hand comer of the window the last parame ter will be described below the effect of bitbit 1 2 7 hdd 2 2 srccopy is shown in fig 42 notice carefully that the entire 7 area of the letter a has been copied including the background color a b figure 42 copying bitmaps using bitblt a before b after bitblt can do more than just copy bitmaps the last parameter gives the possi bility of performing boolean operations to combine the source bitmap and the destination bitmap for example the source can be ored into the destination to merge with it it can also be exclusive ored into it which maintains the characteristics of both source and destination a problem with bitmaps is that they do not scale a character that is in a box of x 12 on a display of x will look reasonable however if this bitmap is copied to a printed page at dots inch which is bits x bits the character width pixels will be 8 1200 inch or 0 17 mm wide in addition copying between devices with different color properties or between monochrome and color does not work well for this reason windows also supports a data structure called a dib device independent bitmap files using this format use the extension bmp these files have file and information headers and a color table before the pixels this information makes it easier to move bitmaps between dissimilar devices fonts in versions of windows before 3 1 characters were represented as bitmaps and copied onto the screen or printer using bitblt the problem with that as we just saw is that a bitmap that makes sense on the screen is too small for the print er also a different bitmap is needed for each character in each size in other input output chap words given the bitmap for a in point type there is no way to compute it for 12 point type because every character of every font might be needed for sizes ranging from 4 point to point a vast number of bitmaps were needed the whole system was just too cumbersome for text the solution was the introduction of truetype fonts which are not bitmaps but outlines of the characters each truetype character is defined by a sequence of points around its perimeter all the points are relative to the 0 0 origin using this system it is easy to scale the characters up or down all that has to be done is to multiply each coordinate by the same scale factor in this way a true type character can be scaled up or down to any point size even fractional point sizes once at the proper size the points ca n be connected using the well known follow the dots algorithm taught in kindergarten note that modern kindergartens use splines for smoother results after the outline has been completed the char acter can be filled in an example of some characters scaled to three different point sizes is given in fig 43 2 p abcdefgh once the filled character is available in mathematical form it can be raster ized that is converted to a bitmap at whatever resolution is desired by first scal ing and then rasterizing we can be sure that the characters displayed on the screen and those that appear on the printer will be as close as possible differing only in quantization error to improve the quality still more it is possible to embed hints in each character telling how to do the rasterization for example both serifs on the top of the letter t should be identical something that might not otherwise be the case due to roundoff error hints improve the final appearance sec 7 thin clients 7 thi n client s over the years the main computing paradigm has oscillated between central ized and decentralized computing the first computers such as the eniac were in fact personal computers albeit large ones because only one person could use one at once then came timesharing systems in which many remote users at sim ple terminals shared a big central computer next came the pc era in which the users had their own personal computers again while the decentralized pc model has advantages it also has some severe disadvantages that are only beginning to be taken seriously probably the biggest problem is that each pc has a large hard disk and complex software that must be maintained for example when a new release of the operating system comes out a great deal of work has to be done to perform the upgrade on each machine sepa rately at most corporations the labor costs of doing this kind of software main tenance dwarf the actual hardware and software costs for home users the labor is technically free but few people are capable of doing it correctly and fewer still enjoy doing it with a centralized system only one or a few machines have to be updated and those machines have a staff of experts to do the work a related issue is that users should make regular backups of their gigabyte file systems but few of them do when disaster strikes a great deal of moaning and wringing of hands tends to follow with a centralized system backups can be made every night by automated tape robots another advantage is that resource sharing is easier with centralized systems a system with remote users each with mb of ram will have most of that ram idle most of the time with a centralized system with gb of ram it never happens that some user temporarily needs a lot of ram but cannot get it because it is on someone else s pc the same argument holds for disk space and other resources finally we are starting to see a shift from pc centric computing to web centric computing one area where this shift is very far along is e mail people used to get their e mail delivered to their home machine and read it there nowa days many people log into gmail hotmail or yahoo and read their mail there the next step is for people to log into other websites to do word processing build spreadsheets and other things that used to require pc software it is even possible that eventually the only software people run on their pc is a web browser and maybe not even that it is probably a fair conclusion to say that most users want high performance interactive computing but do not really want to administer a computer this has led researchers to reexamine timesharing using dumb terminals now politely call ed thin clients that meet modern terminal expectations x was a step in this direction and dedicated x terminals were popular for a little while but they fell out of favor because they cost as much as pcs could do less and still needed some software maintenance the holy grail would be a high performance interac input output chap tive computing system in which the user machines had no software at all interest ingly enough this goal is achievable below we will describe one such thin client system called thinc developed by researchers at columbia university baratto et al kim et al and lai and nieh the basic idea here is to strip the client machine of all it smarts and software and just use it as a display with all the computing including building the bitmap to be displayed done on the server side the protocol between the client and the server just tells the display how to update the video ram nothing more five commands are used in the protocol between the two sides they are listed in fig 44 command description raw display raw pixel data at a given location copy copy frame buffer area to specified coordinates sfili fill an area with a given pixel color value pfill fill an area with a given pixel pattern bitmap fill a region using a bitmap image figure 44 the thinc protocol display commands let us examine the commands now raw is used to transmit pixel data and have them display verbatim on the screen in principle this is the only command needed the others are just optimizations copy instructs the display to move data from one part of its video ram to an other part it is useful for scrolling the screen without having to retransmit all the data sfill fills a region of the screen with a single pixel value many screens have a uniform background in some color and this command is used to first generate the background after which text icons and other items can be painted pfill replicates a pattern over some region it is also used for backgrounds but some backgrounds are slightly more complex than a single color in which case this command does the job finally bitmap also paints a region but with a foreground color and a back ground color all in all these are very simple commands requiring very little software on the client side all the complexity of building the bitmaps that fill the screen are done on the server to improve efficiency multiple commands can be aggregated into a single packet for transmission over the network from server to client on the server side graphical programs use high level commands to paint the screen these are intercepted by the thinc software and translated into com mands that can be sent to the client the commands may be reordered to improve efficiency sec 7 thin clients the paper gives extensive performance measurements running numerous common applications on servers at distances ranging from km to 000 km from the client in general performance exceeded other wide area network sys tems even for real time video for more information we refer you to the papers 8 power management the first general purpose electronic computer the eniac had 18 000 vacuum tubes and consumed 000 watts of power as a result it ran up a non trivial electricity bill after the invention of the transistor power usage dropped dramatically and the computer industry lost interest in power requirements how ever nowadays power management is back in the spotlight for several reasons and the operating system is playing a role here let us start with desktop pcs a desktop pc often has a watt power sup ply which is typically efficient that is loses 15 of the incoming energy to heat if 100 million of these machines are turned on at once worldwide together tiiey use 000 megawatts of electricity this is the total output of average sized nuclear power plants if power requirements could be cut in half we could get rid of nuclear power plants from an environmental point of view getting rid of nuclear power plants or an equivalent number of fossil fuel plants is a big win and well worth pursuing the other place where power is a big issue is on battery powered computers including notebooks handhelds and webpads among others the heart of the problem is that the batteries cannot hold enough charge to last very long a few hours at most furthermore despite massive research efforts by battery com panies computer companies and consumer electronics companies progress is glacial to an industry used to a doubling of performance every 18 months moore s law having no progress at all seems like a violation of the laws of phy sics but that is the current situation as a consequence making computers use less energy so existing batteries last longer is high on everyone s agenda the op erating system plays a major role here as we will see below at the lowest level hardware vendors are trying to make their electronics more energy efficient techniques used include reducing transistor size employ ing dynamic voltage scaling using low swing and adiabatic buses and similar techniques these are outside the scope of this book but interested readers can find a good survey in a paper by venkatachalam and franz there are two general approaches to reducing energy consumption the first one is for the operating system to turn off parts of the computer mostly i o de vices when they are not in use because a device that is off uses little or no ener gy the second one is for the application program to use less energy possibly degrading the quality of the user experience in order to stretch out battery time we will look at each of these approaches in turn but first we will say a little bit about hardware design with respect to power usage input output chap 8 1 hardware issues batteries come in two general types disposable and rechargeable disposable batteries most commonly aaa aa and d cells can be used to run handheld devices but do not have enough energy to power notebook computers with large bright screens a rechargeable battery in contrast can store enough energy to power a notebook for a few hours nickel cadmium batteries used to dominate here but they gave way to nickel metal hydride batteries which last longer and do not pollute the environment quite as badly when they are eventually discarded lithium ion batteries are even better and may be recharged without first being fully drained but their capacities are also severely limited the general approach most computer vendors take to battery conservadon is to design the cpu memory and i o devices to have multiple states on sleeping hibernating and off to use the device it must be on when the device will not be needed for a short time it can be put to sleep which reduces energy consump tion when it is not expected to be needed for a longer interval it can be made to hibernate which reduces energy consumption even more the trade off here is that getting a device out of hibernation often takes more time and energy than get ting it out of sleep state finally when a device is off it does nothing and con sumes no power not all devices have all these states but when they do it is up to the operating system to manage the state transitions at the right moments some computers have two or even three power buttons one of these may put the whole computer in sleep state from which it can be awakened quickly by typ ing a character or moving the mouse another may put the computer into hiberna tion from which wakeup takes much longer in both cases these buttons typi cally do nothing except send a signal to the operating system which does the rest in software in some countries electrical devices must by law have a mechani cal power switch that breaks a circuit and removes power from the device for safety reasons to comply with this law another switch may be needed power management brings up a number of questions that the operating system must deal with many of them deal with resource hibernation selectively and temporarily turning off devices or at least reducing their power consumption when they are idle questions that must be answered include these which devices can be controlled are they on off or do they have intermediate states how much power is saved in the low power states is energy expended to restart the device must some context be saved when going to a low power state how long does it take to go back to full power of course the answers to these ques tions vary from device to device so the operating system must be able to deal with a range of possibilities various researchers have examined notebook computers to see where the power goes li et al measured various workloads and came to the conclu sions shown in fig 5 45 lorch and smith made measurements on other machines and came to the conclusions shown in fig 5 45 weiser et al sec 5 8 pow chapter arrays qualifiers and reading numbers that mysterious independent variable of political calculations public opinion thomas henry huxley arrays in constructing our building we have identified each brick variable by name that process is fine for a small number of bricks but what happens when we want to construct something larger we would like to point to a stack of bricks and say that for the left wall that brick brick brick arrays allow us to do something similar with variables an array is a set of consecutive memory locations used to store data each item in the array is called an element the number of elements in an array is called the dimension of the array a typical array declaration is list of data to be sorted and averaged int the above example declares to be an array of three elements and are separate variables to reference an element of an array you use a number called the index the number inside the square brackets c is a funny language that likes to start counting at so our three elements are numbered to example computes the total and average of five numbers example array array c file array array c include stdio h float data data to average and total float total the total of the data items float average average of the items int main data data data data data total data data data data data average total printf total f average f n total average return this program outputs total average strings strings are sequences of characters c does not have a built in string type instead strings are created out of character arrays in fact strings are just character arrays with a few restrictions one of these restrictions is that the special character nul is used to indicate the end of a string for example char n ame int main name s name a name m name return this code creates a character array of four elements note that we had to allocate one character for the end of string marker string constants consist of text enclosed in double quotes you may have noticed that the first parameter toprintf is a string constant c does not allow one array to be assigned to another so we can t write an assignment of the form name sam illegal instead we must use the standard library function strcpy to copy the string constant into the variable strcpy copies the whole string including the end o f string character to initialize the variable name to sam we would write include string h char name int main strcpy name sam legal return c uses variable length strings for example the declaration include string h char string int main strcpy string sam create an array string that can contain up to characters the size of the array is but the length of the string is any string up to characters long can be stored in string one character is reserved for the nul that indicates end o f string there are several standard routines that work on string variables as shown intable table partial list of string functions function description strcpy copy into strcat concatenate onto the end of length strlen string get the length of a string strcmp equals otherwise nonzero the printf function uses the conversion for printing string variables as shown in example example str str c include string h include stdio h char name first name of someone int main strcpy name sam initialize the name printf the name is n name return example takes a first name and a last name and combines the two strings the program works by initializing the variable first to the first name steve the last name oualline is put in the variablelast to construct the full name the first name is copied into then strcat is used to add a space we callstrcat again to tack on the last name the dimension of the string variable is because we know that no one we are going to encounter has a name more than characters long if we get a name more than characters long our program will mess up what actually happens is that you write into memory that you shouldn t access this access can cause your program to crash run normally and give incorrect results or behave in other unexpected ways example full full c include string h include stdio h char first first name char last last name char full version of first and last name int main strcpy first steve initialize first name strcpy last oualline initialize last name strcpy first full steve note strcat not strcpy strcat full steve strcat last full steve oualline printf the full name is n return the outp ut of this program is the full name is steve oualline reading strings the standard function fgets can be used to read a string from the keyboard the general form of an fgets call is fgets name sizeof name stdin where name identifies a stringvariable fgets will be explained in detail in chapter the arguments are name is the name of a character array the line including the end o f line character is read into this array sizeof name indicates the maximum number of characters to read plus one for the end o f string character the sizeof function provides a convenient way of limiting the number of characters read to the maximum numbers that the variable can h old this function will be discussed in more detail in chapter stdin is the file to read in this case the file is the standard input or keyboard other files are discussed i n chapter example reads a line from the keyboard and reports its length example length length c include string h include stdio h char line line we are looking at int main printf enter a line fgets line sizeof line stdin printf the length of the line is d n strlen line return when we run this program we get enter a line test the length of the line is but the string test is only four characters where the extra character coming from fgets includes the end of line in the string so the fifth character is newline n suppose we wanted to change our name program to ask the user for his first and last name example shows how we could write the program example c include stdio h include string h char first first name of person we are working with char last his last name first and last name of the person computed char full int main printf enter first name fgets first sizeof first stdin printf enter last name fgets last sizeof last stdin strcpy full first strcat full strcat full last printf the name is n full return however when we run this program we get the results enter first name john enter last name doe the name is john doe what we wanted was john doe on the same line what happened the fgets function gets the entire line including the end of line we must get rid of this character before printing for example the name john would be stored as first j first o first h first n first n first end of string by setting first to nul we can shorten the string by one character and get rid of the unwanted newline this change can be done with the statement first the problem is that this method will work only for four character names we need a general algorit hm to solve this problem the length of this string is the index of the end o f string null character the character before it is the one we want to get rid of so to trim the string we use the statement first strlen first our new program is shown in example example c include stdio h include string h char first first name of person we are working with char last his last name first and last name of the person computed char full int main printf enter first name fgets first sizeof first stdin trim off last character first strlen first printf enter last name fgets last sizeof last stdin trim off last character last strlen last strcpy full first strcat full strcat full last printf the name is n full return running this program gives us the following results enter first name john enter last name smith the name is john smith multidimensional arrays arrays can have more than one dimension the declaration for a two dimensiona l array is type variable comment for example int matrix a typical matrix notice that c does not follow the notation used in other languages of matrix to access an element of the matrix we use the notation matrix c allows the programmer to use as many dimensions as needed limited only by the amount of memory available additional dimensions can be tacked on question why does example print the wrong answer click here for the answer section example c include stdio h int arr ay array of numbers int main int x y loop indicies array array array array array array printf array d printf d array printf d array printf n printf array d printf d array printf d array printf n printf array d printf d array printf d array printf n return reading numbers so far we have only read simple strings but we want more we want to read numbers as well the function scanf works like printf except that scanf reads numbers instead of writing them scanf provides a simple and easy way of reading numbers that almost never works the function scanf is notorious for its poor end o f line handling which makes scanf useless for all but an expert however we ve found a simple way to get around the deficiencies of scanf we don t use it instead we use fgets to read a line of input andsscanf to convert the text into numbers the name sscanf stands for string scanf sscanf is like scanf but works on strings instead of the standard input normally we use the variable line for lines read from the keyboard char line line of keyboard input when we want to process input we use the statements fgets line sizeof line stdin sscanf line format here fgets reads a line and sscanf processes it format is a string similar to the printf format string note the ampersand in front of the variable names this symbol is used to indicate that sscanf will change the value of the associated variables for information on why we need the ampersand see chapter in example we use sscanf to get and then double a number from the user example double double c file double double c include stdio h char line input line from console int value a value to double int main printf enter a value fgets line sizeof line stdin sscanf line d value printf twice d is d n value value return this program reads in a single number and then doubles it notice that there is no n at the end of enter a value this omission is intentional because we do not want the computer to print a newline after the prompt for example a sample run of the program might look like enter a value twice is if we replaced enter a value with enter a value n the result would be enter a value twice is question example computes the area of a triangle given the triangle width and height for some strange reason the compiler refuses to believe that we declared the variable width the declaration is right there on line just after the definition of height why isn t the compiler seeing it click here for the answer section example tri tri c include stdio h char line line of input data int height the height of the triangle int width the width of the triangle int area area of the triangle computed int main printf enter width height fgets line sizeof line stdin sscanf line d d width height area width height print f the area is d n area return initializing variables c allows variables to be initialized in the declaration statement for example the following statement declares the integer counter and initializes it to int counter number cases counted so far arrays can also be initialized in this manner the element list must be enclosed in curly braces for example product numbers for the parts we are making int the previous initia lization is equivalent to the number of elements in does not have to match the array size if too many numbers are present a warning will be issued if an insufficient amount of n umbers are present c will initialize the extra elements to if no dimension is given c will determine the dimension from the number of elements in the initialization list for example we could have initialized our variable with the state ment product numbers for the parts we are making int 972 initializing multidimensional arrays is similar to initializing single dimension arrays a set of brackets encloses each dimension the declaration int mat rix a typical matrix can be thought of as a declaration of an array of dimension with elements that are arrays of dimension this array is initialized as follows a typical matrix int matrix strings can be initialized in a similar manner for example to initialize the variable name to the string sam we use the statement char name s a m c has a special shorthand for initializing strin gs surround the string with double quotes to simplify initialization the previous example could have been written char name sam the dimension of name is because c allocates a place for the character that ends the string the following declaration char string sam is equivalent to char string strcpy string sam an array of characters is allocated but the length of the string is types of integers c is considered a medium level language because it allows you to get very close to the actual hardware of the machine some languages like basic go to great lengths to completely isolate the user fro m the details of how the processor works this simplification comes at a great loss of efficiency c lets you give detailed information about how the hardware is to be used some more advanced versions of basic do have number types however for this e xample we are talking about basic basic for example most machines let you use different length numbers basic provides the programmer with only one numeric type though this restriction simplifies the programming basic programs are extremely inefficien t c allows the programmer to specify many different flavors of integers so that the programmer can make best use of hardware the type specifier int tells c to use the most efficient size for the machine you are using for the integer this can be two to four bytes depending on the machine some less common machines use strange integer sizes such as or bits sometimes you need extra digits to store numbers larger than those allowed in a normal int the declaration long int answer the result of our calculations is used to allocate a long integer the long qualifier informs c that we wish to allocate extra storage for the integer if we are going to use small numbers and wish to reduce storage we use the qualifier short for example short int year year including the part c guarantees that the size of storage for short int long in actual practice short almost always allocates two bytes long four bytes and int two or four bytes see appendix b for numeric ranges the type short int usually uses bytes or bits bits are used normally for the number and bit for the sign this format gives the type a range of to an unsigned short int uses all bits for the number giving it the range of to all int declarations default to signed so that the declaration signed long int answer final result is the same as long int answer final result finally we consider the very short integer of type char character variables use byte they can also be used for numbers in the range of to signed char or to unsigned char unlike integers they do not default to signed the default is compiler dependent very short integers may be printed using the integer conversion d turbo c and gnu gcc even have a command line switch to make the default for typechar either signed or unsigned you cannot read a very short integer directly you must read the number into an integer and then use an assignment statement for example include stdio h signed char a very short integer char line input buffer int temp a temporary number int main read a very short integer fgets line sizeof line stdin sscanf line d temp temp table contains the printf and sscanf conversions for integers table integer printf sscanf conversions conversion uses hd signed short int d signed int ld signed long int hu unsigned short int u unsigned int lu unsigned long int the range of the various flavors of integers is listed in appendix b long int declarations allow the program to explicitly specify extra precision where it is needed at the expense of memory short int numbers save space but have a more limited range the most compact integers have type char they also have the most li mited range unsigned numbers provide a way of doubling the positive range at the expense of eliminating negative numbers they are also useful for things that can never be negative like counters and indices the flavor of number you use will depend on yo ur program and storage requirements types of floats the float type also comes in various flavors float denotes normal precision usually bytes double indicates double precision usually bytes double precision variables give the programmer many times the range and precision of single precision float variables the qualifier long double denotes extended precision on some systems this is the same as double on others it offers additional precision all types of floating point numbers are a lways signed table contains the printf and sscanf conversions for floating point numbers table float printf sscanf conversions conversion uses notes f float printf only lf double scanf only lf long double not available on all compilers the f format works for printing double and float because of an automatic conversion built into c parameter passing on some machines single precision floating point instructions execute faster but less accurately than double precision instructions double precision instructions gain accuracy at the expense of time and storage in most cases float is adequate however if accuracy is a problem switch to double see chapter constant declarations sometimes you want to use a value that does not change such as the keyword const indicates a variable that never changes for example to declare a value for pi we use the statement const float pi the classic circle constant constants must be initialized at declaration time and can never be changed for example if we tried to reset the value of pi to we would generate an error message pi illegal integer constants can be used as a size p arameter when declaring an array max number of elements in the total list const int float total values for each category hexadecimal and octal constants integer numbers are specified as a string of digits su ch as etc these strings are decimal base numbers or computers deal with binary base numbers the octal base system easily converts to and from binary each group of three digits can be transforme d into a single octal digit thus can be written as and changed to the octal hexadecimal base numbers have a similar conversion only bits are used at a time the c language has conventions for representing octal and hexade cimal values leading zeros are used to signal an octal constant for example is octal or decimal starting a number with indicates a hexadecimal base constant so is decimal table shows several numbers in all three bases table integer examples base base base 011 0xf operators for performing shortcuts c not only provides you with a rich set of declarations but also gives you a large number of special purpose operators frequently the programmer wants to increment increase by a variable using a normal assignment statement this operation would look like c provides us with a shorthand for performing this common task the operator is used for incrementing a similar operator can be used for decrementing decreasing by a variable is the same as but suppose that we want to add instead of then we can use the following notation this notation is equivalent to total_entries each of the simple operators as shown in table can be used in this manner table shorthand operators operator shorthand equivalent statement x x x x x x x x x x x x x x x side effects unfortunately c allows the programmer to use side effects a side effect is an operation that is performed in addition to the main operation executed by the statement for example the follo wing is legal c code size result size the first statement assigns tosize the value of the second statement assigns to result the value of size main operation and increments size side effect but in what order are these processes performed there are four possible answers result is assigned the value of size and then size is incremented result is and size is size is incremented and then result is assigned the value of size result is and size is the answer is compile r dependent and varies from computer to computer if we don t write code like this then we don t have to worry about such questions the correct answer is number the increment occurs before the assignment however number is a much better answer th e main effects of c are confusing enough without having to worry about side effects c actually provides two flavors of the operator one is variable and the other is variable the first number result number evaluates the expressions and then increments the number result is the second number result number increments the number first and then evaluates the expression result is however using or in this way can lead to some surprising code o o o the problem with this line is that it looks as if someone wrote morse code the programmer doesn t read this statement but rather decodes it if we never use or as part of any other statement and instead always put them on lines by themselves the difference between the two flavors of these operators will not be noticeable x or x the two forms of the increment operator are called the prefix form x and the postfix form x which form should you use actually in c your choice doesn t matter however if you use c with its overloadable operators the prefix version x is more efficient so to develop good habits for learning c use the prefix form for details see the book practical c programming o reilly associates consider the irony of a language with its name in postfix form c working more efficiently with prefix forms of the increment and decrement operators maybe the name should be c more side effect problems more complex side effects can confuse even the c compiler consider the following code fragment value result value value this expression tells c to perform the following steps multiply value by and add to value multiply value by and add to value add the results of the two multiplications together steps and are of equal priority unlike in the previous e xample so the compiler can choose the order of execution suppose the compiler executes step first as shown in figure figure expression evaluation method or suppose the compiler executes step first as shown in figure figure expression evaluation method by using the first method we get a result of by using the second method we get a result of the result of this expression is ambiguous depending on how the compiler was implemented the result may be or even worse some compilers change the behavior if optimization is turned on so what was working code may break when optimized by using the operator in the middle of a larger expression we created a problem this problem is not the only problem that and can cause we will get into more trouble in chapter in order to avoid trouble and keep the program simple always put and on a line by themselves answers answer the problem is the use of the expression array x y in the printf statement printf d array x y each index to a multidimension array must be placed inside its own set of square brackets the statement should read printf d array x y for those of you who want to read ahead a little the comma operator can be used to string multiple expressions together the value of this operator is the value of the last expressions as a result x y is equivalent to y and array y is actually a pointer to row y of the array because pointers have strange values the printf outputs strange results see chapter and chapter answer the programmer accidentally omitted the end comment after the comment for height the comment continues onto the next line and engulfs the declaration as shown in example example comment answer consider another minor problem with this program if width and height are both odd we get an answer that slightly wrong how would you correct this error programming exercises exercise write a program t hat converts centigrade to fahrenheit exercise write a program to calculate the volume of a sphere exercise write a program that prints the perimeter of a rectangle given its height and width perimeter width height exercise write a program that converts kilometers per hour to miles per h our miles kilometer exercise write a program that takes hours and minutes as input and then outputs the total number of minutes hour minutes minutes exercise write a program that takes an integer as the number of minutes and outputs the total hours and minutes minutes hour minutes chapter decision and control statements once a decision was made i did not worry about it afterward harry truman calculations and expressions are only a small part of computer programming decision and control statements are needed they specify the order in which statements are to be executed so far we have constructed linear programs that is programs that execute in a straight line one statement after another in this chapter we will see how to change the control flow of a program with branching statements and looping statements branching statements cause one section of code to be executed or not executed depending on a conditional clause looping statements are used to repeat a section of code a number of times or until some condition occurs if statement the if statement allows us to put some decision making into our programs the general form of the if statement is if condition statement if the condition is true nonzero the statement will be executed if the condition is false the statement will not be executed for example suppose we are writing a billing program at the end if the customer owes us nothing or has a credit owes us a ne gative amount we want to print a message in c this program is written if printf you owe nothing n the operator is a relational operator that represents less than or equal to this statement reads if the is less than or equal to zero print the message the complete list of relational operators is found in table table relational operators operator meaning less than or equal to less than greater than greater than or equal to equal not equal the equal test is different from the assignment operator one of the most common problems the c programmer faces is mixing them up multiple statements may be grouped by putting them inside curly braces for example if printf you owe nothing n for readability the statements enclosed in are usually indented this allows the programmer to quickly tell which statements are to be conditionally executed as we will see later mistakes in indentation can result in programs that are misleading and hard to read else statement an alternate form of the if statement is if condition statement else statement if the condition is true nonzero the first statement is executed if it is false the second statement is executed in our accounting example we wrote out a message only if nothing was owed in real life we probably would want to tell the customer how much is owed if there is a balance due if printf you owe nothing n else printf you owe d dollars n now consider this program fragment with incorrect indentation if count if if count if printf condition white n else printf condition tan n there are two if statements and one else which if does the else belong to it belongs to if it belongs to if if you never write code like this don t worry about this situation the correct answer is c according to the c syntax rules the else goes with the nearest if so b is synta ctically correct but writing code like this violates the kiss principle keep it simple stupid we should write code as clearly and simply as possible this code fragment should be written as if count if if count if printf condition white n else printf condition tan n in our original example we could not clearly determine which if statement had the else clause however by adding an extra set of braces we improve re adability understanding and clarity how not to use strcmp the function strcmp compares two strings and then returns zero if they are equal or nonzero if they are different to check if two strings are equal we use the code check to see if if strcmp printf strings equal n else printf strings not equal n some programmers omit the comment and the clause these omissions lead to the following confusing code if strcmp printf at first glance this program obviously compares two strings and executes the printf statement if they are equal unfortunately the obvious is wrong if the strings are equal strcmp returns and theprintf is not executed because of this backward behavior of strcmp you should be very careful in your use of strcmp and always comment its use it also helps to put in a comment explaining what you re doing looping statements looping statements allow the program to repeat a section of code any number of times or until some condition occurs for example loops are used to count the number of words in a document or to count the number of accounts that have past due balances while statement the while statement is used when the program needs to perform repetitive tasks the general form of a while statement is while condition statement the program will repeatedly execute the statement inside the while until the condition becomes false if the condition is initially false the statement will not be executed for example example later in this chapter will compute all the fibonacci numbers that are less than the fibonacci sequence is the terms are computed from the equations etc in general terms this is f n fn fn this is a mathematical equation using mathematical variable names fn mathematicians use this very terse style of naming variables in programming terse is dangerous so we translate these names into something verbose for c table shows this translation table math to c name translation math style name c style name fn fn fn in c code the equation is expressed as we want to loop until our current term is or larger the while loop while will repeat our computation and printing until we reach this limit figure shows what happens to the variable duringthe execution of the program at the beginning and are we print the value of the current term then the variable is computed value next we advance one term by putting into and into this process is repeated until we compute the last term and the while loop exits figure fibonacci execution this completes t he body of the loop the first two terms of the fibonacci sequence are and we initialize our first two terms to these values putting it all together we get the code in example example fib fib c include stdio h int previous fibonacci number int current fibonacci number int next number in the series int main start things out printf n print first number while printf d n return break statement we have used a while statement to compute the fibonacci numbers less than the loop exits when the condition after the while becomes false loops can be exited at any point through the use of a break statement suppose we want to add a series of numbers but we don t know how many numbers are to be added together we need some way of letting the program know that we have reached the end of our list in example we use the number zero to signal the end of list note that the while statement begins with while left to its own devices the program will loop forever because the while will exit only when the expression is the only way to exit this loop is through a break statement when we see the end of the list indicator we use the statement if item break to exit the loop example total total c include stdio h char line line of data for input int total running total of all numbers so far int item next item to add to the list int main total while printf enter to add n printf or to stop fgets line sizeof line stdin sscanf line d item if item break total item printf total d n total printf final total d n total return continue statement the continue statement is very similar to the break statement except that instead of terminating the loop continue starts reexecuting the body of the loop from the top for example if we want to modify the previous program to total only numbers larger than we could write a program such as example example totalb totalb c file totalb totalb c include stdio h char line line from input int total running total of all numbers so far int item next item to add to the list int number of negative items int main total while printf enter to add n printf or to stop fgets line sizeof line stdin sscanf line d item if item break if item continue total item printf total d n total printf final total d n total printf with d negative items omitted n return assignment anywhere side effect c allows the use of assignment statements almost anywhere for example you can put assignment statements inside assignment statements don t program like this average last first this is the equivalent of saying program like this last first average the first version buries the assignment of inside the expression programs should be clear and simple and should not hide anything the most important rule of programming is keep it simple c also allows the programmer to put assignment statements in the while conditional for example do not program like this while printf term d n avoid this type of programming notice how much clearer the logic is in the version below program like this while last_number old_number if break printf term d n question for some strange reason example thinks that everyone owes a balance of dollars why click here for the answer section example c include stdio h char line input line int amount owed int main printf enter number of dollars owed fgets line sizeof line stdin sscanf line d if printf you owe nothing n else printf you owe d dollars n return sample output enter number of dollars owed you owe dollars answer answer this program illustrates one of the most common and frustrating of c errors the problem is that c allows assignment statements inside if conditionals the statement if uses a single equal sign instead of the double equal sign c will assign the value and test the result which is if the result was nonzero true theif clause would be executed beca use the result is false the else clause is executed and the program prints the wrong answer the statement if is equivalent to if the statement should be written if this e rror is the most common error that beginning c programmers make programming exercises exercise write a program to find the square of the distance between two points for a more advanced problem find the actual distance this problem involves u sing the standard function sqrt use your help system to find out more about how to use this function exercise a professor generates letter grades using table table grade values right grade f d c b a given a numeric grade print the letter exercise modify the pre vious program to print a or after the letter grade based on the last digit of the score the modifiers are listed in table table grade modification values last digit modifier blank for example b a and d note an f is only an f there is no f or f exercise given an amount of money less than compute the number of quarters dimes nickels and pennies needed exercise a leap year is any year divisible by unless the year is divisible by but not write a program to tell if a year is a leap year exercise write a program that given the number of hours an employee worked and the hourly wage computes the employee weekly pay count any hours over as overtime at time and a half chapter programming process it just a simple matter of programming any boss who has never written a program programming is more than just writing code software has a life cycle it is born grows up becomes mature and finally dies only to be replaced by a newer younger product figure illustrates the life cycle of a program understanding this cycle is important because as a programmer you will spend only a small amount of time writing new code most programming time is spent modifying and debugging existing code software does not exist in a vacuum it must be docume nted maintained enhanced and sold in this chapter we will take a look at a small programming project using one programmer larger projects that involve many people will be discussed in chapter although our final code is less than lines the principles used in its construction can be applied to programs with thousands of lines of code figure software life cycle the major steps in making a program are requirements programs start when someone gets an idea and starts to implement it the requirement document describes in very general terms what is wanted program sp ecification the specification is a description of what the program does in the beginning a preliminary specification is used to describe what the program is going to do later as the program becomes more refined so does the specification finally whe n the program is finished the specification serves as a complete description of what the program does code design the programmer does an overall design of the program the design should include major algorithms module definitions file formats and data structures coding the next step is writing the program this step involves first writing a prototype and then filling it in to create the full program testing the programmer should design a test plan and then use it to test his program when possib le the programmer should have someone else test the program debugging unfortunately very few programs work the first time they must be corrected and tested again release the program is packaged documented and sent out into the world to be used maintenance programs are never perfect bugs will be found and will need correction this step is the maintenance phase of programming revision and updating after a program has been working for a while the users will want changes such as more feature or more intelligent algorithms at this point a new specification is created and the process starts again setting up the operating system allows you to group files in directories just as file folders serve as a way of keeping papers together in a filing cabinet directories serve as a way of keeping files together windows goes so far as to call its directories folders in this chapter we create a simple calculator program all the files for this program are stored in a directory named calc in unix we create a new directory under our home directory and then move to it as shown in the following example cd mkdir calc cd calc on ms dos type c cd c mkdir calc c cd calc c calc this directory setup is extremely imple as you generate more and more programs you will probably want a more elaborate directory structure more information on how to organize directories or folders can be found in your operating system manual specification for this chapter we assume that we have the requirement to write a program that acts like a four function calculator typically the requirements that you are given is vague and incomplete the programmer refines it into something that exactly defines the program that he is go ing to produce so the first step is to write a preliminary users specification document that describes what your program is going to do and how to use it the document does not describe the internal structure of the program or the algorithm you plan on u sing a sample specification for our four function calculator appears below in calc a four function calculator the preliminary specification serves two purposes first you should give it to your boss or customer to make sure that you agree on what each of you said second you can circulate it among your colleagues and see if they have any suggestions or corrections this preliminary specification was circulated and received the comments how are you going to get out of the program what happens when you try to divide by calc a four function calculator preliminary specification dec steve oualline warning this document is a preliminary specification any resemblance to any software living or dead is purely coincidental calc is a program that allows the user to turn a computer into a four function calculator the program will add subtract multiply and divide simple integers when the progra m is run it will zero the result register and display the register contents the user can then type in an operator and number the result will be updated and displayed the following operators are valid operator meaning addition subtraction mu ltiplication division for example user input is in boldface calc result enter operator and number result enter operator and number result enter operator and number result enter operator and number result so we add a new operator q for quit and we add the statement dividing by results in an error message and the result register is left unchanged code design after the preliminary specification has been approved we can start designing code in the code design phase the programmer plans his work in large programming projects involving many people the code would be broken up into modules to be assigned to the programmers at this stage file formats are planned data structures are designed and major algorithms are decided upon our simple ca lculator uses no files and requires no fancy data structures what left for this phase is to design the major algorithm outlined in pseudo code a shorthand halfway between english and real code the major algorithm is loop read an operator and number do the calculation display the result end loop prototype after the code design is completed we can begin writing the program but rather than try to write the entire program at once and then debug it we will use a method called fast prototyping we implement the smallest portion of the specification that will still do something in our case we will cut our four functions down to a one function calculator after we get this small part working we can build the rest of the functions onto this stable foundation also the prototype gives the boss something to look at and play with giving him a good idea of the project direction good communication is the key to good programming and the more you can show someone the better the code for th e first version of our four function calculator is found in example example c include stdio h char line line of data from the input int result the result of the calculations char operator operator the user specified int value value specified after the operator int main result initialize the result loop forever or till we hit the break statement while printf result d n result printf enter operator and number fgets line sizeof line stdin sscanf line c d operator value if operator result value else printf unknown operator c n operator the program begins by initializing the variable result to the main body of the program is a loop starting with while this loop will repeat until a break statement is reached the code printf enter operator and number fgets line sizeof line stdin sscanf line c d operator value asks the user for an operator and number these are scanned and stored in the variables operator and value next we start checking the operators if the operator is a plus sign we perform an addition using the line if operator result value so far we only recognize the plus operator as soon as this operator works correctly we will add more operators by adding more if statements finally if an illegal operator is entered the line else printf unknown operator c n operator writes an error message telling the user that he made a mistake makefile after the source has been entered it needs to be compiled and linked up until now we have been running the compiler manually this process is somewhat tedious and prone to error also larger programs consist of many modules and are extremely difficult to compile by hand fortunately both unix and ms dos windows have a utility called make that will handle the details of compilation for n ow use this example as a template and substitute the name of your program in place of calc make will be discussed in detail in chapter the program looks at the file called makefile for a description of how to compile your program and runs the compiler for you microsoft visual c calls this utility nmake because the makefile contains the rules for compilation it is customized for the compiler the following is a set of makefiles for all of the compilers described in this book generic unix file makefile unx makefile for unix systems using a gnu c compiler cc gcc cflags g compiler flags g enable debugging c cc cflags o c clean rm f unix with the free software foundation gcc compiler file makefile gcc makefile for unix systems using a gnu c compiler cc gcc cflags g ansi compiler flags g enable debugging wall turn on all warnings not used since it gives away the bug in this program force the compiler to use the correct headers ansi don t use gnu extensions stick to ansi c c cc cflags o c clean rm f borland c file makefile bcc makefile for borland borland c compiler cc bcc flags n check for stack overflow v enable debugging w turn on all warnings ml large model cflags n v w ml exe c cc cflags c clean erase exe turbo c file makefile tcc makefile for dos systems using a turbo c compiler cc tcc cflags v w ml exe c cc cflags exe c clean del exe visual c file makefile msc makefile for dos systems using a microsoft visual c compiler cc cl flags al compile for large model zi enable debugging turn on warnings cflags al zi exe c cc cflags c clean erase exe to compile the program just execute the make command make will determine which compilation commands are needed and then execute them make uses the modification dates of the files to determine whether or not a compile is necessary compilation creates an object file the modification date of the object file is later than the modification date of its source if the source is edited the source modification date is updated and the object file is then out of date make checks these dates and if the source was modified after the object make recompiles the object testing after the program is compiled without errors we can move on to the testing phase now is the time to start writing a test plan this document is simply a list of the steps we perform to make sure the program works it is written for two reasons if a bug is found we want to be able to reproduce it if we change the program we will want to retest it to make sure new code did not break any of the sections of the program that were previously working our test plan starts out as try the following operations result should be result should be x error message should be output after we run the program we get result enter operator and number result enter operator and number result enter operator and number x result something is clearly wrong the entry x should have generated an error message but it didn t a bug is in the progra m so we begin the debugging phase one of the advantages of making a small working prototype is that we can isolate errors early debugging first we inspect the program to see if we can detect the error in such a small program we can easily spot the mistake however let assume that instead of a line program we have a much larger program containing lines such a program would make inspection more difficult so we need to proceed to the next step most systems have c debugging programs however each system is different some systems have no debugger in such a case we must resort to a diagnostic print statement the technique is simple put a printf at the points at which you know the data is good just to make sure the data is really go od then put aprintf at points at which the data is bad run the program and keep putting in printf statements until you isolate the area in the program that contains the mistake our program with diagnostic printf statements added looks like printf enter operator and number fgets line sizeof line stdin sscanf d c value operator printf after scanf c n operator if operator printf after if c n operator result value running our program again results in result enter operator and number result enter operator and number after scanf after if result enter operator and number x after scanf x after if result from this example we see that something is going wrong with the if statement somehow the variable operator is an x going in and a coming out closer inspection reveals that we have made the old mistake of using instead of after we fix this bug the program runs correctly building on this working foundation we add code for the other operators dash asterisk and slash the result is shown in example example c include stdio h char line line of text from input int result the result of the calculations char operator operator the user specified int value value specified after the opera tor int main result initialize the result loop forever or until break reached while printf result d n result printf enter operator and number fgets line sizeof line stdi n sscanf line c d operator value if operator q operator q break if operator result value else if operator result value else if operator result value else if operator if value printf error divide by zero n printf operation ignored n else result value else printf unknown operator c n operator return we expand our test plan to include the new operators and try it again result should be result sho uld be x error message should be output result should be zero result should be result should be divide by zero error result should be q program should exit while testing the prog ram we find that much to our surprise the program works the word preliminary is removed from the specification and the program test plan and specification are released debugging first we inspect the program to see if we can detect the error in such a small program we can easily spot the mistake however let assume that instead of a line program we have a much larger program containing lines such a program would make inspection more difficult so we need to proceed to the next step most systems have c debugging programs however each system is different some systems have no debugger in such a case we must resort to a diagnostic print statement the technique is simple put a printf at the points at which you know the data is good just to make sure the data is really good then put aprintf at points at which the data is bad run the program and keep putting in printf statements until you isolate the area in the program that contains the mistake our program with diagnostic printf statements added looks like printf enter operator and number fgets line sizeof line stdin sscanf d c value operator printf after scanf c n operator if operator printf after if c n operator result value running our program again results in result enter operator and number result enter operator and number after scanf after if result enter operator and number x after scanf x after if result from this example we see that something is going wrong with the if statement somehow the variable operator is an x going in and a coming out closer inspection reveals that we have made the old mistake of using instead of after we fix this bug the program runs correctly building on this working foundation we add code for the other operators dash asterisk and slash the result is shown in example example calc2 c include stdio h char line line of text from input int result the result of the calculations char operator operator the user specified int value value specified after the operator int main result initialize the result loop forever or until break reached while printf result d n result printf enter operator and number fgets line sizeof line stdin sscanf line c d operator value if operator q operator q break if operator result value else if operator result value else if operator result value else if operator if value printf error divide by zero n printf operation ignored n else result value else printf unknown operator c n operator return we expand our test plan to include the new operators and try it again result should be result should be x error message should be output result should be zero result should be result should be divide by zero error q result should be program should exit while testing the program we find that much to our surprise the program works the word preliminary is removed from the specification and the program test plan and specification are released maintenance good programme rs put each program through a long and rigorous testing process before releasing it to the outside world then the first user tries the program and almost immediately finds a bug this step is the maintenance phase bugs are fixed the program is tested t o make sure that the fixes didn t break anything and the program is released again revisions although the program is officially finished we are not done with it after the program is in use for a few months someone will come to us and ask can you add a modulus operator so we revise the specifications add the change to the program update the test plan test the program and then release the program again as time passes more people will come to us with additional requests for changes soon o ur program has trig functions linear regressions statistics binary arithmetic and financial calculations our design is based on the concept of one character operators soon we find ourselves running out of characters to use at this point our program is doing work far in excess of what it was initially designed to do sooner or later we reach the point where the program needs to be scrapped and a new one written from scratch at that point we write a preliminary specification and start the process ag ain electronic archaeology electronic archeology is the art of digging through old code to discover amazing things like how and why the code works unfortunately most programmers don t start a project at the design step instead they are immedia tely thrust into the maintenance or revision stage and must face the worst possible job understanding and modifying someone else code your computer can aid greatly in your search to discover the true meaning of someone else code many tools are available for examining and formatting code some of these tools include cross references these programs have names like xref cxref and cross system v unix has the utility cscope a cross reference prints out a list of variables and indicates where each va riable is used program indenters programs like cb and indent will take a program and indent it correctly correct indentation is something defined by the tool maker pretty printers a pretty printer such as vgrind or cprint will take the source and typeset it for printing on a laser printer call graphs on system v unix the program cflow can be used to analyze the program on other systems there is a public domain utility calls which produces call graphs the call graphs show who calls whom and who is called by whom which tools should you use whichever work for you different programmers work in different ways some of the techniques for examining code are listed in the sections below choose the ones that work for you and use them marking up the program take a printout of the program and make notes all over it use red or blue ink so that you can tell the difference between the printout and the notes use a highlighter to emphasize important sections these notes are useful put them in the program as comments then make a new printout and start the process again using the debugger the debugger is a great tool for understanding how something works most debuggers allow the user to step through the program one line at a time examinin g variables and discovering how things really work after you find out what the code does make notes and put them in the program as comments text editor as a browser one of the best tools for going through someone else code is your text editor suppose you want to find out what the variable sc is used for use the search command to find the first place sc is used search again and find the second time it is used continue searching until you know what the variable does suppose you find out that sc is used as a sequence counter because you re already in the editor you can easily do a global search and replace to change sc to disaster warning before you make the change make sure that is not already defined as a variable also watch out for unwanted replacements such as changing the sc in escape comment the declaration and you re on your way to creating an understandable program add comments don t be afraid of putting any information you have no ma tter how little into the comments some of the comments i ve used include int state controls some sort of state machine int rmxy something to do with color correction finally there is a catch all comment int idn which means i have no idea what this variable does even though the variable purpose is unknown it is now marked as something that needs more work as you go through someone else code adding comments and improving style the structure will become clearer to you by inserting notes comments you make the code better and easier to understand for future programmers for example suppose we are confronted with the following program written by someone from the terser the better school of programming our assignment is to figure out what this code does first we pencil in some comments as shown in figure figure a terse program our mystery program requires some work after going through it and applying the principles described in this section we get a well commented easy to understand program such as example example good good c guess a simple guessing game usage guess a random number is chosen between and the player is given a set of bounds and must choose a number between them if the player chooses the correct number he wins otherwise the bounds are adjusted to reflect the player guess and the game continues restrictions the random number is generated by the statement rand because rand returns a number rand maxint this slightly favors the lower numbers include stdio h include stdlib h int random number to be guessed int current lower limit of player range int current upper l imit of player range int number of times player guessed int number gotten from the player char line input buffer for a single line int main while not a pure random number see restrictions rand initialize variables for loop while tell user what the bounds are and get his guess printf bounds d d n printf value d fgets line sizeof line stdin sscanf line d did he guess right if break adjust bounds for next guess if player_number else player_number printf bingo n programming exercises for each of these assignments follow the software life cycle from specification through release exercise write a program to convert english units to metric i e miles to kilometers gallons to liters etc include a specification and a code design exercise write a program to perform date arithmetic such as how many days there are between and include a specification and a code design exercise a serial transmission line can transmit characters each second write a program that will calculate the time required to send a file given the file size try the prog ram on a byte file use appropriate units a file takes days exercise write a program to add an sales tax to a given amount and round the result to the nearest penny exercise write a program to tell if a number is prime exercise write a program that takes a series of numbers and counts the number of positive and negative values part ii simple programming this part builds on the basics to round out our description of simple c programming in this part we l earn the rest of the control statements as well as some more advanced operations such as bit operations finally we get an introduction to some more sophisticated programming tasks such as file i o and debugging chapter describes additional control statements included are for break and continue the switch statement is discussed in detail chapter introduces local variables functions and parameters chapter describes the c preprocessor which gives the programmer tremendous flexibility in writing code the chapter also p rovides the programmer with a tremendous number of ways to mess up simple rules that help keep the preprocessor from becoming a problem are described chapter discusses the log ical c operators that work on bits chapter explains structures and other advanced types the sizeof operator and the enum type are included chapter introduces c pointer variables and shows some of their uses chapter describes both buffered and unbuffered input output asc ii versus binary files are discussed and you are shown how to construct a simple file chapter describes how to debug a program as well as how to use an interactive debugger you are shown not only how to debug a program but also how to write a program so that it is easy to debug this chapter also describes many optimization techniques for making your program run faster and more efficiently chapter uses a simple decimal floating point format to introduce you to the problems inherent in floating point such as roundoff error precision loss overflow and underflow chapter more control statements grammar which knows how to control even kings molière for statement the for statement allows the programmer to execute a block of code for a specified number of times the general form of the for statement is for initial statement condition iteration statement body statement this statement is equivalent to initial statement while condition body statement iteration statement for example example uses a while loop to add five numbers example totalw c include stdio h int total total of all the numbers int current current value from the user int counter while loop counter char line line from keyboard int main total counter while counter printf number fgets line sizeof line stdin sscanf line d current total current counter printf the grand total is d n total return the same program can be rewritten using a for statement as shown in example example total5f c include stdio h int total total of all the numbers int current current value from the user int counter for loop counter char line input from keyboard int main total for counter counter counter printf number fgets line sizeof line stdin sscanf line d current total current printf the grand total is d n total return note that counter goes from to ordinarily you count five items as but you will perform much better in c if you change your thinking to zero based counting and then count five items as one based counting is one of the main ca uses of array overflow errors see chapter careful examination of the two flavors of our program reveals the similarities between the two versions as seen in figure figure similarities between while and for many other programming languag es do not allow you to change the control variable in this case counter inside the loop c is not so picky you can change the control variable at any time you can jump into and out of the loop and generally do things that would make a pascal or fortran programmer cringe although c gives you the freedom to do such insane things that doesn t mean you should do them question when example runs it prints celsius fahrenheit and nothing more why click here for the answer section example cent cent c include stdio h this program produces a celsius to fahrenheit conversion chart for the numbers to the current celsius temperature we are working with int celsius int main for celsius celsius celsius printf celsius d fahrenheit d n celsius celsius return question example reads a list of five numbers and counts the number of and in the data why does it give us the wron g answers click here for the answer section example seven seven c include stdio h char line line of input int number of in the data int data the data to count and in int the number of in the data int index index into the data int main printf enter numbers n fgets line sizeof line stdin sscanf line d d d d d data data data data data for index index index if data index if data index printf threes d sevens d n return when we run this program with the data the results are threes sevens your results may vary switch statement the switch statement is similar to a chain of if else statements the general form of a switch statement is switch expression case statement break case statement fall through default statement break case statement break the switch statement evaluates the value of an expression and branches to one of the case labels duplicate labels are not allowed so only one case will be selected the expression must evaluate an integer character or enumeration the case labels can be in any order and must be constants th e default label can be put anywhere in the switch no two case labels can have the same value when c sees a switch statement it evaluates the expression and then looks for a matching case label if none is found the default label is used if no default is found the statement does nothing example contains a series of if and else statements example syntax for if and else if operator result value else if operator result value else if operator result value else if operator if value printf error divide by zero n printf operation ignored n else result value else printf unknown operator c n operator this section of code can easily be rewritten as a switch statement in this switch we use a different case for each operation the default clause takes care of all the illegal operators rewriting our program using a switch statement makes it not only simpler but easier to read our revised calc program is shown as example example calc3 c include stdio h char line line of text from input int result the result of the calculations char operator operator the user specified int value value specified after the operator int main result initialize the result loop forever or until break reached while printf result d n result printf enter operator and number fgets line sizeof line stdin sscanf line c d operator value if operator q operator q break switch operator case result value break case result value break case result value break case if value printf error divide by zero n printf operation ignored n else result value break default printf unknown operator c n operator break return a break statement inside a switch tells the computer to continue execution after the switch if a break statement is not there execution will continue with the next statement for example control a not so good example of programming switch control case printf reset n case printf initializing n break case printf working n in this case w hen control the program will print reset initializing case does not end with a break statement after printingreset the program falls through to the next statement case and prints initializing a problem exists with this syntax you cannot determine if the program is supposed to fall through from case to case or if the programmer forgot to put in a break statement in order to clear up this confusion a case section should always end with a break statement or the comment fall through as shown in the following example a better example of programming switch control case printf reset n fall through case printf initializing n break case printf working n because case is last it doesn t need abreak statement a break would cause the program to skip to the end of the switch and we re already there suppose we modify the program slightly and add another case to the switch we have a little problem switch control case printf reset n fall through case printf initializing n break case printf working n case printf closing down n now when control the program prints working closing down this result is an unpleasant surprise the problem is caused by the fact that case is no longer the last case we fall through unintentionally otherwise we would have included a fall through comment a break is now necessary if we always put in a break statement we don t have to worry about whether or not it is really needed almost there switch control case printf reset n fall through case printf initializing n break case printf working n break finally we ask the question what happens when control in this case because no matching case or default clause exists the entire switch statement is skipped in this example the programmer did not include a default statement because control will never be anything but or however variables can get assigned strange values so we need a little more defensive programming as shown in the following example the final version switch control case printf reset n fall through case printf initializing n break case printf working n break default printf internal error control value d impossible n control break although a default is not required it should be put in every switch even though the default may be default do nothing break it should be included this method indicates at the very least that you want to ignore out of range data switch break and continue the break statement has two uses used inside a switch break causes the program to go to the end of the switch inside a for or while loop break causes a loop exit thecontinue statement is valid only inside a loop continue will cause the program to go to the top of the loop figure illu strates both continue and break inside a switch statement the program in figure is designed to convert an integer with a number of different formats into different base if you want to know the value of an octal number you would enter o for octal and the number the command q is used to quit the program for example enter conversion and number o result is enter conversion and number q the help command is special because we don t want to print a number after the command after all the result of help is a few lines of text not a number so a continue is used inside the switch to start the loop at the beginning inside the switch the continue statement work on the loop while the break statement works on the switch there is one break outside the switch that isdesigned to let the user exit the program the control flow for this program can be seen in figure figure switch continue answers answer the problem lies with the semicolon at the end of the for statement the body of the for statement is between the closing parentheses and the semicolon in this case the body does not exist even though the printf statement is indented it is not part of the for statement the indentation is misleading the c compiler does not look at indentation the program does nothing until the expression celsius becomes false celsius then the printf is executed answer the problem is that we read the number into data through data in c the range o f legal array indices is to array size or in this case to data is illegal when we use it strange things happen in this case the variable is changed the solution is to only use data to data so we need to change the sscanf line to read sscanf line d d d d d data data data data data also the for loop must be changed from for index index index to for index index index programming exercises exercise print a checker board b y grid each square should be by characters wide a b y example follows exercise the total resistance of n resistors in parallel is suppose we have a network of two resistors with the values and then our equation would be substituting in the value of the resistors we get so the total resistance of our two resistor network is write a program to compute the total resistance for any number of parallel resistors exercise write a program to average n numbers exercise write a program to print out the multiplication table exercise write a program that reads a character and prints out whether or not it is a vowel or a consonant exercise write a program that converts numbers to words for example results in eight nine five exercise the number is pronounced eighty five not eight five modify the previous program to handle the numbers through so that all numbers come out as we really say them for example would be thirteen and would be one hundred chapter variable scope and functions but in the gross and scope of my opinion this bodes some strange eruption to our state shakesp eare hamlet act scene so far we have been using only global variables in this chapter we will learn about other kinds of variables and how to use them this chapter also tells you how to divide your code into functions scope and class all variables have two attributes scope and class the scope of a variable is the area of the program in which the variable is valid a global variable is valid everywhere hence the name global so its scope is the whole program a local variable has a scope that is limited to the block in which it is declared and cannot be accessed outside that block a block is a section of code enclosed in curly braces figure shows the difference between local and global variables figure local and global variables you can declare a local variable with the same name as a global v ariable normally the scope of the variablecount first declaration would be the whole program the declaration of a second local count takes precedence over the global declaration inside the small block in which the local count is declared in this blo ck the global count ishidden you can also nest local declarations and hide local variables figure illustrates a hidden variable figure hidden variables the variable count is declared as both a local variable and a global variable normally the scope of count global is the entire program however when a variable is declared inside a block that instance of the variable becomes the active one for the length of the block the global count has been hidden by the local count for the scope of this block the shaded block in the figure shows where the scope of count global is hidden a problem exists in that when you have the statement count you cannot tell easily to whichcount you are referring is it the global count the one declared at the top of main or the one in the middle of the while loop you should give these variables different names like and the class of a variable may be either permanent or temporary global variables are always permanent they are created and initialized before the program starts and rema in until it terminates temporary variables are allocated from a section of memory called the stack at the beginning of the block if you try to allocate too many temporary variables you will get a stack overflow error the space used by the temporary variables is returned to the stack at the end of the block each time the block is entered the temporary variables are initialized the size of the stack depends on the system and compiler you are using on many unix systems the program is automatically allocated the largest possible stack on other systems a default stack size is allocated that can be changed by a compiler switch on ms dos windows systems the stack space must be less than bytes this may seem like a lot of space however several large arrays can eat it up quickly you should consider making all large arrays permanent local variables are temporary unless they are declared static example illustrates the difference between permanent and temporary variables we have chosen obvious names temporary is a temporary variable while permanent is permanent the variable temporary is initialized each time it is created at the beginning of the for statement block the variable permanent is initialized only once at startup time in the loop both variables are incremented however at the top of the loop temporary is initialized to one as shown in example example vars vars c include stdio h int main int counter loop counter for counter counter counter int temporary a temporary variable static int permanent a permanent variable printf temporary d permanent d n temporary permanent temporary permanent return the output of this program is temporary permanent temporary permanent temporary permanent table describes the different ways in which a variable can be declared table declaration modifiers declared scope class initialized outside all blocks global permanent once static outside all blocks global permanent once inside a block local temporary each time block is entered static inside a block local permanent once a static declaration made outside blocks indicates the variable is local to the file in which it is declared see chapter for more information on programming with multiple files functions functions allow us to group commonly used code into a compact unit that can be used repeatedly we have already encountered one function main it is a special function called at the beginning of the program all other functions are directly or indirectly called from main suppose we want to write a program to compute the area of three triangles we could write out the formula three times or we could create a function to do the work each function should begin with a comment block containing the following name name of the function description description of what the function does parameters description of each of the parameters to the function returns description of the return value of the function additional sections may be added such as file formats references or notes r efer to chapter for other suggestions our function to compute the area of a triangle begins with triangle computes area of a triangle parameters width width of the triangle height height of the triangle returns area of the triangle the function proper begins with the line float triangle float width float height float is the function type the two parameters are width and height they are of type float also c uses a form of parameter passing called call by value when our procedure triangle is called with code such as triangle c copies the value of the parameters in this ca se and into the function parameters width and height and then starts executing the function code with this form of parameter passing a function cannot pass data back to the caller using parameters this statement is not strictly true we can trick c into passing information back through the use of pointers as we ll see in chapter the function computes the area with the statement area width height what left is to give the result to the caller this step is done with the return statement return area example shows our full triangle function example tri sub tri sub c include stdio h triangle computes area of a triangle parameters width width of the triangle height height of the triangle returns area of the triangle float triangle float width float height float area area of the triangle area width height return area the line size triangle is a call to the functiontriangle c assigns to the parameter width and to height if functions are the rooms of our building then parameters are the doors between the rooms in this case the value is going through the door marked width parameters doors are one way things can go in but they can t go out the return statement is how we get data out of the function in our triangle example the function assigns the local variable area the then executes the statement return area the return value of this function is so our statement size triangle assigns size the value example computes the area of three triangles example tri prog tri prog c file tri sub tri prog c include stdio h triangle computes area of a triangle parameters width width of the triangle height height of the triangle returns area of the triangle float triangle float width float heigh t float area area of the triangle area width height return area int main printf triangle f n triangle printf triangle f n triangle printf triangle f n triangle return if we want to use a function before we define it we must declare it just like a variable to inform the compiler about the function we use the declaration compute a triangle float triangle float width float height for the triangle function this declaration is called the function prototype the variable names are not required when declaring a function prototype our prototype could have just as easily been written as float triangle float float however we use the longer version because it gives the programmer additional information and it easy to create prototypes using the editor cut and paste functions strictly speaking the prototypes are optional for some functions if no prototype is specifie d the c compiler assumes the function returns an int and takes any number of parameters omitting a prototype robs the c compiler of valuable information that it can use to check function calls most compilers have a compile time switch that warns the programmer about function calls without prototypes functions with no parameters a function can have any number of parameters including none but even when using a function with no parameters you still need the parentheses value declaring a prototype for a function without parameters is a little tricky you can t use the statement int because the c compiler will see the empty parentheses and assume that this is a k r style function declaration see chapter for details on this older style the keyword void is used to indicate an empty parameter list so the prototype for our function is int void void is also used to indicate that a function does not return a value void is similar to the fortran subroutine or pascal procedure for example this function just prints a result it does not return a value void int answer if answer printf answer corrupt n return printf the answer is d n answer question example should compute the length of a string instead it insists that all strings are of length why click here for the answer section this function performs the same function as the library function strlen example len len c question why does this program always report the length of any string as a sample main has been provided it will ask for a string and then print the length include stdio h length computes the length of a string parameters string the string whose length we want returns the length of the string int length char string int index index into the string loop until we reach the end of string character for index string index index do nothing return index int main char line input line from user while printf enter line fgets line sizeof line stdin printf length including newline is d n length line structured programming computer scientists spend a great deal of time and effort studying how to program the result is that they come up with absolutely positively the best programming methodology a new one each month some of these systems include flow charts top down programming bottom up programming structured programming and object oriented design ood now that we have learned about functions we can talk about u sing structured programming techniques to design programs these techniques are ways of dividing up or structuring a program into small well defined functions they make the program easy to write and easy to understand i don t claim that this method is the absolute best way to program it happens to be the method that works best for me if another system works better for you use it the first step in programming is to decide what you are going to do this has already been described in chapter next decide how you are going to structure your data finally the coding phase begins when writing a paper you start with an outline of each section in the paper described by a single entence the details will be filled in later writing a program is a similar process you start with an outline and this becomes your main function the details can be hidden within other functions for example example solves all the world problems example solve the world problems int main init return of course some of the details will have to be filled in later start by writing the main function it should be less than three pages long if it grows longer consider splitting it up into two smaller simpler functions after the main function is complete you can start on the others this type o f structured programming is called top down programming you start at the top main and work your way down another type of coding is called bottom up programming this method involves writing the lowest level function first testing it and then building on that working set i tend to use some bottom up techniques when i m working with a new standard function that i haven t used before i write a small function to make sure that i really know how the function works and then continue from there this approach is used in chapter to construct the calculator program so in actual practice both techniques are useful a mostly top down partially bottom up technique results computer scientists have a term for this methodology chaos the one rule you should follow in programming is use what works best recursion recursion occurs when a function calls itself directly or indirectly some programming functions such as the factorial lend themselves naturally to recursive algorithms a recursive function must follow two basic rules it must have an ending point it must make the problem simpler a definition of factorial is fact fact n n fact n in c this definition is int fact int number if number return else return number fact number this definition satisfies our two rules first it has a definite ending point when number second it simplifies the problem because the calculation of fact number is simpler than fact number factorial is legal only for number but what happens if we try to compute fact the program will abort with a stack overflow or similar message fact calls fact which calls fact etc no ending point exists this error is referred to as an infinite recursion error many things that we do iteratively can be done recursively for example summing up the elements of an array we define a function to add eleme nts m n of an array as follows if we have only one element then the sum is simple otherwise we use the sum of the first element and the sum of the rest in c this function is int sum int first int last int array if first last return array first else return array first sum first last array for example sum sum 3 sum 3 3 sum 3 13 answer answers answer the programmer went to a lot of trouble to explain that the for loop did nothing except increment the index however there is no semicolon at the end of the fo r c keeps on reading until it sees a statement in this case return index and then puts that statement in the for loop properly done this program should look like example example len2 c include stdio h int length char string int index index into the string loop until we reach the end of string character for index string index index continue do nothing return index int main char line input line from user while printf enter line fgets line sizeof line stdin printf length including newline is d n length line programming exercises exercise write a procedure that counts the number of words in a string your documentation should describe exactly how you define a word write a program to test your new procedure exercise write a function begins that returns true if begins write a program to test the function exercise 3 write a function count number array length that counts the number of times number appears in array the array has length e lements the function should be recursive write a test program to go with the function exercise write a function that takes a character array and returns a primitive hash code by adding up the value of each character in the array exercise write a function that returns the maximum value of an array of numbers exercise write a function that scans a character array for the character and replaces it with chapter c preprocessor the speech of man is like embroidered tapestries since like them this has to be extended in order to display its patterns but when it is rolled up it conceals and distorts them themistocles in the early days when c was still being developed it soon became apparent that c needed a facility for handling named constants macros and include files the solution was to create a preprocessor that recognized these constructs in the programs before they were passed to the c compiler the preprocessor is nothing more than a specialized text editor its syntax is co mpletely different from that of c and it has no understanding of c constructs the preprocessor was very useful and soon it was merged into the main c compiler on some systems like unix the preprocessor is still a separate program automatically execu ted by the compiler wrapper cc some of the new compilers like turbo c and microsoft visual c have the preprocessor built in define statement example init ializes two arrays data and twice each array contains elements suppose we wanted to change the program to use elements then we would have to change the array size two places and the index limit one place aside from being a lot of work multiple changes can lead to errors example init2a c int data some data int twice twice some data int main int index index into the data for index index index data index index twice index index return we would like to be able to write a generic program in which we can define a constant for the size of the array and then let c adjust the dimensions of our two arrays by using the defi ne statement we can do just that example is a new version of example example init2b c define size work on elements int data size some data int twice size twice some data int main int index index into the data for index index size index data index index twice index index return the line define size acts as a command to a special text editor to globally change size to this line takes the drudgery and guesswork out of making changes all preprocessor commands begin with a hash mark in column one although c is free format the preprocessor is not and it depends on the hash mark being in the first column as we will see the preprocessor knows nothing about c it can be and is used to edit things other than c programs a preprocessor directive terminates at the end o f line this format is different from that of c where a semicolon is used to end a statement putting a semicolon at the end of a preprocessor directive can lead to unexpected results a line may be continued by putting a backslash at the end the simplest use of the preprocessor is to define a replacement macro for example the command define foo bar causes the preprocessor to re place the word foo with the word bar everywhere foo occurs it is common programming practice to use all uppercase letters for macro names this practice makes telling the difference between a variable all lowercase and a macro all uppercase very easy the general form of a simple define statement is define name substitute text where name can be any valid c identifier and substitute text can be anything you could use the following definition define for i i i and use it like this clear the array data i however defining macros in this manner is considered bad programming practice such definitions tend to obscure the basic control flow of the program in this example a programmer who wants to know what the loop does would have to search the beginning of the program for the definition of an even worse practice is to define macros that do large scale replacement of basic c programming constructs for example you can define the following define begin define end if index begin printf starting n end the problem is that you are no longer programming in c but in a half c half pascal mongrel you can find the extremes to which such mimicry can be taken in the bourne shell which uses preprocessor directives to define a language that looks a lot like algol here a sample section of code if x or y then case value of select start select 3 backspace otherwise error esac fi most programmers encountering this program curse at first and then use the editor to turn the source back into a reasonable version of c the preprocessor can cause unexpected problems because it does not check for correct c syntax for example example 3 generates an error on line example 3 big big c define 3 main index for our calculations int index index syntax error on next line while index index index 13 return 15 the problem is in the define statement on line but the error message points to line the definition in line causes the preprocessor to expand line to look like while index because is an illegal operator this expansion generates a syntax error question example generates the answer instead of the expected answer why see the hint below example first first c include stdio h define define define int main printf the square of all the parts is d n return hint the answer may not be readily apparent luckily c allows you to run your program through the preprocessor and view the output in unix the command cc e prog c will send the output of the preprocessor to the standard output in ms dos windows the command c cpp prog c will do the same thing running this program through the preprocessor gives us first c usr include stdio h listing of data in include file stdio h first c main printf the square of all the parts is d n return click here for the answer section question example generates a warning that counter is used before it is set this warning is a surprise to us because the for loop should set it we also get a very strange warning null effect for lin e example max max c warning spacing is very important 3 include stdio h define max int main int counter for counter max counter counter printf hi there n 13 return 15 hint take a look at the preprocessor output click here for the answer section question 3 example 6 computes the wrong value for size why click here for the answer section example 6 size size c include stdio h define size define fudge size 2 int main int size size to really use size fudge printf size is d n size return question example is supposed to print the message fatal error abort and exit when it receives bad data but when it gets good data it exits why click here for the answer se ction example die die c include stdio h 2 include stdlib h 3 define die fprintf stderr fatal error abort n exit 6 int main a random value for testing int value value 12 if value 13 die 14 15 printf we did not die n return 17 define vs const the const keyword is relatively new beforeconst define was the only keyword available to define constants so most older code uses define directives however the useof const is preferred over define for several reasons first of all c checks the syntax of const statements immediately the define directive is not checked until the macro is used also const use c syntax while the define has a syntax all its own finally const follows normal c scope rules while constantsdefined by a define directive continue on forever so in most cases aconst statement is preferred over define here are two ways of defining the same constant define max define a value using the preprocessor this definition can easily cause problems const int max define a c constant integer safer the define directive can only de fine simple constants the const statement can define almost any type of c constant including things like structure classes for example struct box int width height dimensions of the box in pixels const box 4 size of a pink box to be used for input the define directive is however essential for things like conditional compilation and other specialized uses 2 conditional compilation one of the problems programmers have is writing code that can work on many different machines in theory c code is portable in actual practice different operating systems have little quirks that must be accounted for for example this book covers both the ms dos windows compiler and unix c although they are almost the same there are differences especially when you must access the more advanced features of the operating system another portability problem is caused by the fact that the standard leaves some of the features of the language up to the implementers for example the size of an integer is implementation dependent the preprocessor through the use of conditional compilation allows the programmer great flexibility in changing the code generated suppose we want to put debugging code in the program while we are working on it and then remove it in the production version we could do so by including the code in a ifdef endif section ifdef debug printf in value d hash d n value hash endif debug if the beginning of the program contains the directive define debug turn debugg ing on the printf will be included if the program contains the directive undef debug turn debugging off the printf will be omitted strictly speaking the undef debug is unnecessary if there is no define debug statement then debug is undefined the undef debug statement is used to indicate explicitly that debug is used for conditional compilation and is now turned off the directive ifndef will cause the code to be compiled if the symbol is not defined ifndef debug printf pro duction code no debugging enabled n endif debug the else directive reverses the sense of the conditional for example ifdef debug printf test version debugging is on n else debug printf production version n endif debug a programmer may wish to remove a section of code temporarily one common method is to comment out the code by enclosing it in this method can cause problems as shown by the following example comment out this section 2 sec 3 handle the end of section stuff 4 end of commented out section this code generates a syntax error for the fifth line why a better method is to use the ifdef construct to remove the code ifdef undef handle the end of section stuff endif undef of course the code will be included if anyone defines the symbol undef however anyone who does so should be shot the compiler switch dsymbol allows symbols to be defined on the command line for example the command cc ddebug g o prog prog c compiles the program prog c and includes all the code in between ifdef debug and endif debug even though there is no define debug in the program the general form of the option is dsymbol or dsymbol value for example the following sets max to cc dmax o prog prog c notice that the programmer can override the command line options with directives in the program for example t he directive undef debug will result in debug being undefined whether or not you use ddebug most c compilers automatically define some system dependent symbols for example turbo c defines the symbols and the ansi standard compiler defines the symbol most unix compilers define a name for the system i e sun vax celerity etc however they are rarely documented the symbol is always defined for all unix machines 3 include files the include directive allows the program to use source code from another file for example we have been using the directive include stdio h in our programs this directive tells the preprocessor to take the file stdio h standard i o and insert it in the progra m files that are included in other programs are called header files most include directives come at the head of the program the angle brackets indicate that the file is a standard header file on unix these files are located in usr include on ms dos windows they are located in whatever directory was specified at the time the compiler was installed standard include files define data structures and macros used by library routines for example printf is a library routine that prints data on th e standard output the file structure used by printf and its related routines is defined in stdio h sometimes the programmer may want to write her own set of include files local include files are particularly useful for storing constants and data structures when a program spans several files they are especially useful for information passing when a team of programmers is working on a single project see chapter local inclu de files may be specified by using double quotes around the file name for example include defs h the filename defs h can be any valid filename this specification can be a simple file defs h a relative path data h or an absolute path root include const h on ms dos windows you should use backslash instead of slash as a directory separator include files may be nested and this feature can cause problems suppose you define several useful constants in the file const h if the files data h and io h both include const h and you put the following in your program include data h include io h you will generate errors because the preprocessor will set the definitions in const h twice defining a constant twice is not a fatal error however defining a data structure or union twice is a fatal error and must be avoided one way around this problem is to have const h check to see if it has already been included and does not define any symbol that has already been defined the directive ifndef symbol is true if the symbol is not defined the directive is the reverse of ifdef look at the following code ifndef define constants define endif when const h is included it defines the symbol if that symbol is already defined because the file was included earlier the ifndef conditional hides all defines so they don t cause trouble 4 parameterized macros so far we have discussed only simple defines or macros but macros can take parameters the following macro will compute the square of a number define sqr x x x square a number when used the macro will replace x by the text of the following argument sqr expands to 5 5 always put parenthese around the parameters of a macro example illustrates the problems that can occur if this rule is not followed example sqr sqr c include stdio h define sqr x x x int main int counter counter for loop for counter counter 5 counter printf x d x squared d n counter sqr counter return question 5 what does example output try running it on your machine why did it output what it did try checking the output of the preprocessor click here for the answer section the keep it simple system of programming tells us to use the increment and decrement operators only on line by themselves when used in a macro parameter they can lead to un expected results as illustrated by example example sqr i sqr i c include stdio h define sqr x x x int main int counter counter for loop counter while counter 5 printf x d square d n counter sqr counter return question 6 why will example 9 not produce the expected output by how much will the counter go up each time click here for the answer section question example tells us that we have an undefined variablenumber but our only variable name is counter click here for the answer section example rec rec c include stdio h define reciprocal number number int main float counter counter for our table for counter counter counter printf f f n counter reciprocal counter return 5 advanced features this book does not cover the complete list of c preprocessor directives among the more advanced features are an advanced form of the if directive for conditio nal compilations and the pragma directive for inserting compiler dependent commands into a file see your c reference manual for more information about these features 6 summary the c preprocessor is a very useful part of the c language it has a completely different look and feel though and it must be treated apart from the main c compiler problems in macro definitions often do not show up where the macro is defined but result in errors much further down in the program by following a few simple rules you can decrease the chances of having problems put parentheses around everything in particular they should enclose define constants and macro parameters when defining a macro with more than one statement enclose the code in curly braces the preprocessor is not c don t use and finally if you got this far be glad that the worst is over answers answer after the program has been run through the preprocessor the printf statement is expanded to look like printf the square of all the parts is d n 5 5 the equation 5 7 5 evaluates to put parentheses around all expressions in macros if you change the definition of to define first_part the program will execute correctly answer 2 the preprocessor is a very simple minded program when it defines a macro everything past the identifier is part of the macro in this case the definition of max is literally when the for statement is expanded the result is for counter counter 0 counter c allows you to compute a result and throw it away this will generate a null effect warning in some compilers for this statement the program checks to see if counter is and then discards the a nswer removing the from the definition will correct the problem answer 3 as with the previous problem the preprocessor does not respect c syntax conventions in this case the programmer used a semicolon to end the statement but the preprocessor included it as part of the definition for size the assignment statement for size when expanded is size 2 the two semicolons at the end do not hurt us but the one in the middle is the killer this line tells c to do two things assign to size compute the value 2 and throw it away this code results in the null effect warning removing the semicolons will fix the problem answer 4 the output of the preprocessor looks like void exit main int value value if value 0 printf fatal error abort n exit 8 printf we did not die n return 0 the problem is that two statements follow the if line normally they would be put on two lines let look at this progra m properly indented include stdio h include stdlib h main int value a random value for testing value if value 0 printf fatal error abort n exit 8 printf we did not die n return 0 with this new format we can easily determine why we always exit the fact that there were two statements after the if was hidden from us by using a single preprocessor macro the cure for this problem is to put curly braces a round all multistatement macros for example define die printf fatal error abort n exit 8 answer 5 the program prints x x squared x 2 x squared 3 x 3 x squared 5 x 4 x squared 7 x 5 x squared 9 the problem is with the sqr counter expression expanding this expression we get sqr counter 1 counter 1 counter 1 so our sqr macro does not work putting parentheses around the parameters solves this problem define sqr x x x answer 10 6 the answer is that the counter is incremented by two each time through the loop this incrementation occurs because the macro call sqr counter is expanded to counter counter answer 10 7 the only difference between a parameterized macro and one without parameters is the parenthesis immediately following the macro name in this case a space follows the definition of reciprocal so the macro is not parameterized instead it is a simple text replacement macro that will replace reciprocal with number 1 0 number remo ving the space between reciprocal and number will correct the problem brian w kernighan rob pike addison wesley boston san francisco new york toronto montreal london munich paris madrid capetown sydney tokyo singapore mexico city many of the designations used by manufacturers and sellers to distinguish their products as trademarks where those designations appear in this book and addison wesley were trademark claim the designations have been printed in initial capital letters or in all cap the author and publisher have taken care in the preparation of this book but make no implied warranty of any kind and assume no responsibility for errors or omissions no assumed for incidental or consequential damages in connection with or arising out of th information or programs contained herein the publisher offers discounts on this book when ordered in quantity for special sales f tion please contact pearson education corporate sales division w street indianapolis in 5331 corpsales pearsoned com visit aw on the web www awprofessional com this book was typeset grap i pi c i tbl ieqn itroff mpm in times and lucida sans typ authors library of congress cataloging in publication data kernighan brian w the practice of programming brian w kernighan rob pike p cm addison wesley professional computing series includes bibliographical references isbn x i computer programming i pike rob ii title iii series l cip copyright by lucent technologies all rights reserved no part of this publication may be reproduced stored in a retrieval transmitted in any form or by any means electronic mechanical photocopying record wise without the prior consent of the publisher printed in the united states of america simultaneously in canada isbn x text printed in the united states on recycled paper at rr donnelley in harrisonburg virginia twenty second printing february conten preface chapter style names expressions and statements consistency and idioms function macros magic numbers comments why bother chapter algorithms and data structures searching sorting libraries a java quicksort o notation growing arrays lists trees hash tables summary chapter design and implementation the markov chain algorithm data structure alternatives building the data structure in c generating output v vi the practice of programming java c awkandperl performance lessons chapter interfaces comma separated values a prototype library a library for others a c implementation interface principles resource management abort retry fail user interfaces chapter debugging debuggers good clues easy bugs no clues hard bugs last resorts non reproducible bugs debugging tools other people bugs summary chapter testing test as you write the code systematic testing test automation test scaffolds stress tests tips for testing who does the testing testing the markov program summary chapter performance a bottleneck timing and profiling strategies for speed tuning the code space efficiency the practice of programming estimation summary chapter portability language headers and libraries program organization isolation data exchange byte order 7 portability and upgrade internationalization summary chapter notation formatting data regular expressions programmable tools interpreters compilers and virtual machines programs that write programs using macros to generate code 7 compiling on the fly epilogue appendix collected rules index prefa have you ever wasted a lot of time coding the wrong algorithm used a data structure that was much too complicated tested a program but missed an obvious problem spent a day looking for a bug you should have found in five minutes needed to make a program run three times faster and use less memory struggled to move a program from a workstation to a pc or vice versa tried to make a modest change in someone else program rewritten a program because you couldn t understand it was it fun these things happen to programmers all the time but dealing with such pr is often harder than it should be because topics like testing debugging port performance design alternatives and style the practice of programming usually the focus of computer science or programming courses most progra learn them haphazardly as their experience grows and a few never learn them a in a world of enormous and intricate interfaces constantly changing tools a guages and systems and relentless pressure for more of everything one can los of the basic principles simplicity clarity generality that form the bedrock o software one can also overlook the value of tools and notations that mechaniz of software creation and thus enlist the computer in its own programming our approach in this book is based on these underlying interrelated prin which apply at all levels of computing these include simplicity which kee grams short and manageable clarity which makes sure they are easy to unde for people as well as machines generality which means they work well in a range of situations and adapt well as new situations arise and automation whi the machine do the work for us freeing us from mundane tasks by looking a puter programming in a variety of languages from algorithms and data str through design debugging testing and performance improvement we can ill ix x preface universal engineering concepts that are independent of language operating programming paradigm this book comes from many years of experience writing and maintaining software teaching programming courses and working with a wide variety grammers we want to share lessons about practical issues to pass on insig our experience and to suggest ways for programmers of all levels to be mo cient and productive we are writing for several kinds of readers if you are a student who ha programming course or two and would like to be a better programmer this expand on some of the topics for which there wasn t enough time in schoo write programs as part of your work but in support of other activities rathe the goal in itself the information will help you to program more effectively are a professional programmer who didn t get enough exposure to such school or who would like a refresher or if you are a software manager who guide your staff in the right direction the material here should be of value we hope that the advice will help you to write better programs the onl uisite is that you have done some programming preferably in c c or course the more experience you have the easier it will be nothing can take neophyte to expert in days unix and linux programmers will find som examples more familiar than will those who have used only windows and systems but programmers from any environment should discover things to m lives easier the presentation is organized into nine chapters each focusing on o aspect of programming practice chapter discusses programming style good style is so important to g gramming that we have chosen to cover it first well written programs are be badly written ones they have fewer errors and are easier to debug and to so it is important to think about style from the beginning this chapter al duces an important theme in good programming the use of idioms appropria language being used algorithms and data structures the topics of chapter are the core of puter science curriculum and a major part of programming courses since m ers will already be familiar with this material our treatment is intended a review of the handful of algorithms and data structures that show up in alm program more complex algorithms and data structures usually evolve fro building blocks so one should master the basics chapter describes the design and implementation of a small program t trates algorithm and data structure issues in a realistic setting the program mented in five languages comparing the versions shows how the same data are handled in each and how expressiveness and performance vary across a of languages prefac interfaces between users programs and parts of programs are fundamental gramming and much of the success of software is determined by how well int are designed and implemented chapter shows the evolution of a small libr parsing a widely used data format even though the example is small it illu many of the concerns of interface design abstraction information hiding management and error handling much as we try to write programs correctly the first time bugs and th debugging are inevitable chapter gives strategies and tactics for systema effective debugging among the topics are the signatures of common bugs importance of numerology where patterns in debugging output often where a problem lies testing is an attempt to develop a reasonable assurance that a program is correctly and that it stays correct as it evolves the emphasis in chapter is tematic testing by hand and machine boundary condition tests probe at weak spots mechanization and test scaffolds make it easy to do extensive with modest effort stress tests provide a different kind of testing than typica do and ferret out a different class of bugs computers are so fast and compilers are so good that many programs a enough the day they are written but others are too slow or they use too much ory or both chapter 7 presents an orderly way to approach the task of making gram use resources efficiently so that the program remains correct and sound made more efficient chapter 8 covers portability successful programs live long enough th environment changes or they must be moved to new systems or new hardware countries the goal of portability is to reduce the maintenance of a program b mizing the amount of change necessary to adapt it to a new environment computing is rich in languages not just the general purpose ones that we the bulk of programming but also many specialized languages that focus on domains chapter presents several examples of the importance of notation i puting and shows how we can use it to simplify programs to guide implemen and even to help us write programs that write programs to talk about programming we have to show a lot of code most of the ex were written expressly for the book although some small ones were adapte other sources we ve tried hard to write our own code well and have tested it a dozen systems directly from the machine readable text more information i able at the web site for the practice of programming http tpop awl com the majority of the programs are in c with a number of examples in c java and some brief excursions into scripting languages at the lowest level c are almost identical and our c programs are valid c programs as wel and java are lineal descendants of c sharing more than a little of its syntax an of its efficiency and expressiveness while adding richer type systems and li xii preface in our own work we routinely use all three of these languages and many oth choice of language depends on the problem operating systems are best writ efficient and unrestrictive language like c or c quick prototypes are ofte in a command interpreter or a scripting language like awk or perl for user visual basic and tcl tk are strong contenders along with java there is an important pedagogical issue in choosing a language for our just as no language solves all problems equally well no single language is presenting all topics higher level languages preempt some design decision use a lower level language we get to consider alternative answers to the ques exposing more of the details we can talk about them better experience sh even when we use the facilities of high level languages it invaluable to k they relate to lower level issues without that insight it easy to run into perf problems and mysterious behavior so we will often use c for our examp though in practice we might choose something else for the most part however the lessons are independent of any particular ming language the choice of data structure is affected by the language at ha may be few options in some languages while others might support a variety o tives but the way to approach making the choice will be the same the how to test and debug are different in different languages but strategies an are similar in all most of the techniques for making a program efficien applied in any language whatever language you write in your task as a programmer is to do the can with the tools at hand a good programmer can overcome a poor langu clumsy operating system but even a great programming environment will n a bad programmer we hope that no matter what your current experience this book will help you to program better and enjoy it more we are deeply grateful to friends and colleagues who read drafts of the and gave us many helpful comments jon bentley russ cox john lakos derman peter memishian ian lance taylor howard trickey and chris v read the manuscript some more than once with exceptional care and thoro we are indebted to tom cargill chris cleeland steve dewhurst eric andrew herron gerard holzmann doug mcilroy paul mcnamee peter dennis ritchie rich stevens tom szymanski kentaro toyama john wai c wang peter weinberger margaret wright and cliff young for invalua ments on drafts at various stages we also appreciate good advice and though gestions from al aho ken arnold chuck bigelow joshua bloch bill bob flandrena renee french mark kernighan andy koenig sape mulle nemeth marty rabinowitz mark v shaney bjame stroustrup ken thomp phil wadler thank you all brian w kern rob pike design and implementati show me your flowcharts and conceal your tables and i sha tinue to be mystified show me your tables and i won t need your flowcharts they ll be obvious frederick p brooks jr the mythical man as the quotation from brooks classic book suggests the design of the dat tures is the central decision in the creation of a program once the data structu laid out the algorithms tend to fall into place and the coding is comparatively this point of view is oversimplified but not misleading in the previous we examined the basic data structures that are the building blocks of most pro in this chapter we will combine such structures as we work through the desi implementation of a modest sized program we will show how the problem ences the data structures and how the code that follows is straightforward o have the data structures mapped out one aspect of this point of view is that the choice of programming language atively unimportant to the overall design we will design the program in the and then write it in c java c awk and perl comparing the implemen demonstrates how languages can help or hinder and ways in which they are un tant program design can certainly be colored by a language but is not usually nated by it the problem we have chosen is unusual but in basic form it is typical of programs some data comes in some data goes out and the processing depen little ingenuity specifically we re going to generate random english text that reads well emit random letters or random words the result will be nonsense for example gram that randomly selects letters and blanks to separate words might produc xptmxgn xusaja afqnzgxl lhidlwcd rjdjuvpydrlwnjy which is not very convincing if we weight the letters by their frequency o ance in english text we might get this idtefoae tcs trder jcii ofdslnqetacp t ola which isn t a great deal better words chosen from the dictionary at rando make much more sense polydactyl equatorial splashily jowl verandah circumscrib for better results we need a statistical model with more structure such as quency of appearance of whole phrases but where can we find such statistic we could grab a large body of english and study it in detail but there is and more entertaining approach the key observation is that we can use any text to construct a statistical model of the language as used in that text and generate random text that has similar statistics to the original the markov chain algorithm an elegant way to do this sort of processing is a technique called a mark algorithm if we imagine the input as a sequence of overlapping phrases rithm divides each phrase into two parts a multi word prefix and a single su that follows the prefix a markov chain algorithm emits output phrases by choosing the suffix that follows the prefix according to the statistics of in the original text three word phrases work well a two word prefix is used the suffix word set w and w to the first two words in the text print w and loop randomly choose w one of the successors of prefix w w in th print replace w and w by w and w repeat loop to illustrate suppose we want to generate random text based on a few senten phrased from the epigraph above using two word prefixes show your flowcharts and conceal your tables and i will mystified show your tables and your flowcharts will be obvious end these are some of the pairs of input words and the words that follow them input prefix show your your flowcharts flowcharts and flowcharts wi your tables wi be be mystified be obvious suffix words that follow flowcharts tables and will conceal be and and mystified obvious show end a markov algorithm processing this text will begin by printing show your a then randomly pick either flowcharts or tables if it chooses the former t rent prefix becomes your flowcharts and the next word will be and or wil chooses tab es the next word will be and this continues until enough out been generated or until the end marker is encountered as a suffix our program will read a piece of english text and use a markov chain algori generate new text based on the frequency of appearance of phrases of a fixed the number of words in the prefix which is two in our example is a para making the prefix shorter tends to produce less coherent prose making it longe to reproduce the input text verbatim for english text using two words to third is a good compromise it seems to recreate the flavor of the input while its own whimsical touch what is a word the obvious answer is a sequence of alphabetic characters is desirable to leave punctuation attached to the words so words and word different this helps to improve the quality of the generated prose by letting pu tion and therefore indirectly grammar influence the word choice although permits unbalanced quotes and parentheses to sneak in we will therefore d word as anything between white space a decision that places no restrict input language and leaves punctuation attached to the words since most pr ming languages have facilities to split text into white space separated words also easy to implement because of the method all words all two word phrases and all thre phrases in the output must have appeared in the input but there should be man word and longer phrases that are synthesized here are a few sentences produ the program we will develop in this chapter when given the text of chapter the sun also rises by ernest hemingway as i started up the undershirt onto his chest black and big stomach mus cles bulging under the light you see them below the line where hi ribs stopped were two raised white welts see on the forehead oh brett i love you let not talk talking all bilge i m going away tomorrow tomorrow yes didn t i say so i am let have drink then we were lucky here that punctuation came out correctly that need not happen data structure alternatives how much input do we intend to deal with how fast must the program seems reasonable to ask our program to read in a whole book so we shoul pared for input sizes of n words or more the output will be hun perhaps thousands of words and the program should run in a few seconds minutes with words of input text n is fairly large so the algorithms too simplistic if we want the program to be fast the markov algorithm must see all the input before it can begin to gene put so it must store the entire input in some form one possibility is to whole input and store it in a long string but we clearly want the input brok into words if we store it as an array of pointers to words output generation i to produce each word scan the input text to see what possible suffix words f prefix that was just emitted and then choose one at random however th scanning all input words for each word we generate words means hundreds of millions of string comparisons which will not be fast another possibility is to store only unique input words together with where they appear in the input so that we can locate successor words more we could use a hash table like the one in chapter but that version doesn address the needs of the markov algorithm which must quickly locate all the of a given prefix we need a data structure that better represents a prefix and its associated the program will have two passes an input pass that builds the data structu senting the phrases and an output pass that uses the data structure to generate dom output in both passes we need to look up a prefix quickly in the inp update its suffixes and in the output pass to select at random from the poss fixes this suggests a hash table whose keys are prefixes and whose value sets of suffixes for the corresponding prefixes for purposes of description we ll assume a two word prefix so each out is based on the pair of words that precede it the number of words in t doesn t affect the design and the programs should handle any prefix length b ing a number makes the discussion concrete the prefix and the set of all its suffixes we ll call a state which is standard terminology for markov algorith given a prefix we need to store all the suffixes that follow it so we ca them later the suffixes are unordered and added one at a time we don t k many there will be so we need a data structure that grows easily and efficien as a list or a dynamic array when we are generating output we need to b choose one suffix at random from the set of suffixes associated with a partic fix items are never deleted what happens if a phrase appears more than once for example migh twice might appear twice but might appear once only once this could sented by putting twice twice in the suffix list for might appear or by put once with an associated counter set to we ve tried it with and without without is easier since adding a suffix doesn t require checking whether it already and experiments showed that the difference in run time was negligible in summary each state comprises a prefix and a list of suffixes this infor is stored in a hash table with prefix as key each prefix is a fixed size set of if a suffix occurs more than once for a given prefix each occurrence will be in separately in the list the next decision is how to represent the words themselves the easy wa store them as individual strings since most text has many words appearing times it would probably save storage if we kept a second hash table of single so the text of each word was stored only once this would also speed up has prefixes since we could compare pointers rather than individual characters strings have unique addresses we ll leave that design as an exercise for now will be stored individually building the data structure in c let begin with a c implementation the first step is to define some consta enum npref nhash maxgen i number of prefix words i size of state hash table array i maximum words generated this declaration defines the number of words npref for the prefix the size hash table array nhash and an upper limit on the number of words to maxgen if npref is a compile time constant rather than a run time variable management is simpler the array size is set fairly large because we expect the program large input documents perhaps a whole book we chose nhash so that if the input has 000 distinct prefixes word pairs the average chain very short two or three prefixes the larger the size the shorter the expected of the chains and thus the faster the lookup this program is really a toy so t formance isn t critical but if we make the array too small the program will not our expected input in reasonable time on the other hand if we make it too might not fit in the available memory the prefix can be stored as an array of words the elements of the hash tab be represented as a state data type associating the suffix list with the prefix typedef struct state state typedef struct suffix suffix struct state i prefix suffix list i char pref npref prefix words suffix suf i list of suffixes i state next i next in hash table i struct suffix f list of suffixes char word f suffix suffix next f next in list of suffixes state statetab nhash hash table of states i pictorially the data structures look like this statetab a state t pref o show pref l your suf next another state pref o pref suf word next flowcharts another suffix word next ta we need a hash function for prefixes which are arrays of strings it is modify the string hash function from chapter to loop over the strings in t thus in effect hashing the concatenation of the strings hash compute hash value for array of npref strings unsigned int hash char npref unsigned int h unsigned char p int i h o for i o i npref i for p unsigned char i p o p h multiplier h p return h nhash a similar modification to the lookup routine completes the implementati hash table lookup search for prefix create if requested i i returns pointer if present or created null if not i f creation doesn t strdup so strings mustn t change late state lookup char prefix npref int create int i h state sp h hash prefix for sp statetab h sp null sp sp next for i o i npref i if strcmp prefix i sp pref i break if i npref found it i return sp if create sp state emalloc sizeof state for i o i npref i sp pref i prefix i sp suf null sp next statetab h statetab h sp return sp notice that lookup doesn t make a copy of the incoming strings when it creates state it just stores pointers in sp pref callers of lookup must guarantee t data won t be overwritten later for example if the strings are in an bu copy must be made before lookup is called otherwise subsequent input could write the data that the hash table points to decisions about who owns a re shared across an interface arise often we will explore this topic at length in th chapter next we need to build the hash table as the file is read build read input build prefix table i void build char prefix npref file f char buf loo fmt lo create a format string could overflow buf i sprintf fmt ds sizeof buf while fscanf f fmt buf eof add prefix estrdup buf the peculiar call to spri ntf gets around an irritating problem with fscanf is otherwise perfect for the job a call to fscanf with format will read th white space delimited word from the file into the buffer but there is no limit o a long word might overflow the input buffer wreaking havoc if the buffer bytes long which is far beyond what we expect ever to appear in normal text use the format leaving one byte for the terminal which tells stop after bytes a long word will be broken into pieces which is unfort safe we could declare enum bufsize char fmt bufsize but that requires two constants for one arbitrary decision the size of the bu introduces the need to maintain their relationship the problem can be sol and for all by creating the format string dynamically with spri ntf so approach we take the two arguments to build are the prefix array holding the previo words of input and a file pointer it passes the prefix and a copy of the in to add which adds the new entry to the hash table and advances the prefix i add add word to suffix list update prefix i void add char prefix npref char suffix state sp sp lookup prefix i create if not found i addsuffix sp suffix move the words down the prefix i memmove prefix prefix l npref l sizeof prefix prefix npref suffix the call to memmove is the idiom for deleting from an array it shifts el through npref in the prefix down to positions o through npref deleting prefix word and opening a space for a new one at the end the addsuffi x routine adds the new suffix i addsuffix add to state suffix must not change later void addsuffix state sp char suffix suffix suf suf suffix emalloc sizeof suffix suf word suffix suf next sp suf sp suf suf we split the action of updating the state into two functions add performs th service of adding a suffix to a prefix while addsuffi x performs the implem specific action of adding a word to a suffix list the add routine is used by b addsuffi x is used internally only by add it is an implementation detail th change and it seems better to have it in a separate function even though it is only one place generating output with the data structure built the next step is to generate the output the ba is as before given a prefix select one of its suffixes at random print it then the prefix this is the steady state of processing we must still figure out how and stop the algorithm starting is easy if we remember the words of the first and begin with them stopping is easy too we need a marker word to termin algorithm after all the regular input we can add a terminator a word that i anteed not to appear in any input build prefix stdin add prefix should be some value that will never be encountered in regular input the input words are delimited by white space a word of white space will such as a newline character char n cannot appear as real word i one more worry what happens if there is insufficient input to start the algo there are two approaches to this sort of problem either exit prematurely if insufficient input or arrange that there is always enough and don t bother to in this program the latter approach works well we can initialize building and generating with a fabricated prefix which tees there is always enough input for the program to prime the loops initial prefix array to be all words this has the nice benefit that the first the input file will be the first suffix of the fake prefix so the generation loop n print only the suffixes it produces in case the output is unmanageably long we can terminate the algorith some number of words are produced or when we hit as a suffix whi comes first adding a few to the ends of the data simplifies the main pro loops of the program significantly it is an example of the technique of adding values to mark boundaries as a rule try to handle irregularities and exceptions and special cases i code is harder to get right so the control flow should be as simple and regular sible the generate function uses the algorithm we sketched originally it pr one word per line of output which can be grouped into longer lines with a wo cessor chapter shows a simple formatter called frnt for this task with the use of the initial and final strings generate starts an properly generate produce output one word per line void generate int nwords state sp suffix suf char prefix npref w inti nmatch for i o i npref i reset initial prefix prefix i nonword for i o i nwords i sp lookup prefix o nmatch o for suf sp suf suf null suf suf next if rand nmatch prob nmat w suf word if strcmp w nonword o break pri ntf n w memmove prefix prefix l npref l sizeof prefi prefix npref w notice the algorithm for selecting one item at random when we don t know h items there are the variable nmatch counts the number of matches as t scanned the expression rand nmatch increments nmatch and is then true with probability nmatch thus the firs ing item is selected with probability the second will replace it with probab the third will replace the survivor with probability and so on at any ti one of the k matching items seen so far has been selected with probability k at the beginning we set the prefix to the starting value which is guar be installed in the hash table the first suffix values we find will be the fir of the document since they are the unique follow on to the starting prefix a random suffixes will be chosen the loop calls lookup to find the hash table the current prefix then chooses a random suffix prints it and advances the if the suffix we choose is nonword we re done because we have chosen that corresponds to the end of the input if the suffix is not nonword we prin drop the first word of the prefix with a call to memmove promote the suffix last word of the prefix and loop now we can put all this together into amain routine that reads the stand and generates at most a specified number of words markov main markov chain random text generation int main void inti nwords maxgen char prefix npref current input prefix for i i npref i setup initial prefix prefix i build prefix stdin add prefix generate nwords return this completes our c implementation we will return at the end of the cha a comparison of programs in different languages the great strengths of c are gives the programmer complete control over implementation and programs wr it tend to be fast the cost however is that the c programmer must do more work allocating and reclaiming memory creating hash tables and linked lists like c is a razor sharp tool with which one can create an elegant and efficie gram or a bloody mess exercise the algorithm for selecting a random item from a list of un length depends on having a good random number generator design and ca experiments to determine how well the method works in practice d exercise if each input word is stored in a second hash table the text i stored once which should save space measure some documents to estimat much this organization would allow us to compare pointers rather than strings hash chains for prefixes which should run faster implement this version an sure the change in speed and memory consumption d exercise remove the statements that place sentinel at the beg and end of the data and modify generate so it starts and stops properly them make sure it produces correct output for input with and compare this implementation to the version using sentinels d 5 java our second implementation of the markov chain algorithm is in java oriented languages like java encourage one to pay particular attention to the int between the components of the program which are then encapsulated as indep data items called objects or classes with associated functions called methods java has a richer library than c including a set of container classes to group ing objects in various ways one example is a vector that provides a dynam growable array that can store any object type another example is the has class with which one can store and retrieve values of one type using another type as keys in our application vectors of strings are the natural choice to hold pre suffixes we can use a hashtable whose keys are prefix vectors and who are suffix vectors the terminology for this type of construction is a map fixes to suffixes in java we need no explicit state type because hashtabl itly connects maps prefixes to suffixes this design is different from the c in which we installed state structures that held both prefix and suffix list an on the prefix to recover the full state a hashtable provides a put method to store a key value pair and a get to retrieve the value for a key hashtable h new hashtable h put key value sometype v sometype h get key our implementation has three classes the first class prefix holds the the prefix class prefix public vector pref npref adjacent words from inp the second class chain reads the input builds the hash table and gene output here are its class variables class chain static final int npref size of prefix static final string n word that can t appear hashtable statetab new hashtable key prefix value suffix vect prefix prefix new prefix npref initial prefix random rand new random the third class is the public interface it holds main and instantiates a cha class markov static final int maxgen maximum words ge public static void main string args throws ioexce chain chain new chain int nwords maxgen chain build system in chain generate nwords when an instance of class chain is created it in tum creates a hash table a up the initial prefix of npref nonwords the build function uses the library st reamtokeni ze r to parse the input into words separated by white space char the three calls before the loop set the tokenizer into the proper state for our def of word ii chain build build state table from input stream void build inputstream in throws ioexception streamtokenizer st new streamtokenizer in st resetsyntax ii remove defaul st wordchars o character ii turn on all st whitespacechars o while st nexttoken st add st sval add nonword ii except up to the add function retrieves the vector of suffixes for the current prefix fr hash table if there are none the vector is null add creates a new vector and prefix to store in the hash table in either case it adds the new word to the suff tor and advances the prefix by dropping the first word and adding the new wor end ii chain add add word to suffix list update prefix void add string word vector suf vector statetab get prefix if suf null suf new vector statetab put new prefix prefix suf suf addelement word prefix pref removeelementat o prefix pref addelement word notice that if suf is null add installs a new prefix in the hash table rath prefix itself this is because the hashtable class stores items by reference we don t make a copy we could overwrite data in the table this is the sam that we had to deal with in the c program the generation function is similar to the c version but slightly more co because it can index a random vector element directly instead of looping thr list ii chain generate generate output words void generate int nwords prefix new prefix npref nonword for inti o i nwords i vectors vector statetab get prefix int r math abs rand nextint size string suf string elementat r if suf equals nonword break system out println suf prefix pref removeelementat o prefix pref addelement suf the two constructors of prefix create new instances from supplied data copies an existing prefix and the second creates a prefix from n copies of we use it to make npref copies of nonword when initializing ii prefix constructor duplicate existing prefix prefix prefix p pref vector p pref clone ii prefix constructor n copies of str prefix int n string str pref new vector for inti o i n i pref addelement str prefix also has two methods hashcode and equals that are called imp the implementation of hashtabl e to index and search the table it is the nee an explicit class for these two methods for hashtabl e that forced us to mak a full fledged class rather than just a vector like the suffix the hashcode method builds a single hash value by combining th hashcodes for the elements of the vector static final int multiplier ii for hashcode ii prefix hashcode generate hash from all prefix words public int hashcode int h o for inti o i pref size i h multiplier h pref elementat i hashcode return h and equals does an elementwise comparison of the words in two prefixes prefix equals compare two prefixes for equal words public boolean equals object o prefix p prefix o for inti o i pref size i if pref elementat i equals p pref elementat i return false return true the java program is significantly smaller than the c program and takes more details vectors and the hashtabl e are the obvious examples in genera age management is easy since vectors grow as needed and garbage collection care of reclaiming memory that is no longer referenced but to use the has class we still need to write functions hashcode and equals so java isn t taki of all the details comparing the way the c and java programs represent and operate on th basic data structure we see that the java version has better separation of functio for example to switch from vectors to arrays would be easy in the c everything knows what everything else is doing the hash table operates on arra are maintained in various places lookup knows the layout of the state and structures and everyone knows the size of the prefix array java markov txt i fmt wash the blackboard watch it dry the water goes into the air when water goes into the air it evaporates tie a damp cloth to one end of a solid or liquid look around what are the solid things chemical changes take place when something burns if the burning material has liquids they are stable and the sponge rise it looked like dough but it is burning break up the lump of sugar into small pieces and put them together again in the bottom of a liquid exercise 4 revise the java version of markov to use an array instead of a for the prefix in the state class d 6 c our third implementation is in c since c is almost a superset of be used as if it were c with a few notational conveniences and our original c of markov is also a legal c program a more appropriate use of c would be to define classes for the objects in the program more or less as w java this would let us hide implementation details we decided to go even f using the standard template library or stl since the stl has built in mec that will do much of what we need the iso standard for c includes th part of the language definition the stl provides containers such as vectors lists and sets and a family o mental algorithms for searching sorting inserting and deleting using the features of c every stl algorithm works on a variety of containers includ user defined types and built in types like integers containers are expressed templates that are instantiated for specific data types for example there is a container that can be used to make particular types like vector vector stri ng all vector operations including standard algorithms for can be used on such data types in addition to a vector container that is similar to java vector the vides a deque container a deque pronounced deck is a double ended q matches what we do with prefixes it holds npref elements and lets us pop element and add a new one to the end in time for both the stl deque general than we need since it permits push and pop at either end but the perf guarantees make it an obvious choice the stl also provides an explicit map container based on balanced tr stores key value pairs and provides logn retrieval of the value associated key maps might not be as efficient as hash tables but it nice not to write any code whatsoever to use them some non standard c libraries hash or container whose performance may be better we also use the built in comparison functions which in this case will comparisons using the individual strings in the prefix with these components in hand the code goes together smoothly her declarations typedef deque string prefix map prefix vector string statetab prefix suffi the stl provides a template for deques the notation deque stri ng special a deque whose elements are strings since this type appears several times in gram we used a typedef to give it the name prefix the map type that st fixes and suffixes occurs only once however so we did not give it a separa the map declaration declares a variable statetab that is a map from prefixe tors of strings this is more convenient than either c or java because we do to provide a hash function or equals method the main routine initializes the prefix reads the input from standard input cin in the c iostream library adds a tail and generates the output exactl the earlier versions markov main markov chain random text generation int main void int nwords maxgen prefix prefix current input prefi for inti o i npref i setup initial pref add prefix nonword build prefix cin add prefix nonword generate nwords return o the function build uses the i ostream library to read the input one wo time build read input words build state table void build prefix prefix istream in string buf while in buf add prefix buf the string buf will grow as necessary to handle input words of arbitrary length the add function shows more of the advantages of using the stl add add word to suffix list update prefix void add prefix prefix canst string if prefix size npref statetab prefix prefix prefix quite a bit is going on under these apparently simple statements the map co overloads subscripting the operator to behave as a lookup operation the sion statetab prefix does a lookup in statetab with prefix as key and re reference to the desired entry the vector is created if it does not exist alread member functions of vector and deque push a new string onto th end of the vector or deque pops the first element off the deque generation is similar to the previous versions ii generate produce output one word per line void generatecint nwords prefix prefix int i for ci o i npref i ii reset initial prefix addcprefix nonword for ci o i nwords i vector string suf statetab prefix canst string w suf randc suf sizec j if cw nonword break cout w n prefix prefix ii advance overall this version seems especially clear and elegant the code is com data structure is visible and the algorithm is completely transparent sadly price to pay this version runs much slower than the original c version tho not the slowest we ll come back to performance measurements shortly exercise 5 the great strength of the stl is the ease with which one ca ment with different data structures modify the c version of markov to us structures to represent the prefix suffix list and state table how does perf change for the different structures d exercise 6 write a c version that uses only classes and the string but no other advanced library facilities compare it in style and speed to the sions o 7 awk and perl to round out the exercise we also wrote the program in two popular scrip guages awk and perl these provide the necessary features for this applicati ciative arrays and string handling an associative array is a convenient packaging of a hash table it look array but its subscripts are arbitrary strings or numbers or comma separate them it is a form of map from one data type to another in awk all arrays ciative perl has both conventional indexed arrays with integer subscripts and tive arrays which are called hashes a name that suggests how they ar mented the awk and perl implementations are specialized to prefixes of length markov awk markov chain algorithm for word prefixes begin maxgen n wl for i i nf i read all words statetab wl nsuffix wl i wl i end statetab wl nsuffix wl add tail wl for i i maxgen i generate r int rand nsuffix wl nsuffix p statetab wl r if p exit print p wl p advance chain awk is a pattern action language the input is read a line at a time each matched against the patterns and for each match the corresponding action is exe there are two special patterns begin and end that match before the first line o and after the last an action is a block of statements enclosed in braces in the awk version o kov the begin block initializes the prefix and a couple of other variables the next block has no pattern so by default it is executed once for each inp awk automatically splits each input line into fields white space delimited called through nf the variable nf is the number of fields the statement statetab wl nsuffix wl i builds the map from prefix to suffixes the array nsuffi x counts suffixes a element nsuffi x wl counts the number of suffixes associated with that the suffixes themselves are stored in array elements statetab wl statetab wl and so on when the end block is executed all the input has been read at that poi each prefix there is an element of nsuffi x containing the suffix count and th that many elements of statetab containing the suffixes the perl version is similar but uses an anonymous array instead of a thir script to keep track of suffixes it also uses multiple assignment to update the perl uses special characters to indicate the types of variables marks a scalar an indexed array while brackets are used to index arrays and braces to hashes markov pl markov chain algorithm for word prefixes maxgen n wl while foreach split initial state read each line of inpu push statetab wl multiple push statetab nonword wl for i i maxgen i assignment add tail suf statetab array reference r int rand suf suf is number of ele exit if t suf r eq n0nword print t n wl t advance chain as in the previous programs the map is stored using the variable statet heart of the program is the line push statetab which pushes a new suffix onto the end of the anonymous array statetab w2 in the generation phase statetab w2 is ence to an array of suffixes and suf r points to the r th suffix both the perl and awk programs are short compared to the three earlier but they are harder to adapt to handle prefixes that are not exactly two wo core of the c stl implementation the add and generate functions is of ble length and seems clearer nevertheless scripting languages are often choice for experimental programming for making prototypes and even for tion use if run time is not a major issue exercise 7 modify the awk and perl versions to handle prefixes of an experiment to determine what effect this change has on performance d 8 performance we have several implementations to compare we timed the program book of psalms from the king james bible which has words 5 238 words prefixes this text has enough repeated phrases blessed is that one suffix list has more than elements and there are a few hundred with dozens of suffixes so it is a good test data set blessed is the man of the net tum thee unto me and raise me up that may tell all my fears they looked unto him he heard my praise shal be blessed wealth and riches shall be saved thou hast dealt well with thy hid treasure they are cast into a standing water the flint into a stand ing water and dry ground into watersprings the times in the following table are the number of seconds for generating words of output one machine is a mips running lrix 6 4 a other is a pentium ii with megabytes of memory running windo run time is almost entirely determined by the input size generation is very comparison the table also includes the approximate program size in lines of code pentium ii lines of source code c sec 30 sec java 4 9 c stl deque 6 c stl list 7 5 awk 2 2 2 perl 8 0 the c and c versions were compiled with optimizing compilers while th runs had just in time compilers enabled the irix c and c times are the obtained from three different compilers similar results were observed on sun and dec alpha machines the c version of the program is fastest by a large perl comes second the times in the table are a snapshot of our experience with ticular set of compilers and libraries however so you may see very different re your environment something is clearly wrong with the stl deque version on windows ments showed that the deque that represents the prefix accounts for most of t time although it never holds more than two elements we would expect the data structure the map to dominate switching from a deque to a list whi doubly linked list in the stl improves the time dramatically on the other switching from a map to a non standard hash container made no difference o hashes were not available on our windows machine it is a testament to the mental soundness of the stl design that these changes required only substituti word list for the word deque or hash for map in two places and recompilin conclude that the stl which is a new component of c still suffers from im implementations the performance is unpredictable between implementations stl and between individual data structures the same is true of java where mentations are also changing rapidly there are some interesting challenges in testing a program that is mean duce voluminous random output how do we know it works at all how do it works all the time chapter 6 which discusses testing contains some su and describes how we tested the markov programs 9 lessons the markov program has a long history the first version was written b mitchell adapted by bruce ellis and applied to humorous deconstructionist throughout the it lay dormant until we thought to use it in a universit as an illustration of program design rather than dusting off the original w it from scratch in c to refresh our memories of the various issues that arise wrote it again in several other languages using each language unique express the same basic idea after the course we reworked the programs ma to improve clarity and presentation over all that time however the basic design has remained the same th version used the same approach as the ones we have presented here althou employ a second hash table to represent individual words if we were to again we would probably not change much the design of a program is root layout of its data the data structures don t define every detail but they do overall solution some data structure choices make little difference such as lists versus arrays some implementations generalize better than others the perl and a could be readily modified to one or three word prefixes but parameteri choice would be awkward as befits object oriented languages tiny chang c and java implementations would make the data structures suitable fo other than english text for instance programs where white space would b cant or notes of music or even mouse clicks and menu selections for gener sequences of course while the data structures are much the same there is a wide va the general appearance of the programs in the size of the source code and i mance very roughly higher level languages give slower programs than lo ones although it unwise to generalize other than qualitatively big buildin like the c stl or the associative arrays and string handling of scripting can lead to more compact code and shorter development time these are no price although the performance penalty may not matter much for programs kov that run for only a few seconds less clear however is how to assess the loss of control and insight when of system supplied code gets so big that one no longer knows what going o neath this is the case with the stl version its performance is unpredict there is no easy way to address that one immature implementation we use to be repaired before it would run our program few of us have the resources energy to track down such problems and fix them this is a pervasive and growing concern in software as libraries interfac tools become more complicated they become less understood and less contro when everything works rich programming environments can be very producti when they fail there is little recourse indeed we may not even realize that thing is wrong if the problems involve performance or subtle logic errors the design and implementation of this program illustrate a number of less larger programs first is the importance of choosing simple algorithms an structures the simplest that will do the job in reasonable time for the expected lem size if someone else has already written them and put them in a library f that even better our c implementation profited from that following brooks s advice we find it best to start detailed design with data tures guided by knowledge of what algorithms might be used with the data str settled the code goes together easily it s hard to design a program completely and then build it constructing re grams involves iteration and experimentation the act of building forces one t ify decisions that had previously been glossed over that was certainly the cas our programs here which have gone through many changes of detail as m possible start with something simple and evolve it as experience dictates if o had been just to write a personal version of the markov chain algorithm for f would almost surely have written it in awk or perl though not with as much ing as the ones we showed here and let it go at that production code takes much more effort than prototypes do however if w of the programs presented here as production code since they have been polish thoroughly tested production quality requires one or two orders of magnitud effort than a program intended for personal use exercise 3 8 we have seen versions of the markov program in a wide variety guages including scheme tel prolog python generic java ml and haskel presents its own challenges and advantages implement the program in your language and compare its general flavor and performance d supplementary reading the standard template library is described in a variety of books includin eric programming and the stl by matthew austem addison wesley definitive reference on c itself is the c programming la nguage by stroustrup edition addison wesley for java we refer to the jav gramming la nguage edition by ken arnold and james gosling ad wesley the best description of perl is programming perl editi larry wall tom christiansen and randal schwartz o reilly the idea behind design patterns is that there are only a few distinct des structs in most programs in the same way that there are only a few basic d tures very loosely it is the design analog of the code idioms that we disc chapter 1 the standard reference is design patterns elements of reusable oriented software by erich gamma richard helm ralph johnson and jo sides addison wesley the picaresque adventures of the markov program originally called shan described in the computing recreations column of the june scientif ican the article was republished in the magic machine by a k dewdne freeman threads processes and threads multithreading thread functionality types of threads user level and kernel level threads other arrangements multicore and multithreading performance of software on multicore application example valve game software windows thread and smp management process and thread objects multithreading thread states support for os subsystems symmetric multiprocessing support solaris thread and smp management multithreaded architecture motivation process structure thread execution interrupts as threads linux process and thread management linux tasks linux threads mac os x grand central dispatch summary recommended reading key terms review questions and problems chapter threads the basic idea is that the several components in any complex system will perform particular subfunctions that contribute to the overall function the sciences of the artificial herbert simon this chapter examines some more advanced concepts related to process manage ment which are found in a number of contemporary operating systems we show that the concept of process is more complex and subtle than presented so far and in fact embodies two separate and potentially independent concepts one relating to resource ownership and another relating to execution this distinction has led to the development in many operating systems of a construct known as the thread processes and threads the discussion so far has presented the concept of a process as embodying two characteristics resource ownership a process includes a virtual address space to hold the process image recall from chapter that the process image is the collection of program data stack and attributes defined in the process control block from time to time a process may be allocated control or ownership of resources such as main memory i o channels i o devices and files the os performs a protection function to prevent unwanted interference between processes with respect to resources scheduling execution the execution of a process follows an execution path trace through one or more programs e g figure this execution may be interleaved with that of other processes thus a process has an execution state running ready etc and a dispatching priority and is the entity that is scheduled and dispatched by the os some thought should convince the reader that these two characteristics are independent and could be treated independently by the os this is done in a number of operating systems particularly recently developed systems to processes and threads distinguish the two characteristics the unit of dispatching is usually referred to as a thread or lightweight process while the unit of resource ownership is usually referred to as a process or task multithreading multithreading refers to the ability of an os to support multiple concurrent paths of execution within a single process the traditional approach of a single thread of execution per process in which the concept of a thread is not recognized is referred to as a single threaded approach the two arrangements shown in the left half of figure are single threaded approaches ms dos is an example of an os that supports a single user process and a single thread other operating systems such as some variants of unix support multiple user processes but only support one thread per process the right half of figure depicts multithreaded approaches a java run time environment is an example of a system of one process with multi ple threads of interest in this section is the use of multiple processes each of which supports multiple threads this approach is taken in windows solaris and many modern versions of unix among others in this section we give a general description instruction trace figure threads and processes even this degree of consistency is not maintained in ibm mainframe operating systems the con cepts of address space and task respectively correspond roughly to the concepts of process and thread that we describe in this section also in the literature the term lightweight process is used as either equivalent to the term thread a particular type of thread known as a kernel level thread or in the case of solaris an entity that maps user level threads to kernel level threads chapter threads of multithreading the details of the windows solaris and linux approaches are discussed later in this chapter in a multithreaded environment a process is defined as the unit of resource allocation and a unit of protection the following are associated with processes a virtual address space that holds the process image protected access to processors other processes for interprocess communica tion files and i o resources devices and channels within a process there may be one or more threads each with the following a thread execution state running ready etc a saved thread context when not running one way to view a thread is as an independent program counter operating within a process an execution stack some per thread static storage for local variables access to the memory and resources of its process shared with all other threads in that process figure illustrates the distinction between threads and processes from the point of view of process management in a single threaded process model i e there is no distinct concept of thread the representation of a process includes its process control block and user address space as well as user and kernel stacks to manage the call return behavior of the execution of the process while the process is running it controls the processor registers the contents of these registers are saved when the process is not running in a multithreaded environment there is still a single process control block and user address space associated with the proc ess but now there are separate stacks for each thread as well as a separate control figure single threaded and multithreaded process models processes and threads block for each thread containing register values priority and other thread related state information thus all of the threads of a process share the state and resources of that process they reside in the same address space and have access to the same data when one thread alters an item of data in memory other threads see the results if and when they access that item if one thread opens a file with read privileges other threads in the same process can also read from that file the key benefits of threads derive from the performance implications it takes far less time to create a new thread in an existing process than to create a brand new process studies done by the mach developers show that thread creation is ten times faster than process creation in unix it takes less time to terminate a thread than a process it takes less time to switch between two threads within the same process than to switch between processes threads enhance efficiency in communication between different executing programs in most operating systems communication between independent processes requires the intervention of the kernel to provide protection and the mechanisms needed for communication however because threads within the same process share memory and files they can communicate with each other without invoking the kernel thus if there is an application or function that should be implemented as a set of related units of execution it is far more efficient to do so as a collection of threads rather than a collection of separate processes an example of an application that could make use of threads is a file server as each new file request comes in a new thread can be spawned for the file manage ment program because a server will handle many requests many threads will be created and destroyed in a short period if the server runs on a multiprocessor com puter then multiple threads within the same process can be executing simultaneously on different processors further because processes or threads in a file server must share file data and therefore coordinate their actions it is faster to use threads and shared memory than processes and message passing for this coordination the thread construct is also useful on a single processor to simplify the structure of a program that is logically doing several different functions gives four examples of the uses of threads in a single user multi processing system foreground and background work for example in a spreadsheet program one thread could display menus and read user input while another thread executes user commands and updates the spreadsheet this arrangement often increases the perceived speed of the application by allowing the program to prompt for the next command before the previous command is complete asynchronous processing asynchronous elements in the program can be implemented as threads for example as a protection against power failure one can design a word processor to write its random access memory ram buffer to disk once every minute a thread can be created whose sole job is chapter threads periodic backup and that schedules itself directly with the os there is no need for fancy code in the main program to provide for time checks or to coordinate input and output speed of execution a multithreaded process can compute one batch of data while reading the next batch from a device on a multiprocessor system mul tiple threads from the same process may be able to execute simultaneously thus even though one thread may be blocked for an i o operation to read in a batch of data another thread may be executing modular program structure programs that involve a variety of activities or a variety of sources and destinations of input and output may be easier to design and implement using threads in an os that supports threads scheduling and dispatching is done on a thread basis hence most of the state information dealing with execution is maintained in thread level data structures there are however several actions that affect all of the threads in a process and that the os must manage at the process level for example suspension involves swapping the address space of one process out of main memory to make room for the address space of another process because all threads in a process share the same address space all threads are suspended at the same time similarly termination of a process terminates all threads within that process thread functionality like processes threads have execution states and may synchronize with one another we look at these two aspects of thread functionality in turn thread states as with processes the key states for a thread are running ready and blocked generally it does not make sense to associate suspend states with threads because such states are process level concepts in particular if a process is swapped out all of its threads are necessarily swapped out because they all share the address space of the process there are four basic thread operations associated with a change in thread state spawn typically when a new process is spawned a thread for that process is also spawned subsequently a thread within a process may spawn another thread within the same process providing an instruction pointer and argu ments for the new thread the new thread is provided with its own register context and stack space and placed on the ready queue block when a thread needs to wait for an event it will block saving its user registers program counter and stack pointers the processor may now turn to the execution of another ready thread in the same or a different process unblock when the event for which a thread is blocked occurs the thread is moved to the ready queue finish when a thread completes its register context and stacks are deallocated processes and threads a significant issue is whether the blocking of a thread results in the blocking of the entire process in other words if one thread in a process is blocked does this prevent the running of any other thread in the same process even if that other thread is in a ready state clearly some of the flexibility and power of threads is lost if the one blocked thread blocks an entire process we return to this issue subsequently in our discussion of user level versus kernel level threads but for now let us consider the performance benefits of threads that do not block an entire process figure based on one in shows a program that performs two remote procedure calls rpcs to two different hosts to obtain a combined result in a single threaded program the results are obtained in sequence so the program has to wait for a response from each server in turn rewriting the program to use a separate thread for each rpc results in a substantial speedup note that if this program operates on a uniprocessor the requests must be generated sequentially and the results processed in sequence however the program waits concurrently for the two replies rpc time rpc request request process a rpc using single thread thread a process thread b process b rpc using one thread per server on a uniprocessor blocked waiting for response to rpc blocked waiting for processor which is in use by thread b running figure remote procedure call rpc using threads rpc is a technique by which two programs which may execute on different machines interact using procedure call return syntax and semantics both the called and calling program behave as if the partner program were running on the same machine rpcs are often used for client server applications and are discussed in chapter chapter threads time i o request request complete time quantum expires thread a process thread b process thread c process time quantum expires process created blocked ready running figure multithreading example on a uniprocessor on a uniprocessor multiprogramming enables the interleaving of multiple threads within multiple processes in the example of figure three threads in two processes are interleaved on the processor execution passes from one thread to another either when the currently running thread is blocked or when its time slice is exhausted thread synchronization all of the threads of a process share the same address space and other resources such as open files any alteration of a resource by one thread affects the environment of the other threads in the same process it is therefore necessary to synchronize the activities of the various threads so that they do not interfere with each other or corrupt data structures for example if two threads each try to add an element to a doubly linked list at the same time one element may be lost or the list may end up malformed the issues raised and the techniques used in the synchronization of threads are in general the same as for the synchronization of processes these issues and techniques are the subject of chapters and types of threads user level and kernel level threads there are two broad categories of thread implementation user level threads ults and kernel level threads klts the latter are also referred to in the lit erature as kernel supported threads or lightweight processes user level threads in a pure ult facility all of the work of thread management is done by the application and the kernel is not aware of the existence of threads figure illustrates the pure ult approach any application can be this example thread c begins to run after thread a exhausts its time quantum even though thread b is also ready to run the choice between b and c is a scheduling decision a topic covered in part four acronyms ult and klt are not widely used but are introduced for conciseness types of threads a pure user level b pure kernel level c combined user level thread kernel level thread process figure user level and kernel level threads programmed to be multithreaded by using a threads library which is a package of routines for ult management the threads library contains code for creating and destroying threads for passing messages and data between threads for scheduling thread execution and for saving and restoring thread contexts by default an application begins with a single thread and begins running in that thread this application and its thread are allocated to a single process man aged by the kernel at any time that the application is running the process is in the running state the application may spawn a new thread to run within the same process spawning is done by invoking the spawn utility in the threads library control is passed to that utility by a procedure call the threads library creates a data structure for the new thread and then passes control to one of the threads within this process that is in the ready state using some scheduling algorithm when control is passed to the library the context of the current thread is saved and when control is passed from the library to a thread the context of that thread is restored the context essentially consists of the contents of user registers the program counter and stack pointers all of the activity described in the preceding paragraph takes place in user space and within a single process the kernel is unaware of this activity the kernel continues to schedule the process as a unit and assigns a single execution state ready running blocked etc to that process the following examples should clarify the relationship between thread scheduling and process scheduling suppose that process b is executing in its thread the states of the process and two ults that are part of the process are shown in figure each of the following is a possible occurrence the application executing in thread makes a system call that blocks b for example an i o call is made this causes control to transfer to the kernel the kernel invokes the i o action places process b in the blocked state and switches to another process meanwhile according to the data structure maintained by a ready thread blocked running process b ready blocked thread ready blocked running running b ready thread blocked running process b ready blocked thread ready blocked running running c ready thread blocked running process b ready blocked thread ready blocked running running d ready thread blocked running process b ready blocked thread ready blocked running running figure examples of the relationships between user level thread states and process states types of threads the threads library thread of process b is still in the running state it is impor tant to note that thread is not actually running in the sense of being executed on a processor but it is perceived as being in the running state by the threads library the corresponding state diagrams are shown in figure a clock interrupt passes control to the kernel and the kernel determines that the currently running process b has exhausted its time slice the kernel places process b in the ready state and switches to another process meanwhile according to the data structure maintained by the threads library thread of process b is still in the running state the corresponding state diagrams are shown in figure thread has reached a point where it needs some action performed by thread of process b thread enters a blocked state and thread transitions from ready to running the process itself remains in the running state the corresponding state diagrams are shown in figure in cases and figures and when the kernel switches control back to process b execution resumes in thread also note that a process can be interrupted either by exhausting its time slice or by being preempted by a higher priority process while it is executing code in the threads library thus a process may be in the midst of a thread switch from one thread to another when inter rupted when that process is resumed execution continues within the threads library which completes the thread switch and transfers control to another thread within that process there are a number of advantages to the use of ults instead of klts including the following thread switching does not require kernel mode privileges because all of the thread management data structures are within the user address space of a single process therefore the process does not switch to the kernel mode to do thread management this saves the overhead of two mode switches user to kernel kernel back to user scheduling can be application specific one application may benefit most from a simple round robin scheduling algorithm while another might benefit from a priority based scheduling algorithm the scheduling algorithm can be tailored to the application without disturbing the underlying os scheduler ults can run on any os no changes are required to the underlying kernel to support ults the threads library is a set of application level functions shared by all applications there are two distinct disadvantages of ults compared to klts in a typical os many system calls are blocking as a result when a ult executes a system call not only is that thread blocked but also all of the threads within the process are blocked in a pure ult strategy a multithreaded application cannot take advantage of multiprocessing a kernel assigns one process to only one processor at a time therefore only a single thread within a process can execute at a time in effect we have application level multiprogramming within a single process chapter threads while this multiprogramming can result in a significant speedup of the appli cation there are applications that would benefit from the ability to execute portions of code simultaneously there are ways to work around these two problems for example both prob lems can be overcome by writing an application as multiple processes rather than multiple threads but this approach eliminates the main advantage of threads each switch becomes a process switch rather than a thread switch resulting in much greater overhead another way to overcome the problem of blocking threads is to use a tech nique referred to as jacketing the purpose of jacketing is to convert a blocking system call into a nonblocking system call for example instead of directly calling a system i o routine a thread calls an application level i o jacket routine within this jacket routine is code that checks to determine if the i o device is busy if it is the thread enters the blocked state and passes control through the threads library to another thread when this thread later is given control again the jacket routine checks the i o device again kernel level threads in a pure klt facility all of the work of thread management is done by the kernel there is no thread management code in the application level simply an application programming interface api to the kernel thread facility windows is an example of this approach figure depicts the pure klt approach the kernel maintains context information for the process as a whole and for individual threads within the process scheduling by the kernel is done on a thread basis this approach overcomes the two principal drawbacks of the ult approach first the kernel can simultaneously schedule multiple threads from the same process on multiple processors second if one thread in a process is blocked the kernel can schedule another thread of the same process another advantage of the klt approach is that kernel routines themselves can be multithreaded the principal disadvantage of the klt approach compared to the ult approach is that the transfer of control from one thread to another within the same process requires a mode switch to the kernel to illustrate the differences table shows the results of measurements taken on a uniprocessor vax computer running a unix like os the two benchmarks are as follows null fork the time to create schedule execute and complete a process thread that invokes the null procedure i e the overhead of forking a process thread and signal wait the time for a process thread to signal a waiting process thread and then wait on a condition i e the overhead of synchronizing two processes threads together we see that there is an order of magnitude or more of difference between ults and klts and similarly between klts and processes table thread and process operation latencies μs operation user level threads kernel level threads processes null fork signal wait 840 types of threads thus on the face of it while there is a significant speedup by using klt mul tithreading compared to single threaded processes there is an additional signifi cant speedup by using ults however whether or not the additional speedup is realized depends on the nature of the applications involved if most of the thread switches in an application require kernel mode access then a ult based scheme may not perform much better than a klt based scheme combined approaches some operating systems provide a combined ult klt facility figure in a combined system thread creation is done completely in user space as is the bulk of the scheduling and synchronization of threads within an application the multiple ults from a single application are mapped onto some smaller or equal number of klts the programmer may adjust the number of klts for a particular application and processor to achieve the best overall results in a combined approach multiple threads within the same application can run in parallel on multiple processors and a blocking system call need not block the entire process if properly designed this approach should combine the advan tages of the pure ult and klt approaches while minimizing the disadvantages solaris is a good example of an os using this combined approach the current solaris version limits the ult klt relationship to be one to one other arrangements as we have said the concepts of resource allocation and dispatching unit have traditionally been embodied in the single concept of the process that is as a relationship between threads and processes recently there has been much inter est in providing for multiple threads within a single process which is a many to one relationship however as table shows the other two combinations have also been investigated namely a many to many relationship and a one to many relationship many to many relationship the idea of having a many to many relationship between threads and processes has been explored in the experimental operating system trix in trix there are the concepts of domain table relationship between threads and processes threads processes description example systems each thread of execution is a unique process with its own address space and resources traditional unix implementations m a process defines an address space and dynamic resource ownership multiple threads may be created and executed within that process windows nt solaris linux os os mach m a thread may migrate from one process environment to another this allows a thread to be easily moved among distinct systems ra clouds emerald m n combines attributes of m and m cases trix chapter threads and thread a domain is a static entity consisting of an address space and ports through which messages may be sent and received a thread is a single execution path with an execution stack processor state and scheduling information as with the multithreading approaches discussed so far multiple threads may execute in a single domain providing the efficiency gains discussed earlier however it is also possible for a single user activity or application to be per formed in multiple domains in this case a thread exists that can move from one domain to another the use of a single thread in multiple domains seems primarily motivated by a desire to provide structuring tools for the programmer for example consider a program that makes use of an i o subprogram in a multiprogramming environ ment that allows user spawned processes the main program could generate a new process to handle i o and then continue to execute however if the future progress of the main program depends on the outcome of the i o operation then the main program will have to wait for the other i o program to finish there are several ways to implement this application the entire program can be implemented as a single process this is a rea sonable and straightforward solution there are drawbacks related to memory management the process as a whole may require considerable main memory to execute efficiently whereas the i o subprogram requires a relatively small address space to buffer i o and to handle the relatively small amount of program code because the i o program executes in the address space of the larger program either the entire process must remain in main memory during the i o operation or the i o operation is subject to swapping this memory management effect would also exist if the main program and the i o subprogram were implemented as two threads in the same address space the main program and i o subprogram can be implemented as two separate processes this incurs the overhead of creating the subordinate process if the i o activity is frequent one must either leave the subordinate process alive which consumes management resources or frequently create and destroy the subprogram which is inefficient treat the main program and the i o subprogram as a single activity that is to be implemented as a single thread however one address space domain could be created for the main program and one for the i o subprogram thus the thread can be moved between the two address spaces as execu tion proceeds the os can manage the two address spaces independently and no process creation overhead is incurred furthermore the address space used by the i o subprogram could also be shared by other simple i o programs the experiences of the trix developers indicate that the third option has merit and may be the most effective solution for some applications one to many relationship in the field of distributed operating systems designed to control distributed computer systems there has been interest in the multicore and multithreading concept of a thread as primarily an entity that can move among address spaces a notable example of this research is the clouds operating system and especially its kernel known as ra another example is the emerald system a thread in clouds is a unit of activity from the user perspective a process is a virtual address space with an associated process control block upon creation a thread starts executing in a process by invoking an entry point to a program in that process threads may move from one address space to another and actually span computer boundaries i e move from one computer to another as a thread moves it must carry with it certain information such as the controlling terminal global parameters and scheduling guidance e g priority the clouds approach provides an effective way of insulating both users and programmers from the details of the distributed environment a user activity may be represented as a single thread and the movement of that thread among comput ers may be dictated by the os for a variety of system related reasons such as the need to access a remote resource and load balancing multicore and multithreading the use of a multicore system to support a single application with multiple threads such as might occur on a workstation a video game console or a personal computer running a processor intense application raises issues of performance and applica tion design in this section we first look at some of the performance implications of a multithreaded application on a multicore system and then describe a specific example of an application designed to exploit multicore capabilities performance of software on multicore the potential performance benefits of a multicore organization depend on the ability to effectively exploit the parallel resources available to the application let us focus first on a single application running on a multicore system amdahl law see appendix e states that speedup time to execute program on a single processor time to execute program on n parallel processors f f n the law assumes a program in which a fraction f of the execution time involves code that is inherently serial and a fraction f that involves code that is infi nitely parallelizable with no scheduling overhead this law appears to make the prospect of a multicore organization attractive but as figure shows even a small amount of serial code has a noticeable impact if only of the code is inherently serial f running the program on a multicore system with eight processors yields a performance gain of only a factor of in addition software typically incurs overhead as a result of communication movement of processes or threads among address spaces or thread migration on different machines has become a hot topic in recent years chapter explores this topic chapter threads number of processors a speedup with and sequential portions number of processors b speedup with overheads figure performance effect of multiple cores and distribution of work to multiple processors and cache coherence overhead this results in a curve where performance peaks and then begins to degrade because of the increased burden of the overhead of using multiple processors figure from is a representative example however software engineers have been addressing this problem and there are numerous applications in which it is possible to effectively exploit a multicore sys tem reports on a set of database applications in which great attention multicore and multithreading oracle dss way join tmc data mining dss scan aggs oracle ad hoc insurance oltp number of cpus figure scaling of database workloads on multiple processor hardware was paid to reducing the serial fraction within hardware architectures operating systems middleware and the database application software figure shows the result as this example shows database management systems and database applica tions are one area in which multicore systems can be used effectively many kinds of servers can also effectively use the parallel multicore organization because servers typically handle numerous relatively independent transactions in parallel in addition to general purpose server software a number of classes of applica tions benefit directly from the ability to scale throughput with the number of cores lists the following examples multithreaded native applications multithreaded applications are charac terized by having a small number of highly threaded processes examples of threaded applications include lotus domino or siebel crm customer relationship manager multiprocess applications multiprocess applications are characterized by the presence of many single threaded processes examples of multiprocess applications include the oracle database sap and peoplesoft java applications java applications embrace threading in a fundamental way not only does the java language greatly facilitate multithreaded applications but the java virtual machine is a multithreaded process that provides sched uling and memory management for java applications java applications that can benefit directly from multicore resources include application servers such as sun java application server bea weblogic ibm websphere and the open source tomcat application server all applications that use a java platform enterprise edition platform application server can immedi ately benefit from multicore technology chapter threads multiinstance applications even if an individual application does not scale to take advantage of a large number of threads it is still possible to gain from multicore architecture by running multiple instances of the application in parallel if multiple application instances require some degree of isolation virtualization technology for the hardware of the operating system can be used to provide each of them with its own separate and secure environment application example valve game software valve is an entertainment and technology company that has developed a number of popular games as well as the source engine one of the most widely played game engines available source is an animation engine used by valve for its games and licensed for other game developers in recent years valve has reprogrammed the source engine software to use multithreading to exploit the power of multicore processor chips from intel and amd the revised source engine code provides more powerful support for valve games such as half life from valve perspective threading granularity options are defined as follows coarse threading individual modules called systems are assigned to individ ual processors in the source engine case this would mean putting rendering on one processor ai artificial intelligence on another physics on another and so on this is straightforward in essence each major module is single threaded and the principal coordination involves synchronizing all the threads with a timeline thread fine grained threading many similar or identical tasks are spread across mul tiple processors for example a loop that iterates over an array of data can be split up into a number of smaller parallel loops in individual threads that can be scheduled in parallel hybrid threading this involves the selective use of fine grained threading for some systems and single threading for other systems valve found that through coarse threading it could achieve up to twice the performance across two processors compared to executing on a single processor but this performance gain could only be achieved with contrived cases for real world gameplay the improvement was on the order of a factor of valve also found that effective use of fine grained threading was difficult the time per work unit can be variable and managing the timeline of outcomes and consequences involved complex programming valve found that a hybrid threading approach was the most promising and would scale the best as multicore systems with or processors became available valve identified systems that operate very effectively being permanently assigned to a single processor an example is sound mixing which has little user interaction is not constrained by the frame configuration of windows and works on its own set of data other modules such as scene rendering can be organized into a number of threads so that the module can execute on a single processor but achieve greater performance as it is spread out over more and more processors multicore and multithreading figure hybrid threading for rendering module figure illustrates the thread structure for the rendering module in this hier archical structure higher level threads spawn lower level threads as needed the rendering module relies on a critical part of the source engine the world list which is a database representation of the visual elements in the game world the first task is to determine what are the areas of the world that need to be rendered the next task is to determine what objects are in the scene as viewed from multiple angles then comes the processor intensive work the rendering module has to work out the rendering of each object from multiple points of view such as the player view the view of tv monitors and the point of view of reflections in water some of the key elements of the threading strategy for the rendering module are listed in and include the following construct scene rendering lists for multiple scenes in parallel e g the world and its reflection in water overlap graphics simulation compute character bone transformations for all characters in all scenes in parallel allow multiple threads to draw in parallel the designers found that simply locking key databases such as the world list for a thread was too inefficient over of the time a thread is trying to read from a data set and only of the time at most is spent in writing to a data set thus a concurrency mechanism known as the single writer multiple readers model works effectively chapter threads windows thread and smp management windows process design is driven by the need to provide support for a variety of os environments processes supported by different os environments differ in a number of ways including the following how processes are named whether threads are provided within processes how processes are represented how process resources are protected what mechanisms are used for interprocess communication and synchronization how processes are related to each other accordingly the native process structures and services provided by the windows kernel are relatively simple and general purpose allowing each os subsystem to emulate a particular process structure and functionality important characteristics of windows processes are the following windows processes are implemented as objects a process can be created as new process or as a copy of an existing process an executable process may contain one or more threads both process and thread objects have built in synchronization capabilities figure based on one in illustrates the way in which a process relates to the resources it controls or uses each process is assigned a security access figure a windows process and its resources windows thread and smp management token called the primary token of the process when a user first logs on windows creates an access token that includes the security id for the user every process that is created by or runs on behalf of this user has a copy of this access token windows uses the token to validate the user ability to access secured objects or to perform restricted functions on the system and on secured objects the access token controls whether the process can change its own attributes in this case the process does not have a handle opened to its access token if the process attempts to open such a han dle the security system determines whether this is permitted and therefore whether the process may change its own attributes also related to the process is a series of blocks that define the virtual address space currently assigned to this process the process cannot directly modify these structures but must rely on the virtual memory manager which provides a memory allocation service for the process finally the process includes an object table with handles to other objects known to this process figure shows a single thread in addition the process has access to a file object and to a section object that defines a section of shared memory process and thread objects the object oriented structure of windows facilitates the development of a gen eral purpose process facility windows makes use of two types of process related objects processes and threads a process is an entity corresponding to a user job or application that owns resources such as memory and open files a thread is a dispatchable unit of work that executes sequentially and is interruptible so that the processor can turn to another thread each windows process is represented by an object whose general structure is shown in figure each process is defined by a number of attributes and encapsulates a number of actions or services that it may perform a process will perform a service when called upon through a set of published interface methods when windows creates a new process it uses the object class or type defined for the windows process as a template to generate a new object instance at the time of creation attribute values are assigned table gives a brief definition of each of the object attributes for a process object a windows process must contain at least one thread to execute that thread may then create other threads in a multiprocessor system multiple threads from the same process may execute in parallel figure depicts the object structure for a thread object and table defines the thread object attributes note that some of the attributes of a thread resemble those of a process in those cases the thread attribute value is derived from the process attribute value for example the thread processor affinity is the set of processors in a multiprocessor system that may execute this thread this set is equal to or a subset of the process processor affinity note that one of the attributes of a thread object is context which contains the values of the processor registers when the thread last ran this information enables threads to be suspended and resumed furthermore it is possible to alter the behav ior of a thread by altering its context while it is suspended chapter threads object type object body attributes services object type object body attributes services a process object b thread object figure windows process and thread objects table windows process object attributes windows thread and smp management table windows thread object attributes thread id a unique value that identifies a thread when it calls a server thread context the set of register values and other volatile data that defines the execution state of a thread dynamic priority the thread execution priority at any given moment base priority the lower limit of the thread dynamic priority thread processor affinity the set of processors on which the thread can run which is a subset or all of the processor affinity of the thread process thread execution time the cumulative amount of time a thread has executed in user mode and in kernel mode alert status a flag that indicates whether a waiting thread may execute an asynchronous procedure call suspension count the number of times the thread execution has been suspended without being resumed impersonation token a temporary access token allowing a thread to perform operations on behalf of another process used by subsystems termination port an interprocess communication channel to which the process manager sends a message when the thread terminates used by subsystems thread exit status the reason for a thread termination multithreading windows supports concurrency among processes because threads in different processes may execute concurrently appear to run at the same time moreover mul tiple threads within the same process may be allocated to separate processors and execute simultaneously actually run at the same time a multithreaded process achieves concurrency without the overhead of using multiple processes threads within the same process can exchange information through their common address space and have access to the shared resources of the process threads in different processes can exchange information through shared memory that has been set up between the two processes an object oriented multithreaded process is an efficient means of implementing a server application for example one server process can service a number of clients concurrently thread states an existing windows thread is in one of six states figure ready a ready thread may be scheduled for execution the kernel dispatcher keeps track of all ready threads and schedules them in priority order standby a standby thread has been selected to run next on a particular proc essor the thread waits in this state until that processor is made available if the standby thread priority is high enough the running thread on that chapter threads figure windows thread states processor may be preempted in favor of the standby thread otherwise the standby thread waits until the running thread blocks or exhausts its time slice running once the kernel dispatcher performs a thread switch the standby thread enters the running state and begins execution and continues execution until it is preempted by a higher priority thread exhausts its time slice blocks or terminates in the first two cases it goes back to the ready state waiting a thread enters the waiting state when it is blocked on an event e g i o it voluntarily waits for synchronization purposes or an environment subsystem directs the thread to suspend itself when the waiting condition is satisfied the thread moves to the ready state if all of its resources are available transition a thread enters this state after waiting if it is ready to run but the resources are not available for example the thread stack may be paged out of memory when the resources are available the thread goes to the ready state terminated a thread can be terminated by itself by another thread or when its parent process terminates once housekeeping chores are completed the thread is removed from the system or it may be retained by the for future reinitialization windows executive is described in chapter it contains the base operating system services such as memory management process and thread management security i o and interprocess communication windows thread and smp management support for os subsystems the general purpose process and thread facility must support the particular process and thread structures of the various os environments it is the responsibility of each os subsystem to exploit the windows process and thread features to emulate the process and thread facilities of its corresponding os this area of process thread management is complicated and we give only a brief overview here process creation begins with a request for a new process from an application the application issues a create process request to the corresponding protected subsystem which passes the request to the executive the executive creates a proc ess object and returns a handle for that object to the subsystem when windows creates a process it does not automatically create a thread in the case of a new process must always be created with an initial thread therefore for the subsystem calls the windows process manager again to create a thread for the new process receiving a thread handle back from windows the appropriate thread and process information are then returned to the application in the case of posix threads are not supported therefore the posix subsystem obtains a thread for the new process from windows so that the process may be activated but returns only process information to the application the fact that the posix process is imple mented using both a process and a thread from the windows executive is not visible to the application when a new process is created by the executive the new process inherits many of its attributes from the creating process however in the environ ment this process creation is done indirectly an application client process issues its process creation request to the subsystem then the subsystem in turn issues a process request to the windows executive because the desired effect is that the new process inherits characteristics of the client process and not of the server process windows enables the subsystem to specify the parent of the new process the new process then inherits the parent access token quota limits base priority and default processor affinity symmetric multiprocessing support windows supports smp hardware configurations the threads of any process including those of the executive can run on any processor in the absence of affin ity restrictions explained in the next paragraph the kernel dispatcher assigns a ready thread to the next available processor this assures that no processor is idle or is executing a lower priority thread when a higher priority thread is ready multiple threads from the same process can be executing simultaneously on multiple processors as a default the kernel dispatcher uses the policy of soft affinity in assign ing threads to processors the dispatcher tries to assign a ready thread to the same processor it last ran on this helps reuse data still in that processor memory caches from the previous execution of the thread it is possible for an application to restrict its thread execution only to certain processors hard affinity chapter threads solaris thread and smp management solaris implements multilevel thread support designed to provide considerable flexibility in exploiting processor resources multithreaded architecture solaris makes use of four separate thread related concepts process this is the normal unix process and includes the user address space stack and process control block user level threads implemented through a threads library in the address space of a process these are invisible to the os a user level thread ult is a user created unit of execution within a process lightweight processes a lightweight process lwp can be viewed as a map ping between ults and kernel threads each lwp supports ult and maps to one kernel thread lwps are scheduled by the kernel independently and may execute in parallel on multiprocessors kernel threads these are the fundamental entities that can be scheduled and dispatched to run on one of the system processors figure illustrates the relationship among these four entities note that there is always exactly one kernel thread for each lwp an lwp is visible within a process to the application thus lwp data structures exist within their respective process address space at the same time each lwp is bound to a single dispatchable kernel thread and the data structure for that kernel thread is maintained within the kernel address space figure processes and threads in solaris the acronym ult is unique to this book and is not found in the solaris literature solaris thread and smp management a process may consist of a single ult bound to a single lwp in this case there is a single thread of execution corresponding to a traditional unix process when concurrency is not required within a single process an application uses this process structure if an application requires concurrency its process contains multiple threads each bound to a single lwp which in turn are each bound to a single kernel thread in addition there are kernel threads that are not associated with lwps the kernel creates runs and destroys these kernel threads to execute specific system functions the use of kernel threads rather than kernel processes to implement system functions reduces the overhead of switching within the kernel from a process switch to a thread switch motivation the three level thread structure ult lwp kernel thread in solaris is intended to facilitate thread management by the os and to provide a clean interface to appli cations the ult interface can be a standard thread library a defined ult maps onto a lwp which is managed by the os and which has defined states of execution defined subsequently an lwp is bound to a kernel thread with a one to one corre spondence in execution states thus concurrency and execution are managed at the level of the kernel thread in addition an application has access to hardware through an application pro gramming interface consisting of system calls the api allows the user to invoke kernel services to perform privileged tasks on behalf of the calling process such as read or write a file issue a control command to a device create a new process or thread allocate memory for the process to use and so on process structure figure compares in general terms the process structure of a traditional unix system with that of solaris on a typical unix implementation the process struc ture includes the process id the user ids a signal dispatch table which the kernel uses to decide what to do when sending a signal to a process file descriptors which describe the state of files in use by this process a memory map which defines the address space for this process and a processor state structure which includes the kernel stack for this process solaris retains this basic structure but replaces the pro cessor state block with a list of structures containing one data block for each lwp the lwp data structure includes the following elements an lwp identifier the priority of this lwp and hence the kernel thread that supports it a signal mask that tells the kernel which signals will be accepted saved values of user level registers when the lwp is not running the kernel stack for this lwp which includes system call arguments results and error codes for each call level resource usage and profiling data pointer to the corresponding kernel thread pointer to the process structure chapter threads unix process structure solaris process structure priority signal mask registers stack lwp id priority signal mask registers stack figure process structure in traditional unix and solaris thread execution figure shows a simplified view of both thread execution states these states reflect the execution status of both a kernel thread and the lwp bound to it as mentioned some kernel threads are not associated with an lwp the same execu tion diagram applies the states are as follows run the thread is runnable that is the thread is ready to execute onproc the thread is executing on a processor sleep the thread is blocked stop the thread is stopped zombie the thread has terminated free thread resources have been released and the thread is awaiting removal from the os thread data structure a thread moves from onproc to run if it is preempted by a higher priority thread or because of time slicing a thread moves from onproc to sleep if it solaris thread and smp management figure solaris thread states is blocked and must await an event to return the run state blocking occurs if the thread invokes a system call and must wait for the system service to be performed a thread enters the stop state if its process is stopped this might be done for debugging purposes interrupts as threads most operating systems contain two fundamental forms of concurrent activity processes and interrupts processes or threads cooperate with each other and manage the use of shared data structures by means of a variety of primitives that enforce mutual exclusion only one process at a time can execute certain code or access certain data and that synchronize their execution interrupts are synchronized by preventing their handling for a period of time solaris unifies these two concepts into a single model namely kernel threads and the mechanisms for scheduling and executing kernel threads to do this interrupts are converted to kernel threads the motivation for converting interrupts to threads is to reduce overhead interrupt handlers often manipulate data shared by the rest of the kernel therefore while a kernel routine that accesses such data is executing interrupts must be blocked even though most interrupts will not affect that data typically the way this is done is for the routine to set the interrupt priority level higher to block inter rupts and then lower the priority level after access is completed these operations take time the problem is magnified on a multiprocessor system the kernel must protect more objects and may need to block interrupts on all processors chapter threads the solution in solaris can be summarized as follows solaris employs a set of kernel threads to handle interrupts as with any kernel thread an interrupt thread has its own identifier priority context and stack the kernel controls access to data structures and synchronizes among inter rupt threads using mutual exclusion primitives of the type discussed in chapter that is the normal synchronization techniques for threads are used in handling interrupts interrupt threads are assigned higher priorities than all other types of kernel threads when an interrupt occurs it is delivered to a particular processor and the thread that was executing on that processor is pinned a pinned thread cannot move to another processor and its context is preserved it is simply suspended until the interrupt is processed the processor then begins executing an interrupt thread there is a pool of deactivated interrupt threads available so that a new thread crea tion is not required the interrupt thread then executes to handle the interrupt if the handler routine needs access to a data structure that is currently locked in some fashion for use by another executing thread the interrupt thread must wait for access to that data structure an interrupt thread can only be preempted by another interrupt thread of higher priority experience with solaris interrupt threads indicates that this approach provides superior performance to the traditional interrupt handling strategy linux process and thread management linux tasks a process or task in linux is represented by a data structure the data structure contains information in a number of categories state the execution state of the process executing ready suspended stopped zombie this is described subsequently scheduling information information needed by linux to schedule processes a process can be normal or real time and has a priority real time processes are scheduled before normal processes and within each category relative pri orities can be used a counter keeps track of the amount of time a process is allowed to execute identifiers each process has a unique process identifier and also has user and group identifiers a group identifier is used to assign resource access privi leges to a group of processes interprocess communication linux supports the ipc mechanisms found in unix described in chapter links each process includes a link to its parent process links to its siblings processes with the same parent and links to all of its children linux process and thread management times and timers includes process creation time and the amount of proces sor time so far consumed by the process a process may also have associated one or more interval timers a process defines an interval timer by means of a system call as a result a signal is sent to the process when the timer expires a timer may be single use or periodic file system includes pointers to any files opened by this process as well as pointers to the current and the root directories for this process address space defines the virtual address space assigned to this process processor specific context the registers and stack information that constitute the context of this process figure shows the execution states of a process these are as follows running this state value corresponds to two states a running process is either executing or it is ready to execute interruptible this is a blocked state in which the process is waiting for an event such as the end of an i o operation the availability of a resource or a signal from another process uninterruptible this is another blocked state the difference between this and the interruptible state is that in an uninterruptible state a process is wait ing directly on hardware conditions and therefore will not handle any signals figure linux process thread model chapter threads stopped the process has been halted and can only resume by positive action from another process for example a process that is being debugged can be put into the stopped state zombie the process has been terminated but for some reason still must have its task structure in the process table linux threads traditional unix systems support a single thread of execution per process while modern unix systems typically provide support for multiple kernel level threads per process as with traditional unix systems older versions of the linux ker nel offered no support for multithreading instead applications would need to be written with a set of user level library functions the most popular of which is known as pthread posix thread libraries with all of the threads mapping into a single kernel level process we have seen that modern versions of unix offer kernel level threads linux provides a unique solution in that it does not recog nize a distinction between threads and processes using a mechanism similar to the lightweight processes of solaris user level threads are mapped into kernel level processes multiple user level threads that constitute a single user level process are mapped into linux kernel level processes that share the same group id this enables these processes to share resources such as files and memory and to avoid the need for a context switch when the scheduler switches among processes in the same group a new process is created in linux by copying the attributes of the current process a new process can be cloned so that it shares resources such as files sig nal handlers and virtual memory when the two processes share the same virtual memory they function as threads within a single process however no separate type of data structure is defined for a thread in place of the usual fork com mand processes are created in linux using the clone command this command includes a set of flags as arguments defined in table the traditional fork system call is implemented by linux as a clone system call with all of the clone flags cleared when the linux kernel performs a switch from one process to another it checks whether the address of the page directory of the current process is the same as that of the to be scheduled process if they are then they are sharing the same address space so that a context switch is basically just a jump from one location of code to another location of code although cloned processes that are part of the same process group can share the same memory space they cannot share the same user stacks thus the clone call creates separate stack spaces for each process portable operating systems based on unix is an ieee api standard that includes a stan dard for a thread api libraries implementing the posix threads standard are often named pthreads pthreads are most commonly used on unix like posix systems such as linux and solaris but microsoft windows implementations also exist mac os x grand central dispatch table linux clone flags clear the task id the parent does not want a sigchld signal sent on exit share the table that identifies the open files share the table that identifies the root directory and the current working directory as well as the value of the bit mask used to mask the initial file permissions of a new file set pid to zero which refers to an idle task the idle task is employed when all available tasks are blocked waiting for resources create a new namespace for the child caller and new task share the same parent process if the parent process is being traced the child process will also be traced write the tid back to user space create a new tls for the child share the table that identifies the signal handlers share system v semantics insert this process into the same thread group of the parent if this flag is true it implicitly enforces if set the parent does not get scheduled for execution until the child invokes the execve system call share the address space memory descriptor and all page tables mac os x grand central dispatch as was mentioned in chapter mac os x grand central dispatch gcd pro vides a pool of available threads designers can designate portions of applications called blocks that can be dispatched independently and run concurrently the os will provide as much concurrency as possible based on the number of cores avail able and the thread capacity of the system although other operating systems have implemented thread pools gcd provides a qualitative improvement in ease of use and efficiency a block is a simple extension to c or other languages such as c the pur pose of defining a block is to define a self contained unit of work including code plus data here is a simple example of a block definition x printf hello world n a block is denoted by a caret at the start of the function which is enclosed in curly brackets the above block definition defines x as a way of calling the func tion so that invoking the function x would print the words hello world chapter threads blocks enable the programmer to encapsulate complex functions together with their arguments and data so that they can easily be referenced and passed around in a program much like a variable symbolically f data blocks are scheduled and dispatched by means of queues the application makes use of system queues provided by gcd and may also set up private queues blocks are put onto a queue as they are encountered during program execution gcd then uses those queues to describe concurrency serialization and callbacks queues are lightweight user space data structures which generally makes them far more efficient than manually managing threads and locks for example this queue has three blocks queue depending on the queue and how it is defined gcd either treats these blocks as potentially concurrent activities or treats them as serial activities in either case blocks are dispatched on a first in first out basis if this is a concurrent queue then the dispatcher assigns f to a thread as soon as one is available then g then h if this is a serial queue the dispatcher assigns f to a thread and then only assigns g to a thread after f has completed the use of predefined threads saves the cost of creating a new thread for each request reducing the latency associated with process ing a block thread pools are automatically sized by the system to maximize the performance of the applications using gcd while minimizing the number of idle or competing threads in addition to scheduling blocks directly the application can associate a sin gle block and queue with an event source such as a timer network socket or file descriptor every time the source issues an event the block is scheduled if it is not of the material in the remainder of this section is based on mac os x grand central dispatch already running this allows rapid response without the expense of polling or park ing a thread on the event source an example from indicates the ease of using gcd consider a document based application with a button that when clicked will analyze the current document and display some interesting statistics about it in the common case this analysis should execute in under a second so the following code is used to connect the button with an action inaction analyzedocument nsbutton sender nsdictionary stats mydoc analyze mymodel setdict stats mystatsview setneedsdisplay yes stats release the first line of the function body analyzes the document the second line updates the application internal state and the third line tells the application that the statistics view needs to be updated to reflect this new state this code which fol lows a common pattern is executed in the main thread the design is acceptable so long as the analysis does not take too long because after the user clicks the button the main thread of the application needs to handle that user input as fast as pos sible so it can get back to the main event loop to process the next user action but if the user opens a very large or complex document the analyze step may take an unacceptably long amount of time a developer may be reluctant to alter the code to meet this unlikely event which may involve application global objects thread management callbacks argument marshalling context objects new variables and so on but with gcd a modest addition to the code produces the desired result ibaction analyzedocument nsbutton sender nsdictionary stats mydoc analyze mymodel setdict stats mystatsview setneedsdisplay yes stats release chapter threads all functions in gcd begin with the outer async call puts a task on a global concurrent queue this tells the os that the block can be assigned to a separate concurrent queue off the main queue and exe cuted in parallel therefore the main thread of execution is not delayed when the analyze function is complete the inner call is encountered this directs the os to put the following block of code at the end of the main queue to be executed when it reaches the head of the queue so with very little work on the part of the programmer the desired requirement is met summary some operating systems distinguish the concepts of process and thread the for mer related to resource ownership and the latter related to program execution this approach may lead to improved efficiency and coding convenience in a mul tithreaded system multiple concurrent threads may be defined within a single process this may be done using either user level threads or kernel level threads user level threads are unknown to the os and are created and managed by a threads library that runs in the user space of a process user level threads are very efficient because a mode switch is not required to switch from one thread to another however only a single user level thread within a process can execute at a time and if one thread blocks the entire process is blocked kernel level threads are threads within a process that are maintained by the kernel because they are recognized by the kernel multiple threads within the same process can execute in parallel on a multiprocessor and the blocking of a thread does not block the entire process however a mode switch is required to switch from one thread to another recommended reading and provide good overviews of thread concepts and a discus sion of programming strategies the former focuses more on concepts and the latter more on programming but both provide useful coverage of both topics discusses the windows nt thread facility in depth good coverage of unix threads concepts is found in key terms review questions and problems key terms review questions and problems key terms kernel level thread lightweight process message multithreading port process task thread user level thread review questions table lists typical elements found in a process control block for an unthreaded os of these which should belong to a thread control block and which should belong to a process control block for a multithreaded system list reasons why a mode switch between threads may be cheaper than a mode switch between processes what are the two separate and potentially independent characteristics embodied in the concept of process give four general examples of the use of threads in a single user multiprocessing sys tem what resources are typically shared by all of the threads of a process list three advantages of ults over klts list two disadvantages of ults compared to klts define jacketing problems it was pointed out that two advantages of using multiple threads within a process are that less work is involved in creating a new thread within an existing process than in creating a new process and communication among threads within the same process is simplified is it also the case that a mode switch between two threads within the same process involves less work than a mode switch between two threads in different processes in the discussion of ults versus klts it was pointed out that a disadvantage of ults is that when a ult executes a system call not only is that thread blocked but also all of the threads within the process are blocked why is that so os is an obsolete os for pcs from ibm in os what is commonly embodied in the concept of process in other operating systems is split into three separate types of entities session processes and threads a session is a collection of one or more processes associated with a user interface keyboard display and mouse the ses sion represents an interactive user application such as a word processing program or a spreadsheet this concept allows the personal computer user to open more than one application giving each one or more windows on the screen the os must keep track of which window and therefore which session is active so that keyboard and mouse input are routed to the appropriate session at any time one session is in foreground mode with other sessions in background mode all keyboard and mouse input is directed to one of the processes of the foreground session as dictated by chapter threads the applications when a session is in foreground mode a process performing video output sends it directly to the hardware video buffer and thence to the user screen when the session is moved to the background the hardware video buffer is saved to a logical video buffer for that session while a session is in background if any of the threads of any of the processes of that session executes and produces screen output that output is directed to the logical video buffer when the session returns to fore ground the screen is updated to reflect the current contents of the logical video buffer for the new foreground session there is a way to reduce the number of process related concepts in os from three to two eliminate sessions and associate the user interface keyboard mouse and screen with processes thus one process at a time is in foreground mode for further structuring processes can be broken up into threads a what benefits are lost with this approach b if you go ahead with this modification where do you assign resources memory files etc at the process or thread level consider an environment in which there is a one to one mapping between user level threads and kernel level threads that allows one or more threads within a process to issue blocking system calls while other threads continue to run explain why this model can make multithreaded programs run faster than their single threaded coun terparts on a uniprocessor computer if a process exits and there are still threads of that process running will they continue to run the os mainframe operating system is structured around the concepts of address space and task roughly speaking a single address space corresponds to a single application and corresponds more or less to a process in other operat ing systems within an address space a number of tasks may be generated and execute concurrently this corresponds roughly to the concept of multithreading two data structures are key to managing this task structure an address space control block ascb contains information about an address space needed by os whether or not that address space is executing information in the ascb includes dispatching priority real and virtual memory allocated to this address space the number of ready tasks in this address space and whether each is swapped out a task control block tcb represents a user program in execution it contains information needed for managing a task within an address space including processor status information pointers to programs that are part of this task and task execution state ascbs are global structures maintained in system memory while tcbs are local structures maintained within their address space what is the advantage of splitting the control information into global and local portions many current language specifications such as for c and c are inadequate for multithreaded programs this can have an impact on compilers and the correctness of code as this problem illustrates consider the following declarations and function definition int typedef struct list struct list next double val list key terms review questions and problems void list l list p for p l p p p next if p val now consider the case in which thread a performs list containing only negative values while thread b performs a what does the function do b the c language only addresses single threaded execution does the use of two parallel threads create any problems or potential problems but some existing optimizing compilers including gcc which tends to be relatively conservative will optimize to something similar to void list l list p register int r r for p l p p p next if p val r r what problem or potential problem occurs with this compiled version of the program if threads a and b are executed concurrently consider the following code using the posix pthreads api c include pthread h include stdlib h include unistd h include stdio h int myglobal void void arg int i j for i i i j myglobal j j printf fflush stdout sleep myglobal j chapter threads return null int main void mythread int i if mythread null null printf ldquo error creating thread abort for i i i myglobal myglobal printf o fflush stdout sleep if mythread null printf error joining thread abort printf nmyglobal equals d n myglobal exit in main we first declare a variable called mythread which has a type of this is essentially an id for a thread next the if statement cre ates a thread associated with mythread the call returns zero on success and a nonzero value on failure the third argument of create is the name of a function that the new thread will execute when it starts when this returns the thread terminates meanwhile the main program itself defines a thread so that there are two threads executing the function enables the main thread to wait until the new thread completes a what does this program accomplish b here is the output from the executed program o o o o oo o o o o o o o o o o o o o o myglobal equals is this the output you would expect if not what has gone wrong the solaris documentation states that a ult may yield to another thread of the same priority isn t it possible that there will be a runnable thread of higher priority and that therefore the yield function should result in yielding to a thread of the same or higher priority in solaris and solaris there is a one to one mapping between ults and lwps in solaris a single lwp supports one or more ults a what is the possible benefit of allowing a many to one mapping of ults to lwps key terms review questions and problems figure solaris user level thread and lwp states b in solaris the thread execution state of a ult is distinct from that of its lwp explain why c figure shows the state transition diagrams for a ult and its associated lwp in solaris and explain the operation of the two diagrams and their relationships explain the rationale for the uninterruptible state in linux chapter concurrency mutual exclusion and synchronization principles of concurrency a simple example race condition operating system concerns process interaction requirements for mutual exclusion mutual exclusion hardware support interrupt disabling special machine instructions semaphores mutual exclusion the producer consumer problem implementation of semaphores monitors monitor with signal alternate model of monitors with notify and broadcast message passing synchronization addressing message format queueing discipline mutual exclusion readers writers problem readers have priority writers have priority summary recommended reading key terms review questions and problems chapter concurrency mutual exclusion and synchronization designing correct routines for controlling concurrent activities proved to be one of the most difficult aspects of systems programming the ad hoc techniques used by programmers of early multiprogramming and real time systems were always vulnerable to subtle programming errors whose effects could be observed only when certain relatively rare sequences of actions occurred the errors are particularly difficult to locate since the precise conditions under which they appear are very hard to reproduce the computer science and engineering research study mit press the central themes of operating system design are all concerned with the manage ment of processes and threads multiprogramming the management of multiple processes within a unipro cessor system multiprocessing the management of multiple processes within a multiprocessor distributed processing the management of multiple processes executing on multiple distributed computer systems the recent proliferation of clusters is a prime example of this type of system fundamental to all of these areas and fundamental to os design is concurrency concurrency encompasses a host of design issues including communication among pro cesses sharing of and competing for resources such as memory files and i o access synchronization of the activities of multiple processes and allocation of processor time to processes we shall see that these issues arise not just in multiprocessing and distrib uted processing environments but even in single processor multiprogramming systems concurrency arises in three different contexts multiple applications multiprogramming was invented to allow processing time to be dynamically shared among a number of active applications structured applications as an extension of the principles of modular design and structured programming some applications can be effectively programmed as a set of concurrent processes chapter concurrency mutual exclusion and synchronization operating system structure the same structuring advantages apply to systems programs and we have seen that operating systems are themselves often im plemented as a set of processes or threads because of the importance of this topic four chapters and an appendix focus on concurrency related issues chapters and deal with concurrency in multipro gramming and multiprocessing systems chapters and examine concurrency issues related to distributed processing this chapter begins with an introduction to the concept of concurrency and the implications of the execution of multiple concurrent processes we find that the basic requirement for support of concurrent processes is the ability to enforce mutual exclu sion that is the ability to exclude all other processes from a course of action while one process is granted that ability next we examine some hardware mechanisms that can support mutual exclusion then we look at solutions that do not involve busy waiting and that can be supported either by the os or enforced by language compilers we examine three approaches semaphores monitors and message passing two classic problems in concurrency are used to illustrate the concepts and compare the approaches presented in this chapter the producer consumer prob lem is introduced in section and used as a running example the chapter closes with the readers writers problem our discussion of concurrency continues in chapter and we defer a discus sion of the concurrency mechanisms of our example systems until the end of that chapter appendix a covers additional topics on concurrency table lists some key terms related to concurrency a set of animations that illustrate concepts in this chapter is available online click on the rotating globe at this book web site at williamstallings com os html for access table some key terms related to concurrency atomic operation a function or action implemented as a sequence of one or more instructions that appears to be indivisible that is no other process can see an intermediate state or interrupt the operation the sequence of instruction is guaranteed to execute as a group or not execute at all having no visible effect on system state atomicity guarantees isolation from concurrent processes critical section a section of code within a process that requires access to shared resources and that must not be executed while another process is in a corresponding section of code deadlock a situation in which two or more processes are unable to proceed because each is waiting for one of the others to do something livelock a situation in which two or more processes continuously change their states in response to changes in the other process es without doing any useful work mutual exclusion the requirement that when one process is in a critical section that accesses shared resources no other process may be in a critical section that accesses any of those shared resources race condition a situation in which multiple threads or processes read and write a shared data item and the final result depends on the relative timing of their execution starvation a situation in which a runnable process is overlooked indefinitely by the scheduler although it is able to proceed it is never chosen simplicity we generally refer to the concurrent execution of processes in fact as we have seen in the preceding chapter in some systems the fundamental unit of concurrency is a thread rather than a process principles of concurrency principles of concurrency in a single processor multiprogramming system processes are interleaved in time to yield the appearance of simultaneous execution figure even though actual parallel processing is not achieved and even though there is a certain amount of overhead involved in switching back and forth between processes interleaved execution provides major benefits in processing efficiency and in program structuring in a multiple processor system it is possible not only to interleave the execution of multiple processes but also to overlap them figure at first glance it may seem that interleaving and overlapping represent funda mentally different modes of execution and present different problems in fact both techniques can be viewed as examples of concurrent processing and both present the same problems in the case of a uniprocessor the problems stem from a basic characteristic of multiprogramming systems the relative speed of execution of processes cannot be predicted it depends on the activities of other processes the way in which the os handles interrupts and the scheduling policies of the os the following difficulties arise the sharing of global resources is fraught with peril for example if two processes both make use of the same global variable and both perform reads and writes on that variable then the order in which the various reads and writes are executed is critical an example of this problem is shown in the following subsection it is difficult for the os to manage the allocation of resources optimally for example process a may request use of and be granted control of a particular i o channel and then be suspended before using that channel it may be unde sirable for the os simply to lock the channel and prevent its use by other pro cesses indeed this may lead to a deadlock condition as described in chapter it becomes very difficult to locate a programming error because results are typically not deterministic and reproducible e g see for a discussion of this point all of the foregoing difficulties present themselves in a multiprocessor system as well because here too the relative speed of execution of processes is unpredictable a multiprocessor system must also deal with problems arising from the simultaneous execution of multiple processes fundamentally however the problems are the same as those for uniprocessor systems this should become clear as the discussion proceeds a simple example consider the following procedure void echo chin getchar chout chin putchar chout chapter concurrency mutual exclusion and synchronization this procedure shows the essential elements of a program that will provide a char acter echo procedure input is obtained from a keyboard one keystroke at a time each input character is stored in variable chin it is then transferred to variable chout and sent to the display any program can call this procedure repeatedly to accept user input and display it on the user screen now consider that we have a single processor multiprogramming system supporting a single user the user can jump from one application to another and each application uses the same keyboard for input and the same screen for output because each application needs to use the procedure echo it makes sense for it to be a shared procedure that is loaded into a portion of memory global to all applications thus only a single copy of the echo procedure is used saving space the sharing of main memory among processes is useful to permit efficient and close interaction among processes however such sharing can lead to problems consider the following sequence process invokes the echo procedure and is interrupted immediately after getchar returns its value and stores it in chin at this point the most recently entered character x is stored in variable chin process is activated and invokes the echo procedure which runs to conclu sion inputting and then displaying a single character y on the screen process is resumed by this time the value x has been overwritten in chin and therefore lost instead chin contains y which is transferred to chout and displayed thus the first character is lost and the second character is displayed twice the essence of this problem is the shared global variable chin multiple processes have access to this variable if one process updates the global variable and then is interrupted another process may alter the variable before the first process can use its value suppose however that we permit only one process at a time to be in that procedure then the foregoing sequence would result in the following process invokes the echo procedure and is interrupted immediately after the conclusion of the input function at this point the most recently entered character x is stored in variable chin process is activated and invokes the echo procedure however because is still inside the echo procedure although currently suspended is blocked from entering the procedure therefore is suspended awaiting the avail ability of the echo procedure at some later time process is resumed and completes execution of echo the proper character x is displayed when exits echo this removes the block on when is later resumed the echo procedure is successfully invoked this example shows that it is necessary to protect shared global variables and other shared global resources and that the only way to do that is to control the code that accesses the variable if we impose the discipline that only one principles of concurrency process at a time may enter echo and that once in echo the procedure must run to completion before it is available for another process then the type of error just discussed will not occur how that discipline may be imposed is a major topic of this chapter this problem was stated with the assumption that there was a single processor multiprogramming os the example demonstrates that the problems of concur rency occur even when there is a single processor in a multiprocessor system the same problems of protected shared resources arise and the same solution works first suppose that there is no mechanism for controlling access to the shared global variable processes and are both executing each on a separate processor both processes invoke the echo procedure the following events occur events on the same line take place in parallel process process chin getchar chin getchar chout chin chout chin putchar chout putchar chout the result is that the character input to is lost before being displayed and the character input to is displayed by both and again let us add the capa bility of enforcing the discipline that only one process at a time may be in echo then the following sequence occurs processes and are both executing each on a separate processor invokes the echo procedure while is inside the echo procedure invokes echo because is still inside the echo procedure whether is suspended or executing is blocked from entering the procedure therefore is suspended awaiting the availability of the echo procedure at a later time process completes execution of echo exits that procedure and continues executing immediately upon the exit of from echo is resumed and begins executing echo in the case of a uniprocessor system the reason we have a problem is that an interrupt can stop instruction execution anywhere in a process in the case of a mul tiprocessor system we have that same condition and in addition a problem can be caused because two processes may be executing simultaneously and both trying to access the same global variable however the solution to both types of problem is the same control access to the shared resource chapter concurrency mutual exclusion and synchronization race condition a race condition occurs when multiple processes or threads read and write data items so that the final result depends on the order of execution of instructions in the multiple processes let us consider two simple examples as a first example suppose that two processes and share the global variable a at some point in its execution updates a to the value and at some point in its execution updates a to the value thus the two tasks are in a race to write variable a in this example the loser of the race the process that updates last determines the final value of a for our second example consider two process and that share global variables b and c with initial values b and c at some point in its execu tion executes the assignment b b c and at some point in its execution executes the assignment c b c note that the two processes update differ ent variables however the final values of the two variables depend on the order in which the two processes execute these two assignments if executes its assignment statement first then the final values are b and c if executes its assign ment statement first then the final values are b and c appendix a includes a discussion of race conditions using semaphores as an example operating system concerns what design and management issues are raised by the existence of concurrency we can list the following concerns the os must be able to keep track of the various processes this is done with the use of process control blocks and was described in chapter the os must allocate and deallocate various resources for each active process at times multiple processes want access to the same resource these resources include processor time this is the scheduling function discussed in part four memory most operating systems use a virtual memory scheme the topic is addressed in part three files discussed in chapter i o devices discussed in chapter the os must protect the data and physical resources of each process against unintended interference by other processes this involves techniques that relate to memory files and i o devices a general treatment of protection is found in part seven the functioning of a process and the output it produces must be independent of the speed at which its execution is carried out relative to the speed of other concurrent processes this is the subject of this chapter to understand how the issue of speed independence can be addressed we need to look at the ways in which processes can interact principles of concurrency process interaction we can classify the ways in which processes interact on the basis of the degree to which they are aware of each other existence table lists three possible degrees of awareness plus the consequences of each processes unaware of each other these are independent processes that are not intended to work together the best example of this situation is the multipro gramming of multiple independent processes these can either be batch jobs or interactive sessions or a mixture although the processes are not working together the os needs to be concerned about competition for resources for example two independent applications may both want to access the same disk or file or printer the os must regulate these accesses processes indirectly aware of each other these are processes that are not nec essarily aware of each other by their respective process ids but that share access to some object such as an i o buffer such processes exhibit cooperation in sharing the common object processes directly aware of each other these are processes that are able to communicate with each other by process id and that are designed to work jointly on some activity again such processes exhibit cooperation conditions will not always be as clear cut as suggested in table rather several processes may exhibit aspects of both competition and cooperation nevertheless it is productive to examine each of the three items in the preceding list separately and determine their implications for the os table process interaction degree of awareness relationship influence that one process has on the other potential control problems processes unaware of each other competition results of one process independent of the action of others timing of process may be affected mutual exclusion deadlock renewable resource starvation processes indirectly aware of each other e g shared object cooperation by sharing results of one process may depend on infor mation obtained from others timing of process may be affected mutual exclusion deadlock renewable resource starvation data coherence processes directly aware of each other have communication primitives available to them cooperation by commu nication results of one process may depend on infor mation obtained from others timing of process may be affected deadlock consum able resource starvation chapter concurrency mutual exclusion and synchronization competition among processes for resources concurrent processes come into conflict with each other when they are competing for the use of the same resource in its pure form we can describe the situation as follows two or more processes need to access a resource during the course of their execution each process is unaware of the existence of other processes and each is to be unaffected by the execution of the other processes it follows from this that each process should leave the state of any resource that it uses unaffected examples of resources include i o devices memory processor time and the clock there is no exchange of information between the competing processes however the execution of one process may affect the behavior of competing processes in particular if two processes both wish access to a single resource then one process will be allocated that resource by the os and the other will have to wait therefore the process that is denied access will be slowed down in an extreme case the blocked process may never get access to the resource and hence will never termi nate successfully in the case of competing processes three control problems must be faced first is the need for mutual exclusion suppose two or more processes require access to a single nonsharable resource such as a printer during the course of execution each process will be sending commands to the i o device receiving status information sending data and or receiving data we will refer to such a resource as a critical resource and the portion of the program that uses it as a critical section of the program it is important that only one program at a time be allowed in its critical section we cannot simply rely on the os to understand and enforce this restriction because the detailed requirements may not be obvious in the case of the printer for example we want any individual process to have con trol of the printer while it prints an entire file otherwise lines from competing processes will be interleaved the enforcement of mutual exclusion creates two additional control problems one is that of deadlock for example consider two processes and and two resources and suppose that each process needs access to both resources to perform part of its function then it is possible to have the following situation the os assigns to and to each process is waiting for one of the two resources neither will release the resource that it already owns until it has acquired the other resource and performed the function requiring both resources the two processes are deadlocked a final control problem is starvation suppose that three processes each require periodic access to resource r consider the situation in which is in possession of the resource and both and are delayed waiting for that resource when exits its critical section either or should be allowed access to r assume that the os grants access to and that again requires access before completes its critical section if the os grants access to after has finished and subsequently alternately grants access to and then may indefinitely be denied access to the resource even though there is no deadlock situation control of competition inevitably involves the os because it is the os that allocates resources in addition the processes themselves will need to be able to principles of concurrency process void process void process n void pn while true while true while true preceding code entercritical ra preceding code entercritical ra preceding code entercritical ra critical section critical section critical section exitcritical ra exitcritical ra exitcritical ra following code following code following code figure illustration of mutual exclusion express the requirement for mutual exclusion in some fashion such as locking a resource prior to its use any solution will involve some support from the os such as the provision of the locking facility figure illustrates the mutual exclusion mechanism in abstract terms there are n processes to be executed concurrently each process includes a critical section that operates on some resource ra and additional code preceding and following the critical section that does not involve access to ra because all processes access the same resource ra it is desired that only one process at a time be in its critical section to enforce mutual exclusion two functions are provided entercritical and exitcritical each function takes as an argument the name of the resource that is the subject of competition any process that attempts to enter its critical section while another process is in its critical section for the same resource is made to wait it remains to examine specific mechanisms for providing the functions entercritical and exitcritical for the moment we defer this issue while we consider the other cases of process interaction cooperation among processes by sharing the case of cooperation by sharing covers processes that interact with other processes without being explicitly aware of them for example multiple processes may have access to shared variables or to shared files or databases processes may use and update the shared data without reference to other processes but know that other processes may have access to the same data thus the processes must cooperate to ensure that the data they share are properly managed the control mechanisms must ensure the integrity of the shared data because data are held on resources devices memory the control problems of mutual exclusion deadlock and starvation are again present the only difference is that data items may be accessed in two different modes reading and writing and only writing operations must be mutually exclusive however over and above these problems a new requirement is introduced that of data coherence as a simple example consider a bookkeeping application in which various data items may be updated suppose two items of data a and b are to be maintained in the relationship a b that is any program that updates one value chapter concurrency mutual exclusion and synchronization must also update the other to maintain the relationship now consider the following two processes a a b b b b a a if the state is initially consistent each process taken separately will leave the shared data in a consistent state now consider the following concurrent execution sequence in which the two processes respect mutual exclusion on each individual data item a and b a a b b b b a a at the end of this execution sequence the condition a b no longer holds for example if we start with a b at the end of this execution sequence we have a and b the problem can be avoided by declaring the entire sequence in each process to be a critical section thus we see that the concept of critical section is important in the case of cooperation by sharing the same abstract functions of entercritical and exitcritical discussed earlier figure can be used here in this case the argument for the functions could be a variable a file or any other shared object furthermore if critical sections are used to provide data integrity then there may be no specific resource or variable that can be identified as an argument in that case we can think of the argument as being an identifier that is shared among con current processes to identify critical sections that must be mutually exclusive cooperation among processes by communication in the first two cases that we have discussed each process has its own isolated environment that does not include the other processes the interactions among processes are indirect in both cases there is a sharing in the case of competition they are sharing resources without being aware of the other processes in the second case they are sharing values and although each process is not explicitly aware of the other processes it is aware of the need to maintain data integrity when processes cooperate by communication however the various processes participate in a common effort that links all of the processes the communication provides a way to synchronize or coordinate the various activities typically communication can be characterized as consisting of messages of some sort primitives for sending and receiving messages may be provided as part of the programming language or provided by the os kernel because nothing is shared between processes in the act of passing messages mutual exclusion is not a control requirement for this sort of cooperation however mutual exclusion hardware support the problems of deadlock and starvation are still present as an example of dead lock two processes may be blocked each waiting for a communication from the other as an example of starvation consider three processes and that exhibit the following behavior is repeatedly attempting to communicate with either or and and are both attempting to communicate with a sequence could arise in which and exchange information repeatedly while is blocked waiting for a communication from there is no deadlock because remains active but is starved requirements for mutual exclusion any facility or capability that is to provide support for mutual exclusion should meet the following requirements mutual exclusion must be enforced only one process at a time is allowed into its critical section among all processes that have critical sections for the same resource or shared object a process that halts in its noncritical section must do so without interfering with other processes it must not be possible for a process requiring access to a critical section to be delayed indefinitely no deadlock or starvation when no process is in a critical section any process that requests entry to its critical section must be permitted to enter without delay no assumptions are made about relative process speeds or number of processors a process remains inside its critical section for a finite time only there are a number of ways in which the requirements for mutual exclusion can be satisfied one approach is to leave the responsibility with the processes that wish to execute concurrently processes whether they are system programs or application programs would be required to coordinate with one another to enforce mutual exclusion with no support from the programming language or the os we can refer to these as software approaches although this approach is prone to high processing overhead and bugs it is nevertheless useful to examine such approaches to gain a better understanding of the complexity of concurrent processing this topic is covered in appendix a a second approach involves the use of special purpose machine instructions these have the advantage of reducing overhead but nevertheless will be shown to be unattractive as a general purpose solution they are covered in section a third approach is to provide some level of support within the os or a programming language three of the most important such approaches are examined in sections through mutual exclusion hardware support in this section we look at several interesting hardware approaches to mutual exclusion chapter concurrency mutual exclusion and synchronization interrupt disabling in a uniprocessor system concurrent processes cannot have overlapped execution they can only be interleaved furthermore a process will continue to run until it invokes an os service or until it is interrupted therefore to guarantee mutual exclusion it is sufficient to prevent a process from being interrupted this capability can be provided in the form of primitives defined by the os kernel for disabling and enabling interrupts a process can then enforce mutual exclusion in the following way compare figure while true disable interrupts critical section enable interrupts remainder because the critical section cannot be interrupted mutual exclusion is guar anteed the price of this approach however is high the efficiency of execution could be noticeably degraded because the processor is limited in its ability to interleave processes another problem is that this approach will not work in a multiprocessor architecture when the computer includes more than one proces sor it is possible and typical for more than one process to be executing at a time in this case disabled interrupts do not guarantee mutual exclusion special machine instructions in a multiprocessor configuration several processors share access to a common main memory in this case there is not a master slave relationship rather the pro cessors behave independently in a peer relationship there is no interrupt mecha nism between processors on which mutual exclusion can be based at the hardware level as was mentioned access to a memory location excludes any other access to that same location with this as a foundation proc essor designers have proposed several machine instructions that carry out two actions atomically such as reading and writing or reading and testing of a single memory location with one instruction fetch cycle during execution of the instruc tion access to the memory location is blocked for any other instruction referencing that location in this section we look at two of the most commonly implemented instruc tions others are described in and compare swap instruction the compare swap instruction also called a compare and exchange instruction can be defined as follows term atomic means that the instruction is treated as a single step that cannot be interrupted mutual exclusion hardware support int int word int testval int newval int oldval oldval word if oldval testval word newval return oldval this version of the instruction checks a memory location word against a test value testval if the memory location current value is testval it is replaced with newval otherwise it is left unchanged the old memory value is always returned thus the memory location has been updated if the returned value is the same as the test value this atomic instruction therefore has two parts a compare is made between a memory value and a test value if the values are the same a swap occurs the entire compare swap function is carried out atomically that is it is not sub ject to interruption another version of this instruction returns a boolean value true if the swap occurred false otherwise some version of this instruction is available on nearly all processor families sparc ibm z series etc and most operating systems use this instruction for support of concurrency figure shows a mutual exclusion protocol based on the use of this instruc tion a shared variable bolt is initialized to the only process that may enter its critical section is one that finds bolt equal to all other processes attempting a compare and swap instruction b exchange instruction figure hardware support for mutual exclusion construct parbegin pn means the following suspend the execution of the main program initiate concurrent execution of procedures pn when all of pn have ter minated resume the main program chapter concurrency mutual exclusion and synchronization to enter their critical section go into a busy waiting mode the term busy waiting or spin waiting refers to a technique in which a process can do nothing until it gets permission to enter its critical section but continues to execute an instruction or set of instructions that tests the appropriate variable to gain entrance when a process leaves its critical section it resets bolt to at this point one and only one of the wait ing processes is granted access to its critical section the choice of process depends on which process happens to execute the compare swap instruction next exchange instruction the exchange instruction can be defined as follows void exchange int register int memory int temp temp memory memory register register temp the instruction exchanges the contents of a register with that of a memory location both the intel ia architecture pentium and the ia architecture itanium contain an xchg instruction figure shows a mutual exclusion protocol based on the use of an exchange instruction a shared variable bolt is initialized to each process uses a local vari able key that is initialized to the only process that may enter its critical section is one that finds bolt equal to it excludes all other processes from the critical sec tion by setting bolt to when a process leaves its critical section it resets bolt to allowing another process to gain access to its critical section note that the following expression always holds because of the way in which the variables are initialized and because of the nature of the exchange algorithm bolt a keyi n i if bolt then no process is in its critical section if bolt then exactly one pro cess is in its critical section namely the process whose key value equals properties of the machine instruction approach the use of a special machine instruction to enforce mutual exclusion has a number of advantages it is applicable to any number of processes on either a single processor or mul tiple processors sharing main memory it is simple and therefore easy to verify it can be used to support multiple critical sections each critical section can be defined by its own variable there are some serious disadvantages busy waiting is employed thus while a process is waiting for access to a criti cal section it continues to consume processor time semaphores starvation is possible when a process leaves a critical section and more than one process is waiting the selection of a waiting process is arbitrary thus some process could indefinitely be denied access deadlock is possible consider the following scenario on a single processor system process executes the special instruction e g compare swap exchange and enters its critical section is then interrupted to give the processor to which has higher priority if now attempts to use the same resource as it will be denied access because of the mutual exclusion mecha nism thus it will go into a busy waiting loop however will never be dis patched because it is of lower priority than another ready process because of the drawbacks of both the software and hardware solutions we need to look for other mechanisms semaphores we now turn to os and programming language mechanisms that are used to pro vide concurrency table summarizes mechanisms in common use we begin in this section with semaphores the next two sections discuss monitors and message passing the other mechanisms in table are discussed when treating specific os examples in chapters and table common concurrency mechanisms semaphore an integer value used for signaling among processes only three operations may be performed on a semaphore all of which are atomic initialize decrement and incre ment the decrement operation may result in the blocking of a process and the incre ment operation may result in the unblocking of a process also known as a counting semaphore or a general semaphore binary semaphore a semaphore that takes on only the values and mutex similar to a binary semaphore a key difference between the two is that the process that locks the mutex sets the value to zero must be the one to unlock it sets the value to condition variable a data type that is used to block a process or thread until a particular condition is true monitor a programming language construct that encapsulates variables access procedures and initialization code within an abstract data type the monitor variable may only be accessed via its access procedures and only one process may be actively accessing the monitor at any one time the access procedures are critical sections a monitor may have a queue of processes that are waiting to access it event flags a memory word used as a synchronization mechanism application code may associ ate a different event with each bit in a flag a thread can wait for either a single event or a combination of events by checking one or multiple bits in the corresponding flag the thread is blocked until all of the required bits are set and or until at least one of the bits is set or mailboxes messages a means for two processes to exchange information and that may be used for synchronization spinlocks mutual exclusion mechanism in which a process executes in an infinite loop waiting for the value of a lock variable to indicate availability chapter concurrency mutual exclusion and synchronization the first major advance in dealing with the problems of concurrent proc esses came in with dijkstra treatise dijkstra was concerned with the design of an os as a collection of cooperating sequential processes and with the development of efficient and reliable mechanisms for supporting cooperation these mechanisms can just as readily be used by user processes if the processor and os make the mechanisms available the fundamental principle is this two or more processes can cooperate by means of simple signals such that a process can be forced to stop at a specified place until it has received a specific signal any complex coordination requirement can be satisfied by the appropriate structure of signals for signaling special variables called semaphores are used to transmit a signal via semaphore a process exe cutes the primitive semsignal to receive a signal via semaphore a process executes the primitive semwait if the corresponding signal has not yet been transmitted the process is suspended until the transmission takes place to achieve the desired effect we can view the semaphore as a variable that has an integer value upon which only three operations are defined a semaphore may be initialized to a nonnegative integer value the semwait operation decrements the semaphore value if the value becomes negative then the process executing the semwait is blocked otherwise the process continues execution the semsignal operation increments the semaphore value if the resulting value is less than or equal to zero then a process blocked by a semwait oper ation if any is unblocked other than these three operations there is no way to inspect or manipulate semaphores we explain these operations as follows to begin the semaphore has a zero or positive value if the value is positive that value equals the number of processes that can issue a wait and immediately continue to execute if the value is zero either by initialization or because a number of processes equal to the initial semaphore value have issued a wait the next process to issue a wait is blocked and the semaphore value goes negative each subsequent wait drives the semaphore value further into minus territory the negative value equals the number of processes waiting to be unblocked each signal unblocks one of the waiting processes when the semaphore value is negative points out three interesting consequences of the semaphore definition in general there is no way to know before a process decrements a semaphore whether it will block or not dijkstra original paper and in much of the literature the letter p is used for semwait and the letter v for semsignal these are the initials of the dutch words for test proberen and increment verhogen in some of the literature the terms wait and signal are used this book uses semwait and semsig nal for clarity and to avoid confusion with similar wait and signal operations in monitors discussed subsequently semaphores figure a deﬁnition of semaphore primitives after a process increments a semaphore and another process gets woken up both processes continue running concurrently there is no way to know which process if either will continue immediately on a uniprocessor system when you signal a semaphore you don t necessarily know whether another process is waiting so the number of unblocked processes may be zero or one figure suggests a more formal definition of the primitives for sema phores the semwait and semsignal primitives are assumed to be atomic a more restricted version known as the binary semaphore is defined in figure a binary semaphore may only take on the values and and can be defined by the following three operations a binary semaphore may be initialized to or the semwaitb operation checks the semaphore value if the value is zero then the process executing the semwaitb is blocked if the value is one then the value is changed to zero and the process continues execution the semsignalb operation checks to see if any processes are blocked on this semaphore semaphore value equals if so then a process blocked by a semwaitb operation is unblocked if no processes are blocked then the value of the semaphore is set to one in principle it should be easier to implement the binary semaphore and it can be shown that it has the same expressive power as the general semaphore see problem to contrast the two types of semaphores the nonbinary semaphore is often referred to as either a counting semaphore or a general semaphore a concept related to the binary semaphore is the mutex a key difference between the two is that the process that locks the mutex sets the value to zero chapter concurrency mutual exclusion and synchronization figure a deﬁnition of binary semaphore primitives must be the one to unlock it sets the value to in contrast it is possible for one process to lock a binary semaphore and for another to unlock it for both counting semaphores and binary semaphores a queue is used to hold processes waiting on the semaphore the question arises of the order in which processes are removed from such a queue the fairest removal policy is first in first out fifo the process that has been blocked the longest is released from the queue first a semaphore whose definition includes this policy is called a strong semaphore a semaphore that does not specify the order in which proc esses are removed from the queue is a weak semaphore figure based on one in is an example of the operation of a strong semaphore here processes a b and c depend on a result from process d initially a is run ning b c and d are ready and the semaphore count is indicating that one of d results is available when a issues a semwait instruction on semaphore the semaphore decrements to and a can continue to execute subsequently it rejoins the ready queue then b runs eventually issues a semwait instruc tion and is blocked allowing d to run when d completes a new result it issues a semsignal instruction which allows b to move to the ready queue d rejoins the ready queue and c begins to run but is blocked when it issues a semwait instruction similarly a and b run and are blocked on the semaphore allowing d to resume execution when d has a result it issues a semsignal which transfers c to the ready queue later cycles of d will release a and b from the blocked state some of the literature and in some textbooks no distinction is made between a mutex and a binary semaphore however in practice a number of operating systems such as linux windows and solaris offer a mutex facility which conforms to the definition in this book processor semaphores blocked queue blocked queue blocked queue blocked queue blocked queue blocked queue blocked queue semaphore processor semaphore processor semaphore processor semaphore processor semaphore processor semaphore processor semaphore ready queue ready queue ready queue ready queue ready queue ready queue ready queue figure example of semaphore mechanism chapter concurrency mutual exclusion and synchronization figure mutual exclusion using semaphores for the mutual exclusion algorithm discussed in the next subsection and illus trated in figure strong semaphores guarantee freedom from starvation while weak semaphores do not we will assume strong semaphores because they are more convenient and because this is the form of semaphore typically provided by operat ing systems mutual exclusion figure shows a straightforward solution to the mutual exclusion problem using a semaphore compare figure consider n processes identified in the array p i all of which need access to the same resource each process has a critical sec tion used to access the resource in each process a semwait is executed just before its critical section if the value of becomes negative the process is blocked if the value is then it is decremented to and the process immediately enters its critical section because is no longer positive no other process will be able to enter its critical section the semaphore is initialized to thus the first process that executes a semwait will be able to enter the critical section immediately setting the value of to any other process attempting to enter the critical section will find it busy and will be blocked setting the value of to any number of processes may attempt entry each such unsuccessful attempt results in a further decrement of the value of when the process that initially entered its critical section departs is incremented and one of the blocked processes if any is removed from the queue of blocked processes associated with the semaphore and put in a ready state when it is next scheduled by the os it may enter the critical section figure based on one in shows a possible sequence for three processes using the mutual exclusion discipline of figure in this example three processes a b c access a shared resource protected by the semaphore lock process a executes semwait lock because the semaphore has a value of at the time of the semwait operation a can immediately enter its critical section and the semaphore takes on the value while a is in its critical section both b and c semaphores queue for semaphore lock value of semaphore lock a b c critical region normal execution blocked on semaphore lock note that normal execution can proceed in parallel but that critical regions are serialized figure processes accessing shared data protected by a semaphore perform a semwait operation and are blocked pending the availability of the sema phore when a exits its critical section and performs semsignal lock b which was the first process in the queue can now enter its critical section the program of figure can equally well handle a requirement that more than one process be allowed in its critical section at a time this requirement is met simply by initializing the semaphore to the specified value thus at any time the value of count can be interpreted as follows count count is the number of processes that can execute semwait without suspension if no semsignal is executed in the meantime such situations will allow semaphores to support synchronization as well as mutual exclusion count the magnitude of count is the number of processes suspended in queue the producer consumer problem we now examine one of the most common problems faced in concurrent process ing the producer consumer problem the general statement is this there are one or more producers generating some type of data records characters and placing chapter concurrency mutual exclusion and synchronization these in a buffer there is a single consumer that is taking items out of the buffer one at a time the system is to be constrained to prevent the overlap of buffer oper ations that is only one agent producer or consumer may access the buffer at any one time the problem is to make sure that the producer won t try to add data into the buffer if it full and that the consumer won t try to remove data from an empty buffer we will look at a number of solutions to this problem to illustrate both the power and the pitfalls of semaphores to begin let us assume that the buffer is infinite and consists of a linear array of elements in abstract terms we can define the producer and consumer functions as follows producer consumer while true while true produce item v while in out b in v do nothing in w b out out consume item w figure illustrates the structure of buffer b the producer can generate items and store them in the buffer at its own pace each time an index in into the buffer is incremented the consumer proceeds in a similar fashion but must make sure that it does not attempt to read from an empty buffer hence the consumer makes sure that the producer has advanced beyond it in out before proceeding let us try to implement this system using binary semaphores figure is a first attempt rather than deal with the indices in and out we can simply keep track of the number of items in the buffer using the integer variable n in out the semaphore is used to enforce mutual exclusion the semaphore delay is used to force the consumer to semwait if the buffer is empty this solution seems rather straightforward the producer is free to add to the buffer at any time it performs semwaitb before appending and semsignalb afterward to prevent the consumer or any other producer from b b b b b out in note shaded area indicates portion of buffer that is occupied figure inﬁnite buffer for the producer consumer problem semaphores figure an incorrect solution to the inﬁnite buffer producer consumer problem using binary semaphores accessing the buffer during the append operation also while in the critical section the producer increments the value of n if n then the buffer was empty just prior to this append so the producer performs semsignalb delay to alert the con sumer of this fact the consumer begins by waiting for the first item to be produced using semwaitb delay it then takes an item and decrements n in its critical section if the producer is able to stay ahead of the consumer a common situation then the consumer will rarely block on the semaphore delay because n will usually be positive hence both producer and consumer run smoothly there is however a flaw in this program when the consumer has exhausted the buffer it needs to reset the delay semaphore so that it will be forced to wait until the producer has placed more items in the buffer this is the purpose of the state ment if n semwaitb delay consider the scenario outlined in table in line the consumer fails to execute the semwaitb operation the consumer did indeed exhaust the buffer and set n to line but the producer has incremented n before the consumer can test it in line the result is a semsignalb not matched by a prior semwaitb the value of for n in line means that the consumer has consumed an item from the buffer that does not exist it would not do simply to move the conditional statement inside the critical section of the consumer because this could lead to deadlock e g after line of table chapter concurrency mutual exclusion and synchronization table possible scenario for the program of figure producer consumer n delay semwaitb n if n semsignalb delay semsignalb semwaitb delay semwaitb n semsignalb semwaitb n if n semsignalb delay semsignalb if n semwaitb delay semwaitb n semsignalb if n semwaitb delay semwaitb n semsignalb note white areas represent the critical section controlled by semaphore a fix for the problem is to introduce an auxiliary variable that can be set in the consumer critical section for use later on this is shown in figure a careful trace of the logic should convince you that deadlock can no longer occur a somewhat cleaner solution can be obtained if general semaphores also called counting semaphores are used as shown in figure the variable n is now a semaphore its value still is equal to the number of items in the buffer suppose now that in transcribing this program a mistake is made and the opera tions semsignal and semsignal n are interchanged this would require that the semsignal n operation be performed in the producer critical sec tion without interruption by the consumer or another producer would this affect semaphores figure a correct solution to the inﬁnite buffer producer consumer problem using binary semaphores the program no because the consumer must wait on both semaphores before proceeding in any case now suppose that the semwait n and semwait operations are acci dentally reversed this produces a serious indeed a fatal flaw if the consumer ever enters its critical section when the buffer is empty n count then no producer can ever append to the buffer and the system is deadlocked this is a good example of the subtlety of semaphores and the difficulty of producing correct designs finally let us add a new and realistic restriction to the producer consumer problem namely that the buffer is finite the buffer is treated as a circular storage figure and pointer values must be expressed modulo the size of the buffer the following relationships hold block on unblock on producer insert in full buffer consumer item inserted consumer remove from empty buffer producer item removed chapter concurrency mutual exclusion and synchronization figure a solution to the inﬁnite buffer producer consumer problem using semaphores the producer and consumer functions can be expressed as follows variable in and out are initialized to and n is the size of the buffer producer consumer while true while true produce item v while in out while in n out do nothing do nothing w b out b in v out out n in in n consume item w figure shows a solution using general semaphores the semaphore e has been added to keep track of the number of empty spaces another instructive example in the use of semaphores is the barbershop prob lem described in appendix a appendix a also includes additional examples of the problem of race conditions when using semaphores implementation of semaphores as was mentioned earlier it is imperative that the semwait and semsignal oper ations be implemented as atomic primitives one obvious way is to implement them semaphores out in a b b b b b b n in out b figure finite circular buffer for the producer consumer problem in hardware or firmware failing this a variety of schemes have been suggested the essence of the problem is one of mutual exclusion only one process at a time may manipulate a semaphore with either a semwait or semsignal operation thus any of the software schemes such as dekker algorithm or peterson algorithm appendix a could be used this would entail a substantial processing overhead figure a solution to the bounded buffer producer consumer problem using semaphores chapter concurrency mutual exclusion and synchronization semwait while flag do nothing count if count place this process in queue block this process must also set flag to flag semsignal while flag do nothing count if count remove a process p from queue place process p on ready list flag semwait inhibit interrupts count if count place this process in queue block this process and allow inter rupts else allow interrupts semsignal inhibit interrupts count if count remove a process p from queue place process p on ready list allow interrupts a compare and swap instruction b interrupts figure two possible implementations of semaphores another alternative is to use one of the hardware supported schemes for mutual exclusion for example figure shows the use of a compare swap instruc tion in this implementation the semaphore is again a structure as in figure but now includes a new integer component flag admittedly this involves a form of busy waiting however the semwait and semsignal operations are relatively short so the amount of busy waiting involved should be minor for a single processor system it is possible to inhibit interrupts for the duration of a semwait or semsignal operation as suggested in figure once again the relatively short duration of these operations means that this approach is reasonable monitors semaphores provide a primitive yet powerful and flexible tool for enforcing mutual exclusion and for coordinating processes however as figure suggests it may be difficult to produce a correct program using semaphores the difficulty is that sem wait and semsignal operations may be scattered throughout a program and it is not easy to see the overall effect of these operations on the semaphores they affect the monitor is a programming language construct that provides equivalent functionality to that of semaphores and that is easier to control the concept was first formally defined in the monitor construct has been implemented in a number of programming languages including concurrent pascal pascal plus modula modula and java it has also been implemented as a program library this allows programmers to put a monitor lock on any object in particular for monitors something like a linked list you may want to lock all linked lists with one lock or have one lock for each list or have one lock for each element of each list we begin with a look at hoare version and then examine a refinement monitor with signal a monitor is a software module consisting of one or more procedures an initial ization sequence and local data the chief characteristics of a monitor are the following the local data variables are accessible only by the monitor procedures and not by any external procedure a process enters the monitor by invoking one of its procedures only one process may be executing in the monitor at a time any other pro cesses that have invoked the monitor are blocked waiting for the monitor to become available the first two characteristics are reminiscent of those for objects in object oriented software indeed an object oriented os or programming language can readily implement a monitor as an object with special characteristics by enforcing the discipline of one process at a time the monitor is able to pro vide a mutual exclusion facility the data variables in the monitor can be accessed by only one process at a time thus a shared data structure can be protected by placing it in a monitor if the data in a monitor represent some resource then the monitor provides a mutual exclusion facility for accessing the resource to be useful for concurrent processing the monitor must include synchroni zation tools for example suppose a process invokes the monitor and while in the monitor must be blocked until some condition is satisfied a facility is needed by which the process is not only blocked but releases the monitor so that some other process may enter it later when the condition is satisfied and the monitor is again available the process needs to be resumed and allowed to reenter the monitor at the point of its suspension a monitor supports synchronization by the use of condition variables that are contained within the monitor and accessible only within the monitor condition var iables are a special data type in monitors which are operated on by two functions cwait c suspend execution of the calling process on condition c the mon itor is now available for use by another process csignal c resume execution of some process blocked after a cwait on the same condition if there are several such processes choose one of them if there is no such process do nothing note that monitor wait and signal operations are different from those for the semaphore if a process in a monitor signals and no task is waiting on the condition variable the signal is lost figure illustrates the structure of a monitor although a process can enter the monitor by invoking any of its procedures we can think of the monitor as hav ing a single entry point that is guarded so that only one process may be in the moni tor at a time other processes that attempt to enter the monitor join a queue of chapter concurrency mutual exclusion and synchronization figure structure of a monitor processes blocked waiting for monitor availability once a process is in the monitor it may temporarily block itself on condition x by issuing cwait x it is then placed in a queue of processes waiting to reenter the monitor when the condition changes and resume execution at the point in its program following the cwait x call if a process that is executing in the monitor detects a change in the condition variable x it issues csignal x which alerts the corresponding condition queue that the condition has changed as an example of the use of a monitor let us return to the bounded buffer producer consumer problem figure shows a solution using a monitor the monitor module boundedbuffer controls the buffer used to store and retrieve characters the monitor includes two condition variables declared with the con struct cond notfull is true when there is room to add at least one character to the buffer and notempty is true when there is at least one character in the buffer monitors figure a solution to the bounded buffer producer consumer problem using a monitor a producer can add characters to the buffer only by means of the procedure append inside the monitor the producer does not have direct access to buffer the procedure first checks the condition notfull to determine if there is space available in the buffer if not the process executing the monitor is blocked on that condition some other process producer or consumer may now enter the monitor later when the buffer is no longer full the blocked process may be removed from the queue reactivated and resume processing after placing a character in the buffer chapter concurrency mutual exclusion and synchronization the process signals the notempty condition a similar description can be made of the consumer function this example points out the division of responsibility with monitors compared to semaphores in the case of monitors the monitor construct itself enforces mutual exclusion it is not possible for both a producer and a consumer simultaneously to access the buffer however the programmer must place the appropriate cwait and csignal primitives inside the monitor to prevent processes from depositing items in a full buffer or removing them from an empty one in the case of semaphores both mutual exclusion and synchronization are the responsibility of the programmer note that in figure a process exits the monitor immediately after executing the csignal function if the csignal does not occur at the end of the procedure then in hoare proposal the process issuing the signal is blocked to make the moni tor available and placed in a queue until the monitor is free one possibility at this point would be to place the blocked process in the entrance queue so that it would have to compete for access with other processes that had not yet entered the monitor however because a process blocked on a csignal function has already partially performed its task in the monitor it makes sense to give this process precedence over newly entering processes by setting up a separate urgent queue figure one language that uses monitors concurrent pascal requires that csignal only appear as the last operation executed by a monitor procedure if there are no processes waiting on condition x then the execution of csignal x has no effect as with semaphores it is possible to make mistakes in the synchroniza tion function of monitors for example if either of the csignal functions in the boundedbuffer monitor are omitted then processes entering the corresponding condition queue are permanently hung up the advantage that monitors have over semaphores is that all of the synchronization functions are confined to the monitor therefore it is easier to verify that the synchronization has been done correctly and to detect bugs furthermore once a monitor is correctly programmed access to the protected resource is correct for access from all processes in contrast with sema phores resource access is correct only if all of the processes that access the resource are programmed correctly alternate model of monitors with notify and broadcast hoare definition of monitors requires that if there is at least one pro cess in a condition queue a process from that queue runs immediately when another process issues a csignal for that condition thus the process issuing the csignal must either immediately exit the monitor or be blocked on the monitor there are two drawbacks to this approach if the process issuing the csignal has not finished with the monitor then two additional process switches are required one to block this process and another to resume it when the monitor becomes available process scheduling associated with a signal must be perfectly reliable when a csignal is issued a process from the corresponding condition queue must be activated immediately and the scheduler must ensure that no other process monitors enters the monitor before activation otherwise the condition under which the process was activated could change for example in figure when a csignal notempty is issued a process from the notempty queue must be activated before a new consumer enters the monitor another example a producer process may append a character to an empty buffer and then fail before signaling any processes in the notempty queue would be permanently hung up lampson and redell developed a different definition of monitors for the lan guage mesa their approach overcomes the problems just listed and supports several useful extensions the mesa monitor structure is also used in the modula systems programming language in mesa the csignal prim itive is replaced by cnotify with the following interpretation when a process executing in a monitor executes cnotify x it causes the x condition queue to be notified but the signaling process continues to execute the result of the notifica tion is that the process at the head of the condition queue will be resumed at some convenient future time when the monitor is available however because there is no guarantee that some other process will not enter the monitor before the waiting process the waiting process must recheck the condition for example the proce dures in the boundedbuffer monitor would now have the code of figure the if statements are replaced by while loops thus this arrangement results in at least one extra evaluation of the condition variable in return however there are no extra process switches and no constraints on when the waiting process must run after a cnotify one useful refinement that can be associated with the cnotify primitive is a watchdog timer associated with each condition primitive a process that has been waiting for the maximum timeout interval will be placed in a ready state regard less of whether the condition has been notified when activated the process checks the condition and continues if the condition is satisfied the timeout prevents the indefinite starvation of a process in the event that some other process fails before signaling a condition figure bounded buffer monitor code for mesa monitor chapter concurrency mutual exclusion and synchronization with the rule that a process is notified rather than forcibly reactivated it is possible to add a cbroadcast primitive to the repertoire the broadcast causes all processes waiting on a condition to be placed in a ready state this is convenient in situations where a process does not know how many other processes should be reactivated for example in the producer consumer program suppose that both the append and the take functions can apply to variable length blocks of charac ters in that case if a producer adds a block of characters to the buffer it need not know how many characters each waiting consumer is prepared to consume it sim ply issues a cbroadcast and all waiting processes are alerted to try again in addition a broadcast can be used when a process would have difficulty fig uring out precisely which other process to reactivate a good example is a memory manager the manager has j bytes free a process frees up an additional k bytes but it does not know which waiting process can proceed with a total of k j bytes hence it uses broadcast and all processes check for themselves if there is enough memory free an advantage of lampson redell monitors over hoare monitors is that the lampson redell approach is less prone to error in the lampson redell approach because each procedure checks the monitor variable after being signaled with the use of the while construct a process can signal or broadcast incorrectly without causing an error in the signaled program the signaled program will check the rel evant variable and if the desired condition is not met continue to wait another advantage of the lampson redell monitor is that it lends itself to a more modular approach to program construction for example consider the imple mentation of a buffer allocator there are two levels of conditions to be satisfied for cooperating sequential processes consistent data structures thus the monitor enforces mutual exclusion and completes an input or output operation before allowing another operation on the buffer level plus enough memory for this process to complete its allocation request in the hoare monitor each signal conveys the level condition but also car ries the implicit message i have freed enough bytes for your particular allocate call to work now thus the signal implicitly carries the level condition if the pro grammer later changes the definition of the level condition it will be necessary to reprogram all signaling processes if the programmer changes the assumptions made by any particular waiting process i e waiting for a slightly different level invari ant it may be necessary to reprogram all signaling processes this is unmodular and likely to cause synchronization errors e g wake up by mistake when the code is modified the programmer has to remember to modify all procedures in the monitor every time a small change is made to the level condition with a lampson redell monitor a broadcast ensures the level condition and carries a hint that level might hold each process should check the level condition itself if a change is made in the level condition in either a waiter or a signaler there is no possibility of errone ous wakeup because each procedure checks its own level condition therefore the level condition can be hidden within each procedure with the hoare monitor the level condition must be carried from the waiter into the code of every signaling process which violates data abstraction and interprocedural modularity principles message passing message passing when processes interact with one another two fundamental requirements must be satisfied synchronization and communication processes need to be synchro nized to enforce mutual exclusion cooperating processes may need to exchange information one approach to providing both of these functions is message passing message passing has the further advantage that it lends itself to implementation in distributed systems as well as in shared memory multiprocessor and uniprocessor systems message passing systems come in many forms in this section we provide a general introduction that discusses features typically found in such systems the actual function of message passing is normally provided in the form of a pair of primitives send destination message receive source message this is the minimum set of operations needed for processes to engage in mes sage passing a process sends information in the form of a message to another proc ess designated by a destination a process receives information by executing the receive primitive indicating the source and the message a number of design issues relating to message passing systems are listed in table and examined in the remainder of this section table design characteristics of message systems for interprocess communication and synchronization synchronization format send content blocking length nonblocking fixed receive variable blocking nonblocking queueing discipline test for arrival fifo priority addressing direct send receive explicit implicit indirect static dynamic ownership chapter concurrency mutual exclusion and synchronization synchronization the communication of a message between two processes implies some level of syn chronization between the two the receiver cannot receive a message until it has been sent by another process in addition we need to specify what happens to a process after it issues a send or receive primitive consider the send primitive first when a send primitive is executed in a process there are two possibilities either the sending process is blocked until the message is received or it is not similarly when a process issues a receive primi tive there are two possibilities if a message has previously been sent the message is received and execution continues if there is no waiting message then either a the process is blocked until a message arrives or b the process continues to execute abandoning the attempt to receive thus both the sender and receiver can be blocking or nonblocking three combinations are common although any particular system will usually have only one or two combinations implemented blocking send blocking receive both the sender and receiver are blocked un til the message is delivered this is sometimes referred to as a rendezvous this combination allows for tight synchronization between processes nonblocking send blocking receive although the sender may continue on the receiver is blocked until the requested message arrives this is probably the most useful combination it allows a process to send one or more messages to a variety of destinations as quickly as possible a process that must receive a message before it can do useful work needs to be blocked until such a mes sage arrives an example is a server process that exists to provide a service or resource to other processes nonblocking send nonblocking receive neither party is required to wait the nonblocking send is more natural for many concurrent programming tasks for example if it is used to request an output operation such as printing it allows the requesting process to issue the request in the form of a message and then carry on one potential danger of the nonblocking send is that an error could lead to a situation in which a process repeatedly generates messages because there is no blocking to discipline the process these messages could consume system resources including processor time and buffer space to the detriment of other processes and the os also the nonblocking send places the burden on the programmer to deter mine that a message has been received processes must employ reply messages to acknowledge receipt of a message for the receive primitive the blocking version appears to be more natural for many concurrent programming tasks generally a process that requests a mes sage will need the expected information before proceeding however if a message is lost which can happen in a distributed system or if a process fails before it sends an anticipated message a receiving process could be blocked indefinitely this message passing problem can be solved by the use of the nonblocking receive however the dan ger of this approach is that if a message is sent after a process has already executed a matching receive the message will be lost other possible approaches are to allow a process to test whether a message is waiting before issuing a receive and allow a process to specify more than one source in a receive primitive the latter approach is useful if a process is waiting for messages from more than one source and can proceed if any of these messages arrive addressing clearly it is necessary to have a way of specifying in the send primitive which pro cess is to receive the message similarly most implementations allow a receiving process to indicate the source of a message to be received the various schemes for specifying processes in send and receive primi tives fall into two categories direct addressing and indirect addressing with direct addressing the send primitive includes a specific identifier of the destination proc ess the receive primitive can be handled in one of two ways one possibility is to require that the process explicitly designate a sending process thus the proc ess must know ahead of time from which process a message is expected this will often be effective for cooperating concurrent processes in other cases however it is impossible to specify the anticipated source process an example is a printer server process which will accept a print request message from any other process for such applications a more effective approach is the use of implicit addressing in this case the source parameter of the receive primitive possesses a value returned when the receive operation has been performed the other general approach is indirect addressing in this case messages are not sent directly from sender to receiver but rather are sent to a shared data struc ture consisting of queues that can temporarily hold messages such queues are gen erally referred to as mailboxes thus for two processes to communicate one proc ess sends a message to the appropriate mailbox and the other process picks up the message from the mailbox a strength of the use of indirect addressing is that by decoupling the sender and receiver it allows for greater flexibility in the use of messages the relationship between senders and receivers can be one to one many to one one to many or many to many figure a one to one relationship allows a private communi cations link to be set up between two processes this insulates their interaction from erroneous interference from other processes a many to one relationship is use ful for client server interaction one process provides service to a number of other processes in this case the mailbox is often referred to as a port a one to many relationship allows for one sender and multiple receivers it is useful for applications where a message or some information is to be broadcast to a set of processes a many to many relationship allows multiple server processes to provide concurrent service to multiple clients the association of processes to mailboxes can be either static or dynamic ports are often statically associated with a particular process that is the port is created and assigned to the process permanently similarly a one to one relation ship is typically defined statically and permanently when there are many senders chapter concurrency mutual exclusion and synchronization a one to one b many to one c one to many figure indirect process communication d many to many the association of a sender to a mailbox may occur dynamically primitives such as connect and disconnect may be used for this purpose a related issue has to do with the ownership of a mailbox in the case of a port it is typically owned by and created by the receiving process thus when the process is destroyed the port is also destroyed for the general mailbox case the os may offer a create mailbox service such mailboxes can be viewed either as being owned by the creating process in which case they terminate with the process or as being owned by the os in which case an explicit command will be required to destroy the mailbox message format the format of the message depends on the objectives of the messaging facility and whether the facility runs on a single computer or on a distributed system for some operating systems designers have preferred short fixed length messages to mini mize processing and storage overhead if a large amount of data is to be passed the data can be placed in a file and the message then simply references that file a more flexible approach is to allow variable length messages figure shows a typical message format for operating systems that support variable length messages the message is divided into two parts a header which contains information about the message and a body which contains the actual con tents of the message the header may contain an identification of the source and intended destination of the message a length field and a type field to discriminate among various types of messages there may also be additional control information message passing header body figure general message format such as a pointer field so that a linked list of messages can be created a sequence number to keep track of the number and order of messages passed between source and destination and a priority field queueing discipline the simplest queueing discipline is first in first out but this may not be sufficient if some messages are more urgent than others an alternative is to allow the speci fying of message priority on the basis of message type or by designation by the sender another alternative is to allow the receiver to inspect the message queue and select which message to receive next mutual exclusion figure shows one way in which message passing can be used to enforce mutual exclusion compare figures and we assume the use of the blocking receive primitive and the nonblocking send primitive a set of concurrent pro cesses share a mailbox box which can be used by all processes to send and receive figure mutual exclusion using messages chapter concurrency mutual exclusion and synchronization the mailbox is initialized to contain a single message with null content a process wishing to enter its critical section first attempts to receive a message if the mailbox is empty then the process is blocked once a process has acquired the message it performs its critical section and then places the message back into the mailbox thus the message functions as a token that is passed from process to process the preceding solution assumes that if more than one process performs the receive operation concurrently then if there is a message it is delivered to only one process and the others are blocked or if the message queue is empty all processes are blocked when a message is available only one blocked process is activated and given the message these assumptions are true of virtually all message passing facilities as an example of the use of message passing figure is a solution to the bounded buffer producer consumer problem using the basic mutual exclusion power of message passing the problem could have been solved with an algorithmic structure similar to that of figure instead the program of figure takes advantage of the ability of message passing to be used to pass data in addition to signals two mailboxes are used as the producer generates data it is sent as mes sages to the mailbox mayconsume as long as there is at least one message in that mailbox the consumer can consume hence mayconsume serves as the buffer the data in the buffer are organized as a queue of messages the size of the buffer is figure a solution to the bounded buffer producer consumer problem using messages readers writers problem determined by the global variable capacity initially the mailbox mayproduce is filled with a number of null messages equal to the capacity of the buffer the number of messages in mayproduce shrinks with each production and grows with each consumption this approach is quite flexible there may be multiple producers and consum ers as long as all have access to both mailboxes the system may even be distrib uted with all producer processes and the mayproduce mailbox at one site and all the consumer processes and the mayconsume mailbox at another readers writers problem in dealing with the design of synchronization and concurrency mechanisms it is useful to be able to relate the problem at hand to known problems and to be able to test any solution in terms of its ability to solve these known problems in the literature several problems have assumed importance and appear frequently both because they are examples of common design problems and because of their edu cational value one such problem is the producer consumer problem which has already been explored in this section we look at another classic problem the read ers writers problem the readers writers problem is defined as follows there is a data area shared among a number of processes the data area could be a file a block of main mem ory or even a bank of processor registers there are a number of processes that only read the data area readers and a number that only write to the data area writers the conditions that must be satisfied are as follows any number of readers may simultaneously read the file only one writer at a time may write to the file if a writer is writing to the file no reader may read it thus readers are processes that are not required to exclude one another and writers are processes that are required to exclude all other processes readers and writers alike before proceeding let us distinguish this problem from two others the general mutual exclusion problem and the producer consumer problem in the readers writ ers problem readers do not also write to the data area nor do writers read the data area while writing a more general case which includes this case is to allow any of the processes to read or write the data area in that case we can declare any por tion of a process that accesses the data area to be a critical section and impose the general mutual exclusion solution the reason for being concerned with the more restricted case is that more efficient solutions are possible for this case and that the less efficient solutions to the general problem are unacceptably slow for example suppose that the shared area is a library catalog ordinary users of the library read the catalog to locate a book one or more librarians are able to update the catalog in the general solution every access to the catalog would be treated as a critical sec tion and users would be forced to read the catalog one at a time this would clearly impose intolerable delays at the same time it is important to prevent writers from chapter concurrency mutual exclusion and synchronization interfering with each other and it is also required to prevent reading while writing is in progress to prevent the access of inconsistent information can the producer consumer problem be considered simply a special case of the readers writers problem with a single writer the producer and a single reader the consumer the answer is no the producer is not just a writer it must read queue pointers to determine where to write the next item and it must determine if the buffer is full similarly the consumer is not just a reader because it must adjust the queue pointers to show that it has removed a unit from the buffer we now examine two solutions to the problem readers have priority figure is a solution using semaphores showing one instance each of a reader and a writer the solution does not change for multiple readers and writers the writer process is simple the semaphore wsem is used to enforce mutual exclusion as long as one writer is accessing the shared data area no other writers and no readers may access it the reader process also makes use of wsem to enforce mutual exclusion however to allow multiple readers we require that when there are no readers reading the first reader that attempts to read should wait on wsem when figure a solution to the readers writers problem using semaphore readers have priority readers writers problem there is already at least one reader reading subsequent readers need not wait before entering the global variable readcount is used to keep track of the number of readers and the semaphore x is used to assure that readcount is updated properly writers have priority in the previous solution readers have priority once a single reader has begun to access the data area it is possible for readers to retain control of the data area as long as there is at least one reader in the act of reading therefore writers are sub ject to starvation figure shows a solution that guarantees that no new readers are allowed access to the data area once at least one writer has declared a desire to write for figure a solution to the readers writers problem using semaphore writers have priority chapter concurrency mutual exclusion and synchronization table state of the process queues for program of figure readers only in the system wsem set no queues writers only in the system wsem and rsem set writers queue on wsem both readers and writers with read first wsem set by reader rsem set by writer all writers queue on wsem one reader queues on rsem other readers queue on z both readers and writers with write first wsem set by writer rsem set by writer writers queue on wsem one reader queues on rsem other readers queue on z writers the following semaphores and variables are added to the ones already defined a semaphore rsem that inhibits all readers while there is at least one writer desiring access to the data area a variable writecount that controls the setting of rsem a semaphore y that controls the updating of writecount for readers one additional semaphore is needed a long queue must not be allowed to build up on rsem otherwise writers will not be able to jump the queue therefore only one reader is allowed to queue on rsem with any additional readers queueing on semaphore z immediately before waiting on rsem table summarizes the possibilities an alternative solution which gives writers priority and which is implemented using message passing is shown in figure in this case there is a controller process that has access to the shared data area other processes wishing to access the data area send a request message to the controller are granted access with an ok reply message and indicate completion of access with a finished message the controller is equipped with three mailboxes one for each type of message that it may receive the controller process services write request messages before read request messages to give writers priority in addition mutual exclusion must be enforced to do this the variable count is used which is initialized to some number greater than the maximum possible number of readers in this example we use a value of the action of the controller can be summarized as follows if count then no writer is waiting and there may or may not be read ers active service all finished messages first to clear active readers then service write requests and then read requests if count then the only request outstanding is a write request allow the writer to proceed and wait for a finished message summary void reader int i message rmsg while true rmsg i send readrequest rmsg receive mbox i rmsg readunit rmsg i send finished rmsg void writer int j message rmsg while true rmsg j send writerequest rmsg receive mbox j rmsg writeunit rmsg j send finished rmsg void controller while true if count if empty finished receive finished msg count else if empty writerequest receive writerequest msg msg id count count else if empty readrequest receive readrequest msg count send msg id ok if count send ok receive finished msg count while count receive finished msg count figure a solution to the readers writers problem using message passing if count then a writer has made a request and is being made to wait to clear all active readers therefore only finished messages should be serviced summary the central themes of modern operating systems are multiprogramming multipro cessing and distributed processing fundamental to these themes and fundamen tal to the technology of os design is concurrency when multiple processes are executing concurrently either actually in the case of a multiprocessor system or vir tually in the case of a single processor multiprogramming system issues of conflict resolution and cooperation arise concurrent processes may interact in a number of ways processes that are unaware of each other may nevertheless compete for resources such as processor time or access to i o devices processes may be indirectly aware of one another because they share access to a common object such as a block of main memory or a file finally processes may be directly aware of each other and cooperate by the exchange of information the key issues that arise in these interactions are mutual exclusion and deadlock chapter concurrency mutual exclusion and synchronization mutual exclusion is a condition in which there is a set of concurrent processes only one of which is able to access a given resource or perform a given function at any time mutual exclusion techniques can be used to resolve conflicts such as competition for resources and to synchronize processes so that they can coop erate an example of the latter is the producer consumer model in which one process is putting data into a buffer and one or more processes are extracting data from that buffer one approach to supporting mutual exclusion involves the use of special pur pose machine instructions this approach reduces overhead but is still inefficient because it uses busy waiting another approach to supporting mutual exclusion is to provide features within the os two of the most common techniques are semaphores and message facili ties semaphores are used for signaling among processes and can be readily used to enforce a mutual exclusion discipline messages are useful for the enforcement of mutual exclusion and also provide an effective means of interprocess communication recommended reading the misnamed little book of semaphores pages provides numer ous examples of the uses of semaphores available free online surveys many of the mechanisms described in this chapter provides a very clear and even entertaining discussion of concurrency mutual exclusion semaphores and other related topics a more formal treatment expanded to include distributed systems is contained in is another readable and useful treatment it also contains a number of problems with worked out solutions is a comprehensive and lucid collection of algorithms for mutual exclusion covering software e g dekker and hardware approaches as well as semaphores and messages is a very readable classic that presents a formal approach to defining sequential processes and concur rency is a lengthy formal treatment of mutual exclusion is a useful aid in understanding concurrency is a well organized treatment of concurrency provides a good practical introduction to programming using concurrency is an exhaustive survey of monitors is an instructive analysis of different scheduling policies for the readers writers problem key terms review questions and problems key terms review questions and problems key terms atomic critical resource nonblocking binary semaphore critical section race condition blocking deadlock semaphore busy waiting general semaphore spin waiting concurrency message passing starvation concurrent processes monitor strong semaphore coroutine mutual exclusion weak semaphore counting semaphore mutex review questions list four design issues for which the concept of concurrency is relevant what are three contexts in which concurrency arises what is the basic requirement for the execution of concurrent processes list three degrees of awareness between processes and briefly define each what is the distinction between competing processes and cooperating processes list the three control problems associated with competing processes and briefly de fine each list the requirements for mutual exclusion what operations can be performed on a semaphore chapter concurrency mutual exclusion and synchronization what is the difference between binary and general semaphores what is the difference between strong and weak semaphores what is a monitor what is the distinction between blocking and nonblocking with respect to messages what conditions are generally associated with the readers writers problem problems at the beginning of section it is stated that multiprogramming and multiprocess ing present the same problems with respect to concurrency this is true as far as it goes however cite two differences in terms of concurrency between multiprogram ming and multiprocessing processes and threads provide a powerful structuring tool for implementing programs that would be much more complex as simple sequential programs an earlier con struct that is instructive to examine is the coroutine the purpose of this problem is to introduce coroutines and compare them to processes consider this simple problem from read column cards and print them on character lines with the following changes after every card image an extra blank is inserted and every adjacent pair of asterisks on a card is replaced by the character a develop a solution to this problem as an ordinary sequential program you will find that the program is tricky to write the interactions among the various elements of the program are uneven because of the conversion from a length of to furthermore the length of the card image after conversion will vary depending on the number of double asterisk occurrences one way to improve clarity and to minimize the potential for bugs is to write the application as three separate procedures the first procedure reads in card images pads each image with a blank and writes a stream of characters to a temporary file after all of the cards have been read the second procedure reads the temporary file does the character substitution and writes out a second temporary file the third procedure reads the stream of characters from the second temporary file and prints lines of characters each b the sequential solution is unattractive because of the overhead of i o and tempo rary files conway proposed a new form of program structure the coroutine that allows the application to be written as three programs connected by one character buffers figure in a traditional procedure there is a master slave relation ship between the called and calling procedure the calling procedure may execute a call from any point in the procedure the called procedure is begun at its entry point and returns to the calling procedure at the point of call the coroutine exhib its a more symmetric relationship as each call is made execution takes up from the last active point in the called procedure because there is no sense in which a calling procedure is higher than the called there is no return rather any co routine can pass control to any other coroutine with a resume command the first time a coroutine is invoked it is resumed at its entry point subsequently the co routine is reactivated at the point of its own last resume command note that only one coroutine in a program can be in execution at one time and that the transition points are explicitly defined in the code so this is not an example of concurrent processing explain the operation of the program in figure c the program does not address the termination condition assume that the i o routine readcard returns the value true if it has placed an character image in inbuf otherwise it returns false modify the program to include this contingency note that the last printed line may therefore contain less than characters d rewrite the solution as a set of three processes using semaphores key terms review questions and problems char rs sp char inbuf outbuf void read while true readcard inbuf for int i i i rs inbuf i resume squash rs resume squash void print while true for int j j j outbuf j sp resume squash output outbuf void squash while true if rs sp rs resume print else resume read if rs sp resume print else sp resume print sp rs resume print resume read figure an application of coroutines consider the following program shared int x shared int x x x while while x x x x x x x x if x if x printf x is d x printf x is d x note that the scheduler in a uniprocessor system would implement pseudo parallel execution of these two concurrent processes by interleaving their instructions without restriction on the order of the interleaving a show a sequence i e trace the sequence of interleavings of statements such that the statement x is is printed b show a sequence such that the statement x is is printed you should remember that the increment decrements at the source language level are not done atomi cally that is the assembly language code ld x load from memory location x incr increment sto x store the incremented value back in x implements the single c increment instruction x x chapter concurrency mutual exclusion and synchronization consider the following program const int n int tally void total int count for count count n count tally void main tally parbegin total total write tally a determine the proper lower bound and upper bound on the final value of the shared variable tally output by this concurrent program assume processes can execute at any relative speed and that a value can only be incremented after it has been loaded into a register by a separate machine instruction b suppose that an arbitrary number of these processes are permitted to execute in parallel under the assumptions of part a what effect will this modification have on the range of final values of tally is busy waiting always less efficient in terms of using processor time than a blocking wait explain consider the following program boolean blocked int turn void p int id while true blocked id true while turn id while blocked id do nothing turn id critical section blocked id false remainder void main blocked false blocked false turn parbegin p p this software solution to the mutual exclusion problem for two processes is proposed in find a counterexample that demonstrates that this solution is incor rect it is interesting to note that even the communications of the acm was fooled on this one key terms review questions and problems a software approach to mutual exclusion is lamport bakery algorithm so called because it is based on the practice in bakeries and other shops in which every customer receives a numbered ticket on arrival allowing each to be served in turn the algorithm is as follows boolean choosing n int number n while true choosing i true number i getmax number n choosing i false for int j j n j while choosing j while number j number j j number i i critical section number i remainder the arrays choosing and number are initialized to false and respectively the ith element of each array may be read and written by process i but only read by other processes the notation a b c d is defined as a c or a c and b d a describe the algorithm in words b show that this algorithm avoids deadlock c show that it enforces mutual exclusion now consider a version of the bakery algorithm without the variable choosing then we have int number n while true number i getmax number n for int j j n j while number j number j j number i i critical section number i remainder does this version violate mutual exclusion explain why or why not consider the following program which provides a software approach to mutual exclusion integer array control n integer k where k n and each element of control is either or all elements of control are initially zero the initial value of k is immaterial the program of the ith process i n is begin integer j control i l li for j k step l until n l step l until k do begin if j i then goto chapter concurrency mutual exclusion and synchronization if control j then goto end control i for j step until n do if j i and control j then goto if control k and k i then goto k i critical section for j k step until n step until k do if j k and control j then begin k j goto end control i remainder of cycle goto end this is referred to as the eisenberg mcguire algorithm explain its operation and its key features consider the first instance of the statement bolt in figure a achieve the same result using the exchange instruction b which method is preferable when a special machine instruction is used to provide mutual exclusion in the fash ion of figure there is no control over how long a process must wait before being granted access to its critical section devise an algorithm that uses the compare swap instruction but that guarantees that any process waiting to enter its critical section will do so within n turns where n is the number of processes that may require access to the critical section and a turn is an event consisting of one process leaving the critical section and another process being granted access consider the following definition of semaphores void semwait if count count else place this process in queue block void semsignal if there is at least one process blocked on semaphore remove a process p from queue place process p on ready list else count key terms review questions and problems compare this set of definitions with that of figure note one difference with the preceding definition a semaphore can never take on a negative value is there any difference in the effect of the two sets of definitions when used in programs that is could you substitute one set for the other without altering the meaning of the program consider a sharable resource with the following characteristics as long as there are fewer than three processes using the resource new processes can start using it right away once there are three process using the resource all three must leave before any new processes can begin using it we realize that counters are needed to keep track of how many processes are waiting and active and that these counters are themselves shared resources that must be protected with mutual exclusion so we might create the following solution semaphore mutex block share variables semaphores int active waiting counters and boolean false state information semwait mutex enter the mutual exclusion if if there are or were then waiting we must wait but we must leave semsignal mutex the mutual exclusion first semwait block wait for all current users to depart semwait mutex reenter the mutual exclusion waiting and update the waiting count active update active count and remember active if the count reached semsignal mutex leave the mutual exclusion critical section semwait mutex enter mutual exclusion active and update the active count if active last one to leave int n if waiting n waiting else n if so unblock up to while n waiting processes semsignal block n false all active processes have left semsignal mutex leave the mutual exclusion the solution appears to do everything right all accesses to the shared variables are protected by mutual exclusion processes do not block themselves while in the mutual exclusion new processes are prevented from using the resource if there are or were three active users and the last process to depart unblocks up to three waiting processes a the program is nevertheless incorrect explain why b suppose we change the if in line to a while does this solve any problem in the program do any difficulties remain chapter concurrency mutual exclusion and synchronization now consider this correct solution to the preceding problem semaphore mutex block share variables semaphores int active waiting counters and boolean false state information semwait mutex enter the mutual exclusion if if there are or were then waiting we must wait but we must leave semsignal mutex the mutual exclusion first semwait block wait for all current users to depart else active update active count and active remember if the count reached semsignal mutex leave mutual exclusion critical section semwait mutex enter mutual exclusion active and update the active count if active last one to leave int n if waiting n waiting else n if so see how many processes to unblock waiting n deduct this number from waiting count active n and set active to this number while n now unblock the processes semsignal block one by one n 30 active remember if the count is 32 semsignal mutex leave the mutual exclusion a explain how this program works and why it is correct b this solution does not completely prevent newly arriving processes from cutting in line but it does make it less likely give an example of cutting in line c this program is an example of a general design pattern that is a uniform way to implement solutions to many concurrency problems using semaphores it has been referred to as the i ll do it for you pattern describe the pattern now consider another correct solution to the preceding problem semaphore mutex block share variables semaphores int active waiting counters and boolean false state information semwait mutex enter the mutual exclusion if if there are or were then waiting we must wait but we must leave semsignal mutex the mutual exclusion first semwait block wait for all current users to depart waiting we ve got the mutual exclusion update count active update active count and remember active if the count reached key terms review questions and problems if waiting if there are others waiting semsignal block and we don t yet have active unblock a waiting process else semsignal mutex otherwise open the mutual exclusion critical section 21 semwait mutex enter mutual exclusion active and update the active count if active if last one to leave must_wait false set up to let new processes enter if waiting must_wait if there are others waiting semsignal block and we don t have active unblock a waiting process else semsignal mutex otherwise open the mutual exclusion a explain how this program works and why it is correct b does this solution differ from the preceding one in terms of the number of pro cesses that can be unblocked at a time explain c this program is an example of a general design pattern that is a uniform way to implement solutions to many concurrency problems using semaphores it has been referred to as the pass the baton pattern describe the pattern it should be possible to implement general semaphores using binary semaphores we can use the operations semwaitb and semsignalb and two binary semaphores delay and mutex consider the following void semwait semaphore semwaitb mutex if semsignalb mutex semwaitb delay else semsignalb mutex void semsignal semaphore semwaitb mutex if semsignalb delay semsignalb mutex initially is set to the desired semaphore value each semwait operation decrements and each semsignal operation increments the binary semaphore mutex which is initialized to assures that there is mutual exclusion for the updating of the bi nary semaphore delay which is initialized to is used to block processes there is a flaw in the preceding program demonstrate the flaw and propose a change that will fix it hint suppose two processes each call semwait when is initially and after the first has just performed semsignalb mutex but not per formed semwaitb delay the second call to semwait proceeds to the same point all that you need to do is move a single line of the program in dijkstra put forward the conjecture that there was no solution to the mutual exclusion problem avoiding starvation applicable to an unknown but finite number of processes using a finite number of weak semaphores in j m morris refuted chapter concurrency mutual exclusion and synchronization this conjecture by publishing an algorithm using three weak semaphores the behavior of the algorithm can be described as follows if one or several process are waiting in a semwait s operation and another process is executing semsignal s the value of the semaphore s is not modified and one of the waiting processes is unblocked inde pendently of semwait s apart from the three semaphores the algorithm uses two nonnegative integer variables as counters of the number of processes in certain sections of the algorithm thus semaphores a and b are initialized to while semaphore m and counters na and nm are initialized to the mutual exclusion semaphore b pro tects access to the shared variable na a process attempting to enter its critical section must cross two barriers represented by semaphores a and m counters na and nm respectively contain the number of processes ready to cross barrier a and those having already crossed barrier a but not yet barrier m in the second part of the protocol the nm processes blocked at m will enter their critical sections one by one using a cascade technique similar to that used in the first part define an algorithm that conforms to this description the following problem was once used on an exam jurassic park consists of a dinosaur museum and a park for safari riding there are m passengers and n single passenger cars passengers wander around the museum for a while then line up to take a ride in a safari car when a car is available it loads the one passenger it can hold and rides around the park for a random amount of time if the n cars are all out riding passengers around then a passenger who wants to ride waits if a car is ready to load but there are no waiting passengers then the car waits use semaphores to synchronize the m passenger processes and the n car processes the following skeleton code was found on a scrap of paper on the floor of the exam room grade it for correctness ignore syntax and missing variable declarations remember that p and v correspond to semwait and semsignal resource sem process passenger i to do true nap int random p v p p od end passenger process car j to do true v p v nap int random v od end car end in the commentary on figure and table it was stated that it would not do simply to move the conditional statement inside the critical section controlled by of the consumer because this could lead to deadlock demonstrate this with a table similar to table consider the solution to the infinite buffer producer consumer problem defined in figure suppose we have the common case in which the producer and consumer are running at roughly the same speed the scenario could be producer append semsignal produce append semsignal produce consumer consume take semwait consume take semwait key terms review questions and problems the producer always manages to append a new element to the buffer and signal during the consumption of the previous element by the consumer the producer is always appending to an empty buffer and the consumer is always taking the sole item in the buffer although the consumer never blocks on the semaphore a large number of calls to the semaphore mechanism is made creating considerable overhead construct a new program that will be more efficient under these circumstances hints allow n to have the value which is to mean that not only is the buffer empty but that the consumer has detected this fact and is going to block until the producer sup plies fresh data the solution does not require the use of the local variable m found in figure 21 consider figure would the meaning of the program change if the following were interchanged a semwait e semwait b semsignal semsignal n c semwait n semwait d semsignal semsignal e the following pseudocode is a correct implementation of the producer consumer problem with a bounded buffer item buffer initially empty semaphore empty initialized to semaphore full initialized to mutex initialized to void producer while true item produce wait empty wait mutex append item signal mutex signal full void consumer while true wait full wait mutex item take signal mutex signal empty consume item labels and refer to the lines of code shown above and each cover three lines of code semaphores empty and full are linear semaphores that can take unbounded negative and positive values there are multiple producer processes referred to as pa pb pc etc and multiple consumer processes referred to as ca cb cc etc each semaphore maintains a fifo first in first out queue of blocked pro cesses in the scheduling chart below each line represents the state of the buffer and semaphores after the scheduled execution has occurred to simplify we assume that scheduling is such that processes are never interrupted while executing a given por tion of code or or your task is to complete the following chart scheduled step of execution full state and queue buffer empty state and queue initialization full ooo empty ca executes full ca ooo empty cb executes full ca cb ooo empty chapter concurrency mutual exclusion and synchronization scheduled step of execution full state and queue buffer empty state and queue pa executes full ca cb ooo empty pa executes full ca cb x oo empty pa executes full cb ca x oo empty ca executes full cb ooo empty ca executes full cb ooo empty pb executes full empty pa executes full empty pa executes full empty pb executes full empty pb executes full empty pc executes full empty cb executes full empty pc executes full empty cb executes full empty pa executes full empty pb executes full empty pc executes full empty pa executes full empty pd executes full empty ca executes full empty pa executes full empty cc executes full empty pa executes full empty cc executes full empty pd executes full empty 23 this problem demonstrates the use of semaphores to coordinate three types of pro cesses santa claus sleeps in his shop at the north pole and can only be wakened by either all nine reindeer being back from their vacation in the south pacific or some of the elves having difficulties making toys to allow santa to get some sleep the elves can only wake him when three of them have problems when three elves are having their problems solved any other elves wishing to visit santa must wait for those elves to return if santa wakes up to find three elves waiting at his shop door along with the last reindeer having come back from the tropics santa has decided that the elves can wait until after christmas because it is more important to get his sleigh ready it is assumed that the reindeer do not want to leave the tropics and therefore they stay there until the last possible moment the last reindeer to arrive must get santa while the others wait in a warming hut before being harnessed to the sleigh solve this problem using semaphores show that message passing and semaphores have equivalent functionality by a implementing message passing using semaphores hint make use of a shared buffer area to hold mailboxes each one consisting of an array of message slots b implementing a semaphore using message passing hint introduce a separate synchronization process am grateful to john trono of st michael college in vermont for supplying this problem key terms review questions and problems explain what is the problem with this implementation of the one writer many readers problem int readcount shared and initialized to semaphore mutex wrt shared and initialized to writer readers semwait mutex readcount readcount semwait wrt if readcount then semwait wrt writing performed semsignal mutex semsignal wrt reading performed semwait mutex readcount readcount if readcount then up wrt semsignal mutex chapter concurrency deadlock and starvation principles of deadlock reusable resources consumable resources resource allocation graphs the conditions for deadlock deadlock prevention mutual exclusion hold and wait no preemption circular wait deadlock avoidance process initiation denial resource allocation denial deadlock detection deadlock detection algorithm recovery an integrated deadlock strategy dining philosophers problem solution using semaphores solution using a monitor unix concurrency mechanisms linux kernel concurrency mechanisms solaris thread synchronization primitives windows concurrency mechanisms summary recommended reading key terms review questions and problems principles of deadlock when two trains approach each other at a crossing both shall come to a full stop and neither shall start up again until the other has gone statute passed by the kansas state legislature early in the century a treasury of railroad folklore b a botkin and alvin f harlow this chapter examines two problems that plague all efforts to support concurrent processing deadlock and starvation we begin with a discussion of the underlying principles of deadlock and the related problem of starvation then we examine the three common approaches to dealing with deadlock prevention detection and avoidance we then look at one of the classic problems used to illustrate both synchronization and deadlock issues the dining philosophers problem as with chapter the discussion in this chapter is limited to a consideration of concurrency and deadlock on a single system measures to deal with distributed deadlock problems are assessed in chapter an animation illustrating deadlock is available online click on the rotating globe at williamstallings com os html for access principles of deadlock deadlock can be defined as the permanent blocking of a set of processes that either compete for system resources or communicate with each other a set of processes is deadlocked when each process in the set is blocked awaiting an event typically the freeing up of some requested resource that can only be triggered by another blocked process in the set deadlock is permanent because none of the events is ever triggered unlike other problems in concurrent process management there is no efficient solution in the general case chapter concurrency deadlock and starvation c b d a a deadlock possible b deadlock figure illustration of deadlock all deadlocks involve conflicting needs for resources by two or more proc esses a common example is the traffic deadlock figure shows a situation in which four cars have arrived at a four way stop intersection at approximately the same time the four quadrants of the intersection are the resources over which con trol is needed in particular if all four cars wish to go straight through the intersec tion the resource requirements are as follows car traveling north needs quadrants a and b car needs quadrants b and c car needs quadrants c and d car needs quadrants d and a the rule of the road in the united states is that a car at a four way stop should defer to a car immediately to its right this rule works if there are only two or three cars at the intersection for example if only the northbound and westbound cars arrive at the intersection the northbound car will wait and the westbound car pro ceeds however if all four cars arrive at about the same time and all four follow the rule each will refrain from entering the intersection this causes a potential deadlock it is only a potential deadlock because the necessary resources are available for any of the cars to proceed if one car eventually chooses to proceed it can do so however if all four cars ignore the rules and proceed cautiously into the intersection at the same time then each car seizes one resource one quadrant but cannot proceed because the required second resource has already been seized by another car this is an actual deadlock let us now look at a depiction of deadlock involving processes and com puter resources figure based on one in which we refer to as a joint progress diagram illustrates the progress of two processes competing for two principles of deadlock progress of q a required release a release b b required get a get b both p and q want resource a both p and q want resource b deadlock inevitable region get a get b a required release a release b b required progress of p possible progress path of p and q horizontal portion of path indicates p is executing and q is waiting vertical portion of path indicates q is executing and p is waiting figure example of deadlock resources each process needs exclusive use of both resources for a certain period of time two processes p and q have the following general form process p process q get a get b get b get a release a release b release b release a in figure the x axis represents progress in the execution of p and the y axis represents progress in the execution of q the joint progress of the two processes is therefore represented by a path that progresses from the origin in a northeasterly direction for a uniprocessor system only one process at a time may execute and the path consists of alternating horizontal and vertical segments with a horizontal chapter concurrency deadlock and starvation segment representing a period when p executes and q waits and a vertical segment representing a period when q executes and p waits the figure indicates areas in which both p and q require resource a upward slanted lines both p and q require resource b downward slanted lines and both p and q require both resources because we assume that each process requires exclusive control of any resource these are all forbidden regions that is it is impossible for any path representing the joint execution progress of p and q to enter these regions the figure shows six different execution paths these can be summarized as follows q acquires b and then a and then releases b and a when p resumes execution it will be able to acquire both resources q acquires b and then a p executes and blocks on a request for a q releases b and a when p resumes execution it will be able to acquire both resources q acquires b and then p acquires a deadlock is inevitable because as execution proceeds q will block on a and p will block on b p acquires a and then q acquires b deadlock is inevitable because as execu tion proceeds q will block on a and p will block on b p acquires a and then b q executes and blocks on a request for b p releases a and b when q resumes execution it will be able to acquire both resources p acquires a and then b and then releases a and b when q resumes execution it will be able to acquire both resources the gray shaded area of figure which can be referred to as a fatal region applies to the commentary on paths and if an execution path enters this fatal region then deadlock is inevitable note that the existence of a fatal region depends on the logic of the two processes however deadlock is only inevitable if the joint progress of the two processes creates a path that enters the fatal region whether or not deadlock occurs depends on both the dynamics of the execu tion and on the details of the application for example suppose that p does not need both resources at the same time so that the two processes have the following form process p process q get a get b release a get a get b release b release b release a this situation is reflected in figure some thought should convince you that regardless of the relative timing of the two processes deadlock cannot occur as shown the joint progress diagram can be used to record the execution his tory of two processes that share resources in cases where more than two processes principles of deadlock progress of q a required b required release a release b get a get b get a release a get b release b progress of p both p and q want resource a both p and q want resource b a required b required possible progress path of p and q horizontal portion of path indicates p is executing and q is waiting vertical portion of path indicates q is executing and p is waiting figure example of no deadlock may compete for the same resource a higher dimensional diagram would be required the principles concerning fatal regions and deadlock would remain the same reusable resources two general categories of resources can be distinguished reusable and consumable a reusable resource is one that can be safely used by only one process at a time and is not depleted by that use processes obtain resource units that they later release for reuse by other processes examples of reusable resources include processors i o channels main and secondary memory devices and data structures such as files databases and semaphores as an example of deadlock involving reusable resources consider two processes that compete for exclusive access to a disk file d and a tape drive t the programs engage in the operations depicted in figure deadlock occurs if each process holds one resource and requests the other for example deadlock occurs if the multiprogramming system interleaves the execution of the two processes as follows q1 q2 chapter concurrency deadlock and starvation step process p action step process q action figure example of two processes competing for reusable resources it may appear that this is a programming error rather than a problem for the os designer however we have seen that concurrent program design is challenging such deadlocks do occur and the cause is often embedded in complex program logic making detection difficult one strategy for dealing with such a deadlock is to impose system design constraints concerning the order in which resources can be requested another example of deadlock with a reusable resource has to do with requests for main memory suppose the space available for allocation is kbytes and the following sequence of requests occurs deadlock occurs if both processes progress to their second request if the amount of memory to be requested is not known ahead of time it is difficult to deal with this type of deadlock by means of system design constraints the best way to deal with this particular problem is in effect to eliminate the possibility by using virtual memory which is discussed in chapter consumable resources a consumable resource is one that can be created produced and destroyed con sumed typically there is no limit on the number of consumable resources of a particular type an unblocked producing process may create any number of such resources when a resource is acquired by a consuming process the resource ceases to exist examples of consumable resources are interrupts signals messages and information in i o buffers principles of deadlock as an example of deadlock involving consumable resources consider the following pair of processes in which each process attempts to receive a message from the other process and then send a message to the other process deadlock occurs if the receive is blocking i e the receiving process is blocked until the message is received once again a design error is the cause of the deadlock such errors may be quite subtle and difficult to detect furthermore it may take a rare combination of events to cause the deadlock thus a program table summary of deadlock detection prevention and avoidance approaches for operating systems approach resource allocation policy different schemes major advantages major disadvantages prevention conservative undercommits resources requesting all resources at once works well for processes that perform a single burst of activity no preemption necessary inefficient delays process initiation future resource require ments must be known by processes preemption convenient when applied to resources whose state can be saved and restored easily preempts more often than necessary resource ordering feasible to enforce via compile time checks needs no run time com putation since problem is solved in system design disallows incremental resource requests avoidance midway between that of detection and prevention manipulate to find at least one safe path no preemption necessary future resource require ments must be known by os processes can be blocked for long periods detection very liberal requested resources are granted where possible invoke peri odically to test for deadlock never delays process initiation facilitates online handling inherent preemption losses chapter concurrency deadlock and starvation could be in use for a considerable period of time even years before the deadlock actually occurs there is no single effective strategy that can deal with all types of deadlock table summarizes the key elements of the most important approaches that have been developed prevention avoidance and detection we examine each of these in turn after first introducing resource allocation graphs and then discussing the conditions for deadlock resource allocation graphs a useful tool in characterizing the allocation of resources to processes is the resource allocation graph introduced by holt the resource allocation graph is a directed graph that depicts a state of the system of resources and pro cesses with each process and each resource represented by a node a graph edge directed from a process to a resource indicates a resource that has been requested by the process but not yet granted figure within a resource node a dot is shown for each instance of that resource examples of resource types that may have multiple instances are i o devices that are allocated by a resource management module in the os a graph edge directed from a reusable resource node dot to a process indicates a request that has been granted figure that is the process a resource is requested b resource is held c circular wait figure examples of resource allocation graphs d no deadlock principles of deadlock figure resource allocation graph for figure has been assigned one unit of that resource a graph edge directed from a consum able resource node dot to a process indicates that the process is the producer of that resource figure shows an example deadlock there is only one unit each of resources ra and rb process holds rb and requests ra while holds ra but requests rb figure has the same topology as figure but there is no dead lock because multiple units of each resource are available the resource allocation graph of figure corresponds to the deadlock situa tion in figure note that in this case we do not have a simple situation in which two processes each have one resource the other needs rather in this case there is a circular chain of processes and resources that results in deadlock the conditions for deadlock three conditions of policy must be present for a deadlock to be possible mutual exclusion only one process may use a resource at a time no process may access a resource unit that has been allocated to another process hold and wait a process may hold allocated resources while awaiting assign ment of other resources no preemption no resource can be forcibly removed from a process holding it in many ways these conditions are quite desirable for example mutual exclusion is needed to ensure consistency of results and the integrity of a data base similarly preemption should not be done arbitrarily for example when data resources are involved preemption must be supported by a rollback recovery mech anism which restores a process and its resources to a suitable previous state from which the process can eventually repeat its actions the first three conditions are necessary but not sufficient for a deadlock to exist for deadlock to actually take place a fourth condition is required circular wait a closed chain of processes exists such that each process holds at least one resource needed by the next process in the chain e g figure and figure chapter concurrency deadlock and starvation the fourth condition is actually a potential consequence of the first three that is given that the first three conditions exist a sequence of events may occur that lead to an unresolvable circular wait the unresolvable circular wait is in fact the definition of deadlock the circular wait listed as condition is unresolvable because the first three conditions hold thus the four conditions taken together constitute necessary and sufficient conditions for deadlock to clarify this discussion it is useful to return to the concept of the joint progress diagram such as the one shown in figure recall that we defined a fatal region as one such that once the processes have progressed into that region those processes will deadlock a fatal region exists only if all of the first three con ditions listed above are met if one or more of these conditions are not met there is no fatal region and deadlock cannot occur thus these are necessary conditions for deadlock for deadlock to occur there must not only be a fatal region but also a sequence of resource requests that has led into the fatal region if a circular wait condition occurs then in fact the fatal region has been entered thus all four conditions listed above are sufficient for deadlock to summarize three general approaches exist for dealing with deadlock first one can prevent deadlock by adopting a policy that eliminates one of the conditions conditions through second one can avoid deadlock by making the appropri ate dynamic choices based on the current state of resource allocation third one can attempt to detect the presence of deadlock conditions through hold and take action to recover we discuss each of these approaches in turn deadlock prevention the strategy of deadlock prevention is simply put to design a system in such a way that the possibility of deadlock is excluded we can view deadlock prevention methods as falling into two classes an indirect method of deadlock prevention is to prevent the occurrence of one of the three necessary conditions listed previ ously items through a direct method of deadlock prevention is to prevent the occurrence of a circular wait item we now examine techniques related to each of the four conditions all textbooks simply list these four conditions as the conditions needed for deadlock but such a presentation obscures some of the subtler issues item the circular wait condition is fundamentally different from the other three conditions items through are policy decisions while item is a circum stance that might occur depending on the sequencing of requests and releases by the involved processes linking circular wait with the three necessary conditions leads to inadequate distinction between preven tion and avoidance see and for a discussion deadlock prevention mutual exclusion in general the first of the four listed conditions cannot be disallowed if access to a resource requires mutual exclusion then mutual exclusion must be supported by the os some resources such as files may allow multiple accesses for reads but only exclusive access for writes even in this case deadlock can occur if more than one process requires write permission hold and wait the hold and wait condition can be prevented by requiring that a process request all of its required resources at one time and blocking the process until all requests can be granted simultaneously this approach is inefficient in two ways first a process may be held up for a long time waiting for all of its resource requests to be filled when in fact it could have proceeded with only some of the resources second resources allocated to a process may remain unused for a considerable period during which time they are denied to other processes another problem is that a process may not know in advance all of the resources that it will require there is also the practical problem created by the use of modular program ming or a multithreaded structure for an application an application would need to be aware of all resources that will be requested at all levels or in all modules to make the simultaneous request no preemption this condition can be prevented in several ways first if a process holding certain resources is denied a further request that process must release its original resources and if necessary request them again together with the additional resource alternatively if a process requests a resource that is currently held by another pro cess the os may preempt the second process and require it to release its resources this latter scheme would prevent deadlock only if no two processes possessed the same priority this approach is practical only when applied to resources whose state can be easily saved and restored later as is the case with a processor circular wait the circular wait condition can be prevented by defining a linear ordering of resource types if a process has been allocated resources of type r then it may subsequently request only those resources of types following r in the ordering to see that this strategy works let us associate an index with each resource type then resource ri precedes rj in the ordering if i j now suppose that two processes a and b are deadlocked because a has acquired ri and requested rj and b has acquired rj and requested ri this condition is impossible because it implies i j and j i as with hold and wait prevention circular wait prevention may be inefficient slowing down processes and denying resource access unnecessarily chapter concurrency deadlock and starvation deadlock avoidance an approach to solving the deadlock problem that differs subtly from deadlock prevention is deadlock avoidance in deadlock prevention we constrain resource requests to prevent at least one of the four conditions of deadlock this is either done indirectly by preventing one of the three necessary policy conditions mutual exclusion hold and wait no preemption or directly by preventing circular wait this leads to inefficient use of resources and inefficient execution of processes deadlock avoidance on the other hand allows the three necessary conditions but makes judicious choices to assure that the deadlock point is never reached as such avoidance allows more concurrency than prevention with deadlock avoidance a decision is made dynamically whether the current resource allocation request will if granted potentially lead to a deadlock deadlock avoidance thus requires knowl edge of future process resource requests in this section we describe two approaches to deadlock avoidance do not start a process if its demands might lead to deadlock do not grant an incremental resource request to a process if this allocation might lead to deadlock process initiation denial consider a system of n processes and m different types of resources let us define the following vectors and matrices resource r c rm total amount of each resource in the system available v c vm total amount of each resource not allocated to any process claim c c μ c f f f f c cnm cij requirement of process i for resource j allocation a c μ c f f f f c anm aij current allocation to process i of resource j the matrix claim gives the maximum requirement of each process for each resource with one row dedicated to each process this information must be term avoidance is a bit confusing in fact one could consider the strategies discussed in this section to be examples of deadlock prevention because they indeed prevent the occurrence of a deadlock deadlock avoidance declared in advance by a process for deadlock avoidance to work similarly the matrix allocation gives the current allocation to each process the following rela tionships hold rj vj an aij for all j all resources are either available or allocated i cij rj for all i j no process can claim more than the total amount of resources in the system aij cij for all i j no process is allocated more resources of any type than the process originally claimed to need with these quantities defined we can define a deadlock avoidance policy that refuses to start a new process if its resource requirements might lead to deadlock start a new process pn only if n rj ú c n j cij for all j i that is a process is only started if the maximum claim of all current processes plus those of the new process can be met this strategy is hardly optimal because it assumes the worst that all processes will make their maximum claims together resource allocation denial the strategy of resource allocation denial referred to as the banker algorithm was first proposed in let us begin by defining the concepts of state and safe state consider a system with a fixed number of processes and a fixed number of resources at any time a process may have zero or more resources allocated to it the state of the system reflects the current allocation of resources to processes thus the state consists of the two vectors resource and available and the two matrices claim and allocation defined earlier a safe state is one in which there is at least one sequence of resource allocations to processes that does not result in a deadlock i e all of the processes can be run to completion an unsafe state is of course a state that is not safe the following example illustrates these concepts figure shows the state of a system consisting of four processes and three resources the total amount of resources and are and units respectively in the cur rent state allocations have been made to the four processes leaving unit of used this name because of the analogy of this problem to one in banking with customers who wish to borrow money corresponding to processes and the money to be borrowed corresponding to resources stated as a banking problem the bank has a limited reserve of money to lend and a list of customers each with a line of credit a customer may choose to borrow against the line of credit a por tion at a time and there is no guarantee that the customer will make any repayment until after having taken out the maximum amount of loan the banker can refuse a loan to a customer if there is a risk that the bank will have insufficient funds to make further loans that will permit the customers to repay eventually chapter concurrency deadlock and starvation claim matrix c allocation matrix a c a resource vector r a initial state available vector v claim matrix c allocation matrix a c a resource vector r available vector v b runs to completion claim matrix c allocation matrix a c a resource vector r available vector v c runs to completion claim matrix c allocation matrix a c a resource vector r available vector v d runs to completion figure determination of a safe state deadlock avoidance and unit of available is this a safe state to answer this question we ask an intermediate question can any of the four processes be run to completion with the resources available that is can the difference between the maximum require ment and current allocation for any process be met with the available resources in terms of the matrices and vectors introduced earlier the condition to be met for process i is cij aij vj for all j clearly this is not possible for which has only unit of and requires more units of units of and units of however by assigning one unit of to process has its maximum required resources allocated and can run to completion let us assume that this is accomplished when completes its resources can be returned to the pool of available resources the resulting state is shown in figure now we can ask again if any of the remaining processes can be completed in this case each of the remaining processes could be completed suppose we choose allocate the required resources complete and return all of resources to the available pool we are left in the state shown in figure next we can complete p3 resulting in the state of figure finally we can complete at this point all of the processes have been run to completion thus the state defined by figure is a safe state these concepts suggest the following deadlock avoidance strategy which ensures that the system of processes and resources is always in a safe state when a process makes a request for a set of resources assume that the request is granted update the system state accordingly and then determine if the result is a safe state if so grant the request and if not block the process until it is safe to grant the request consider the state defined in figure suppose makes a request for one additional unit of and one additional unit of if we assume the request is granted then the resulting state is that of figure we have already seen that this is a safe state therefore it is safe to grant the request now let us return to the state of figure and suppose that makes the request for one additional unit each of and if we assume that the request is granted we are left in the state of figure is this a safe state the answer is no because each process will need at least one additional unit of and there are none available thus on the basis of deadlock avoidance the request by should be denied and should be blocked it is important to point out that figure is not a deadlocked state it merely has the potential for deadlock it is possible for example that if were run from this state it would subsequently release one unit of and one unit of prior to needing these resources again if that happened the system would return to a safe state thus the deadlock avoidance strategy does not predict deadlock with certainty it merely anticipates the possibility of deadlock and assures that there is never such a possibility chapter concurrency deadlock and starvation p3 claim matrix c p3 allocation matrix a p3 c a resource vector r a initial state available vector v p3 claim matrix c p3 allocation matrix a p3 c a resource vector r r2 available vector v b p1 requests one unit each of and figure determination of an unsafe state figure gives an abstract version of the deadlock avoidance logic the main algorithm is shown in part b with the state of the system defined by the data structure state request is a vector defining the resources requested by process i first a check is made to assure that the request does not exceed the original claim of the process if the request is valid the next step is to determine if it is possible to fulfill the request i e there are sufficient resources available if it is not possible then the process is suspended if it is possible the final step is to determine if it is safe to fulfill the request to do this the resources are tentatively assigned to process i to form newstate then a test for safety is made using the algorithm in figure deadlock avoidance has the advantage that it is not necessary to preempt and rollback processes as in deadlock detection and is less restrictive than deadlock prevention however it does have a number of restrictions on its use the maximum resource requirement for each process must be stated in advance the processes under consideration must be independent that is the order in which they execute must be unconstrained by any synchronization requirements there must be a fixed number of resources to allocate no process may exit while holding resources deadlock avoidance struct state int resource m int available m int claim n m int alloc n m a global data structures if alloc i request claim i error total request claim else if request available suspend process else simulate alloc define newstate by alloc i alloc i request available available request if safe newstate carry out allocation else restore original state suspend process b resource alloc algorithm boolean safe state s int currentavail m process rest number of processes currentavail available rest all processes possible true while possible find a process pk in rest such that claim k alloc k currentavail if found simulate execution of pk currentavail currentavail alloc k rest rest pk else possible false return rest null c test for safety algorithm banker algorithm figure deadlock avoidance logic chapter concurrency deadlock and starvation deadlock detection deadlock prevention strategies are very conservative they solve the problem of deadlock by limiting access to resources and by imposing restrictions on pro cesses at the opposite extreme deadlock detection strategies do not limit resource access or restrict process actions with deadlock detection requested resources are granted to processes whenever possible periodically the os performs an algorithm that allows it to detect the circular wait condition described earlier in condition and illustrated in figure deadlock detection algorithm a check for deadlock can be made as frequently as each resource request or less frequently depending on how likely it is for a deadlock to occur checking at each resource request has two advantages it leads to early detection and the algorithm is relatively simple because it is based on incremental changes to the state of the system on the other hand such frequent checks consume consider able processor time a common algorithm for deadlock detection is one described in the allocation matrix and available vector described in the previous section are used in addition a request matrix q is defined such that qij represents the amount of resources of type j requested by process i the algorithm proceeds by marking processes that are not deadlocked initially all processes are unmarked then the following steps are performed mark each process that has a row in the allocation matrix of all zeros initialize a temporary vector w to equal the available vector find an index i such that process i is currently unmarked and the ith row of q is less than or equal to w that is qik wk for k m if no such row is found terminate the algorithm if such a row is found mark process i and add the corresponding row of the allocation matrix to w that is set wk wk aik for k m return to step a deadlock exists if and only if there are unmarked processes at the end of the algorithm each unmarked process is deadlocked the strategy in this algo rithm is to find a process whose resource requests can be satisfied with the available resources and then assume that those resources are granted and that the process runs to completion and releases all of its resources the algorithm then looks for another process to satisfy note that this algorithm does not guarantee to prevent deadlock that will depend on the order in which future requests are granted all that it does is determine if deadlock currently exists we can use figure to illustrate the deadlock detection algorithm the algorithm proceeds as follows mark because has no allocated resources set w deadlock detection r2 p1 p3 request matrix q r2 r4 p1 p3 allocation matrix a r2 r4 resource vector r2 r4 figure example for deadlock detection available vector the request of process p3 is less than or equal to w so mark p3 and set w w no other unmarked process has a row in q that is less than or equal to w therefore terminate the algorithm the algorithm concludes with p1 and unmarked indicating that these processes are deadlocked recovery once deadlock has been detected some strategy is needed for recovery the follow ing are possible approaches listed in order of increasing sophistication abort all deadlocked processes this is believe it or not one of the most common if not the most common solution adopted in operating systems back up each deadlocked process to some previously defined checkpoint and restart all processes this requires that rollback and restart mechanisms be built in to the system the risk in this approach is that the original deadlock may recur however the nondeterminancy of concurrent processing may ensure that this does not happen successively abort deadlocked processes until deadlock no longer exists the order in which processes are selected for abortion should be on the basis of some criterion of minimum cost after each abortion the detection algorithm must be reinvoked to see whether deadlock still exists successively preempt resources until deadlock no longer exists as in a cost based selection should be used and reinvocation of the detection algorithm is required after each preemption a process that has a resource preempted from it must be rolled back to a point prior to its acquisition of that resource for and the selection criteria could be one of the following choose the process with the least amount of processor time consumed so far least amount of output produced so far most estimated time remaining chapter concurrency deadlock and starvation least total resources allocated so far lowest priority some of these quantities are easier to measure than others estimated time remaining is particularly suspect also other than by means of the priority measure there is no indication of the cost to the user as opposed to the cost to the system as a whole an integrated deadlock strategy as table suggests there are strengths and weaknesses to all of the strategies for dealing with deadlock rather than attempting to design an os facility that employs only one of these strategies it might be more efficient to use different strategies in different situations suggests one approach group resources into a number of different resource classes use the linear ordering strategy defined previously for the prevention of circular wait to prevent deadlocks between resource classes within a resource class use the algorithm that is most appropriate for that class as an example of this technique consider the following classes of resources swappable space blocks of memory on secondary storage for use in swapping processes process resources assignable devices such as tape drives and files main memory assignable to processes in pages or segments internal resources such as i o channels the order of the preceding list represents the order in which resources are assigned the order is a reasonable one considering the sequence of steps that a process may follow during its lifetime within each class the following strategies could be used swappable space prevention of deadlocks by requiring that all of the required resources that may be used be allocated at one time as in the hold and wait prevention strategy this strategy is reasonable if the maximum storage requirements are known which is often the case deadlock avoidance is also a possibility process resources avoidance will often be effective in this category because it is reasonable to expect processes to declare ahead of time the resources that they will require in this class prevention by means of resource ordering within this class is also possible main memory prevention by preemption appears to be the most appropriate strategy for main memory when a process is preempted it is simply swapped to secondary memory freeing space to resolve the deadlock internal resources prevention by means of resource ordering can be used dining philosophers problem dining philosophers problem we now turn to the dining philosophers problem introduced by dijkstra five philosophers live in a house where a table is laid for them the life of each phi losopher consists principally of thinking and eating and through years of thought all of the philosophers had agreed that the only food that contributed to their think ing efforts was spaghetti due to a lack of manual skill each philosopher requires two forks to eat spaghetti the eating arrangements are simple figure a round table on which is set a large serving bowl of spaghetti five plates one for each philosopher and five forks a philosopher wishing to eat goes to his or her assigned place at the table and using the two forks on either side of the plate takes and eats some spaghetti the problem devise a ritual algorithm that will allow the philosophers to eat the algorithm must satisfy mutual exclusion no two philosophers can use the same fork at the same time while avoiding deadlock and starvation in this case the term has literal as well as algorithmic meaning this problem may not seem important or relevant in itself however it does illustrate basic problems in deadlock and starvation furthermore attempts to develop solutions reveal many of the difficulties in concurrent programming e g see in addition the dining philosophers problem can be seen as repre sentative of problems dealing with the coordination of shared resources which may figure dining arrangement for philosophers chapter concurrency deadlock and starvation occur when an application includes concurrent threads of execution accordingly this problem is a standard test case for evaluating approaches to synchronization solution using semaphores figure suggests a solution using semaphores each philosopher picks up first the fork on the left and then the fork on the right after the philosopher is finished eating the two forks are replaced on the table this solution alas leads to deadlock if all of the philosophers are hungry at the same time they all sit down they all pick up the fork on their left and they all reach out for the other fork which is not there in this undignified position all philosophers starve to overcome the risk of deadlock we could buy five additional forks a more sanitary solution or teach the philosophers to eat spaghetti with just one fork as another approach we could consider adding an attendant who only allows four philosophers at a time into the dining room with at most four seated philosophers at least one philosopher will have access to two forks figure shows such a solu tion again using semaphores this solution is free of deadlock and starvation solution using a monitor figure shows a solution to the dining philosophers problem using a monitor a vector of five condition variables is defined one condition variable per fork these condition variables are used to enable a philosopher to wait for the availability of a fork in addition there is a boolean vector that records the availability status of each fork true means the fork is available the monitor consists of two procedures the procedure is used by a philosopher to seize his or her left and program diningphilosophers semaphore fork int i void philosopher int i while true think wait fork i wait fork i mod eat signal fork i mod signal fork i void main parbegin philosopher philosopher philosopher philosopher philosopher figure a first solution to the dining philosophers problem unix concurrency mechanisms program diningphilosophers semaphore fork semaphore room int i void philosopher int i while true think wait room wait fork i wait fork i mod eat signal fork i mod signal fork i signal room void main parbegin philosopher philosopher philosopher philosopher philosopher figure a second solution to the dining philosophers problem right forks if either fork is unavailable the philosopher process is queued on the appropriate condition variable this enables another philosopher process to enter the monitor the release forks procedure is used to make two forks available note that the structure of this solution is similar to that of the semaphore solution proposed in figure in both cases a philosopher seizes first the left fork and then the right fork unlike the semaphore solution this monitor solution does not suffer from deadlock because only one process at a time may be in the monitor for example the first philosopher process to enter the monitor is guaranteed that it can pick up the right fork after it picks up the left fork before the next philosopher to the right has a chance to seize its left fork which is this philosopher right fork unix concurrency mechanisms unix provides a variety of mechanisms for interprocessor communication and syn chronization here we look at the most important of these pipes messages shared memory semaphores signals chapter concurrency deadlock and starvation monitor cond forkready condition variable for synchronization boolean fork true availability status of each fork void int pid pid is the philosopher id number int left pid int right pid grant the left fork if fork left cwait forkready left queue on condition variable fork left false grant the right fork if fork right cwait forkready right queue on condition variable fork right false void int pid int left pid int right pid release the left fork if empty forkready left no one is waiting for this fork fork left true else awaken a process waiting on this fork csignal forkready left release the right fork if empty forkready right no one is waiting for this fork fork right true else awaken a process waiting on this fork csignal forkready right void philosopher k to the five philosopher clients while true think k client requests two forks via monitor eat spaghetti k client releases forks via the monitor figure a solution to the dining philosophers problem using a monitor unix concurrency mechanisms pipes messages and shared memory can be used to communicate data between processes whereas semaphores and signals are used to trigger actions by other processes pipes one of the most significant contributions of unix to the development of operating systems is the pipe inspired by the concept of coroutines a pipe is a circu lar buffer allowing two processes to communicate on the producer consumer model thus it is a first in first out queue written by one process and read by another when a pipe is created it is given a fixed size in bytes when a process attempts to write into the pipe the write request is immediately executed if there is sufficient room otherwise the process is blocked similarly a reading process is blocked if it attempts to read more bytes than are currently in the pipe otherwise the read request is immediately executed the os enforces mutual exclusion that is only one process can access a pipe at a time there are two types of pipes named and unnamed only related processes can share unnamed pipes while either related or unrelated processes can share named pipes messages a message is a block of bytes with an accompanying type unix provides msgsnd and msgrcv system calls for processes to engage in message passing associated with each process is a message queue which functions like a mailbox the message sender specifies the type of message with each message sent and this can be used as a selection criterion by the receiver the receiver can either retrieve messages in first in first out order or by type a process will block when trying to send a message to a full queue a process will also block when trying to read from an empty queue if a process attempts to read a message of a certain type and fails because no message of that type is present the process is not blocked shared memory the fastest form of interprocess communication provided in unix is shared memory this is a common block of virtual memory shared by multiple processes processes read and write shared memory using the same machine instructions they use to read and write other portions of their virtual memory space permission is read only or read write for a process determined on a per process basis mutual exclusion constraints are not part of the shared memory facility but must be provided by the processes using the shared memory semaphores the semaphore system calls in unix system v are a generalization of the semwait and semsignal primitives defined in chapter several operations can be per formed simultaneously and the increment and decrement operations can be values greater than the kernel does all of the requested operations atomically no other process may access the semaphore until all operations have completed chapter concurrency deadlock and starvation a semaphore consists of the following elements current value of the semaphore process id of the last process to operate on the semaphore number of processes waiting for the semaphore value to be greater than its current value number of processes waiting for the semaphore value to be zero associated with the semaphore are queues of processes blocked on that semaphore semaphores are actually created in sets with a semaphore set consisting of one or more semaphores there is a semctl system call that allows all of the sema phore values in the set to be set at the same time in addition there is a system call that takes as an argument a list of semaphore operations each defined on one of the semaphores in a set when this call is made the kernel performs the indicated operations one at a time for each operation the actual function is speci fied by the value the following are the possibilities if is positive the kernel increments the value of the semaphore and awakens all processes waiting for the value of the semaphore to increase if is the kernel checks the semaphore value if the semaphore value equals the kernel continues with the other operations on the list otherwise the kernel increments the number of processes waiting for this semaphore to be and suspends the process to wait for the event that the value of the semaphore equals if is negative and its absolute value is less than or equal to the sema phore value the kernel adds a negative number to the semaphore value if the result is the kernel awakens all processes waiting for the value of the semaphore to equal if is negative and its absolute value is greater than the semaphore value the kernel suspends the process on the event that the value of the sema phore increases this generalization of the semaphore provides considerable flexibility in per forming process synchronization and coordination signals a signal is a software mechanism that informs a process of the occurrence of asyn chronous events a signal is similar to a hardware interrupt but does not employ priorities that is all signals are treated equally signals that occur at the same time are presented to a process one at a time with no particular ordering processes may send each other signals or the kernel may send signals inter nally a signal is delivered by updating a field in the process table for the process to which the signal is being sent because each signal is maintained as a single bit signals of a given type cannot be queued a signal is processed just after a process wakes up to run or whenever the process is preparing to return from a system call a process may respond to a signal by performing some default action e g termina tion executing a signal handler function or ignoring the signal table lists signals defined for unix linux kernel concurrency mechanisms table unix signals value name description sighup hang up sent to process when kernel assumes that the user of that process is doing no useful work sigint interrupt sigquit quit sent by user to induce halting of process and production of core dump sigill illegal instruction sigtrap trace trap triggers the execution of code for process tracing sigiot iot instruction sigemt emt instruction sigfpe floating point exception sigkill kill terminate process sigbus bus error sigsegv segmentation violation process attempts to access location outside its virtual address space sigsys bad argument to system call sigpipe write on a pipe that has no readers attached to it sigalrm alarm clock issued when a process wishes to receive a signal after a period of time sigterm software termination user defined signal user defined signal sigchld death of a child 19 sigpwr power failure linux kernel concurrency mechanisms linux includes all of the concurrency mechanisms found in other unix systems such as including pipes messages shared memory and signals in addi tion linux includes a rich set of concurrency mechanisms specifically intended for use when a thread is executing in kernel mode that is these are mechanisms used within the kernel to provide concurrency in the execution of kernel code this section examines the linux kernel concurrency mechanisms atomic operations linux provides a set of operations that guarantee atomic operations on a variable these operations can be used to avoid simple race conditions an atomic operation executes without interruption and without interference on a uniprocessor system a thread performing an atomic operation cannot be interrupted once the operation has started until the operation is finished in addition on a multiprocessor system chapter concurrency deadlock and starvation the variable being operated on is locked from access by other threads until this oper ation is completed two types of atomic operations are defined in linux integer operations which operate on an integer variable and bitmap operations which operate on one bit in a bitmap table these operations must be implemented on any architecture that implements linux for some architectures there are cor responding assembly language instructions for the atomic operations on other architectures an operation that locks the memory bus is used to guarantee that the operation is atomic for atomic integer operations a special data type is used the atomic integer operations can be used only on this data type and no other operations table linux atomic operations atomic integer operations int i at declaration initialize an to i int v read integer value of v void v int i set the value of v to integer i void int i v add i to v void int i v subtract i from v void v add to v void v subtract from v int int i v subtract i from v return if the result is zero return otherwise int int i v add i to v return if the result is negative return otherwise used for implementing semaphores int v subtract from v return if the result is zero return otherwise int v add to v return if the result is zero return otherwise atomic bitmap operations void int nr void addr set bit nr in the bitmap pointed to by addr void int nr void addr clear bit nr in the bitmap pointed to by addr void int nr void addr invert bit nr in the bitmap pointed to by addr int int nr void addr set bit nr in the bitmap pointed to by addr return the old bit value int int nr void addr clear bit nr in the bitmap pointed to by addr return the old bit value int int nr void addr invert bit nr in the bitmap pointed to by addr return the old bit value int int nr void addr return the value of bit nr in the bitmap pointed to by addr linux kernel concurrency mechanisms are allowed on this data type lists the following advantages for these restrictions the atomic operations are never used on variables that might in some circum stances be unprotected from race conditions variables of this data type are protected from improper use by nonatomic operations the compiler cannot erroneously optimize access to the value e g by using an alias rather than the correct memory address this data type serves to hide architecture specific differences in its imple mentation a typical use of the atomic integer data type is to implement counters the atomic bitmap operations operate on one of a sequence of bits at an arbi trary memory location indicated by a pointer variable thus there is no equivalent to the data type needed for atomic integer operations atomic operations are the simplest of the approaches to kernel synchroniza tion more complex locking mechanisms can be built on top of them spinlocks the most common technique used for protecting a critical section in linux is the spin lock only one thread at a time can acquire a spinlock any other thread attempting to acquire the same lock will keep trying spinning until it can acquire the lock in essence a spinlock is built on an integer location in memory that is checked by each thread before it enters its critical section if the value is the thread sets the value to and enters its critical section if the value is nonzero the thread continually checks the value until it is zero the spinlock is easy to implement but has the disadvantage that locked out threads continue to execute in a busy waiting mode thus spinlocks are most effective in situations where the wait time for acquiring a lock is expected to be very short say on the order of less than two context changes the basic form of use of a spinlock is the following lock critical section lock basic spinlocks the basic spinlock as opposed to the reader writer spinlock explained subsequently comes in four flavors table plain if the critical section of code is not executed by interrupt handlers or if the interrupts are disabled during the execution of the critical section then the plain spinlock can be used it does not affect the interrupt state on the processor on which it is run if interrupts are always enabled then this spinlock should be used if it is not known if interrupts will be enabled or disabled at the time of execution then this version should be used when a lock is acquired the cur rent state of interrupts on the local processor is saved to be restored when the lock is released chapter concurrency deadlock and starvation table linux spinlocks void lock acquires the specified lock spinning if needed until it is available void lock like but also disables interrupts on the local processor void lock unsigned long flags like but also saves the current interrupt state in flags void lock like but also disables the execution of all bottom halves void lock releases given lock void lock releases given lock and enables local interrupts void lock unsigned long flags releases given lock and restores local interrupts to given previous state void lock releases given lock and enables bottom halves void lock initializes given spinlock int lock tries to acquire specified lock returns nonzero if lock is currently held and zero otherwise int lock returns nonzero if lock is currently held and zero otherwise when an interrupt occurs the minimum amount of work necessary is performed by the corresponding interrupt handler a piece of code called the bottom half performs the remainder of the interrupt related work allow ing the current interrupt to be enabled as soon as possible the spinlock is used to disable and then enable bottom halves to avoid conflict with the protected critical section the plain spinlock is used if the programmer knows that the protected data is not accessed by an interrupt handler or bottom half otherwise the appropriate nonplain spinlock is used spinlocks are implemented differently on a uniprocessor system versus a mul tiprocessor system for a uniprocessor system the following considerations apply if kernel preemption is turned off so that a thread executing in kernel mode cannot be interrupted then the locks are deleted at compile time they are not needed if kernel preemption is enabled which does permit interrupts then the spinlocks again compile away i e no test of a spinlock memory location occurs but are sim ply implemented as code that enables disables interrupts on a multiple processor system the spinlock is compiled into code that does in fact test the spinlock loca tion the use of the spinlock mechanism in a program allows it to be independent of whether it is executed on a uniprocessor or multiprocessor system reader writer spinlock the reader writer spinlock is a mechanism that allows a greater degree of concurrency within the kernel than the basic spinlock the reader writer spinlock allows multiple threads to have simultaneous access to the same data structure for reading only but gives exclusive access to the linux kernel concurrency mechanisms spinlock for a thread that intends to update the data structure each reader writer spinlock consists of a 24 bit reader counter and an unlock flag with the following interpretation counter flag interpretation the spinlock is released and available for use spinlock has been acquired for writing by one thread n n spinlock has been acquired for reading by n threads n n not valid as with the basic spinlock there are plain and versions of the reader writer spinlock note that the reader writer spinlock favors readers over writers if the spin lock is held for readers then so long as there is at least one reader the spinlock cannot be preempted by a writer furthermore new readers may be added to the spinlock even while a writer is waiting semaphores at the user level linux provides a semaphore interface corresponding to that in unix internally linux provides an implementation of semaphores for its own use that is code that is part of the kernel can invoke kernel semaphores these kernel semaphores cannot be accessed directly by the user program via sys tem calls they are implemented as functions within the kernel and are thus more efficient than user visible semaphores linux provides three types of semaphore facilities in the kernel binary sema phores counting semaphores and reader writer semaphores binary and counting semaphores the binary and counting semaphores defined in linux table have the same functionality as described for such semaphores in chapter the function names down and up are used for the functions referred to in chapter as semwait and semsignal respectively a counting semaphore is initialized using the function which gives the semaphore a name and assigns an initial value to the semaphore binary sema phores called mutexes in linux are initialized using the and functions which initialize the semaphore to or respectively linux provides three versions of the down semwait operation the down function corresponds to the traditional semwait operation that is the thread tests the semaphore and blocks if the semaphore is not available the thread will awaken when a corresponding up operation on this sema phore occurs note that this function name is used for an operation on either a counting semaphore or a binary semaphore the function allows the thread to receive and respond to a kernel signal while being blocked on the down operation if the thread is woken up by a signal the function incre ments the count value of the semaphore and returns an error code known in chapter concurrency deadlock and starvation linux as eintr this alerts the thread that the invoked semaphore function has aborted in effect the thread has been forced to give up the semaphore this feature is useful for device drivers and other services in which it is conve nient to override a semaphore operation the function makes it possible to try to acquire a semaphore without being blocked if the semaphore is available it is acquired otherwise this function returns a nonzero value without blocking the thread reader writer semaphores the reader writer semaphore divides users into readers and writers it allows multiple concurrent readers with no writers but only a single writer with no concurrent readers in effect the semaphore functions as a counting semaphore for readers but a binary semaphore mutex for writers table shows the basic reader writer semaphore operations the reader writer semaphore uses uninterruptible sleep so there is only one version of each of the down operations table linux semaphores traditional semaphores void struct semaphore sem int count initializes the dynamically created semaphore to the given count void struct semaphore sem initializes the dynamically created semaphore with a count of initially unlocked void struct sema phore sem initializes the dynamically created semaphore with a count of initially locked void down struct semaphore sem attempts to acquire the given semaphore entering uninterruptible sleep if semaphore is unavailable int struct semaphore sem attempts to acquire the given semaphore enter ing interruptible sleep if semaphore is unavailable returns eintr value if a signal other than the result of an up operation is received int struct semaphore sem attempts to acquire the given semaphore and returns a nonzero value if semaphore is unavailable void up struct semaphore sem releases the given semaphore reader writer semaphores void struct rwsem initializes the dynamically created semaphore with a count of void struct rwsem down operation for readers void struct rwsem up operation for readers void struct rwsem down operation for writers void struct rwsem up operation for writers linux kernel concurrency mechanisms barriers in some architectures compilers and or the processor hardware may reorder mem ory accesses in source code to optimize performance these reorderings are done to optimize the use of the instruction pipeline in the processor the reordering algorithms contain checks to ensure that data dependencies are not violated for example the code a b may be reordered so that memory location b is updated before memory location a is updated however the code a b a will not be reordered even so there are occasions when it is important that reads or writes are executed in the order specified because of use of the information that is made by another thread or a hardware device to enforce the order in which instructions are executed linux provides the memory barrier facility table lists the most important functions that are defined for this facility the rmb operation insures that no reads occur across the bar rier defined by the place of the rmb in the code similarly the wmb operation insures that no writes occur across the barrier defined by the place of the wmb in the code the mb operation provides both a load and store barrier two important points to note about the barrier operations the barriers relate to machine instructions namely loads and stores thus the higher level language instruction a b involves both a load read from loca tion b and a store write to location a the rmb wmb and mb operations dictate the behavior of both the compiler and the processor in the case of the compiler the barrier operation dictates that the compiler not reorder instructions during the compile process in the case of the processor the barrier operation dictates that any instructions pend ing in the pipeline before the barrier must be committed for execution before any instructions encountered after the barrier table linux memory barrier operations rmb prevents loads from being reordered across the barrier wmb prevents stores from being reordered across the barrier mb prevents loads and stores from being reordered across the barrier barrier prevents the compiler from reordering loads or stores across the barrier on smp provides a rmb and on up provides a barrier on smp provides a wmb and on up provides a barrier on smp provides a mb and on up provides a barrier note smp symmetric multiprocessor up uniprocessor chapter concurrency deadlock and starvation the barrier operation is a lighter weight version of the mb operation in that it only controls the compiler behavior this would be useful if it is known that the processor will not perform undesirable reorderings for example the intel processors do not reorder writes the and operations provide an optimization for code that may be compiled on either a uniprocessor up or a symmetric multiproc essor smp these instructions are defined as the usual memory barriers for an smp but for a up they are all treated only as compiler barriers the opera tions are useful in situations in which the data dependencies of concern will only arise in an smp context 9 solaris thread synchronization primitives in addition to the concurrency mechanisms of unix solaris supports four thread synchronization primitives mutual exclusion mutex locks semaphores multiple readers single writer readers writer locks condition variables solaris implements these primitives within the kernel for kernel threads they are also provided in the threads library for user level threads figure shows the data structures for these primitives the initialization functions for the primi tives fill in some of the data members once a synchronization object is created there are essentially only two operations that can be performed enter acquire lock and release unlock there are no mechanisms in the kernel or the threads library to enforce mutual exclusion or to prevent deadlock if a thread attempts to access a piece of data or code that is supposed to be protected but does not use the appropriate synchronization primitive then such access occurs if a thread locks an object and then fails to unlock it no kernel action is taken all of the synchronization primitives require the existence of a hardware instruction that allows an object to be tested and set in one atomic operation mutual exclusion lock a mutex is used to ensure that only one thread at a time can access the resource protected by the mutex the thread that locks the mutex must be the one that unlocks it a thread attempts to acquire a mutex lock by executing the enter primitive if cannot set the lock because it is already set by another thread the blocking action depends on type specific information stored in the mutex object the default blocking policy is a spinlock a blocked thread polls the status of the lock while executing in a busy waiting loop an interrupt based blocking mechanism is optional in this latter case the mutex includes a turnstile id that identifies a queue of threads sleeping on this lock 9 solaris thread synchronization primitives owner octets lock octet waiters octets type specific info octets possibly a turnstile id lock type filler or statistics pointer a mutex lock c reader writer lock waiters octets d condition variable b semaphore figure solaris synchronization data structures the operations on a mutex lock are acquires the lock potentially blocking if it is already held releases the lock potentially unblocking a waiter acquires the lock if it is not already held the primitive provides a nonblocking way of performing the mutual exclusion function this enables the programmer to use a busy wait approach for user level threads which avoids blocking the entire process because one thread is blocked semaphores solaris provides classic counting semaphores with the following primitives decrements the semaphore potentially blocking the thread increments the semaphore potentially unblocking a waiting thread decrements the semaphore if blocking is not required again the primitive permits busy waiting chapter concurrency deadlock and starvation readers writer lock the readers writer lock allows multiple threads to have simultaneous read only access to an object protected by the lock it also allows a single thread to access the object for writing at one time while excluding all readers when the lock is acquired for writing it takes on the status of write lock all threads attempting access for reading or writing must wait if one or more readers have acquired the lock its status is read lock the primitives are as follows attempts to acquire a lock as reader or writer releases a lock as reader or writer acquires the lock if blocking is not required a thread that has acquired a write lock converts it to a read lock any waiting writer remains waiting until this thread releases the lock if there are no waiting writers the primitive wakes up any pending readers attempts to convert a reader lock into a writer lock condition variables a condition variable is used to wait until a particular condition is true condition variables must be used in conjunction with a mutex lock this implements a monitor of the type illustrated in figure the primitives are as follows blocks until the condition is signaled wakes up one of the threads blocked in wakes up all of the threads blocked in releases the associated mutex before blocking and reacquires it before returning because reacquisition of the mutex may be blocked by other threads waiting for the mutex the condition that caused the wait must be retested thus typical usage is as follows m while cv m m this allows the condition to be a complex expression because it is protected by the mutex windows concurrency mechanisms windows provides synchronization among threads as part of the object architecture the most important methods of synchronization are executive dispatcher objects user mode critical sections slim reader writer locks condition variables and lock free windows concurrency mechanisms operations dispatcher objects make use of wait functions we first describe wait func tions and then look at the synchronization methods wait functions the wait functions allow a thread to block its own execution the wait functions do not return until the specified criteria have been met the type of wait func tion determines the set of criteria used when a wait function is called it checks whether the wait criteria have been met if the criteria have not been met the calling thread enters the wait state it uses no processor time while waiting for the criteria to be met the most straightforward type of wait function is one that waits on a single object the waitforsingleobject function requires a handle to one synchroni zation object the function returns when one of the following occurs the specified object is in the signaled state the time out interval elapses the time out interval can be set to infinite to specify that the wait will not time out dispatcher objects the mechanism used by the windows executive to implement synchronization facilities is the family of dispatcher objects which are listed with brief descriptions in table table windows synchronization objects object type definition set to signaled state when effect on waiting threads notification event an announcement that a system event has occurred thread sets the event all released synchronization event an announcement that a system event has occurred thread sets the event one thread released mutex a mechanism that provides mutual exclusion capabilities equivalent to a binary semaphore owning thread or other thread releases the mutex one thread released semaphore a counter that regulates the number of threads that can use a resource semaphore count drops to zero all released waitable timer a counter that records the passage of time set time arrives or time interval expires all released file an instance of an opened file or i o device i o operation completes all released process a program invocation including the address space and resources required to run the program last thread terminates all released thread an executable entity within a process thread terminates all released note shaded rows correspond to objects that exist for the sole purpose of synchronization chapter concurrency deadlock and starvation the first five object types in the table are specifically designed to support synchronization the remaining object types have other uses but also may be used for synchronization each dispatcher object instance can be in either a signaled or unsignaled state a thread can be blocked on an object in an unsignaled state the thread is released when the object enters the signaled state the mechanism is straight forward a thread issues a wait request to the windows executive using the handle of the synchronization object when an object enters the signaled state the windows executive releases one or all of the thread objects that are waiting on that dispatcher object the event object is useful in sending a signal to a thread indicating that a par ticular event has occurred for example in overlapped input and output the system sets a specified event object to the signaled state when the overlapped operation has been completed the mutex object is used to enforce mutually exclusive access to a resource allowing only one thread object at a time to gain access it there fore functions as a binary semaphore when the mutex object enters the signaled state only one of the threads waiting on the mutex is released mutexes can be used to synchronize threads running in different processes like mutexes semaphore objects may be shared by threads in multiple processes the windows semaphore is a counting semaphore in essence the waitable timer object signals at a certain time and or at regular intervals critical sections critical sections provide a synchronization mechanism similar to that provided by mutex objects except that critical sections can be used only by the threads of a single process event mutex and semaphore objects can also be used in a single process application but critical sections provide a much faster more efficient mech anism for mutual exclusion synchronization the process is responsible for allocating the memory used by a critical section typically this is done by simply declaring a variable of type before the threads of the process can use it initialize the critical section by using the initializecriticalsection function a thread uses the entercriticalsection or tryentercriticalsection function to request ownership of a critical section it uses the leavecriticalsection function to release ownership of a critical section if the critical section is currently owned by another thread entercriticalsection waits indefinitely for owner ship in contrast when a mutex object is used for mutual exclusion the wait functions accept a specified time out interval the tryentercriticalsection function attempts to enter a critical section without blocking the calling thread critical sections use a sophisticated algorithm when trying to acquire the mutex if the system is a multiprocessor the code will attempt to acquire a spinlock this works well in situations where the critical section is acquired for only a short time effectively the spinlock optimizes for the case where the thread that currently owns the critical section is executing on another processor if the spinlock cannot be acquired within a reasonable number of iterations a dispatcher object is used to block the thread so that the kernel can dispatch another thread onto the processor windows concurrency mechanisms the dispatcher object is only allocated as a last resort most critical sections are needed for correctness but in practice are rarely contended by lazily allocating the dispatcher object the system saves significant amounts of kernel virtual memory slim read writer locks and condition variables windows vista added a user mode reader writer like critical sections the reader writer lock enters the kernel to block only after attempting to use a spinlock it is slim in the sense that it normally only requires allocation of a single pointer sized piece of memory to use an srw lock a process declares a variable of type srwlock and a calls initializesrwlock to initialize it threads call acquiresrwlockexclusive or acquiresrwlockshared to acquire the lock and releasesrwlockexclusive or releasesrwlockshared to release it windows also has condition variables the process must declare a and initialize it in some thread by calling initializeconditionvariable condition variables can be used with either crit ical sections or srw locks so there are two methods sleepconditionvariablecs and sleepconditionvariablesrw which sleep on the specified condition and releases the specified lock as an atomic operation there are two wake methods wakeconditionvariable and wake allconditionvariable which wake one or all of the sleeping threads respec tively condition variables are used as follows acquire exclusive lock while predicate false sleepconditionvariable perform the protected operation release the lock lock free synchronization windows also relies heavily on interlocked operations for synchronization interlocked operations use hardware facilities to guarantee that memory locations can be read modified and written in a single atomic operation examples include interlockedincrement and interlockedcompareexchange the latter allows a memory location to be updated only if it hasn t changed values since being read many of the synchronization primitives use interlocked operations within their implementation but these operations are also available to programmers for situations where they want to synchronize without taking a software lock these so called lock free synchronization primitives have the advantage that a thread can never be switched away from a processor say at the end of its timeslice while still holding a lock thus they cannot block another thread from running more complex lock free primitives can be built out of the interlocked oper ations most notably windows slists which provide a lock free lifo queue slists are managed using functions like interlockedpushentryslist and interlockedpopentryslist chapter concurrency deadlock and starvation summary deadlock is the blocking of a set of processes that either compete for system resources or communicate with each other the blockage is permanent unless the os takes some extraordinary action such as killing one or more processes or forcing one or more processes to backtrack deadlock may involve reusable resources or consumable resources a reusable resource is one that is not depleted or destroyed by use such as an i o channel or a region of memory a consumable resource is one that is destroyed when it is acquired by a process examples include messages and information in i o buffers there are three general approaches to dealing with deadlock prevention detection and avoidance deadlock prevention guarantees that deadlock will not occur by assuring that one of the necessary conditions for deadlock is not met deadlock detection is needed if the os is always willing to grant resource requests periodically the os must check for deadlock and take action to break the deadlock deadlock avoidance involves the analysis of each new resource request to deter mine if it could lead to deadlock and granting it only if deadlock is not possible recommended reading the classic paper on deadlocks is still well worth a read as is another good survey is is a thorough treatment of deadlock detection is a nice overview of deadlocks two papers by levine clarify some of the concepts used in discussions of deadlock is a useful overview of deadlock describes a deadlock detection package the concurrency mechanisms in unix linux and solaris are well covered in and respectively is a thorough treat ment of unix concurrency and interprocess communication mechanisms key terms review questions and problems key terms review questions and problems key terms banker algorithm deadlock prevention pipe circular wait hold and wait preemption consumable resource joint progress diagram resource allocation graph deadlock memory barrier reusable resource deadlock avoidance message spinlock deadlock detection mutual exclusion starvation review questions give examples of reusable and consumable resources what are the three conditions that must be present for deadlock to be possible what are the four conditions that create deadlock how can the hold and wait condition be prevented list two ways in which the no preemption condition can be prevented how can the circular wait condition be prevented what is the difference among deadlock avoidance detection and prevention problems show that the four conditions of deadlock apply to figure show how each of the techniques of prevention avoidance and detection can be applied to figure for figure provide a narrative description of each of the six depicted paths simi lar to the description of the paths of figure provided in section it was stated that deadlock cannot occur for the situation reflected in figure justify that statement chapter concurrency deadlock and starvation given the following state for the banker algorithm processes through resource types a instances b instances c 9 instances d instances snapshot at time available a b c d current allocation maximum demand process a b c d a b c d p1 p2 p3 p4 a verify that the available array has been calculated correctly b calculate the need matrix c show that the current state is safe that is show a safe sequence of processes in addition to the sequence show how the available working array changes as each process terminates d given the request from process should this request be granted why or why not in the code below three processes are competing for six resources labeled a to f a using a resource allocation graph figures and show the possibility of a deadlock in this implementation b modify the order of some of the get requests to prevent the possibility of any deadlock you cannot move requests across procedures only change the order inside each procedure use a resource allocation graph to justify your answer void while true get a get b get c critical region use a b c release a release b release c void p1 while true get d get e get b critical region use d e b release d release e release b void p2 while true get c get f get d critical region use c f d release c release f release d a spooling system figure consists of an input process i a user process p and an output process o connected by two buffers the processes exchange data in key terms review questions and problems figure a spooling system blocks of equal size these blocks are buffered on a disk using a floating boundary between the input and the output buffers depending on the speed of the processes the communication primitives used ensure that the following resource constraint is satisfied where i o max max maximum number of blocks on disk i number of input blocks on disk o number of output blocks on disk the following is known about the processes as long as the environment supplies data process i will eventually input it to the disk provided disk space becomes available as long as input is available on the disk process p will eventually consume it and output a finite amount of data on the disk for each block input provided disk space becomes available as long as output is available on the disk process o will eventually consume it show that this system can become deadlocked suggest an additional resource constraint that will prevent the deadlock in problem but still permit the boundary between input and output buffers to vary in accor dance with the present needs of the processes 9 in the the multiprogramming system a drum precursor to the disk for secondary storage is divided into input buffers processing areas and output buffers with floating boundaries depending on the speed of the processes involved the current state of the drum can be characterized by the following parameters max maximum number of pages on drum i number of input pages on drum p number of processing pages on drum o number of output pages on drum reso minimum number of pages reserved for output resp minimum number of pages reserved for processing formulate the necessary resource constraints that guarantee that the drum capacity is not exceeded and that a minimum number of pages is reserved permanently for output and processing in the the multiprogramming system a page can make the following state transitions empty input buffer input production input buffer processing area input consumption processing area output buffer output production output buffer empty output consumption empty processing area procedure call processing area empty procedure return a define the effect of these transitions in terms of the quantities i o and p b can any of them lead to a deadlock if the assumptions made in problem about input processes user processes and output processes hold chapter concurrency deadlock and starvation consider a system with a total of units of memory allocated to three processes as shown process max hold apply the banker algorithm to determine whether it would be safe to grant each of the following requests if yes indicate a sequence of terminations that could be guar anteed possible if no show the reduction of the resulting allocation table a a fourth process arrives with a maximum memory need of and an initial need of 25 units b a fourth process arrives with a maximum memory need of and an initial need of units evaluate the banker algorithm for its usefulness in an os a pipeline algorithm is implemented so that a stream of data elements of type t pro duced by a process passes through a sequence of processes p1 p2 pn which operates on the elements in that order a define a generalized message buffer that contains all the partially consumed data elements and write an algorithm for process pi i n of the form repeat receive from predecessor consume element send to successor forever assume receives input elements sent by pn the algorithm should enable the processes to operate directly on messages stored in the buffer so that copying is unnecessary b show that the processes cannot be deadlocked with respect to the common buffer suppose the following two processes foo and bar are executed concurrently and share the semaphore variables s and r each initialized to and the integer variable x initialized to a can the concurrent execution of these two processes result in one or both being blocked forever if yes give an execution sequence in which one or both are blocked forever b can the concurrent execution of these two processes result in the indefinite postponement of one of them if yes give an execution sequence in which one is indefinitely postponed 15 consider a system consisting of four processes and a single resource the current state of the claim and allocation matrices are c what is the minimum number of units of the resource needed to be available for this state to be safe consider the following ways of handling deadlock banker algorithm detect deadlock and kill thread releasing all resources reserve all resources in advance restart thread and release all resources if thread needs to wait resource order ing and detect deadlock and roll back thread actions a one criterion to use in evaluating different approaches to deadlock is which approach permits the greatest concurrency in other words which approach allows the most threads to make progress without waiting when there is no deadlock give a rank order from to for each of the ways of handling deadlock just listed where allows the greatest degree of concurrency comment on your ordering b another criterion is efficiency in other words which requires the least processor overhead rank order the approaches from to with being the most efficient assuming that deadlock is a very rare event comment on your ordering does your ordering change if deadlocks occur frequently 17 comment on the following solution to the dining philosophers problem a hungry phi losopher first picks up his left fork if his right fork is also available he picks up his right fork and starts eating otherwise he puts down his left fork again and repeats the cycle suppose that there are two types of philosophers one type always picks up his left fork first a lefty and the other type always picks up his right fork first a righty the behavior of a lefty is defined in figure the behavior of a righty is as follows begin repeat think wait fork i mod wait fork i eat signal fork i signal fork i mod forever end prove the following a any seating arrangement of lefties and righties with at least one of each avoids deadlock b any seating arrangement of lefties and righties with at least one of each prevents starvation 19 figure 17 shows another solution to the dining philosophers problem using moni tors compare to figure and report your conclusions in table some of the linux atomic operations do not involve two accesses to a variable such as v a simple read operation is obvi ously atomic in any architecture therefore why is this operation added to the reper toire of atomic operations 21 consider the following fragment of code on a linux system where is a reader writer lock what is the effect of this code chapter concurrency deadlock and starvation monitor enum states thinking hungry eating state cond needfork condition variable void int pid pid is the philosopher id number state pid hungry announce that i m hungry if state pid eating state pid eating cwait needfork pid wait if either neighbor is eating state pid eating proceed if neither neighbor is eating void int pid state pid thinking give right higher neighbor a chance to eat if state pid hungry state pid eating csignal needfork pid give left lower neighbor a chance to eat else if state pid hungry state pid eating csignal needfork pid void philosopher k to the five philosopher clients while true think k client requests two forks via monitor eat spaghetti k client releases forks via the monitor figure 17 another solution to the dining philosophers problem using a monitor the two variables a and b have initial values of and respectively the following code is for a linux system thread thread a mb b c b rmb d a what possible errors are avoided by the use of the memory barriers part memory chapter memory management memory management requirements relocation protection sharing logical organization physical organization memory partitioning fixed partitioning dynamic partitioning buddy system relocation paging segmentation security issues buffer overflow attacks defending against buffer overflows summary recommended reading key terms review questions and problems appendix loading and linking chapter memory management i cannot guarantee that i carry all the facts in my mind intense mental concentration has a curious way of blotting out what has passed each of my cases displaces the last and mlle carère has blurred my recollection of baskerville hall tomorrow some other little problem may be submitted to my notice which will in turn dispossess the fair french lady and the infamous upwood the hound of the baskervilles arthur conan doyle in a uniprogramming system main memory is divided into two parts one part for the operating system resident monitor kernel and one part for the program cur rently being executed in a multiprogramming system the user part of memory must be further subdivided to accommodate multiple processes the task of subdi vision is carried out dynamically by the operating system and is known as memory management effective memory management is vital in a multiprogramming system if only a few processes are in memory then for much of the time all of the processes will be waiting for i o and the processor will be idle thus memory needs to be allocated to ensure a reasonable supply of ready processes to consume available processor time we begin with the requirements that memory management is intended to satisfy next we discuss a variety of simple schemes that have been used for memory management table introduces some key terms for our discussion a set of animations that illustrate concepts in this chapter is available online click on the rotating globe at williamstallings com os html for access memory management requirements table memory management terms frame a fixed length block of main memory page a fixed length block of data that resides in secondary memory such as disk a page of data may temporarily be copied into a frame of main memory segment a variable length block of data that resides in secondary memory an entire segment may tempo rarily be copied into an available region of main memory segmentation or the segment may be divided into pages which can be individually copied into main memory combined segmentation and paging memory management requirements while surveying the various mechanisms and policies associated with memory man agement it is helpful to keep in mind the requirements that memory management is intended to satisfy these requirements include the following relocation protection sharing logical organization physical organization relocation in a multiprogramming system the available main memory is generally shared among a number of processes typically it is not possible for the programmer to know in advance which other programs will be resident in main memory at the time of execution of his or her program in addition we would like to be able to swap active processes in and out of main memory to maximize processor utilization by providing a large pool of ready processes to execute once a program is swapped out to disk it would be quite limiting to specify that when it is next swapped back in it must be placed in the same main memory region as before instead we may need to relocate the process to a different area of memory thus we cannot know ahead of time where a program will be placed and we must allow for the possibility that the program may be moved about in main memory due to swapping these facts raise some technical concerns related to addressing as illustrated in figure the figure depicts a process image for simplicity let us assume that the process image occupies a contiguous region of main memory clearly the operating system will need to know the location of process control information and of the execution stack as well as the entry point to begin execution of the program for this process because the operating system is managing mem ory and is responsible for bringing this process into main memory these addresses are easy to come by in addition however the processor must deal with memory chapter memory management process control information branch instruction increasing address values reference to data figure addressing requirements for a process references within the program branch instructions contain an address to reference the instruction to be executed next data reference instructions contain the address of the byte or word of data referenced somehow the processor hardware and oper ating system software must be able to translate the memory references found in the code of the program into actual physical memory addresses reflecting the current location of the program in main memory protection each process should be protected against unwanted interference by other processes whether accidental or intentional thus programs in other processes should not be able to reference memory locations in a process for reading or writing purposes without permission in one sense satisfaction of the relocation require ment increases the difficulty of satisfying the protection requirement because the location of a program in main memory is unpredictable it is impossible to check absolute addresses at compile time to assure protection furthermore most programming languages allow the dynamic calculation of addresses at run time e g by computing an array subscript or a pointer into a data structure hence all memory references generated by a process must be checked at run time to ensure that they refer only to the memory space allocated to that process fortunately we shall see that mechanisms that support relocation also support the protection requirement normally a user process cannot access any portion of the operating system neither program nor data again usually a program in one process cannot branch to an instruction in another process without special arrangement a program in one process cannot access the data area of another process the processor must be able to abort such instructions at the point of execution memory management requirements note that the memory protection requirement must be satisfied by the proces sor hardware rather than the operating system software this is because the os cannot anticipate all of the memory references that a program will make even if such anticipation were possible it would be prohibitively time consuming to screen each program in advance for possible memory reference violations thus it is only possible to assess the permissibility of a memory reference data access or branch at the time of execution of the instruction making the reference to accomplish this the processor hardware must have that capability sharing any protection mechanism must have the flexibility to allow several processes to access the same portion of main memory for example if a number of processes are executing the same program it is advantageous to allow each process to access the same copy of the program rather than have its own separate copy processes that are cooperating on some task may need to share access to the same data structure the memory management system must therefore allow controlled access to shared areas of memory without compromising essential protection again we will see that the mechanisms used to support relocation support sharing capabilities logical organization almost invariably main memory in a computer system is organized as a linear or one dimensional address space consisting of a sequence of bytes or words secondary memory at its physical level is similarly organized while this organi zation closely mirrors the actual machine hardware it does not correspond to the way in which programs are typically constructed most programs are organized into modules some of which are unmodifiable read only execute only and some of which contain data that may be modified if the operating system and computer hardware can effectively deal with user programs and data in the form of modules of some sort then a number of advantages can be realized modules can be written and compiled independently with all references from one module to another resolved by the system at run time with modest additional overhead different degrees of protection read only execute only can be given to different modules it is possible to introduce mechanisms by which modules can be shared among processes the advantage of providing sharing on a module level is that this corresponds to the user way of viewing the problem and hence it is easy for the user to specify the sharing that is desired the tool that most readily satisfies these requirements is segmentation which is one of the memory management techniques explored in this chapter physical organization as we discussed in section computer memory is organized into at least two levels referred to as main memory and secondary memory main memory provides fast access at relatively high cost in addition main memory is volatile that is it chapter memory management does not provide permanent storage secondary memory is slower and cheaper than main memory and is usually not volatile thus secondary memory of large capacity can be provided for long term storage of programs and data while a smaller main memory holds programs and data currently in use in this two level scheme the organization of the flow of information between main and secondary memory is a major system concern the responsibility for this flow could be assigned to the individual programmer but this is impractical and undesirable for two reasons the main memory available for a program plus its data may be insufficient in that case the programmer must engage in a practice known as overlaying in which the program and data are organized in such a way that various modules can be assigned the same region of memory with a main program responsible for switching the modules in and out as needed even with the aid of compiler tools overlay programming wastes programmer time in a multiprogramming environment the programmer does not know at the time of coding how much space will be available or where that space will be it is clear then that the task of moving information between the two levels of memory should be a system responsibility this task is the essence of memory management memory partitioning the principal operation of memory management is to bring processes into main memory for execution by the processor in almost all modern multiprogramming systems this involves a sophisticated scheme known as virtual memory virtual memory is in turn based on the use of one or both of two basic techniques segmen tation and paging before we can look at these virtual memory techniques we must prepare the ground by looking at simpler techniques that do not involve virtual memory table summarizes all the techniques examined in this chapter and the next one of these techniques partitioning has been used in several variations in some now obsolete operating systems the other two techniques simple paging and simple segmentation are not used by themselves however it will clarify the dis cussion of virtual memory if we look first at these two techniques in the absence of virtual memory considerations fixed partitioning in most schemes for memory management we can assume that the os occupies some fixed portion of main memory and that the rest of main memory is available for use by multiple processes the simplest scheme for managing this available memory is to partition it into regions with fixed boundaries partition sizes figure shows examples of two alternatives for fixed partitioning one possibility is to make use of equal size partitions in this case any process whose size is less than or equal to the partition size can be loaded into memory partitioning table memory management techniques technique description strengths weaknesses fixed partitioning main memory is divided into a number of static partitions at system generation time a process may be loaded into a partition of equal or greater size simple to implement little operating system overhead inefficient use of memory due to internal fragmenta tion maximum number of active processes is fixed dynamic partitioning partitions are created dynamically so that each process is loaded into a partition of exactly the same size as that process no internal fragmentation more efficient use of main memory inefficient use of processor due to the need for com paction to counter external fragmentation simple paging main memory is divided into a number of equal size frames each process is divided into a number of equal size pages of the same length as frames a process is loaded by loading all of its pages into available not nec essarily contiguous frames no external fragmentation a small amount of internal fragmentation simple segmentation each process is divided into a number of segments a process is loaded by load ing all of its segments into dynamic partitions that need not be contiguous no internal fragmentation improved memory utiliza tion and reduced overhead compared to dynamic partitioning external fragmentation virtual memory paging as with simple paging except that it is not necessary to load all of the pages of a process nonresident pages that are needed are brought in later automatically no external fragmentation higher degree of multipro gramming large virtual address space overhead of complex memory management virtual memory segmentation as with simple segmenta tion except that it is not necessary to load all of the segments of a process nonresident segments that are needed are brought in later automatically no internal fragmentation higher degree of multipro gramming large virtual address space protection and sharing support overhead of complex memory management any available partition if all partitions are full and no process is in the ready or running state the operating system can swap a process out of any of the partitions and load in another process so that there is some work for the processor there are two difficulties with the use of equal size fixed partitions a program may be too big to fit into a partition in this case the programmer must design the program with the use of overlays so that only a portion of the program need be in main memory at any one time when a module is needed chapter memory management a equal size partitions b unequal size partitions figure example of fixed partitioning of a mbyte memory that is not present the user program must load that module into the pro gram partition overlaying whatever programs or data are there main memory utilization is extremely inefficient any program no matter how small occupies an entire partition in our example there may be a pro gram whose length is less than mbytes yet it occupies an mbyte partition whenever it is swapped in this phenomenon in which there is wasted space internal to a partition due to the fact that the block of data loaded is smaller than the partition is referred to as internal fragmentation both of these problems can be lessened though not solved by using unequal size partitions figure in this example programs as large as mbytes can be accommodated without overlays partitions smaller than mbytes allow smaller programs to be accommodated with less internal fragmentation placement algorithm with equal size partitions the placement of processes in memory is trivial as long as there is any available partition a process can be memory partitioning loaded into that partition because all partitions are of equal size it does not matter which partition is used if all partitions are occupied with processes that are not ready to run then one of these processes must be swapped out to make room for a new process which one to swap out is a scheduling decision this topic is explored in part four with unequal size partitions there are two possible ways to assign processes to partitions the simplest way is to assign each process to the smallest partition within which it will fit in this case a scheduling queue is needed for each parti tion to hold swapped out processes destined for that partition figure the advantage of this approach is that processes are always assigned in such a way as to minimize wasted memory within a partition internal fragmentation although this technique seems optimum from the point of view of an indi vidual partition it is not optimum from the point of view of the system as a whole in figure for example consider a case in which there are no processes with a size between and at a certain point in time in that case the partition will remain unused even though some smaller process could have been assigned to it thus a preferable approach would be to employ a single queue for all processes figure when it is time to load a process into main memory the smallest available partition that will hold the process is selected if all partitions are occupied then a swapping decision must be made preference might be given to swapping out of the smallest partition that will hold the incoming process it is also possible to new processes new processes a one process queue per partition b single queue figure memory assignment for fixed partitioning assumes that one knows the maximum amount of memory that a process will require this is not always the case if it is not known how large a process may become the only alternatives are an overlay scheme or the use of virtual memory chapter memory management consider other factors such as priority and a preference for swapping out blocked processes versus ready processes the use of unequal size partitions provides a degree of flexibility to fixed partitioning in addition it can be said that fixed partitioning schemes are relatively simple and require minimal os software and processing overhead however there are disadvantages the number of partitions specified at system generation time limits the number of active not suspended processes in the system because partition sizes are preset at system generation time small jobs will not utilize partition space efficiently in an environment where the main storage requirement of all jobs is known beforehand this may be reasonable but in most cases it is an inefficient technique the use of fixed partitioning is almost unknown today one example of a suc cessful operating system that did use this technique was an early ibm mainframe operating system os mft multiprogramming with a fixed number of tasks dynamic partitioning to overcome some of the difficulties with fixed partitioning an approach known as dynamic partitioning was developed again this approach has been supplanted by more sophisticated memory management techniques an important operating system that used this technique was ibm mainframe operating system os mvt multiprogramming with a variable number of tasks with dynamic partitioning the partitions are of variable length and number when a process is brought into main memory it is allocated exactly as much mem ory as it requires and no more an example using mbytes of main memory is shown in figure initially main memory is empty except for the os a the first three processes are loaded in starting where the operating system ends and occupying just enough space for each process b c d this leaves a hole at the end of memory that is too small for a fourth process at some point none of the processes in memory is ready the operating system swaps out process e which leaves sufficient room to load a new process process f because process is smaller than process another small hole is created later a point is reached at which none of the processes in main memory is ready but process in the ready suspend state is available because there is insufficient room in memory for process the operating system swaps process out g and swaps process back in h as this example shows this method starts out well but eventually it leads to a situation in which there are a lot of small holes in memory as time goes on mem ory becomes more and more fragmented and memory utilization declines this phenomenon is referred to as external fragmentation indicating that the memory that is external to all partitions becomes increasingly fragmented this is in contrast to internal fragmentation referred to earlier one technique for overcoming external fragmentation is compaction from time to time the os shifts the processes so that they are contiguous and so that all of the free memory is together in one block for example in figure compaction memory partitioning a operating system process process e b operating system process process process f c operating system process process g d operating system process process process h figure the effect of dynamic partitioning will result in a block of free memory of length this may well be sufficient to load in an additional process the difficulty with compaction is that it is a time consuming procedure and wasteful of processor time note that compaction implies the need for a dynamic relocation capability that is it must be possible to move a program from one region to another in main memory without invalidating the memory references in the program see appendix placement algorithm because memory compaction is time consuming the os designer must be clever in deciding how to assign processes to memory how to plug the holes when it is time to load or swap a process into main memory and if there is more than one free block of memory of sufficient size then the operating system must decide which free block to allocate three placement algorithms that might be considered are best fit first fit and next fit all of course are limited to choosing among free blocks of main memory that are equal to or larger than the process to be brought in best fit chooses the block that is closest in size to the request first fit begins to scan memory from the chapter memory management first fit last allocated block 18m 8m a before best fit allocated block free block possible new allocation next fit 8m 6m 14m 20m b after figure example memory conﬁguration before and after allocation of mbyte block beginning and chooses the first available block that is large enough next fit begins to scan memory from the location of the last placement and chooses the next avail able block that is large enough figure shows an example memory configuration after a number of place ment and swapping out operations the last block that was used was a 22 mbyte block from which a mbyte partition was created figure shows the difference between the best first and next fit placement algorithms in satisfying a mbyte allocation request best fit will search the entire list of available blocks and make use of the 18 mbyte block leaving a mbyte fragment first fit results in a mbyte fragment and next fit results in a 20 mbyte fragment which of these approaches is best will depend on the exact sequence of proc ess swappings that occurs and the size of those processes however some general comments can be made see also and the first fit algorithm is not only the simplest but usually the best and fastest as well the next fit algorithm tends to produce slightly worse results than the first fit the next fit algorithm will more frequently lead to an allocation from a free block at the end of memory the result is that the largest block of free memory which usually memory partitioning appears at the end of the memory space is quickly broken up into small fragments thus compaction may be required more frequently with next fit on the other hand the first fit algorithm may litter the front end with small free partitions that need to be searched over on each subsequent first fit pass the best fit algorithm despite its name is usually the worst performer because this algorithm looks for the smallest block that will satisfy the requirement it guarantees that the fragment left behind is as small as possible although each memory request always wastes the smallest amount of memory the result is that main memory is quickly littered by blocks too small to satisfy memory allocation requests thus memory compac tion must be done more frequently than with the other algorithms replacement algorithm in a multiprogramming system using dynamic partitioning there will come a time when all of the processes in main memory are in a blocked state and there is insufficient memory even after compaction for an additional process to avoid wasting processor time waiting for an active process to become unblocked the os will swap one of the processes out of main memory to make room for a new process or for a process in a ready suspend state therefore the operating system must choose which process to replace because the topic of replacement algorithms will be covered in some detail with respect to various virtual memory schemes we defer a discussion of replacement algorithms until then buddy system both fixed and dynamic partitioning schemes have drawbacks a fixed partition ing scheme limits the number of active processes and may use space inefficiently if there is a poor match between available partition sizes and process sizes a dynamic partitioning scheme is more complex to maintain and includes the over head of compaction an interesting compromise is the buddy system in a buddy system memory blocks are available of size words l k u where smallest size block that is allocated largest size block that is allocated generally is the size of the entire memory available for allocation to begin the entire space available for allocation is treated as a single block of size if a request of size such that is made then the entire block is allocated otherwise the block is split into two equal buddies of size if then the request is allocated to one of the two buddies otherwise one of the buddies is split in half again this process continues until the smallest block greater than or equal to is generated and allocated to the request at any time the buddy system maintains a list of holes unallocated blocks of each size a hole may be removed from the i list by splitting it in half to create two buddies of size in the i list whenever a pair of buddies on the i list both become unallocated they are removed from that list and coalesced into a single block on the i 318 chapter memory management list presented with a request for an allocation of size k such that k cs the following recursive algorithm is used to find a hole of size void int i if i u failure if empty i split hole into buddies put buddies on take first hole on figure gives an example using a mbyte initial block the first request a is for kbytes for which a block is needed the initial block is divided into two buddies the first of these is divided into two buddies and the first of these is divided into two buddies one of which is allocated to a the next request b requires a block such a block is already available and is allocated the process continues with splitting and coalescing occurring as needed note that when e is released two buddies are coalesced into a block which is immediately coalesced with its buddy figure shows a binary tree representation of the buddy allocation immedi ately after the release b request the leaf nodes represent the current partitioning of the memory if two buddies are leaf nodes then at least one must be allocated otherwise they would be coalesced into a larger block mbyte block request request request request release b release a request release c release e release d figure example of buddy system memory partitioning a c 64k d 256k leaf node for allocated block leaf node for unallocated block non leaf node figure tree representation of buddy system the buddy system is a reasonable compromise to overcome the disadvantages of both the fixed and variable partitioning schemes but in contemporary operating systems virtual memory based on paging and segmentation is superior however the buddy system has found application in parallel systems as an efficient means of allocation and release for parallel programs e g see a modified form of the buddy system is used for unix kernel memory allocation described in chapter relocation before we consider ways of dealing with the shortcomings of partitioning we must clear up one loose end which relates to the placement of processes in memory when the fixed partition scheme of figure is used we can expect that a pro cess will always be assigned to the same partition that is whichever partition is selected when a new process is loaded will always be used to swap that process back into memory after it has been swapped out in that case a simple relocating loader such as is described in appendix can be used when the process is first loaded all relative memory references in the code are replaced by absolute main memory addresses determined by the base address of the loaded process in the case of equal size partitions figure and in the case of a single proc ess queue for unequal size partitions figure a process may occupy different partitions during the course of its life when a process image is first created it is chapter memory management loaded into some partition in main memory later the process may be swapped out when it is subsequently swapped back in it may be assigned to a different partition than the last time the same is true for dynamic partitioning observe in figure and figure that process occupies two different regions of memory on the two occasions when it is brought in furthermore when compaction is used processes are shifted while they are in main memory thus the locations of instructions and data referenced by a process are not fixed they will change each time a process is swapped in or shifted to solve this problem a distinction is made among several types of addresses a logical address is a reference to a memory location independ ent of the current assignment of data to memory a translation must be made to a physical address before the memory access can be achieved a relative address is a particular example of logical address in which the address is expressed as a location relative to some known point usually a value in a processor register a physical address or absolute address is an actual location in main memory programs that employ relative addresses in memory are loaded using dynamic run time loading see appendix for a discussion typically all of the memory references in the loaded process are relative to the origin of the program thus a hard ware mechanism is needed for translating relative addresses to physical main memory addresses at the time of execution of the instruction that contains the reference figure shows the way in which this address translation is typically accom plished when a process is assigned to the running state a special processor register sometimes called the base register is loaded with the starting address in main memory of the program there is also a bounds register that indicates the ending location relative address figure hardware support for relocation process image in main memory paging of the program these values must be set when the program is loaded into memory or when the process image is swapped in during the course of execution of the proc ess relative addresses are encountered these include the contents of the instruc tion register instruction addresses that occur in branch and call instructions and data addresses that occur in load and store instructions each such relative address goes through two steps of manipulation by the processor first the value in the base register is added to the relative address to produce an absolute address second the resulting address is compared to the value in the bounds register if the address is within bounds then the instruction execution may proceed otherwise an interrupt is generated to the operating system which must respond to the error in some fashion the scheme of figure allows programs to be swapped in and out of mem ory during the course of execution it also provides a measure of protection each process image is isolated by the contents of the base and bounds registers and safe from unwanted accesses by other processes paging both unequal fixed size and variable size partitions are inefficient in the use of memory the former results in internal fragmentation the latter in external frag mentation suppose however that main memory is partitioned into equal fixed size chunks that are relatively small and that each process is also divided into small fixed size chunks of the same size then the chunks of a process known as pages could be assigned to available chunks of memory known as frames or page frames we show in this section that the wasted space in memory for each process is due to internal fragmentation consisting of only a fraction of the last page of a process there is no external fragmentation figure 9 illustrates the use of pages and frames at a given point in time some of the frames in memory are in use and some are free a list of free frames is main tained by the os process a stored on disk consists of four pages when it is time to load this process the os finds four free frames and loads the four pages of process a into the four frames figure process b consisting of three pages and process c consisting of four pages are subsequently loaded then process b is suspended and is swapped out of main memory later all of the processes in main memory are blocked and the os needs to bring in a new process process d which consists of five pages now suppose as in this example that there are not sufficient unused contiguous frames to hold the process does this prevent the operating system from loading d the answer is no because we can once again use the concept of logical address a simple base address register will no longer suffice rather the operating system maintains a page table for each process the page table shows the frame location for each page of the process within the program each logical address consists of a page number and an offset within the page recall that in the case of simple partition a logical address is the location of a word relative to the beginning of the program the processor translates that into a physical address with paging the logical to physical address translation is still done by processor hardware now the processor must know how to access the page table of the current process presented with a logical chapter memory management frame number 5 9 main memory main memory 5 9 main memory 5 9 13 a fifteen available frames 13 b load process a 11 13 c load process b main memory main memory main memory a a a a a a a a a a a a b d 5 b 5 5 d b d c c c c c c 9 c 9 c 9 c c c c 11 11 11 d d 13 13 13 d load process c e swap out b f load process d figure 9 assignment of process to free frames address page number offset the processor uses the page table to produce a physi cal address frame number offset continuing our example the five pages of process d are loaded into frames 5 11 and 12 figure 10 shows the various page tables at this time a page table contains one entry for each page of the process so that the table is easily indexed by the page number starting at page each page table entry contains the number of the frame in main memory if any that holds the corresponding page in addition the os maintains a single free frame list of all the frames in main memory that are currently unoccupied and available for pages thus we see that simple paging as described here is similar to fixed parti tioning the differences are that with paging the partitions are rather small a 3 paging 3 process a process b page table 3 3 process c free frame list page table page table process d page table figure 10 data structures for the example of figure 9 at time epoch f program may occupy more than one partition and these partitions need not be contiguous to make this paging scheme convenient let us dictate that the page size hence the frame size must be a power of with the use of a page size that is a power of 2 it is easy to demonstrate that the relative address which is defined with reference to the origin of the program and the logical address expressed as a page number and offset are the same an example is shown in figure 11 in this exam ple bit addresses are used and the page size is bytes the relative address in binary form is with a page size of an offset field of 10 bits is needed leaving bits for the page number thus a program can consist of a maximum of pages of bytes each as figure shows rel ative address corresponds to an offset of on page 000001 which yields the same bit number the consequences of using a page size that is a power of 2 are twofold first the logical addressing scheme is transparent to the programmer the assembler and relative address a partitioning figure 11 logical addresses logical address page offset b paging page size logical address segment offset c segmentation chapter memory management the linker each logical address page number offset of a program is identical to its relative address second it is a relatively easy matter to implement a function in hardware to perform dynamic address translation at run time consider an address of n m bits where the leftmost n bits are the page number and the rightmost m bits are the offset in our example figure n and m 10 the following steps are needed for address translation extract the page number as the leftmost n bits of the logical address use the page number as an index into the process page table to find the frame number k the starting physical address of the frame is k and the physical address of the referenced byte is that number plus the offset this physical address need not be calculated it is easily constructed by appending the frame number to the offset in our example we have the logical address which is page number offset suppose that this page is residing in main memory frame 6 binary then the physical address is frame number 6 offset figure bit logical address 6 bit page 10 bit offset bit logical address a paging bit physical address bit segment 12 bit offset 1 1 1 1 0 1 1 0 0 0 1 0 0 0 0 bit physical address b segmentation figure 12 examples of logical to physical address translation 4 segmentation to summarize with simple paging main memory is divided into many small equal size frames each process is divided into frame size pages smaller processes require fewer pages larger processes require more when a process is brought in all of its pages are loaded into available frames and a page table is set up this approach solves many of the problems inherent in partitioning 4 segmentation a user program can be subdivided using segmentation in which the program and its associated data are divided into a number of segments it is not required that all seg ments of all programs be of the same length although there is a maximum segment length as with paging a logical address using segmentation consists of two parts in this case a segment number and an offset because of the use of unequal size segments segmentation is similar to dynamic partitioning in the absence of an overlay scheme or the use of virtual memory it would be required that all of a program s segments be loaded into mem ory for execution the difference compared to dynamic partitioning is that with segmentation a program may occupy more than one partition and these partitions need not be contiguous segmentation eliminates internal fragmentation but like dynamic partitioning it suffers from external fragmentation however because a process is broken up into a number of smaller pieces the external fragmentation should be less whereas paging is invisible to the programmer segmentation is usually visible and is provided as a convenience for organizing programs and data typically the programmer or compiler will assign programs and data to different segments for purposes of modular programming the program or data may be further broken down into multiple segments the principal inconvenience of this service is that the programmer must be aware of the maximum segment size limitation another consequence of unequal size segments is that there is no simple rela tionship between logical addresses and physical addresses analogous to paging a simple segmentation scheme would make use of a segment table for each process and a list of free blocks of main memory each segment table entry would have to give the starting address in main memory of the corresponding segment the entry should also provide the length of the segment to assure that invalid addresses are not used when a process enters the running state the address of its segment table is loaded into a special register used by the memory management hardware consider an address of n m bits where the leftmost n bits are the segment number and the rightmost m bits are the offset in our example figure n 4 and m 12 thus the maximum segment size is the following steps are needed for address translation extract the segment number as the leftmost n bits of the logical address use the segment number as an index into the process segment table to find the starting physical address of the segment compare the offset expressed in the rightmost m bits to the length of the seg ment if the offset is greater than or equal to the length the address is invalid chapter memory management the desired physical address is the sum of the starting physical address of the segment plus the offset in our example we have the logical address which is segment number 1 offset suppose that this segment is residing in main mem ory starting at physical address then the physical address is figure to summarize with simple segmentation a process is divided into a number of segments that need not be of equal size when a process is brought in all of its segments are loaded into available regions of memory and a segment table is set up 5 security issues main memory and virtual memory are system resources subject to security threats and for which security countermeasures need to be taken the most obvious secu rity requirement is the prevention of unauthorized access to the memory contents of processes if a process has not declared a portion of its memory to be sharable then no other process should have access to the contents of that portion of memory if a process declares that a portion of memory may be shared by other designated processes then the security service of the os must ensure that only the designated processes have access the security threats and countermeasures discussed in chapter 3 are relevant to this type of memory protection in this section we summarize another threat that involves memory protection part seven provides more detail buffer overflow attacks one serious security threat related to memory management remains to be intro duced buffer overflow also known as a buffer overrun which is defined in the nist national institute of standards and technology glossary of key information security terms as follows a buffer overflow can occur as a result of a programming error when a process attempts to store data beyond the limits of a fixed sized buffer and consequently overwrites adjacent memory locations these locations could hold other program variables or parameters or program control flow data such as return addresses and pointers to previous stack frames the buffer could be located on the stack in the heap or in the data section of the process the consequences of this error include corruption of data used by the program unexpected transfer of control in the program possibly memory access violations and very likely eventual program 5 security issues termination when done deliberately as part of an attack on a system the transfer of control could be to code of the attacker s choosing resulting in the ability to execute arbitrary code with the privileges of the attacked process buffer overflow attacks are one of the most prevalent and dangerous types of security attacks to illustrate the basic operation of a common type of buffer overflow known as stack overflow consider the c main function given in figure this contains three variables valid and 2 whose values will typically be saved in adjacent memory locations their order and location depends on the type of variable local or global the language and compiler used and the target machine architecture for this example we assume that they are saved in consecu tive memory locations from highest to lowest as shown in figure 3 this is typically the case for local variables in a c function on common processor archi tectures such as the intel pentium family the purpose of the code fragment is to call the function to copy into some expected tag value int main int argc char argv int valid false char char 8 gets if strncmp 8 0 valid true printf s s valid d n valid a basic buffer overflow c code cc g o c start start start valid 1 evilinputvalue tvalue evilinputvalue valid 0 badinputbadinput buffer1 badinput badinputbadinput valid 1 b basic buffer overflow example runs figure 13 basic buffer overﬂow example this example the flag variable is saved as an integer rather than a boolean this is done both because it is the classic c style and to avoid issues of word alignment in its storage the buffers are deliberately small to accentuate the buffer overflow issue being illustrated and data values are specified in hexadecimal in this and related figures data values are also shown in ascii where appropriate chapter memory management memory address before gets after gets contains value of bffffbec bffffbdc figure 14 basic buffer overﬂow stack values argv argc return addr old base ptr valid 4 0 3 4 0 3 let s assume this will be the string start it then reads the next line from the standard input for the program using the c library gets function and then compares the string read with the expected tag if the next line did indeed contain just the string start this comparison would succeed and the variable valid would be set to true 4 this case is shown in the first of the three example program runs in figure any other input tag would leave it with the value false such a code fragment might be used to parse some structured network protocol interac tion or formatted text file the problem with this code exists because the traditional c library gets function does not include any checking on the amount of data copied it reads the next line of text from the program s standard input up until the first char acter occurs and copies it into the supplied buffer followed by the null terminator c the logical values false and true are simply integers with the values 0 and 1 or indeed any nonzero value respectively symbolic defines are often used to map these symbolic names to their underlying value as was done in this program newline nl or linefeed lf character is the standard end of line terminator for unix systems and hence for c and is the character with the ascii value 5 security issues used with c strings 6 if more than seven characters are present on the input line when read in they will along with the terminating null character require more room than is available in the buffer consequently the extra characters will overwrite the values of the adjacent variable in this case for example if the input line contained evilinputvalue the result will be that will be over written with the characters tvalue and will use not only the eight characters allocated to it but seven more from as well this can be seen in the second example run in figure the overflow has resulted in corruption of a variable not directly used to save the input because these strings are not equal valid also retains the value false further if 16 or more characters were input additional memory locations would be overwritten the preceding example illustrates the basic behavior of a buffer overflow at its simplest any unchecked copying of data into a buffer could result in corruption of adjacent memory locations which may be other variables or possibly program control addresses and data even this simple example could be taken further knowing the structure of the code processing it an attacker could arrange for the overwritten value to set the value in equal to the value placed in result ing in the subsequent comparison succeeding for example the input line could be the string badinputbadinput this results in the comparison succeeding as shown in the third of the three example program runs in figure and illus trated in figure 14 with the values of the local variables before and after the call to gets note also that the terminating null for the input string was written to the memory location following this means the flow of control in the program will continue as if the expected tag was found when in fact the tag read was some thing completely different this will almost certainly result in program behavior that was not intended how serious this is depends very much on the logic in the attacked program one dangerous possibility occurs if instead of being a tag the values in these buffers were an expected and supplied password needed to access privileged features if so the buffer overflow provides the attacker with a means of accessing these features without actually knowing the correct password to exploit any type of buffer overflow such as those we have illustrated here the attacker needs 1 to identify a buffer overflow vulnerability in some program that can be trig gered using externally sourced data under the attackers control and 2 to understand how that buffer will be stored in the processes memory and hence the potential for corrupting adjacent memory locations and potentially altering the flow of execution of the program identifying vulnerable programs may be done by inspection of program source tracing the execution of programs as they process oversized input or using tools such as fuzzing which we discuss in part seven to automatically identify potentially in c are stored in an array of characters and terminated with the null character which has the ascii value any remaining locations in the array are undefined and typically contain whatever value was previously saved in that area of memory this can be clearly seen in the value in the variable in the before column of figure 14 chapter memory management vulnerable programs what the attacker does with the resulting corruption of memory varies considerably depending on what values are being overwritten defending against buffer overflows finding and exploiting a stack buffer overflow is not that difficult the large num ber of exploits over the previous couple of decades clearly illustrates this there is consequently a need to defend systems against such attacks by either prevent ing them or at least detecting and aborting such attacks countermeasures can be broadly classified into two categories compile time defenses which aim to harden programs to resist attacks in new programs run time defenses which aim to detect and abort attacks in existing programs while suitable defenses have been known for a couple of decades the very large existing base of vulnerable software and systems hinders their deployment hence the interest in run time defenses which can be deployed in operating systems and updates and can provide some protection for existing vulnerable programs 6 summary one of the most important and complex tasks of an operating system is memory management memory management involves treating main memory as a resource to be allocated to and shared among a number of active processes to use the pro cessor and the i o facilities efficiently it is desirable to maintain as many processes in main memory as possible in addition it is desirable to free programmers from size restrictions in program development the basic tools of memory management are paging and segmentation with paging each process is divided into relatively small fixed size pages segmentation provides for the use of pieces of varying size it is also possible to combine segmen tation and paging in a single memory management scheme recommended reading because partitioning has been supplanted by virtual memory techniques most os books offer only cursory coverage one of the more complete and interesting treat ments is in a thorough discussion of partitioning strategies is found in the topics of linking and loading are covered in many books on program development computer architecture and operating systems a particularly detailed treatment is also contains a good discussion a thorough practical discussion of this topic with numerous os examples is 8 key terms review questions and problems 8 key terms review questions and problems key terms absolute loading linkage editor physical address buddy system linking physical organization compaction loading protection dynamic linking logical address relative address dynamic partitioning logical organization relocatable loading dynamic run time loading memory management relocation external fragmentation page segment fixed partitioning page table segmentation frame paging sharing internal fragmentation partitioning review questions 1 what requirements is memory management intended to satisfy 2 why is the capability to relocate processes desirable 3 why is it not possible to enforce memory protection at compile time 4 what are some reasons to allow two or more processes to all have access to a particu lar region of memory 5 in a fixed partitioning scheme what are the advantages of using unequal size partitions 6 what is the difference between internal and external fragmentation what are the distinctions among logical relative and physical addresses 8 what is the difference between a page and a frame 9 what is the difference between a page and a segment problems 1 in section 2 3 we listed five objectives of memory management and in section 1 we listed five requirements argue that each list encompasses all of the concerns ad dressed in the other chapter memory management 2 consider a fixed partitioning scheme with equal size partitions of bytes and a total main memory size of bytes a process table is maintained that includes a pointer to a partition for each resident process how many bits are required for the pointer 3 consider a dynamic partitioning scheme show that on average the memory contains half as many holes as segments 4 to implement the various placement algorithms discussed for dynamic partitioning section 2 a list of the free blocks of memory must be kept for each of the three methods discussed best fit first fit next fit what is the average length of the search 5 another placement algorithm for dynamic partitioning is referred to as worst fit in this case the largest free block of memory is used for bringing in a process a discuss the pros and cons of this method compared to first next and best fit b what is the average length of the search for worst fit 6 this diagram shows an example of memory configuration under dynamic partition ing after a number of placement and swapping out operations have been carried out addresses go from left to right gray areas indicate blocks occupied by processes white areas indicate free memory blocks the last process placed is 2 mbyte and is marked with an x only one process was swapped out after that 1 m x 8m 2m a what was the maximum size of the swapped out process b what was the size of the free block just before it was partitioned by x c a new 3 mbyte allocation request must be satisfied next indicate the intervals of memory where a partition will be created for the new process under the following four placement algorithms best fit first fit next fit worst fit for each algorithm draw a horizontal segment under the memory strip and label it clearly a 1 mbyte block of memory is allocated using the buddy system a show the results of the following sequence in a figure similar to figure 6 request request request return a request return b return d return c b show the binary tree representation following return b 8 consider a buddy system in which a particular block under the current allocation has an address of a if the block is of size 4 what is the binary address of its buddy b if the block is of size 16 what is the binary address of its buddy 9 let buddyk x address of the buddy of the block of size whose address is x write a general expression for buddyk x 10 the fibonacci sequence is defined as follows 0 1 fn 2 fn 1 fn n 0 a could this sequence be used to establish a buddy system b what would be the advantage of this system over the binary buddy system described in this chapter 11 during the course of execution of a program the processor will increment the contents of the instruction register program counter by one word after each instruction fetch but will alter the contents of that register if it encounters a branch or call instruction that causes execution to continue elsewhere in the program now consider figure 8 there are two alternatives with respect to instruction addresses maintain a relative address in the instruction register and do the dynamic address translation using the instruction register as input when a successful branch or call is encountered the relative address generated by that branch or call is loaded into the instruction register maintain an absolute address in the instruction register when a successful branch or call is encountered dynamic address translation is employed with the results stored in the instruction register which approach is preferable 8 key terms review questions and problems 12 consider a simple paging system with the following parameters bytes of physical memory page size of bytes pages of logical address space a how many bits are in a logical address b how many bytes in a frame c how many bits in the physical address specify the frame d how many entries in the page table e how many bits in each page table entry assume each page table entry contains a valid invalid bit 13 write the binary translation of the logical address under the following hypothetical memory management schemes and explain your answer a a paging system with a address page size using a page table in which the frame number happens to be four times smaller than the page number b a segmentation system with a address maximum segment size using a segment table in which bases happen to be regularly placed at real addresses 22 4 x segment 14 consider a simple segmentation system that has the following segment table starting address length bytes 1 222 for each of the following logical addresses determine the physical address or indicate if a segment fault occurs a 0 198 b 2 156 c 1 530 d 3 444 e 0 222 15 consider a memory in which contiguous segments sn are placed in their order of creation from one end of the store to the other as suggested by the following figure sn hole when segment sn 1 is being created it is placed immediately after segment sn even though some of the segments sn may already have been deleted when the boundary between segments in use or deleted and the hole reaches the other end of the memory the segments in use are compacted a show that the fraction of time f spent on compacting obeys the following inequality where f ú 1 f 1 kf where k t 1 2s s average length of a segment in words t average lifetime of a segment in memory references f fraction of the memory that is unused under equilibrium conditions hint find the average speed at which the boundary crosses the memory and assume that the copying of a single word requires at least two memory references b find f for f 0 2 t 1 000 and s chapter memory management appendix loading and linking the first step in the creation of an active process is to load a program into main memory and create a process image figure 15 figure 16 depicts a scenario typ ical for most systems the application consists of a number of compiled or assembled modules in object code form these are linked to resolve any references between modules at the same time references to library routines are resolved the library routines themselves may be incorporated into the program or referenced as shared code that must be supplied by the operating system at run time in this appendix we summarize the key features of linkers and loaders for clarity in the presentation we begin with a description of the loading task when a single program module is involved no linking is required loading in figure 16 the loader places the load module in main memory starting at loca tion x in loading the program the addressing requirement illustrated in figure 1 must be satisfied in general three approaches can be taken absolute loading relocatable loading dynamic run time loading absolute loading an absolute loader requires that a given load module always be loaded into the same location in main memory thus in the load module presented to the loader all address references must be to specific or absolute main program data object code figure 15 the loading function process image in main memory appendix loading and linking static library x module 1 module 2 load module dynamic library module n figure 16 a linking and loading scenario main memory memory addresses for example if x in figure 16 is location then the first word in a load module destined for that region of memory has address the assignment of specific address values to memory references within a program can be done either by the programmer or at compile or assembly time table 3a there are several disadvantages to the former approach first every programmer would have to know the intended assignment strategy for placing mod ules into main memory second if any modifications are made to the program that involve insertions or deletions in the body of the module then all of the addresses will have to be altered accordingly it is preferable to allow memory references within programs to be expressed symbolically and then resolve those symbolic refer ences at the time of compilation or assembly this is illustrated in figure 17 every reference to an instruction or item of data is initially represented by a symbol in preparing the module for input to an absolute loader the assembler or compiler will convert all of these references to specific addresses in this example for a module to be loaded starting at location as shown in figure 17b relocatable loading the disadvantage of binding memory references to specific addresses prior to loading is that the resulting load module can only be placed in one region of main memory however when many programs share main memory it may not be desirable to decide ahead of time into which region of memory a particular module should be loaded it is better to make that decision at load time thus we need a load module that can be located anywhere in main memory to satisfy this new requirement the assembler or compiler produces not actual main memory addresses absolute addresses but addresses that are relative to some known point such as the start of the program this technique is illustrated in figure 17c the start of the load module is assigned the relative address 0 and chapter memory management table 7 3 address binding a loader binding time function programming time all actual physical addresses are directly specified by the programmer in the program itself compile or assembly time the program contains symbolic address references and these are converted to actual physical addresses by the compiler or assembler load time the compiler or assembler produces relative addresses the loader translates these to absolute addresses at the time of program loading run time the loaded program retains relative addresses these are converted dynamically to absolute addresses by processor hardware b linker linkage time function programming time no external program or data references are allowed the programmer must place into the program the source code for all subprograms that are referenced compile or assembly time the assembler must fetch the source code of every subroutine that is referenced and assemble them as a unit load module creation all object modules have been assembled using relative addresses these modules are linked together and all references are restated relative to the origin of the final load module load time external references are not resolved until the load module is to be loaded into main memory at that time referenced dynamic link modules are appended to the load module and the entire package is loaded into main or virtual memory run time external references are not resolved until the external call is executed by the processor at that time the process is interrupted and the desired module is linked to the calling program symbolic addresses absolute addresses relative addresses main memory addresses 0 x x y 400 x x a object module b absolute load module c relative load module d relative load module loaded into main memory starting at location x figure 7 17 absolute and relocatable load modules appendix loading and linking all other memory references within the module are expressed relative to the begin ning of the module 